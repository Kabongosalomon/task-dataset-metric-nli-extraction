<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bag of Visual Words and Fusion Methods for Action Recognition: Comprehensive Study and Good Practice</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2014-05-18">18 May 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Peng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingxing</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Information Sciences and Technology</orgName>
								<orgName type="institution">Southwest Jiaotong University</orgName>
								<address>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong SAR</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">School of Electrical and Electronic Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Bag of Visual Words and Fusion Methods for Action Recognition: Comprehensive Study and Good Practice</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2014-05-18">18 May 2014</date>
						</imprint>
					</monogr>
					<note>Noname manuscript No. (will be inserted by the editor)</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>has never been investigated before. This paper aims to provide a comprehensive study of all steps in BoVW and different fusion methods, and uncover some good practice to produce a state-of-the-art action recognition system. Specifically, we explore two kinds of local features, ten kinds of encoding methods, eight kinds of pooling and normalization strategies, and three kinds of fusion methods. We conclude that every step is crucial for contributing to the final recognition rate and improper choice in one of the steps may counteract the performance improvement of other steps. Furthermore, based on our comprehensive study, we propose a simple yet effective representation, called hybrid representation, by exploring the complementarity of different BoVW frameworks and local descriptors. Using this representation, we obtain the state-of-the-art on the three challenging datasets: HMDB51 (61.1%), UCF50 (92.3%), and UCF101 (87.9%).</p><p>Keywords Action recognition · Bag of Visual Words · Fusion methods · Survey</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Human action recognition <ref type="bibr" target="#b21">[1,</ref><ref type="bibr" target="#b62">43]</ref> has become an important area in computer vision research, whose aim is to automatically classify the action ongoing in a video. It is one of the challenging problems in computer vision for serval reasons. Firstly, there are large intra-class variations in the same action class, caused by various motion speeds, viewpoint changes, and background clutter. Secondly, the identification of an action class is related to many other high-level visual clues, such as human pose, interacting objects, and scene class. These related problems are very difficult themselves. Furthermore, the determination of temporal extent for an actions is more subjective than a static object, which means there is no precise definition about when an action starts and finishes. Finally, the high dimension and low quality of video data usually add difficulty to developing robust and efficient recognition algorithm.</p><p>Early approaches interpret an action as a set of space-time trajectories of 2-dimensional or 3-dimensional points of human joints <ref type="bibr" target="#b74">[55,</ref><ref type="bibr" target="#b51">32,</ref><ref type="bibr" target="#b27">7,</ref><ref type="bibr" target="#b79">60]</ref>. These methods usually need dedicate techniques to detect body parts or track them at each frame. However, the detection and tracking of body part is still an unsolved problem in realistic videos. Recently, recognition methods using local spatiotemporal features <ref type="bibr" target="#b43">[24,</ref><ref type="bibr" target="#b44">25,</ref><ref type="bibr" target="#b65">46,</ref><ref type="bibr" target="#b72">53]</ref> have become the main stream and obtained the state-of-theart performance on many datasets <ref type="bibr" target="#b66">[47]</ref>. These methods do not require algorithms to detect human body, which treat the action volume as a rigid 3D-object and extract appropriate features to describe the patterns of each 3D volume. They are robust to background clutter, illumination changes, and noise.</p><p>Bag of Visual Words (BoVW) framework with local features and its variants <ref type="bibr" target="#b67">[48,</ref><ref type="bibr" target="#b76">57,</ref><ref type="bibr" target="#b39">20,</ref><ref type="bibr" target="#b48">29,</ref><ref type="bibr" target="#b53">34]</ref> have dominated the research work of action recognition and showed their effectiveness in the recent THUMOS'13 Action Recognition Challenge <ref type="bibr" target="#b37">[18]</ref>. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, the pipeline of BoVW for video based action recognition consists of five steps: (i) feature extraction, (ii) feature pre-processing, (iii) codebook generation, (iv) feature encoding, and (v) pooling and normalization. In each step, many efforts have been made and several progress has been obtained. Regarding local features, many successful feature extractors (e.g. STIPs <ref type="bibr" target="#b43">[24]</ref>, Dense Trajectories <ref type="bibr" target="#b65">[46]</ref>) and descriptors (e.g. HOG <ref type="bibr" target="#b44">[25]</ref>, HOF <ref type="bibr" target="#b44">[25]</ref>, MBH <ref type="bibr" target="#b65">[46]</ref>) have been designed for representing the visual patterns of cuboid. Feature pre-processing technique mainly de-correlates these descriptors to make the following representation learning more stable. For codebook generation, it aims to describe the local feature space and provide a partition (e.g. k-means <ref type="bibr" target="#b23">[3]</ref>) or generative process (e.g. GMMs <ref type="bibr" target="#b23">[3]</ref>) for local descriptor. Feature encoding is a hot topic in image classification and many alternatives have been developed for effective representation and efficient implementation (see good surveys <ref type="bibr" target="#b29">[9,</ref><ref type="bibr" target="#b33">14]</ref>). Max pooling <ref type="bibr" target="#b80">[61]</ref> and sum pooling <ref type="bibr" target="#b83">[64]</ref> are usually used to aggregate information from a spatiotemporal region. For normalization methods, typical choices include ℓ 1normalization <ref type="bibr" target="#b83">[64]</ref>, ℓ 2 -normalization <ref type="bibr" target="#b69">[50]</ref>, power normalization <ref type="bibr" target="#b54">[35]</ref>, and intra normalization <ref type="bibr" target="#b22">[2]</ref>. How to make decision in each step to obtain the best pipeline of BoVW for action recognition still remains unknown and needs to be extensively explored.</p><p>Meanwhile, unlike static image, video data exhibits different views of visual pattern, such as appearance, motion, and motion boundary, and all of them play important roles in action recognition. Therefore, multiple descriptors are usually extracted from a cuboid and each descriptor corresponds to the specific aspect of the visual data <ref type="bibr" target="#b65">[46,</ref><ref type="bibr" target="#b44">25]</ref>. BoVW is mainly designed for a single descriptor and ignores the problem of fusing multiple descriptors. Many research works have been devoted to fusing multiple descriptor for boosting performance <ref type="bibr" target="#b31">[11,</ref><ref type="bibr" target="#b63">44,</ref><ref type="bibr" target="#b60">41,</ref><ref type="bibr" target="#b66">47,</ref><ref type="bibr" target="#b26">6]</ref>. Typical fusion methods include descriptor level fusion <ref type="bibr" target="#b44">[25,</ref><ref type="bibr" target="#b73">54]</ref>, representation level fusion <ref type="bibr" target="#b67">[48,</ref><ref type="bibr" target="#b65">46]</ref>, and score level fusion <ref type="bibr" target="#b60">[41,</ref><ref type="bibr" target="#b49">30]</ref>. For descriptor level fusion, multiple descriptors from the same cuboid are concatenated as a whole one and fed into BoVW framework. For representation level fusion, the fusion is conducted in the video level, where each descriptor is firstly fed into BoVW framework independently and the resulting global representations are then concatenated to train a final classifier. For score level fusion, each descriptor is separately input into BoVW framework and used to train a recognition classifier. Then the scores from multiple classifiers are fused using arithmetic mean or geometric mean. In general, these fusion methods are developed in different scenarios and adapted for action recognition by different works. How these fusion methods influence the final recognition of BoVW framework and whether there exists a best one for action recognition is an interesting question and well worth of a detailed investigation.</p><p>Several related study works have been performed about encoding methods for image classification <ref type="bibr" target="#b29">[9,</ref><ref type="bibr" target="#b33">14]</ref> and action recognition <ref type="bibr" target="#b73">[54]</ref>. But these study works are with image classification task or lacking full exploration of all steps in BoVW framework. Meanwhile, the study work of action recognition <ref type="bibr" target="#b73">[54]</ref> is limited regarding the evaluation dataset and ignores the influence of fusion methods. This article aims to provide a comprehensive study of all steps in BoVW and different fusion methods, and uncover some good practice to produce a stateof-the-art action recognition system. Our work is mainly composed of three parts:</p><p>Exploration of BoVW. We place an emphasis on extensively explorations about all components in BoVW pipeline and discovery of useful practice tips. Specifically, we investigate two widely-used local features, namely Space Time Interest Points (STIPs) with HOG, HOF <ref type="bibr" target="#b43">[24]</ref>, and Improved Dense Trajectories (iDTs) with HOG, HOF, MBH <ref type="bibr" target="#b66">[47]</ref>. For feature encoding methods, the current approaches can be roughly classified into three categories: (i) voting based encoding methods, (ii) reconstruction based encoding methods, (iii) super vector based encoding methods. For each type of encoding methods, we choose several representative approaches and totally analyze ten encoding methods. Meanwhile, we explore the relations among these different encoding methods and provide an unified and generative perspective over these encoding methods. We fully explored eight pooling and normalization strategies for each encoding method. From our extensive study of different components in BoVW, server good practice can be concluded:</p><p>-Dense features with more descriptors are more informative in capturing the content of video data and suitable for action recognition. Meanwhile, dense features may exhibit different properties with sparse features with respect to variations of BoVW such as codebook size and encoding methods. -Data pre-processing is an important step in BoVW pipeline and able to greatly improve the final recognition performance. -Basically, high dimensional representation of super vector is more effective and efficient than the other two types of encoding methods. -Pooling and normalization is a crucial step in BoVW, whose importance may not be highlighted in previous studies. Sum pooling with power ℓ 2normalization is the best choice during all the possible combinations. -In above, every step is crucial for contributing to the final recognition rate. Improper choice in one of the steps may counteract the performance improvement of other steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Investigation of Fusion Methods.</head><p>As combination of multiple descriptors is very crucial for performance improvement, we also investigate the influence of different fusion methods inour designed action recognition system. Specifically, we study three kinds of fusion methods, namely descriptor level fusion, representation level fusion, and descriptor level fusion. We find that the way different descriptors correlate with each other determines the effectiveness of fusion methods. The performance gain obtained from fusing multiple descriptors mainly owns to their complementarity. We observe that this complementarity is not only with multiple descriptors, but also with multiple BoVW models. Based on this view, we propose a new representation, called hybrid representation, combining the outputs of multiple BoVW models of different descriptors. This representation utilizes the benefit of each BoVW and fully considers the complementarity among them. In spite of its simplicity, this representation turns out to be effective for improving final recognition rate.</p><p>Comparison with the State of the Art. Guided by the practice tips concluded from our insightful analysis of BoVW variants and feature fusion methods, we design an effective action recognition system using our proposed hybrid representation, and demonstrates its performance on three challenging datasets: HMDB51 <ref type="bibr" target="#b42">[23]</ref>, UCF50 <ref type="bibr" target="#b55">[36]</ref>, and UCF101 <ref type="bibr" target="#b59">[40]</ref>. Specifically, we leverage the richness and effectiveness of low-level features, design a hybrid super vector, a combination of Fisher vector <ref type="bibr" target="#b54">[35]</ref> and SVC-k, and resort to representation level fusion to boost final recognition performance. From comparison with other methods, we conclude that our recognition system reaches the stateof-the-art performance on the three datasets, and our hybrid representation acts as a new baseline for further research of action recognition.</p><p>The rest of this paper is organized as follows. In Section 2, we give an detailed description of each step in BoVW framework of action recognition system. Meanwhile, we uncover several useful techniques commonly adopted in these encoding methods, and provide a unified generative perspective over these encoding methods. Then, several fusion methods and a new representation are introduced in Section 3. Finally, we empirically evaluate the BoVW frameworks and fusion methods on three challenging datasets. We analyze these experiment results and uncover good practice for constructing a state-of-the-art action recognition system. We conclude the paper in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Framework of Bag of Visual Words</head><p>As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, the pipeline of Bag of Visual Words (BoVWs) framework consists of five steps: (i) feature extraction, (ii) feature pre-processing, (iii) codebook generation, (iv) feature encoding, and (v) pooling and normalization. Then the global representation is fed into a classifier such as linear SVM for action recognition. In this section, we will give detailed descriptions of the popular technical choices in each step, which are very important for constructing a state-of-the-art recognition system. Furthermore, we summarize several use techniques in these encoding methods and provide a unified generative perspective over these different encoding methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Feature Extraction</head><p>Low-level local features have become popular in action recognition due to their robustness to background clutter and independence on detection and tracking techniques. These local features are typically divided into two parts: detecting a local region (detector) and describing the detected region (descriptor) <ref type="bibr" target="#b68">[49]</ref>. Many feature detectors have been developed such as  <ref type="bibr" target="#b43">[24]</ref>, 3D-Hessian <ref type="bibr" target="#b75">[56]</ref>, Cuboid <ref type="bibr" target="#b30">[10]</ref>, Dense Trajectories <ref type="bibr" target="#b65">[46]</ref>, and Improved Dense Trajectories <ref type="bibr" target="#b66">[47]</ref>. These detectors try to select locations and scales in video by maximizing certain kind of function or using dense sampling strategy. To describe the extracted region, several hand-crafted features have been designed such as Histogram of Oriented Gradients (HOG) <ref type="bibr" target="#b44">[25,</ref><ref type="bibr" target="#b65">46]</ref>, Histogram of Oriented Flow (HOF) <ref type="bibr" target="#b44">[25,</ref><ref type="bibr" target="#b65">46]</ref>, and Motion Boundary Histogram (MBH) <ref type="bibr" target="#b65">[46,</ref><ref type="bibr" target="#b66">47]</ref>. Multiple descriptors are usually adopted to represent the local region, each of which corresponds to a certain aspect of visual pattern such as static appearance, motion, and motion boundary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3D-Harris</head><p>Among these local features, Space Time Interest Points (STIPs) <ref type="bibr" target="#b43">[24]</ref> and Improved Dense Trajectories (iDTs) <ref type="bibr" target="#b65">[46]</ref> are widely used due to their easy usages and good performance. STIPs resort to 3D-Harris to extract regions of high motion salience, which resulting a set of sparse interest points. For each interest point, STIPs extracted two kinds of descriptors, namely HOG and HOF. iDTs features are an improved version from Dense Trajectories (DTs), where a set of dense trajectories are firstly obtained by tracking pixels with median filter, and five kinds of descriptors are extracted, namely trajectory shape, HOG, HOF, MBHx, and MBHy. iDTs improve the performance of DTs by taking into account camera motion correction. Generally speaking, iDTs resort to more sophisticated engineering skills and integrate much richer low-level visual cues compared with STIPs. Therefore, they represent two different kinds of low level features, namely sparse features and dense features, and may exhibit different properties with respect to variants of BoVWs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Feature Pre-processing</head><p>The low-level local descriptors are usually high dimensional and strong correlated, which results in great challenges in the subsequent unsupervised learning such as k-means clustering and GMM training. Principal component analysis (PCA) <ref type="bibr" target="#b23">[3]</ref> is a statistical procedure to pre-process these features, which uses orthogonal transform to map feature into a set of linearly uncorrelated variables called principal components. Typically, the number of used principal components is less than the number of original variables, thus resulting in dimension reduction. Whitening technique usually follows the PCA, which aims to ensure the feature have the same variance through different dimensions. The transform formula of pre-processing is as followings:</p><formula xml:id="formula_0">x = ΛU ⊤ f ,<label>(1)</label></formula><formula xml:id="formula_1">where f ∈ R M is the original feature, x ∈ R N is the PCA-whitened result, U ∈ R M×N is the dimension re- duction matrix from PCA, Λ is the diagonal whitening matrix diag(Λ) = [1/ √ λ 1 , · · · , 1/ √ λ N ]</formula><p>, and λ i is the i th largest eigenvalue of covariance matrix.</p><p>It is worth noting that this step is not necessary and many previous encoding approaches skip this step, such as Vector Quantization <ref type="bibr" target="#b57">[38]</ref>, Sparse Coding <ref type="bibr" target="#b80">[61]</ref>, and  </p><formula xml:id="formula_2">s(i) = 1, if i = arg min j ||x − d j || 2 2 , s.t. ||s|| 0 = 1 K 2. Soft-assignment (SA) / Kernel Codebook Coding (KCB) s(i) = exp(−β||x−d i || 2 2 ) K i=1 exp(−β||x−d i || 2 2 ) K 3. Localized Soft Assignment (SA-k) s(i) = exp(−β||x−d i || 2 2 ) k i=1 exp(−β||x−d i || 2 2 ) , if d i ∈ N k (x), s.t. ||s|| 0 = k K 4. Salient Coding (SC) s(i) = k j=2 ( ||x−d j || 2 −||x−d 1 || 2 ||x−d j || 2 ), d j ∈ N k (x), if i = arg min j ||x − d j || 2 2 , s.t. ||s|| 0 = 1 K 5. Group Salient Coding (GSC) s(i) = max{v k (i)}, group size k = 1, ..., M , v k (i) = M+1−k j=1 ||x − d k+j || 2 − ||x − d k || 2 , if d i ∈ N k (x),</formula><formula xml:id="formula_3">S = [s(i), s(i)(x − d i ) T U i ] K i=1 , U i ∈ R D×C is a projection matrix K(1 + C)</formula><p>11. Super Vector Coding (SVC) S = [0, 0, · · · , αs(i)</p><formula xml:id="formula_4">N √ p i , s(i) N √ p i (x − d i ), · · · , 0, 0], where i = arg min j ||x − d j || 2 2 K(1 + D)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12.">Fisher Vector (FV)</head><formula xml:id="formula_5">S = [G x µ,1 , ..., G x µ,K , G x σ,1 , ..., G x σ,K ], where G x µ,i = 1 √ π i γ i ( x−µ i σ i ), G x σ,i = 1 √ 2π i γ i [ (x−µ i ) 2 σ 2 i − 1], γ(i) = π i N (x;µ i ,Σ i ) K j=1 π j N (x;µ j ,Σ j ) 2KD 13. Vector of Locally Aggregated Descriptors (VLAD) S = [0, · · · , (x − d i ), · · · , 0], where i = arg min j ||x − d j || 2 2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KD</head><p>Vector of Locally Aggregated Descriptor <ref type="bibr" target="#b35">[16]</ref>. However, in our evaluation, we found this step is of great importance to improve the recognition performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Codebook Generation</head><p>In this section, we present the codebook generation algorithms used for the following feature encoding methods. Generally there are two kinds of approaches:</p><p>(i) partitioning the feature space into regions, each of which is represented by its center, called codeword, and (ii) using generative model to capture the probability distribution of features. k-mean <ref type="bibr" target="#b23">[3]</ref> is a typical method for the first type, and Gaussian Mixture Model (GMM) <ref type="bibr" target="#b23">[3]</ref> is widely used for the second.</p><p>k-means. There are many vector quantization methods such as k-means clustering <ref type="bibr" target="#b23">[3]</ref>, hierarchical clustering <ref type="bibr" target="#b38">[19]</ref>, and spectral clustering <ref type="bibr" target="#b50">[31]</ref>. Among them, k-means is probably the most popular way to construct codebook. Given a set of local features {x 1 , · · · , x M }, where x m ∈ R D . Our goal is to partition the feature set into K clusters {d 1 , · · · , d K }, where d k ∈ R D is a prototype associated with the k th cluster. Suppose for each feature x m , we introduce a corresponding set of binary indicator variables r mk ∈ {0, 1}. If descriptor x m is assigned to cluster k, then r mk = 1 and r mj = 0 for j = k. We can then define an objective function:</p><formula xml:id="formula_6">min J ({r mk , d k }) = M m=1 K k=1 r mk x m − d k 2 2 .</formula><p>(</p><p>The problem is to find values for {r mk } and {d k } to minimize the objective function J . Usually, we can optimize it in an iterative procedure where each iteration involves two successive steps corresponding to optimization with respect to the r nk and d k . The details can be found in <ref type="bibr" target="#b23">[3]</ref>. GMM. Gaussian Mixture Model is a generative model to describe the distribution over feature space:</p><formula xml:id="formula_8">p(x; θ) = K k=1 π k N (x; µ k , Σ k ),<label>(3)</label></formula><p>where K is mixture number, and</p><formula xml:id="formula_9">θ = {π 1 , µ 1 , Σ 1 , · · · , π K , µ K , Σ K } are model parameters. N (x; µ k , Σ k ) is D- dimensional Gaussian distribution.</formula><p>Given the feature set X = {x 1 , · · · , x M }, the optimal parameters of GMM are learned through maximum likelihood estimation arg max θ ln p(X; θ). We use the iterative EM algorithm <ref type="bibr" target="#b23">[3]</ref> to solve this problem.</p><p>k-means algorithm performs a hard assignment of feature descriptor to codeword, while the EM algorithm of GMM makes soft assignment of feature to each mixture component based on posterior probabilities p(k|x). But unlike k-means, GMM delivers not only the mean information of code words, but also the shape of their distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Encoding Methods</head><p>In this section, we provide a detailed description of thirteen feature encoding methods. According to the characteristics of encoding methods, they can be roughly classified into three groups, namely (i) voting based encoding method, (ii) reconstruction based encoding method, and (iv) super vector encoding method, as shown in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>Let X be a set of D-dimensional local descriptors extracted from a video,</p><formula xml:id="formula_10">X = [x 1 , x 2 , · · · , x N ] ∈ R D×N . Given a codebook with K codewords, D = [d 1 , d 2 , · · · , d K ] ∈ R D×K .</formula><p>The objective of encoding is to compute a code s (or S) 1 for input x with D. <ref type="table" target="#tab_0">Table  1</ref> lists all the formulations and dimension of encoding methods, where s(i) denotes the i th element of s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1">Voting based encoding methods</head><p>Voting based encoding methods <ref type="bibr" target="#b57">[38,</ref><ref type="bibr">12,</ref><ref type="bibr" target="#b47">28,</ref><ref type="bibr" target="#b32">13,</ref><ref type="bibr" target="#b78">59]</ref> are designed from the perspective of encoding process and each descriptor directly votes for the codeword using a specific strategy. A K-dimensional (K is the size of codebook) code s is constructed for each single descriptor to represent the votes of the whole codebook. Methods along this line include Vector Quantization(or Hard Voting) <ref type="bibr" target="#b57">[38]</ref>, Soft Assignment (or Kernel Codebook Coding) <ref type="bibr">[12]</ref>, Localized Soft Assignment <ref type="bibr" target="#b47">[28]</ref>, Salient Coding <ref type="bibr" target="#b32">[13]</ref>, and Group Salient Coding <ref type="bibr" target="#b78">[59]</ref>, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>.</p><p>For each descriptor x, the voting value for the codeword d i can be viewed as a function of x, namely s(i) = φ(x). Different encoding methods differ in the formulation of φ(x). For encoding of Vector Quantization (VQ):</p><formula xml:id="formula_11">VQ: φ(x) =        1, if i = arg min j ||x − d j || 2 , 0, otherwise,<label>(4)</label></formula><p>where each descriptor x only votes for its nearest codeword. The VQ encoding method can be viewed as a hard quantization and may cause much information loss. To encounter this problem, Soft Assignment (SA) encoding method votes for all the codewords:</p><formula xml:id="formula_12">SA: φ(x) = ω i ,<label>(5)</label></formula><p>where ω i is the normalized weight of descriptor x with respect to codeword d i :</p><formula xml:id="formula_13">ω i = exp(−β x − d i 2 2 ) K j=1 exp(−β x − d j 2 2 ) ,<label>(6)</label></formula><p>where β is a smoothing factor controlling the softness of the assignment. Considering the manifold structure in the descriptor space, localized Soft Assignment (SA-k) votes for its k-nearest codewords: where I(x, d i ) is the indicator function to identify whether d i belongs to the k nearest neighbor of x:</p><formula xml:id="formula_14">SA-k : φ(x) = ω ′ i = I(x, d i ) exp(−β x − d i 2 2 ) K j=1 I(x, d j ) exp(−β x − d j 2 2 ) ,<label>(7)</label></formula><formula xml:id="formula_15">(a) VQ (b) SA (c) SA-k (d) SC/GSC</formula><formula xml:id="formula_16">I(x, d i ) =        1 if d i ∈ N N k (x), 0 otherwise.<label>(8)</label></formula><p>Note that VQ can be viewed as a special case of SA-k when k is set as 1. <ref type="figure" target="#fig_1">Figure 2</ref> illustrates the difference of these voting based encoding methods. VQ, Salient coding, and Group salient coding are all hard assignment strategies. Unlike VQ, the Salient coding employs the difference between the closest visual word and the other k − 1 closest ones to obtain the voted weight but not 1. The detailed formulations of Salient coding and Group salient coding can be found in <ref type="table" target="#tab_0">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2">Reconstruction based encoding methods</head><p>Reconstruction based encoding methods <ref type="bibr" target="#b80">[61,</ref><ref type="bibr" target="#b69">50,</ref><ref type="bibr" target="#b82">63,</ref><ref type="bibr" target="#b61">42]</ref> are designed from the perspective of decoding process, where the codes s are enforced to reconstruct the input descriptor x. This kind of algorithm includes Orthogonal Matching Pursuit (OMP) <ref type="bibr" target="#b61">[42]</ref>, Sparse Coding (SPC) <ref type="bibr" target="#b80">[61]</ref>, Local Coordinate Coding(LCC) <ref type="bibr" target="#b82">[63]</ref>, and Locality-constrained Linear Coding (LLC) <ref type="bibr" target="#b69">[50]</ref>. Typically, these encoding methods are formulated in a least square framework with a regularization term:</p><formula xml:id="formula_17">arg min s ||x − Ds|| 2 2 + λψ(s),<label>(9)</label></formula><p>where the least square term enforce the small reconstruction error, ψ(s) encourages some properties of codes s, λ is a weight factor to balance this two terms. Among these methods, OMP and SPC pursue a sparse representation. As for OMP, this constraint is conducted by ℓ 0 -norm:</p><formula xml:id="formula_18">OMP: ψ(s) = ||s|| 0<label>(10)</label></formula><p>where ℓ 0 -norm means the number of non-zero elements in s. However, due to the non-convexity of ℓ 0 -norm, solution to this problem usually needs some heuristic strategy and obtains an approximate optimal solution. SPC relaxes this non-convex ℓ 0 -norm with ℓ 1 -norm:</p><formula xml:id="formula_19">SPC: ψ(s) = ||s|| 1<label>(11)</label></formula><p>where ℓ 1 -norm can also encourage the sparsity in code s, and the solution is equal to the solution of ℓ 0norm under some conditions <ref type="bibr" target="#b25">[5]</ref>. The ℓ 1 -norm relaxation allows for more efficient optimization algorithm <ref type="bibr" target="#b46">[27]</ref> and obtaining the global optimal solution. OMP and SPC is empirically observed to tend to be local, i.e. nonzero coefficients are often assigned to bases nearby to the encoded data <ref type="bibr" target="#b82">[63]</ref>. But this locality can not be ensured theoretically and they suggested a modification to SPC, called Local Coordinate Coding (LCC). This encoding method explicitly encourages the coding to be local, and they theoretically pointed out that under certain assumptions locality is more essential than sparsity, for successful nonlinear function learning using the obtained codes. Specifically, the LCC is defined as follows:</p><formula xml:id="formula_20">LCC: ψ(s) = ê ⊙ |s| 1 , s.t. 1 T s = 1,<label>(12)</label></formula><p>where ⊙ denotes the element-wise multiplication,ê is the locality adaptor that give weights for each basis vector proportional to its similarity to the input descriptor x:</p><formula xml:id="formula_21">e = [dist(x, d 1 ), · · · , dist(x, d K )] ⊤ ,<label>(13)</label></formula><p>where dist(x, d k ) is the Euclidean distance between x and d k . Due to the problem of ℓ 1 -norm optimization in both SPC and LCC, it is computationally expensive and hard to apply to large scale problem. Then, a practical coding scheme called Locality-constrained Linear Coding (LLC) <ref type="bibr" target="#b69">[50]</ref> is designed, which can be viewed as a fast implementation of LCC that utilizes the locality constraint to project each descriptor into its local-coordinate system:</p><formula xml:id="formula_22">LLC: ψ(s) = e ⊙ s 2 2 , s.t. 1 T s = 1,<label>(14)</label></formula><p>where e is the exponentiation ofê:</p><formula xml:id="formula_23">e = exp dist(x, D) σ ,<label>(15)</label></formula><p>where σ is used for adjusting the weighted decay speed for the locality adaptor. The constraint 1 T s = 1 follows the shift-invariant requirements of the final code vector.</p><p>In practice, an approximate solution can be used to improve the computational efficiency of LLC. It directly selects the k nearest basis vectors of x to minimize the first term in Equation <ref type="formula" target="#formula_17">(9)</ref> by solving a much smaller linear system. This gives the code coefficients for the selected k basis vectors and other code coefficients are simply set to be zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.3">Super vector based encoding methods</head><p>Super vector based encoding methods yield a very high dimensional representation by aggregating high order statistics. Typical methods include Local Tangentbased Coding (LTC) <ref type="bibr" target="#b81">[62]</ref>, Super Vector Coding (SVC) <ref type="bibr" target="#b84">[65]</ref>, Vector of Locally Aggregated Descriptors (VLAD) <ref type="bibr" target="#b35">[16]</ref>, and Fisher Vector (FV) <ref type="bibr" target="#b54">[35]</ref> . Local Tangent-based Coding <ref type="bibr" target="#b81">[62]</ref> assumes that codebook and descriptors are embedded in a smooth manifold. The main contents of LTC are manifold approximation and intrinsic dimensionality estimation. Under the Lipschitz smooth condition, the nonlinear function f (x) can be approximated by a local linear function as:</p><formula xml:id="formula_24">f (x) ≈ K i=1 s(i) f (d i ) + 0.5∇f (d i ) T (x − d i ) ,<label>(16)</label></formula><p>where s(i) is obtained by LCC <ref type="bibr" target="#b82">[63]</ref>. Then, this approximate function can be viewed as a linear function of a coding vector [s(i),</p><formula xml:id="formula_25">s(i)(x − d i )] K i=1 ∈ R K×(1+D)</formula><p>. LTC argues that there is lower intrinsic dimensionality in the feature manifold. To obtain it, Principal Component Analysis (PCA) is applied to the term of s(i)(x − d i ) using a projection matrix U i = [u i 1 , · · · , u i C ] ∈ R D×C trained from training data, i.e., the local tangent directions of the manifold. Therefore, the final coding vector for LTC is written as follows:</p><formula xml:id="formula_26">LTC: S = αs(i), s(i)(x − d i ) T U i K i=1 ,<label>(17)</label></formula><p>where α is a positive scaling factor to balance the two types of codes. Super Vector Coding (SVC) <ref type="bibr" target="#b84">[65]</ref> is a simple version of LTC. Unlike LTC, SVC yields the s(i) via VQ and does not apply PCA to the term of s(i)(x− d i ). Consequently, the coding vector of SVC is defined as follows:</p><formula xml:id="formula_27">SVC: S = [0, 0, · · · , αs(i) N √ p i , s(i) N √ p i (x − d i ), · · · , 0, 0],<label>(18)</label></formula><p>where s(i) = 1, d i is the closest visual word to x, and α is a positive constant. Fisher vector is another super vector based encoding method derived from fisher kernel <ref type="bibr" target="#b34">[15]</ref> and is introduced for large-scale image categorization <ref type="bibr" target="#b54">[35]</ref>. The fisher kernel is a generic framework which combines the benefits of generative and discriminative approaches. As it is known, the gradient of the log-likelihood with respect to a parameter can describe how that parameter contributes to the process of generating a particular example. Then the video can be described by the gradient vector of log likelihood with respect to the model parameters <ref type="bibr" target="#b34">[15]</ref>:</p><formula xml:id="formula_28">G x θ = ∇ θ log p(x; θ).<label>(19)</label></formula><p>Note that the dimensionality of this vector depends on the number of parameters in θ. Perronnin et al. <ref type="bibr" target="#b54">[35]</ref> developed an improved fisher vector which is as follows,</p><formula xml:id="formula_29">G x µ,k = 1 √ π k γ k x − µ k σ k ,<label>(20)</label></formula><formula xml:id="formula_30">G x σ,k = 1 √ 2π k γ k (x − µ k ) 2 σ 2 k − 1 ,<label>(21)</label></formula><p>where γ k is the weight of local descriptor x to k th Gaussian Mixture:</p><formula xml:id="formula_31">γ k = π k N (x; µ k , Σ k ) K i=1 π i N (x; µ i , Σ i ) .<label>(22)</label></formula><p>The final fisher vector is the concatenation this two gradients:</p><formula xml:id="formula_32">FV : S = [G x µ,1 , G x σ,1 , · · · , G x µ,K , G x σ,K ].<label>(23)</label></formula><p>Vector of Locally Aggregated Descriptors (VLAD) <ref type="bibr" target="#b35">[16]</ref> can be viewed as a hard version of FV and only keeps the 1 st order statistics:</p><formula xml:id="formula_33">VLAD: S = [0, · · · , s(i)(x − d i ), · · · , 0],<label>(24)</label></formula><p>where s(i) = 1, d i is the closest visual word to x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.4">Relations of Encoding Methods</head><p>In this section, we summarize several practical techniques widely used in these encoding methods, and give a unified generative perspective of these encoding methods. This analysis will uncover the underline relations between these methods and provide insights for developing new encoding methods. From "hard" to "soft". These encoding methods transform local features from descriptor space to codeword space. There are two typical transformation rules in these methods, namely hard assignment and soft assignment. Hard assignment quantizes the feature descriptor into a single codeword, while soft assignment enables the feature descriptor to vote for multiple codewords. In general, soft assignment accounts for the codeword uncertainty and plausibility <ref type="bibr">[12]</ref>, and reduces the information loss during encoding. This technical skill of soft assignment can be found in several encoding algorithms, such as SA-all vs. VQ, and VLAD vs. Fisher Vector. By the same techniques, we can extend the VLAD to VLAD-all, SVC to SVC-all:</p><formula xml:id="formula_34">VLAD−all : S = [ω 1 (x − d 1 ), · · · , ω K (x − d K )], (25) SVC−all : S = αω 1 N √ p 1 , αω 1 N √ p 1 (x − d 1 ), · · · , αω K N √ p K , αω K N √ p K (x − d K ) ,<label>(26)</label></formula><p>where ω i is the normalized weight of feature descriptor x with respect to codeword d i defined in Equation <ref type="formula" target="#formula_13">(6)</ref>. From "global" to "local". In several encoding methods, the manifold structure in descriptor space is captured to improve the stability of encoding algorithms. In the traditional soft assignment, each descriptor is assigned with all the codewords, which is called global assignment. However, in the high dimensional space of feature descriptor, Euclidian distance may be not reliable especially when the codeword is outside the neighborhood of feature descriptor. Therefore, in the encoding methods such as SA-k and LLC, each descriptor is enforced to only vote for these codewords belonging to its k-nearest neighbors, called local assignment. In general, the incorporation of local structure in encoding methods is able to improve the stability and reduce the sensitivity to noise in descriptor. Using the same techniques, we can also extend the VLADall to VLAD-k, SVC-all to SVC-k by replacing the ω i in Equation <ref type="formula" target="#formula_12">(25)</ref>, <ref type="bibr" target="#b45">(26)</ref> with localized ω ′ i defined in Equation <ref type="formula" target="#formula_14">(7)</ref>:</p><formula xml:id="formula_35">VLAD−k : S = [ω ′ 1 (x−d 1 ), · · · , ω ′ K (x−d K )],<label>(27)</label></formula><formula xml:id="formula_36">SVC−k : S = αω ′ 1 N √ p 1 , αω ′ 1 N √ p 1 (x − d 1 ), · · · , αω ′ K N √ p K , αω ′ K N √ p K (x − d K ) ,<label>(28)</label></formula><p>From "zero order statistics" to "high order statistics". In these super vector based encoding methods, they preserve not only the affiliations of descriptors to codewords (zero order statistics), but also the high order information such as the difference between descriptors mean and codeword, thus resulting a high-dimensional super vector representation. As these super vectors keep much richer information for each codeword, the codebook size is usually much smaller than that of voting and reconstruction based encoding methods. Above all, these super vector is with high dimension, storing more information, and is proved to outperform the other two kinds of encoding methods in Section 4. The high dimensional super vector will be a promising representation and designing effective dimension reduction algorithms for super vector will be an interesting problem.</p><p>Generative perspective of encoding methods. Although these encoding methods are developed in different scenarios, a unified generative probabilistic model can be used to uncover the underline relations among them. These encoding methods can be interpreted in a latent generative model:</p><formula xml:id="formula_37">p(h) ∈ P, p(x|h) = N (x; W h + µ x , Σ),<label>(29)</label></formula><p>where x ∈ R D represents the descriptor, h ∈ R K denotes the latent factor, N (x; W h + µ x , Σ) is multivariate Gaussian distribution. Different encoding methods mainly different in two aspects: How to model the prior distribution P of latent factor h and How to use the probabilistic model to transform the descriptor into codeword space.</p><p>For encoding methods such as VQ, SA-all, VLADall, and Fisher vector, they choose the prior distribution p(h) as follows:</p><formula xml:id="formula_38">p(h) = K i=1 π hi i ,<label>(30)</label></formula><p>where h ∈ {0, 1} K is discrete random variable, and the prior distribution is a Multinomial distribution. For SA-all, this Mutinomial distribution is specified by uniform distribution, i.e. π 1 = · · · = π K = 1 K , where for Fisher vector, this Multinomial distribution is learned during GMM training. Meanwhile the SA-all choose the latent variable embedding to encodes the descriptor by computing conditional expectation, i.e. s(x) = E(h|x), while the Fisher vector choose the gradient embedding <ref type="bibr" target="#b34">[15]</ref>, i.e. S(x) = ∇ θ log p(x; θ). The VQ encoding can be viewed an extreme case of Soft-all, when:</p><formula xml:id="formula_39">p(x|s) = N (x; W s + µ x , ǫI), ǫ → 0.<label>(31)</label></formula><p>VLAD-all and SVC-all can be viewed as the gradient embedding in this extreme case.</p><p>For encoding methods such as sparse coding, the latent variable h is continuous and its corresponding prior distribution is specified as:</p><formula xml:id="formula_40">p(h) = K i=1 λ 2 exp(−λ|h i |).<label>(32)</label></formula><p>This prior distribution is called Laplace prior and sparse coding can be viewed as the latent variable embedding of this generative model using the maximum a posteriori value (MAP), i.e. s(x) = arg max h p(h|x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Pooling and Normalization Methods</head><p>Given the code coefficients of all local descriptors in a video, a pooling operation is often used to obtain a global representation p for the video. Specifically, there are two common pooling strategies:</p><p>-Sum Pooling. With sum pooling scheme <ref type="bibr" target="#b45">[26]</ref>, the</p><formula xml:id="formula_41">k th component of p is p k = N n=1 s n (k). -Max Pooling. With max pooling scheme [61], the k th component of p is p k = max(s 1 (k), · · · , s N (k)),</formula><p>where N is the number of extracted local descriptors, s n denotes the code of descriptor x n .</p><p>In <ref type="bibr" target="#b24">[4]</ref>, the authors presented a theoretical analysis of average pooling and max pooling. Their results indicate sparse features may prefer max pooling.</p><p>To make this representation invariant to the number of extracted local descriptors, the pooling result p is further normalized by some methods. Generally, there are three common normalization techniques:</p><p>ℓ 1 -Normalization. In ℓ 1 normalization <ref type="bibr" target="#b80">[61]</ref>, the feature p is divided by its ℓ 1 -norm: p = p/ p 1 . -ℓ 2 -Normalization. In ℓ 2 normalization <ref type="bibr" target="#b54">[35]</ref>, the feature p is divided by its ℓ 2 -norm: p = p/ p 2 . -Power Normalization. In power normalization <ref type="bibr" target="#b54">[35]</ref>, we apply in each dimension the following function:</p><formula xml:id="formula_42">f (p k ) = sign(p k )|p k | α .</formula><p>where 0 ≤ α ≤ 1 is a parameter for normalization. We can combine power normalization with ℓ 1normalization or ℓ 2 -normalization.</p><p>Recently, a special normalization strategy is proposed for the VLAD, called intra-normalization <ref type="bibr" target="#b22">[2]</ref>. In this paper, we extend it to all the super vector based encoding algorithms. This method carries out normalization operation in a block by block manner, where each block denotes the vector related to one codeword. Generally, the intra-normalization can be formulated as follows:</p><formula xml:id="formula_43">p = p 1 p 1 , · · · , p k p k , · · · , p K p K ,<label>(33)</label></formula><p>where p k denotes a vector related to codeword d k (or the k th Gaussian), · may be ℓ 1 -norm or ℓ 2 -norm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Feature Fusion</head><p>Fusing multiple local features has turned out to be an effective method to boost the performance of recognition system in computer vision community <ref type="bibr" target="#b31">[11,</ref><ref type="bibr" target="#b63">44,</ref><ref type="bibr" target="#b60">41,</ref><ref type="bibr" target="#b66">47,</ref><ref type="bibr" target="#b26">6]</ref>. The video data is usually characterized in multiple views, such as static appearance, motion pattern, and motion boundary. The essence of multiview data requires fusing different features for action recognition. In this section, we present several feature fusion methods for action recognition, and analyze its corresponding properties. Meanwhile, based on the analysis of fusion methods, we propose a simple yet effective representation, called hybrid representation. As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, the fusion methods are usually conducted in different levels, typically including: descriptor level, representation level, and score level. For descriptor level fusion, it is performed in the cuboid level, where multiple descriptors from the same cuboid are concatenated into a single one, and then it is fed into the BoVW to obtain the global representation. For representation-level fusion, it is performed in the video level, where different descriptors are input into BoVW separately and the resulting global representations are fused as a single one, which is further fed into classifier for recognition. For scorelevel fusion, it is also performed in the video level, but the representations of different descriptors are used independently for classifier training. The final recognition score is obtained by fusing the scores from multiple classifiers. For fusing the scores, arithmetical mean or geometrical mean is often used.</p><p>In general, these fusion methods at different levels owns their pros and cons, and the choice of fusion method should be guided by the dependence of descriptors. If these multiple descriptors from the same cuboid are highly correlated, it will be better to resort to descriptor level feature fusion. Otherwise, the choice of descriptor level fusion is not a good one, as descriptor level fusion usually results in a higher dimension and adds the difficulty for unsupervised feature learning such as k-means and sparse coding. For the case where different views of features are less correlated in cuboid level but highly correlated in video level, representation level fusion is usually a good choice. When these different features are independently with each other, it will be appropriate to choose score level fusion, as this fusion reduce the dimension for classifier training and make the learning faster and more stable. The performance boosting of fusing multiple features mainly owns the complementarity of these features. However, the complementarity can be explored not only for different features, but also for different types of BoVW methods. As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, we propose a simple yet effective representation, called hybrid representation, which combines the outputs from multiple variants of BoVW and multiple descriptors. The resulting hybrid representation effectively explores the complementarity of different encoding methods and greatly enhances the descriptive power for action recognition. As we shall see in Section 4.7, this representation will improve the recognition rate of a single BoVW model and obtain the state-of-the-art results on the three challenging datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Empirical Study</head><p>In this section, we describe the detailed experimental settings and the empirical study of variants of BoVW and different fusion methods. We first introduce the datasets used for evaluation and their corresponding experimental setup. We then extensively study different aspects of BoVW, including pre-processing techniques, encoding methods, pooling strategies, and normalization approaches. After that, we explore the different choices of fusion methods for multiple features. Finally, we compare the performance of our hybrid representation with that of the state-of-the-art methods on three challenging datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Evaluation Protocols</head><p>We conduct experiments on three public datasets: HMDB51 <ref type="bibr" target="#b42">[23]</ref>, UCF50 <ref type="bibr" target="#b55">[36]</ref>, and UCF101 <ref type="bibr" target="#b59">[40]</ref>. Some examples of video frames are illustrated in <ref type="figure" target="#fig_3">Figure 4</ref>. Totally, we work with 26,704 videos in this paper.</p><p>The HMDB51 dataset has 51 action classes with total 6,766 videos and each class has more than 100 videos 2 . All the videos are obtained from real world scenarios such as: movies, youtube. The intra-class variation is very high due to many factors, such as viewpoint, scale, background, illumination etc. Thus, HMDB51 is a very difficult benchmark for action recognition. There are three training and testing splits released on the website of this dataset. We conduct experiments based on these splits and report average accuracy for evaluation.</p><p>The UCF50 dataset has 50 action classes with total 6,618 videos, and each action class is divided into 25 groups with at least 100 videos for each class. The video clips in the same group are usually with similar background. We choose the suggested evaluation protocols of Leave One Group Out cross validation (LOGO) and report the average accuracy <ref type="bibr" target="#b55">[36]</ref>.</p><p>The UCF101 dataset is an extension of the UCF50 dataset and has 101 action classes. The action classes can be divided into five types: humanobject interaction, body-motion only, human-human interaction, playing musical instruments, and sports. Totally, it has 13,320 video clips, with fixed frame rate and resolution 25 FPS and 320 × 240 respectively. To our best knowledge, this dataset has been the largest dataset so far. We perform evaluation according to the three train/test splits released in Thumos'13 challenge 3 and report the mean average accuracy of these splits.</p><p>In our evaluation experiment, we choose linear Support Vector Machine (SVM) as our recognition classifier. Specifically, we use the implementation of LIBSVM <ref type="bibr" target="#b28">[8]</ref>. For multiclass classification, we adopt onevs-all training scheme and choose the prediction with highest score as our predicted label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Local Features and Codebook Generation</head><p>In our evaluation, we choose two widely-used local features, namely Space Time Interest Points (STIPs) <ref type="bibr" target="#b43">[24]</ref> with HOG, HOF descriptors <ref type="bibr" target="#b44">[25]</ref>, and improved Dense Trajectories (iDTs) with HOG, HOF, MBHx, MBHy descriptors <ref type="bibr" target="#b65">[46]</ref>. Specifically, we use the implementation released on the website of Laptev 4 for STIPs and Wang 5 for iDTs. We choose the default parameter settings for both local features. STIPs and iDTs represent two types of local features: sparse interest points and densely-sampled trajectories. They may exhibit different properties with varying BoVW settings, and thus it is well worth exploring both STIPs and iDTs.</p><p>Regarding codebook generation, we randomly sample 100, 000 features to conduct k-means, where codebook size range from 1,000 to 10,000 for STIPs, and from 1,000 to 20,000 for iDTs. For GMM training, we randomly sample 256,000 features to learn GMMs with mixture number ranging from 16 to 512 for both STIPs and iDTs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Importance of Pre-processing</head><p>In this section, we explore the importance of preprocessing in BoVW framework. Specifically, we use STIPs as local features and choose a representative method for each type of encoding, namely FV, LLC, and VQ . For pooling and normalization strategy, we use sum pooling and power ℓ 2 -normalization. We use the descriptor-level fusion method to combine HOG and HOF descriptors.</p><p>We conduct experiments on the UCF101 dataset and investigate the importance of pre-processing for these encoding methods. With pre-processing step, the descriptors of STIPs are firstly reduced to 100dimension and then whitened to have unit variance. The results are shown in <ref type="figure" target="#fig_4">Figure 5</ref>. We observe that the pre-processing technique of PCA-Whiten is very important to boost the performance of encoding methods. Surprisingly, the performance of FV (state-of-the-art) without PCA-Whiten is lower than or comparable to VQ and LLC with PCA-Whiten. In previous research work, PCA-Whiten is often done for FV encoding methods but seldom used for other encoding methods.</p><p>Our study suggests that using PCA-Whiten techniques enable us to greatly improve final recognition rate for all encoding methods. We obtain the recognition rate 56.1% for VQ, which significantly outperform over the result 43.9% reported in <ref type="bibr" target="#b59">[40]</ref>, where the same local feature and encoding method is used.</p><p>In the remaining part of evaluation, we will use PCA-Whiten to de-correlate the descriptor, reduce the dimension, and normalize the variance. For descriptor level fusion of STIP, the dimension of concatenated descriptor is reduced from 162 to 100. For HOG and HOF, the dimension is reduced from 72 to 40, and from 90 to 60, respectively. For descriptor level fusion of iDT, the dimension of concatenated descriptor is reduced from 396 to 200. For separate descriptor, the dimensions of HOG, MBHx, and MBHy are all reduced from 96 to 48. HOF descriptor is reduced from 108 to 54.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Exploration of Encoding Methods</head><p>In this section, we compare and analyze the performance of different encoding methods. For each encoding method, we fix other settings, such as parameter setting, pooling and normalization strategy, the same with previous papers. We explore these encoding methods with descriptor level fusion, for both STIPs and iDTs. The influence of different pooling and normalization strategy, and fusion methods will be investigated in the following sections.</p><p>Encoding methods selection and setting. We select six popular encoding methods according to categorization in <ref type="table" target="#tab_0">Table 1</ref>. For voting based encoding methods, we choose VQ as a baseline and SA-k as a representative method. LLC is selected as the representative of reconstruction-based encoding methods due to its computational efficiency and performance <ref type="bibr" target="#b73">[54]</ref>. Super vector based encoding methods have shown the state-of-the-art performance on several datasets <ref type="bibr" target="#b66">[47]</ref> . We choose three super vector based encoding methods for evaluation, namely FV, VLAD, and SVC.</p><p>Baseline: Vector Quantization Encoding (VQ). In the baseline method, each descriptor is quantized into a single codeword. Following the suggested settings in object recognition <ref type="bibr" target="#b83">[64]</ref>, the final histogram is obtained by sum pooling and normalized with ℓ 1 norm.</p><p>Localized Soft Assignment Encoding (SA-k). In the localized soft assignment, each descriptor is assigned to its corresponding k nearest neighborhood. It requires a single parameter β, which is the smoothing factor controlling the softness. According to <ref type="bibr" target="#b47">[28]</ref>, we set β as 1 and k as 5 in our evaluation. We use max pooling and ℓ 2 -normalization.</p><p>Locality-constrained Linear Encoding (LLC). Following <ref type="bibr" target="#b69">[50]</ref>, we use approximated LLC for fast encoding, where we simply use k nearest neighborhood of descriptor as the local bases. The parameter of k is set as 5, and we choose max pooling and ℓ 2 -normalization strategy.</p><p>Fisher Vector (FV). For GMM training, we use the k-means result to initialize iteration and the covariance matrix of each mixture is set as a diagonal one. Following <ref type="bibr" target="#b54">[35]</ref>, we use sum pooling and power ℓ 2normalization.</p><p>Vector of Locally Aggregated Vector (VLAD). VLAD was originally designed for image retrieval in <ref type="bibr" target="#b35">[16]</ref> and can be viewed as a simplified version FV for fast implementation. Just like FV, we choose sum pooling and power ℓ 2 -normalization.</p><p>Super Vector Coding (SVC). From the view of statistics, SVC can be viewed as a combination of VQ and VLAD. It contains the zeros and first-order statistics, and the parameter α keep balance between these two components. Following <ref type="bibr" target="#b84">[65]</ref>, we set α as 0.1. Like other super vector based encoding methods, we choose sum pooling and power ℓ 2 -normalization.</p><p>Results and analysis. The experimental results of STIPs and iDTs on the three datasets are shown in <ref type="figure" target="#fig_6">Figure 6</ref>. Several rules can be found from these experimental results:</p><p>-Basically, the recognition performance of all selected encoding methods increases as the size of codebook (GMM) becomes larger and will reach a plateau when the size exceeds a threshold. For super vector based encoding methods, the performances reach a saturation when size of codebook (GMM) becomes 256 for both STIPs and iDTs. There is a slight change of the recognition rate when GMM size grows from 256 to 512. For the other two types of encoding methods, the performances are saturated as the size of codebook reaches 8, 000. We also notice that these encoding methods using iDTs have slight improvements when the codebook size varies from 8,000 to 20,000 , while the performances using STIPs start shaking when the codebook size becomes larger than 8,000 due to the over-fitting effect. This difference may be ascribed to the dimension of local descriptors and sampling strategy. The descriptors dimension of iDTs is twice of STIPs and requires more codewords to divide the feature space. Meanwhile, STIPs is a set of interest points and the extracted descriptors distribute sparsely in   the feature space. The codebook with large size will result in an over-partition of feature space, which means for a specific video, there may be no descriptors falling into the corresponding regions for some codewords. iDTs are more densely sampled features and codebook with large size is more suitable to divide the space of dense features. Above all, for a good balance between performance and efficiency, sizes of 256 and 8, 000 are good choices for super vector based encoding and other encoding respectively.</p><p>-For local features of both SITPs and iDTs, super vector based encoding methods outperform the other types of encoding methods on the three datasets. According to previous introduction, these super vector encoding methods not only preserve the affiliations of descriptors to codewords, but also keep high order information such as the difference of means and variances. These high order information enables the encoding methods to better capture the distribution shape of descriptor in feature space. In these super vector based methods, FV is typically better than VLAD and SVC, whose performance is quite similar. This can be own to two facts: (i) FV keeps both 1 st and 2 nd statistics, which is more informative than VLAD (only 1 st statistics) and SVC (0 th statistics and 1 st statistics). (ii) FV is based on GMM and each descriptor is softly assigned to codewords using posterior probability, while VLAD and SVC are based on k-means results and use hard assignment. We also notice that the difference between FV and the other two methods (VLAD, SVC) for iDTs seems smaller than STIPs. The more dense descriptors may make the learned codebook more stable for SVC and VLAD, and reduce the influence of soft assignment in FV. Meanwhile, the information contained in 2 nd statistics may be less complementary to 1 st statistics for iDTs. In conclusion, super vector based representation, aggregating high order information, is a more suitable choice for good performance, when the high dimension of representation is acceptable. -For reconstruction based and voting based encoding methods, VQ reaches the lowest recognition rate for STIPs and iDTs on the three datasets. This can be ascribed to the hard assignment and descriptor ambiguity in the VQ method. In essence, the LLC and SA-k are quite similar in spirt, for that they both consider locality when mapping descriptor into codeword. The performance of LLC is better than SA-k for STIPs, while the performances of them are almost the same for iDTs. This can be explained by the mapping strategy in LLC and SA-k. The mappings of descriptor to the nearest codewords in LLC are determined jointly according to their effect in minimizing the reconstruction error, while the mappings in SA-k are calculated independently for each individual codeword according to the Euclidean distance. The mapping method in LLC may be more effective to deal with manifold structure than just considering Euclidean distance in SA-k.</p><p>For sparse features such as STIPs, the descriptors distribute sparsely around each codeword, and using Euclidean distance may introduce noise and instability for SA-k. For dense features such as iDTs, the descriptors are usually sampled densely and more compact around codewords, reducing the influence caused by the usage of Euclidean distance. In a word, compared with hard assignment, locality and soft assignment is an effective strategy to improve the performance of encoding methods. -STIPs and iDTs represents two types of local features, namely sparsely-sampled and densely-sampled features. In general, they exhibit consistent performance trends for different encoding methods, for example, super vector encoding methods outperforms others, soft-assignment is better than hardassignment. However, there is a slight difference between them in some aspects, such as sensitivity to codebook size and encoding methods, performance gaps among super vector based methods, difference between LLC and SA-k, as previously observed. From the perspective of data manifold, the more densely-sampled features can help us more accurately describe the data structure in the feature space. We can obtain a more compact clustering result using k-means, and the local Euclidean distance is more stable. Thus, when choosing codebook size and encoding method, the type of local feature can be a factor needed to be considered.</p><p>Computational costs. We also compare the efficiency of different encoding methods and the running time is shown in <ref type="figure" target="#fig_7">Figure 7</ref>. Our codes are all implemented in Matlab, and running on a workstation with 2x Intel Xeon 5560 2.8GHz CPU and 32G RAM. We randomly sample 50 videos from the UCF101 dataset and report the total time for these videos. For super vector based methods, FV is much slower due to the calculation of posterior probability during encoding, and the time of VLAD and SVC is almost the same. For the other types of encoding methods, LLC is less efficient as it solves a least square problem. The computational cost of super vector encoding methods are usually lower than that of the other types of encoding methods, due to their smaller codebook sizes.</p><p>Based on the above analysis, super vector based encoding methods are more promising for high performance and fast implementation, especially for SVC, VLAD. However, the feature dimension of super vector methods is much higher than the other two kinds of encoding methods, for example, when the codebook size is 256, the dimension of FV and VLAD is 102,400 and 51,200 respectively for iDT features. The effective dimension reduction may be a future research direction for super vector encoding methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Exploration of Pooling and Normalization</head><p>In this section, we mainly investigate the influence on recognition rate for different pooling and normalization strategies on the UCF101 dataset. Based on the performances of different encoding methods on the UCF101 dataset in previous section, we choose the codebook (GMM) size as 512 for super vector based methods and codebook size as 8, 000 for the other two types of encoding approaches. Meanwhile, according to our conclusion that super vector based encoding is a promising method and soft assignment is an effective way to improve the encoding methods, we extend VLAD to VLAD-k and VLAD-all, SVC to SVC-k and SVC-all as described in Section 2.4.4. Thus, there are totally 10 kinds of encoding methods.</p><p>For super vector based encoding methods, we evaluate eight normalization methods, specified by with or without intra-normalization, with or without power operation, final ℓ 1 or ℓ 2 normalization. For the other two types of encoding methods, we choose two pooling methods, namely max pooling and sum pooling, and four normalization methods, namely ℓ 1 -normalization, ℓ 2 -normalization, power ℓ 1 -normalization, and power ℓ 2 -normalization. In our evaluation, the parameter α in power normalization is set as 0.5.</p><p>The experimental results are shown in <ref type="figure">Figure 8</ref> and <ref type="figure">Figure 9</ref>. Several observation can be concluded from these results:</p><p>-For super vector based encoding methods, intranormalization is an effective way to balance the weight of different codewords and suppress the burst of features corresponding to background. We found this technique works very well when dense features are chosen. A large number of features in iDTs are irrelevant with the action class and intranormalization can suppress this influence. However, for sparse features, the effect of intra normalization is not so evident, and even cause performance degradation in the case of hard assignment such as VLAD, SVC. We ascribe this phenomenon to the fact that the STIPs features are usually located in the moving foreground and related with action class. Thus, these descriptors only vote for a subset of codewords, that are highly related with action class. In this case, intra-normalization can decrease the discriminative power of action-related codewords and increase the influence of irrelevant codewords.</p><p>In conclusion, intra-normalization is effective in handling burst of irrelevant features in the case of dense-sampling strategy. -For different encoding methods and local features, we observe that the final ℓ 2 -normalization outperforms ℓ 1 -normalization. In fact, the normalization method is related to kernel used in final classifier. In our case of linear SVM, the kernel is k(x, y) = x ⊤ y. The choice of ℓ 2 -normalization can ensure two things: (i) k(x, x) = const.; (ii) k(x, x) ≥ k(x, y). This can guarantee a simple consistency criterion: by interpreting k(x, y) as a similarity score, x should be the most similar point to itself <ref type="bibr" target="#b64">[45]</ref>. However, the choice of ℓ 1 -normalization can not make sure that the point is most similar to self and may cause the instability during SVM training. Above all, ℓ 2 -normalization generally outperforms ℓ 1 -normalization when using linear SVM. -The influence of power operation in normalization is highly related with pooling method. We observe that power normalization is an effective approach to boost the performance of representation obtained from sum pooling, such as super vector based representation, LLC, SA-k with sum pooling. However, power normalization have little effect for max pooling and sometimes even cause the performance degradation for LLC, SA-k. The operation of power usually reduces the difference between different  <ref type="figure">Fig. 8</ref> Comparison of different pooling-normalization strategies with STIPs features using descriptor level fusion on the UCF101 dataset. Note that there is only max pooling for voting and reconstruction based encoding methods, and there is only intra normalization for super vector based encoding methods.  <ref type="figure">Fig. 9</ref> Comparison of different pooling-normalization strategies with iDTs features using descriptor level fusion on the UCF101 dataset. Note that there is only max pooling for voting and reconstruction based encoding methods, and there is only intra normalization for super vector based encoding methods.</p><p>codewords, which means smoothing the histogram. This smooth effect can reduce the influence of high frequent codeword on the kernel calculation and improve the influence of less frequent codeword. For sum pooling, the resulting histogram is usually very sharp and unbalanced due to feature burst, and the smooth operation has a positive effect for suppress the high frequent codeword. However, for max pooling, the histogram is itself not so sharp as sum pooling, and thus the power normalization may have a side effect. In above, power operation is an effective strategy to smooth the resulting histogram and can greatly improve the performance of sum pooling representation.</p><p>-Among different choices of pooling operations and normalization methods, we conclude that sum pooling and power ℓ 2 -normalization is the best combination. For dense features, intra normalization is an extra bonus for performance boosting. For sparse features, intra normalization sometimes may have a negative effect. The success of sum pooling and power ℓ 2 -normalization can be explained by the Hellinger's kernel <ref type="bibr" target="#b64">[45]</ref>, which has turned out to be an effective kernel to calculate the similarity between two histograms. The linear kernel calculation in the feature space resulting from power ℓ 2 -normalization is equivalent to the Hellinger's kernel calculation in the original space just using ℓ 1 normalization:</p><formula xml:id="formula_44">&lt; √ x √ x 2 , √ y √ y 2 &gt;=&lt; x x 1 , y y 1 &gt;<label>(34)</label></formula><p>which means power ℓ 2 -normalization explicitly introduces non-linear kernel in the final classifier. In a word, sum pooling and power ℓ 2 -normalization is effective and efficient way to enable linear SVM to have the power of non-linear classifier and boost final recognition rate.</p><p>In conclusion, pooling and normalization is a crucial step in the pipeline of BoVW framework, whose importance may not be highlighted in previous research work. Proper choice of pooling and normalization strategy may largely reduce the performance gap of different encoding methods. For sum pooling and power ℓ 2normalization, which is the best combination in all these possible choices, the performances of LLC, SA-k, and VQ are comparable to each other for iDT features. Thus, in the remaining evaluation for fusion methods, we fix the pooling and normalization strategy as sum pooling and power ℓ 2 -normalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Exploration of Fusion Methods</head><p>The local features usually have multiple descriptors, such as HOG, HOF, MBHx, and MBHy, each of which corresponds to a specific view of video data. For the empirical study in previous section, we choose a simple method to combine these multiple descriptors, where we just concatenate them into a single one, namely descriptor level fusion. In this section, we mainly analyze the influence of different fusion methods on final recognition performance.</p><p>For encoding methods, we choose the same ten approaches as in previous section. The codebook size of super vector based methods is set as 512 and the one of other encoding methods is set as 8, 000. For pooling and normalization methods, we use sum pooling and power ℓ 2 -normalization, according to the observations in Section 4.5. We also use intra normalization for super vector based encoding methods of iDTs features. For fusion methods, we evaluate three kinds of methods, namely descriptor level fusion, representation level fusion, and score level fusion, as described in Section 3. For score level fusion, we use the geometrical mean to combine the scores from multiple SVMs.</p><p>The experimental results on three datasets are shown in <ref type="table" target="#tab_3">Table 2</ref>, <ref type="table" target="#tab_4">Table 3, and Table 4</ref>. From these results, we observe serval trends:</p><p>-For iDTs features, representation level fusion is the best choice for all of the selected encoding methods on the three datasets. This result indicates that these multiple descriptors are most correlated in the video level. Descriptor level fusion emphasizes the dependance in cuboid and results in high dimension features for codebook training and encoding. This may make these unsupervised learning algorithm unstable. -For STIPs features, representation level fusion is more effective for reconstruction based and voting based encoding methods. For super vector based encoding methods, the performance of representative level fusion is comparable to that of descriptor level fusion. This trend is consistent with the finds with iDTs features.</p><p>-For both features, SA-k, LLC, and VQ encoding methods are much sensitive to fusion methods than those super vector based encoding methods. Great improvement can be obtained for SA-k, LLC, and VQ when using representation level fusion, but slight improvements happen to those super vector methods. We analyze this is due to two facts. Firstly, for reconstruction and voting based encoding methods, the final dimension of representation level fusion is M (the number of descriptors) times of the dimension of descriptor level fusion. However, for super vector based encoding methods, the dimension of descriptor level fusion is the same with representation level fusion. The higher dimension of final representation may enable SVM to classify more easily. Secondly, the codebook size K of super vector methods is much smaller than that of other types of encoding methods, where clustering algorithm may be more stable for high dimensionality in descriptor level fusion method.</p><p>Based on the observation and analysis above, we conclude that fusion method is a very important component for handling combination of multiple descriptors in the action recognition system. Representation level fusion method is a suitable choice for different kinds of encoding methods due to its good performance. From our analysis, we know that the performance boosting of fusing multiple features mainly owns the complementarity of these features. This complementarity may be not limited to the exploration of different descriptors, but also can be extended to the different BoVWs. From the perspective of statistics, FV aggregates information using 1 st and 2 nd order statistics, while SVC is about zero and 1 st order statistics. Intuitively, these two kinds of super vector encoding methods are complementary to each other. Thus, we present a new feature representation, called hybrid representation, combining the outputs FV and soft version SVC of multiple descriptors, including HOG, HOF, MBHx, and MBHy. This representation is simple but proved to be effective in next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Comparison to the State-of-the-Art Results</head><p>In this section, we demonstrate the effectiveness of our proposed hybrid representation according to our previous insightful analysis. Specifically, we choose two super vector based encoding methods, namely SVC-k and FV, for iDTs features. We use the power operation and then intra ℓ 2 -normalization. For feature fusion, we adopt the representation level fusion method.    <ref type="table">Table 5</ref> Comparison our hybrid representation with the sate-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HMDB51</head><p>Year % UCF50</p><p>Year % UCF101</p><p>Year % <ref type="table">Table 5</ref> shows our final recognition rates and compare our results to that of state-of-the-art approaches. For the HMDB51 dataset, we obtain a recognition rate of 61.1%, which is superior to the best result [47] by 3.9%. Our system reaches classification accuracy of 92.3% on the dataset of UCF50 and 87.9% on the dataset of UCF101, which outperform the best results by 1.2% and 2.0% respectively. It is worth noting that UCF101 is newest and largest dataset, so few published papers have reported results on this dataset. We mainly compare with those top performers in the Thumos'13 Action Recognition Challenge <ref type="bibr" target="#b37">[18]</ref>. We also compare with three latest papers in CVPR 2014. Karpathy et al. <ref type="bibr" target="#b40">[21]</ref> resorts to a large deep Convolutional Neural Network trained with an extra 1-M training dataset. Cai et al. <ref type="bibr" target="#b26">[6]</ref> propose a complex and less efficient encoding method by considering the correlation of different descriptors. Wu et al. <ref type="bibr" target="#b77">[58]</ref> propose a simple, lightweight, but powerful bimodal encoding method. Our results outperform these top performer and latest papers on the UCF101. From these comparisons, our hybrid representation is an efficient and effective method and obtains the state-of-the-art performance on the three challenging datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we have comprehensively studied each step in the BoVW pipeline and tried to uncover good practice to build a more accurate and efficient action recognition system. Specifically, we mainly explore five aspects, namely local features, pre-processing techniques, encoding methods, pooling and normalization strategy, fusion methods. We conclude that every step is crucial for contributing to the final recognition rate and improper choice in one of the steps may counteract the performance improvement of other steps. Meanwhile, based on the insights from our comprehensive study, we propose a simple yet effective representation, called hybrid representation. Using this representation, our action recognition system obtains the state-of-the-art performance on the three challenging datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1</head><label>1</label><figDesc>The pipeline of obtaining Bag of Visual Words (BoVWs) representation for action recognition. It is mainly composed of five steps: (i) feature extraction, (ii) feature pre-processing, (iii) codebook generation, (iv) feature encoding, and (v) pooling and normalization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2</head><label>2</label><figDesc>Comparison among all the voting based encoding methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3</head><label>3</label><figDesc>Feature fusion is performed in different levels: descriptor level, representation level, and score level. The complementary effect of varied BoVW models can also be taken into account and a hybrid representation is obtained by fusing outputs from different BoVW models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4</head><label>4</label><figDesc>Sample frames from the HMDB51, UCF50 and UCF101 datasets. Note that UCF50 is a subset of UCF101.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5</head><label>5</label><figDesc>Comparison the results with PCA+Whiten and without PCA-Whiten of different encoding methods on the UCF101 dataset, where STIPs are chosen as the local features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6</head><label>6</label><figDesc>Performance of different encoding methods with varying codebook (GMM) sizes on the HMDB51, UCF50, and UCF101 datasets for STIPs and iDTs features using descriptor-level fusion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7</head><label>7</label><figDesc>Average time of different encoding methods with varying codebook (GMM) sizes on the UCF101 datasets for STIPs and iDTs features using descriptor-level fusion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>List of encoding methods and their formulations. The detailed descriptions of these encoding methods can be found in the text.</figDesc><table><row><cell>Type Method</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc>Comparison of different fusion methods for the encoding methods on the HMDB51 dataset.</figDesc><table><row><cell>Methods</cell><cell>FV</cell><cell>SVC</cell><cell>SVC-k</cell><cell>SVC-all</cell><cell>VLAD</cell><cell cols="2">VLAD-k VLAD-all</cell><cell>LLC</cell><cell>SA-k</cell><cell>VQ</cell></row><row><cell cols="4">Space Time Interest Points (STIPs)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>HOG</cell><cell>22.81</cell><cell>17.76</cell><cell>21.09</cell><cell>21.87</cell><cell>18.13</cell><cell>19.87</cell><cell>20.04</cell><cell>20.46</cell><cell>18.39</cell><cell>16.10</cell></row><row><cell>HOF</cell><cell>31.96</cell><cell>30.44</cell><cell>32.68</cell><cell>33.36</cell><cell>30.46</cell><cell>31.53</cell><cell>31.55</cell><cell>27.19</cell><cell>26.27</cell><cell>24.49</cell></row><row><cell>d-Fusion</cell><cell>38.82</cell><cell>35.12</cell><cell>36.64</cell><cell>37.19</cell><cell>34.81</cell><cell>36.18</cell><cell>36.23</cell><cell>29.87</cell><cell>28.13</cell><cell>25.66</cell></row><row><cell>r-Fusion</cell><cell>37.32</cell><cell>34.36</cell><cell>36.73</cell><cell>37.19</cell><cell>34.23</cell><cell>35.84</cell><cell>35.88</cell><cell>33.44</cell><cell>32.59</cell><cell>30.35</cell></row><row><cell>s-Fusion</cell><cell>36.71</cell><cell>32.14</cell><cell>34.51</cell><cell>34.99</cell><cell>32.11</cell><cell>33.90</cell><cell>34.01</cell><cell>32.52</cell><cell>30.96</cell><cell>27.54</cell></row><row><cell cols="4">Improved Dense Trajectories (iDTs)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>HOG</cell><cell>45.12</cell><cell>36.93</cell><cell>39.32</cell><cell>38.10</cell><cell>36.93</cell><cell>39.30</cell><cell>37.08</cell><cell>37.08</cell><cell>35.45</cell><cell>34.81</cell></row><row><cell>HOF</cell><cell>50.70</cell><cell>47.70</cell><cell>49.00</cell><cell>48.00</cell><cell>47.70</cell><cell>49.00</cell><cell>45.80</cell><cell>42.20</cell><cell>42.70</cell><cell>42.10</cell></row><row><cell>MBHx</cell><cell>44.14</cell><cell>39.35</cell><cell>43.01</cell><cell>41.68</cell><cell>39.43</cell><cell>43.03</cell><cell>41.55</cell><cell>35.51</cell><cell>35.51</cell><cell>34.6</cell></row><row><cell>MBHy</cell><cell>50.04</cell><cell>44.25</cell><cell>47.02</cell><cell>46.51</cell><cell>44.27</cell><cell>47.02</cell><cell>44.68</cell><cell>40.39</cell><cell>40.35</cell><cell>39.78</cell></row><row><cell>d-Fusion</cell><cell>58.37</cell><cell>54.12</cell><cell>56.82</cell><cell>56.86</cell><cell>54.2</cell><cell>56.88</cell><cell>54.73</cell><cell>48.25</cell><cell>48.58</cell><cell>47.93</cell></row><row><cell>r-Fusion</cell><cell>60.22</cell><cell>58.19</cell><cell>60.09</cell><cell>60.07</cell><cell>58.26</cell><cell>60.09</cell><cell>58.58</cell><cell>55.45</cell><cell>55.8</cell><cell>55.27</cell></row><row><cell>s-Fusion</cell><cell>59.62</cell><cell>57.27</cell><cell>59.11</cell><cell>58.78</cell><cell>57.14</cell><cell>59.17</cell><cell>57.54</cell><cell>53.68</cell><cell>53.94</cell><cell>53.27</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc>Comparison of different fusion methods for the encoding methods on the UCF50 dataset.</figDesc><table><row><cell>Methods</cell><cell>FV</cell><cell>SVC</cell><cell>SVC-k</cell><cell>SVC-all</cell><cell>VLAD</cell><cell cols="2">VLAD-k VLAD-all</cell><cell>LLC</cell><cell>SA-k</cell><cell>VQ</cell></row><row><cell cols="4">Space Time Interest Points (STIPs)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>HOG</cell><cell>66.20</cell><cell>60.76</cell><cell>63.98</cell><cell>63.94</cell><cell>60.22</cell><cell>62.32</cell><cell>62.22</cell><cell>60.42</cell><cell>59.11</cell><cell>56.21</cell></row><row><cell>HOF</cell><cell>73.10</cell><cell>71.93</cell><cell>74.14</cell><cell>74.56</cell><cell>71.30</cell><cell>72.36</cell><cell>72.51</cell><cell>64.72</cell><cell>63.80</cell><cell>61.55</cell></row><row><cell>d-Fusion</cell><cell>78.32</cell><cell>76.33</cell><cell>77.60</cell><cell>77.59</cell><cell>75.57</cell><cell>76.06</cell><cell>76.13</cell><cell>70.13</cell><cell>68.66</cell><cell>67.16</cell></row><row><cell>r-Fusion</cell><cell>77.21</cell><cell>76.07</cell><cell>78.42</cell><cell>78.91</cell><cell>75.36</cell><cell>75.95</cell><cell>75.99</cell><cell>74.05</cell><cell>73.67</cell><cell>71.95</cell></row><row><cell>s-Fusion</cell><cell>76.33</cell><cell>76.19</cell><cell>77.76</cell><cell>77.25</cell><cell>73.79</cell><cell>74.91</cell><cell>74.98</cell><cell>72.95</cell><cell>71.66</cell><cell>69.16</cell></row><row><cell cols="4">Improved Dense Trajectories (iDTs)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>HOG</cell><cell>84.39</cell><cell>78.22</cell><cell>80.29</cell><cell>79.97</cell><cell>78.19</cell><cell>80.20</cell><cell>78.33</cell><cell>72.73</cell><cell>73.76</cell><cell>74.27</cell></row><row><cell>HOF</cell><cell>86.33</cell><cell>85.18</cell><cell>85.92</cell><cell>84.94</cell><cell>85.15</cell><cell>85.87</cell><cell>83.48</cell><cell>80.23</cell><cell>80.58</cell><cell>80.29</cell></row><row><cell>MBHx</cell><cell>84.03</cell><cell>81.33</cell><cell>83.19</cell><cell>82.46</cell><cell>81.28</cell><cell>83.12</cell><cell>81.16</cell><cell>77.77</cell><cell>77.91</cell><cell>77.04</cell></row><row><cell>MBHy</cell><cell>87.02</cell><cell>84.64</cell><cell>86.38</cell><cell>85.29</cell><cell>84.60</cell><cell>86.32</cell><cell>84.04</cell><cell>80.36</cell><cell>80.6</cell><cell>80.3</cell></row><row><cell>d-Fusion</cell><cell>90.84</cell><cell>89.39</cell><cell>90.72</cell><cell>90.62</cell><cell>89.43</cell><cell>90.64</cell><cell>90.18</cell><cell>84.18</cell><cell>84.76</cell><cell>84.67</cell></row><row><cell>r-Fusion</cell><cell>92.07</cell><cell>90.87</cell><cell>91.89</cell><cell>91.50</cell><cell>90.82</cell><cell>91.80</cell><cell>90.56</cell><cell>87.56</cell><cell>87.92</cell><cell>88.12</cell></row><row><cell>s-Fusion</cell><cell>91.03</cell><cell>90.08</cell><cell>90.71</cell><cell>90.36</cell><cell>90.11</cell><cell>90.63</cell><cell>89.67</cell><cell>87.37</cell><cell>87.86</cell><cell>87.41</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc>Comparison of different fusion methods for the encoding methods on the UCF101 dataset.</figDesc><table><row><cell>Methods</cell><cell>FV</cell><cell>SVC</cell><cell>SVC-k</cell><cell>SVC-all</cell><cell>VLAD</cell><cell cols="2">VLAD-k VLAD-all</cell><cell>LLC</cell><cell>SA-k</cell><cell>VQ</cell></row><row><cell cols="4">Space Time Interest Points (STIPs)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>HOG</cell><cell>53.74</cell><cell>47.56</cell><cell>50.07</cell><cell>50.31</cell><cell>47.15</cell><cell>49.21</cell><cell>49.35</cell><cell>46.70</cell><cell>45.79</cell><cell>42.85</cell></row><row><cell>d-Fusion</cell><cell>85.32</cell><cell>83.36</cell><cell>85.19</cell><cell>85.17</cell><cell>83.39</cell><cell>85.14</cell><cell>85.45</cell><cell>77.65</cell><cell>77.96</cell><cell>76.76</cell></row><row><cell>r-Fusion</cell><cell>87.11</cell><cell>84.87</cell><cell>86.54</cell><cell>86.19</cell><cell>84.90</cell><cell>86.16</cell><cell>85.59</cell><cell>81.43</cell><cell>81.65</cell><cell>81.37</cell></row><row><cell>s-Fusion</cell><cell>85.49</cell><cell>83.34</cell><cell>84.84</cell><cell>84.57</cell><cell>83.29</cell><cell>85.04</cell><cell>83.83</cell><cell>80.11</cell><cell>80.39</cell><cell>79.81</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We use s to denote the code of voting and reconstruction based encoding methods, and S to represent the one of super vector based encoding methods.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://serre-lab.clps.brown.edu/resources/HMDB/index.htm</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">http://crcv.ucf.edu/ICCV13-Action-Workshop/ 4 http://www.di.ens.fr/ laptev/download.html 5 https://lear.inrialpes.fr/people/wang/improved trajectories</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kuehne</surname></persName>
		</author>
		<idno>23] 2011 23.0</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sadanand</surname></persName>
		</author>
		<idno>37] 2012 57.9</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Soomro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">40</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sadanand</surname></persName>
		</author>
		<idno>37] 2012 26.9</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kliper-Gross</surname></persName>
		</author>
		<idno>22] 2012 72.7</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Karpthy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kliper-Gross</surname></persName>
		</author>
		<idno>22] 2012 29.2 Solmaz et al. [39] 2012 73.7</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jiang</surname></persName>
		</author>
		<idno>17] 2012 40.7</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reddy</surname></persName>
		</author>
		<idno>36] 2012 76.9</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">58</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<idno>52] 2013 42.1</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<idno>52] 2013 78.4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">46</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">46</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Murthy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peng</surname></persName>
		</author>
		<idno>33] 2013 49.2 Wang et al. [51] 2013 85.7</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Karaman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<idno>47] 2013 57.2 Wang et al. [47] 2013 91.1</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">48</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Human activity analysis: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">All about VLAD</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="1578" to="1585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Pattern Recognition and Machine Learning (Information Science and Statistics)</title>
		<meeting><address><addrLine>Secaucus, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag New York, Inc</publisher>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A theoretical analysis of feature pooling in visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="111" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">From sparse solutions of systems of equations to sparse modeling of signals and images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Bruckstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Review</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="34" to="81" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Multi-view super vector for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Recognition of human body motion using phase space constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">W</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Bobick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<publisher>ICCV</publisher>
			<biblScope unit="page" from="624" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">LIBSVM: A library for support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM TIST</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The devil is in the details: an evaluation of recent feature encoding methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Behavior recognition via sparse spatio-temporal features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rabaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cottrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>VS-PETS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">On feature combination for multiclass object classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Veenman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Geusebroek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1271" to="1283" />
		</imprint>
	</monogr>
	<note>Visual word ambiguity</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Salient coding for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="1753" to="1760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Feature coding in image classification: A comprehensive study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="493" to="506" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Exploiting generative models in discriminative classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Haussler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="487" to="493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Aggregating local image descriptors into compact codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1704" to="1716" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Trajectory-based modeling of human actions with motion reference points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Ngo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>ECCV</publisher>
			<biblScope unit="page" from="425" to="438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">THUMOS challenge: Action recognition with a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Piccardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<ptr target="http://crcv.ucf.edu/ICCV13-Action-Workshop/" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Hierarchical clustering schemes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="241" to="254" />
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">L1-regularized logistic regression stacking and transductive CRF smoothing for action recognition in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Seidenari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Bimbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshop on Action Recognition with a Large Number of Classes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Motion interchange patterns for action recognition in unconstrained videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kliper-Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gurovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">HMDB: A large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2556" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">On space-time interest points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="107" to="123" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning realistic human actions from movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rozenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="2169" to="2178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Efficient sparse coding algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Battle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="801" to="808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<title level="m">defense of soft-assignment coding</title>
		<imprint>
			<publisher>ICCV</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2486" to="2493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Combined ordered and improved trajectories for large scale human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">V R</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goecke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshop on Action Recognition with a Large Number of Classes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Evaluating multimedia features and fusion for examplebased event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">K</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Hout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pancoast</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Habibian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koelma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E A</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Vis. Appl</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="17" to="32" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">On spectral clustering: Analysis and an algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="849" to="856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Analyzing and recognizing walking figures in XYT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="469" to="474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Exploring motion boundary based sampling and spatial-temporal context descriptors for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<imprint>
			<publisher>BMVC</publisher>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Hybrid super vector with improved dense trajectories for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshop on Action Recognition with a Large Number of Classes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Improving the fisher kernel for large-scale image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>ECCV</publisher>
			<biblScope unit="page" from="143" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Recognizing 50 human action categories of web videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Vis. Appl</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="971" to="981" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Action bank: A high-level representation of activity in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sadanand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Video google: A text retrieval approach to object matching in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="1470" to="1477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Classifying web videos using a global video descriptor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Solmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Assari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Vis. Appl</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1473" to="1485" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">UCF101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno>abs/1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Combining the right features for complex event recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2696" to="2703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Signal recovery from random measurements via orthogonal matching pursuit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Tropp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Gilbert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4655" to="4666" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Machine recognition of human activities: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Turaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Subrahmanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Udrea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Techn</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1473" to="1488" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Multiple kernels for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gulshan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="606" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Efficient additive kernels via explicit feature maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="480" to="492" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Dense trajectories and motion boundary descriptors for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kläser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="60" to="79" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>ICCV</publisher>
			<biblScope unit="page" from="3551" to="3558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Lear-inria submission for the thumos workshop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshop on Action Recognition with a Large Number of Classes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Evaluation of local spatio-temporal features for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Ullah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kläser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Locality-constrained linear coding for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="3360" to="3367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Mining motion atoms and phrases for complex action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2680" to="2687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Motionlets: Mid-level 3D parts for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="2674" to="2681" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Latent hierarchical model of temporal structure for complex activity classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="810" to="822" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">A comparative study of encoding, pooling and normalization methods for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>ACCV</publisher>
			<biblScope unit="page" from="572" to="585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Structure from motion of rigid and jointed objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Webb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="page" from="686" to="691" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">An efficient dense and scale-invariant spatio-temporal interest point detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Willems</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J V</forename><surname>Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>ECCV</publisher>
			<biblScope unit="page" from="650" to="663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Towards good practices for action video encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshop on Action Recognition with a Large Number of Classes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Towards good practices for action video encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Group encoding of local features in image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1505" to="1508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Parameterized modeling and recognition of activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yacoob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="232" to="247" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Linear spatial pyramid matching using sparse coding for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="1794" to="1801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Improved local coordinate coding using local tangents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1215" to="1222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Nonlinear learning using local coordinate coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2223" to="2231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Local features and kernels for classification of texture and object categories: A comprehensive study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="213" to="238" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Image classification using super-vector coding of local image descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>ECCV</publisher>
			<biblScope unit="page" from="141" to="154" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
