<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning monocular depth estimation with unsupervised trinocular assumptions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Poggi</surname></persName>
							<email>m.poggi@unibo.it</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Bologna</orgName>
								<address>
									<addrLine>Engineering Viale del Risorgimento 2</addrLine>
									<settlement>Bologna</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Tosi</surname></persName>
							<email>fabio.tosi5@unibo.it</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Bologna</orgName>
								<address>
									<addrLine>Engineering Viale del Risorgimento 2</addrLine>
									<settlement>Bologna</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Mattoccia</surname></persName>
							<email>stefano.mattoccia@unibo.it</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Bologna</orgName>
								<address>
									<addrLine>Engineering Viale del Risorgimento 2</addrLine>
									<settlement>Bologna</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning monocular depth estimation with unsupervised trinocular assumptions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Obtaining accurate depth measurements out of a single image represents a fascinating solution to 3D sensing. CNNs led to considerable improvements in this field, and recent trends replaced the need for ground-truth labels with geometry-guided image reconstruction signals enabling unsupervised training. Currently, for this purpose, state-ofthe-art techniques rely on images acquired with a binocular stereo rig to predict inverse depth (i.e., disparity) according to the aforementioned supervision principle. However, these methods suffer from well-known problems near occlusions, left image border, etc inherited from the stereo setup. Therefore, in this paper, we tackle these issues by moving to a trinocular domain for training. Assuming the central image as the reference, we train a CNN to infer disparity representations pairing such image with frames on its left and right side. This strategy allows obtaining depth maps not affected by typical stereo artifacts. Moreover, being trinocular datasets seldom available, we introduce a novel interleaved training procedure enabling to enforce the trinocular assumption outlined from current binocular datasets. Exhaustive experimental results on the KITTI dataset confirm that our proposal outperforms state-of-the-art methods for unsupervised monocular depth estimation trained on binocular stereo pairs as well as any known methods relying on other cues.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Depth plays a crucial role in many computer vision applications and active 3D sensors are becoming very popular. Nonetheless, such sensors may have severe shortcomings. For instance, the Kinect 1 is not suited at all for outdoor environments flooded by sunlight. Moreover, such sensor allows only for close range depth measurements. On the other hand, a popular active depth sensor perfectly suited for outdoor environments is LIDAR (e.g., Velodyne). However, sensors based on such technology are typically expensive and often cumbersome for some practical applications.  <ref type="bibr" target="#b28">[29]</ref>, our network learns depth representations according to two additional points of view on left (b) and right (d) of the input (a), enabling to infer a more accurate depth map (c). White arrows highlight the different points of view.</p><p>Thus, inferring depth with passive sensors based on standard imaging technology would be highly desirable being cheap, lightweight and suited for indoor and outdoor environments. In this context, acquiring images from different viewpoints allows inferring depth exploiting geometry constraints. On the other hand, estimating depth from a single image is indeed an ill-posed problem. Nonetheless, this latter approach would overcome some major constraints such as the need for simultaneous acquisition in binocular stereo or handling dynamic objects in depth-from-motion approaches. Although a geometrically ambiguous problem, Convolutional Neural Networks (CNNs) achieved outstanding results in monocular depth estimation by casting it as a learning task in both supervised and unsupervised manner. In particular, the latter paradigm addresses the hunger for data typical of deep learning tasks by training networks to produce a depth representation minimizing the warping error between images acquired from multiple points of view rather than the error with respect to difficult to source ground-truth depth labels. In this field, the work of Godard et al. <ref type="bibr" target="#b11">[12]</ref> represents state-of-the-art for unsupervised monocular depth estimation. Deploying stereo imagery for training, a CNN learns to infer disparity from a single reference image and warps the target image accordingly to minimize the appearance error between the warped and the reference image. This strategy yields state-of-the-art performance <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b40">41]</ref>. The CNN is trained to infer disparity from a single reference image and the target image is warped accordingly minimizing the appearance error between warped and reference image. This way, the depth representation learned by the network is affected by artifacts in specific image regions inherited from the stereo setup (e.g., the left border using the left image as the reference) and in occluded areas. The post-processing step proposed in <ref type="bibr" target="#b11">[12]</ref> partially compensates for these artifacts. However, it requires a double forward of the input image and its horizontally flipped version thus obtaining two predictions with artifacts, respectively, on the left and right side of depth discontinuities. Such issues are softened in the final map at the cost of doubling processing time and memory footprint.</p><p>In this paper, we propose to explicitly take into account these artifacts training our network on imagery acquired by a trinocular setup. By assuming the availability of three horizontally aligned images at training time, our network learns to process the frame in the middle and produce inverse depth (i.e., disparity) maps according to all the available viewpoints. By doing so, we can attenuate the aforementioned occlusion artifacts because they occur in different regions of the estimated outputs. However, since trinocular setups are generally uncommon and hence datasets seldom available, we will show how to rely on popular stereo datasets such as CityScapes <ref type="bibr" target="#b2">[3]</ref> and KITTI <ref type="bibr" target="#b10">[11]</ref> to enforce our trinocular training assumption. Experimental results clearly prove that, deploying stereo pairs with a smart strategy aimed at emulating a trinocular setup, our Three-view Network (3Net) is able anyway to learn a three-view representation of the scene as shown intuitively in <ref type="figure" target="#fig_0">Figure 1</ref> and how it leads to more robust monocular depth estimation compared to state-of-the-art methods trained on the same binocular stereo pair with a conventional paradigm. <ref type="figure" target="#fig_0">Figure  1</ref> highlights the behavior of 3Net: we can see how disparity maps (b) and (d), from the point of view of two frames respectively on the left and right side of the reference image, show mirrored artifacts in occluded regions. Combining the two opposite views enables to compensate for these issues and produces a more accurate map (c) centered on the reference frame. Please note that KITTI does not explicitly contain trinocular views as those shown in <ref type="figure" target="#fig_0">Figure  1</ref> and that this behavior is learned by 3Net trained only on standard binocular data. Indeed, images and depth maps in (b) and (d) are inferred by our network. Exhaustive experimental results on the KITTI 2015 stereo dataset <ref type="bibr" target="#b28">[29]</ref> and the Eigen split <ref type="bibr" target="#b5">[6]</ref> of the KITTI dataset <ref type="bibr" target="#b10">[11]</ref> clearly show that 3Net, trained on standard binocular stereo pairs, improves state-of-the-art methods for unsupervised monocular depth estimation, regardless of the cues deployed for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we review the literature concerning single view depth estimation in both supervised and unsupervised manner. Moreover, we also consider early works on multibaseline stereo setup being these approaches relevant to our proposal. Supervised depth-from-mono. The following techniques share the need for difficult to source ground-truth depth measurements for training, thus posing a substantial limitation to their practical deployment. Saxena et al. <ref type="bibr" target="#b32">[33]</ref> estimated depth and local planes using a MRF framework. Ladicky et al. <ref type="bibr" target="#b20">[21]</ref> proved that semantic can help depth estimation using a boosting classifier. More recently, CNN has emerged as mainstream strategy to estimate depth from a single image <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>. Ummenhofer et al. <ref type="bibr" target="#b34">[35]</ref> proposed DeMoN, a deep model to infer both depth and ego-motion from a pair of subsequent frames acquired by a single camera. Fu et al. <ref type="bibr" target="#b7">[8]</ref> introduced a novel strategy to discretize depth and cast the learning process as an ordinal regression problem, while Xu et al. <ref type="bibr" target="#b38">[39]</ref> integrated CRF models into deep architectures to improve depth prediction. Luo et al. <ref type="bibr" target="#b24">[25]</ref> formulated the monocular depth estimation problem as a view synthesis procedure followed by a deep stereo matching approach. Kumar et al. <ref type="bibr" target="#b3">[4]</ref> introduced a Recurrent Neural Network (RNN) aimed at predicting depth from monocular video sequences. Lastly, Atapour et al. <ref type="bibr" target="#b0">[1]</ref> exploited image style transfer and adversarial training to predict depth from real images training the network on a large amount of synthetic data. Unsupervised depth-from-mono. Rethinking depth estimation as an image reconstruction task allowed to avoid the need for ground-truth depth labels and some works concerned with view synthesis paved the way for this purpose. Flynn et al. <ref type="bibr" target="#b6">[7]</ref> proposed DeepStereo to generate new points of view training on images acquired by multiple cameras. Xie et al. <ref type="bibr" target="#b37">[38]</ref> trained their Deep3D framework to create, from a single image, a target frame paired with the input according to a stereo setup by learning a disparity representation.</p><p>Unsupervised monocular depth estimation methods can be broadly categorized into two main categories according to the cues used to replace ground-truth labels. The first one <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12]</ref> leverages images with known relative camera pose, typically acquired by a calibrated stereo rig, following the strategy outlined by Deep3D <ref type="bibr" target="#b37">[38]</ref>. A seminal work using this methodology was proposed by Garg et al. <ref type="bibr" target="#b9">[10]</ref>. Godard et al. <ref type="bibr" target="#b11">[12]</ref> deploying spatial transformer networks <ref type="bibr" target="#b16">[17]</ref> and left-right consistency were able to notably improve depth accuracy. More compact models <ref type="bibr" target="#b31">[32]</ref> can be trained the same way and deployed on embedded systems as well.</p><p>The second category concerns the use of imagery acquired by an unconstrained moving camera <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b26">27]</ref>. Differently, from the previous methodology, temporally adjacent frames acquired by a single moving camera may contain dynamic objects that need to be explicitly handled during re-projection. Moreover, camera pose is unknown and needs to be estimated together with depth. On the other hand, such a strategy does not require a stereo camera to collect training samples. On this track, Zhou et al. <ref type="bibr" target="#b41">[42]</ref> proposed a model to infer depth from unconstrained video sequences by computing a reconstruction loss between subsequent frames and predicting, at the same time, the relative pose between them. This strategy was improved by Mahjourian et al. <ref type="bibr" target="#b26">[27]</ref> thanks to a 3D point-cloud alignment loss and by Wang et al. <ref type="bibr" target="#b35">[36]</ref> including a differentiable implementation of Direct Visual Odometry (DVO) with a novel depth normalization strategy. Yin et al. <ref type="bibr" target="#b39">[40]</ref> proposed GeoNet, a framework for depth and optical flow estimation from monocular sequences. Finally, we mention the work of Zhan et al. <ref type="bibr" target="#b40">[41]</ref> which combined both strategies (i.e., training on stereo sequences) and the semi-supervised works of Kuznietsov et al. <ref type="bibr" target="#b19">[20]</ref> and Kumar et al. <ref type="bibr" target="#b4">[5]</ref>.</p><p>Multi-baseline stereo. It is generally recognized that using more than two views has the potential to improve the quality of depth estimation. An early work concerning multi-camera stereo was proposed by Minoru and Akira <ref type="bibr" target="#b15">[16]</ref> deploying a triangular rig, while Okutomi and Kanade <ref type="bibr" target="#b30">[31]</ref> achieved accurate depth measurements combining stereo from multiple baseline cameras horizontally aligned. Kang et al. <ref type="bibr" target="#b17">[18]</ref> proposed a method to handle the increasing number of occlusions occurring in multiview stereo setup, while Ayache and Lustman <ref type="bibr" target="#b1">[2]</ref> designed a three cameras rig for robotic applications and Garcia et al. <ref type="bibr" target="#b8">[9]</ref> proposed a pose detection algorithm based on a trinocular stereo system. In the last decade, along with the availability of off-the-shelf stereo cameras (e.g., Intel RealSense) some multi-baseline stereo systems too were commercially made available. For instance, the Bumblebee XB3 was used to acquire the RobotCar dataset <ref type="bibr" target="#b25">[26]</ref>, counting millions of images acquired driving for about 1000 Km. Honneger et al. <ref type="bibr" target="#b14">[15]</ref> developed a multi-baseline camera with on-board FPGA, enabling real-time processing of dense disparity maps. Therefore, a trinocular stereo configuration for training, like the one we advocate in our work, would be undoubtedly feasible. Nonetheless, our strat-a) b) <ref type="figure">Figure 2</ref>. Training frameworks enforcing a) binocular <ref type="bibr" target="#b11">[12]</ref> and b) trinocular assumptions. egy is feasible and useful even with conventional binocular datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method overview</head><p>In this section, we propose a framework aimed at enforcing a trinocular assumption for training in an unsupervised manner a network for monocular depth estimation. We will outline the rationale behind this choice and the differences with known techniques in the literature. Then, deploying a conventional binocular stereo dataset, we will show how our strategy allows advancing state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Trinocular assumption and network design</head><p>While traditional depth-from-mono frameworks learn to estimate d(I) from an input image I by minimizing the prediction error with respect to a ground-truth mapd(I) whose pixels are labelled with real depth measurements, the introduction of image-reconstruction based losses moved this task to an unsupervised learning paradigm. In particular, estimated depth is used to project across different points of view exploiting 3D geometry and camera pose thus obtaining supervision signals through the minimization of the re-projection error. According to the literature reviewed in Section 2, the training methodology based on images acquired with a stereo camera, as in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12]</ref>, removes the need to infer pose estimation required when gathering data with a single unconstrained camera.</p><p>Coaching a CNN to infer depth emulating a stereo system for training introduces artifacts in the learned representation (i.e., disparity) intrinsically because of well-known issues arising when dealing with pixels having no direct matches across the two images, such as on left border or occlusions. Godard et al. <ref type="bibr" target="#b11">[12]</ref> deal with this problem using a simple, yet effective, trick. By processing a horizontally flipped input image and then back-flipping the result, artifacts will appear on the opposite side w.r.t. the result obtained on the un-flipped frame (e.g., on the right border rather than on the left). Thus, combining the two predictions allows removing artifacts partially. Nevertheless, this strategy requires two forwards, hence doubling memory footprint and runtime, which would not be necessary if the CNN could learn to estimate disparity concerning a frame acquired on the left w.r.t reference image. Guided by this intuition, we rethink the training protocol of <ref type="bibr" target="#b11">[12]</ref> to exploit a trinocular configuration, on which the image we want to learn the depth of is the central frame of three horizontally aligned points of view. <ref type="figure">Figure 2</ref> gives an overview of our framework b) and the one by Godard et al. <ref type="bibr" target="#b11">[12]</ref> leveraging binocular stereo a). While a) trains the network to estimate a depth representation for I l by means of disparity map d l , used to warp I r toĨ l and measure the appearance difference with I l , we process I c to obtain d lc and d cr , disparity maps assuming as target I l and I r , then we warp these latter two images to obtainĨ cl andĨ cr to finally compute supervision signals as re-projection error w.r.t. I c . Finally, in our framework, d lc and d cl are combined to attenuate occlusions and obtain the final d c from a single forward pass, conversely to <ref type="bibr" target="#b11">[12]</ref> which requires two forwards. Eventually, as <ref type="bibr" target="#b11">[12]</ref> estimates d r to enforce losses betweenĨ r ,I r and the LRC consistency, our network generates d lc and d rc to exploit losses betweenĨ l ,I l andĨ r ,I r . <ref type="figure">Figure 2</ref> also highlights a further main difference between the two frameworks. While a traditional UNet architecture is used by previous works in literature <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b11">12]</ref>, we build two separate decoders respectively in charge of estimating d cl and d cr separately. This strategy adds a negligible overhead regarding memory and runtime requirements, being the encoder the most computationally expensive module of the framework (i.e., the decoder mostly applies upsampling operations). According to our experiments, training a single decoder to infer a disparity representation for both points of view yields slightly worse results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Interleaved training for binocular images</head><p>To effectively learn mirrored representation and compensate for occlusions/borders, the framework outlined so far relies on a set of three horizontally aligned images at training time. Although sensors designed to acquire such imagery are currently available, for instance the aforementioned Bumblebee XB3, it is still quite uncommon to find publicly available images obtained in such configuration. Indeed, in this sense, the Oxford RobotCar dataset <ref type="bibr" target="#b25">[26]</ref> represents an exception providing a large amount of street scenes captured with the trinocular XB3 sensor. Unfortunately, the provided calibration parameters only allow obtaining aligned views between left-right and center-right cameras, hence not permitting to align the three views as we desire. Nonetheless, we describe in this section how to train our framework leveraging the proposed trinocular assumption with a much more common binocular setup (e.g., KITTI dataset). Given a stereo pair made of images L and R, <ref type="figure" target="#fig_1">Figure 3</ref> depicts how to enforce the trinocular assumption by scheduling an interleaved training of the network. We update the parameters of the network by optimizing its four outputs d cl , d lc , d rc and d cr in two steps:</p><p>1. Firstly, we assign L to I l and R to I c as shown by the blue arrows in <ref type="figure" target="#fig_1">Figure 3</ref>. In other words, we assume that the stereo pair represents the left and center images of a virtual trinocular system in which the right frame is missing. In this case, we use as supervision signal the reconstruction error betweenĨ cl , I c andĨ l , I l , producing gradients that flow to the left decoder and the encoder.</p><p>2. Then, as shown by the red arrows in <ref type="figure" target="#fig_1">Figure 3</ref> we change the role of L and R assuming them, respectively, as I c and I r . In this phase, we suppose to have the center and right images available hence implicitly assuming that in our virtual trinocular system the left image is missing. Thus, using the supervision given by re-projection errors on pairsĨ r , I r andĨ cr , I c , we optimize the parameters of the right decoder and the (shared) encoder.</p><p>It is worth to note that, following this protocol, every time we run a training iteration on a stereo pair the network learns all the depth representations output of our framework. Moreover, the two learned disparity pairs from the two views are obtained according to the same baseline (i.e., the same of the training stereo pairs), making them consistent and hence easy to combine in d c . Therefore, the network learns a trinocular representation even if it actually never sees the scene with such setup. Indeed, this strategy is very effective as supported by experimental evidence in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Implementation details</head><p>In this section, we provide a detailed description of our framework, designed with the TensorFlow APIs. The source code is available at https://github.com/ mattpoggi/3net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Network architecture</head><p>For our 3Net we follow a quite established design strategy adopted by other methods in this field <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b11">12]</ref>, based on an encoder-decoder architecture. The peculiarity of our approach consists in two different decoders, as depicted in <ref type="figure" target="#fig_1">Figure 3</ref>, in charge of learning disparity representations w.r.t. two points of view located respectively on the left and right side of the input image. In our network, depicted in <ref type="figure" target="#fig_1">Figure 3</ref>, each decoder generates outputs at four different scales, respectively: full, half, quarter and 1 8 resolution. As encoder, we tested VGG <ref type="bibr" target="#b33">[34]</ref> and ResNet50 <ref type="bibr" target="#b12">[13]</ref> to obtain the most fair and complete comparison w.r.t. <ref type="bibr" target="#b11">[12]</ref>, being it our baseline and state-of-the-art. To obtain the final map d c , we merge the contribution of d cl and d cr using the same post-processing procedure applied in <ref type="bibr" target="#b11">[12]</ref>, thus keeping 5% left-most pixels from d cl , 5% right-most from d cr and averaging the remaining ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Training losses</head><p>We train 3Net to minimize a multi-component loss made of appearance, smoothness and consistency-check terms similarly to <ref type="bibr" target="#b11">[12]</ref>, namely L ap , L ds and L lcr .</p><formula xml:id="formula_0">L total = β ap (L ap ) + β ds (L ds ) + β lcr (L lcr )<label>(1)</label></formula><p>The first term uses a weighted sum of SSIM <ref type="bibr" target="#b36">[37]</ref> and L1 between all four warped pairs and real images as shown on top of <ref type="figure" target="#fig_1">Figure 3</ref>. The second applies an edge aware smoothness constraint to estimated disparities d cl , d lc , d rc and d cr as described in <ref type="bibr" target="#b11">[12]</ref>. Finally, the consistency-check term includes left-right losses between pairs d cl , d lc .</p><formula xml:id="formula_1">L lcr = L lr (d cl , d lc ) + L lr (d cr , d rc )<label>(2)</label></formula><p>For a detailed description of L ap , L ds and L lr please refer to <ref type="bibr" target="#b11">[12]</ref> or our supplementary material. Thus, according to the interleaved training schedule described in Section 3.2, we optimize 3Net splitting the function 5 into two sub-losses L p1 , L p2 deployed in the two different phases:</p><formula xml:id="formula_2">L p1 =β ap (L ap (Ĩ cl , I c ) + L ap (Ĩ l , I l )) + β ds (L ds (d cl , I c ) + L ds (d lc , I l )) + β lcr (L lr (d cl , d lc ) (3) L p2 =β ap (L ap (Ĩ cr , I c ) + L ap (Ĩ r , I r )) + β ds (L ds (d cr , I c ) + L ds (d rc , I l )) + β lcr (L lr (d cr , d lr )<label>(4)</label></formula><p>We also evaluated an additional loss term L cc = |d cl − d cr | to enforce consistency between depth representation centered on I c , being the baseline equal on both directions. However, this term propagates occlusions artifacts between the two depth maps leading to worse results. We point out that despite the interleaved training protocol outlined, in any phase the outcome of 3Net always consists of four depth maps d cl , d lc , d rc and d cr . Of course, this happens at testing/inference time as well, when 3Net is fed with a single image. Considering that decoders outputs depth maps at four scales, all losses are computed for each of them as in <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Training protocol</head><p>We assume as baseline the framework proposed by Godard et al. <ref type="bibr" target="#b11">[12]</ref> using a binocular setup for unsupervised training. For a fair comparison, we train our models following the same guidelines reported in <ref type="bibr" target="#b11">[12]</ref>. In particular, we use CityScapes <ref type="bibr" target="#b2">[3]</ref> (CS) and KITTI raw sequences <ref type="bibr" target="#b10">[11]</ref> datasets for training, this latter sub-sampled according to two training splits of data <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b5">6]</ref> to be able to compare our results with any recent works in this field using unsupervised learning. We refer to these two subsets as KITTI split (K) and Eigen split (E) <ref type="bibr" target="#b5">[6]</ref>. The three training sets count respectively about 23k, 29k and 22.6k stereo pairs. As pointed out by first works using image reconstruction losses <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b41">42]</ref>, training on different datasets helps the network to achieve higher-quality results. Therefore, to better assess the performance of each method, we report experimental results training the networks on K or E. Moreover, we also report results training on CityScapes and then fine tuning on K or E (respectively, referred to as CS+K and CS+E in the tables). Consistently with <ref type="bibr" target="#b11">[12]</ref>, we run 50 epochs of training on each single dataset using a batch size of 8 and input resized to 256 × 512. We use Adam optimizer <ref type="bibr" target="#b18">[19]</ref> with β 1 = 0.9, β 2 = 0.999 and ε = 10 −8 , setting an initial learning rate of 10 −4 halved after 30 and 40 epochs. We maintain the same hyperparameters configuration for β ap , β ds and β lrc defined in <ref type="bibr" target="#b11">[12]</ref> and the same data augmentation procedure as well.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental results</head><p>In this section, we assess the performance of our 3Net framework with respect to state-of-the-art. In all our tests, we report 7 main metrics measuring the average depth error (Abs Rel, Sq Rel, RMSE and RMSE log, the lower the better) and three accuracy scores (δ &lt; 1.25, δ &lt; 1.25 2 and δ &lt; 1.25 3 , the higher the better), First, we report experiments on the K split assuming Godard et al. <ref type="bibr" target="#b11">[12]</ref> as baseline. Then, we exhaustively compare 3Net with top performing unsupervised frameworks for depth-from-mono estimation, highlighting how our proposal is state-of-theart. It is worth stressing that the proposed interleaved training procedure of 3Net, described in Section 3.2, allows for a fair comparison with any other method included in our evaluation being all trained exactly on the same (binocular) datasets. Finally, we report qualitative results concerning the trinocular representation learned by 3Net. <ref type="table" target="#tab_0">Table 1</ref> reports experimental results on the KITTI 2015 stereo dataset <ref type="bibr" target="#b28">[29]</ref>. The evaluation was carried out on 200 stereo pairs with available high quality ground-truth dispar-ity annotations. Additionally, being the outputs of <ref type="bibr" target="#b11">[12]</ref> and 3Net disparity maps, in our evaluation we include the D1-all score representing the percentage of pixels having a disparity error larger than 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">KITTI split</head><p>We compare the raw output d c of our network with the map predicted by Godard et al. with and without postprocessing <ref type="bibr" target="#b11">[12]</ref> (namely "+pp" in the table) running, respectively, a single or two forwards of the network. Moreover, since 3Net can benefit from the same refinement technique by running two predictions on I c and its horizontally flipped version, we also provide results for our network applying the same post-processing. Therefore, we estimate post-processed d cl and d cr before combining them into d c . Anyway, we report for clarity in the last column of the table, the number of forwards required by each entry.</p><p>As reported on the first two rows of <ref type="table" target="#tab_0">Table 1</ref>, training the networks on KITTI data only, our method outperforms the competitor on all metrics except D1-all when running a single forward and it performs very similar to the postprocessed version of <ref type="bibr" target="#b11">[12]</ref> reported in the third row of the table. Rows 3 and 4 highlight that, performing two forwards and post-processing, 3Net + pp outperforms Godard et al. + pp again on all metrics except D1-all.</p><p>Previous works in literature <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b26">27]</ref> proved that transfer learning from CityScape dataset <ref type="bibr" target="#b2">[3]</ref> to KITTI is beneficial and leads to more accurate depth estimation, thus we follow this guideline training on CS+K as well. The last four rows of <ref type="table" target="#tab_0">Table 1</ref> compare both frameworks with and without post-processing. Without postprocessing, we can notice how pre-training on CityScapes dataset allows 3Net to outperform <ref type="bibr" target="#b11">[12]</ref> on all metrics including D1-all. In the last two rows, applying postprocessing to the output of both models, 3Net outperforms the competitor on all metrics tying on δ &lt; 1.25 2 and δ &lt; 1.25 <ref type="bibr" target="#b2">3</ref> . <ref type="figure" target="#fig_3">Figure 4</ref> qualitatively shows depth maps predicted by <ref type="bibr" target="#b11">[12]</ref> (b) and 3Net (c) without applying any postprocessing to better perceive the improvements lead by our framework.</p><p>Summarizing, experiments on the KITTI split highlighted how enforcing the trinocular assumption is more effective than leveraging a conventional stereo paradigm for training. Moreover, these results prove that stereo pairs can  be used in a smarter way following our interleaving strategy. <ref type="table">Table 3</ref> reports evaluation with the split of data of Eigen et al. <ref type="bibr" target="#b5">[6]</ref>, made of 697 images and relative depth measurements acquired with a Velodyne sensor. The table collects results concerning most recent works addressing unsupervised monocular depth estimation. For each method, we indicate the kind of supervision it leverages on: monocular sequences (Temporal), stereo pairs (Stereo) or stereo sequences (Stereo+Temp.). We report results either training on E only or on CS+E, allowing to compare our scores with state-of-the-art approaches. We point out that all methods, including our proposal, are trained exactly on the same images and all of them see the same scenes 1 . On top, we report results for models trained on the Eigen split of data. For GeoNet <ref type="bibr" target="#b39">[40]</ref>, Godard et al. <ref type="bibr" target="#b11">[12]</ref> and our method we report results for both VGG and ResNet50 encoders. We can notice that, in general, methods trained using stereo data usually outperform those trained on monocular video sequences, as evident from recent literature <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b26">27]</ref>. Zhan et al. <ref type="bibr" target="#b40">[41]</ref> leveraging temporally adjacent stereo frames outperform, on most metrics, <ref type="bibr" target="#b11">[12]</ref>. Nevertheless, 3Net achieves better scores except for δ &lt; 1.25 3 still without exploiting temporal supervision. This proves that a smarter deployment of binocular training samples, i.e. by applying our interleaved training to fulfill trinocular hypothesis, is an effective alternative to sequence supervision. It is worth to note that Wang et al. <ref type="bibr" target="#b35">[36]</ref> obtain better scores on most metrics (RMSE, RMSE log and δ metrics) w.r.t. <ref type="bibr" target="#b11">[12]</ref> and 3Net with the VGG encoder. However, by switching to the ResNet50 encoder, Godard et al. <ref type="bibr" target="#b11">[12]</ref> overtakes most recent works that use Time supervision <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b39">40]</ref> with and without post-processing. Systematically, 3Net always outmatches <ref type="bibr" target="#b11">[12]</ref> and consequently all its competitors. In particular, we point out that 3Net ResNet50 without post-processing already achieves some better scores compared to Godard et al. ResNet50 + pp performing, respectively, a single and a double forward.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Eigen split</head><p>On the bottom of <ref type="table">Table 3</ref>, we resume results achieved by models trained on CS+E. We observe the same trend highlighted in the previous experiments, being <ref type="bibr" target="#b11">[12]</ref> and our proposal the most effective solutions for this task thanks to stereo supervision. In equal conditions, i.e. same encoder and number of forwards, 3Net always outperforms the framework of Godard et al. exploiting the trinocular assumption. Moreover, the proposed technique leads to major improvements such that 3Net VGG outperforms ResNet50 model by Godard et al. <ref type="bibr" target="#b11">[12]</ref> (rows 20 th and 21 st ), 3Net ResNet50 without post-processing achieves more accurate Running traditional stereo algorithms <ref type="bibr" target="#b13">[14]</ref>, assuming the left-most generated frame as reference, allows to obtain disparity maps with narrow (c) and wide (d) baseline (encoded with colormap jet). We point out that the center frame (a) is the only real image. More qualitative examples in the supplementary material.</p><p>results than the best configuration ResNet50 + pp <ref type="bibr" target="#b11">[12]</ref> (rows 22 nd and 23 rd ) and finally 3Net ResNet50 + pp outmatches all known frameworks for unsupervised depth-from-mono estimation. These facts clearly highlight that our proposal is state-of-the-art.</p><p>It is important to underline that the availability of a real trinocular rig would most probably allow training a more accurate model, given the larger amount of images w.r.t. a binocular stereo rig. The interleaved training proposed in this paper allows to overcome the lack of trinocular training samples using binocular pairs and also allows for a more fair comparison with other techniques leveraging this latter configuration only. This fact proves that the effectiveness of our strategy is due to the rationale behind it and not driven by a more extensive availability of data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">View synthesis</head><p>Finally, we show through qualitative images some outcomes of 3Net obtainable exploiting the embodied trinocular assumption. <ref type="bibr" target="#b1">2</ref> . A peculiar experiment allowed by our framework consists of generating three horizontally aligned views from a single input image. This feature is possible thanks to estimated d lc and d rc , used to warp the input image towards two new viewpoints, respectively, on the left and the right. In other words, given I c at test time we com-puteĨ l andĨ r , producing a trinocular setup of horizontally aligned images. <ref type="figure" target="#fig_4">Figure 5</ref> shows an example of a single frame (a) taken from the KITTI dataset and how our network generates the three views superimposed in (b). These views effectively enable to realize a multi-baseline setup. Thus we can run any stereo algorithm between the possible pairs. For instance, we run the Semi-Global Matching algorithm (SGM) <ref type="bibr" target="#b13">[14]</ref> betweenĨ l and I c , showing the results in <ref type="figure" target="#fig_4">Figure 5</ref> (c), then we run SGM betweenĨ l andĨ r obtaining the disparity map shown in (d). The two dispar-ity maps assume the same frame as the reference image (Ĩ l ) and two different targets, according to two different narrow and wide virtual baseline. The shorter baseline is learned from the KITTI acquisition rig while the longer one is inherited by our trinocular assumption although actually, it does not exist at all in the training set. This fact can be perceived by looking at the different disparity ranges encoded, with colormap jet, on (c) and (d). This feature of our network paves the way to exciting novel developments. For instance, a conceivable application would consist in the synthesis of augmented stereo pairs to train CNNs for disparity inference <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b11">12]</ref> or to improve recent techniques such as single view stereo <ref type="bibr" target="#b24">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions</head><p>In this paper, we proposed a novel methodology for unsupervised training of a depth-from-mono CNN. By enforcing a trinocular assumption, we overcome some limitations due to binocular stereo images used as supervision and obtain a more accurate depth estimation with our 3Net architecture. Although three horizontally aligned views are seldom available, we proposed an interleaved training protocol allowing to leverage on traditional binocular datasets. This latter technique also ensures for a fair comparison w.r.t. all previous works and allows us to prove that 3Net outperforms all unsupervised techniques known in the literature, establishing itself as state-of-the-art. Moreover, 3Net learns a trinocular representation of the world, making it suitable for image synthesis purposes and other interesting future developments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary material</head><p>This document provides additional details and experimental results concerned with paper "Learning monocular depth estimation under unsupervised trinocular assumption". The supplementary material is organized as follows: Section 8 reports detailed explanation of the loss functions used at training time, Section 9 describes how we obtain d c with 3Net and how we post-process it, Section 10 comments additional experiments on the Eigen split <ref type="bibr" target="#b5">[6]</ref> assuming as maximum depth 50 meters and Section 11 collects additional qualitative results, Finally, Section 12 reports run time analysis for 3Net and <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Training losses</head><p>In the paper, all loss functions are computed at four scales, ranging from full image resolution to <ref type="bibr">1 8</ref> . The global loss function is defined as:</p><formula xml:id="formula_3">L total = β ap (L ap ) + β ds (L ds ) + β lcr (L lcr )<label>(5)</label></formula><p>where L ap , L ds and L lcr represent, respectively, the appearance, smoothness and consistency terms, while β ap , β ds and β lcr are hyper-parameters. In particular, we set β ap = β lcr = 1 and β ds = 0.1.</p><p>Appearance Loss. It measures the reconstruction error between a warped image and the original one. It is obtained by a weighted sum of a SSIM based score <ref type="bibr" target="#b36">[37]</ref> and a L1 distance over pixel intensities.</p><formula xml:id="formula_4">L ap (I l , I r ) = 1 N ij α 1 − SSIM (I l ij ,Ĩ r ij ) 2 + (1 − α)||I l ij −Ĩ r ij ||<label>(6)</label></formula><p>Smoothness Loss. This term favours the propagation of similar disparity values in low-textured regions, thus enforcing smoothness. It is obtained computing horizontal and vertical gradients on both disparity image and reference image, discouraging disparity smoothness in presence of strong image gradients.</p><formula xml:id="formula_5">L ds (d, I) = 1 N ij |∂ x d l ij |e −||∂xI l ij || + |∂ y d l ij |e −||∂yI l ij ||<label>(7)</label></formula><p>Left-Right Disparity Consistency Loss. It enforces consistency between reference-to-target and targetto-reference disparity maps. It relies on the L1 distance between reference-to-target map and warped, according to the former, target-to-reference map.</p><formula xml:id="formula_6">L lr (d l , d r ) = 1 N ij |d l ij − d r ij+d l ij | (8)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Depth computation and post-processing</head><p>For the sake of clarity, we describe in detail how we combine d cl and d cr to obtain the final output map d c . In <ref type="bibr" target="#b11">[12]</ref> the authors obtained d l andd l by processing, respectively, both I and its horizontally flipped versionÎ. The two maps were combined as follows:</p><formula xml:id="formula_7">d pp = ω · d l + (1 − ω) ·d l<label>(9)</label></formula><p>with ω obtained as:</p><formula xml:id="formula_8">ω =      0 ifj ≤ 0.05 1 ifj &gt; 0.95 0.5 otherwise<label>(10)</label></formula><p>being i, j normalized pixel coordinates. Following this principle, we combine our d cl and d cr maps as follows:</p><formula xml:id="formula_9">d c = ω · d cr + (1 − ω) · d cl<label>(11)</label></formula><p>Running two forwards, we can post-process both intermediate maps and</p><formula xml:id="formula_10">d c pp = ω · d cr pp + (1 − ω) · d cl pp<label>(12)</label></formula><p>being d cr pp and d cl pp obtained as:</p><formula xml:id="formula_11">d cr pp = ω · d cr + (1 − ω) ·d cr (13) d cl pp = ω ·d cl + (1 − ω) · d cl<label>(14)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.">Depth estimation: additional experiments with 50m cap</head><p>We report additional experimental results on the Eigen split <ref type="bibr" target="#b5">[6]</ref>, evaluating depth maps up to a maximum distance of 50 meters as reported in some recent works <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b41">42]</ref>. <ref type="table">Table 3</ref> contains a comparison between all previous works reporting this experiment as well and our best model, i.e. 3Net ResNet50 + pp. This further evaluation confirms, once again, the superiority of our technique with respect to all competitors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">View synthesis and multi-baseline stereo</head><p>Finally, deploying 3Net ResNet50 + pp trained on CS+E, we provide additional qualitative results for depth-frommono estimation and view synthesis. <ref type="figure">Figure 6 and 7</ref>   <ref type="table">Table 3</ref>. Evaluation on the KITTI dataset <ref type="bibr" target="#b10">[11]</ref> using the split of Eigen et al. <ref type="bibr" target="#b5">[6]</ref>, with maximum depth set to 50m. Results concerned with state-of-the-art techniques for unsupervised monocular depth estimation leveraging video sequences (Temporal), binocular stereo pairs (Stereo) and both cues (Stereo+Temp.).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>256×512 384×1280 1×</head><p>2× 1× 2× <ref type="bibr" target="#b11">[12]</ref> ResNet50 0.57s 1.10s 1.98s 3.92s 3Net ResNet50 0.80s 1.55s 2.95s 5.87s <ref type="table">Table 4</ref>. Run time comparison between Godard et al. <ref type="bibr" target="#b11">[12]</ref> and 3Net running single and double forward on a CPU Intel Core i7-7700K. <ref type="bibr" target="#b5">[6]</ref>. In particular, we show in the leftmost column the generated left view (a), the single input image fed to our network (b) and the generated right view (c). In the mid column, the three output maps of 3Net, respectively, d cl (d), d c (e) and d cr (f). Finally, in the rightmost column, we report disparity maps obtained processing with SGM <ref type="bibr" target="#b13">[14]</ref> the three stereo pairs obtainable with 3Net from the three views (one real, two synthetic) depicted in the leftmost column. In particular, the disparity maps computed by SGM are concerned with three stereo pairs: left-to-center (g), center-to-right (h) and left-to-right (i). It is worth to note that the left-to-right stereo pair (i) is made of two completely novel views synthesized by our network. The other two stereo pairs contain the input image and a a novel image synthesized by 3Net.</p><p>Observing (a), (b) and (c) we can easily notice three different view points: the two virtual cameras are located at the left and right side of the real camera (i.e., the central one). The three maps in the middle column clearly show artifacts occurring near depth discontinuities and occlusions in (d) and (f) and how they are greatly dampen in the final output of our network (e). Finally, we can perceive how (g) and (i) share the same reference image (synthetic left) and how they compute different disparity values according to different baselines, narrow and wide, made available by the three-view virtual rig enabled by 3Net.</p><p>A video showing the performance of 3Net on the KITTI sequence 2011 10 03 drive 0047 sync <ref type="bibr" target="#b10">[11]</ref> not part of the Eigen split imagery used for training is available at this link: https://www.youtube.com/watch?v= uMA5YWJME4M. Finally, the source code is available at this link: https: //github.com/mattpoggi/3net</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12.">Runtime analysis</head><p>In this section, we briefly compare the runtime of 3Net compared to the models by Godard et al. <ref type="bibr" target="#b11">[12]</ref>. On highend GPUs (e.g., Titan X Pascal), the difference between the two models either running single or double forward is negligible, taking between 0.09 and 0.11 seconds both. Nevertheless, in case of applications deploying different architectures the margin rises.</p><p>In particular, <ref type="table">Table 4</ref> compares the execution times of the considered models using ResNet50 encoder on a CPU Intel Core i7-7700K. Times are averaged on the entire Eigen split testing set. We report numbers at 256 × 512 resolution (i.e., the dimensions used by <ref type="bibr" target="#b11">[12]</ref> at inference time), as well as at full KITTI resolution, to stress how the difference between them increases with the image size. We can see how the second encoder in 3Net adds about 50% overhead, while 2× forwards usually doubles it. However, by recalling results reported in the main paper ( <ref type="table" target="#tab_2">Table 2</ref>, last 3 rows on bottom), 3Net ResNet50 running a single forward is more accurate and faster than <ref type="bibr" target="#b11">[12]</ref> ResNet50 running two forwards.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Overview of 3Net. a) Given a single reference image from KITTI 2015 training set</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Scheme of interleaved training. A binocular stereo pair is used to train the network enforcing a trinocular assumption by first setting L → I l , R → I c (blue arrows) and optimizing the model according to losses onĨ l ,Ĩ cl , then setting L → I c , R → I r (red arrows) and optimizing the model according to losses onĨ cr ,Ĩ r .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Depth maps predicted from input image (a) by Godard et al. [12] (b) and 3Net (c) running a single forward pass.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Qualitative example of learned trinocular setup. Given a single, input image (a), 3Net can generate two additional points of view, shown superimposed to the real frame in (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison between 3Net and<ref type="bibr" target="#b11">[12]</ref>, both using VGG as encoder, on KITTI 2015 training dataset<ref type="bibr" target="#b28">[29]</ref>.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Proposed method</cell><cell cols="2">Lower is better</cell><cell cols="2">Higher is better</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="3">Train set Abs Rel Sq Rel</cell><cell>RMSE</cell><cell cols="6">RMSE log D1-all δ &lt;1.25 δ &lt; 1.25 2 δ &lt; 1.25 3 Forwards</cell></row><row><cell>Godard et al. [12]</cell><cell>K</cell><cell>0.124</cell><cell>1.388</cell><cell>6.125</cell><cell>0.217</cell><cell>30.272</cell><cell>0.841</cell><cell>0.936</cell><cell>0.975</cell><cell>×1</cell></row><row><cell>3Net</cell><cell>K</cell><cell>0.119</cell><cell>1.201</cell><cell>5.888</cell><cell>0.208</cell><cell>31.851</cell><cell>0.844</cell><cell>0.941</cell><cell>0.978</cell><cell>×1</cell></row><row><cell>Godard et al. [12] + pp</cell><cell>K</cell><cell>0.117</cell><cell>1.177</cell><cell>5.804</cell><cell>0.206</cell><cell>29.945</cell><cell>0.848</cell><cell>0.943</cell><cell>0.977</cell><cell>×2</cell></row><row><cell>3Net + pp</cell><cell>K</cell><cell>0.114</cell><cell>1.088</cell><cell>5.756</cell><cell>0.203</cell><cell>31.141</cell><cell>0.848</cell><cell>0.944</cell><cell>0.979</cell><cell>×2</cell></row><row><cell>Godard et al. [12]</cell><cell>CS+K</cell><cell>0.104</cell><cell>1.070</cell><cell>5.417</cell><cell>0.188</cell><cell>25.523</cell><cell>0.875</cell><cell>0.956</cell><cell>0.983</cell><cell>×1</cell></row><row><cell>3Net</cell><cell>CS+K</cell><cell>0.101</cell><cell>0.954</cell><cell>5.211</cell><cell>0.181</cell><cell>24.632</cell><cell>0.875</cell><cell>0.958</cell><cell>0.985</cell><cell>×1</cell></row><row><cell>Godard et al. [12] + pp</cell><cell>CS+K</cell><cell>0.100</cell><cell>0.934</cell><cell>5.141</cell><cell>0.178</cell><cell>25.077</cell><cell>0.878</cell><cell>0.961</cell><cell>0.986</cell><cell>×2</cell></row><row><cell>3Net + pp</cell><cell>CS+K</cell><cell>0.097</cell><cell>0.893</cell><cell>5.079</cell><cell>0.176</cell><cell>23.867</cell><cell>0.881</cell><cell>0.961</cell><cell>0.986</cell><cell>×2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Abs Rel Sq Rel RMSE RMSE log δ &lt;1.25 δ &lt; 1.25 2 δ &lt; 1.25<ref type="bibr" target="#b2">3</ref> Kumar et al.<ref type="bibr" target="#b4">[5]</ref> (photo. + adv.)</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Proposed method</cell><cell cols="2">Lower is better</cell><cell cols="2">Higher is better</cell><cell></cell></row><row><cell>Method</cell><cell cols="2">Supervision Train set Temporal E</cell><cell>0.211</cell><cell>1.980</cell><cell>6.154</cell><cell>0.264</cell><cell>0.732</cell><cell>0.898</cell><cell>0.959</cell></row><row><cell>Zhou et al. [42]</cell><cell>Temporal</cell><cell>E</cell><cell>0.208</cell><cell>1.768</cell><cell>6.856</cell><cell>0.283</cell><cell>0.678</cell><cell>0.885</cell><cell>0.957</cell></row><row><cell>Zhou et al. [42] updated [40]</cell><cell>Temporal</cell><cell>E</cell><cell>0.183</cell><cell>1.595</cell><cell>6.709</cell><cell>0.270</cell><cell>0.734</cell><cell>0.902</cell><cell>0.959</cell></row><row><cell>Mahjourian et al. [27]</cell><cell>Temporal</cell><cell>E</cell><cell>0.163</cell><cell>1.240</cell><cell>6.220</cell><cell>0.250</cell><cell>0.762</cell><cell>0.916</cell><cell>0.968</cell></row><row><cell>Yin et al. [40] GeoNet</cell><cell>Temporal</cell><cell>E</cell><cell>0.164</cell><cell>1.303</cell><cell>6.090</cell><cell>0.247</cell><cell>0.765</cell><cell>0.919</cell><cell>0.968</cell></row><row><cell>Yin et al. [40] GeoNet ResNet50</cell><cell>Temporal</cell><cell>E</cell><cell>0.155</cell><cell>1.296</cell><cell>5.857</cell><cell>0.233</cell><cell>0.793</cell><cell>0.931</cell><cell>0.973</cell></row><row><cell>Wang et al. [36]</cell><cell>Temporal</cell><cell>E</cell><cell>0.151</cell><cell>1.257</cell><cell>5.583</cell><cell>0.228</cell><cell>0.810</cell><cell>0.936</cell><cell>0.974</cell></row><row><cell>Poggi et al. [32] PyD-Net (200)</cell><cell>Stereo</cell><cell>E</cell><cell>0.153</cell><cell>1.363</cell><cell>6.030</cell><cell>0.252</cell><cell>0.789</cell><cell>0.918</cell><cell>0.963</cell></row><row><cell>Godard et al. [12]</cell><cell>Stereo</cell><cell>E</cell><cell>0.148</cell><cell>1.344</cell><cell>5.927</cell><cell>0.247</cell><cell>0.803</cell><cell>0.922</cell><cell>0.964</cell></row><row><cell>Zhan et al. [41]</cell><cell>Stereo+Temp.</cell><cell>E</cell><cell>0.144</cell><cell>1.391</cell><cell>5.869</cell><cell>0.241</cell><cell>0.803</cell><cell>0.928</cell><cell>0.969</cell></row><row><cell>3Net</cell><cell>Stereo</cell><cell>E</cell><cell>0.142</cell><cell>1.207</cell><cell>5.702</cell><cell>0.240</cell><cell>0.809</cell><cell>0.928</cell><cell>0.967</cell></row><row><cell>Godard et al. [12] ResNet50</cell><cell>Stereo</cell><cell>E</cell><cell>0.133</cell><cell>1.142</cell><cell>5.533</cell><cell>0.230</cell><cell>0.830</cell><cell>0.936</cell><cell>0.970</cell></row><row><cell>3Net ResNet50</cell><cell>Stereo</cell><cell>E</cell><cell>0.129</cell><cell>0.996</cell><cell>5.281</cell><cell>0.223</cell><cell>0.831</cell><cell>0.939</cell><cell>0.974</cell></row><row><cell>Godard et al. [12] ResNet50 + pp</cell><cell>Stereo</cell><cell>E</cell><cell>0.128</cell><cell>1.038</cell><cell>5.355</cell><cell>0.223</cell><cell>0.833</cell><cell>0.939</cell><cell>0.972</cell></row><row><cell>3Net ResNet50 + pp</cell><cell>Stereo</cell><cell>E</cell><cell>0.126</cell><cell>0.961</cell><cell>5.205</cell><cell>0.220</cell><cell>0.835</cell><cell>0.941</cell><cell>0.974</cell></row><row><cell>Zhou et al. [42]</cell><cell>Temporal</cell><cell>CS+E</cell><cell>0.198</cell><cell>1.836</cell><cell>6.565</cell><cell>0.275</cell><cell>0.718</cell><cell>0.901</cell><cell>0.960</cell></row><row><cell>Mahjourian et al. [27]</cell><cell>Temporal</cell><cell>CS+E</cell><cell>0.159</cell><cell>1.231</cell><cell>5.912</cell><cell>0.243</cell><cell>0.784</cell><cell>0.923</cell><cell>0.970</cell></row><row><cell>Yin et al. [40] GeoNet ResNet50</cell><cell>Temporal</cell><cell>CS+E</cell><cell>0.153</cell><cell>1.328</cell><cell>5.737</cell><cell>0.232</cell><cell>0.802</cell><cell>0.934</cell><cell>0.972</cell></row><row><cell>Wang et al. [36]</cell><cell>Temporal</cell><cell>CS+E</cell><cell>0.148</cell><cell>1.187</cell><cell>5.496</cell><cell>0.226</cell><cell>0.812</cell><cell>0.938</cell><cell>0.975</cell></row><row><cell>Poggi et al. [32] PyD-Net (200)</cell><cell>Stereo</cell><cell>CS+E</cell><cell>0.146</cell><cell>1.291</cell><cell>5.907</cell><cell>0.245</cell><cell>0.801</cell><cell>0.926</cell><cell>0.967</cell></row><row><cell>Godard et al. [12]</cell><cell>Stereo</cell><cell>CS+E</cell><cell>0.124</cell><cell>1.076</cell><cell>5.311</cell><cell>0.219</cell><cell>0.847</cell><cell>0.942</cell><cell>0.973</cell></row><row><cell>3Net</cell><cell>Stereo</cell><cell>CS+E</cell><cell>0.117</cell><cell>0.905</cell><cell>4.982</cell><cell>0.210</cell><cell>0.856</cell><cell>0.948</cell><cell>0.976</cell></row><row><cell>Godard et al. [12] ResNet50</cell><cell>Stereo</cell><cell>CS+E</cell><cell>0.121</cell><cell>1.037</cell><cell>5.212</cell><cell>0.216</cell><cell>0.854</cell><cell>0.944</cell><cell>0.973</cell></row><row><cell>3Net ResNet50</cell><cell>Stereo</cell><cell>CS+E</cell><cell>0.113</cell><cell>0.885</cell><cell>4.898</cell><cell>0.204</cell><cell>0.862</cell><cell>0.950</cell><cell>0.977</cell></row><row><cell>Godard et al. [12] ResNet50 + pp</cell><cell>Stereo</cell><cell>CS+E</cell><cell>0.114</cell><cell>0.898</cell><cell>4.935</cell><cell>0.206</cell><cell>0.861</cell><cell>0.949</cell><cell>0.976</cell></row><row><cell>3Net ResNet50 + pp</cell><cell>Stereo</cell><cell>CS+E</cell><cell>0.111</cell><cell>0.849</cell><cell>4.822</cell><cell>0.202</cell><cell>0.865</cell><cell>0.952</cell><cell>0.978</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table /><note>Evaluation on the KITTI dataset [11] using the split of Eigen et al. [6], with maximum depth set to 80m. Results concerned with state-of-the-art techniques for unsupervised monocular depth estimation leveraging video sequences (Temporal), binocular stereo pairs (Stereo) and both cues (Stereo+Temp.).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>reports six examples taken from the evaluation set of the Eigen split Abs Rel Sq Rel RMSE RMSE log δ &lt;1.25 δ &lt; 1.25 2 δ &lt; 1.25<ref type="bibr" target="#b2">3</ref> </figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Proposed method</cell><cell cols="2">Lower is better</cell><cell cols="2">Higher is better</cell><cell></cell></row><row><cell cols="3">Method Train set Zhou et al. [42] Supervision Temporal E</cell><cell>0.201</cell><cell>1.391</cell><cell>5.181</cell><cell>0.264</cell><cell>0.696</cell><cell>0.900</cell><cell>0.966</cell></row><row><cell>Mahjourian et al. [27]</cell><cell>Temporal</cell><cell>E</cell><cell>0.155</cell><cell>0.927</cell><cell>4.549</cell><cell>0.231</cell><cell>0.781</cell><cell>0.931</cell><cell>0.975</cell></row><row><cell>Zhan et al. [41]</cell><cell>Stereo+Temp.</cell><cell>E</cell><cell>0.135</cell><cell>0.905</cell><cell>4.366</cell><cell>0.225</cell><cell>0.818</cell><cell>0.937</cell><cell>0.973</cell></row><row><cell>Godard et al. [12] ResNet50 + pp</cell><cell>Stereo</cell><cell>E</cell><cell>0.1217</cell><cell>0.7630</cell><cell>4.047</cell><cell>0.210</cell><cell>0.847</cell><cell>0.946</cell><cell>0.976</cell></row><row><cell>3Net ResNet50 + pp (ours)</cell><cell>Stereo</cell><cell>E</cell><cell>0.1207</cell><cell>0.7185</cell><cell>3.968</cell><cell>0.208</cell><cell>0.849</cell><cell>0.948</cell><cell>0.977</cell></row><row><cell>Zhou et al. [42]</cell><cell>Temporal</cell><cell>CS+E</cell><cell>0.190</cell><cell>1.436</cell><cell>4.975</cell><cell>0.258</cell><cell>0.735</cell><cell>0.915</cell><cell>0.968</cell></row><row><cell>Mahjourian et al. [27]</cell><cell>Temporal</cell><cell>CS+E</cell><cell>0.151</cell><cell>0.949</cell><cell>4.383</cell><cell>0.227</cell><cell>0.802</cell><cell>0.935</cell><cell>0.974</cell></row><row><cell>Poggi et al. [32] PyD-Net (200)</cell><cell>Stereo</cell><cell>CS+E</cell><cell>0.138</cell><cell>0.937</cell><cell>4.488</cell><cell>0.230</cell><cell>0.815</cell><cell>0.934</cell><cell>0.972</cell></row><row><cell>Godard et al. [12] ResNet50 + pp</cell><cell>Stereo</cell><cell>CS+E</cell><cell>0.108</cell><cell>0.657</cell><cell>3.729</cell><cell>0.194</cell><cell>0.873</cell><cell>0.954</cell><cell>0.979</cell></row><row><cell>3Net ResNet50 + pp (ours)</cell><cell>Stereo</cell><cell>CS+E</cell><cell>0.091</cell><cell>0.572</cell><cell>3.459</cell><cell>0.183</cell><cell>0.889</cell><cell>0.955</cell><cell>0.979</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Zhan et al.<ref type="bibr" target="#b40">[41]</ref> report scores training on E only or after pre-training on NYU dataset<ref type="bibr" target="#b29">[30]</ref>. For fairness we report the first setup only.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">A video is available at http://youtu.be/uMA5YWJME4M</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Figure 7. Qualitative evaluation of 3Net. In the leftmost column, we show (always from top to bottom) synthetic left (a), real central (b) and synthetic right (c) view. In the middle column, d cl (d), d c (e) and d cr (f) depth maps computed by our network processing the input image. In the rightmost column, disparity maps obtained with SGM algorithm<ref type="bibr" target="#b13">[14]</ref> processing respectively, left-center (g), center-right (h) and left-right (i) stereo pair.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan X GPU used for this research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan X GPU used for this research.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>(a)</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Real-time monocular depth estimation using synthetic data with domain adaptation via image style transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Atapour-Abarghouei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Breckon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Trinocular stereo vision for robotics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ayache</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lustman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="73" to="85" />
			<date type="published" when="1991-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Depthnet: A recurrent neural network architecture for monocular depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Bhandarkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mukta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">1st International Workshop on Deep Learning for Visual SLAM, (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Monocular depth prediction using generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Bhandarkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mukta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">1st International Workshop on Deep Learning for Visual SLAM</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deepstereo: Learning to predict new views from the world&apos;s imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Neulander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5515" to="5524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep ordinal regression network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Batmanghelich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A new approach to pose detection using a trinocular stereovision system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Batlle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salvi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Real-Time Imaging</title>
		<imprint>
			<date type="published" when="2002-04" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="73" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised cnn for single view depth estimation: Geometry to the rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Bg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="740" to="756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised monocular depth estimation with left-right consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Accurate and efficient stereo processing by semi-global matching and mutual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hirschmuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Embedded realtime multi-baseline stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Honegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Three-view stereo analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="524" to="532" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Handling occlusions in dense multi-view stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2001 IEEE Computer Society Conference on</title>
		<meeting>the 2001 IEEE Computer Society Conference on</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Computer Vision and Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semi-supervised deep learning for monocular depth map prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kuznietsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stuckler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pulling things out of perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="89" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deeper depth prediction with fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3D Vision (3DV), 2016 Fourth International Conference on</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Depth and surface normal estimation from monocular images using regression on deep features and hierarchical crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1119" to="1127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning depth from single monocular images using deep convolutional neural fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Single view stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">1 Year, 1000km: The Oxford RobotCar Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Maddern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pascoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Linegar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research (IJRR)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from monocular video using 3d geometric constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mahjourian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4040" to="4048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Object scene flow for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Nathan Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A multiple-baseline stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Okutomi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="353" to="363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Towards real-time unsupervised monocular depth estimation on cpu</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Aleotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mattoccia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/JRS Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Make3d: Learning 3d scene structure from a single still image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="824" to="840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Demon: Depth and motion network for learning monocular stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ummenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning depth from monocular videos using direct methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Buenaposada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Image quality assessment: From error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Img. Proc</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2004-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep3d: Fully automatic 2d-to-3d video conversion with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Structured attention guided convolutional neural fields for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Geonet: Unsupervised learning of dense depth, optical flow and camera pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unsupervised learning of monocular depth estimation and visual odometry with deep feature reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Weerasekera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
