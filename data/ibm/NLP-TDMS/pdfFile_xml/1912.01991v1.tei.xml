<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-Supervised Learning of Pretext-Invariant Representations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Self-Supervised Learning of Pretext-Invariant Representations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The goal of self-supervised learning from images is to construct image representations that are semantically meaningful via pretext tasks that do not require semantic annotations for a large training set of images. Many pretext tasks lead to representations that are covariant with image transformations. We argue that, instead, semantic representations ought to be invariant under such transformations. Specifically, we develop Pretext-Invariant Representation Learning (PIRL, pronounced as "pearl") that learns invariant representations based on pretext tasks. We use PIRL with a commonly used pretext task that involves solving jigsaw puzzles. We find that PIRL substantially improves the semantic quality of the learned image representations. Our approach sets a new state-of-the-art in selfsupervised learning from images on several popular benchmarks for self-supervised learning. Despite being unsupervised, PIRL outperforms supervised pre-training in learning image representations for object detection. Altogether, our results demonstrate the potential of self-supervised learning of image representations with good invariance properties.</p><p>arXiv:1912.01991v1 [cs.CV]  </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Modern image-recognition systems learn image representations from large collections of images and corresponding semantic annotations. These annotations can be provided in the form of class labels <ref type="bibr" target="#b58">[58]</ref>, hashtags <ref type="bibr" target="#b41">[41]</ref>, bounding boxes <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b39">39]</ref>, etc. Pre-defined semantic annotations scale poorly to the long tail of visual concepts <ref type="bibr" target="#b66">[66]</ref>, which hampers further improvements in image recognition.</p><p>Self-supervised learning tries to address these limitations by learning image representations from the pixels themselves without relying on pre-defined semantic annotations. Often, this is done via a pretext task that applies a transformation to the input image and requires the learner to predict properties of the transformation from the transformed image (see <ref type="figure">Figure 1</ref>). Examples of image transformations used include rotations <ref type="bibr" target="#b17">[18]</ref>, affine transformations <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b57">57,</ref><ref type="bibr" target="#b77">76]</ref>, and jigsaw transformations <ref type="bibr" target="#b46">[46]</ref>. As  <ref type="figure">Figure 1</ref>: Pretext-Invariant Representation Learning (PIRL). Many pretext tasks for self-supervised learning <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b77">76]</ref> involve transforming an image I, computing a representation of the transformed image, and predicting properties of transformation t from that representation. As a result, the representation must covary with the transformation t and may not contain much semantic information. By contrast, PIRL learns representations that are invariant to the transformation t and retain semantic information.</p><formula xml:id="formula_0">v 6 F m z A R x Z o K M j / k x x z p E K U F o C G T l G g + N Q Q T y U x W R M Z Y Y q J N T a W s h M s U Z 9 9 f X i S d R q 1 + U j u 9 a 1 S a V 3 k d R T i A Q z i G O p x D E 2 6 g B W 0 g E M E j P M O L F V t P 1 q v 1 N h 8 t W P n O P v y C 9 f 4 F i w C R f w = = &lt; / l a t e x i t &gt; ConvNet</formula><p>the pretext task involves predicting a property of the image transformation, it encourages the construction og image representations that are covariant to the transformations. Although such covariance is beneficial for tasks such as predicting 3D correspondences <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b57">57]</ref>, it is undesirable for most semantic recognition tasks. Representations ought to be invariant under image transformations to be useful for image recognition <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b28">29]</ref> because the transformations do not alter visual semantics.</p><p>Motivated by this observation, we propose a method that learns invariant representations rather than covariant ones. Instead of predicting properties of the image transformation, Pretext-Invariant Representation Learning (PIRL) constructs image representations that are similar to the representation of transformed versions of the same image and different from the representations of other images. We adapt the "Jigsaw" pretext task <ref type="bibr" target="#b46">[46]</ref> to work with PIRL and find that the resulting invariant representations perform better than their covariant counterparts across a range of vision tasks. PIRL substantially outperforms all prior art in selfsupervised learning from ImageNet ( <ref type="figure" target="#fig_1">Figure 2</ref>) and from uncurated image data <ref type="table" target="#tab_5">(Table 4</ref>). Interestingly, PIRL even outperforms supervised pre-training in learning image representations suitable for object detection <ref type="table" target="#tab_1">(Tables 1 &amp; 6</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">PIRL: Pretext-Invariant Representation Learning</head><p>Our work focuses on pretext tasks for self-supervised learning in which a known image transformation is applied to the input image. For example, the "Jigsaw" task divides the image into nine patches and perturbs the image by randomly permuting the patches <ref type="bibr" target="#b46">[46]</ref>. Prior work used Jigsaw as a pretext task by predicting the permutation from the perturbed input image. This requires the learner to construct a representation that is covariant to the perturbation. The same is true for a range of other pretext tasks that have recently been studied <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b77">76]</ref>. In this work, we adopt the existing Jigsaw pretext task in a way that encourages the image representations to be invariant to the image patch perturbation. While we focus on the Jigsaw pretext task in this paper, our approach is applicable to any pretext task that involves image transformations (see Section 4.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Overview of the Approach</head><p>Suppose we are given an image dataset, D = {I 1 , . . . , I |D| } with I n ∈ R H×W ×3 , and a set of image transformations, T . The set T may contain transformations such as a re-shuffling of patches in the image <ref type="bibr" target="#b46">[46]</ref>, image rotations <ref type="bibr" target="#b17">[18]</ref>, etc. We aim to train a convolutional network, φ θ (·), with parameters θ that constructs image representations v I = φ θ (I) that are invariant to image transformations t ∈ T . We adopt an empirical risk minimization approach to learning the network parameters θ. Specifically, we train the network by minimizing the empirical risk:</p><formula xml:id="formula_1">inv (θ; D) = E t∼p(T ) 1 |D| I∈D L (v I , v I t ) ,<label>(1)</label></formula><p>where p(T ) is some distribution over the transformations in T , and I t denotes image I after application of transformation t, that is, I t = t(I). The function L(·, ·) is a loss function that measures the similarity between two image representations. Minimization of this loss encourages the network φ θ (·) to produce the same representation for image I as for its transformed counterpart I t , i.e., to make representation invariant under transformation t.</p><p>We contrast our loss function to losses <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b77">76</ref>] that aim to learn image representations v I = φ θ (I) that are covariant to image transformations t ∈ T by minimizing:</p><formula xml:id="formula_2">co (θ; D) = E t∼p(T ) 1 |D| I∈D L co (v I , z(t)) ,<label>(2)</label></formula><p>where z is a function that measures some properties of transformation t. Such losses encourage network φ θ (·) to learn image representations that contain information on transformation t, thereby encouraging it to maintain information that is not semantically relevant. Loss function. We implement inv (·) using a contrastive loss function L(·, ·) <ref type="bibr" target="#b21">[22]</ref>. Specifically, we define a matching score, s(·, ·), that measures the similarity of two image representations and use this matching score in a noise contrastive estimator <ref type="bibr" target="#b20">[21]</ref>. In our noise contrastive estimator (NCE), each "positive" sample (I, I t ) has N corresponding "negative" samples. The negative samples are obtained by computing features from other images, I = I. The noise contrastive estimator models the probability of the binary event that (I, I t ) originates from data distribution as:</p><formula xml:id="formula_3">h(v I , v I t ) = exp s(v I ,v I t ) τ exp s(v I ,v I t ) τ + I ∈D N exp s(v I t ,v I ) τ .</formula><p>(3) Herein, D N ⊆ D \ {I} is a set of N negative samples that are drawn uniformly at random from dataset D excluding image I, τ is a temperature parameter, and s(·, ·) is the cosine similarity between the representations.</p><p>In practice, we do not use the convolutional features v directly but apply to different "heads" to the features before computing the score s(·, ·). Specifically, we apply head f (·) on features (v I ) of I and head g(·) on features (v I t ) of I t ; see <ref type="figure" target="#fig_5">Figure 3</ref> and Section 2.3. NCE then amounts to minimizing the following loss:</p><formula xml:id="formula_4">L NCE I, I t = − log [h (f (v I ), g(v I t ))] (4) − I ∈D N log 1 − h g(v t I ), f (v I ) .</formula><p>This loss encourages the representation of image I to be similar to that of its transformed counterpart I t , whilst also encouraging the representation of I t to be dissimilar to that of other images I .    </p><formula xml:id="formula_5">Z 6 / c c X v K g Y w h N U 0 I J d x Z B j S n I 8 E U R Q f F U y D L O Y 4 s v 4 + p 2 p X y 4 x 4 6 T I z 8 W q x L M M z n O S E g S F T k X 9 z n J / G J Y L E o V i g Q U 8 C D M o F n E q P 6 j P 4 u X Q 0 U W T Q J D K j 0 p v w 4 r j E q J r O M f T Z E l K n s M M 8 5 n 8 W i t R G p D g V E u p t x L y E j I 4 r 7 i S b B 4 r 6 Y 2 C V 6 4 3 O j J u H C j n B v y U F V / y N f T w 2 M C C 2 n u a W Y a b q O F v o Z m K 5 B / R a r j F e o 4 h X Z N 6 h u 2 w d h u U N e R 2 x h c N 5 b 8 Z f U N W u / F Y O f / N G P U G W k 1 t 7 n b g t 8 E A t H Y W 9 b 6 F S Y G q D O c C U c j 5 1 P d K M Z O Q C Y I o V s 5 m Z 3 T Y 9 q V p i 7 u v M 4 m b F k y v X L h 1 d v O E h B n n q y z W S C O T 3 6 y Z 5 G 2 1 a S X S t z N J 8 r I S O E f N R W l F X V G 4 Z t r c h D C M B F 3 p A C J G t F Y X L f R M I K F n s v m E Y 2 N H 6 y d v B x f B</formula><formula xml:id="formula_6">v I t &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 8 o Y h A T V Q x n + k C v 7 h S q 1 / x F V v w E s = " &gt; A A A C S H i c b Z B L S 8 N A F I U n 9 V X r q + r S T b A p 1 E 1 J K q L L o h v d V b A P 6 C N M p p N 2 6 O T B z E 2 h h P w 8 N y 7 d + R v c u F D E n Z O 2 Y G 1 7 Y e D w</formula><formula xml:id="formula_7">E j v / M 1 D X W W 7 0 Y k s S w 8 w W z b E 5 L X x X W X B T Q v G p 2 / r X T D 0 j k U R 8 I x 1 K 2 L T O E b o w F M M J p k u t E k o a Y j P C A t p X 0 s U d l N 5 4 G k e h F R f q 6 G w j 1 f N C n d H E i x p 6 U E 8 9 R n e m d c t l L 4 T q v H Y F 7 0 4 2 Z H 0 Z A f T J b 5 E Z c h 0 B P U 9 X 7 T F A C f K I E J o K p W 3 U y x A I T U N n n V A j W 8 p d X R a N S t i 7 L V 4 + V Q v V 2 H k c W n a F z V E I W u k Z V d</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Using a Memory Bank of Negative Samples</head><p>Prior work has found that it is important to use a large number of negatives in the NCE loss of Equation 4 <ref type="bibr" target="#b51">[51,</ref><ref type="bibr" target="#b72">72]</ref>. In a mini-batch SGD optimizer, it is difficult to obtain a large number of negatives without increasing the batch to an infeasibly large size. To address this problem, we follow <ref type="bibr" target="#b72">[72]</ref> and use a memory bank of "cached" features. Concurrent work used a similar memory-bank approach <ref type="bibr" target="#b23">[24]</ref>.</p><p>The memory bank, M, contains a feature representation m I for each image I in dataset D. The representation m I is an exponential moving average of feature representations f (v I ) that were computed in prior epochs. This allows us to replace negative samples, f (v I ), by their memory bank representations, m I , in Equation 4 without having to increase the training batch size. We emphasize that the representations that are stored in the memory bank are all computed on the original images, I, without the transformation t. Final loss function. A potential issue of the loss in Equation 4 is that it does not compare the representations of untransformed images I and I . We address this issue by using a convex combination of two NCE loss functions in inv (·):</p><formula xml:id="formula_8">L I, I t = λL NCE (m I , g(v I t )) +(1 − λ)L NCE (m I , f (v I )).<label>(5)</label></formula><p>Herein, the first term is simply the loss of Equation . Setting λ = 0 in Equation 5 leads to the loss used in <ref type="bibr" target="#b72">[72]</ref>.</p><p>We study the effect of λ on the learned representations in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Implementation Details</head><p>Although PIRL can be used with any pretext task that involves image transformations, we focus on the Jigsaw pretext task <ref type="bibr" target="#b46">[46]</ref> in this paper. To demonstrate that PIRL is more generally applicable, we also experiment with the Rotation pretext task <ref type="bibr" target="#b17">[18]</ref> and with a combination of both tasks in Section 4.3. Below, we describe the implementation details of PIRL with the Jigsaw pretext task. Convolutional network. We use a ResNet-50 (R-50) network architecture in our experiments <ref type="bibr" target="#b24">[25]</ref>. The network is used to compute image representations for both I and I t . These representations are obtained by applying function f (·) or g(·) on features extracted from the the network.</p><p>Specifically, we compute the representation of I, f (v I ), by extracting res5 features, average pooling, and a linear projection to obtain a 128-dimensional representation.</p><p>To compute the representation g(v I t ) of a transformed image I t , we closely follow <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b46">46]</ref>. We: (1) extract nine patches from image I, (2) compute an image representation for each patch separately by extracting activations from the res5 layer of the ResNet-50 and average pool the activations, (3) apply a linear projection to obtain a 128-dimensional patch representations, and (4) concatenate the patch representations in random order and apply a second linear projection on the result to obtain the final 128dimensional image representation, g(v I t ). Our motivation for this design of g(v I t ) is the desire to remain as close as possible to the covariant pretext task of <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b46">46]</ref>. This allows apples-to-apples comparisons between the covariant approach and our invariant approach. Hyperparameters. We implement the memory bank as described in <ref type="bibr" target="#b72">[72]</ref> and use the same hyperparameters for the memory bank. Specifically, we set the temperature in Equation 3 to τ = 0.07, and use a weight of 0.5 to compute the exponential moving averages in the memory bank. Unless stated otherwise, we use λ = 0.5 in Equation 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head><p>Following common practice in self-supervised learning <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b79">78]</ref>, we evaluate the performance of PIRL in transfer-learning experiments. We perform experiments on a variety of datasets, focusing on object detection and image classification tasks. Our empirical evaluations cover: (1) a learning setting in which the parameters of the convolutional network are finetuned during transfer, thus evaluating the network "initialization" obtained using self-supervised learning and (2) a learning setting in which the parameters of the network are fixed during transfer learning, thus using the network as a feature extractor. Code reproducing the results of our experiments will be published online. Baselines. Our most important baseline is the Jigsaw ResNet-50 model of <ref type="bibr" target="#b18">[19]</ref>. This baseline implements the co-  variant counterpart of our PIRL approach with the Jigsaw pretext task. We also compare PIRL to a range of other selfsupervised methods. An important comparison is to NPID <ref type="bibr" target="#b72">[72]</ref>. NPID is a special case of PIRL: setting λ = 0 in Equation 5 leads to the loss function of NPID. We found it is possible to improve the original implementation of NPID by using more negative samples and training for more epochs (see <ref type="bibr">Section 4)</ref>. We refer to our improved version of NPID as NPID++. Comparisons between PIRL and NPID++ allow us to study the effect of the pretext-invariance that PIRL aims to achieve, i.e., the effect of using λ &gt; 0 in Equation 5. Pre-training data. To facilitate comparisons with prior work, we use the 1.28M images from the ImageNet <ref type="bibr" target="#b58">[58]</ref> train split (without labels) to pre-train our models. Training details. We train our models using mini-batch SGD using the cosine learning rate decay <ref type="bibr" target="#b40">[40]</ref> scheme with an initial learning rate of 1.2×10 −1 and a final learning rate of 1.2 × 10 −4 . We train the models for 800 epochs using a batch size of 1, 024 images and using N = 32, 000 negative samples in Equation 3. We do not use data-augmentation approaches such as Fast AutoAugment <ref type="bibr" target="#b38">[38]</ref> because they are the result of supervised-learning approaches. We provide a full overview of all hyperparameter settings that were used in the supplemental material. Transfer learning. Prior work suggests that the hyperparameters used in transfer learning can play an important role in the evaluation pre-trained representations <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b79">78]</ref>. To facilitate fair comparisons with prior work, we closely follow the transfer-learning setup described in <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b79">78]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Object Detection</head><p>Following prior work <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b72">72]</ref>, we perform objectdetection experiments on the the Pascal VOC dataset <ref type="bibr" target="#b14">[15]</ref> using the VOC07+12 train split. We use the Faster R-CNN <ref type="bibr" target="#b56">[56]</ref> C4 object-detection model implemented in De-tectron2 <ref type="bibr" target="#b71">[71]</ref> with a ResNet-50 (R-50) backbone. We pretrain the ResNet-50 using PIRL to initialize the detection model before finetuning it on the VOC training data. We use the same training schedule as <ref type="bibr" target="#b18">[19]</ref> for all models finetuned on VOC and follow <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b71">71]</ref> to keep the BatchNorm parameters fixed during finetuning. We evaluate object-detection performance in terms of AP all , AP 50 , and AP 75 <ref type="bibr" target="#b39">[39]</ref>.</p><p>The results of our detection experiments are presented in <ref type="table" target="#tab_1">Table 1</ref>. The results demonstrate the strong performance of PIRL: it outperforms all alternative self-supervised learnings in terms of all three AP measures. Compared to pretraining on the Jigsaw pretext task, PIRL achieves AP improvements of 5 points. These results underscore the importance of learning invariant (rather than covariant) image representations. PIRL also outperforms NPID++, which demonstrates the benefits of learning pretext invariance.</p><p>Interestingly, PIRL even outperforms the supervised ImageNet-pretrained model in terms of the more conservative AP all and AP 75 metrics. Similar to concurrent work <ref type="bibr" target="#b23">[24]</ref>, we find that a self-supervised learner can outperform supervised pre-training for object detection. We emphasize that PIRL achieves this result using the same backbone model, the same number of finetuning epochs, and the exact same pre-training data (but without the labels). This result is a substantial improvement over prior self-supervised approaches that obtain slightly worse performance than fully supervised baselines despite using orders of magnitude more curated training data <ref type="bibr" target="#b18">[19]</ref> or much larger backbone models <ref type="bibr" target="#b25">[26]</ref>. In <ref type="table">Table 6</ref>, we show that PIRL also outperforms supervised pretraining when finetuning is done on the much smaller VOC07 train+val set. This suggests that PIRL learns image representations that are amenable to sample-efficient supervised learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Image Classification with Linear Models</head><p>Next, we assess the quality of image representations by training linear classifiers on fixed image representations. We follow the evaluation setup from <ref type="bibr" target="#b18">[19]</ref> and measure the performance of such classifiers on four image-classification datasets: ImageNet <ref type="bibr" target="#b58">[58]</ref>, VOC07 <ref type="bibr" target="#b14">[15]</ref>, Places205 <ref type="bibr" target="#b80">[79]</ref>, and iNaturalist2018 <ref type="bibr" target="#b65">[65]</ref>. These datasets involve diverse tasks such as object classification, scene recognition and finegrained recognition. Following <ref type="bibr" target="#b18">[19]</ref>, we evaluate representations extracted from all intermediate layers of the pretrained network, and report the image-classification results for the best-performing layer in <ref type="table">Table 2</ref>. ImageNet results. The results on ImageNet highlight the benefits of learning invariant features: PIRL improves recognition accuracies by over 15% compared to its covariant counterpart, Jigsaw. PIRL achieves the highest singlecrop top-1 accuracy of all self-supervised learners that use a single ResNet-50 model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameters</head><p>Transfer Dataset ImageNet VOC07 Places205 iNat.</p><p>ResNet-50 using evaluation setup of <ref type="bibr">[</ref>  <ref type="table">Table 2</ref>: Image classification with linear models. Image-classification performance on four datasets using the setup of <ref type="bibr" target="#b18">[19]</ref>. We train linear classifiers on image representations obtained by self-supervised learners that were pre-trained on ImageNet (without labels). We report the performance for the best-performing layer for each method. We measure mean average precision (mAP) on the VOC07 dataset and top-1 accuracy on all other datasets. Numbers for PIRL, NPID++, Rotation were obtained by us; the other numbers were adopted from their respective papers. Numbers with † were measured using 10-crop evaluation. The best-performing self-supervised learner on each dataset is boldfaced.</p><p>The benefits of pretext invariance are further highlighted by comparing PIRL with NPID. Our re-implementation of NPID (called NPID++) substantially outperforms the results reported in <ref type="bibr" target="#b72">[72]</ref>. Specifically, NPID++ achieves a single-crop top-1 accuracy of 59%, which is higher or on par with existing work that uses a single ResNet-50. Yet, PIRL substantially outperforms NPID++. We note that PIRL also outperforms concurrent work <ref type="bibr" target="#b23">[24]</ref> in this setting.</p><p>Akin to prior approaches, the performance of PIRL improves with network size. For example, CMC <ref type="bibr" target="#b64">[64]</ref> uses a combination of two ResNet-50 models and trains the linear classifier for longer to obtain 64.1% accuracy. We performed an experiment in which we did the same for PIRL, and obtained a top-1 accuracy of 65.7%; see "PIRL-ens." in <ref type="figure" target="#fig_1">Figure 2</ref>. To compare PIRL with larger models, we also performed experiments in which we followed <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b76">75]</ref> by doubling the number of channels in ResNet-50; see "PIRL-c2x" in <ref type="figure" target="#fig_1">Figure 2</ref>. PIRL-c2x achieves a top-1 accuracy of 67.4%, which is close to the accuracy obtained by AMDIM <ref type="bibr" target="#b3">[4]</ref> with a model that has 6× more parameters.</p><p>Altogether, the results in <ref type="figure" target="#fig_1">Figure 2</ref> demonstrate that PIRL outperforms all prior self-supervised learners on ImageNet in terms of the trade-off between model accuracy and size. Indeed, PIRL even outperforms most self-supervised learn-  <ref type="bibr" target="#b25">[26]</ref> R-170 and R-11 64.0 84.9 <ref type="table">Table 3</ref>: Semi-supervised learning on ImageNet. Single-crop top-5 accuracy on the ImageNet validation set of self-supervised models that are finetuned on 1% and 10% of the ImageNet training data, following <ref type="bibr" target="#b72">[72]</ref>. All numbers except for Jigsaw, NPID++ and PIRL are adopted from the corresponding papers. Best performance is boldfaced.</p><p>ers that use much larger models <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b51">51]</ref>.</p><p>Results on other datasets. The results on the other imageclassification datasets in <ref type="table">Table 2</ref> are in line with the results on ImageNet: PIRL substantially outperforms its covariant counterpart (Jigsaw). The performance of PIRL is within 2% of fully supervised representations on Places205, and improves the previous best results of <ref type="bibr" target="#b18">[19]</ref> on VOC07 by more than 16 AP points. On the challenging iNaturalist dataset, which has over 8, 000 classes, we obtain a gain of 11% in top-1 accuracy over the prior best result <ref type="bibr" target="#b17">[18]</ref>. We observe that the NPID++ baseline performs well on these three datasets but is consistently outperformed by PIRL. Indeed, PIRL sets a new state-of-the-art for selfsupervised representations in this learning setting on the VOC07, Places205, and iNaturalist datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Semi-Supervised Image Classification</head><p>We perform semi-supervised image classification experiments on ImageNet following the experimental setup of <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b72">72,</ref><ref type="bibr" target="#b76">75]</ref>. Specifically, we randomly select 1% and 10% of the ImageNet training data (with labels). We finetune our models on these training-data subsets following the procedure of <ref type="bibr" target="#b72">[72]</ref>. <ref type="table">Table 3</ref> reports the top-5 accuracy of the resulting models on the ImageNet validation set.</p><p>The results further highlight the quality of the image representations learned by PIRL: finetuning the models on just 1% (∼13,000) labeled images leads to a top-5 accuracy of 57%. PIRL performs at least as well as S 4 L [75] and better than VAT <ref type="bibr" target="#b19">[20]</ref>, which are both methods specifically designed for semi-supervised learning. In line with earlier results, PIRL also outperforms Jigsaw and NPID++.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Pre-Training on Uncurated Image Data</head><p>Most representation learning methods are sensitive to the data distribution used during pre-training <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b62">62]</ref>.</p><p>To study how much changes in the data distribution im-  pact PIRL, we pre-train models on uncurated images from the unlabeled YFCC dataset <ref type="bibr" target="#b63">[63]</ref>. Following <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19]</ref>, we randomly select a subset of 1 million images (YFCC-1M) from the 100 million images in YFCC. We pre-train PIRL ResNet-50 networks on YFCC-1M using the same procedure that was used for ImageNet pre-training. We evaluate using the setup in Section 3.2 by training linear classifiers on fixed image representations. <ref type="table" target="#tab_5">Table 4</ref> reports the top-1 accuracy of the resulting classifiers. In line with prior results, PIRL outperforms competing self-supervised learners. In fact, PIRL even outperforms Jigsaw and DeeperCluster models that were trained on 100× more data from the same distribution. Comparing pre-training on ImageNet <ref type="table">(Table 2)</ref> with pre-training YFCC-1M <ref type="table" target="#tab_5">(Table 4</ref>) leads to a mixed set of observations. On ImageNet classification, pre-training (without labels) on ImageNet works substantially better than pre-training on YFCC-1M. In line with prior work <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b29">30]</ref>, however, pretraining on YFCC-1M leads to better representations for image classification on the Places205 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Analysis</head><p>We performed a set of experiments aimed at better understanding the properties of PIRL. To make it feasible to train the larger number of models needed for these experiments, we train the models we study in this section for fewer epochs (400) and with fewer negatives (N = 4, 096) than in Section 3. As a result, we obtain lower absolute performances. Apart from that, we did not change the experimental setup or any of the other hyperparameters. Throughout the section, we use the evaluation setup from Section 3.2 that trains linear classifiers on fixed image representations to measure the quality of image representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Analyzing PIRL Representations</head><p>Does PIRL learn invariant representations? PIRL was designed to learn representations that are invariant to image transformation t ∈ T . We analyzed whether the learned representations actually have the desired invari-  ance properties. Specifically, we normalize the representations to have unit norm and compute l 2 distances between the (normalized) representation of image, f (v I ), and the (normalized) representation its transformed version, g(v I t ).</p><p>We repeat this for all transforms t ∈ T and for a large set of images. We plot histograms of the distances thus obtained in <ref type="figure" target="#fig_8">Figure 4</ref>. The figure shows that, for PIRL, an image representation and the representation of a transformed version of that image are generally similar. This suggests that PIRL has learned representations that are invariant to the transformations. By contrast, the distances between Jigsaw representations have a much larger mean and variance, which suggests that Jigsaw representations covary with the image transformations that were applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Which layer produces the best representations?</head><p>All prior experiments used PIRL representations that were extracted from the res5 layer and Jigsaw representations that were extracted from the res4 layer (which work better for Jigsaw). <ref type="figure">Figure 5</ref> studies the quality of representations in earlier layers of the convolutional networks. The figure reveals that the quality of Jigsaw representations improves from the conv1 to the res4 layer but that their quality sharply decreases in the res5 layer. We surmise this happens because the res5 representations in the last layer of the network covary with the image transformation t and are not encouraged to contain semantic information. By contrast, PIRL representations are invariant to image transformations, which allows them to focus on modeling semantic information. As a result, the best image representations are extracted from the res5 layer of PIRL-trained networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Analyzing the PIRL Loss Function</head><p>What is the effect of λ in the PIRL loss function?</p><p>The PIRL loss function in Equation 5 contains a hyperparameter λ that trades off between two NCE losses. All prior  experiments were performed with λ = 0.5. NPID(++) <ref type="bibr" target="#b72">[72]</ref> is a special case of PIRL in which λ = 0, effectively removing the pretext-invariance term from the loss. At λ = 1, the network does not compare untransformed images at training time and updates to the memory bank m I are not dampened.</p><p>We study the effect of λ on the quality of PIRL representations. As before, we measure representation quality by the top-1 accuracy of linear classifiers operating on fixed ImageNet representations. <ref type="figure">Figure 6</ref> shows the results of these experiments. The results show that the performance of PIRL is quite sensitive to the setting of λ, and that the best performance if obtained by setting λ = 0.5.</p><p>What is the effect of the number of image transforms? Both in PIRL and Jigsaw, it is possible to vary the complexity of the task by varying the number of permutations of the nine image patches that are included in the set of image transformations, T . Prior work on Jigsaw suggests that increasing the number of possible patch permutations leads to better performance <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b46">46]</ref>. However, the largest value |T | can take is restricted because the number of learnable parameters in the output layer grows linearly with the number 100 2,000 10,000 3.6M  of patch permutations in models trained to solve the Jigsaw task. This problem does not apply to PIRL because it never outputs the patch permutations, and thus has a fixed number of model parameters. As a result, PIRL can use all 9! ≈ 3.6 million permutations in T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head># of permutations of patches</head><p>We study the quality of PIRL and Jigsaw as a function of the number of patch permutations included in T . To facilitate comparison with <ref type="bibr" target="#b18">[19]</ref>, we measure quality in terms of performance of linear models on image classification using the VOC07 dataset, following the same setup as in Section 3.2. The results of these experiments are presented in <ref type="figure" target="#fig_12">Figure 7</ref>. The results show that PIRL outperforms Jigsaw for all cardinalities of T but that PIRL particularly benefits from being able to use very large numbers of image transformations (i.e., large |T |) during training.</p><p>What is the effect of the number of negative samples?</p><p>We study the effect of the number of negative samples, N , on the quality of the learned image representations. We measure the accuracy of linear ImageNet classifiers on fixed representations produced by PIRL as a function of the value of N used in pre-training. The results of these experiments are presented in <ref type="figure" target="#fig_13">Figure 8</ref>. They suggest that increasing the number of negatives tends to have a positive influence on the quality of the image representations constructed by PIRL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Generalizing PIRL to Other Pretext Tasks</head><p>Although we studied PIRL in the context of Jigsaw in this paper, PIRL can be used with any set of image transformations, T . We performed an experiment evaluating the performance of PIRL using the Rotation pretext task <ref type="bibr" target="#b17">[18]</ref>. We define T to contain image rotations by {0 • , 90 • , 180 • , 270 • }, and measure representation quality in terms of image-classification accuracy of linear models (see the supplemental material for details).</p><p>The results of these experiments are presented in Top-1 Accuracy (Rotation) outperform those trained using the Rotation pretext task of <ref type="bibr" target="#b17">[18]</ref>. The performance gains obtained from learning a rotation-invariant representation are substantial, e.g. +11% top-1 accuracy on ImageNet. We also note that PIRL (Rotation) outperforms NPID++ (see <ref type="table">Table 2</ref>). In a second set of experiments, we combined the pretext image transforms from both the Jigsaw and Rotation tasks in the set of image transformations, T . Specifically, we obtain I t by first applying a rotation and then performing a Jigsaw transformation. The results of these experiments are shown in <ref type="table" target="#tab_6">Table 5</ref> (bottom). The results demonstrate that combining image transforms from multiple pretext tasks can further improve image representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Related Work</head><p>Our study is related to prior work that tries to learn characteristics of the image distribution without considering a corresponding (image-conditional) label distribution. A variety of work has studied reconstructing images from a small, intermediate representation, e.g., using sparse coding <ref type="bibr" target="#b50">[50]</ref>, adversarial training <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b43">43]</ref>, autoencoders <ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b55">55,</ref><ref type="bibr" target="#b67">67]</ref>, or probabilistic versions thereof <ref type="bibr" target="#b59">[59]</ref>.</p><p>More recently, interest has shifted to specifying pretext tasks <ref type="bibr" target="#b8">[9]</ref> that require modeling a more limited set of properties of the data distribution. For video data, these pretext tasks learn representations by ordering video frames <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b70">70,</ref><ref type="bibr" target="#b74">74]</ref>, tracking <ref type="bibr" target="#b54">[54,</ref><ref type="bibr" target="#b68">68]</ref>, or using cross-modal signals like audio <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b53">53]</ref>.</p><p>Our work focuses on image-based pretext tasks. Prior pretext tasks include image colorization <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b78">77,</ref><ref type="bibr" target="#b79">78]</ref>, orientation prediction <ref type="bibr" target="#b17">[18]</ref>, affine transform prediction <ref type="bibr" target="#b77">[76]</ref>, predicting contextual image patches <ref type="bibr" target="#b8">[9]</ref>, reordering image patches <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b48">48]</ref>, counting visual primitives <ref type="bibr" target="#b47">[47]</ref>, or their combinations <ref type="bibr" target="#b9">[10]</ref>. In contrast, our works learns image representations that are invariant to the image transformations rather than covariant.</p><p>PIRL is related to approaches that learn invariant image  representations via contrastive learning <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b60">60,</ref><ref type="bibr" target="#b68">68,</ref><ref type="bibr" target="#b72">72]</ref>, clustering <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b69">69]</ref>, or maximizing mutual information <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b28">29]</ref>. PIRL is most similar to methods that learn representations that are invariant under standard data augmentation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b72">72,</ref><ref type="bibr" target="#b73">73]</ref>. PIRL learns representations that are invariant to both the data augmentation and to the pretext image transformations. Finally, PIRL is also related to approaches that use a contrastive loss <ref type="bibr" target="#b21">[22]</ref> in predictive learning <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b61">61,</ref><ref type="bibr" target="#b64">64]</ref>. These prior approaches predict missing parts of the data, e.g., future frames in videos <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b51">51]</ref>, or operate on multiple views <ref type="bibr" target="#b64">[64]</ref>. In contrast to those approaches, PIRL learns invariances rather than predicting missing data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion and Conclusion</head><p>We studied Pretext-Invariant Representation Learning (PIRL) for learning representations that are invariant to image transformations applied in self-supervised pretext tasks. The rationale behind PIRL is that invariance to image transformations maintains semantic information in the representation. We obtain state-of-the-art results on multiple benchmarks for self-supervised learning in image classification and object detection. PIRL even outperforms supervised ImageNet pre-training on object detection.</p><p>In this paper, we used PIRL with the Jigsaw and Rotation image transformations. In future work, we aim to extend to richer sets of transformations. We also plan to investigate combinations of PIRL with clustering-based approaches <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>. Like PIRL, those approaches use inter-image statistics but they do so in a different way. A combination of the two approaches may lead to even better image representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplemental Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Training architecture and Hyperparameters</head><p>Base Architecture for PIRL PIRL models in Section 3, 4 and 5 in the main paper are standard ResNet-50 <ref type="bibr" target="#b24">[25]</ref> models with 25.6 million parameters. <ref type="figure" target="#fig_1">Figure 2</ref> and Section 3.2 also use a larger model 'PIRL-c2x' which has double the number of channels in each 'stage' of the ResNet, e.g., 128 channels in the first convolutional layer, 4096 channels in the final res5 layer etc., and a total of 98 million parameters. The heads f (·) and g(·) are linear layers that are used for the pre-training stage <ref type="figure" target="#fig_5">(Figure 3)</ref>. When evaluating the model using transfer learning (Section 4 and 5 of the main paper), the heads f , g are removed.</p><p>Training Hyperparameters for Section 3 PIRL models in Section 3 are trained for 800 epochs using the ImageNet training set (1.28 million images). We use a batchsize of 32 per GPU and use a total of 32 GPUs to train each model. We optimize the models using mini-batch SGD with the cosine learning rate decay <ref type="bibr" target="#b40">[40]</ref> scheme with an initial learning rate of 1.2 × 10 −1 and a final learning rate of 1.2 × 10 −4 , momentum of 0.9 and a weight decay of 10 −4 . We use a total of 32, 000 negatives to compute the NCE loss (Equation 5) and the negatives are sampled randomly for each data point in the batch.</p><p>Training Hyperparameters for Section 4 The hyperparameters used are exactly the same as Section 3, except that the models are trained with 4096 negatives and for 400 epochs only. This results in a lower absolute performance, but the main focus of Section 4 is to analyze PIRL.</p><p>Common Data Pre-processing We use data augmentations as implemented in PyTorch and the Python Imaging Library. These data augmentations are used for all methods including PIRL and NPID++. We do not use supervised policies like Fast AutoAugment. Our 'geometric' data pre-processing involves extracting a randomly resized crop from the image, and horizontally flipping it. We follow this by 'photometric' pre-processing that alter the RGB color values by using random values of color jitter, contrast, brightness, saturation, sharpness, equalize etc. transforms to the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Details for PIRL with Jigsaw</head><p>Data pre-processing We base our Jigsaw implementation on <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b32">33]</ref>. To construct I t , we extract a random resized crop that occupies at least 60% of the image. We resize this crop to 255 × 255, and then divide into a 3 × 3 grid. We extract a patch of 64 × 64 from each of these grids and get 9</p><p>patches. We apply photometric data augmentation (random color jitter, hue, contrast, saturation) to each patch independently and finally obtain the 9 patches that constitute I t .</p><p>The image I is a 224×224 image obtained by applying standard data augmentation (flip, random resized crop, color jitter, hue, contrast, saturation) to the image from the dataset.</p><p>Architecture The 9 patches of I t are individually input to the network to get their res5 features which are average pooled to get a single 2048 dimensional vector for each patch. We then apply a linear projection to these features to get a 128 dimensional vector for each patch. These patch features are concatenated to get a 1152 dimensional vector which is then input to another single linear layer to get the final 128 dimensional feature v I t .</p><p>We feed forward the image I and average pool its res5 feature to get a 2048 dimensional vector for the image. We then apply a single linear projection to get a 128 dimensional feature v I .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Details for PIRL with Rotation</head><p>Data pre-processing We base our Rotation implementation on <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b32">33]</ref>. To construct I t we use the standard data augmentation described earlier (geometric + photometric) to get a 224 × 224 image, followed by a random rotation from {0 • , 90 • , 180 • , 270 • }. The image I is a 224×224 image obtained by applying standard data augmentation (flip, random resized crop, color jitter, hue, contrast, saturation) to the image from the dataset.</p><p>Architecture We feed forward the image I t , average pool the res5 feature, and apply a single linear layer to get the final 128 dimensional feature for v I t . We feed forward the image I and average pool its res5 feature to get a 2048 dimensional vector for the image. We then apply a single linear projection to get a 128 dimensional feature v I .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Object Detection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. VOC07 train+val set for detection</head><p>In Section 3.1 of the main paper, we presented object detection results using the VOC07+12 training set which has 16K images. In this section, we use the smaller VOC07 train+val set (5K images) for finetuning the Faster R-CNN C4 detection models. All models are finetuned using the Detectron2 <ref type="bibr" target="#b71">[71]</ref> codebase and hyperparameters from <ref type="bibr" target="#b18">[19]</ref>. We report the detection AP in <ref type="table">Table 6</ref>. We see that the PIRL model outperforms the ImageNet supervised pretrained models on the stricter AP <ref type="bibr" target="#b76">75</ref> and AP all metrics without extra pretraining data or changes to the network architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Detection hyperparameters:</head><p>We use a batchsize of 2 images per GPU, a total of 8 GPUs and finetune models for 12.5K iterations with the learning rate dropped by 0.1 at 9.5K iterations. The supervised and Jigsaw baseline models use an initial learning rate of 0.02 with a linear warmup for 100 iterations with a slope of 1/3, while the NPID++ and PIRL models use an initial learning rate of 0.003 with a linear warmup for 1000 iterations and a slope of 1/3. We keep the BatchNorm parameters of all models fixed during the detection finetuning stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Network  <ref type="table">Table 6</ref>: Object detection on VOC07 using Faster R-CNN. Detection AP on the VOC07 test set after finetuning Faster R-CNN models with a ResNet-50 backbone pre-trained using self-supervised learning on Im-ageNet. Results for supervised ImageNet pre-training are presented for reference. All methods are finetuned using the images from the VOC07 train+val set. Numbers with * are adopted from the corresponding papers. We see that PIRL outperforms supervised pre-training without extra pretraining data or changes in the network architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. VOC07+12 train set for detection</head><p>In <ref type="table" target="#tab_1">Table 1</ref>, we use the VOC07+12 training split and the VOC07 test set as used in prior work <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b51">51]</ref>. Detection hyperparamters: We use a batchsize of 2 images per GPU, a total of 8 GPUs and finetune models for 25K iterations with the learning rate dropped by 0.1 at 17K iterations. The supervised and Jigsaw baseline models use an initial learning rate of 0.02 with a linear warmup for 100 iterations with a slope of 1/3, while the NPID++ and PIRL models use an initial learning rate of 0.003 with a linear warmup for 1000 iterations and a slope of 1/3. We keep the BatchNorm parameters of all models fixed during the detection finetuning stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Linear Models for Transfer</head><p>We train linear models on representations from the intermediate layers of a ResNet-50 model following the procedure outlined in <ref type="bibr" target="#b18">[19]</ref>. We briefly outline the hyperparameters from their work. The features from each of the layers are average pooled such that they are of about 9, 000 dimensions each. The linear model is trained with mini-batch SGD using a learning rate of 0.01 decayed by 0.1 at two equally spaced intervals, momentum of 0.9 and weight decay of 5×10 −4 . We train the linear models for 28 epochs on ImageNet (1.28M training images), 14 epochs on Places205 (2.4M training images) and for 84 epochs on iNaturalist-2018 (437K training images). Thus, the number of parameter updates for training the linear models are roughly constant across all these datasets. We report the center crop top-1 accuracy for the ImageNet, Places205 and iNaturalist-2018 datasets in <ref type="table">Table 2</ref>.</p><p>For training linear models on VOC07, we train linear SVMs following the procedure in <ref type="bibr" target="#b18">[19]</ref> and report mean average Precision (mAP).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Per layer results</head><p>The results of all these models are in <ref type="table">Table 7</ref>.  <ref type="table">Table 7</ref>: ResNet-50 linear evaluation on ImageNet and Places205 datasets: We report the performance of all the layers following the evaluation protocol in <ref type="bibr" target="#b18">[19]</ref>. All numbers except NPID++ and Ours are from their respective papers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ImageNet</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " b 9 D b 7 5 l e V u t s W G y j r h n + y R c I S j 8 = " &gt; A A A B 9 X i c b V D L S g M x F M 3 4 r P V V d e k m 2 A q u y k w V H 7 u i G 9 1 V s A 9 o p y W T Z t r Q T G Z I 7 i h l 6 H + 4 c a G I W / / F n X 9 j Z l p E r Q c C h 3 P u 5 Z 4 c L x J c g 2 1 / W g u L S 8 s r q 7 m 1 / P r G 5 t Z 2 Y W e 3 o c N Y U V a n o Q h V y y O a C S 5 Z H T g I 1 o o U I 4 E n W N M b X a V + 8 5 4 p z U N 5 B + O I u Q E Z S O 5 z S s B I 3 V I n I D D 0 / O R m 0 o V S r 1 C 0 y 3 Y G P E + c G S m i G W q 9 w k e n H 9 I 4 Y B K o I F q 3 H T s C N y E K O B V s k u / E m k W E j s i A t Q 2 V J G D a T b L U E 3 x o l D 7 2 Q 2 W e B J y p P z c S E m g 9 D j w z m Y b U f 7 1 U / M 9 r x + C f u w m X U Q x M 0 u k h P x Y Y Q p x W g P t c M Q p i b A i h i p u s m A 6 J I h R M U f m s h I s U p 9 9 f n i e N S t k 5 L p / c V o r V y 1 k d O b S P D t A R c t A Z q q J r V E N 1 R J F C j + g Z v V g P 1 p P 1 a r 1 N R x e s 2 c 4 e + g X r / Q s c 8 p J l &lt; / l a t e x i t &gt; I &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " z G 0 + 8 6 A C K s 0 Z i v m N X L / 5 7 l d l v G 0 = " &gt; A A A B 8 3 i c b V D L S s N A F L 2 p r 1 p f V Z d u B l v B V U m r + N g V 3 e i u g n 1 A E 8 p k O m m H T i Z h Z i K U 0 N 9 w 4 0 I R t / 6 M O / / G S R p E r Q c G D u f c y z 1 z v I g z p W 3 7 0 y o s L a + s r h X X S x u b W 9 s 7 5 d 2 9 j g p j S W i b h D y U P Q 8 r y p m g b c 0 0 p 7 1 I U h x 4 n H a 9 y X X q d x + o V C w U 9 3 o a U T f A I 8 F 8 R r A 2 k l N 1 A q z H n p / c z q q D c s W u 2 R n Q I q n n p A I 5 W o P y h z M M S R x Q o Q n H S v X r d q T d B E v N C K e z k h M r G m E y w S P a N 1 T g g C o 3 y T L P 0 J F R h s g P p X l C o 0 z 9 u Z H g Q K l p 4 J n J N K L 6 6 6X i f 1 4 / 1 v 6 F m z A R x Z o K M j / k x x z p E K U F o C G T l G g + N Q Q T y U x W R M Z Y Y q J N T a W s h M s U Z 9 9 f X i S d R q 1 + U j u 9 a 1 S a V 3 k d R T i A Q z i G O p x D E 2 6 g B W 0 g E M E j P M O L F V t P 1 q v 1 N h 8 t W P n O P vy C 9 f 4 F i w C R f w = = &lt; / l a t e x i t &gt; I t &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " b 9 D b 7 5 l e V u t s W G y j r h n + y R c I S j 8 = " &gt; A A A B 9 X i c b V D L S g M x F M 3 4 r P V V d e k m 2 A q u y k w V H 7 u i G 9 1 V s A 9 o p y W T Z t r Q T G Z I 7 i h l 6 H + 4 c a G I W / / F n X 9 j Z l p E r Q c C h 3 P u 5 Z 4 c L x J c g 2 1 / W g u L S 8 s r q 7 m 1 / P r G 5 t Z 2 Y W e 3 o c N Y U V a n o Q h V y y O a C S 5 Z H T g I 1 o o U I 4 E n W N M b X a V + 8 5 4 p z U N 5 B + O I u Q E Z S O 5 z S s B I 3 V I n I D D 0 / O R m 0 o V S r 1 C 0 y 3 Y G P E + c G S m i G W q 9 w k e n H 9 I 4 Y B K o I F q 3 H T s C N y E K O B V s k u / E m k W E j s i A t Q 2 V J G D a T b L U E 3 x o l D 7 2 Q 2 W e B J y p P z c S E m g 9 D j w z m Y b U f 7 1 U / M 9 r x + C f u w m X U Q x M 0 u k h P x Y Y Q p x W g P t c M Q p i b A i h i p u s m A 6 J I h R M U f m s h I s U p 9 9 f n i e N S t k 5 L p / c V o r V y 1 k d O b S P D t A R c t A Z q q J r V E N 1 R J F C j + g Z v V g P 1 p P 1 a r 1 N R x e s 2 c 4 e + g X r / Q s c 8 p J l &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4 = " b 9 D b 7 5 l e V u t s W G y j r h n + y R c I S j 8 = " &gt; A A A B 9 X i c b V D L S g M x F M 3 4 r P V V d e k m 2 A q u y k w V H 7 u i G 9 1 V s A 9 o p y W T Z t r Q T G Z I 7 i h l 6 H + 4 c a G I W / / F n X 9 j Z l p E r Q c C h 3 P u 5 Z 4 c L x J c g 2 1 / W g u L S 8 s r q 7 m 1 / P r G 5 t Z 2 Y W e 3 o c N Y U V a n o Q h V y y O a C S 5 Z H T g I 1 o o U I 4 E n W N M b X a V + 8 5 4 p z U N 5 B + O I u Q E Z S O 5 z S s B I 3 V I n I D D 0 / O R m 0 o V S r 1 C 0 y 3 Y G P E + c G S m i G W q 9 w k e n H 9 I 4 Y B K o I F q 3 H T s C N y E K O B V s k u / E m k W E j s i A t Q 2 V J G D a T b L U E 3 x o l D 7 2 Q 2 W e B J y p P z c S E m g 9 D j w z m Y b U f 7 1 U / M 9 r x + C f u w m X U Q x M 0 u k h P x Y Y Q p x W g P t c M Q p i b A i h i p u s m A 6 J I h R M U f m s h I s U p 9 9 f n i e N S t k 5 L p / c V o r V y 1 k d O b S P D t A R c t A Z q q J r V E N 1 R J F C j + g Z v V g P 1 p P 1 a r 1 N R x e s 2 c 4 e + g X r / Q s c 8 p J l &lt; / l a t e x i t &gt; I &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " z G 0 + 8 6 A C K s 0 Z i v m N X L / 5 7 l d l v G 0 = " &gt; A A A B 8 3 i c b V D L S s N A F L 2 p r 1 p f V Z d u B l v B V U m r + N g V 3 e i u g n 1 A E 8 p k O m m H T i Z h Z i K U 0 N 9 w 4 0 I R t / 6 M O / / G S R p E r Q c G D u f c y z 1 z v I g z p W 3 7 0 y o s L a + s r h X X S x u b W 9 s 7 5 d 2 9 j g p j S W i b h D y U P Q 8 r y p m g b c 0 0 p 7 1 I U h x 4 n H a 9 y X X q d x + o V C w U 9 3 o a U T f A I 8 F 8 R r A 2 k l N 1 A q z H n p / c z q q D c s W u 2 R n Q I q n n p A I 5 W o P y h z M M S R x Q o Q n H S v X r d q T d B E v N C K e z k h M r G m E y w S P a N 1 T g g C o 3 y T L P 0 J F R h s g P p X l C o 0 z 9 u Z H g Q K l p 4 J n J N K L 6 6 6 X i f 1 4 / 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>ImageNet classification with linear models. Single-crop top-1 accuracy on the ImageNet validation data as a function of the number of parameters in the model that produces the representation ("A" represents AlexNet). Pretext-Invariant Representation Learning (PIRL) sets a new state-of-the-art in this setting (red marker) and uses significantly smaller models (ResNet-50). See Section 3.2 for more details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>I &lt; l a t e x i t s h a 1 _ b a s e 6 4 =</head><label>14</label><figDesc>" z G 0 + 8 6 A C K s 0 Z i v m N X L / 5 7 l d l v G 0 = " &gt; A A A B 8 3 i c b V D L S s N A F L 2 p r 1 p f V Z d u B l v B V U m r + N g V 3 e i u g n 1 A E 8 p k O m m H T i Z h Z i K U 0 N 9 w 4 0 I R t / 6 M O / / G S R p E r Q c G D u f c y z 1 z v I g z p W 3 7 0 y o s L a + s r h X X S x u b W 9 s 7 5 d 2 9 j g p j S W i b h D y U P Q 8 r y p m g b c 0 0 p 7 1 I U h x 4 n H a 9 y X X q d x + o V C w U 9 3 o a U T f A I 8 F 8 R r A 2 k l N 1 A q z H n p / c z q q D c s W u 2 R n Q I q n n p A I 5 W o P y h z M M S R x Q o Q n H S v X r d q T d B E v N C K e z k h M r G m E y w S P a N 1 T g g C o 3 y T L P 0 J F R h s g P p X l C o 0 z 9 u Z H g Q K l p 4 J n J N K L 6 6 6 X i f 1 4 / 1 v 6 F m z A R x Z o K M j / k x x z p E K U F o C G T l G g + N Q Q T y U x W R M Z Y Y q J N T a W s h M s U Z 9 9 f X i S d R q 1 + U j u 9 a 1 S a V 3 k d R T i A Q z i G O p x D E 2 6 g B W 0 g E M E j P M O L F V t P 1 q v 1 N h 8 t W P n O P v y C 9 f 4 F i w C R f w = = &lt; / l a t e x i t &gt; M &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T n 5 i j Q g O n A y 4 m P i B G 3 g Q M h v S F H 4 = " &gt; A A A C E 3 i c b V D L S s N A F J 3 U V 6 2 v q E s 3 w a Z Q X Z S k i o 9 d 0 Y 0 u h A r 2 A U 0 t k + m k H T p 5 M H M j l N B / c O O v u H G h i F s 3 7 v w b k z S I W g 8 M n D n n X u 6 9 x w 4 4 k 2 A Y n 0 p u b n 5 h c S m / X F h Z X V v f U D e 3 m t I P B a E N 4 n N f t G 0 s K W c e b Q A D T t u B o N i 1 O W 3 Z o / P E b 9 1 R I Z n v 3 c A 4 o F 0 X D z z m M I I h l n r q f k m 3 g i H r W T C k g M u W i 2 F o O 9 H l 5 B b 2 9 I K e / g n m 0 d V E 7 6 l F o 2 K k 0 G a J m Z E i y l D v q R 9 W 3 y e h S z 0 g H E v Z M Y 0 A u h E W w A i n k 4 I V S h p g M s I D 2 o m p h 1 0 q u 1 F 6 0 0 Q r x U p f c 3 w R P w + 0 V P 3 Z E W F X y r F r x 5 X J j v K v l 4 j / e Z 0 Q n J N u x L w g B O q R 6 S A n 5 B r 4 W h K Q 1 m e C E u D j m G A i W L y r R o Z Y Y A J x j I U 0 h N M E R 9 8 n z 5 J m t W I e V A 6 v q 8 X a W R Z H H u 2 g X V R G J j p G N X S B 6 q i B C L p H j + g Z v S g P y p P y q r x N S 3 N K 1 r O N f k F 5 / w J m 4 J 1 f &lt; / l a t e x i t &gt; Memory Bank Similar ✓ &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " C Z M u 9 i 9 m q Y h 9 p d z T Q n b e w b q T X R 0 = " &gt; A A A B 7 3 i c b V D L S s N A F J 3 U V 6 2 v q k s 3 g 6 3 g q i R V f O y K b l x W s A 9 o Q 5 l M J + 3 Q y S T O 3 A g l 9 C f c u F D E r b / j z r 9 x k g Z R 6 4 E L h 3 P u 5 d 5 7 v E h w D b b 9 a R W W l l d W 1 4 r r p Y 3 N r e 2 d 8 u 5 e W 4 e x o q x F Q x G q r k c 0 E 1 y y F n A Q r B s p R g J P s I 4 3 u U 7 9 z g N T m o f y D q Y R c w M y k t z n l I C R u t U + j B m Q 6 q B c s W t 2 B r x I n J x U U I 7 m o P z R H 4 Y 0 D p g E K o j W P c e O w E 2 I A k 4 F m 5 X 6 s W Y R o R M y Y j 1 D J Q m Y d p P s 3 h k + M s o Q + 6 E y J Q F n 6 s + J h A R a T w P P d A Y E x v q v l 4 r / e b 0 Y / A s 3 4 T K K g U k 6 X + T H A k O I 0 + f x k C t G Q U w N I V R x c y u m Y 6 I I B R N R K Q v h M s X Z 9 8 u L p F 2 v O S e 1 0 9 t 6 p X G V x 1 F E B + g Q H S M H n a M G u k F N 1 E I U C f S I n t G L d W 8 9 W a / W 2 7 y 1 Y O U z + + g X r P c v d u y P u g = = &lt; / l a t e x i t &gt; m I&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " L y 2 C 3 v 3 m H Z f g 5 k O K 3 1 I r Z o Y R c r g = " &gt; A A A C t 3 i c b V F N b 9 Q w E H X C V w k f X e D I x W K 3 U p H Q K h u W w t 6 q c o E D U p H Y t m K z R I 7 j b K w 6 T r A n h Z X l v 8 i B G / 8 G J x v K l j K S r T d v 3 s h v x m k t u I Y w / OX 5 N 2 7 e u n 1 n 5 2 5 w 7 / 6 D h 7 u D R 4 9 P d N U o y u a 0 E p U 6 S 4 l m g k s 2 B w 6 C n d W K k T I V 7 D Q 9 f 9 v W T y + Y 0 r y S n 2 B d s 2 V J V p L n n B J w V D L 4 s T e K 6 4 I n M R Q M y H 5 c E i j S 3 L y 3 X + D 5 K H D F l q B E m A / W p X G j W U 3 o O V m x R X b B a y 1 J y f T S f O + M W C f I W O 6 c d K k h u i a K r B p t j V q l 1 o T j 6 A U O x w f t N Y 1 s c E V 8 p K p v 8 l L 4 a t a K o u 4 O b W D i b d H o j 8 v S J u a v Y z u y Q T I Y u o Y u 8 H U w 6 c E Q 9 X G c D H 7 G W U W b k k m g g m i 9 m I Q 1 L A 1 R w K l g z u P W x A 7 2 8 2 7 G x X u O y X B e K X c k 4 I 7 d 7 j C k 1 H p d p k 7 Z u t T / 1 l r y f 7 V F A / m b p e G y b o B J u n k o b w S G C r e f i D O u G A W x d o B Q x Z 1 X T A u 3 a w r u q z d L m L V x c D n y d X A S j S c v x 9 O P 0 f D w q F / H D n q K n q F 9 N E G v 0 S F 6 h 4 7 R H F F v 6 n 3 2 q J f 5 M z / x c 7 / Y S H 2 v 7 3 m C r o T / 9 T d n f 9 X X &lt; / l a t e x i t &gt; m I 0 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " y 2 F W s p h i + t + V G d c X l c 7 6 o / L k i 9 c = " &gt; A A A D D H i c b V L L b t Q w F H X C q 4 R H p 7 B k Y 3 W m o k h o l A l t o b u q b G C B V K R O W 2 k y R I 7 j z F h 1 H M t 2 C i P L H 8 C G X 2 H D A o T Y 8 g H s + B u c T D p M H 1 f K 1 f G 9 5 5 6 c 3 D g V j C o d h n 8 9 / 8 b N W 7 f v r N w N 7 t 1 / 8 H C 1 s / b o S J W V x G S I S 1 b K k x Q p w i g n Q 0 0 1 I y d C E l S k j B y n p 6 / r / v E Z k Y q W / F D P B B k X a M J p T j H S r p S s e e s b v V h M a R L r K d F o M y 6 Q n q a 5 e W s / 6 G e 9 w D X r A k b M v L P u G F e K C I R P 0 Y S M s j M q F E c F U W P z q X F i H S E j u b P S H A 1 S A k k 0 q Z Q 1 c p J a E / a j 5 z D s 7 9 R p K 7 L B J f q + L D / y B X V 7 t 6 Z F T Q 6 d s o m X W b 1 z o 4 V N z H / T t u d U L 4 g e E s Q W m m E t t t 0 k G 5 w L N o z r 9 Z 4 2 g k m n 6 0 a b g F f B o A V d 0 M Z B 0 v k T Z y W u C s I 1 Z k i p 0 S A U e m y Q 1 B Q z Y o P l L T r Y 7 n C + Q r j h K h n M S + k e r m F T X Z 4 w q F B q V q S O W d t U l 3 t 1 8 b r e q N L 5 q 7 G h X F S a c D x / U V 4 x q E t Y 3 w y Y U U m w Z j M H E J b U e Y V 4 6 v 4 f 1 u 7 + z J e w W 8 f O 4 p O v g q O o P 3 j R 3 3 o f d f f 2 2 3 W s g C d g H W y C A X g J 9 s A b c A C G A H u f v a / e d + + H / 8 X / 5 v / 0 f 8 2 p v t f O P A Y X w v / 9 D 6 / i 9 X A = &lt; / l a t e x i t &gt; Dissimilar Dissimilar m I 0 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " N Z U 1 S u G 2 4 a 2 Z Q j j M L X D C x l D X M h Q = " &gt; A A A D Y H i c j V L P b 9 M w F H Y T f p S w s R Z u c I l o J 4 a E q i R 0 g 9 2 m c Y E D 0 p D W b V J T I s d x W m v O D 9 l O o b L 8 T 3 L j w I W / B D s J p a x D 4 k l + e n 7 v 8 + f P f i 8 u K e H C 8 7 5 3 L P v O 3 X v 3 u w + c h z u 7 j /</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>y H 8 9 G n 8 K B i e n 7 X d 0 w T P w H B w A H 7 w B J + A 9 O A M T g D o / L N v a s X a t n 3 b X 3 r P 7 D d T q t G e e g L / M f v o L c h w P O g = = &lt; / l a t e x i t &gt; Similar ✓ &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " C Z M u 9 i 9 m q Y h 9 p d z T Q n b e w b q T X R 0 = " &gt; A A A B 7 3 i c b V D L S s N A F J 3 U V 6 2 v q k s 3 g 6 3 g q i R V f O y K b l x W s A 9 o Q 5 l M J + 3 Q y S T O 3 A g l 9 C f c u F D E r b / j z r 9 x k g Z R 6 4 E L h 3 P u 5 d 5 7 v E h w D b b 9 a R W W l l d W 1 4 r r p Y 3 N r e 2 d 8 u 5 e W 4 e x o q x F Q x G q r k c 0 E 1 y y F n A Q r B s p R g J P s I 4 3 u U 7 9 z g N T m o f y D q Y R c w M y k t z n l I C R u t U + j B m Q 6 q B c s W t 2 B r x I n J x U U I 7 m o P z R H 4 Y 0 D p g E K o j W P c e O w E 2 I A k 4 F m 5 X 6 s W Y R o R M y Y j 1 D J Q m Y d p P s 3 h k + M s o Q + 6 E y J Q F n 6 s + J h A R a T w P P d A Y E x v q v l 4 r / e b 0 Y / A s 3 4 T K K g U k 6 X + T H A k O I 0 + f x k C t G Q U w N I V R x c y u m Y 6 I I B R N R K Q v h M s X Z 9 8 u L p F 2 v O S e 1 0 9 t 6 p X G V x 1 F E B + g Q H S M H n a M G u k F N 1 E I U C f S I n t G L d W 8 9 W a / W 2 7 y 1 Y O U z + + g X r P c v d u y P u g = = &lt; / l a t e x i t &gt; res5 res5 I t &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Z Y k F v c O H A P M c Q b c K w h P q 3 u j e h + c = " &gt; A A A C G 3 i c b V D L S s N A F J 3 4 r P E V d e l m s C n U T U k q o s u i G 9 1 V s A 9 o Y 5 l M J + 3 Q y S T M T I Q S + h 9 u / B U 3 L h R x J b j w b 5 y 0 W d T W A w O H c 8 5 l 7 j 1 + z K h U j v N j r K y u r W 9 s F r b M 7 Z 3 d v X 3 r 4 L A p o 0 R g 0 s A R i 0 T b R 5 I w y k l D U c V I O x Y E h T 4 j L X 9 0 n f m t R y I k j f i 9 G s f E C 9 G A 0 4 B i p L T U s 6 o l e 1 D u h k g N / S C 9 n T y o U 9 s s 2 c G c p A V 7 P m D 3 r K J T c a a A y 8 T N S R H k q P e s r 2 4 / w k l I u M I M S d l x n V h 5 K R K K Y k Y m Z j e R J E Z 4 h A a k o y l H I Z F e O r 1 t A k t a 6 c M g E v p x B a f q / E S K Q i n H o a + T 2 Z J y 0 c v E / 7 x O o o J L L 6 U 8 T h T h e P Z R k D C o I p g V B f t U E K z Y W B O E B d W 7 Q j x E A m G l 6 z R 1 C e 7 i y c u k W a 2 4 Z 5 X z u 2 q x d p X X U Q D H 4 A S U g Q s u Q A 3 c g D p o A A y e w A t 4 A + / G s / F q f B i f s + i K k c 8 c g T 8 w v n 8 B R C K f G g = = &lt; / l a t e x i t &gt; g(v I t ) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " K W m f C B M J G y 4 6 B F q y Z V b S A l i Z W 7 g = " &gt; A A A C K 3 i c b V D L S s N A F J 3 4 r P E V d e k m 2 B b q p i Q V 0 W W p G 9 1 V s A 9 o Y 5 h M J + 3 Q y S T M T A o l 5 H / c + C s u d O E D t / 6 H 0 z a L 2 H p g 4 H D O u c y 9 x 4 s o E d K y P r W 1 9 Y 3 N r e 3 C j r 6 7 t 3 9 w a B w d t 0 U Y c 4 R b K K Q h 7 3 p Q Y E o Y b k k i K e 5 G H M P A o 7 j j j W 9 m f m e C u S A h e 5 D T C D s B H D L i E w S l k l y j U S 4 N K / 0 A y p H n J 3 f p o z w v 6 e W S n 5 O U k I t M U j f J x 5 X t G k W r a s 1 h r h I 7 I 0 W Q o e k a r / 1 B i O I A M 4 k o F K J n W 5 F 0 E s g l Q R S n e j 8 W O I J o D I e 4 p y i D A R Z O M r 8 1 N c t K G Z h + y N V j 0 p y r + Y k E B k J M A 0 8 l Z 2 u K Z W 8 m / u f 1 Y u l f O w l h U S w x Q 4 u P / J i a M j R n x Z k D w j G S d K o I R J y o X U 0 0 g h w i q e r V V Q n 2 8 s m r p F 2 r 2 h f V y / t a s d 7 I 6i i A U 3 A G K s A G V 6 A O b k E T t A A C T + A F v IM P 7 V l 7 0 7 6 0 7 0 V 0 T c t m T s A f a D + / X p S m B Q = = &lt; / l a t e x i t &gt; f (v I ) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 1 X I y 5 P t m 3 y j n f A U 4 Z v S D t u 2 g 1 M 8 = " &gt; A A A C K X i c b V D L S s N A F J 3 4 r P E V d e k m 2 B b q p i Q V 0 W X R j e 4 q 2 A e 0 M U y m k 3 b o Z B J m J o U S 8 j t u / B U 3 C o q 6 9 U e c t A F r 6 4 G B M + f c y 7 3 3 e B E l Q l r W p 7 a y u r a + s V n Y 0 r d 3 d v f 2 j Y P D l g h j j n A T h T T k H Q 8 K T A n D T U k k x Z 2 I Y x h 4 F L e 9 0 X X m t 8 e Y C x K y e z m J s B P A A S M + Q V A q y T X q 5 d K g 0 g u g H H p + c p s + y N O S X i 7 5 c 5 I S 5 v 7 j 1 E 1 + P W W 6 R t G q W l O Y y 8 T O S R H k a L j G a 6 8 f o j j A T C I K h e j a V i S d B H J J E M W p 3 o s F j i A a w Q H u K s p g g I W T T C 9 N z b J S + q Y f c v W Y N K f q f E c C A y E m g a c q s y X F o p e J / 3 n d W P q X T k J Y F E v M 0 G y Q H 1 N T h m Y W m 9 k n H C N J J 4 p A x I n a 1 U R D y C G S K l x d h W A v n r x M W r W q f V Y 9 v 6 s V 6 1 d 5 H A V w D E 5 A B d j g A t T B D W i A J k D g E T y D N / C u P W k v 2 o f 2 N S t d 0 f K e I / A H 2 v c P p m 6 l H g = = &lt; / l a t e x i t &gt; v I &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " U J 5 U s v g N f P v u S U T D I T K 4 O t V 6 C v g = " &gt; A A A C R H i c d Z D L S s N A F I Y n 9 V b j L e r S T b A p 1 E 1 J K q L L o h v d V b A X S G O Y T C f t 0 M m F m U m h h D y c G x / A n U / g x o U i b s V J W 7 C 2 e m D g 5 / v P 4 Z z 5 v Z g S L k z z W S m s r K 6 t b x Q 3 1 a 3 t n d 0 9 b f + g x a O E I d x E E Y 1 Y x 4 M c U x L i p i C C 4 k 7 M M A w 8 i t v e 8 C r 3 2 y P M O I n C O z G O s R P A f k h 8 g q C Q y N X s s t G v d A M o B p 6 f 3 m T 3 4 s R Q y 4 Y / h x b A K H P T H z N 3 j X 8 s w 9 V K Z t W c l L 4 s r J k o g V k 1 X O 2 p 2 4 t Q E u B Q I A o 5 t y 0 z F k 4 K m S C I 4 k z t J h z H E A 1 h H 9 t S h j D A 3 E k n I W R 6 W Z K e 7 k d M v l D o E z o / k c K A 8 3 H g y c 7 8 R r 7 o 5 f A v z 0 6 E f + G k J I w T g U M 0 X e Q n V B e R n i e q 9 w j D S N C x F B A x I m / V 0 Q A y i I T M X Z U h W I t f X h a t W t U 6 r Z 7 d 1 k r 1 y 1 k c R X A E j k E F W O A c 1 M E 1 a I A m Q O A B v I A 3 8 K 4 8 K q / K h / I 5 b S 0 o s 5 l D 8 K u U r 2 + x R b B F &lt; / l a t e x i t &gt;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>n X u 5 d 4 4 T c i b B N N + 0 z M b m 1 v Z O d j e 3 t 3 9 w e J Q / P m n I I B K E 1 k n A A 9 F y s K S c + b Q O D D h t h Y J i z + G 0 6 Y z u U r 8 5 p k K y w H + C S U i 7 H h 7 4 z G U E g 0 J 2 3 i 4 a g 1 L H w z B 0 3 P g h 6 c G F k S s a 7 g J a A u P</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>I 9 q q I 4 I e k b v 6 B N 9 a S / a h / a t / c x a M 9 p 8 5 h T 9 q 0 z m F 4 F D s T g = &lt; / l a t e x i t &gt; Overview of PIRL. Pretext-Invariant Representation Learning (PIRL) aims to construct image representations that are invariant to the image transformations t ∈ T . PIRL encourages the representations of the image, I, and its transformed counterpart, I t , to be similar. It achieves this by minimizing a contrastive loss (see Section 2.1). Following [72], PIRL uses a memory bank, M, of negative samples to be used in the contrastive learning. The memory bank contains a moving average of representations, m I ∈ M, for all images in the dataset (see Section 2.2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>4 but uses memory representations m I and m I instead of f (v I ) and f (v I ), respectively. The second term does two things: (1) it encourages the representation f (v I ) to be similar to its memory representation m I , thereby dampening the parameter updates; and (2) it encourages the representations f (v I ) and f (v I ) to be dissimilar. We note that both the first and the second term use m I instead of f (v I ) in Equation 4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 :</head><label>4</label><figDesc>Invariance of PIRL representations. Distribution of l 2 distances between unit-norm image representations, f (v I )/ f (v I ) 2 , and unit-norm representations of the transformed image, g(v I t )/ g(v I t ) 2 . Distance distributions are shown for PIRL and Jigsaw representations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Quality of PIRL representations per layer. Top-1 accuracy of linear models trained to predict ImageNet classes based on representations extracted from various layers in ResNet-50 trained using PIRL and Jigsaw. Effect of varying the trade-off parameter λ. Top-1 accuracy of linear classifiers trained to predict ImageNet classes from PIRL representations as a function of hyperparameter λ in Equation 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 7 :</head><label>7</label><figDesc>Effect of varying the number of patch permutations in T . Performance of linear image classification models trained on the VOC07 dataset in terms of mAP. Models are initialized by PIRL and Jigsaw, varying the number of image transformations, T , from 1 to 9! ≈ 3.6 million.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>ImageNetFigure 8 :</head><label>8</label><figDesc>Effect of varying the number of negative samples. Top-1 accuracy of linear classifiers trained to perform ImageNet classification using PIRL representations as a function of the number of negative samples, N .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>MethodNetwork AP all AP 50 AP 75 ∆AP 75</figDesc><table><row><cell>Supervised</cell><cell>R-50</cell><cell cols="3">52.6 81.1 57.4</cell><cell>=0.0</cell></row><row><cell>Jigsaw [19]</cell><cell>R-50</cell><cell cols="3">48.9 75.1 52.9</cell><cell>-4.5</cell></row><row><cell>Rotation [19]</cell><cell>R-50</cell><cell cols="3">46.3 72.5 49.3</cell><cell>-8.1</cell></row><row><cell>NPID++ [72]</cell><cell>R-50</cell><cell cols="3">52.3 79.1 56.9</cell><cell>-0.5</cell></row><row><cell>PIRL (ours)</cell><cell>R-50</cell><cell cols="3">54.0 80.7 59.7</cell><cell>+2.3</cell></row><row><cell>CPC-Big [26]</cell><cell>R-101</cell><cell>-</cell><cell>70.6  *</cell><cell>-</cell></row><row><cell cols="2">CPC-Huge [26] R-170</cell><cell>-</cell><cell>72.1  *</cell><cell>-</cell></row><row><cell>MoCo [24]</cell><cell cols="2">R-50 55.2</cell><cell></cell><cell></cell></row></table><note>* † 81.4 * † 61.2 * †</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Object detection on VOC07+12 using Faster R-CNN. Detection AP on the VOC07 test set after finetuning Faster R-CNN models (keeping BatchNorm fixed) with a ResNet-50 backbone pre-trained using self-supervised learning on ImageNet. Results for supervised ImageNet pre-training are presented for reference. Numbers with * are adopted from the corresponding papers. Method with † finetunes BatchNorm. PIRL significantly outperforms supervised pre-training without extra pre-training data or changes in the network architecture. Additional results inTable 6.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Pre-training on uncurated YFCC images. Top-1 accuracy or mAP (for VOC07) of linear image classifiers for four image-classification tasks, using various image representations. All numbers (except those for PIRL) are adopted from the corresponding papers. Deep(er)Cluster uses VGG-16 rather than ResNet-50. The best performance on each dataset is boldfaced. Top: Representations obtained by training ResNet-50 models on a randomly selected subset of one million images. Bottom: Representations learned from about 100 million YFCC images.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>(top). In line with earlier results, models trained using PIRL</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Using PIRL with (combinations of) different pretext tasks. Top-1 accuracy / mAP of linear image classifiers trained on PIRL image representations. Top: Performance of PIRL used in combination with the Rotation pretext task<ref type="bibr" target="#b17">[18]</ref>. Bottom: Performance of PIRL using a combination of multiple pretext tasks.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>AP all AP 50 AP 75 ∆AP 75</figDesc><table><row><cell>Supervised</cell><cell>R-50</cell><cell cols="3">43.8 74.5 45.9</cell><cell>=0.0</cell></row><row><cell>Jigsaw [19]</cell><cell>R-50</cell><cell cols="3">37.7 64.9 38.0</cell><cell>-7.9</cell></row><row><cell>PIRL (ours)</cell><cell>R-50</cell><cell cols="3">44.7 73.4 47.0</cell><cell>+1.1</cell></row><row><cell>NPID [72]</cell><cell>R-50</cell><cell>-</cell><cell>65.4  *</cell><cell>-</cell></row><row><cell>LA [80]</cell><cell>R-50</cell><cell>-</cell><cell>69.1  *</cell><cell>-</cell></row><row><cell cols="2">Multi-task [10] R-101</cell><cell>-</cell><cell>70.5</cell><cell></cell></row></table><note>* -</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments:</head><p>We are grateful to Rob Fergus, and Andrea Vedaldi for encouragement and their feedback on early versions of the manuscript; Yann LeCun for helpful discussions; Aaron Adcock, Naman Goyal, Priya Goyal, and Myle Ott for their help with the code development for this research; and Rohit Girdhar, and Ross Girshick for feedback on the manuscript. We thank Yuxin Wu, and Kaiming He for help with Detectron2.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Video jigsaw: Unsupervised learning of spatiotemporal context for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishi</forename><surname>Unaiza Ahsan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irfan</forename><surname>Madhok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Look, listen and learn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Objects that sound</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Buchwalter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.00910</idno>
		<title level="m">Learning representations by maximizing mutual information across views</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Domain generalization by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fabio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio D&amp;apos;</forename><surname>Carlucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Innocente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Bucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatiana</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tommasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised pre-training of image features on non-curated data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning large-scale automatic image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Deshpande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Rock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1422" to="1430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-task selfsupervised visual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adversarial feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Large scale adversarial representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.02544</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Discriminative unsupervised feature learning with exemplar convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><forename type="middle">Tobias</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Temporal cycleconsistency learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debidatta</forename><surname>Dwibedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuf</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Self-supervised video representation learning with odd-one-out networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning to separate object sounds by watching unlabeled video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruohan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07728</idno>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Scaling and benchmarking self-supervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semi-supervised learning by entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="529" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Noise-contrastive estimation: A new estimation principle for unnormalized statistical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="297" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Video representation learning by dense predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengda</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshop</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno>arXiv 1911.05722</idno>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Hénaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Sm Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den Oord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09272</idno>
		<title level="m">Data-efficient image recognition with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>R Devon Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06670</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Let there be color!: joint end-to-end learning of global and local image priors for automatic image colorization with simultaneous classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Iizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Ishikawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">110</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Invariant information clustering for unsupervised image classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning visual features from large weakly supervised data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Vasilache</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Warpnet: Weakly supervised matching for singleview reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3253" to="3261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Selfsupervised video representation learning with space-time cubic puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghyeon</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In</forename><forename type="middle">So</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Revisiting self-supervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.09005</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Cooperative learning of audio and video models from self-supervised synchronization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Korbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning representations for automatic colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustav</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Colorization as a proxy task for visual understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustav</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Ying</forename><surname>Lee</surname></persName>
		</author>
		<title level="m">Jia-Bin Huang, Maneesh Singh, and Ming</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by sorting sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Fast autoaugment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungbin</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ildoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesup</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiheon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungwoong</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Ashwin Bharambe, and Laurens van der Maaten. Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Stacked convolutional auto-encoders for hierarchical feature extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICANN</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="52" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Adversarial variational Bayes: Unifying variational autoencoders and generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Shuffle and learn: unsupervised learning using temporal order verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Virtual adversarial training: a regularization method for supervised and semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Shin-Ichi Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shin</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">41</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Representation learning by learning to count</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Boosting self-supervised learning via knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananth</forename><surname>Vinjimoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Self-supervised learning of geometrically stable features through probabilistic introspection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Novotny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Larlus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3637" to="3645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Emergence of simple-cell receptive field properties by learning a sparse code for natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Olshausen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Field</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">381</biblScope>
			<biblScope unit="issue">6583</biblScope>
			<biblScope unit="page">607</biblScope>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Audio-visual scene analysis with self-supervised multisensory features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Ambient sound provides supervision for visual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><forename type="middle">H</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Learning features by watching objects move</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Unsupervised learning of invariant feature hierarchies with applications to object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcaurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu-Jie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y-Lan</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Convolutional neural network architecture for geometric matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6148" to="6157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<title level="m">ImageNet Large Scale Visual Recognition Challenge. IJCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Deep Boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AI-STATS</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="448" to="455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Time-contrastive networks: Self-supervised learning from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corey</forename><surname>Lynch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yevgen</forename><surname>Chebotar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasmine</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Schaal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Brain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICRA</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabien</forename><surname>Baradel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05743</idno>
		<title level="m">Contrastive bidirectional transformer for temporal representation learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Revisiting unreasonable effectiveness of data in deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Friedland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Elizalde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damian</forename><surname>Poland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.01817</idno>
		<title level="m">The new data in multimedia research</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">100</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05849</idno>
		<title level="m">Contrastive multiview coding</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">The inaturalist species classification and detection dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Grant Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8769" to="8778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">The devil is in the tails: Fine-grained classification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grant</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.01450</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations using videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Transitive invariance for self-supervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1329" to="1338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Learning and using the arrow of time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglai</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Yen</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Detectron2</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron2" />
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.12848</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Unsupervised data augmentation. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dejing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Xie</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Self-supervised spatiotemporal learning via video clip order prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">S4l: Self-supervised semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.03670</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Aet vs. aed: Unsupervised representation learning by autoencoding transformations rather than data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.04596</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Split-brain autoencoders: Unsupervised learning by cross-channel prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Learning deep features for scene recognition using places database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Local aggregation for unsupervised learning of visual embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxu</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">Lin</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Yamins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
