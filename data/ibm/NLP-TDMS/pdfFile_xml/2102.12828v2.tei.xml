<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Negative Augmentation with Language Model for Reading Comprehension of Abstract Meaning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-04">Apr 2021 ZJUKLAB at SemEval-2021 Task 4</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Xie</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Chen</surname></persName>
							<email>xiangchen@zju.edu.cnzhangningyu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shumin</forename><surname>Deng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Chen</surname></persName>
							<email>huajunsir@zju.edu.cnwangyon@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Negative Augmentation with Language Model for Reading Comprehension of Abstract Meaning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-04">Apr 2021 ZJUKLAB at SemEval-2021 Task 4</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents our systems for the three Subtasks of SemEval Task4: Reading Comprehension of Abstract Meaning (ReCAM). We explain the algorithms used to learn our models and the process of tuning the algorithms and selecting the best model. Inspired by the similarity of the ReCAM task and the language pre-training, we propose a simple yet effective technology, namely, negative augmentation with language model. Evaluation results demonstrate the effectiveness of our proposed approach. Our models achieve the 4th rank on both official test sets of Subtask 1 and Subtask 2 with an accuracy of 87.9% and an accuracy of 92.8%, respectively 1 . We further conduct comprehensive model analysis and observe interesting error cases, which may promote future researches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Past decades have witnessed the huge progress of representation learning in Natural Language Processing (NLP). With pre-trained language models, machine reading comprehension (MRC) models can extract answers from given documents and even yield better performance than humans on benchmark datasets such as Squad <ref type="bibr" target="#b17">(Rajpurkar et al., 2016)</ref>. However, these successes sometimes lead to the hype in which these models are being described as "understanding" language or capturing "meaning" <ref type="bibr" target="#b0">(Bender and Koller, 2020)</ref>. Note that the intention of MRC is letting the systems read a text like human beings, extracting text information and understanding the meaning of a text then answering questions, which means the systems can not only conclude the semantic of the text but also comprehend the abstract concepts under the constraint of general knowledge regarding * Equal contribution and shared co-first authorship. 1 Our implementation is publicly available at https://github.com/zjunlp/SemEval2021Task4 the world <ref type="bibr" target="#b24">(Wang and Jiang, 2016)</ref>. Nevertheless, little works as well as benchmarks focus on this direction.</p><p>SemEval 2021 Task4 <ref type="bibr" target="#b31">(Zheng et al., 2021)</ref> is an MRC task that focuses on evaluating the model's ability to understand abstract words. Reading Comprehension of Abstract Meaning (ReCAM) task is divided into three Subtasks including Subtask 1: ReCAM-Imperceptibility, Subtask 2: ReCAM-Nonspecificity and Subtask 3: ReCAM-Intersection.</p><p>Unlike previous MRC datasets such as CNN/Daily Mail <ref type="bibr" target="#b8">(Hermann et al., 2015)</ref>, SQuAD <ref type="bibr" target="#b16">(Rajpurkar et al., 2018)</ref>, and CoQA <ref type="bibr" target="#b18">(Reddy et al., 2019)</ref> that request computers to predict concrete concepts, e.g. named entities. This task challenges the model's ability to fill the abstract words removed from human-written summaries based on the English context.</p><p>Note that this task's input format is similar to the MLM pre-training task of BERT <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref>, which aims to predict the mask tokens. Pretrained language models (PLMs) such as BERT <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref>, RoBERTa <ref type="bibr" target="#b10">(Liu et al., 2019)</ref>, ALBERT <ref type="bibr" target="#b9">(Lan et al., 2020)</ref>, DeBERTa <ref type="bibr" target="#b7">(He et al., 2021)</ref> have achieved success on MRC tasks. Inspired by this, we introduce a simple yet effective method, namely, Negative Augmentation with Language model (NAL) in SemEval 2021 Task4. Specifically, we augment the answer distribution with an additional negative candidate from the mask language model's prediction. Previous work <ref type="bibr" target="#b14">(Petroni et al., 2019;</ref><ref type="bibr" target="#b32">Zhou et al., 2020)</ref> indicates that the pre-trained language model has already captured much world knowledge. Thus, we argue that knowledge can help guild the model training and identify those ambiguous abstract meanings. Further, we introduce other technologies such as label smoothing, domain-adaptive pre-training in our system. We describe the detailed approaches used for the Subtasks in Section 3.</p><p>We conduct comprehensive experiments in Section 3, and we achieve the 4th system for Subtask 1: ReCAM-Imperceptibility and the 4th system for Subtask 2: ReCAM-Nonspecificity in the leaderboard. In our experiments, we observe that PLMs without fine-tuning can easily get 60+% accuracy on both Subtask 1 and Subtask 2, demonstrating that pre-trained language models already capture some abstract meanings. We further find that our negative augmentation with language model can improve the performance with 2.6% in Subtask 1 and 4.6% in Subtask 2. Finally, we conduct error analysis to promote future researches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Machine reading comprehension (MRC) has received increasing attention recently, which is a challenging task. According to the type of the answer, reading comprehension tasks can be divided into four categories <ref type="bibr" target="#b2">(Chen, 2018)</ref>: 1) Cloze-style: The question contains a "@placeholder," and the system must choose a word or entity from the set of candidate answers to fill in the "@placeholder" to make the sentence complete. 2) Multiple choice: In this type of task, Choosing a suitable answer from K sets of given answers. This answer can be one word or a sentence. 3) Span prediction: This kind of task is also called (Extractive question answering), which requires the system to extract a suitable range of text fragments from a given original text based on the question as to the answer. 4) Free-form answer: This task allows the answer to be any type of text, which is necessary to mine deep-level contextual semantic information according to a given question and a collection of candidate documents, and even combine multiple articles to give the best answer.</p><p>In SemEval 2021 Task4, it requires the system to have a strong ability of reading comprehension not only because the task is the cloze-style format as mentioned above but also the abstract words in answers. There are two definitions of abstract words: imperceptibility and nonspecificity. Concrete words refer to things, events, and properties that we can perceive directly with our senses <ref type="bibr" target="#b21">(Spreen and Schulz, 1966;</ref><ref type="bibr" target="#b23">Turney et al., 2011)</ref>. Compared to concrete words like "trees" and "red," abstract words for imperceptibility are created by humans instead of pointing the things in the natural world. For example, as shown in <ref type="table" target="#tab_0">Table 1</ref>, "want" and "achieve" means a person's attitude to-P: Briton Davies won F42 shot put gold with a Games record at Rio 2016, but was unable to defend his 2012 discus title as it did not feature in Brazil. "I don't normally say what I'm going for," said the Welshman, 25. "But this time I'm definitely going for the two golds in both disciplines and nothing will be better than being in front of a home crowd." ... Q: Paralympic champion Aled Sion Davies @placeholder two gold medals at the 2017 World Para Athletics Championships in London.  wards something and a person's accomplishment about something. Meanwhile, the abstract words for nonspecificity can be described as upper words. By determining whether one word can generalize another word, we can get dictionaries of different levels. The words with higher levels are the nonspecificity words. Compared to concrete concepts like groundhog and whale, hypernyms such as vertebrate are regarded as more abstract <ref type="bibr" target="#b1">(Changizi, 2008)</ref>. The difference between Subtask 1 and Subtask 2 is the definition of abstract words. So the input of both Subtask 1 and Subtask 2 are the same. The input of these tasks are shown in <ref type="table" target="#tab_0">Table 1</ref>, it can be represented as a triple &lt; P, Q, A &gt;, where P = s 1 , s 2 , ..., s m is the passage from CNN daily <ref type="bibr" target="#b8">(Hermann et al., 2015)</ref>, Q is a human-written summary based on the passage with one abstract word replaced by "@placeholder" and A is a set of can-didate abstract words for filling in the "@placeholder" in the question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">System Overview</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model Design</head><p>Recently, with the development of the large Pretrained Language Models (PLMs), such as GPT <ref type="bibr" target="#b15">(Radford et al., 2018)</ref>, BERT <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref>, RoBERTa <ref type="bibr" target="#b10">(Liu et al., 2019)</ref>, ALBERT <ref type="bibr" target="#b9">(Lan et al., 2020)</ref>, DeBERTa <ref type="bibr" target="#b7">(He et al., 2021)</ref>, have overwhelm the NLP community <ref type="bibr" target="#b30">(Zhang et al., 2020c)</ref>. The powerful semantic feature extraction capabilities of the PLMs make us only need to make better use of the BERT-like model itself for downstream tasks instead of adding different layers to the model.</p><p>Similar to the normal multi-choice task, we have five candidates, one passage, and one question per sample. Here we leverage PLMs as encoders to capture the global context representation about the passage, question, and answer. Then a decoder is used to determine the score of each &lt; P, Q, A &gt; pair. Since we get A 1 , ..., A n n answers, for every passage, we construct n input samples as [Q − A i ; P ], the concatenation of Q − A i and P . Because the question is the summary with an abstract word removed. We construct Q − A by replacing "@placeholder" with the option from the candidate set instead of concatenating Q and A. After encoding all n inputs for a single passage, we get the global representations T i for different options in the candidate set. During fine-tuning PLMs, the first special token [CLS] represents the global meaning of the whole input. We use an dense decoder layer to compute the score for all T i , the calculation of score is as follow:</p><formula xml:id="formula_0">T i = P LM (Q − A; P )</formula><p>(1)</p><formula xml:id="formula_1">score i = exp (f (T i )) i ′ exp (f (T i ′ ))<label>(2)</label></formula><p>where the [Q − A; P ] is the input constructed according to the instruction of PLMs and MRC tasks, and the T * is the final hidden state of the first token <ref type="bibr">[CLS]</ref>. The candidate answers with higher scores will be identified as the final prediction.</p><p>Since previous research <ref type="bibr" target="#b5">(Gao et al., 2020;</ref> demonstrate that there exists a gap between language model pre-training and fine-tuning the models in the downstream task and inspire by the similar task definition as MLM, we introduce the negative augmentation with language model mechanism (Section 3.2). Note that the additional label will enhance the discriminability of the abstract meanings in a contrastive manner. In other words, the model is encouraged NOT to generate those abstract tokens from the language model, but the golden candidates from the given documents. We further introduce the label smoothing (Section 3.3), which can enhance the model performance. Finally, we leverage task-adaptive pre-training (Section 3.4) inspired by <ref type="bibr" target="#b6">(Gururangan et al., 2020)</ref> to obtain better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Negative Augmentation with Language Model</head><p>Inspired by the same format of MLM and this task, we first conduct a toy experiment to test whether a PLM can get the right answer without any supervised signal. Firstly we replace the "@placeholder" with [MASK] to reconstruct the input and ask the BERT model with MLM head to predict the word token at the [MASK]. Then we calculate the similarity between the word model predict and the options from the set of candidate answers. We set the option with the highest similarity score as the model's choice. Then we find that the BERT model without any fine-tuning gets 60+% accuracy in both Subtask 1 and Subtask 2. The result above shows that PLMs have the ability to predict abstract words, and those predicted words can be leveraged as negative candidates in the fine-tuning period.</p><p>Note that huge languages have quantities of parameters; the PLMs are able to store much knowledge through pre-training tasks. However, [MASK] is not used when fine-tuning the model for downstream tasks; how to use the knowledge stored by the model on pre-training tasks more explicitly on downstream tasks has become a hot topic of current research. Motivated by this, we try to bridge the gap between pre-train and downstream tasks. Inspired by the contrastive learning <ref type="bibr" target="#b19">Robinson et al., 2020)</ref> as stronger negative samples will help the model learning with better performance, we introduce our negative augmentation with language model method. Specifically, we let the PLMs predict the "@placeholder" replaced with [MASK] token to generate negative candidates. Thus, we can leverage those negative words that may mislead the models to help train the models. Formally, we have:</p><formula xml:id="formula_2">P = p(m i |θ, [Q−A; P ]), m i ∈ [1, 2, ..., |V |] (3)</formula><p>where P are the distribution of words model, predict, m i is the token in the vocabulary, and |V | is the total number of the vocabulary. We can use the distribution to get the top confusing words to augment our models, which is described in <ref type="figure" target="#fig_2">Figure  2</ref>. Due to the limitation of GPU, we add the most possible word to augment our models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Label Smoothing</head><p>Label smoothing is a well-known "trick" to improve the model's performance effectively. It encourages the activations of the penultimate layer to be close to the template of the correct class and equally distant to the templates of the incorrect classes <ref type="bibr" target="#b12">(Müller et al., 2019)</ref>. With more options than the original dataset by the approach mentioned in Section 3.2, label smoothing will magnify our method's effect while fine-tuning the models. Suppose the output of the final layer and softmax layer as follows:</p><formula xml:id="formula_3">p k = e x T w k L l=1 e x T w l<label>(4)</label></formula><p>where p k is the likelihood the model assigns to the k-th class, w k represents the weights and biases of the last layer. x is the vector containing the activations of the penultimate layer of a neural network concatenated with "1" to account for the bias. let us see the equitation about the cross entropy loss.</p><formula xml:id="formula_4">L = − M c=1 y k log (p k )<label>(5)</label></formula><p>The cross-entropy formula without Label smoothing only focuses on whether the positive example is true and does not pay attention to the negative examples' relationship. We make the soft y as follows:</p><formula xml:id="formula_5">y i = (1 − ε), right answer ε K−1 , wrong answer<label>(6)</label></formula><p>We set ε as 0.1 in our models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Task-Adaptive Pre-training</head><p>The BERT-like model is pre-trained in the general domain corpus such as Wikipedia. Since passages  mainly come from CNN daily, the data distribution may be quite different from pre-training data. Therefore, we utilize task-adaptive pre-train BERT with masked language model and next sentence prediction tasks on the domain-specific data. Taskadaptive pre-training not only makes the model better fit the distribution in the domain but also helps the model to predict good negative words to enhance the original dataset, which is described in Section 3.2. We take two different approaches for task-adaptive pre-training as follows:</p><p>1) In-domain pre-training, we use the source data: CNN Daily to task-adaptive pre-training our base models .</p><p>2) Within-task pre-training, practically we replace the "@placeholder" with the correct answer and put the same input format as the fine-tuning steps, which is [Q − A; P ] (Gururangan et al., 2020).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>In Subtask 1, the training/trail/development/test contains 3, 227/1, 000/837/2, 025 instances. In Subtask 2, the training/trail/development/test contains 3, 318/1, 000/851/2, 017 instances. The overall statistics can be found in <ref type="table" target="#tab_2">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Pre-processing</head><p>For data pre-processing, we use the byte-level BPE encoding <ref type="bibr" target="#b20">(Sennrich et al., 2016)</ref>, and the official vocabulary contains more than fifty thousand byte-level tokens. All tokens are stored in MERGES.TXT, while VOCAB.JSON is a byte-toindex mapping. Generally speaking, the higher the frequency, the smaller the byte index. Since the average length of the passage about Subtask 1 and Subtask 2 is 262 and 418, we divide those long context paragraphs. We limit the max number of tokens in an input sample [Q−A; P ] to 256 for our system. Statically, 60% of the paragraphs exceeds the 256 tokens (including the special tokens like [CLS], [SEP] and so on. For these input samples, we divide them into new input samples with at most 256 tokens. To be more specific, we divide the passage to different inputs with the same question and answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Hyper-parameter Setting</head><p>Our system is implemented with PyTorch <ref type="bibr" target="#b13">(Paszke et al., 2019)</ref> and we use the PyTorch version of the pre-trained language models 2 . We employ RoBERTa, ALBERT, and DeBERTa large models as our PLM encoder. We use AdamW optimizer <ref type="bibr" target="#b11">(Loshchilov and Hutter, 2018)</ref> to fine-tune the models. We set the batch size to 1, and the max length of input to 256 for RoBERTa, 128 for ALBERT. Usually, the batch size has a significant influence on the BERT-like model; due to the limit of GPU memory, we use gradient accumulation in our training steps. We set the gradient accumulation step as 32, which means the formal number of batch sizes is 32 in training. We pick the best learning rate from the dev set, fine-tuning the RoBERTa, ALBERT, DeBERTa with the learning rate of 9×10 −6 , 1×10 −5 and 1×10 −5 respectively. We set the number of epoch to 8 for ALBERT and 12 for RoBERTa and DeBERTa. Furthermore, we save the best model on the validation set for testing during training. Because the formats of both Subtask 1 and Subtask 2 are the same, we set the same batch size and max length of the input sequence for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Subtask 1 Results</head><p>On Subtask 1 , the ReCAM-Imperceptibility task, the evaluation results are illustrated in <ref type="table" target="#tab_4">Table 3</ref>. We set the three baseline models: RoBERTa Large , DeBERTa Large , and ALBERTxxLarge. RoBERTa Large + NAL, DeBERTa Large + NAL, and ALBERT Large + NAL denotes the language model with our proposed negative augmentation with language model. Ensemble refers to the ensemble model of the three models as mentioned above with all strategies. We find that ALBERT achieves better performance in Subtask 1 but fails to get good performance in Subtask 2, while DeBERTa and RoBERTa have better performance in Subtask 2.   Comparing with the original RoBERTa, DeBERTa, and ALBERT models, each model is hugely improved with NAL by about 2.1% accuracy. We further observe that DeBERTa and RoBERTa, which have the same architecture, obtain better performance than ALBERT in the dev and test sets. We think the possible reason is that ALBERT uses layer weight sharing, which reduces the model's generalization ability in reading comprehension, especially the abstract words meaning. Finally, the ensemble of the best model of RoBERTa, De-BERTa, and ALBERT lead to a significant improvement (4.3% accuracy) compared with baselines, which is also our final submission to the leaderboard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Subtask 2 Results</head><p>On Subtask 2, the ReCAM-Nonspecificity task, the experiment results are showed in <ref type="table" target="#tab_5">Table  4</ref>. Similar to the models in Subtask 1, we choose RoBERTa, DeBERTa and ALBERT as our baseline models. All RoBERTa Large + NAL , ALBERT xxLarge + NAL and DeBERTa Large + NAL are the models with negative augmentation with language model. Ensemble refers to the ensemble model of RoBERTa, DeBERTa, and ALBERT with all strategies. We notice that our proposed mechanism brings significant improvement (averaging 4.3% of the accuracy score) compared with baselines, demonstrating the effectiveness of our proposed strategies such as negative augmentation with a language model, label smoothing, and taskadaptive pre-training. We observe that ensemble approach of three enhanced models (RoBERTa Large + NAL, ALBERT xxLarge + NAL and DeBERTa Large + NAL) obtain the best accuracy of 92.8% at test set, which is also our final submit to the leaderboard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Subtask3 Results</head><p>Subtask3 focuses on the model's transferability. During the evaluation period, we use the data on Subtask 2 to evaluate the models trained on the Subtask 1 and vice versa. We obtain the 82% accuracy of the model trained on Subtask 1 and evaluated on Subtask 2 on the dev set.</p><p>During experiments for all tasks, we have tried to use different decoders like MLP and other network architecture. Eventually, we find that it does not help to improve the system's performance. An explanation is that the pre-trained language models (PLMs) have already captured global contextual sentence meaning at the [CLS] token.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Further Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">Analysis of Negative Augmentation with Language Model</head><p>During our experiments, we conduct case studies to figure out how our method of NAL helps the model to boost performance. From <ref type="table" target="#tab_7">Table 5</ref>, we notice that the original PLM considers using the "all", "half" as its choice instead of "parts". Although fine-tuned on the downstream task, the baseline model still choose "half". In our NAL method, we add some misleading negative words to help models correct the knowledge learned from the pre-training task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">Analysis of Passage Length</head><p>In usual MRC tasks, the length of the passage is a key factor for the models to solve the problems. We conduct experiments to analyze the performance regarding different lengths of passage. Contrary to the common assumption, from <ref type="figure">Figure  3</ref> and <ref type="figure" target="#fig_3">Figure 4</ref>, we observe that the instances with long passage obtain better performance. We think   <ref type="table">Table.</ref> that abstract mean understanding may need comprehensive context information from the long sentence, and we will conduct further analysis in future works.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.3">Case Study</head><p>We select four kinds of different types of error cases to promote further researches. We classify the examples according to the main causes (pretraining, fine-tuning, and so on) of the error. We think it will help us better understand what the model learns from pre-training and fine-tuning. token. Just like the "IQ" model predict in the "@placeholder". Even after fine-tuning, our models still cannot recognize the strong evidence "being adversely affected". In our daily life, we wouldn't hold that being adversely affected by lack of sleep can lead to a decrease in IQ. We usually say that lack of sleeping may lower one's achievement in the future. • How to help models? To prevent the model from relying too much on pre-training tasks, we create more negative samples to help the model to understand what is wrong or right about the abstract words.</p><p>Case 2 -Adverse affected by fine-tuning • Passage: " 17 May 2017 Last updated at 12:44 BST Adrien Gulfo, wearing red, who plays for the Swiss side Pully Football, tried to clear the ball away from his goal with a spectacular bicycle kick. Unfortunately for him it all went very wrong -watch the video... There was a happy ending to the story for Gulfo though, Pully went through to the cup final on penalties after the match finished 3-3." • Question: You won't believe this own goal that was @placeholder in the Swiss lower league ! • Answer: (A) scored (B) born (C) eliminated (D) closed (E) beaten • Negative Augmented Choice: (F) scored (model predict "scored." Because it is the right answer, so we choose another choice "played" as an augmented choice. ) • Right Option: (A) scored • Wrong Option: (E) beaten • Potential Causes: It is quite weird that the original PLMs can predict the right answer, but fail to make it after fine-tuning. We suppose that in the process of fine-tuning, the inconsistency of abstract vocabulary prediction and the interference of other vocabulary caused the model's effect in some cases to decrease instead. • How to help models? We could use our approach of NAL to increase the weight of the knowledge learned in the pre-training task or leverage external knowledge <ref type="bibr" target="#b29">(Zhang et al., 2019</ref><ref type="bibr" target="#b28">(Zhang et al., , 2020b</ref><ref type="bibr" target="#b26">Yu et al., 2020;</ref><ref type="bibr" target="#b27">Zhang et al., 2020a)</ref>.</p><p>Case 3 -Obscure abstract word meaning • Passage: " ...Mr Habgood said: "We're pretty sure it will be popular because it was when East Street was closed for other reasons and we want to make it a friendlier place to be. "It does fit with our larger objectives to improve the town and make it safer for cyclists and pedestrians." ..." • Question: Three busy town center streets are to be pedestrianised in a bid to improve @placeholder for shoppers and cyclists . • Answer: (A) opportunities (B) services (C) quality (D) disruption (E) safety</p><p>• Negative Augmented Choice: (F) access • Right Option: (E) safety • Wrong Option: (B) services • Potential Causes: Due the limit of GPU memory, we cannot put the long passage into the model once a time. So during the training, the model can only see a small chunk of the passage, so that it cannot get the global representation of the passage. • How to help models? We chunk those long sentences with the approach of the sliding window to help the model understanding the whole passage.</p><p>Case 4 -Hypernyms is not always right • Passage: " North Wales Fire and Rescue Service was called to Express Linen Services on Vale Road in Llandudno Junction just before 19:30 GMT on Thursday. North Wales Police said a man was treated at the scene for smoke inhalation. Police have asked people to avoid the area..." • Question: A number of @placeholder have been evacuated as firefighters tackle a blaze at a commercial laundry firm 's premises in Conwy county. • Answer: (A) families (B) properties (C) water (D) disruption (E) vehicles • Negative Augmented Choice: (F) homes • Right Option: (B) properties • Wrong Option: (A) families • Potential Causes: Hypernyms is the main focus of Subtask 2, the model may consider the "families" as the upper level of the "people" occur in the passage and choose the "(A) families" instead of the right answer "(B) properties". • How to help models? We try to use the proposed NAL to add more abstract words learned from the pre-training to mitigate this issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This paper presents our system design for the Se-mEval 2021 Task4. We propose a simple yet effective method called negative augmentation with language model. Comprehensive experiments demonstrate the effectiveness of our proposed approach. We also conduct case studies and investigate why the model fails to obtain the correct prediction. Note that language models are pre-trained from the huge corpus; recently, researchers have iden-tified the bias in the language model, which may mislead the model prediction. Our proposed negative augmentation with language model can help the model better discriminate candidates in finetuning, thus boost the performance. From another perspective, as depicts in Section 3.2, the language model without any fine-tuning gets 60+% accuracy in both Subtask 1 and Subtask 2. This indicates that bias exists in the datasets (Part of the abstract meaning can be obtained from the language model). More strong benchmarks should be constructed in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>A: (A) suffered (B) promoted (C) remains (D) wants (E) achieved P: ... Low vitamin D levels can lead to brittle bones and rickets in children. The figures from the HSCNI show a dramatic rise in Vitamin D prescriptions over the last 10 years: The data does not include Vitamin D bought over the counter... Q: Rickets does not have the ring of a 21st Century problem -it sounds more like the @placeholder of a bygone era . A: (A) horror (B) size (C) fate (D) tale (E) death</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>The procedure of Negative Augmentation with Language Model (NAL).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>System overview (Best viewed in color.). The top of the Figure refers to the normal fine-tuning of multi-choice models, ignoring the form of pre-training tasks. While the bottom of the Figure refers to our system with Negative Augmentation with Language Model (NAL), which uses the abstract words predict by the original PLM as negative candidates to augment fine-tuning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>FigureFigure 4 :</head><label>4</label><figDesc>Results (Accuracy) on Subtask 2 with the length of passage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Case 1 -</head><label>1</label><figDesc>Influenced by the original pre-training task • Passage: "...found the United States to have the highest number of sleep deprived students, with 73% of 9 and 10 year olds and 80% of 13 and 14 year olds identified by their teachers as being adversely affected. The BBC's Jane O'Brien reports." • Question: Sleep deprivation is a significant hidden factor in lowering the @placeholder of school pupils , according to researchers carrying out international education tests .• Answer: (A) morale (B) IQ (C) mortality (D) closure (E) achievement • Negative augmented choice: (F) intelligence • Right Option: (E) achievement • Wrong Option: (B) IQ • Potential causes: After pre-training on the large general domain corpus, PLMs have a huge bias on predicting the [MASK]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Examples of the SemEval 2021 Task 4.</figDesc><table><row><cell>Given a passage and a question, the model needs to</cell></row><row><cell>pick the best one from the five candidates to replace</cell></row><row><cell>@placeholder.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Statistics of the SemEval 2021 Task 4 dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Results (Accuracy) on Subtask 1.</figDesc><table><row><cell>Model</cell><cell>Dev</cell><cell>Test</cell></row><row><cell>Baseline</cell><cell></cell><cell></cell></row><row><cell>RoBERTa Large</cell><cell>86.7</cell><cell>-</cell></row><row><cell>ALBERT xxLarge</cell><cell>84.3</cell><cell>-</cell></row><row><cell>DeBERTa Large</cell><cell>87.7</cell><cell>-</cell></row><row><cell>Ours</cell><cell></cell><cell></cell></row><row><cell>RoBERTa Large + NAL</cell><cell>91.1</cell><cell>89.7</cell></row><row><cell>ALBERT xxLarge + NAL</cell><cell>89.3</cell><cell>88.6</cell></row><row><cell>DeBERTa Large + NAL</cell><cell>91.3</cell><cell>90.3</cell></row><row><cell>Ensemble</cell><cell>93.7</cell><cell>92.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Results (Accuracy) on Subtask 2.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>The Aurora Borealis, better known as the Northern Lights, was spotted across @placeholder of England on Sunday.</figDesc><table><row><cell>Example</cell><cell></cell></row><row><cell>Question:</cell><cell></cell></row><row><cell>Answer set</cell><cell>{(A) millions, (B) parts, (C) half, (D) isle, (E) remains}</cell></row><row><cell>NAL set</cell><cell>{all, half, parts}</cell></row><row><cell>Baseline</cell><cell>(C) half</cell></row><row><cell>Model with NAL</cell><cell>(B) parts</cell></row><row><cell>Question:</cell><cell>The BBC is providing live coverage of the Scottish National Party conference</cell></row><row><cell></cell><cell>in Glasgow. This live @placeholder has finished .</cell></row><row><cell>Answer set</cell><cell>{(A) results, (B) recording, (C) event, (D) action, (E) center}</cell></row><row><cell>NAL set</cell><cell>{blog, recording , stream}</cell></row><row><cell>Baseline</cell><cell>(B) recording</cell></row><row><cell>Model with NAL</cell><cell>(C) event</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>We can clearly see the negative options can help the model better understand the abstract meaning in the passage and question. Answers are bold in the</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/huggingface/transformers (version 3.3.0)</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgments</head><p>We want to express gratitude to the anonymous reviewers for their hard work and kind comments.</p><p>This work is funded by 2018YFB1402800/NSFC91846204/NSFCU19B2027.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Climbing towards NLU: On meaning, form, and understanding in the age of data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><forename type="middle">M</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Koller</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.740</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5185" to="5198" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Economically organized hierarchies in WordNet and the Oxford English Dictionary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Changizi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.740</idno>
	</analytic>
	<monogr>
		<title level="j">Cognitive Systems Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="214" to="228" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Neural Reading Comprehension and Beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.740</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.740</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020-07" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
	<note>Virtual Event</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.740</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Making pre-trained language models better few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.740</idno>
		<idno>abs/2012.15723</idno>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Don&apos;t stop pretraining: Adapt language models to domains and tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ana</forename><surname>Suchin Gururangan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swabha</forename><surname>Marasović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.740</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8342" to="8360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.740</idno>
		<idno type="arXiv">arXiv:2006.03654[cs].ArXiv:2006.03654</idno>
		<title level="m">DeBERTa: Decoding-enhanced BERT with Disentangled Attention</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomás</forename><surname>Kociský</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.740</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-07" />
			<biblScope unit="page" from="1693" to="1701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">ALBERT: A lite BERT for self-supervised learning of language representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.740</idno>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Roberta: A robustly optimized BERT pretraining approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.740</idno>
		<idno>abs/1907.11692</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Fixing Weight Decay Regularization in Adam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.740</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">When does label smoothing help?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.740</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08" />
			<biblScope unit="page" from="4696" to="4705" />
		</imprint>
	</monogr>
	<note>Hinton</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Köpf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.740</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08" />
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Association for Computational Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Bakhtin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.740</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2463" to="2473" />
		</imprint>
	</monogr>
	<note>Language models as knowledge bases?</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Tim Salimans, and Ilya Sutskever</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Know what you don&apos;t know: Unanswerable questions for SQuAD</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="784" to="789" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Coqa: A conversational question answering challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Assoc. Comput. Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="249" to="266" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Contrastive learning with hard negative samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Yao</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suvrit</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno>abs/2010.04592</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Parameters of abstraction, meaningfulness, and pronunciability for 329 nouns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otfried</forename><surname>Spreen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolph</forename><forename type="middle">W</forename><surname>Schulz</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0022-5371(66)80061-0</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Verbal Learning and Verbal Behavior</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="459" to="468" />
			<date type="published" when="1966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">How to Fine-Tune BERT for Text Classification?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yige</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.05583[cs].ArXiv:1905.05583</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Literal and metaphorical sense identification through concrete and abstract context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Neuman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Assaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yohai</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Edinburgh, Scotland, UK.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="680" to="690" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Machine comprehension using match-lstm and answer pointer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<idno>abs/1608.07905</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; BC, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Vancouver</publisher>
			<date type="published" when="2019-12-08" />
			<biblScope unit="page" from="5754" to="5764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bridging text and knowledge with multi-prototype embedding for few-shot relational triple extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shumin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbin</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics<address><addrLine>Barcelona, Spain (Online)</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-08" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="6399" to="6410" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Summarizing chinese medical answer with graph convolution networks and question-focused dual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shumin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.2</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, EMNLP 2020</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, EMNLP 2020</meeting>
		<imprint>
			<date type="published" when="2020-11-20" />
			<biblScope unit="page" from="15" to="24" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Relation adversarial network for low resource knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shumin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanlin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaoyan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1145/3366423.3380089</idno>
	</analytic>
	<monogr>
		<title level="m">WWW &apos;20: The Web Conference 2020</title>
		<meeting><address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<publisher>ACM / IW3C2</publisher>
			<date type="published" when="2020-04-20" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Long-tail relation extraction via knowledge graph embeddings and graph convolution networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shumin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanlin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanying</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3016" to="3025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianghuai</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kangping</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nengwei</forename><surname>Hua</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.10813</idno>
		<title level="m">Conceptualized representation learning for chinese biomedical text mining</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">SemEval-2021 task 4: Reading comprehension of abstract meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyuan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuping</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen-Hua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th International Workshop on Semantic Evaluation</title>
		<meeting>the 15th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>SemEval-2021</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Evaluating commonsense in pretrained language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuhui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leyang</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandan</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="9733" to="9740" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
