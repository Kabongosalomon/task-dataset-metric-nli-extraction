<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Code Prediction by Feeding Trees to Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seohyun</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinman</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchi</forename><surname>Tian</surname></persName>
							<email>yuchi.tian@columbia.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satish</forename><surname>Chandra</surname></persName>
							<email>schandra@acm.org</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Facebook Inc. U.S.A</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Wisconsin-Madison</orgName>
								<address>
									<country key="US">U.S.A</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Columbia University</orgName>
								<address>
									<country key="US">U.S.A</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Facebook Inc</orgName>
								<address>
									<country key="US">U.S.A</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Code Prediction by Feeding Trees to Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we describe how to leverage Transformer, a recent neural architecture for learning from sequential data (such as text), for code completion. As in the realm of natural language processing, Transformers surpass the prediction accuracy achievable by RNNs; we provide an experimental confirmation of this over a Python dataset.</p><p>Furthermore, we show that the way to obtain even better accuracy from Transformers is to expose the syntactic structure of code, which is easily recovered by parsing, to the neural network. This works significantly better than presenting the code as a linear token sequence, which is how Transformers were originally intended to be used.</p><p>To accomplish this, we propose a novel enhancement to the selfattention mechanism of the Transformer. We enable the mechanism to learn weights-that is, how much to focus on each preceding token in the input-not only on the basis of a token's value, but also on the basis of the spatial relationships, as in their positions in the abstract syntax tree, between each pair of tokens.</p><p>We provide comprehensive experimental evaluation of our proposal, along with alternative design choices, on a standard Python dataset, as well as on a Facebook internal Python corpus.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Last several years have witnessed exciting progress in the application of machine learning techniques to developer productivity tools <ref type="bibr" target="#b3">[5]</ref>, and in particular, to code prediction <ref type="bibr" target="#b11">[13,</ref><ref type="bibr" target="#b20">22,</ref><ref type="bibr" target="#b24">26,</ref><ref type="bibr" target="#b32">34]</ref>. The idea of code prediction in general is to predict the next code element given previously written code. Code prediction is commonly used in an IDE for auto-complete, where based on the developer's cursor position and the code already written up to the cursor position, the IDE offers the most likely next tokens (perhaps as a drop down list to choose from.) Auto-complete, not only saves the developer from having to type in the next token(s), but is also an effective code learning mechanism: for instance, a developer might not know the name of an API call he needs off the top of his head, but is able to choose among the choices shown by an auto-complete tool.</p><p>Recent work has shown the promise of code prediction based on machine learning. A common idea here is to use language models trained over large code corpora-treating code as text in a natural language <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b20">22]</ref>-to enable highly accurate code prediction. These models have leveraged natural language processing techniques: * Both authors contributed equally to this research. n-grams <ref type="bibr" target="#b18">[20,</ref><ref type="bibr" target="#b20">22]</ref>, and more recently, deep neural networks such as RNNs <ref type="bibr" target="#b24">[26,</ref><ref type="bibr" target="#b27">29,</ref><ref type="bibr" target="#b30">32]</ref>.</p><p>A different line of work has proposed code prediction based on statistics on syntactic structure of code, as opposed to seeing code as text. These include probabilistic context-free grammars and probabilistic higher-order grammars <ref type="bibr" target="#b9">[11,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" target="#b33">35]</ref>. This class of models considers code artifacts as abstract syntax trees, and make their predictions based on information gleaned selectively across paths in the code's AST. Specifically, Raychev et al. <ref type="bibr" target="#b32">[34]</ref> learn a decision tree model that uses this information essentially as features.</p><p>Researchers in the NLP community have recently developed Transformers, a new neural architecture for even more effective natural language processing <ref type="bibr" target="#b38">[40]</ref>. As we discuss later, Transformers promise to overcome some of the limitations of RNNs. We investigated the use of Transformers for code prediction, treating code as textual data, and validated experimentally that Transformers indeed outperform RNNs on the next code token prediction task.</p><p>Given this already strong baseline, we consider the question of whether informing the Transformer of code's syntactic structure can further improve prediction accuracy. Our main result is that a better way to use transformers for code prediction is to expose the syntactic structure of code to the network. The details of how to do this are interesting, as encoding the structure of a program's abstract syntax tree is not natural for sequence models. We show a range of design choices for communicating the AST structure to the Transformer. We find that the more faithfully we communicate the tree structure to the Transformer, the better the accuracy we obtain!</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Key Results</head><p>We report results based on training and evaluating various models code prediction on the py150 [1] dataset.</p><p>• We show that a neural model based on the Transformer architecture is able to outperform state-of-the-art neural (e.g. RNN-based (e.g. in <ref type="bibr" target="#b19">[21,</ref><ref type="bibr" target="#b23">25]</ref>) as well as non-neural models (e.g. Deep3 <ref type="bibr" target="#b32">[34]</ref>). Measured on the leaf tokens of the ASTs, our best Transformer model improves the mean reciprocal rank (reported as a percentage, see Sec 5) significantly over the prior work: upon the RNN model <ref type="bibr">(40.</ref>0% v 55.5%) as well as upon the corresponding Deep3 model <ref type="bibr">(43.</ref>9% v 73.6%). • We show that a key to obtaining superior performance from the Transformer model is to feed not just the source token sequence as is common in NLP tasks, but in making the Transformer aware of the syntactic structure of the code. We show that with more detailed syntactic structure, we get better accuracy (from 65.7% to 74.1% on leaf tokens). We provide a preliminary investigation into why the Transformer model that is aware of tree structure works better than one without, by using saliency maps <ref type="bibr" target="#b36">[38]</ref>. • Our key technical novelty is a novel enhancement to the Transformer's self-attention mechanism. We enable the mechanism to learn weights-how much to focus on each preceding token in the input-by factoring in the spatial relationship in the abstract syntax tree between each pair of tokens. • We also evaluated our trained model on a dataset selected from a Python code repository internal to Facebook, and found the relative benefits to be similar to those on py150. The accuracy on this other corpus indicates that the Transformer model is generalizable to other corpora.</p><p>Outline. Sec 2 articulates the code prediction problem in a couple of different forms, and introduces a running example. Sec 3 gives an introduction to Transformers, including how they would apply to source code. This section also describes how to communicate tree structure to the transformer. Sec 4 provides a quick recap of the previous work, focusing on the ones against which we compare our models. Sec 5 describes our datasets and implementation. Sec 6 presents our quantitative results. Sec 6.4 takes a closer look into why our models worked well (or did not.) Sec 7 discusses related work in the area of code prediction and in using Transformers. We conclude the paper with our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">CODE PREDICTION</head><p>Consider the Python code fragment in <ref type="figure" target="#fig_0">Fig 1.</ref> Suppose a developer has written code up to string following by a dot. At this point, it will be helpful for the IDE to prompt the developer with attribute names that are likely to follow, preferably, with atoi ranked at the top because in this case that is the correct next token.</p><p>Our goal is to devise a model such that it takes some code fragment as input and predicts the next token. In this section, we describe two main methods of representing code as inputs to be fed into various models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Sequence-based Representation</head><p>In NLP, a common way of feeding in information for next token prediction is with a linearized token sequence. The same technique can be applied with source code, where we parse the source code 1 data/JeremyGrosser/supervisor/src/supervisor/medusa/test/test_11.py into tokens. To predict "atoi", we would look at the tokens: [..., "map", "(", "string", "."]. This is a natural approach for next token prediction since each input and prediction in the sequence equates to a token in source code, so we can easily evaluate on all tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">AST-based Representation</head><p>An alternate to a source token sequence is the Abstract Syntax Tree (AST), as shown in <ref type="figure" target="#fig_1">Fig 2 for</ref> the code fragment in <ref type="figure" target="#fig_0">Fig 1.</ref> An AST can better represent spatial relationship between nodes. For example, in source code, the tokens ip (node 3) and chain (node 41) are separated by 30 tokens, but they are related in the AST via a specific (short) path.</p><p>ASTs represent some source tokens explicitly and others implicitly. Tokens corresponding to identifiers, field names, and constants appear explicitly as leaf (terminal) nodes: for instance, ip and host appear as the leaf (terminal) nodes 3 and 11, respectively. Keywords and other syntactic tokens (e.g. =) are implied by the type of internal nodes (e.g. Assign). Accordingly, the prediction task can be separated into:</p><p>• Value prediction: Predicting the values at leaf nodes. For example, given nodes 0-10 of the tree, we want to predict host, which is the value of the leaf node at node 11. • Type prediction: Predicting the types at internal nodes. For example, given nodes 0-33 of the tree, we want to predict Attr, which is the type of the internal node at node 34. Knowing that the type of a node is Attr implies that after the source tokens corresponding to its left child, there will be a token "." (dot) before the (single) token from its right child. Thus, value prediction and type prediction together can simulate next token prediction problem, though there will need to be a stack-based controller that would call the right predictor, maintain some state, and emit the predicted source tokens appropriately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">A preview of results</head><p>In this paper, we explore both sequence-based and AST-based representation of code for code prediction, using various models (RNN, Decision Tree, Transformers). <ref type="table" target="#tab_2">Table 1</ref> shows the ranks (lower is better) of predicting the correct leaf node for all the leaf nodes in the AST in <ref type="figure" target="#fig_1">Fig 2.</ref> It compares two models of previous work and four Transformer-based models (our work). Transformer models generally achieve lower ranks, and in some cases they are the only models that produce the right token in their top-10 predictions. This table also shows (via one example here, but the results carry over) that feeding ASTs to Transformer models brings better results than feeding them source token sequences. The core of our paper is about how to feed ASTs to Transformers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">TRANSFORMERS FOR CODE PREDICTION</head><p>In this section, we explain the four models of our own creation: SrcSeq , RootPath , DFS , DFSud . All four models use Transformers <ref type="bibr" target="#b38">[40]</ref>, a class of deep learning models that have achieved the state-of-the-art results <ref type="bibr" target="#b14">[16,</ref><ref type="bibr" target="#b15">17,</ref><ref type="bibr" target="#b31">33]</ref> for a variety of NLP tasks such as language modeling, question answering, and sentence entailment. In this section, we discuss how we can apply Transformers for next code token prediction, feeding in both sequence-based (SrcSeq ) and AST-based (RootPath , DFS , DFSud ) inputs.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">A Primer on Transformer</head><p>Transformers belong to a class of deep neural networks that are designed for sequence processing. Transformers eschew the hidden states of earlier generation sequence networks (such as RNNs, see Sec 4) in favor of exposing the entire input sequence simultaneously, solely relying on attention mechanisms. In Transformers, information from any previous location of the sequence can directly affect the encoding of the next token, through a mechanism called self-attention, which helps greatly improve the connectivity in long sequences. Transformers also uses multiple heads of these self-attention blocks, called multi-headed attention, which enables the model to simultaneously consider different ways of attending to previous information within one block and also across other blocks. This section explains self-attention in detail ( <ref type="figure">Figure 3</ref>), as it is the crux of the model. The purpose of self-attention is to give higher attention to more relevant tokens in the input sequence. To illustrate this, let's take an example input sequence: ["map", "(", "string", "."], and the target token being "atoi." This input sequence is first fed through the initial Embedding layer to give: E = e map , e ( , e string , e . . Then, this embedding is used as input to three fully-connected networks (W q ,W k ,W v ) to create a query, key, and value embedding for the sequence:</p><formula xml:id="formula_0">Q = EW q , K = EW k , V = EW v</formula><p>In our example, Q = q map , q ( , q string , q . , K = k map , k ( , k string , k . , and V = v map , v ( , v string , v . . We use the query vector Q to "query" the "keys" K to see which token relationships are the most important by calculating QK ⊺ . This results in a matrix of size n x n, as seen in <ref type="table">Table 2</ref>, where n is the length of the input sequence. Each row is then normalized (by square root of d k ) and passed through a softmax layer so all the scores are positive and add up to 1. <ref type="table">Table 3</ref> shows an example of the self-attention weights 2 ; looking at the last row, we can see that most of the self-attention is given to ".", meaning it has a greater factor in predicting the next token "atoi". Also note how the matrix is a lower triangular matrix -this is because self-attention cannot be applied to tokens that have not been seen before. Finally, this matrix is multiplied with the value vector to weight the token embeddings:</p><formula xml:id="formula_1">A = Attn(Q, K, V ) = softmax( QK ⊤ d k )V , In our example, A = 0.2 * v map , 0.1 * v ( , 0.2 * v string , 0.4 * v .</formula><p>. A is then fed through a fully-connected network, coupled with skip connections and layer normalizations. This process is repeated num_layer number of times. Finally, the output of the last layer Figure 3: Schematic of a GPT2 Transformer. The selfattention layer is able to consider all tokens in the input up to the point of prediction. Here the self-attention box depicts the information flow when predicting next token after the "."; see <ref type="table">Table 3</ref> for where the numbers come from. <ref type="table">Table 2</ref>: Matrix for calculating the self-attention "scores" for each token combination in the input sequence for Transformers. We use the query vector Q to "query" the "keys" K to see which tokens are the most relevant for the next token prediction. The matrix multiplication is calculated with QK ⊺ . <ref type="table">Table 3</ref>: Example matrix for the numerical self-attention "scores" after taking the softmax over the normalized values in <ref type="table">Table 2</ref>. Note that the rows listed here do not sum up to exactly 1 since there are previous tokens in the input sequence (not shown in this matrix) that self-attention gives scores to as well.</p><formula xml:id="formula_2">... map ( string . map q map k map ( q ( k map q ( k ( string q st r inд k map q st r inд k ( q st r inд k st r inд . q . k map q . k ( q . k st r inд q . k .</formula><formula xml:id="formula_3">... map ( string . map 0.9 ( 0.6 0.1 string 0.1 0.1 0.7 . 0.2 0.1 0.2 0.4</formula><p>goes through a classification layer at the end to generate predictions for the next token.</p><p>For other details, please refer to Vaswani et al. <ref type="bibr" target="#b38">[40]</ref> (especially the multi-head attention part) and in particular, GPT-2 <ref type="bibr" target="#b31">[33]</ref>, for a more thorough description.</p><p>The next sections discuss various ways of feeding code fragments into this Transformer architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">SrcSeq</head><p>Our first attempt is to apply a Transformer over source token sequences. As a baseline for later models that takes more tree information, as well as a straightforward application of Transformer models, we apply a Transformer (GPT-2) over source token sequences:</p><formula xml:id="formula_4">o = Trans(e t ), t ∈ source_tokens</formula><p>where o is the output of the Transformer to be used for prediction, and e represents the embedding of the source tokens. It does next token prediction by taking all preceding source tokens, up to the point of prediction, as input. As the inputs and outputs are the same as the SrcRNN model (introduced in the next section), we can do a direct comparison between RNNs and Transformers. As we show in the experiments, this turns out to be an already strong baseline.</p><p>The next two subsections discuss how to present the AST to the Transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">DFS</head><p>One way to present all AST nodes to a Transformer is to linearize them in the using a pre-order traversal, or a depth-first-search (DFS). For <ref type="figure" target="#fig_1">Fig 2,</ref> for node 29, the previous nodes in DFS order would be: [..., "Call", "NameLoad", "map", "AttributeLoad", "NameLoad", "string", "Attr"]</p><p>The DFS model simply feeds this sequence to the Transformer:</p><formula xml:id="formula_5">o = Trans(e t ), t ∈ AST _nodes</formula><p>where o is the output of the Transformer to be used for prediction, and e represents the embedding of the AST nodes. DFS predicts the next node in the AST; thus, it does both value (leaf) prediction and type (internal) prediction. DFS presents the tree nodes in a pre-determined order, but still does not retain detailed structural relationship between nodes. For example, consider the sequence of nodes 26 -28 in <ref type="figure" target="#fig_1">Fig 2.</ref> This would be represented as ["NameLoad", "string", "attr"], the three nodes appearing consecutively in DFS order. Looking at the AST, we can see that the relations between ("NameLoad" &amp; "string", and "string" &amp; "attr") are actually quite different: "NameLoad" is one node up from "string", while "string" is two nodes up and one node down from "attr". This path-based relation between the nodes provides richer information about the actual structure of the tree.</p><p>While DFS itself shows only a small improvement on SrcSeq (Table 6), it allows us to augment it with the richer information indicated above, leading to the DFSud model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">DFSud</head><p>DFSud is an extension to the DFS model that incorporates more tree structure. Specifically, given any two nodes a and b in the AST, we want to capture the shortest path needed to reach from a to b, and communicate this to the Transformer. The path from a to b is <ref type="bibr" target="#b23">25</ref> 26 <ref type="table">Table 4</ref>: Matrix for calculating the self-attention "scores" for DFSud . Matrix R, which contains the up down path information, is multiplied with QK ⊺ , from the traditional Transformer. In this example, node 25 represents "AttributeLoad", 26 is "NameLoad", 27 is "string", and 28 is "Attr".</p><formula xml:id="formula_6">27 28 25 U 1 (q 25 k 25 ) 26 U 2 (q 26 k 25 ) U 1 (q 26 k 26 ) 27 U 1 (q 27 k 25 ) U 1 D 1 (q 27 k 26 ) U 1 D 2 (q 27 k 27 ) 28 U 2 (q 28 k 25 ) U 2 D 1 (q 28 k 26 ) U 2 D 2 (q 28 k 27 ) U 1 (q 28 k 28 )</formula><p>represented abstractly only in terms of up and down moves:</p><formula xml:id="formula_7">UDpath(a, b) = U i D j</formula><p>where i, and j are the number of up and down nodes, respectively, node a has to travel to reach node b. <ref type="bibr" target="#b1">3</ref> We create a matrix R to contain UDpath(a, b) for each pair of nodes (a, b), where a comes after b in DFS order. <ref type="table">Table 4</ref> (ignoring the qk parts inside the parenthesis) shows an example of the R in the context of our running example (nodes 25-29 in the AST). <ref type="bibr" target="#b2">4</ref> Notice that this matrix has the same shape (lower triangular matrix) as the QK matrix in <ref type="table">Table 2</ref>. We add in the R matrix in the Attn block (after passing by an embedding layer):</p><formula xml:id="formula_8">Attn TreeRel (Q, K, V , R) = softmax( R ⊙ (QK ⊤ ) d k )V (1)</formula><p>where ⊙ is element-wise product. <ref type="table">Table 4</ref> shows an example of the new self-attention, R ⊙ (QK ⊤ ). One detail to note here that R(a, b) = UDpath(a + 1, b) since we want the path relations to be relative to the next token we are predicting.</p><p>The rest of the Transformer model is the same as DFS 's, with the updated Attn TreeRel calculation:</p><formula xml:id="formula_9">o = Trans ud (e t , R), t ∈ AST _nodes</formula><p>where o is the output of the Transformer to be used for prediction, e represents the embedding of the AST nodes, and R represents the embedding of the UDpath relations.</p><p>Why might adding R help the model do better? Note that QK ⊤ provides a way for the model to learn the strength of attention it needs to pay to previous tokens, organized in the order of inputs to the network (this order is implicit in the indices used in the matrix in <ref type="table">Table 3</ref>.) R provides a way for the model to learn the strength of the attention to pay to previous tokens, considering the AST relationship between pairs of nodes as well.</p><p>To recap, our key insight is to fortify the self-attention mechanism of the Transformer to enable it to learn weights on the basis of AST relationships between tokens as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Variations of Models</head><p>In this section, we discuss some alternate models and variations of models we have explored.</p><p>RootPath . RootPath is an AST-based model that feeds tree structure information to the model in an alternate way than DFS does. RootPath first creates a sequence based on the leaf nodes of the AST. To expose tree structure to the Transformer, it fortifies each leaf node with the path from the leaf node to the root of the AST by traversing up its ancestors; we call such a path to be root-path. <ref type="figure" target="#fig_1">For Fig 2,</ref> for node 29, the root-path would be: [..., (["NameLoad", "Call", ... "Module"], âĂĲmapâĂİ), (["NameLoad", "AttributeLoad", "Call", ..., "Module"], âĂĲstringâĂİ), (["Attr", "AttributeLoad", "Call", ..., "Module"], ?) ]</p><p>The root-paths are first fed into a sequence encoder (such as an LSTM), coupled with the leaf node, and is fed through the Transformer:</p><formula xml:id="formula_10">o = Trans(e t + LSTM(P t )), t ∈ leaf _nodes</formula><p>where o is the output of the Transformer to be used for prediction, and e represents the embedding of the leaf nodes, and P is the embedding for all the root-paths. Since RootPath predicts only leaf nodes, it does only value prediction.</p><p>LeafTokens . LeafTokens is a lightweight variation of Root-Path , where only the leaf nodes are used. <ref type="figure" target="#fig_1">For Fig 2,</ref> for node 29, the input sequence would be: [..., "map", "string"], and would predict "atoi". LeafTokens feeds in the leaf nodes of the AST into a Transformer:</p><formula xml:id="formula_11">o = Trans(e t ), t ∈ leaf _nodes</formula><p>where o is the output of the Transformer to be used for prediction, and e represents the embedding of the leaf nodes. We compare this model with RootPath to determine the importance of root-path information in next token prediction.</p><p>DFSud+ . DFSud+ is a variation to DFSud that uses a richer vocabulary to the up-down paths to include some child index information, as it provides extra information about the tree structure. While DFSud uses only U and D to describe the relation between two nodes in the AST, DFSud+ expands D into three sub words: D first , D last , D middle ; this describes whether the node is either the first child, the last child, or somewhere in in between, respectively. For example, in <ref type="table">Table 4</ref>, the new relation for node 27 and 27 would expand from U 1 D 2 into U 1 D first D first . We chose this minimal extension to limit the possible exponential growth in path vocabulary size; even with this minor extension, our path vocabulary increases from 250 to 100k to cover more than 90 % of the vocab (with a long right tail). The rest of the model is same as DFSud , as described in Sec 3.4. We compare this model with DFSud to examine whether adding in more information (at the expense of enlarging the model) improves MRR.</p><p>A high-level overview of the models is presented in <ref type="table">Table 5</ref>. The next section will cover two previous models from literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><p>Model  <ref type="table">Table 5</ref>: Overview of the models presented in this paper. The first two are models from previous work using RNN and Decision Tree, and remainder are models of our own creation that uses a Transformer (the last three are exploratory and variations). The models differ in the type of prediction task, and in what the model inputs and predicts. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">BACKGROUND ON PREVIOUS WORK</head><p>In this section, we recap two different methods for code prediction, representative of recent previous work, against which we compare our work. These are (1) a method based on language models that uses a sequence of source code tokens, and (2) a method based on decision trees <ref type="bibr" target="#b32">[34]</ref> that works on ASTs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Language Model based prediction</head><p>A language model computes the probability of the next word w t +1 , given some window of preceding words:</p><formula xml:id="formula_12">P(w t +1 |w t w t −1 w t −2 . . .).</formula><p>Here we use an RNN to compute a language model; n-grams would be another choice. 5 <ref type="figure">Fig 4</ref> shows an Recurrent Neural Network (RNN) operating on some of the tokens from the example in <ref type="figure" target="#fig_0">Fig 1.</ref> As the name suggests, RNNs consume input tokens recurrently, one per time step, and produce output tokens one per time step as well. The bottom layer of the RNN embeds input tokens into a vector: x t = emb(w t ), where w t is the source token seen at the t'th time step. The hidden state h t is computed as h t = W xh x t + W hh h t −1 , using both x t and the hidden state from the previous time step. The output is a vector of probabilities of various tokens computed by using softmax over y t = W hy h t ; the diagram shows the top-ranked predictions or <ref type="bibr" target="#b3">5</ref> The jury seems to be out on which one is necessarily better for the task <ref type="bibr" target="#b19">[21,</ref><ref type="bibr" target="#b23">25]</ref>.  the ground truth. W hh , W xh and W hy are the parameters of the network, to be learned during training.</p><p>The pertinent point to note is that the hidden state h t encodes the knowledge of not just the current token, but of last several previous tokens via the propagation of information in previous hidden states. Thus, RNNs implicitly compute a language model over tokens.</p><p>A limitation of RNNs is the difficulty they have in tracking longrange dependence, even with various proposals to mitigate the problem (e.g. long-short-term-memory (LSTM) cells <ref type="bibr" target="#b21">[23]</ref>, which we do use in our implementation, attention on top of RNNs <ref type="bibr" target="#b22">[24]</ref>, and skip-connections between sequence locations <ref type="bibr" target="#b39">[41]</ref>).</p><p>In our experiments, we feed the source code tokens into an RNN and call this model SrcRNN .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Decision Tree based prediction</head><p>Raychev et al. <ref type="bibr" target="#b32">[34]</ref> presented a system, Deep3, based on a learned decision tree combined with count-based probabilities at the leaves of the decision tree. We provide only a sketch here, highlighting how they use paths on an AST. <ref type="figure" target="#fig_3">Fig 5 shows</ref> part of a learned decision tree, written in the form of program in a specialized language they call TGEN. Given an AST t and a starting node n, a TGEN program walks certain paths in t starting from n. For example, Up WriteValue (line 1) goes to the parent of n and records the label. If the label is Attr, it walks a different path (line 2) in the vicinity of n. The branch outcomes and observations collected by running this TGEN program on (t, n) form a context, which is then used to look up a probability distribution conditioned on that context. For the AST in <ref type="figure" target="#fig_1">Fig 2,</ref> starting with node 29, the TGEN program will produce a context for which the probabilities of different tokens for node 29 might be: [atoi: 40%, length: 20%, ...]. The flexibility of focusing on arbitrary paths in the AST allows the model to condition selectively on nodes farther back in the AST.</p><p>A TGEN program is learned-on a specific corpus-by a genetic search procedure that simultaneously selects paths and grows the decision tree from the training data, with an entropy minimization objective. The details are not important for this paper; in this paper, we use their pretrained model <ref type="bibr" target="#b0">[2]</ref> as well as their Python dataset <ref type="bibr">[1]</ref> for our experiments.</p><p>The reader will notice that the notion of UDpath in Section 3.4 is akin to the AST paths expressed in TGEN programs. The paths in TGEN are more general, but at a high-level, the idea that certain "spatial" relation between nodes is important is common to both approaches. This, along with the competitive quality of results of the Deep3 model in <ref type="table" target="#tab_2">Table 1</ref>, makes it an interesting comparison. We explore this similarity further in Appendix B.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">IMPLEMENTATION AND DATASETS 5.1 Dataset</head><p>We train our models using the py150 dataset [1] used in Raychev et al. <ref type="bibr" target="#b32">[34]</ref>. The dataset consists of 150k Python 2 source code files from GitHub repositories, along with their parsed ASTs, split into 100k for training and 50k for evaluation. From the ASTs extracted from the py150 dataset, we modify the AST to ensure that the internal nodes only has types and the leaf nodes only have values. For implementation details, please refer to AppendixA.1. To incorporate large trees (greater than 1000 nodes), we deploy a technique adopted by <ref type="bibr" target="#b2">[4]</ref>, which slices a large tree into shorter segments with a sliding window to maintain part of the previous context. For implementation details, please refer to AppendixA.2.</p><p>We evaluate our models on two evaluation datasets:</p><p>• py150: We use the evaluation dataset used in Raychev et al. <ref type="bibr" target="#b32">[34]</ref>, which consists of 50k Python ASTs. We perform the two modifications as listed above before feeding them into our models, there are 16,003,628 leaf nodes and 30,417,894 internal nodes. • internal: We also created an evaluation dataset consisting of 5000 Python files from a code repository internal to Facebook. With this dataset, we can evaluate how our trained model can generalize to a different dataset, even if the code comes from disjoint projects. After the modifications, there are 1,669,085 leaf nodes and 3,067,147 internal nodes.</p><p>Recent works <ref type="bibr" target="#b19">[21,</ref><ref type="bibr" target="#b23">25]</ref> have divided evaluations into static and dynamic, where in the dynamic evaluations, the model continues to update its parameters during evaluation. This may increase accuracy by having the model adapt to the characteristics of the evaluation dataset. In our experiments, we choose to evaluate statically, and realize that evaluating dynamically may improve accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Implementation</head><p>Transformers. . For the model that use Transformers (RootPath , DFS , SrcSeq , DFSud ), we adapt the Pytorch implementation 6 of GPT-2 small <ref type="bibr" target="#b31">[33]</ref>. We use six Transformer blocks, six heads in each block, n c tx = 1000, and set embedding dimension d model = d k = d q = 300. We borrow other hyperparameters from Radford et al. <ref type="bibr" target="#b31">[33]</ref>. We limit the token vocabulary size to 100k, which covers over 90% of the tokens used in the training dataset. For DFSud , we limit the vocabulary to 250, which covers over 95% of the path relations. For RootPath , we limit the maximum length of the path from leaf node to root to be 13, which covers over 90% of the nodes. For any path longer than 13, we keep the nodes closest to the leaf, and truncate the nodes near the root.</p><p>RNN. For the SrcRNN model, we adapt the PyTorch example implementation 7 of a word-level language model LSTM. We use embedding dimension d model = 300, with dropout = 0.5 and n_layers = 1. We limit the token vocabulary size to 100K, which covers over 90% of the tokens.</p><p>Deep3. For the Deep3 model, since the authors have shared only the model and not the training algorithm, we used the model pretrained on py150.</p><p>We trained all models (except Deep3) on Nvidia Tesla V100 (using 4 GPUs at a time) until the loss converged, with all of the parameters randomly initialized. We used the Adam optimizer with the learning rate set to 1e-3. For convergence, DFS took 11 epochs, DFSud took 21 epochs, SrcSeq took 9 epochs, and SrcRNN took 9 epochs (each epoch took around 45 minutes -1 hour).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Evaluation Task</head><p>We evaluate the models on the code prediction tasks that we defined in Sec 2: next token prediction, which pertains to source code tokens taken as a linear sequence; value prediction, which pertains to predicting leaf nodes of the AST; and type prediction, which pertains to predicting internal nodes of the AST.</p><p>To measure performance on these tasks, we use mean reciprocal rank (MRR). The rank is defined as</p><formula xml:id="formula_13">MRR = 1 n n i=1 1 rank i<label>(2)</label></formula><p>where n is the number of predicting locations and rank i is the rank of the correct label given by the model for the i th data point. We present MRR as a percentage, in keeping with prior work <ref type="bibr" target="#b19">[21,</ref><ref type="bibr" target="#b23">25]</ref>. While Acc@1 only gives score when the correct label is ranked at the top, MRR also give scores when the true label is not ranked as the top, but among top few prediction. Comparing to the hit-ormiss style metric (Acc@1), this is closer to the realistic scenario when completion suggestions are presented to developers. With this practical perspective and for ease of computation, we only consider rank i ≤ 10 for each location i (all rank i &gt; 10 will have a score of 0).</p><p>We share our data processing scripts and model implementations at https://github.com/facebookresearch/code-prediction-transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EVALUATION 6.1 Research Questions</head><p>At a high level, we want to answer the following research questions.</p><p>RQ1 Overall, do Transformer-based models provide better accuracy compared to prior state-of-the-art methods of code prediction? RQ2 Does syntactic structure of code help get better accuracy out of Transformers, and if so, by how much? RQ3 What did the Transformer model variants learn from the code? Did they learn the right things? What can we learn from the learned models? We describe the experiments to answer the research questions RQ1 and RQ2. We discuss the evaluation of RQ3 in Section 6.4.</p><p>For RQ1, recall that prior work (Section 2) works on two different kinds of inputs: all source tokens as in program text, and ASTs of each program unit. To carry out a direct comparison against prior work, we split RQ1 into two specific questions: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Results</head><p>Our main evaluation results are reported in <ref type="table" target="#tab_8">Table 6 and Table 7</ref>.</p><p>Q1. For RQ1.1, see the SrcRNN and SrcSeq columns in <ref type="table" target="#tab_8">Table 6  and Table 7</ref>. For the py150 dataset, we can see a significant improvement in MRR, from 65.7% to 74.1% for the SrcRNN and Src-Seq models, respectively. The same holds for comparing on the internal dataset: 57.4% vs 66.8%. <ref type="table" target="#tab_11">(Table 9</ref> and 11 in the Appendix B.1 break down the data for different kinds of next token predictions.) Not surprisingly, <ref type="table" target="#tab_8">Table 6</ref> also shows that predicting the identifier and constant tokens (as in value prediction) is more challenging than predicting the keywords and punctuation tokens, which form almost 2/3 of all the source tokens.</p><p>For RQ1.2, we compare the Deep3 model against DFS and DF-Sud models. Overall, we found that all the Transformer models (SrcSeq , DFS DFSud ) achieve higher scores compared to Deep3. <ref type="table" target="#tab_8">Table 6</ref> shows that DFSud achieves the best MRR of 73.6 for leaf node prediction compared with Deep3's MRR of 43.9. Similar results can be seen for the internal dataset, as shown in <ref type="table" target="#tab_9">Table 7</ref>.</p><p>Q2. To answer RQ2.1, we compare the value prediction results for SrcSeq against the AST-based models (DFS , DFSud ). <ref type="table" target="#tab_8">Table 6</ref> shows that DFS outperforms SrcSeq by 7.9%, and DFSud significantly outperforms SrcSeq by 23.5% (73.6% vs 50.1%). These results demonstrate that representing the source code as AST vs linearized source code provides better results for next value prediction.</p><p>For RQ2.2, we compare the results amongst the AST-based models. First, comparing DFS and DFSud , DFSud provides more detailed structural information. <ref type="table" target="#tab_8">Table 6</ref> shows significant improvements to the accuracy, achieving 15.6% higher MRR for value prediction and 9.4% higher MRR for type prediction than DFS . Similar trends can be seen for the internal data set in <ref type="table" target="#tab_9">Table 7</ref>. <ref type="table" target="#tab_10">Table 8</ref> shows a significant drop in accuracy between Root-Path and LeafTokens (55.1% vs 41.9% for all leaf nodes). This shows that the information captured by the leaf to root paths (both in terms of its values and tree structural information) gives a solid boost to accuracy. These results demonstrate that feeding the model with more structural information does improve results.</p><p>Next, we compare RootPath and DFS . These models are similar because both models take all of the AST nodes as the context, but are different in how they digest the context. RootPath first aggregates the context information for each leaf node before predicting the next leaf node, while DFS captures both leaf and internal nodes in one context. Results show that performance between the two models are pretty comparable (58.0% vs 55.1% for value prediction in Tables 6 and 8). One drawback of RootPath is that it can only predict leaf nodes, while DFS can predict all nodes in the AST, including internal nodes for type prediction. <ref type="table" target="#tab_10">Table 8</ref> shows that DFSud+ did not outperform DFSud , which shows that simply expanding the up-down vocab may not be the right approach in exposing child index information to the model. Areas of explorations may include whether a vocabulary size of 100k is too sparse for the models to learn effectively, or whether child indices are inherently not as crucial for code prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Threats to Validity</head><p>SrcRNN Implementation. Our SrcRNN implementation is based on a PyTorch implementation 8 whereas related papers have generally built off of a Tensorflow implementation. <ref type="bibr" target="#b7">9</ref> As the hyperparameters were similar (dropout = 0.5, num_layers = 1, hidden_size =    512vs300) to recent publications, we do expect our implementation to be comparable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BPE.</head><p>We have not integrated byte-pair encoding (BPE) <ref type="bibr" target="#b23">[25]</ref> into our RNN model. We expect BPE to benefit both RNN and transformer models, and plan to explore this in future work.</p><p>Training Corpus. While larger Python corpora have appeared, py150 is still sizable at500MB; we do not expect the larger corpora to reverse our findings.</p><p>Python specificity. We have only carried out evaluations on Python, and have not demonstrated that our results would carry over (in trends) to other languages. The Deep3 paper did find their results (in trends) to roughly carry over from Python to Javascript.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Model Inspection</head><p>In this part, we study the influence of each input features to shed light on the black box of how our models make their predictions. Particularly, we study how each input token attributes to the models' predictions (attribute analysis, this section) and which UDpaths are learned to be important by DFSud <ref type="figure" target="#fig_1">(Appendix B.2)</ref>. For the latter, we found that local syntactical context is generally important and similarities exist compared to the heavily utilized Deep3 TGEN paths.</p><p>We use saliency maps <ref type="bibr" target="#b36">[38]</ref> for attribute analysis, which are constructed by taking the partial derivative of the loss function with respect to the inputs. <ref type="figure" target="#fig_4">Fig 6 visualizes</ref> the magnitudes of the gradients falls at each input token when the model predicts a particular output. Intuitively, the larger the value for a particular token, the more sensitive the output is to the variations at that token.</p><p>Examining the saliency maps for DFS and DFSud , we first observe that that parent node of the AST (the internal node right above the leaf) is generally important for both models. <ref type="figure" target="#fig_4">From Fig 6b,</ref> we can see DFSud is influenced by string when predicting atoi and by request_size when predicting num_requests. It is not shown in the figure but when predicting 2, DFSud is influenced by the previous occurrence of sys.argv indexed by 0 and 1. Looking at the differences between <ref type="figure" target="#fig_4">Fig 6a and Fig 6b,</ref> we found that DF-Sud is influenced by ip while predicting gethostbyname correctly but DFS is not while predicting it wrong. Generally, we found that DFSud attributes more towards terminal values relevant to the values to be predicted, while DFS attributes little to values other than non-terminals. This provides an evidence that DFSud is more likely to have learned the right features for next value prediction.</p><p>On an orthogonal note, we also observe that for many predicting locations, the magnitude of gradients are very small, suggesting the robustness of the model in the sense that it is less sensitive to minor perturbations of the input sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">RELATED WORK</head><p>Due to the vastness of the topic, we focus on two themes of related work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Statistical Code Completion</head><p>Simply put, the task of code completion is to predict the rest of the code a user is typing. Code completion is widely used by commercial or free integrated development environments (IDEs) <ref type="bibr">10 11 12</ref> to accelerate or ease the process of developing software.</p><p>Since Hindle et al. <ref type="bibr" target="#b20">[22]</ref>, there have been the rise of statistical learning for the task of code completion, exploiting naturalness of code <ref type="bibr" target="#b3">[5]</ref>. Learning methods used starting from n-gram <ref type="bibr" target="#b20">[22,</ref><ref type="bibr" target="#b28">30]</ref>  to probabilistic grammar <ref type="bibr" target="#b6">[8,</ref><ref type="bibr" target="#b9">11]</ref> and decision trees <ref type="bibr" target="#b32">[34]</ref>. Recently there have been increasing application of deep learning to code completion, especially recurrent neural networks <ref type="bibr" target="#b24">[26,</ref><ref type="bibr" target="#b26">28,</ref><ref type="bibr" target="#b27">29]</ref> and graph neural networks <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b11">13,</ref><ref type="bibr" target="#b41">43]</ref>.</p><p>Among other flavors of code completion, such as where program after the predicting location is available <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b7">9,</ref><ref type="bibr" target="#b11">13,</ref><ref type="bibr" target="#b34">36]</ref> or where the granularity of prediction is smaller (e.g. characters <ref type="bibr" target="#b10">[12]</ref> or subtokens <ref type="bibr" target="#b23">[25]</ref>) or larger (e.g. sub-ASTs <ref type="bibr" target="#b7">[9]</ref>), we focus on predicting next token given only partial program up to the predicting location.</p><p>PHOG <ref type="bibr" target="#b9">[11]</ref>, DeepSyn <ref type="bibr" target="#b33">[35]</ref> and Deep3 <ref type="bibr" target="#b32">[34]</ref> are particularly related as all of them utilize AST information for code completion. PHOG and DeepSyn uses a conditional probabilistic context-aware grammar based on AST walks. Deep3 further enriched the probabilistic model with a decision tree to allow more fine-grained modeling of context-dependent code occurrences. However, these probabilistic models have been surpassed by deep neural networks, namely LSTMs over serialized ASTs <ref type="bibr" target="#b26">[28]</ref>. Accuracy can be further improved by stacking attention and pointernetwork over an LSTM <ref type="bibr" target="#b24">[26]</ref> or by augmenting LSTMs with stacks for which the operations are guided by the AST structure <ref type="bibr" target="#b27">[29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Transformers</head><p>Transformers, popularized by Vaswani et al. <ref type="bibr" target="#b38">[40]</ref>, are sequence-tosequence (seq2seq) neural networks based-on layers of multi-head self-attentions. Surpassing RNNs, Transformer models <ref type="bibr" target="#b14">[16,</ref><ref type="bibr" target="#b15">17,</ref><ref type="bibr" target="#b31">33]</ref> have become the state-of-the-art natural language models, breaking records for a range of NLP tasks, including sentence entailment, question answering and language modeling. See Sec 3 for a more thorough introduction to Transformers.</p><p>There have been reported applications of Transformer models for code completion. Galois <ref type="bibr" target="#b11">13</ref> is an open source project that uses GPT-2 <ref type="bibr" target="#b31">[33]</ref> for code completion. The approach is similar to our SrcSeq model, despite their use of non-standard tokenizer and a subtoken segmenter. TabNine™ published a blog post <ref type="bibr" target="#b37">[39]</ref> in July 2019 mentioning the use of GPT-2 in their code completion but revealed no technical detail. To this point, we found no formal investigation up to this date on using transformers for the task of code completion.</p><p>There has been a surge of interest from 2019 in extending Transformer models to handle beyond sequential structures, for NLP <ref type="bibr" target="#b1">[3,</ref><ref type="bibr" target="#b29">31,</ref><ref type="bibr" target="#b40">42]</ref> and for learning source code <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b35">37]</ref>. Wang et al. <ref type="bibr" target="#b40">[42]</ref> put constraints on self-attentions to induce tree structures. Ahmed et al. <ref type="bibr" target="#b1">[3]</ref>, Harer et al. <ref type="bibr" target="#b17">[19]</ref> and Nguyen et al. <ref type="bibr" target="#b29">[31]</ref> modify the attention block to mix node representations according to tree structures. Shiv and Quirk <ref type="bibr" target="#b35">[37]</ref> proposed a tree-induced positional encoding. As for learning source code, it has been showed that taking tree structured helped code correction <ref type="bibr" target="#b17">[19]</ref> and code translation <ref type="bibr" target="#b35">[37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">FUTURE WORK</head><p>Handling Out-of-Vocabulary Words. Source code presents a difficulty shared with natural language processing in handling large vocabularies and rare words. The token/word to be predicted in test data may not appear in the training data. This is even more challenging when predicting identifiers, such as method names, variable names, and so on, as developers can come up with arbitrary identifier names. Possible mitigation includes copying mechanism <ref type="bibr" target="#b5">[7,</ref><ref type="bibr" target="#b11">13,</ref><ref type="bibr" target="#b16">18]</ref> and open-vocabulary models <ref type="bibr" target="#b12">[14,</ref><ref type="bibr" target="#b23">25]</ref>.</p><p>Exposing Tree Structure even more completely. We saw significant improvement in performance by providing more tree structure (DFS vs DFSud ). Our attempt at DFSud+ , a variation to DFSud that enhances the path relation vocabulary, did not improve performance. This leaves open the possibility that our way of representing AST paths needs to be improved.</p><p>Using Semantic Information. Recent work has also shown the promise of using easy-to-compute static analysis information, such as def-use information. While it is harder to get such info for dynamic languages, it is still an interesting question as to how to communicate those to transformers, and compare it to graph neural networks <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b25">27]</ref> that do use it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A IMPLEMENTATION DETAILS A.1 Modifying the AST</head><p>For the AST, we want the internal AST nodes to only have type information, and the leaf nodes to have value information. This way, our model can predict one information given a node (instead of both type and value). However, in the py150 dataset, there are internal and leaf nodes with both type and value information. To accomodate for this, we slightly modify the trees to fit our definition of ASTs. For nodes with both type and value information, we take the value information, and create a new node (now a leaf node) as the node's first child. <ref type="figure">Fig 7 illustrates</ref> an example of the modification. This increases the average number of nodes in a tree from 623.4 to 951.9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Splitting Large Trees</head><p>For neural network models, we need to set a maximum number of nodes in the tree that the model can take as input. Ideally, we would want to set the maximum to be high enough to take in any tree of any length; however, in practice, this is infeasible due to memory constraints (and the number of nodes could be infinitely large hypothetically.) We choose the maximum number of context (number of nodes) to be 1000, inspired by the maximum number of context set by GPT2 models and as this covers &gt; 70% of the training data. For trees with number of nodes greater than 1000, we deploy a technique adopted by <ref type="bibr" target="#b2">[4]</ref>. Given a large tree, we slice it into shorter segments with a sliding window (in our implementation, we used 500, which is half the context). For example, if a tree has 1700 nodes, we would have 3 new shorter trees: from nodes 0-999, nodes 500-1499, and 699-1699. For the last two trees, we would take loss and evaluate only on the nodes that the model has not seen before (1000-1499 and 1500-1699, respectively). In this way, we provide each subsequent shorter segment with some previous context, while increasing the number of training and testing datapoints at a reasonable amount (in our datasets, it doubled the number). An improvement to this sliding window technique would be to maintain the hidden states at each segment to pass along more context information, as explained in <ref type="bibr" target="#b13">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Why not Positional Encoding?</head><p>Some Transformers uses positional encoding <ref type="bibr" target="#b38">[40]</ref> or positional embedding <ref type="bibr" target="#b31">[33]</ref> to provide model extra positional information over elements. However, our early trials with LeafSeq suggested positional embedding is rather hurting than helping. Thus, we do not use positional encoding or embedding for all our models. Recently, Shiv and Quirk <ref type="bibr" target="#b35">[37]</ref> tried to introduce tree structures to Transformer models via positional encoding. However, their relative improvement is small compared to what we see with tree-relational prior in Section 6.  <ref type="figure">Figure 7</ref>: Example AST and our modification to allow nodes to have either only value or type information. <ref type="table" target="#tab_2">Table 11</ref> and <ref type="table" target="#tab_2">Table 12</ref> show respectively the breakdown results for terminal and non-terminal value prediction at various type of locations over internal dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B EXTRA RESULTS B.1 Extra Evaluation Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Inspecting Attention Heads</head><p>DFSud learns weights for various UDpaths between a node and other nodes in its context as a component of self-attention. In this part, we inspect the learned weights for UDpaths in the DF-Sud model in order to understand which UDpaths are the most important for the model's prediction.</p><p>There are six attention layers and six attention heads within each layer in DFSud . All of them collectively determine the importance of each previous node in the prediction of the next token. We look into the maximally and minimally weighted UDpaths at each attention head. The results are shown in <ref type="figure" target="#fig_6">Fig 8.</ref> Presumably, the extreme-weighted UDpaths are the most salient features for the model's prediction. The more extreme the weight is, the more conspicuous the path is among other paths for the particular head.</p><p>For example, we found that U 1 , U 1 D 1 , U 1 D 2 , U 2 and U 2 D 2 are important across multiple heads. U 1 , U 1 D 1 and U 12 D 1 are particularly up-weighted by some heads; while U 1 D 1 , U 1 D 2 , U 6 D 17 and U 1 are particularly down-weighted by some heads. The frequent presences of U 1 , U 1 D 1 and U 1 D 2 suggest the importance of syntactical local context in the next value prediction. The extreme weights of very long paths, e.g. U 12 D 1 , is at first baffling. However, we found cases where they can be useful in, for example, referring to class names (U 12 D 1 in <ref type="figure">Fig 9a)</ref> or to related variable names under similar scopes (U 6 D 17 in <ref type="figure">Fig 9b)</ref>.</p><p>Comparing to Deep3. As mentioned in Sec 4, Deep3 also relies on the values collected by their tree-walk programs (in TGEN, see <ref type="figure" target="#fig_3">Fig 5)</ref> executed over ASTs.</p><p>Deep3's TGEN programs are strictly more expressive than our UDpaths, which are based on only up and down counts. However, for many of the tree walks, we can find corresponding UDpaths that represent the same movement in an AST. For example, TGEN expression [Up][Up][WRITE_TYPE] is similar to our U 2 . WRITE is disregarded as our models naturally have access to the values associated at the destination. We collected the most frequently used TGEN's tree-walk expressions when evaluating their model (E13) over the py150 testing set. <ref type="table" target="#tab_2">Table 13</ref> lists the top equivalent UDpaths and their counts, assuming the node to be predicted is a leaf with a left sibling leaf.     3.0 × 10 6 U 2 D 2 2.9 × 10 6 <ref type="table" target="#tab_2">Table 13</ref>: Top UDpath-convertible tree-walks used by E13 when predicting values over py150.</p><p>We found that U 1 , U 2 and U 2 D 2 are at the both extremely weighted by many heads in our DFSud and heavily utilized in Deep3. However, some of the potentially useful UDpaths heavily used by Deep3 are not often extremely weighted by DFSud . For example U 3 , potentially useful for knowing the scope of the value to be predicted, only appears once as the maximally value in layer 5, head 5 of DFSud <ref type="figure" target="#fig_6">(Fig 8a)</ref>.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Running example of Python code. The code snippet 1 is from the py150 dataset [1].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>AST for the example in Fig 1. The leaf (terminal) nodes have values and the interior (non-terminal) nodes have types.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Fragment of a TGEN program encoding a decision tree. The bold words are steps that comprise a path in a given AST.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Influence of previous nodes in value prediction of the example in Fig 2 by DFS and DFSud . x-axis is labeled with the input values. y-axis is labeled with the values to be predicted. Color indicates the model's prediction is correct or wrong.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Maximally (a) or minimally (b) weighted tree-relations and their weights at each attention head in DFSud . Red means more extremal values. # ... class Permissions (unittest.TestCase) : new_roles = {} @utils.allow(services=list_permissions) def setUp(self) : acc = self.account if acc.service in list_permissions: self . test_folder = utils.create_or_get_test_folder(acc) self . test_file = utils.create_test_file(acc) # ... # ... def test_folder_permissions_set(self) : if self.account.service in change_folder_permissions: self.new_roles = { # ... } result = self . test_folder .permissions.create( data=self.new_roles) self.assertIsInstance(result.permissions, list) self.list_helper(self.test_folder) # ... def test_file_permissions_set(self) : if self.account.service in change_file_permissions: self.new_roles = { # ... } result = self . test_file .permissions.create( data=self.new_roles) self.assertIsInstance(result.permissions, list) self.list_helper(self.test_file) # ... # ...</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>(a) legofy_gui.py<ref type="bibr" target="#b12">14</ref> , highlighting U 12 D 1 # Create your views here. # ... def map_location_json(request, ne_lat= 0 , ne_lon= 0 , sw_lat= 0 , sw_lon= 0 ) : ne_lat = float(ne_lat) ne_lon = float(ne_lon) sw_lat = float(sw_lat) sw_lon = float(sw_lon) featureset = Recording.objects\ . filter (lat__lt= ne_lat , lat__gt= sw_lat ,lon__lt= ne_lon , lon__gt= sw_lon )\ .order_by( '-date' )\ .exclude(location__isnull= True )\ .exclude(location__exact= '' )\ .exclude(location__exact= 'No description available' )\ .exclude( location__exact = '0.0, 0.0' )[: 750 ]if len ( featureset ) &lt; 1 : return HttpResponse( "{\"objects\":[]}" , mimetype= "application/json" ) resp = encode_queryset(featureset) return HttpResponse(resp, mimetype= "application/json" ) # ...</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>17 Figure 9 :</head><label>179</label><figDesc>(b) views.py<ref type="bibr" target="#b13">15</ref> , highlighting U 6 D Two code excerpts from py150 evaluation set. Highlighted tokens are picked by some long UDpath in prediction of the underlined tokens.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Ranks for the predictions for the leaf nodes listed in Fig 2. &gt;10 means the model did not get the right answer in the top 10 results. DFSud is our most powerful model.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>RQ1 . 1</head><label>.</label><figDesc>Is the Transformer-based model more accurate than the RNN-based model on the next token prediction problem? ( Sec 2)?To answer this question, we compare SrcRNN model against the SrcSeq model on the source tokens. RQ1.2 Are the Transformer-based models more accurate than Deep3, on the value prediction and on the type predic-</figDesc><table><row><cell>tion problems (Sec 2)?</cell></row><row><cell>To answer this question, we compare Deep3 model against</cell></row><row><cell>the DFS variant of the Transformer on the ASTs variants:</cell></row><row><cell>DFS , DFSud+ , DFSud , and RootPath ,</cell></row><row><cell>For RQ2, we ask two sub-questions:</cell></row><row><cell>RQ2.1 Does a Transformer model based on an AST outper-</cell></row><row><cell>form a Transformer model that takes the correspond-</cell></row><row><cell>ing source token sequences?</cell></row><row><cell>This question can be answered directly only on tokens that</cell></row><row><cell>appear both in ASTs and source token sequences: these are</cell></row><row><cell>precisely the values at the leaf nodes of the AST. We compare</cell></row><row><cell>SrcSeq and DFS models on the terminal value prediction</cell></row><row><cell>problem.</cell></row><row><cell>RQ2.2 Does providing more detailed structural information</cell></row><row><cell>help with accuracy?</cell></row><row><cell>To answer this question, we compare among the tree-based</cell></row><row><cell>Transformer models (DFS , DFSud+ , DFSud , and Root-</cell></row><row><cell>Path ) on the terminal value prediction and on the internal/-</cell></row><row><cell>type prediction problems.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>MRR and Acc@1 (in parenthesis) of various prediction tasks for py150.</figDesc><table><row><cell></cell><cell cols="2">Prior work</cell><cell></cell><cell>Our work</cell><cell></cell></row><row><cell>Applications</cell><cell>SrcRNN</cell><cell>Deep3</cell><cell>SrcSeq</cell><cell>DFS</cell><cell>DFSud</cell></row><row><cell cols="2">Next token prediction 57.4 (48.3)</cell><cell>n/a</cell><cell>66.8 (60.2)</cell><cell>n/a</cell><cell>n/a</cell></row><row><cell>Value prediction</cell><cell cols="5">23.8 (17.7) 36.1 (33.3) 36.5 (30.7) 43.9 (38.8) 58.4 (55.3)</cell></row><row><cell>Type prediction</cell><cell>n/a</cell><cell>79.9 (73.1)</cell><cell>n/a</cell><cell cols="2">87.7 (80.2) 98.0 (96.3)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>MRR and Acc@1 (in parenthesis) of various prediction tasks for the internal dataset.</figDesc><table><row><cell>Applications</cell><cell>DFSud</cell><cell cols="3">RootPath LeafTokens DFSud+</cell></row><row><cell cols="5">Value prediction 73.6 (71.0) 55.1 (48.4) 41.9 (34.1) 73.3 (70.8)</cell></row><row><cell cols="2">Type prediction 98.7 (97.6)</cell><cell>n/a</cell><cell>n/a</cell><cell>97.8 (96.1)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>MRR and Acc@1 (in parenthesis) of the alternate models and variations of models for py150, compared against the best performing model, DFSud .</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 and</head><label>9</label><figDesc>Table 10show respectively the breakdown results for terminal and non-terminal value prediction at various type of locations over py150.</figDesc><table><row><cell cols="2">type: AttributeLoad</cell><cell>type: AttributeLoad</cell><cell></cell></row><row><cell>0</cell><cell></cell><cell>0</cell><cell></cell></row><row><cell>type: NameLoad value: logging</cell><cell>type: Attr value: getLogger</cell><cell>type: NameLoad 1</cell><cell>type: Attr 3</cell></row><row><cell>1</cell><cell>2</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>value: logging</cell><cell>value: getLogger</cell></row><row><cell></cell><cell></cell><cell>2</cell><cell>4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>(31.6) 45.3 (41.7) 55.9 (49.0) 60.5 (54.4) 75.6 (73.3) Numeric constant 40.6 (29.3) 53.2 (46.4) 55.9 (45.7) 63.5 (53.7) 83.1 (79.0) Name (variable, module) 38.2 (29.6) 48.9 (45.4) 54.1 (46.5) 66.6 (61.0) 79.8 (77.4) Function parameter name 57.7 (54.0) 58.1 (56.6) 66.2 (62.8) 67.2 (63.6) 87.1 (84.7) All values 36.6 (29.1) 43.9 (40.5) 50.1 (43.4) 58.0 (52.4) 98.7 (97.6)</figDesc><table><row><cell></cell><cell cols="2">Prior work</cell><cell></cell><cell>Our work</cell><cell></cell></row><row><cell>Applications</cell><cell>SrcRNN</cell><cell>Deep3</cell><cell>SrcSeq</cell><cell>DFS</cell><cell>DFSud</cell></row><row><cell>Attribute access</cell><cell>39.3</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 9 :</head><label>9</label><figDesc>MRR and Acc@1 (in parenthesis) of various types of value predictions for py150.</figDesc><table><row><cell></cell><cell>Prior work</cell><cell cols="2">Our work</cell></row><row><cell>Applications</cell><cell>Deep3</cell><cell>DFS</cell><cell>DFSud</cell></row><row><cell>Function call</cell><cell>81.6 (74.2)</cell><cell cols="2">88.5 (81.0) 98.7 (97.5)</cell></row><row><cell>Assignment</cell><cell>76.5 (66.7)</cell><cell cols="2">78.9 (64.3) 98.7 (97.5)</cell></row><row><cell>Return</cell><cell>52.8 (40.8)</cell><cell cols="2">67.8 (51.8) 97.8 (95.9)</cell></row><row><cell>List</cell><cell>59.4 (54.2)</cell><cell cols="2">76.0 (65.8) 97.1 (94.7)</cell></row><row><cell>Dictionary</cell><cell>66.3 (61.0)</cell><cell cols="2">15.0 (9.0) 83.8 (74.3)</cell></row><row><cell>Raise</cell><cell>35.0 (27.1)</cell><cell cols="2">63.3 (47.6) 97.0 (94.6)</cell></row><row><cell>All types</cell><cell>81.9 (75.8)</cell><cell cols="2">87.3 (79.6) 98.7 (97.6)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 10 :</head><label>10</label><figDesc>MRR and Acc@1 (in parenthesis) of various type predictions for py150. (20.9) 38.5 (36.0) 41.0 (35.5) 44.7 (39.9) 59.3 (56.7) Numeric constant 32.2 (20.3) 46.5 (38.2) 51.7 (40.5) 61.5 (50.4) 84.0 (78.6) Name (variable, module) 25.0 (17.8) 41.0 (38.2) 39.3 (32.7) 50.7 (45.6) 62.8 (60.1) Function parameter name 45.5 (42.8) 50.6 (49.0) 54.3 (51.7) 53.3 (49.6) 73.7 (70.7) All values 23.8 (17.7) 36.1 (33.3) 36.5 (30.7) 43.9 (38.8) 58.4 (55.3)</figDesc><table><row><cell></cell><cell cols="2">Prior work</cell><cell></cell><cell>Our work</cell><cell></cell></row><row><cell>Applications</cell><cell>SrcRNN</cell><cell>Deep3</cell><cell>SrcSeq</cell><cell>DFS</cell><cell>DFSud</cell></row><row><cell>Attribute access</cell><cell>26.4</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 11 :</head><label>11</label><figDesc>MRR and Acc@1 (in parenthesis) of various types of next token value prediction for internal dataset.</figDesc><table><row><cell></cell><cell>Prior work</cell><cell cols="2">Our work</cell></row><row><cell>Applications</cell><cell>Deep3</cell><cell>DFS</cell><cell>DFSud</cell></row><row><cell>Function call</cell><cell>78.2 (70.3)</cell><cell cols="2">86.0 (77.1) 97.8 (95.9)</cell></row><row><cell>Assignment</cell><cell>78.5 (69.1)</cell><cell cols="2">79.7 (65.8) 98.7 (97.4)</cell></row><row><cell>Return</cell><cell>59.9 (47.8)</cell><cell cols="2">72.2 (58.3) 97.6 (95.5)</cell></row><row><cell>List</cell><cell>40.8 (33.9)</cell><cell cols="2">63.1 (48.7) 94.3 (89.6)</cell></row><row><cell>Dictionary</cell><cell>39.8 (31.2)</cell><cell cols="2">23.5 (16.7) 81.0 (70.4)</cell></row><row><cell>Raise</cell><cell>33.5 (25.8)</cell><cell cols="2">59.3 (41.7) 96.4 (93.5)</cell></row><row><cell>All types</cell><cell>79.9 (73.1)</cell><cell cols="2">87.7 (80.2) 98.0 (96.3)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 12 :</head><label>12</label><figDesc>MRR and Acc@1 (in parenthesis) of various types of next token type prediction for internal dataset.</figDesc><table><row><cell>Equivalent UDpath</cell><cell>Count</cell></row><row><cell>U 1</cell><cell>1.8 × 10 7</cell></row><row><cell>U 2 D 1</cell><cell>4.7 × 10 6</cell></row><row><cell>U 3</cell><cell>4.2 × 10 6</cell></row><row><cell>U 2</cell><cell>3.4 × 10 6</cell></row><row><cell>U 4</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The rows do not sum to 1 since there are previous tokens in the sequence that is not shown in this table</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Code2vec<ref type="bibr" target="#b8">[10]</ref> used (embeddings of) leaf-to-leaf AST paths to capture information for the purpose of code summarization; by contrast, UD paths specifically retain information on how a pair of tree nodes are situated with respect to each other.<ref type="bibr" target="#b2">4</ref> Node 24 was omitted due to space constraints for the table.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://github.com/graykode/gpt-2-Pytorch. We do not use positional encoding. Refer to Appendix A.3 for the explanation. 7 https://github.com/pytorch/examples/tree/master/word_language_model</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">https://github.com/pytorch/examples/blob/master/word_language_model/model.py 9 https://github.com/tensorflow/models/blob/master/tutorials/rnn/ptb/ptb_word_lm. py</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">https://code.visualstudio.com/docs/editor/intellisense 11 https://www.jetbrains.com/help/idea/auto-completing-code.html 12 https://flight-manual.atom.io/using-atom/sections/autocomplete/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13">https://github.com/iedmrc/galois-autocompleter</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14">data/JuanPotato/Legofy/legofy/legofy_gui.py<ref type="bibr" target="#b13">15</ref> data/Miserlou/OpenWatch/openwatch/map/views.py</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Pretrained Probabilistic Models for Code</title>
		<ptr target="https://github.com/eth-sri/ModelsPHOG" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">You Only Need Attention to Traverse Trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahtab</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><forename type="middle">Rifayat</forename><surname>Samee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">E</forename><surname>Mercer</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/P19-1030/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="316" to="322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dokook</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandy</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.04444</idno>
		<title level="m">Character-Level Language Modeling with Deeper Self-Attention</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>cs.CL</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A survey of machine learning for big code and naturalness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miltiadis</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Earl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Premkumar</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Devanbu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="1" to="37" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning to Represent Programs with Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miltiadis</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmoud</forename><surname>Khademi</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=BJOFETxR-" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A Convolutional Attention Network for Extreme Summarization of Source Code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miltiadis</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v48/allamanis16.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 33rd International Conference on Machine Learning (Proceedings of Machine Learning Research</title>
		<editor>Maria Florina Balcan and Kilian Q. Weinberger</editor>
		<meeting>The 33rd International Conference on Machine Learning ( Machine Learning Research<address><addrLine>New York, New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="2091" to="2100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mining idioms from source code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miltiadis</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering</title>
		<meeting>the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="472" to="483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Structural Language Models for Any-Code Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Sadaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>Yahav</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HylZIT4Yvr" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning distributed representations of code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meital</forename><surname>Zilberstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>Yahav</surname></persName>
		</author>
		<idno type="DOI">10.1145/3290353</idno>
		<ptr target="https://doi.org/10.1145/3290353" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the ACM on Programming Languages</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="29" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>POPL</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">PHOG: probabilistic model for code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavol</forename><surname>Bielik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Raychev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Vechev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2933" to="2942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Program synthesis for character level language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavol</forename><surname>Bielik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Raychev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Vechev</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=ry_sjFqgx" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generative Code Modeling with Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miltiadis</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">L</forename><surname>Gaunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Polozov</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Bke4KsA5FX" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Open Vocabulary Learning on Source Code with a Graph-Structured Cache</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milan</forename><surname>Cvitkovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Badal</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Animashree</forename><surname>Anandkumar</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="http://proceedings.mlr.press/v97/cvitkovic19b.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning (Proceedings of Machine Learning Research</title>
		<editor>Kamalika Chaudhuri and Ruslan Salakhutdinov</editor>
		<meeting>the 36th International Conference on Machine Learning ( Machine Learning Research<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="1475" to="1485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Transformer-XL: Attentive Language Models beyond a Fixed-Length Context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1285</idno>
		<ptr target="https://doi.org/10.18653/v1/P19-1285" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2978" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unified language model pre-training for natural language understanding and generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiao-Wuen</forename><surname>Hon</surname></persName>
		</author>
		<ptr target="https://papers.nips.cc/paper/9464-unified-language-model-pre-training-for-natural-language-understanding-and-generation" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems. 13042-13054</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Structured Neural Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miltiadis</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=H1ersoRqtm" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Tree-Transformer: A Transformer-Based Method for Correction of Tree-Structured Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Harer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Reale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Chin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.00449</idno>
		<ptr target="https://arxiv.org/abs/1908.00449" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Are deep neural networks the best choice for modeling source code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Premkumar</forename><surname>Hellendoorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Devanbu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering</title>
		<meeting>the 2017 11th Joint Meeting on Foundations of Software Engineering</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="763" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Are deep neural networks the best choice for modeling source code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hellendoorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Prem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Devanbu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering</title>
		<meeting>the 2017 11th Joint Meeting on Foundations of Software Engineering</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On the naturalness of software</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abram</forename><surname>Hindle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Earl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhendong</forename><surname>Gabel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Premkumar</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Devanbu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="122" to="131" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Summarizing Source Code using a Neural Attention Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivasan</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvin</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1195</idno>
		<ptr target="https://doi.org/10.18653/v1/P16-1195" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2073" to="2083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Big Code != Big Vocabulary: Open-Vocabulary Models for Source Code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael-Michael</forename><surname>Karampatsis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hlib</forename><surname>Babii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Robbes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Janes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Software Engineering (ICSE)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Code Completion with Neural Attention and Pointer Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">R</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 27th International Joint Conference on Artificial Intelligence<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4159" to="4184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Gated Graph Sequence Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<ptr target="https://www.microsoft.com/en-us/research/publication/gated-graph-sequence-neural-networks/" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (proceedings of iclr</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>16 ed.</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rJbPBt9lg" />
		<title level="m">Neural code completion</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Modeling Programs Hierarchically with Stack-Augmented LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jss.2020.110547</idno>
		<ptr target="https://doi.org/10.1016/j.jss.2020.110547" />
	</analytic>
	<monogr>
		<title level="j">Journal of Systems and Software</title>
		<imprint>
			<biblScope unit="page">110547</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A Statistical Semantic Language Model for Source Code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><forename type="middle">Tuan</forename><surname>Tung Thanh Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Hoan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tien</forename><forename type="middle">N</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nguyen</surname></persName>
		</author>
		<idno type="DOI">10.1145/2491411.2491458</idno>
		<ptr target="https://doi.org/10.1145/2491411.2491458" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering</title>
		<meeting>the 2013 9th Joint Meeting on Foundations of Software Engineering<address><addrLine>Saint Petersburg, Russia; New York, NY, USA, 532âĂŞ542</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Tree-Structured Attention with Hierarchical Accumulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan-Phi</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HJxK5pEYvr" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Building Language Models for Text with Named Entities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saikat</forename><surname>Md Rizwan Parvez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baishakhi</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1221</idno>
		<ptr target="https://doi.org/10.18653/v1/P18-1221" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2373" to="2383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Probabilistic Model for Code with Decision Trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Raychev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavol</forename><surname>Bielik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Vechev</surname></persName>
		</author>
		<idno type="DOI">10.1145/2983990.2984041</idno>
		<ptr target="https://doi.org/10.1145/2983990.2984041" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications</title>
		<meeting>the 2016 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications<address><addrLine>Amsterdam, Netherlands; New York, NY, USA, 731âĂŞ747</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>OOPSLA 2016)</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning Programs from Noisy Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Raychev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavol</forename><surname>Bielik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Vechev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Krause</surname></persName>
		</author>
		<idno type="DOI">10.1145/2914770.2837671</idno>
		<ptr target="https://doi.org/10.1145/2914770.2837671" />
	</analytic>
	<monogr>
		<title level="j">SIGPLAN Not</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="761" to="774" />
			<date type="published" when="2016-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Code Completion with Statistical Language Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Raychev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Vechev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>Yahav</surname></persName>
		</author>
		<idno type="DOI">10.1145/2594291.2594321</idno>
		<ptr target="https://doi.org/10.1145/2594291.2594321" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation</title>
		<meeting>the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation<address><addrLine>Edinburgh, United Kingdom; New York, NY, USA, 419âĂŞ428</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>PLDI âĂŹ14)</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Novel positional encodings to enable tree-based transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vighnesh</forename><surname>Shiv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<ptr target="https://papers.nips.cc/paper/9376-novel-positional-encodings-to-enable-tree-based-transformers" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems. 12058-12068</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=cO4ycnpqxKcS9" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Autocompletion with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tabnine</surname></persName>
		</author>
		<ptr target="https://tabnine.com/blog/deep" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.03134</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Pointer Networks</publisher>
		</imprint>
	</monogr>
	<note>stat.ML</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Tree Transformer: Integrating Tree Structures into Self-Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaushian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Yi</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Nung</forename><surname>Chen</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/D19-1098/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1060" to="1070" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Improve language modelling for code completion through learning general token repetition of source code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixiao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">31st Int. Conf. Software Engineering and Knowledge Engineering</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="667" to="777" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
