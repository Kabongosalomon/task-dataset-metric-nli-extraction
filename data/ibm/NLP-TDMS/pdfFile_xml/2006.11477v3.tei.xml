<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
							<email>abaevski@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Zhou</surname></persName>
							<email>henryzhou7@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
							<email>michaelauli@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech recognition with limited amounts of labeled data. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural networks benefit from large quantities of labeled training data. However, in many settings labeled data is much harder to come by than unlabeled data: current speech recognition systems require thousands of hours of transcribed speech to reach acceptable performance which is not available for the vast majority of the nearly 7,000 languages spoken worldwide <ref type="bibr" target="#b30">[31]</ref>. Learning purely from labeled examples does not resemble language acquisition in humans: infants learn language by listening to adults around them -a process that requires learning good representations of speech.</p><p>In machine learning, self-supervised learning has emerged as a paradigm to learn general data representations from unlabeled examples and to fine-tune the model on labeled data. This has been particularly successful for natural language processing <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b8">9]</ref> and is an active research area for computer vision <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b5">6]</ref>.</p><p>In this paper, we present a framework for self-supervised learning of representations from raw audio data. Our approach encodes speech audio via a multi-layer convolutional neural network and then masks spans of the resulting latent speech representations <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b55">56]</ref>, similar to masked language modeling <ref type="bibr" target="#b8">[9]</ref>. The latent representations are fed to a Transformer network to build contextualized representations and the model is trained via a contrastive task where the true latent is to be distinguished from distractors <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b27">28]</ref> ( § 2).</p><p>As part of training, we learn discrete speech units <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b17">18]</ref> via a gumbel softmax <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b4">5]</ref> to represent the latent representations in the contrastive task ( <ref type="figure" target="#fig_1">Figure 1</ref>) which we find to be more effective than non-quantized targets. After pre-training on unlabeled speech, the model is fine-tuned  on labeled data with a Connectionist Temporal Classification (CTC) loss <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b3">4]</ref> to be used for downstream speech recognition tasks ( § 3)</p><formula xml:id="formula_0">v P T F t u J K P M E t Y E J O x 5 B G n B K z k D 2 I C E 0 p E 1 p 4 P q z W 3 7 i 6 A 1 4 l X k B o q 0 B p W v w Y j R d O Y S a C C G O N 7 b g J B R j R w K t i 8 M k g N S w i d k j H z L Z U k Z i b I F p H n + M I q I x w p b Z 8 E v F B / b 2 Q k N m Y W h 3 Y y j 2 h W v V z 8 z / N T i G 6 C j M s k B S b p 8 q M o F R g U z u / H I 6 4 Z B T G z h F D N b V Z M J 0 Q T C</formula><p>Previous work learned a quantization of the data followed by a contextualized representations with a self-attention model <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b3">4]</ref>, whereas our approach solves both problems end-to-end. Masking parts of the input with Transformer networks for speech has been explored <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b25">26]</ref>, but prior work relies either on a two-step pipeline or their model is trained by reconstructing the filter bank input features. Other related work includes learning representations from auto-encoding the input data <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b10">11]</ref> or directly predicting future timesteps <ref type="bibr" target="#b7">[8]</ref>.</p><p>Our results show that jointly learning discrete speech units with contextualized representations achieves substantially better results than fixed units learned in a prior step <ref type="bibr" target="#b3">[4]</ref>. We also demonstrate the feasibility of ultra-low resource speech recognition: when using only 10 minutes of labeled data, our approach achieves word error rate (WER) 4.8/8.2 on the clean/other test sets of Librispeech. We set a new state of the art on TIMIT phoneme recognition as well as the 100 hour clean subset of Librispeech. Moreover, when we lower the amount of labeled data to just one hour, we still outperform the previous state of the art self-training method of <ref type="bibr" target="#b41">[42]</ref> while using 100 times less labeled data and the same amount of unlabeled data. When we use all 960 hours of labeled data from Librispeech, then our model achieves 1.8/3.3 WER ( § 4, § 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model</head><p>Our model is composed of a multi-layer convolutional feature encoder f : X → Z which takes as input raw audio X and outputs latent speech representations z 1 , . . . , z T for T time-steps. They are then fed to a Transformer g : Z → C to build representations c 1 , . . . , c T capturing information from the entire sequence <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b3">4]</ref>. The output of the feature encoder is discretized to q t with a quantization module Z → Q to represent the targets <ref type="figure" target="#fig_1">(Figure 1</ref>) in the self-supervised objective ( § 3.2). Compared to vq-wav2vec <ref type="bibr" target="#b4">[5]</ref>, our model builds context representations over continuous speech representations and self-attention captures dependencies over the entire sequence of latent representations end-to-end.</p><p>Feature encoder. The encoder consists of several blocks containing a temporal convolution followed by layer normalization <ref type="bibr" target="#b0">[1]</ref> and a GELU activation function <ref type="bibr" target="#b20">[21]</ref>. The raw waveform input to the encoder is normalized to zero mean and unit variance. The total stride of the encoder determines the number of time-steps T which are input to the Transformer ( § 4.2).</p><p>Contextualized representations with Transformers. The output of the feature encoder is fed to a context network which follows the Transformer architecture <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b32">33]</ref>. Instead of fixed positional embeddings which encode absolute positional information, we use a convolutional layer similar to <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b56">57]</ref> which acts as relative positional embedding. We add the output of the convolution followed by a GELU to the inputs and then apply layer normalization.</p><p>Quantization module. For self-supervised training we discretize the output of the feature encoder z to a finite set of speech representations via product quantization <ref type="bibr" target="#b24">[25]</ref>. This choice led to good results in prior work which learned discrete units in a first step followed by learning contextualized representations <ref type="bibr" target="#b4">[5]</ref>. Product quantization amounts to choosing quantized representations from multiple codebooks and concatenating them. Given G codebooks, or groups, with V entries e ∈ R V ×d/G , we choose one entry from each codebook and concatenate the resulting vectors e 1 , . . . , e G and apply a linear transformation</p><formula xml:id="formula_1">R d → R f to obtain q ∈ R f .</formula><p>The Gumbel softmax enables choosing discrete codebook entries in a fully differentiable way <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b34">35]</ref>. We use the straight-through estimator <ref type="bibr" target="#b25">[26]</ref> and setup G hard Gumbel softmax operations <ref type="bibr" target="#b23">[24]</ref>. The feature encoder output z is mapped to l ∈ R G×V logits and the probabilities for choosing the v-th codebook entry for group g are</p><formula xml:id="formula_2">p g,v = exp(l g,v + n v )/τ V k=1 exp(l g,k + n k )/τ ,<label>(1)</label></formula><p>where τ is a non-negative temperature, n = − log(− log(u)) and u are uniform samples from U(0, 1).</p><p>During the forward pass, codeword i is chosen by i = argmax j p g,j and in the backward pass, the true gradient of the Gumbel softmax outputs is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Training</head><p>To pre-train the model we mask a certain proportion of time steps in the latent feature encoder space ( § 3.1), similar to masked language modeling in BERT <ref type="bibr" target="#b8">[9]</ref>. The training objective requires identifying the correct quantized latent audio representation in a set of distractors for each masked time step ( § 3.2) and the final model is fine-tuned on the labeled data ( § 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Masking</head><p>We mask a proportion of the feature encoder outputs, or time steps before feeding them to the context network and replace them with a trained feature vector shared between all masked time steps; we do not mask inputs to the quantization module. To mask the latent speech representations output by the encoder, we randomly sample without replacement a certain proportion p of all time steps to be starting indices and then mask the subsequent M consecutive time steps from every sampled index; spans may overlap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Objective</head><p>During pre-training, we learn representations of speech audio by solving a contrastive task L m which requires to identify the true quantized latent speech representation for a masked time step within a set of distractors. This is augmented by a codebook diversity loss L d to encourage the model to use the codebook entries equally often.</p><formula xml:id="formula_3">L = L m + αL d<label>(2)</label></formula><p>where α is a tuned hyperparameter.</p><p>Contrastive Loss. Given context network output c t centered over masked time step t, the model needs to identify the true quantized latent speech representation q t in a set of K + 1 quantized candidate representationsq ∈ Q t which includes q t and K distractors <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b53">54]</ref>. Distractors are uniformly sampled from other masked time steps of the same utterance. The loss is defined as</p><formula xml:id="formula_4">L m = − log exp(sim(c t , q t )/κ) q∼Qt exp(sim(c t ,q)/κ)<label>(3)</label></formula><p>where we compute the cosine similarity sim(a, b) = a T b/ a b between context representations and quantized latent speech representations <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b5">6]</ref>.</p><p>Diversity Loss. The contrastive task depends on the codebook to represent both positive and negative examples and the diversity loss L d is designed to increase the use of the quantized codebook representations <ref type="bibr" target="#b9">[10]</ref>. We encourage the equal use of the V entries in each of the G codebooks by maximizing the entropy of the averaged softmax distribution l over the codebook entries for each codebookp g across a batch of utterances; the softmax disribution does not contain the gumbel noise nor a temperature: 2</p><formula xml:id="formula_5">L d = 1 GV G g=1 −H(p g ) = 1 GV G g=1 V v=1p g,v logp g,v<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Fine-tuning</head><p>Pre-trained models are fine-tuned for speech recognition by adding a randomly initialized linear projection on top of the context network into C classes representing the vocabulary of the task <ref type="bibr" target="#b3">[4]</ref>. For Librispeech, we have 29 tokens for character targets plus a word boundary token. Models are optimized by minimizing a CTC loss <ref type="bibr" target="#b13">[14]</ref> and we apply a modified version of SpecAugment <ref type="bibr" target="#b40">[41]</ref> by masking to time-steps and channels during training which delays overfitting and significantly improves the final error rates, especially on the Libri-light subsets with few labeled examples.</p><p>4 Experimental Setup</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>As unlabeled data we consider the Librispeech corpus <ref type="bibr" target="#b39">[40]</ref> without transcriptions containing 960 hours of audio (LS-960) or the audio data from LibriVox (LV-60k). For the latter we follow the preprocessing of <ref type="bibr" target="#b26">[27]</ref> resulting in 53.2k hours of audio. We fine-tune on five labeled data settings: 960 hours of transcribed Librispeech, the train-clean-100 subset comprising 100 hours (100 hours labeled), as well as the Libri-light limited resource training subsets originally extracted from Librispeech, these are train-10h (10 hours labeled), train-1h (1 hour labeled), train-10min (10 min labeled). We follow the evaluation protocol of Libri-light for these splits and evaluate on the standard Librispech dev-other/clean and test-clean/other sets.</p><p>We fine-tune the pre-trained models for phoneme recognition on the TIMIT dataset <ref type="bibr" target="#b12">[13]</ref>. It contains five hours of audio recordings with detailed phoneme labels. We use the standard train, dev and test split and follow the standard protocol of collapsing phone labels to 39 classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Pre-training</head><p>Models are implemented in fairseq <ref type="bibr" target="#b38">[39]</ref>. For masking, we sample p = 0.065 of all time-steps to be starting indices and mask the subsequent M = 10 time-steps. This results in approximately 49% of all time steps to be masked with a mean span length of 14.7, or 299ms (see Appendix A for more details on masking).</p><p>The feature encoder contains seven blocks and the temporal convolutions in each block have 512 channels with strides (5,2,2,2,2,2,2) and kernel widths (10,3,3,3,3,2,2). This results in an encoder output frequency of 49 hz with a stride of about 20ms between each sample, and a receptive field of 400 input samples or 25ms of audio. The convolutional layer modeling relative positional embeddings has kernel size 128 and 16 groups.</p><p>We experiment with two model configurations which use the same encoder architecture but differ in the  <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b11">12]</ref>; there is no layer drop for LV-60k. <ref type="bibr" target="#b1">2</ref> Our implementation maximizes perplexity</p><formula xml:id="formula_6">GV − G g=1 exp(− V v=1 pgv log pgv ) GV which is equivalent.</formula><p>We optimize with Adam <ref type="bibr" target="#b28">[29]</ref>, warming up the learning rate for the first 8% of updates to a peak of 5 × 10 −4 for BASE and 3 × 10 −4 for LARGE, and then linearly decay it. LARGE trains for 250k updates, BASE for 400k updates, and LARGE on LV-60k for 600k updates. We use weight α = 0.1 for the diversity loss Equation 2. For the quantization module we use G = 2 and V = 320 for both models, resulting in a theoretical maximum of 102.4k codewords. Entries are of size d/G = 128 for BASE amd d/G = 384 for LARGE. The Gumbel softmax temperature τ is annealed from 2 to a minimum of 0.5 for BASE and 0.1 for LARGE by a factor of 0.999995 at every update. The temperature in the contrastive loss (Equation 3) is set to κ = 0.1. For the smaller Librispeech dataset, we regularize the model by applying an L2 penalty to the activations of the final layer of the feature encoder and scale down the gradients for the encoder by a factor of 10. We also use a slightly different encoder architecture where we do not use layer normalization, and instead of normalizing the raw waveform, the output of the first encoder layer is normalized. In the contrastive loss we use K = 100 distractors. We choose the training checkpoint with the lowest L m on the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Fine-tuning</head><p>After pre-training we fine-tune the learned representations on labeled data and add a randomly initialized output layer on top of the Transformer to predict characters (Librispeech/Libri-light) or phonemes (TIMIT). For Libri-light, we train three seeds with two different learning rates (2e-5 and 3e-5) for all subsets and choose the configuration with lowest WER on dev-other subset decoded with the official 4-gram language model (LM) with beam 50 and fixed model weights (LM weight 2, word insertion penalty -1). For BASE on the labeled 960h subset we use a learning rate of 1e-4.</p><p>We optimize with Adam and a tri-state rate schedule where the learning rate is warmed up for the first 10% of updates, held constant for the next 40% and then linearly decayed for the remainder. BASE uses a batch size of 3.2m samples per GPU and we fine-tune on 8 GPUs, giving a total batch size of 1,600sec. LARGE batches 1.28m samples on each GPU and we fine-tune on 24 GPUs, resulting in an effective batch size of 1,920sec. For the first 10k updates only the output classifier is trained, after which the Transformer is also updated. The feature encoder is not trained during fine-tuning. We mask the feature encoder representations with a strategy similar to SpecAugment <ref type="bibr" target="#b40">[41]</ref> detailed in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Language Models and Decoding</head><p>We consider two types of language models (LM): a 4-gram model and a Transformer <ref type="bibr" target="#b2">[3]</ref> trained on the Librispeech LM corpus. The Transformer LM is identical to <ref type="bibr" target="#b50">[51]</ref> and contains 20 blocks, model dimension 1,280, inner dimension 6,144 and 16 attention heads. We tune the weights of the language model (interval [0, 5]) and a word insertion penalty ([−5, 5]) via Bayesian optimization 3 : we run 128 trials with beam 500 for the 4-gram LM and beam 50 for the Transformer LM and choose the best set of weights according to performance on dev-other. Test performance is measured with beam 1,500 for the n-gram LM and beam 500 for the Transformer LM. We use the beam search decoder of <ref type="bibr" target="#b43">[44]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Low-Resource Labeled Data Evaluation</head><p>We first evaluate our pre-trained models in settings where the amount of labeled data is limited to get a sense of how the representations learned on unlabeled data can improve low resource settings. If a pre-trained model captures the structure of speech, then it should require few labeled examples to fine-tune it for speech recognition. The models are pre-trained on the audio data of either Librispeech (LS-960) or LibriVox (LV-60k) and most results are obtained by decoding with a Transformer language model (Transf.); Appendix C shows results with no language model at all as well as with an n-gram language model.</p><p>The LARGE model pre-trained on LV-60k and fine-tuned on only 10 minutes of labeled data achieves a word error rate of 5.2/8.6 on the Librispeech clean/other test sets. Ten minutes of labeled data corresponds to just 48 recordings with an average length of 12.5 seconds. This demonstrates that ultra-low resource speech recognition is possible with self-supervised learning on unlabeled data.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">High-Resource Labeled Data Evaluation on Librispeech</head><p>In this section we evaluate the performance when large quantities of labeled speech are available to assess the effectiveness of our approach in a high resource setup. Specifically, we fine-tune the same models as before on the full 960 hours of labeled Librispeech: BASE and LARGE pre-trained on LS-960 as well as LARGE pre-trained on LV-60k. <ref type="table" target="#tab_2">Table 2</ref> shows that our approach achieves WER 1.8/3.3 on test-clean/other on the full Librispeech benchmark. This is despite a weaker baseline architecture: supervised training of our architecture achieves WER 2.1/4.6 (LARGE -from scratch) compared to WER 1.9/4.1 for ContextNet <ref type="bibr" target="#b16">[17]</ref>, the baseline architecture of the state of the art <ref type="bibr" target="#b41">[42]</ref>. We use a simple Transformer with CTC which does not perform as well as seq2seq models <ref type="bibr" target="#b50">[51]</ref>.</p><p>Note that the vocabulary of our acoustic model (characters) does not match the vocabulary of the LM (words) which delays feedback from the LM and is likely to be detrimental. Most recent work <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b41">42]</ref> uses the better performing word pieces <ref type="bibr" target="#b49">[50]</ref> for both models. Moreover, our result is achieved without any data balancing such as <ref type="bibr" target="#b41">[42]</ref>. Finally, self-training is likely complimentary to pre-training and their combination may yield even better results. Appendix E presents a detailed error analysis of our pre-trained models in various labeled data setups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Phoneme Recognition on TIMIT</head><p>Next, we evaluate accuracy on TIMIT phoneme recognition by fine-tuning the pre-trained models on the labeled TIMIT training data. We fine-tune as for the 10 hour subset of Libri-light but do not use a language model. <ref type="table" target="#tab_3">Table 3</ref> shows that our approach can achieve a new state of the art on this dataset, reducing PER by a relative 23%/29% over the next best result on the dev/test sets. Appendix D shows an analysis of how the discrete latent speech representations related to phonemes. Other recent work on pre-training which evaluates on TIMIT includes <ref type="bibr" target="#b46">[47]</ref> who solve multiple tasks to learn good representations of speech.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Ablations</head><p>A difference to previous work <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b3">4]</ref> is that we quantize the latent audio representations only for the contrastive loss, i.e., when latents are used as targets, but not when the latents are input to the Transformer network. We motivate this choice by an ablating for which we adopt a reduced training setup to increase experimental turn around: we pre-train BASE on LS-960 for 250k updates with masking probability p = 0.075, fine-tune on train-10h for 60k updates on a single GPU with 640k samples per batch, or 40 sec of speech audio. We report the average WER and standard deviation on the concatenation of dev-clean and dev-other (dev PER) for three seeds of fine-tuning. <ref type="table" target="#tab_4">Table 4</ref> shows that our strategy of continuous inputs with quantized targets (Baseline) performs best. Continuous latent speech representations retain more information to enable better context representations and quantizing the target representations leads to more robust training. Quantizing the latents both in the input and the targets performs least well, and explains the lower performance of prior work <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b3">4]</ref>. Continuous targets reduce the effectiveness of self-supervised training since targets can capture detailed artifacts of the current sequence, e.g. speaker and background information, which make the task easier and prevent the model from learning general representations beneficial to speech recognition. The training accuracy of identifying the correct latent audio representation increases from 62% to 78.0% when switching from quantized to continuous targets. Continuous inputs and continuous targets perform second best but various attempts to improve it did not lead to better results (see Appendix F for this experiment and other ablations on various hyperparameters).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We presented wav2vec 2.0, a framework for self-supervised learning of speech representations which masks latent representations of the raw waveform and solves a contrastive task over quantized speech representations. Our experiments show the large potential of pre-training on unlabeled data for speech processing: when using only 10 minutes of labeled training data, or 48 recordings of 12.5 seconds on average, we achieve a WER of 4.8/8.2 on test-clean/other of Librispeech.</p><p>Our model achieves results which achieve a new state of the art on the full Librispeech benchmark for noisy speech. On the clean 100 hour Librispeech setup, wav2vec 2.0 outperforms the previous best result while using 100 times less labeled data. The approach is also effective when large amounts of labeled data are available. We expect performance gains by switching to a seq2seq architecture and a word piece vocabulary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>There are around 7,000 languages in the world and many more dialects. However, for most of them no speech recognition technology exists since current systems require hundreds or thousands of hours of labeled data which is hard to collect for most languages. We have shown that speech recognition models can be built with very small amounts of annotated data at very good accuracy. We hope our work will make speech recognition technology more broadly available to many more languages and dialects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Masking distribution</head><p>When choosing which time-steps to mask, each latent speech representation in an utterance is considered a candidate starting time-step with probability p where M is the length of each masked span starting from the respective time step; both are hyper-parameters. Sampled starting time steps are expanded to length M and spans can overlap.</p><p>For a 15 sec long audio sample, the average mask length is 14.7 time-steps, corresponding to 299ms of audio, with a median of 10 time-steps, and a maximum of about 100 time steps; about 49% of all time-steps in the sample will be masked. A plot of the corresponding mask length distribution is shown in <ref type="figure" target="#fig_4">Figure 2</ref> and an ablation of M and p as well as the effect of other masking strategies is shown in <ref type="table" target="#tab_5">Table 5</ref>. Reducing M results in increased prediction accuracy for the self-supervised but the task becomes trivial when spans with length one are masked, leading to poor performance on downstream speech recognition tasks. We also consider other masking strategies:    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Fine-tuning Setup</head><p>During fine-tuning we apply a masking strategy to the feature encoder outputs similar to SpecAugment <ref type="bibr" target="#b40">[41]</ref>: we randomly choose a number of starting time steps for which a span of ten subsequent time-steps is replaced with a mask embedding; spans may overlap and we use the same masked time step embedding as during pre-training. We also mask channels by choosing a number of channels as starting indices and then expand each one to cover the subsequent 64 channels. Spans may overlap and the selected channel spans are set to zero value. We use LayerDrop <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b11">12]</ref> at a rate of 0.05 for BASE and 0.1 for LARGE during fine-tuning. <ref type="table" target="#tab_6">Table 6</ref> summarizes the fine-tuning hyper-parameter settings used for the different labeled data setup. <ref type="table" target="#tab_7">Table 7</ref> shows the decoding parameters used for final evaluations of the various labeled data setups for Librispeech pre-trained models and <ref type="table" target="#tab_8">Table 8</ref> shows decoding parameters for LibriVox.     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Speech Recognition Error Analysis</head><p>In this section we study the most common errors our models make when fine-tuned on different amounts of labeled data <ref type="table" target="#tab_1">(Table 11</ref>). We also show transcriptions of a few relatively challenging utterances from the dev-clean subset of Librispeech <ref type="table" target="#tab_1">(Table 12)</ref>.</p><p>We consider models with no lexicon or no language model decoding, marked None in <ref type="table" target="#tab_9">Table 9</ref>: Larger capacity decreases error rates: LARGE on LS-960 improves the word error rate on dev-clean from 46.1 to 43 compared to BASE. Increasing the amount of unlabeled training data further decreases the error rate to 33.8 for LARGE on LS-960.</p><p>In the ten minute labeled data setup, the model is still able to recognize basic units of speech: <ref type="table" target="#tab_1">Table 11</ref> shows that most errors are around spelling of words, e.g., omitting silent characters such as could → coud, know → now, or ignoring repeated letters such as still → stil, little → litle. The LARGE LV-60k model achieves WER 38.3 on dev-clean and adding a Transformer language model enables to choose more likely pronunciations during the search and gives a large WER improvement to 5.0.</p><p>The ten minute models without lexicon and language model tend to spell words phonetically and omit repeated letters, e.g., will → wil <ref type="table" target="#tab_1">(Table 11</ref>). Spelling errors decrease with more labeled data: with one hour of labeled data, slightly less common words move into the list of the most frequent errors, e.g., heaven and food are spelled phonetically. At ten hours, top errors include articles, e.g., a, the which are a common source of errors in speech recognition in general. There are also alternative spellings, color vs. colour as well as relatively rare words including person names, still spelled phonetically, e.g., phoebe → feeby.</p><p>At 100 hours, person names dominate the most frequent errors: phoebe → phebe, along with incorrect spacing anyone → any one, awhile → a while. Finally at 960 hours the word error rate falls to 2% and top errors are mostly articles, incorrect splits, and some very rare words or names such as deucalion or gryce.</p><p>The "from scratch" 960 hour model has a similar word error rate as the 100 hour pre-trained model and displays a similar pattern of errors.</p><p>The pre-trained speech representations can be easily adapted to recognize specific sounds while fine-tuning grounds these representations to the actual spelling. <ref type="table" target="#tab_1">Table 11</ref>: Top word errors for models trained on 10m, 1h and 10h, 100h, 960h of labeled data and decoded on the Librispeech dev-clean subset without a language model or lexicon (see <ref type="table" target="#tab_9">Table 9</ref> and  <ref type="formula" target="#formula_4">(53)</ref> led → lead <ref type="formula" target="#formula_2">(12)</ref> cucumbers → cucombers (5) your → yor <ref type="bibr" target="#b52">(53)</ref> sea → see <ref type="formula" target="#formula_2">(12)</ref> egg → eg (5) could → coud <ref type="bibr" target="#b50">(51)</ref> thee → the <ref type="formula" target="#formula_2">(12)</ref> macklewain → macklewaine (5) here → hear <ref type="bibr" target="#b50">(51)</ref> tom → tome <ref type="formula" target="#formula_2">(12)</ref> magpie → magpi (5) know → now <ref type="bibr" target="#b44">(45)</ref> add → ad <ref type="formula" target="#formula_2">(11)</ref> milner → millner (5) there → ther <ref type="bibr" target="#b44">(45)</ref> good → god (11) stacy → staci (5) three → thre <ref type="bibr" target="#b44">(45)</ref> heaven → heven <ref type="formula" target="#formula_2">(11)</ref> trevelyan → trevellion (5) still → stil <ref type="bibr" target="#b41">(42)</ref> mary → marry <ref type="formula" target="#formula_2">(11)</ref> verloc → verlock (5) off → of <ref type="bibr" target="#b39">(40)</ref> randal → randel <ref type="formula" target="#formula_2">(11)</ref> ann → an (4) don't → dont <ref type="bibr" target="#b36">(37)</ref> answered → ansered <ref type="formula" target="#formula_2">(10)</ref> anyone → one (4) shall → shal <ref type="bibr" target="#b35">(36)</ref> blood → blod (10) apartment → appartment (4) little → litl <ref type="bibr" target="#b34">(35)</ref> bozzle → bosel (10) basin → bason <ref type="formula" target="#formula_5">(4)</ref> 100h LARGE LV-60k 960h LARGE LV-60k 960h LARGE from scratch a → the (13) a → the <ref type="bibr" target="#b11">(12)</ref> and → in <ref type="bibr" target="#b19">(20)</ref> and → in <ref type="bibr" target="#b9">(10)</ref> and → in (9) a → the (16) in → and <ref type="formula" target="#formula_2">(10)</ref> macklewain → mackelwaine <ref type="bibr" target="#b6">(7)</ref> in → and (13) o → oh <ref type="bibr" target="#b7">(8)</ref> in → and <ref type="formula">(6)</ref> the → a (10) minnetaki → minnitaki <ref type="bibr" target="#b6">(7)</ref> o → oh <ref type="formula">(6)</ref> in → an (8) randal → randall <ref type="bibr" target="#b6">(7)</ref> bozzle → bosell <ref type="bibr" target="#b4">(5)</ref> and → an (5) christie → cristy <ref type="bibr" target="#b5">(6)</ref> criss → chris <ref type="formula">(5)</ref> clarke → clark (4) macklewain → mackelwane (6) bozzle → bosel <ref type="bibr" target="#b3">(4)</ref> grethel → gretel (4) randal → randoll <ref type="bibr" target="#b5">(6)</ref> clarke → clark (4) macklewain → mackelwaine (4) bozzle → bosall <ref type="bibr" target="#b4">(5)</ref> colored → coloured (4) this → the (4) kaliko → calico <ref type="bibr" target="#b4">(5)</ref> grethel → gretel (4) an → and <ref type="formula" target="#formula_4">(3)</ref>   FEEBY merely glanced at it and gave it back 100h LV-60k BEBE merely glanced at it and gave it back 960h LV-60k phoebe merely glanced at it and gave it back 960h scratch phoebe merely glanced at it and gave it back Reference sauterne is a white bordeaux a strong luscious wine the best known varieties being 10m LV-60k SULTERIN is a white BORDOE a strong LUCHOUS WIN the best NOWN VERIATYS being 1h LV-60k CLTEREN is a white BORDO a strong LUCHIOUS wine the best known VERIETIES being 10h LV-60k SOTERN is a white BOURDO a strong LUCIOUS wine the best known VORIETIES being 100h LV-60k SOTERN is a white BORDAUX a strong LUCIOUS wine the best known varieties being 960h LV-60k SOTERN is a white bordeaux a strong luscious wine the best known varieties being 960h scratch SOTERAN is a white bordeaux a strong luscious wine the best known varieties being Reference i happen to have mac connell's box for tonight or there'd be no chance of our getting places 10m LV-60k i HAPEND to have MECONALES BOXS for TONIT ORE THIRLD be no chance of OR GETING places 1h LV-60k i happen to have MACCONNEL'S BOCXS for tonight or TE'ELD be no chance of our getting places 10h LV-60k i HAPPENED to have MUKONNEL'S box for tonight or THERED be no chance of our getting places 100h LV-60k i HAPPENED to have MC CONNEL'S box for TO NIGHT or there'd be no chance of our getting places 960h LV-60k i happen to have MC CONALL'S box for TO NIGHT or there'd be no chance of our getting places 960h scratch i HAPPENE to have MACONEL'S box for TO NIGHT or there'd be no chance of our getting places F Ablations <ref type="table" target="#tab_1">Table 13</ref> ablates various hyperparameter choices of our architecture. The setup for the baseline model is described in § 5.4. First, we tried to improve the continuous input and continuous target model ( § 5.4) by adding an MLP on top of the continuous target representation and we also tried to use a separate set of encoder parameters for the representations used as input and targets (Separate encoders). Both did not lead to meaningful improvements.</p><p>Increasing the receptive field size from 25ms to 30ms had little effect. Setting the diversity penalty weight (α) too low results in lower codebook usage and lower performance. Setting it too high leads to slight instability. Doubling the number of relative positional embeddings to 256 also did not help. Stopping gradients from the quantizer to the encoder shows that the encoder requires training signal from the quantizer as well.</p><p>Next, increasing the number of negatives did not result in better performance (K = 200) and sampling negatives from the entire batch of utterances hurt performance, likely because candidates from other utterances are easy to distinguish. Sampling negatives from any time step in the utterance, masked or unmasked, does not help and is more computationally expensive. Gumbel noise is important and increasing the number of codebooks did not result in better performance. We also investigated predicting only time steps immediately next to the last unmasked time step for each span. This enables to better control the difficulty of the pre-training task. Given the leftmost or rightmost unmasked time step next to a masked span, we compute the contrastive loss only for the first U masked time steps next to these unsmasked spans. Predicting only up to one time step performs poorly because there is little training signal from each utterance and predicting more time steps performs better but does not significantly outperform predicting all masked time steps. Increasing the number of training updates helps but this increases training time.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>X</head><label></label><figDesc>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " e F c / C c u f j Q X k I I g e g Q K Z W d i r b K 0 = " &gt; A A A B 8 n i c b V B N S 8 N A F H y p X 7 V + V T 1 6 W S y C p 5 K I o M e C F 4 9 V b C 2 k o W y 2 m 3 b p J h t 2 X 4 Q S + j O 8 e F D E q 7 / G m / / G T Z u D t g 4 s D D P v s f M m T K U w 6 L r f T m V t f W N z q 7 p d 2 9 n d 2 z + o H x 5 1 j c o 0 4 x 2 m p N K 9 k B o u R c I 7 K F D y X q o 5 j U P J H 8 P J T e E / P n F t h E o e c J r y I K a j R E S C U b S S 3 4 8 p j h m V e W 8 2 q D f c p j s H W S V e S R p Q o j 2 o f / W H i m U x T 5 B J a o z v u S k G O d U o m O S z W j 8 z P K V s Q k f c t z S h M T d B P o 8 8 I 2 d W G Z J I a f s S J H P 1 9 0 Z O Y 2 O m c W g n i 4 h m 2 S v E / z w / w + g 6 y E W S Z s g T t v g o y i R B R Y r 7 y V B o z l B O L a F M C 5 u V s D H V l K F t q W Z L 8 J Z P X i X d i 6 b n N r 2 7 y 0 b r v q y j C i d w C u f g w R W 0 4 B b a 0 A E G C p 7 h F d 4 c d F 6 c d + d j M V p x y p 1 j + A P n 8 w e Y E Z F + &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " e F c / C c u f j Q X k I I g e g Q K Z W d i r b K 0 = " &gt; A A A B 8 n i c b V B N S 8 N A F H y p X 7 V + V T 1 6 W S y C p 5 K I o M e C F 4 9 V b C 2 k o W y 2 m 3 b p J h t 2 X 4 Q S + j O 8 e F D E q 7 / G m / / G T Z u D t g 4 s D D P v s f M m T K U w 6 L r f T m V t f W N z q 7 p d 2 9 n d 2 z + o H x 5 1 j c o 0 4 x 2 m p N K 9 k B o u R c I 7 K F D y X q o 5 j U P J H 8 P J T e E / P n F t h E o e c J r y I K a j R E S C U b S S 3 4 8 p j h m V e W 8 2 q D f c p j s H W S V e S R p Q o j 2 o f / W H i m U x T 5 B J a o z v u S k G O d U o m O S z W j 8 z P K V s Q k f c t z S h M T d B P o 8 8 I 2 d W G Z J I a f s S J H P 1 9 0 Z O Y 2 O m c W g n i 4 h m 2 S v E / z w / w + g 6 y E W S Z s g T t v g o y i R B R Y r 7 y V B o z l B O L a F M C 5 u V s D H V l K F t q W Z L 8 J Z P X i X d i 6 b n N r 2 7 y 0 b r v q y j C i d w C u f g w R W 0 4 B b a 0 A E G C p 7 h F d 4 c d F 6 c d + d j M V p x y p 1 j + A P n 8 w e Y E Z F + &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " e F c / C c u f j Q X k I I g e g Q K Z W d i r b K 0 = " &gt; A A A B 8 n i c b V B N S 8 N A F H y p X 7 V + V T 1 6 W S y C p 5 K I o M e C F 4 9 V b C 2 k o W y 2 m 3 b p J h t 2 X 4 Q S + j O 8 e F D E q 7 / G m / / G T Z u D t g 4 s D D P v s f M m T K U w 6 L r f T m V t f W N z q 7 p d 2 9 n d 2 z + o H x 5 1 j c o 0 4 x 2 m p N K 9 k B o u R c I 7 K F D y X q o 5 j U P J H 8 P J T e E / P n F t h E o e c J r y I K a j R E S C U b S S 3 4 8 p j h m V e W 8 2 q D f c p j s H W S V e S R p Q o j 2 o f / W H i m U x T 5 B J a o z v u S k G O d U o m O S z W j 8 z P K V s Q k f c t z S h M T d B P o 8 8 I 2 d W G Z J I a f s S J H P 1 9 0 Z O Y 2 O m c W g n i 4 h m 2 S v E / z w / w + g 6 y E W S Z s g T t v g o y i R B R Y r 7 y V B o z l B O L a F M C 5 u V s D H V l K F t q W Z L 8 J Z P X i X d i 6 b n N r 2 7 y 0 b r v q y j C i d w C u f g w R W 0 4 B b a 0 A E G C p 7 h F d 4 c d F 6 c d + d j M V p x y p 1 j + A P n 8 w e Y E Z F + &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " e F c / C c u f j Q X k I I g e g Q K Z W d i r b K 0 = " &gt; A A A B 8 n i c b V B N S 8 N A F H y p X 7 V + V T 1 6 W S y C p 5 K I o M e C F 4 9 V b C 2 k o W y 2 m 3 b p J h t 2 X 4 Q S + j O 8 e F D E q 7 / G m / / G T Z u D t g 4 s D D P v s f M m T K U w 6 L r f T m V t f W N z q 7 p d 2 9 n d 2 z + o H x 5 1 j c o 0 4 x 2 m p N K 9 k B o u R c I 7 K F D y X q o 5 j U P J H 8 P J T e E / P n F t h E o e c J r y I K a j R E S C U b S S 3 4 8 p j h m V e W 8 2 q D f c p j s H W S V e S R p Q o j 2 o f / W H i m U x T 5 B J a o z v u S k G O d U o m O S z W j 8 z P K V s Q k f c t z S h M T d B P o 8 8 I 2 d W G Z J I a f s S J H P 1 9 0 Z O Y 2 O m c W g n i 4 h m 2 S v E / z w / w + g 6 y E W S Z s g T t v g o y i R B R Y r 7 y V B o z l B O L a F M C 5 u V s D H V l K F t q W Z L 8 J Z P X i X d i 6 b n N r 2 7 y 0 b r v q y j C i d w C u f g w R W 0 4 B b a 0 A E G C p 7 h F d 4 c d F 6 c d + d j M V p x y p 1 j + A P n 8 w e Y E Z F + &lt; / l a t e x i t &gt; Z &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " A b U B 2 0 Z k M B J J r N G S 9 C v v O Z F Q G F 8 = " &gt; A A A B 8 n i c b V D L S g M x F L 1 T X 7 W + q i 7 d B I v g q s x I Q d 0 V 3 L i s Y h 8 4 H U o m z b S h m W R I M k I Z + h l u X C j i 1 q 9 x 5 9 + Y a W e h r Q c C h 3 P u J e e e M O F M G 9 f 9 d k p r 6 x u b W + X t y s 7 u 3 v 5 B 9 f C o o 2 W q C G 0 T y a X q h V h T z g R t G 2 Y 4 7 S W K 4 j j k t B t O b n K / + 0 S V Z l I 8 m G l C g x i P B I s Y w c Z K f j / G Z k w w z x 5 n g 2 r N r b t z o F X i F a Q G B V q D 6 l d / K E k a U 2 E I x 1 r 7 n p u Y I M P K M M L p r N J P N U 0 w m e A R 9 S 0 V O K Y 6 y O a R Z + j M K k M U S W W f M G i u / t 7 I c K z 1 N A 7 t Z B 5 R L 3 u 5 + J / n p y a 6 C j I m k t R Q Q R Y f R S l H R q L 8 f j R k i h L D p 5 Z g o p j N i s g Y K 0 y M b a l i S / C W T 1 4 l n Y u 6 1 6 h f 3 z V q z f u i j j K c w C m c g w e X 0 I R b a E E b C E h 4 h l d 4 c 4 z z 4 r w 7 H 4 v R k l P s H M M f O J 8 / n v u R j A = = &lt; / l a t e x i t &gt; … … C &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " M d Y k d S c T E P C F Z + z k z F Y H z x 6 v s f U = " &gt; A A A B 8 n i c b V D L S g M x F M 3 U V 6 2 v q k s 3 w S K 4 K j N S U H e F b l x W s Q + Y D i W T Z t r Q T D I k d 4 Q y 9 D P c u F D E r V / j z r 8 x 0 8 5 C W w 8 E D u f c S 8 4 9 Y S K 4 A d f 9 d k o b m 1 v b O + X d y t 7 + w e F R 9 f i k a 1 S q K e t Q J Z T u h 8 Q w w S X r A A f B + o l m J A 4 F 6 4 X T V u 7 3 n p g 2 X M l H m C U s i M l Y 8 o h T A l b y B z G B C S U i a 8 2 H 1 Z p b d x f A 6 8 Q r S A 0 V a A + r X 4 O R o m n M J F B B j P E 9 N 4 E g I x o 4 F W x e G a S G J Y R O y Z j 5 l k o S M x N k i 8 h z f G G V E Y 6 U t k 8 C X q i / N z I S G z O L Q z u Z R z S r X i 7 + 5 / k p R D d B x m W S A p N 0 + V G U C g w K 5 / f j E d e M g p h Z Q q j m N i u m E 6 I J B d t S x Z b g r Z 6 8 T r p X d a 9 R v 7 1 v 1 J o P R R 1 l d I b O 0 S X y 0 D V q o j v U R h 1 E k U L P 6 B W 9 O e C 8 O O / O x 3 K 0 5 B Q 7 p + g P n M 8 f f A i R d Q = = &lt; / l a t e x i t &gt; Q &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " e d U m 7 D + S m 7 O 5 b C 2 L 0 b y Z u g O T X b o = " &gt; A A A B 8 n i c b V D L S g M x F M 3 U V 6 2 v q k s 3 w S K 4 K j N S U H c F N y 5 b s Q + Y D i W T Z t r Q T D I k d 4 Q y 9 D P c u F D E r V / j z r 8 x 0 8 5 C W w 8 E D u f c S 8 4 9 Y S K 4 A d f 9 d k o b m 1 v b O + X d y t 7 + w e F R 9 f i k a 1 S q K e t Q J Z T u h 8 Q w w S X r A A f B + o l m J A 4 F 6 4 X T u 9 z</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>r a l i i 3 B W z 1 5 n X S v 6 l 6 j f t t u 1 J o P R R 1 l d I b O 0 S X y 0 D V q o n v U Q h 1 E k U L P 6 B W 9 O e C 8 O O / O x 3 K 0 5 B Q 7 p + g P n M 8 f k U 6 R g w = = &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4 = " U o 4 s F A N + 3 p v Z 3 U m v 6 / J o B H X 7 l 0 Q = " &gt; A A A B 8 n i c b V D L S g M x F L 1 T X 7 W + q i 7 d B I v g q s x I Q d 0 V 3 L h w U c U + Y D q U T J p p Q z P J k G S E M v Q z 3 L h Q x K 1 f 4 8 6 / M d P O Q l s P B A 7 n 3 E v O P W H C m T a u + + 2 U 1 t Y 3 N r f K 2 5 W d 3 b 3 9 g + r h U U f L V B H a J p J L 1 Q u x p p w J 2 j b M c N p L F M V x y G k 3 n N z k f v e J K s 2 k e D T T h A Y x H g k W M Y K N l f x + j M 2 Y Y J 7 d z Q b V m l t 3 5 0 C r x C t I D Q q 0 B t W v / l C S N K b C E I 6 1 9 j 0 3 M U G G l W G E 0 1 m l n 2 q a Y D L B I + p b K n B M d Z D N I 8 / Q m V W G K J L K P m H Q X P 2 9 k e F Y 6 2 k c 2 s k 8 o l 7 2 c v E / z 0 9 N d B V k T C S p o Y I s P o p S j o x E + f 1 o y B Q l h k 8 t w U Q x m x W R M V a Y G N t S x Z b g L Z + 8 S j o X d a 9 R v 7 5 v 1 J o P R R 1 l O I F T O A c P L q E J t 9 C C N h C Q 8 A y v 8 O Y Y 5 8 V 5 d z 4 W o y W n 2 D m G P 3 A + f w C J t Z F + &lt; / l a t e x i t &gt;`C Illustration of our framework which jointly learns contextualized speech representations and an inventory of discretized speech units.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>w/o overlap uniform(a,b) samples for each starting index a span length M s from interval a to b and masks the subsequent M s time-steps taking care not to overlap with existing spans; poisson(λ) and normal(µ, σ) sample M s from Poisson and normal distributions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Mask length distribution for a 15 second sample with p = 0.065 and M = 10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Transformer setup: BASE contains 12 transformer blocks, model dimension 768, inner dimension (FFN) 3,072 and 8 attention heads. Batches are built by cropping 250k audio samples, or 15.6sec, from each example. Crops are batched together to not exceed 1.4m samples per GPU and we train on a total of 64 V100 GPUs for 1.6 days<ref type="bibr" target="#b37">[38]</ref>; the total batch size is 1.6h.The LARGE model contains 24 transformer blocks with model dimension 1,024, inner dimension 4,096 and 16 attention heads. We crop 320k audio samples, or 20sec, with a limit of 1.2m samples per GPU and train on 128 V100 GPUs over 2.3 days for Librispeech and 5.2 days for LibriVox; the total batch size is 2.7h. We use dropout 0.1 in the Transformer, at the output of the feature encoder and the input to the quantization module. Layers are dropped at a rate of 0.05 for BASE and 0.2 for LARGE</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>WER on the Librispeech dev/test sets when training on the Libri-light low-resource labeled data setups of 10 min, 1 hour, 10 hours and the clean 100h subset of Librispeech. Models use either the audio of Librispeech (LS-960) or the larger LibriVox (LV-60k) as unlabeled data. We consider two model sizes: BASE (95m parameters) and LARGE (317m parameters). Prior work used 860 unlabeled hours (LS-860) but the total with labeled data is 960 hours and comparable to our setup. recent iterative self-training approach<ref type="bibr" target="#b41">[42]</ref> represents the state of the art on the clean 100 hour subset of Librispeech but it requires multiple iterations of labeling, filtering, and re-training. Our approach is simpler: we pre-train on the unlabeled data and fine-tune on the labeled data. On the 100 hour subset of Librispeech, their method achieves WER 4.2/8.6 on test-clean/other which compares to</figDesc><table><row><cell>Model</cell><cell>Unlabeled data</cell><cell>LM</cell><cell cols="2">dev clean other</cell><cell cols="2">test clean other</cell></row><row><cell>10 min labeled</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Discrete BERT [4]</cell><cell>LS-960</cell><cell>4-gram</cell><cell>15.7</cell><cell>24.1</cell><cell>16.3</cell><cell>25.2</cell></row><row><cell>BASE</cell><cell>LS-960</cell><cell>4-gram</cell><cell>8.9</cell><cell>15.7</cell><cell>9.1</cell><cell>15.6</cell></row><row><cell></cell><cell></cell><cell>Transf.</cell><cell>6.6</cell><cell>13.2</cell><cell>6.9</cell><cell>12.9</cell></row><row><cell>LARGE</cell><cell>LS-960</cell><cell>Transf.</cell><cell>6.6</cell><cell>10.6</cell><cell>6.8</cell><cell>10.8</cell></row><row><cell></cell><cell>LV-60k</cell><cell>Transf.</cell><cell>4.6</cell><cell>7.9</cell><cell>4.8</cell><cell>8.2</cell></row><row><cell>1h labeled</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Discrete BERT [4]</cell><cell>LS-960</cell><cell>4-gram</cell><cell>8.5</cell><cell>16.4</cell><cell>9.0</cell><cell>17.6</cell></row><row><cell>BASE</cell><cell>LS-960</cell><cell>4-gram</cell><cell>5.0</cell><cell>10.8</cell><cell>5.5</cell><cell>11.3</cell></row><row><cell></cell><cell></cell><cell>Transf.</cell><cell>3.8</cell><cell>9.0</cell><cell>4.0</cell><cell>9.3</cell></row><row><cell>LARGE</cell><cell>LS-960</cell><cell>Transf.</cell><cell>3.8</cell><cell>7.1</cell><cell>3.9</cell><cell>7.6</cell></row><row><cell></cell><cell>LV-60k</cell><cell>Transf.</cell><cell>2.9</cell><cell>5.4</cell><cell>2.9</cell><cell>5.8</cell></row><row><cell>10h labeled</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Discrete BERT [4]</cell><cell>LS-960</cell><cell>4-gram</cell><cell>5.3</cell><cell>13.2</cell><cell>5.9</cell><cell>14.1</cell></row><row><cell>Iter. pseudo-labeling [58]</cell><cell>LS-960</cell><cell cols="3">4-gram+Transf. 23.51 25.48</cell><cell cols="2">24.37 26.02</cell></row><row><cell></cell><cell>LV-60k</cell><cell cols="3">4-gram+Transf. 17.00 19.34</cell><cell cols="2">18.03 19.92</cell></row><row><cell>BASE</cell><cell>LS-960</cell><cell>4-gram</cell><cell>3.8</cell><cell>9.1</cell><cell>4.3</cell><cell>9.5</cell></row><row><cell></cell><cell></cell><cell>Transf.</cell><cell>2.9</cell><cell>7.4</cell><cell>3.2</cell><cell>7.8</cell></row><row><cell>LARGE</cell><cell>LS-960</cell><cell>Transf.</cell><cell>2.9</cell><cell>5.7</cell><cell>3.2</cell><cell>6.1</cell></row><row><cell></cell><cell>LV-60k</cell><cell>Transf.</cell><cell>2.4</cell><cell>4.8</cell><cell>2.6</cell><cell>4.9</cell></row><row><cell>100h labeled</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Hybrid DNN/HMM [34]</cell><cell>-</cell><cell>4-gram</cell><cell>5.0</cell><cell>19.5</cell><cell>5.8</cell><cell>18.6</cell></row><row><cell>TTS data augm. [30]</cell><cell>-</cell><cell>LSTM</cell><cell></cell><cell></cell><cell>4.3</cell><cell>13.5</cell></row><row><cell>Discrete BERT [4]</cell><cell>LS-960</cell><cell>4-gram</cell><cell>4.0</cell><cell>10.9</cell><cell>4.5</cell><cell>12.1</cell></row><row><cell>Iter. pseudo-labeling [58]</cell><cell>LS-860</cell><cell>4-gram+Transf.</cell><cell>4.98</cell><cell>7.97</cell><cell>5.59</cell><cell>8.95</cell></row><row><cell></cell><cell>LV-60k</cell><cell>4-gram+Transf.</cell><cell>3.19</cell><cell>6.14</cell><cell>3.72</cell><cell>7.11</cell></row><row><cell>Noisy student [42]</cell><cell>LS-860</cell><cell>LSTM</cell><cell>3.9</cell><cell>8.8</cell><cell>4.2</cell><cell>8.6</cell></row><row><cell>BASE</cell><cell>LS-960</cell><cell>4-gram</cell><cell>2.7</cell><cell>7.9</cell><cell>3.4</cell><cell>8.0</cell></row><row><cell></cell><cell></cell><cell>Transf.</cell><cell>2.2</cell><cell>6.3</cell><cell>2.6</cell><cell>6.3</cell></row><row><cell>LARGE</cell><cell>LS-960</cell><cell>Transf.</cell><cell>2.1</cell><cell>4.8</cell><cell>2.3</cell><cell>5.0</cell></row><row><cell></cell><cell>LV-60k</cell><cell>Transf.</cell><cell>1.9</cell><cell>4.0</cell><cell>2.0</cell><cell>4.0</cell></row><row><cell cols="7">Our approach of jointly learning discrete units and contextualized representations clearly improves</cell></row><row><cell cols="7">over previous work which learned quantized audio units in a separate step [4], reducing WER by a</cell></row><row><cell>about a third.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>AWER 2.3/5.0 with the LARGE model in a like for like setup, a relative WER reduction of 45%/42%. When the LARGE model uses an order of magnitude less labeled data (10h labeled), then it still achieves WER 3.2/6.1, an error reduction of 24%/29% relative to iterative self-training. Using only a single hour of labeled data, the same model achieves WER 3.9/7.6 which improves on both test-clean and test-other by 7%/12% -with two orders of magnitude less labeled data. We note that the Libri-</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>WER on Librispeech when using all 960 hours of labeled data (cf.Table 1).</figDesc><table><row><cell>Model</cell><cell>Unlabeled data</cell><cell>LM</cell><cell cols="2">dev clean other</cell><cell cols="2">test clean other</cell></row><row><cell>Supervised</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CTC Transf [51]</cell><cell>-</cell><cell>CLM+Transf.</cell><cell>2.20</cell><cell>4.94</cell><cell>2.47</cell><cell>5.45</cell></row><row><cell>S2S Transf. [51]</cell><cell>-</cell><cell>CLM+Transf.</cell><cell>2.10</cell><cell>4.79</cell><cell>2.33</cell><cell>5.17</cell></row><row><cell>Transf. Transducer [60]</cell><cell>-</cell><cell>Transf.</cell><cell>-</cell><cell>-</cell><cell>2.0</cell><cell>4.6</cell></row><row><cell>ContextNet [17]</cell><cell>-</cell><cell>LSTM</cell><cell>1.9</cell><cell>3.9</cell><cell>1.9</cell><cell>4.1</cell></row><row><cell>Conformer [15]</cell><cell>-</cell><cell>LSTM</cell><cell>2.1</cell><cell>4.3</cell><cell>1.9</cell><cell>3.9</cell></row><row><cell>Semi-supervised</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CTC Transf. + PL [51]</cell><cell>LV-60k</cell><cell>CLM+Transf.</cell><cell>2.10</cell><cell>4.79</cell><cell>2.33</cell><cell>4.54</cell></row><row><cell>S2S Transf. + PL [51]</cell><cell>LV-60k</cell><cell>CLM+Transf.</cell><cell>2.00</cell><cell>3.65</cell><cell>2.09</cell><cell>4.11</cell></row><row><cell>Iter. pseudo-labeling [58]</cell><cell>LV-60k</cell><cell>4-gram+Transf.</cell><cell>1.85</cell><cell>3.26</cell><cell>2.10</cell><cell>4.01</cell></row><row><cell>Noisy student [42]</cell><cell>LV-60k</cell><cell>LSTM</cell><cell>1.6</cell><cell>3.4</cell><cell>1.7</cell><cell>3.4</cell></row><row><cell>This work</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LARGE -from scratch</cell><cell>-</cell><cell>Transf.</cell><cell>1.7</cell><cell>4.3</cell><cell>2.1</cell><cell>4.6</cell></row><row><cell>BASE</cell><cell>LS-960</cell><cell>Transf.</cell><cell>1.8</cell><cell>4.7</cell><cell>2.1</cell><cell>4.8</cell></row><row><cell>LARGE</cell><cell>LS-960</cell><cell>Transf.</cell><cell>1.7</cell><cell>3.9</cell><cell>2.0</cell><cell>4.1</cell></row><row><cell></cell><cell>LV-60k</cell><cell>Transf.</cell><cell>1.6</cell><cell>3.0</cell><cell>1.8</cell><cell>3.3</cell></row></table><note>light data splits contain both clean and noisy data leading to better accuracy on test-other compared to test-clean. Increasing model size reduces WER on all setups with the largest improvements on test-other (BASE vs. LARGE both on LS-960) and increasing the amount of unlabeled training data also leads to large improvements (LARGE LS-960 vs. LV-60k).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>TIMIT phoneme recognition accuracy in terms of phoneme error rate (PER).</figDesc><table><row><cell></cell><cell cols="2">dev PER test PER</cell></row><row><cell>CNN + TD-filterbanks [59]</cell><cell>15.6</cell><cell>18.0</cell></row><row><cell>PASE+ [47]</cell><cell>-</cell><cell>17.2</cell></row><row><cell>Li-GRU + fMLLR [46]</cell><cell>-</cell><cell>14.9</cell></row><row><cell>wav2vec [49]</cell><cell>12.9</cell><cell>14.7</cell></row><row><cell>vq-wav2vec [5]</cell><cell>9.6</cell><cell>11.6</cell></row><row><cell>This work (no LM)</cell><cell></cell><cell></cell></row><row><cell>LARGE (LS-960)</cell><cell>7.4</cell><cell>8.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Average WER and standard deviation on combined dev-clean/other of Librispeech for three training seeds. We ablate quantizing the context network input and the targets in the contrastive loss.</figDesc><table><row><cell>avg. WER</cell><cell>std.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Ablations on settings for the masking strategy during pre-training. When masking without overlap, we choose starting time steps with p = 0.037 which results in the total number of masked tokens to match the baseline.</figDesc><table><row><cell></cell><cell>avg WER</cell><cell>std</cell></row><row><cell>Baseline (p = 0.075)</cell><cell cols="2">7.97 0.02</cell></row><row><cell>Mask length M = 8</cell><cell cols="2">8.33 0.05</cell></row><row><cell>Mask length M = 12</cell><cell cols="2">8.19 0.08</cell></row><row><cell>Mask length M = 15</cell><cell cols="2">8.43 0.19</cell></row><row><cell>Mask probability p = 0.065</cell><cell cols="2">7.95 0.08</cell></row><row><cell>Mask probability p = 0.06</cell><cell cols="2">8.14 0.22</cell></row><row><cell>Mask w/o overlap, uniform(1,31)</cell><cell cols="2">8.39 0.02</cell></row><row><cell>Mask w/o overlap, uniform(10,30)</cell><cell cols="2">9.17 0.05</cell></row><row><cell>Mask w/o overlap, poisson(15)</cell><cell cols="2">8.13 0.04</cell></row><row><cell>Mask w/o overlap, normal(15, 10)</cell><cell cols="2">8.37 0.03</cell></row><row><cell>Mask w/o overlap, length 10</cell><cell cols="2">9.15 0.02</cell></row><row><cell>Mask w/o overlap, length 15</cell><cell cols="2">9.43 0.26</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">Fine-tuning hyperparameters</cell><cell></cell></row><row><cell></cell><cell cols="3">timestep mask prob. channel mask prob. updates</cell></row><row><cell>10 min</cell><cell>0.075</cell><cell>0.008</cell><cell>12k</cell></row><row><cell>1 hour</cell><cell>0.075</cell><cell>0.004</cell><cell>13k</cell></row><row><cell>10 hours</cell><cell>0.065</cell><cell>0.004</cell><cell>20k</cell></row><row><cell>100 hours</cell><cell>0.05</cell><cell>0.008</cell><cell>50k</cell></row><row><cell>960 hours</cell><cell>0.05</cell><cell>0.0016</cell><cell>320k</cell></row><row><cell>TIMIT</cell><cell>0.065</cell><cell>0.012</cell><cell>40k</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Decoding parameters for Librispeech subsets for models pre-trained on Librispeech 4gram LM weight 4gram word insert. TransLM weight TransLM word insert.</figDesc><table><row><cell>10 min</cell><cell>3.23</cell><cell>-0.26</cell><cell>1.20</cell><cell>-1.39</cell></row><row><cell>1 hour</cell><cell>2.90</cell><cell>-1.62</cell><cell>1.15</cell><cell>-2.08</cell></row><row><cell>10 hours</cell><cell>2.46</cell><cell>-0.59</cell><cell>1.06</cell><cell>-2.32</cell></row><row><cell>100 hours</cell><cell>2.15</cell><cell>-0.52</cell><cell>0.87</cell><cell>-1.00</cell></row><row><cell>960 hours</cell><cell>1.74</cell><cell>0.52</cell><cell>0.92</cell><cell>-0.86</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Decoding parameters for Librispeech subsets for models pre-trained on Librivox. 4gram LM weight 4gram word insert. TransLM weight TransLM word insert.</figDesc><table><row><cell>10 min</cell><cell>3.86</cell><cell>-1.18</cell><cell>1.47</cell><cell>-2.82</cell></row><row><cell>1 hour</cell><cell>3.09</cell><cell>-2.33</cell><cell>1.33</cell><cell>-0.69</cell></row><row><cell>10 hours</cell><cell>2.12</cell><cell>-0.90</cell><cell>0.94</cell><cell>-1.05</cell></row><row><cell>100 hours</cell><cell>2.15</cell><cell>-0.52</cell><cell>0.87</cell><cell>-1.00</cell></row><row><cell>960 hours</cell><cell>1.57</cell><cell>-0.64</cell><cell>0.90</cell><cell>-0.31</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>WER on the Librispeech dev/test sets when training on the Libri-light low-resource labeled data setups (cf. Table 1).</figDesc><table><row><cell>Model</cell><cell>Unlabeled data</cell><cell>LM</cell><cell cols="2">dev clean other</cell><cell cols="2">test clean other</cell></row><row><cell cols="2">10 min labeled</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BASE</cell><cell>LS-960</cell><cell>None</cell><cell>46.1</cell><cell>51.5</cell><cell>46.9</cell><cell>50.9</cell></row><row><cell></cell><cell></cell><cell>4-gram</cell><cell>8.9</cell><cell>15.7</cell><cell>9.1</cell><cell>15.6</cell></row><row><cell></cell><cell></cell><cell>Transf.</cell><cell>6.6</cell><cell>13.2</cell><cell>6.9</cell><cell>12.9</cell></row><row><cell>LARGE</cell><cell>LS-960</cell><cell>None</cell><cell>43.0</cell><cell>46.3</cell><cell>43.5</cell><cell>45.3</cell></row><row><cell></cell><cell></cell><cell>4-gram</cell><cell>8.6</cell><cell>12.9</cell><cell>8.9</cell><cell>13.1</cell></row><row><cell></cell><cell></cell><cell>Transf.</cell><cell>6.6</cell><cell>10.6</cell><cell>6.8</cell><cell>10.8</cell></row><row><cell>LARGE</cell><cell>LV-60k</cell><cell>None</cell><cell>38.3</cell><cell>41.0</cell><cell>40.2</cell><cell>38.7</cell></row><row><cell></cell><cell></cell><cell>4-gram</cell><cell>6.3</cell><cell>9.8</cell><cell>6.6</cell><cell>10.3</cell></row><row><cell></cell><cell></cell><cell>Transf.</cell><cell>4.6</cell><cell>7.9</cell><cell>4.8</cell><cell>8.2</cell></row><row><cell cols="2">1h labeled</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BASE</cell><cell>LS-960</cell><cell>None</cell><cell>24.1</cell><cell>29.6</cell><cell>24.5</cell><cell>29.7</cell></row><row><cell></cell><cell></cell><cell>4-gram</cell><cell>5.0</cell><cell>10.8</cell><cell>5.5</cell><cell>11.3</cell></row><row><cell></cell><cell></cell><cell>Transf.</cell><cell>3.8</cell><cell>9.0</cell><cell>4.0</cell><cell>9.3</cell></row><row><cell>LARGE</cell><cell>LS-960</cell><cell>None</cell><cell>21.6</cell><cell>25.3</cell><cell>22.1</cell><cell>25.3</cell></row><row><cell></cell><cell></cell><cell>4-gram</cell><cell>4.8</cell><cell>8.5</cell><cell>5.1</cell><cell>9.4</cell></row><row><cell></cell><cell></cell><cell>Transf.</cell><cell>3.8</cell><cell>7.1</cell><cell>3.9</cell><cell>7.6</cell></row><row><cell>LARGE</cell><cell>LV-60k</cell><cell>None</cell><cell>17.3</cell><cell>20.6</cell><cell>17.2</cell><cell>20.3</cell></row><row><cell></cell><cell></cell><cell>4-gram</cell><cell>3.6</cell><cell>6.5</cell><cell>3.8</cell><cell>7.1</cell></row><row><cell></cell><cell></cell><cell>Transf.</cell><cell>2.9</cell><cell>5.4</cell><cell>2.9</cell><cell>5.8</cell></row><row><cell cols="2">10h labeled</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BASE</cell><cell>LS-960</cell><cell>None</cell><cell>10.9</cell><cell>17.4</cell><cell>11.1</cell><cell>17.6</cell></row><row><cell></cell><cell></cell><cell>4-gram</cell><cell>3.8</cell><cell>9.1</cell><cell>4.3</cell><cell>9.5</cell></row><row><cell></cell><cell></cell><cell>Transf.</cell><cell>2.9</cell><cell>7.4</cell><cell>3.2</cell><cell>7.8</cell></row><row><cell>LARGE</cell><cell>LS-960</cell><cell>None</cell><cell>8.1</cell><cell>12.0</cell><cell>8.0</cell><cell>12.1</cell></row><row><cell></cell><cell></cell><cell>4-gram</cell><cell>3.4</cell><cell>6.9</cell><cell>3.8</cell><cell>7.3</cell></row><row><cell></cell><cell></cell><cell>Transf.</cell><cell>2.9</cell><cell>5.7</cell><cell>3.2</cell><cell>6.1</cell></row><row><cell>LARGE</cell><cell>LV-60k</cell><cell>None</cell><cell>6.3</cell><cell>9.8</cell><cell>6.3</cell><cell>10.0</cell></row><row><cell></cell><cell></cell><cell>4-gram</cell><cell>2.6</cell><cell>5.5</cell><cell>3.0</cell><cell>5.8</cell></row><row><cell></cell><cell></cell><cell>Transf.</cell><cell>2.4</cell><cell>4.8</cell><cell>2.6</cell><cell>4.9</cell></row><row><cell cols="2">100h labeled</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BASE</cell><cell>LS-960</cell><cell>None</cell><cell>6.1</cell><cell>13.5</cell><cell>6.1</cell><cell>13.3</cell></row><row><cell></cell><cell></cell><cell>4-gram</cell><cell>2.7</cell><cell>7.9</cell><cell>3.4</cell><cell>8.0</cell></row><row><cell></cell><cell></cell><cell>Transf.</cell><cell>2.2</cell><cell>6.3</cell><cell>2.6</cell><cell>6.3</cell></row><row><cell>LARGE</cell><cell>LS-960</cell><cell>None</cell><cell>4.6</cell><cell>9.3</cell><cell>4.7</cell><cell>9.0</cell></row><row><cell></cell><cell></cell><cell>4-gram</cell><cell>2.3</cell><cell>5.7</cell><cell>2.8</cell><cell>6.0</cell></row><row><cell></cell><cell></cell><cell>Transf.</cell><cell>2.1</cell><cell>4.8</cell><cell>2.3</cell><cell>5.0</cell></row><row><cell>LARGE</cell><cell>LV-60k</cell><cell>None</cell><cell>3.3</cell><cell>6.5</cell><cell>3.1</cell><cell>6.3</cell></row><row><cell></cell><cell></cell><cell>4-gram</cell><cell>1.8</cell><cell>4.5</cell><cell>2.3</cell><cell>4.6</cell></row><row><cell></cell><cell></cell><cell>Transf.</cell><cell>1.9</cell><cell>4.0</cell><cell>2.0</cell><cell>4.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>WER on Librispeech when using all 960 hours of Librispeech as labeled data (cf.Table 2).</figDesc><table><row><cell>Model</cell><cell>Unlabeled data</cell><cell>LM</cell><cell cols="2">dev clean other</cell><cell cols="2">test clean other</cell></row><row><cell>LARGE -from scratch</cell><cell>-</cell><cell>None</cell><cell>2.8</cell><cell>7.6</cell><cell>3.0</cell><cell>7.7</cell></row><row><cell></cell><cell>-</cell><cell>4-gram</cell><cell>1.8</cell><cell>5.4</cell><cell>2.6</cell><cell>5.8</cell></row><row><cell></cell><cell>-</cell><cell>Transf.</cell><cell>1.7</cell><cell>4.3</cell><cell>2.1</cell><cell>4.6</cell></row><row><cell>BASE</cell><cell>LS-960</cell><cell>None</cell><cell>3.2</cell><cell>8.9</cell><cell>3.4</cell><cell>8.5</cell></row><row><cell></cell><cell></cell><cell>4-gram</cell><cell>2.0</cell><cell>5.9</cell><cell>2.6</cell><cell>6.1</cell></row><row><cell></cell><cell></cell><cell>Transf.</cell><cell>1.8</cell><cell>4.7</cell><cell>2.1</cell><cell>4.8</cell></row><row><cell>LARGE</cell><cell>LS-960</cell><cell>None</cell><cell>2.6</cell><cell>6.5</cell><cell>2.8</cell><cell>6.3</cell></row><row><cell></cell><cell></cell><cell>4-gram</cell><cell>1.7</cell><cell>4.6</cell><cell>2.3</cell><cell>5.0</cell></row><row><cell></cell><cell></cell><cell>Transf.</cell><cell>1.7</cell><cell>3.9</cell><cell>2.0</cell><cell>4.1</cell></row><row><cell>LARGE</cell><cell>LV-60k</cell><cell>None</cell><cell>2.1</cell><cell>4.5</cell><cell>2.2</cell><cell>4.5</cell></row><row><cell></cell><cell></cell><cell>4-gram</cell><cell>1.4</cell><cell>3.5</cell><cell>2.0</cell><cell>3.6</cell></row><row><cell></cell><cell></cell><cell>Transf.</cell><cell>1.6</cell><cell>3.0</cell><cell>1.8</cell><cell>3.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 -</head><label>10</label><figDesc>None). In brackets is the total number of occurrences of each error.</figDesc><table><row><cell>10m LARGE LV-60k</cell><cell>1h LARGE LV-60k</cell><cell>10h LARGE LV-60k</cell></row><row><cell>all → al (181) are → ar (115) will → wil (100) you → yo (90) one → on (89) two → to (81) well → wel (80) been → ben (73) upon → apon (73) good → god (67) see → se (66) we → whe (60) little → litle (54) great → grate</cell><cell>too → to (26) until → untill (24) new → knew (22) door → dor (18) says → sais (18) soul → sol (17) bread → bred (16) poor → pore (16) a → the (13) either → ither (13) food → fud (13) doubt → dout (12) earth → erth (12)</cell><cell>in → and (15) a → the (11) o → oh (10) and → in (9) mode → mod (9) ursus → ersus (9) tom → tome (8) randal → randol (7) the → a (7) color → colour (6) flour → flower (6) phoebe → feeby (6) an → and (5)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 12 :</head><label>12</label><figDesc>Examples of transcription of selected utterances from the dev-clean subset by various models without a language model or lexicon. Capitalized words indicate errors.</figDesc><table><row><cell>Model</cell><cell>Transcription</cell></row><row><cell>Reference</cell><cell>i'm mister christopher from london</cell></row><row><cell cols="2">10m LV-60k IM mister CRESTIFER FROME LUNDEN</cell></row><row><cell>1h LV-60k</cell><cell>IM mister CRISTIFFHER from LOUNDEN</cell></row><row><cell>10h LV-60k</cell><cell>i'm mister CHRYSTEPHER from london</cell></row><row><cell cols="2">100h LV-60k i'm mister christopher from london</cell></row><row><cell cols="2">960h LV-60k i'm mister christopher from london</cell></row><row><cell cols="2">960h scratch I MISSTER christopher from london</cell></row><row><cell>Reference</cell><cell>il popolo e una bestia</cell></row><row><cell cols="2">10m LV-60k ILPOPULAR ONABESTIA</cell></row><row><cell>1h LV-60k</cell><cell>O POPOLAONABASTIA</cell></row><row><cell>10h LV-60k</cell><cell>U POPULAONABASTIAR</cell></row><row><cell cols="2">100h LV-60k O POPALOON A BASTYA</cell></row><row><cell cols="2">960h LV-60k YOU'LL POP A LAWYE ON A BAISTYE</cell></row><row><cell cols="2">960h scratch OL POPALOY ON ABESTIA</cell></row><row><cell>Reference</cell><cell>he smelt the nutty aroma of the spirit</cell></row><row><cell cols="2">10m LV-60k he SMELTD the NUDY aroma of the spirit</cell></row><row><cell>1h LV-60k</cell><cell>he SMELTD the NUDDY ARROMA of the spirit</cell></row><row><cell>10h LV-60k</cell><cell>he smelt the NUDDY ERROMA of the spirit</cell></row><row><cell cols="2">100h LV-60k he smelt the NUDDY aroma of the spirit</cell></row><row><cell cols="2">960h LV-60k he smelt the NUTTIE aroma of the spirit</cell></row><row><cell cols="2">960h scratch he smelt the nutty EROMA of the spirit</cell></row><row><cell>Reference</cell><cell>phoebe merely glanced at it and gave it back</cell></row><row><cell cols="2">10m LV-60k FEABY MEARLY glanced at it and gave it BAK</cell></row><row><cell>1h LV-60k</cell><cell>FIEABY merely glanced at it and gave it back</cell></row><row><cell>10h LV-60k</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 13 :</head><label>13</label><figDesc>Ablation of various hyper-parmeter choices. We report average WER and standard deviation on combined dev-clean/other of Librispeech for three seeds of training.avg. WER std.</figDesc><table><row><cell>Baseline (p = 0.075, α = 0.1)</cell><cell>7.97 0.02</cell></row><row><cell>Continuous inputs, continuous targets</cell><cell>8.58 0.08</cell></row><row><cell>+ MLP on targets</cell><cell>8.51 0.05</cell></row><row><cell>+ Separate encoders</cell><cell>8.90 0.01</cell></row><row><cell>receptive field 30ms</cell><cell>7.99 0.06</cell></row><row><cell>diversity penalty</cell><cell></cell></row><row><cell>α = 0</cell><cell>8.48 0.08</cell></row><row><cell>α = 0.05</cell><cell>8.34 0.08</cell></row><row><cell>α = 0.2</cell><cell>8.58 0.45</cell></row><row><cell>Conv pos emb, kernel 256</cell><cell>8.14 0.05</cell></row><row><cell>No gradient to encoder from quantizer</cell><cell>8.41 0.08</cell></row><row><cell>Negatives</cell><cell></cell></row><row><cell>K = 200 same utterance</cell><cell>8.12 0.05</cell></row><row><cell>K = 50 same utterance + K = 50 from batch</cell><cell>8.79 0.06</cell></row><row><cell>Sample negatives from any time step</cell><cell>8.07 0.02</cell></row><row><cell>No Gumbel noise</cell><cell>8.73 0.42</cell></row><row><cell>Codebook</cell><cell></cell></row><row><cell>G=4, V=18</cell><cell>9.02 0.38</cell></row><row><cell>G=8, V=8</cell><cell>8.13 0.07</cell></row><row><cell>Predict exactly U time steps from edges</cell><cell></cell></row><row><cell>U = 1</cell><cell>9.53 0.91</cell></row><row><cell>U = 5</cell><cell>8.19 0.07</cell></row><row><cell>U = 10</cell><cell>8.07 0.07</cell></row><row><cell>U = 15</cell><cell>7.89 0.10</cell></row><row><cell>U = 20</cell><cell>7.90 0.01</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Code and models are available at https://github.com/pytorch/fairseq Preprint. Under review.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/facebook/Ax</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Tatiana Likhomanenko and Qiantong Xu for helpful discussion and their help with wav2letter integration.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Analysis of Discrete Latent Speech Representations</head><p>Next, we investigate whether the discrete latent speech representations q t learned by the quantizer relate to phonetic information: Using LARGE pre-trained on LV-60k and without any fine-tuning, we compute the discrete latents for the training data of TIMIT and compute the co-occurrence between human annotated phonemes and the latents. Ties are broken by choosing the phoneme which is most represented in the receptive field of q t . The training data contains 3696 utterances of average length 13.6 sec, or 563k discrete latents. <ref type="figure">Figure 3</ref> plots P (phoneme|q t ) and shows that many discrete latents appear to specialize in specific phonetic sounds. The silence phoneme (bcl) represents 22% of all human annotated speech data and is therefore also modeled by many different latents.  <ref type="figure">Figure 3</ref>: Visualization of the co-occurrence between discrete latent speech representations and phonemes. We plot the conditional probability P (phoneme|q t ) on TIMIT train data. The y-axis shows the collapsed 39 classes of phonemes and the x-axis is over the different discrete latents.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<title level="m">Layer normalization. arXiv</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning representations by maximizing mutual information across views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Buchwalter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NeurIPS</title>
		<meeting>of NeurIPS</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adaptive input representations for neural language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Effectiveness of self-supervised pre-training for speech recognition. arXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">vq-wav2vec: Self-supervised learning of discrete speech representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno>abs/2002.05709</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<idno>abs/1901.08810</idno>
		<title level="m">Unsupervised speech representation learning using wavenet autoencoders. arXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">An unsupervised autoregressive model for speech representation learning. arXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Glass</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bert</surname></persName>
		</author>
		<idno>abs/1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding. arXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The challenge of realistic music generation: modelling raw audio at scale. arXiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Unsupervised acoustic unit discovery for speech synthesis using discrete latent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Eloff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nortje</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Niekerk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Govender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nortje</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pretorius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Van Biljon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Van Der Westhuizen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Staden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kamper</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>variable neural networks. arXiv, abs/1904.07556</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Reducing transformer depth on demand with structured dropout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Garofolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">F</forename><surname>Lamel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Fiscus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Pallett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">L</forename><surname>Dahlgren</surname></persName>
		</author>
		<title level="m">The DARPA TIMIT Acoustic-Phonetic Continuous Speech Corpus CDROM. Linguistic Data Consortium</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<title level="m">Conformer: Convolution-augmented transformer for speech recognition. arXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Statistical theory of extreme values and some practical applications: a series of lectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Gumbel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">US Government Printing Office</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="1954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<title level="m">Contextnet: Improving convolutional neural networks for automatic speech recognition with global context. arXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning hierarchical discrete linguistic units from visually-grounded speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Harwath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-N</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Momentum contrast for unsupervised visual representation learning. arXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">J</forename><surname>Hénaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<idno>abs/1905.09272</idno>
		<title level="m">Data-efficient image recognition with contrastive predictive coding. arXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<title level="m">Gaussian error linear units (gelus). arXiv</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
		<title level="m">Deep networks with stochastic depth. arXiv</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Noise-contrastive estimation: A new estimation principle for unnormalized statistical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G A</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AISTATS</title>
		<meeting>of AISTATS</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Categorical reparameterization with gumbel-softmax. arXiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<idno>abs/1611.01144</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Product quantization for nearest neighbor search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="117" to="128" />
			<date type="published" when="2011-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Improving transformer-based speech recognition using unsupervised pre-training. arXiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<idno>abs/1910.09932</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Libri-light: A benchmark for asr with limited or no supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<title level="m">Learning robust and multilingual speech representations. arXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">You do not need more data: Improving end-to-end speech recognition by text-to-speech data augmentation. arXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Korostik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Svischev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andrusenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Medennikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rybin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Ethnologue: Languages of the world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">F</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Fennig</surname></persName>
		</author>
		<ptr target="http://www.ethnologue.com" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>nineteenth edition</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Towards unsupervised speech recognition and synthesis with quantized speech representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yi Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shan Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Rwth asr systems for librispeech: Hybrid vs attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lüscher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kitza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A* sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Minka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3086" to="3094" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Self-supervised learning of pretext-invariant representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Okhonko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<title level="m">Transformers with convolutional context for ASR. arXiv, abs</title>
		<imprint>
			<date type="published" when="1904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Scaling neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of WMT</title>
		<meeting>of WMT</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">fairseq: A fast, extensible toolkit for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL System Demonstrations</title>
		<meeting>of NAACL System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Librispeech: an asr corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Specaugment: A simple data augmentation method for automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Interspeech</title>
		<meeting>of Interspeech</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Improved noisy student training for automatic speech recognition. arXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Wav2letter++: A fast open-source speech recognition system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pratap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Liptchinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Light gated recurrent units for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omologo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Emerging Topics in Computational Intelligence</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="92" to="102" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Multi-task self-supervised learning for robust speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pascual</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Swietojanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Monteiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Trmal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Unsupervised pretraining transfers well across languages. arXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rivière</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-E</forename><surname>Mazaré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dupoux</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">wav2vec: Unsupervised pre-training for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Interspeech</title>
		<meeting>of Interspeech</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Japanese and korean voice search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nakajima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">End-to-end ASR: from Supervised to Semi-Supervised Learning with Modern Architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Likhomanenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pratap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sriram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Liptchinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<idno>abs/1911.08460</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Vqvae unsupervised unit discovery and multi-scale code2spec inverter for zerospeech challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tjandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sisman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sakti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nakamura</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1905" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Neural discrete representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6306" to="6315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno>abs/1807.03748</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Representation learning with contrastive predictive coding. arXiv</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Unsupervised pre-training of bidirectional speech encoders via masked reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Livescu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Pay less attention with lightweight and dynamic convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Iterative pseudo-labeling for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Likhomanenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Learning filterbanks from raw speech for phone recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zeghidour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schaiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dupoux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Transformer transducer: A streamable speech recognition model with transformer encoders and rnn-t loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
