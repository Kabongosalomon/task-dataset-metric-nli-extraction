<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CoMatch: Semi-supervised Learning with Contrastive Graph Regularization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
							<email>junnan.li@salesforce.com</email>
							<affiliation key="aff0">
								<orgName type="department">Salesforce Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
							<email>cxiong@salesforce.com</email>
							<affiliation key="aff0">
								<orgName type="department">Salesforce Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
							<email>shoi@salesforce.com</email>
							<affiliation key="aff0">
								<orgName type="department">Salesforce Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">CoMatch: Semi-supervised Learning with Contrastive Graph Regularization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semi-supervised learning has been an effective paradigm for leveraging unlabeled data to reduce the reliance on labeled data. We propose CoMatch, a new semi-supervised learning method that unifies dominant approaches and addresses their limitations. CoMatch jointly learns two representations of the training data, their class probabilities and low-dimensional embeddings. The two representations interact with each other to jointly evolve. The embeddings impose a smoothness constraint on the class probabilities to improve the pseudo-labels, whereas the pseudo-labels regularize the structure of the embeddings through graph-based contrastive learning. CoMatch achieves state-of-the-art performance on multiple datasets. It achieves substantial accuracy improvements on the label-scarce CIFAR-10 and STL-10. On ImageNet with 1% labels, CoMatch achieves a top-1 accuracy of 66.0%, outperforming FixMatch [34] by 12.6%. Furthermore, CoMatch achieves better representation learning performance on downstream tasks, outperforming both supervised learning and self-supervised learning. Code and pre-trained models are available at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Semi-supervised learning (SSL) -learning from few labeled data and a large amount of unlabeled data -has been a long-standing problem in computer vision and machine learning. Recent state-of-the-art methods mostly follow two trends: (1) using the model's class prediction to produce a pseudo-label for each unlabeled sample as the label to train against <ref type="bibr" target="#b19">[21,</ref><ref type="bibr" target="#b1">3,</ref><ref type="bibr" target="#b0">2,</ref><ref type="bibr" target="#b32">34]</ref>; <ref type="bibr" target="#b0">(2)</ref> unsupervised or self-supervised pre-training, followed by supervised finetuning <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b13">15,</ref><ref type="bibr" target="#b12">14,</ref><ref type="bibr" target="#b2">4]</ref> and pseudo-labeling <ref type="bibr" target="#b5">[7]</ref>.</p><p>However, existing methods have several limitations. Pseudo-labeling (also called self-training) methods heavily rely on the quality of the model's class prediction, thus suffering from confirmation bias where the prediction mistakes would accumulate. Self-supervised learning methods are task-agnostic, and the widely adopted contrastive learn-ing <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b13">15]</ref> may learn representations that are suboptimal for the specific classification task. Another branch of methods explore graph-based semi-supervised learning <ref type="bibr" target="#b23">[25,</ref><ref type="bibr" target="#b16">18]</ref>, but have yet shown competitive performance especially on larger datasets such as ImageNet <ref type="bibr" target="#b8">[10]</ref>.</p><p>We propose CoMatch, a new semi-supervised learning method that addresses the existing limitations. A conceptual illustration is shown in <ref type="figure">Figure 1</ref>. In CoMatch, each image has two compact representations: a class probability produced by the classification head and a low-dimensional embedding produced by the projection head. The two representations interact with each other and jointly evolve in a co-training framework. Specifically, the classification head is trained using memory-smoothed pseudo-labels, where pseudo-labels are refined by aggregating information from nearby samples in the embedding space. The projection head is trained using contrastive learning on a pseudo-label graph, where samples with similar pseudo-labels are trained to have similar embeddings. CoMatch unifies dominant ideas including consistency regularization, entropy minimization, contrastive learning, and graph-based SSL.</p><p>We perform experiments on multiple datasets and compare with state-of-the-art semi-supervised and selfsupervised methods. CoMatch substantially outperforms all baselines across all benchmarks, especially in label-scarce scenarios. On CIFAR-10 with 4 labeled samples per class, CoMatch outperforms FixMatch <ref type="bibr" target="#b32">[34]</ref> by 6.11% in accuracy. On STL-10, CoMatch outperforms FixMatch by 13.27%. On ImageNet with only 1% of labels, CoMatch achieves a top-1 accuracy of 66.0% (67.1% with self-supervised pretraining), whereas the best baseline (MoCov2 <ref type="bibr" target="#b6">[8]</ref> followed by FixMatch <ref type="bibr" target="#b32">[34]</ref>) has an accuracy of 59.9%. Furthermore, we demonstrate that CoMatch achieves better representation learning performance on down-stream image classification and object detection tasks, outperforming both supervised learning and self-supervised learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>To set the stage for CoMatch, we first introduce existing SSL methods, mainly focusing on current state-of-theart methods that are relevant. More comprehensive reviews Figure 1: Conceptual illustration of different methods that leverage unlabeled data. (a) Task-specific self-training: the model predicts class probabilities for the unlabeled samples as the pseudo-label to train against <ref type="bibr" target="#b19">[21,</ref><ref type="bibr" target="#b1">3,</ref><ref type="bibr" target="#b0">2,</ref><ref type="bibr" target="#b32">34]</ref>. (b) Task-agnostic self-supervised learning: the model projects samples into low-dimensional embeddings, and performs contrastive learning to discriminate embeddings of different images <ref type="bibr" target="#b35">[37,</ref><ref type="bibr" target="#b4">6,</ref><ref type="bibr" target="#b13">15]</ref>. (c) CoMatch: class probabilities and embeddings interact with each other and jointly evolve in a co-training framework. The embeddings impose a smoothness constraint on the class probabilities to improve the pseudo-labels. The pseudo-labels are used as the target to train both the classification head with a cross-entropy loss, and the projection head with a graph-based contrastive loss.</p><p>can be found in <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b34">36]</ref>. In the following, we refer to a deep encoder network (a convolutional neural network) as f (·), which produces a high-dimensional feature f (x) given an input image x. A classification head (a fully-connected layer followed by softmax) is defined as h(·), which outputs a distribution over classes p(y|x) = h(f (x)). We also define a non-linear projection head (a MLP) g(·), which transforms a feature f (x) into a normalized low-dimensional embedding z(x) = g(f (x)). Consistency regularization is a crucial piece for many state-of-the-art SSL methods. It utilizes the assumption that a classifier should output the same class probability for an unlabeled sample even after it is augmented. In the simplest form, prior works <ref type="bibr" target="#b31">[33,</ref><ref type="bibr" target="#b18">20]</ref> add the following consistency regularization loss on unlabeled samples:</p><formula xml:id="formula_0">p(y|Aug(x)) − p(y|Aug(x)) 2 2 ,<label>(1)</label></formula><p>where Aug(·) is a stochastic transformation that does not alter the label of the image. Mean Teacher <ref type="bibr" target="#b33">[35]</ref> replaces one of the terms in eq.(1) with the output of an EMA model. VAT <ref type="bibr" target="#b25">[27]</ref> uses an adversarial transformation in place of Aug. MixMatch <ref type="bibr" target="#b1">[3]</ref> averages predictions across multiple augmentations to produce p(y). UDA <ref type="bibr" target="#b36">[38]</ref>, ReMix-Match <ref type="bibr" target="#b0">[2]</ref>, and FixMatch <ref type="bibr" target="#b32">[34]</ref> use a cross-entropy loss in place of the squared error, and apply stronger augmentation. Entropy minimization is a common method in many SSL algorithms, which encourages the classifier's decision boundary to pass through low-density regions of the data distribution. It is either achieved explicitly by minimizing the entropy of p(y|x) on unlabeled samples <ref type="bibr" target="#b11">[13]</ref>, or implicitly by constructing low-entropy pseudo-labels on unlabeled samples and using them as training targets in a crossentropy loss <ref type="bibr" target="#b19">[21,</ref><ref type="bibr" target="#b1">3,</ref><ref type="bibr" target="#b0">2,</ref><ref type="bibr" target="#b32">34]</ref>. Some methods <ref type="bibr" target="#b36">[38,</ref><ref type="bibr" target="#b1">3,</ref><ref type="bibr" target="#b0">2]</ref> postprocess the "soft" pseudo-labels with a sharpening function to reduce entropy, whereas FixMatch <ref type="bibr" target="#b32">[34]</ref> produces "hard" pseudo-labels for samples whose largest class probability fall above a predefined threshold. Most methods <ref type="bibr" target="#b32">[34,</ref><ref type="bibr" target="#b0">2,</ref><ref type="bibr" target="#b36">38]</ref> use weakly-augmented samples to produce pseudo-labels and train the model on strongly-augmented samples. However, since the pseudo-labels purely rely on the classifier, such self-training strategy suffers from the confirmation bias problem, where the error in the pseudo-labels would accumulate and harms learning. Self-supervised contrastive learning has attracted much attention, due to its ability to leverage unlabeled data for model pre-training. The widely adopted contrastive learning <ref type="bibr" target="#b35">[37,</ref><ref type="bibr" target="#b28">30,</ref><ref type="bibr" target="#b4">6,</ref><ref type="bibr" target="#b5">7,</ref><ref type="bibr" target="#b13">15]</ref> optimizes for the task of instance discrimination, and formulates the loss using the normalized low-dimensional embeddings z:</p><formula xml:id="formula_1">− log exp(z(Aug(x i )) · z(Aug(x i ))/t) N j=1 exp(z(Aug(x i )) · z(Aug(x j ))/t)<label>(2)</label></formula><p>where Aug(·) is a stochastic transformation similar as in eq.(1), and x j include x i and N − 1 other images (i.e. negative samples). Self-supervised contrastive learning can be interpreted as a form of class-agnostic consistency regularization, which enforces the same image with different augmentations to have similar embeddings, while different images have different embeddings. Among recent methods, SimCLR <ref type="bibr" target="#b4">[6]</ref> uses images from the same batch to calculate pairwise similarity, whereas MoCo <ref type="bibr" target="#b13">[15]</ref> maintains a queue of embeddings from an EMA model. Self-supervised pre-training followed by supervised fine-tuning has shown strong performance on semisupervised learning tasks <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b13">15,</ref><ref type="bibr" target="#b12">14,</ref><ref type="bibr" target="#b20">22,</ref><ref type="bibr" target="#b2">4]</ref>. SimCLR v2 <ref type="bibr" target="#b5">[7]</ref> further utilizes larger models for distillation. However, since self-supervised learning is a task-agnostic process, the contrastive loss in eq.(2) optimizes for an objective that partially contradicts with task-specific learning. It enforces images from the same class to have different representations, which is undesirable for classification tasks. Graph-based semi-supervised learning defines the similarity of data samples with a graph and encourages smooth predictions with respect to the graph structure <ref type="bibr" target="#b40">[42,</ref><ref type="bibr" target="#b42">43]</ref>. Recent works use deep networks to generate graph representations. <ref type="bibr" target="#b16">[18,</ref><ref type="bibr" target="#b21">23]</ref> perform iterative label propagation and network training. <ref type="bibr" target="#b23">[25,</ref><ref type="bibr" target="#b3">5]</ref> connect data samples that have the same pseudo-labels, and perform metric learning to enforce connected samples to have similar representations. However, these methods define representations as the highdimensional feature f (x), which leads to several limitations: (1) since the features are highly-correlated with the class predictions, the same types of errors are likely to exist in both the feature space and the label space; (2) due to the curse of dimensionality, Euclidean distance becomes less meaningful; (3) computation cost is high which harms the scalability of the methods. Furthermore, the loss functions in <ref type="bibr" target="#b23">[25,</ref><ref type="bibr" target="#b3">5]</ref> consider the absolute distance between pairs, whereas CoMatch optimizes for relative distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>In this section, we introduce our proposed semisupervised learning method. Different from most existing semi-supervised and self-supervised learning methods, Co-Match jointly learns the encoder f (·), the classification head h(·), and the projection head g(·). Given a batch of B labeled samples</p><formula xml:id="formula_2">X = {(x b , y b )} B b=1</formula><p>where y b are one-hot labels, and a batch of unlabeled samples U = {u b } µB b=1 where µ determines the relative size of X and U, CoMatch jointly optimizes three losses: (1) a supervised classification loss on labeled data L x , (2) an unsupervised classification loss on unlabeled data L cls u , and (3) a graph-based contrastive loss on unlabeled data L ctr u . Specifically, L x is defined as the cross-entropy between the ground-truth labels and the model's predictions:</p><formula xml:id="formula_3">L x = 1 B B b=1 H(y b , p(y|Aug w (x b ))),<label>(3)</label></formula><p>where H(y, p) denotes the cross-entropy between two distributions y and p, and Aug w refers to weak augmentations. The unsupervised classification loss L cls u is defined as the cross-entropy between the pseudo-labels q b and the model's predictions:</p><formula xml:id="formula_4">L cls u = 1 µB µB b=1 1(max q b ≥ τ )H(q b , p(y|Aug s (u b ))),<label>(4)</label></formula><p>where Aug s refers to strong augmentations. Following Fix-Match <ref type="bibr" target="#b32">[34]</ref>, we retain pseudo-labels whose largest class probability are above a threshold τ . Different from Fix-Match, our soft pseudo-labels q b are not converted to hard labels for entropy minimization. Instead, we achieve entropy minimization by optimizing the contrastive loss L ctr u . Section 3.2 explains the details of pseudo-labelling and contrastive learning.</p><p>Our overall training objective is:</p><formula xml:id="formula_5">L = L x + λ cls L cls u + λ ctr L ctr u ,<label>(5)</label></formula><p>where λ cls and λ ctr are scalar hyperparameters to control the weight of the unsupervised losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">CoMatch</head><p>In CoMatch, the high-dimensional feature of each sample is transformed to two compact representations: its class probability p and its normalized low-dimensional embedding z, which reside in the label space and the embedding space, respectively. Given a batch of unlabeled samples U, we first perform memory-smoothed pseudo-labeling on weak augmentations Aug w (U) to produce pseudo-labels. Then, we construct a pseudo-label graph W q which defines the similarity of samples in the label space. We use W q as the target to train an embedding graph W z , which measures the similarity of strongly-augmented samples Aug s (U) in the embedding space. An illustration of CoMatch is shown in <ref type="figure">Fig 2,</ref> and a pseudo-code is given in the appendix. Next, we first introduce the pseudo-labeling process, then we describe the graph-based contrastive learning algorithm. Memory-smoothed pseudo-labeling aims to mitigate confirmation bias by leveraging the structure of the embeddings to refine pseudo-labels. Given each sample in X and U, we first obtain its class probability. For a labeled sample, it is defined as the ground-truth label: p w = y. For an unlabeled sample, it is defined as the model's prediction on its weak-augmentation: p w = h • f (Aug w (u)). Following <ref type="bibr" target="#b0">[2]</ref>, we perform distribution alignment (DA) on unlabeled samples: p w = DA(p w ). DA prevents the model's prediction from collapsing to certain classes. Specifically, we maintain a moving-averagep w of p w during training, and adjust the current p w with p w = Normalize(p w /p w ), where Normalize(p) i = p i / j p j renormalizes the scaled result to a valid probability distribution.</p><p>For each sample in X and U, we also obtain its embedding z w by forwarding the weakly-augmented sample through f and g. Then, we create a memory bank to store class probabilities and embeddings of the past K weaklyaugmented samples:</p><formula xml:id="formula_6">MB = {(p w k , z w k )} K k=1 .</formula><p>The memory bank contains both labeled samples and unlabeled samples and is updated with first-in-first-out strategy.</p><p>For each unlabeled sample u b in the current batch with p w b and z w b , we generate a pseudo-label q b by aggregating class probabilities from neighboring samples in the memory bank. Specifically, we optimize the following objective:</p><formula xml:id="formula_7">J(q b ) = (1 − α) K k=1 a k q b − p w k 2 2 + α q b − p w b 2 2 (6)</formula><p>The first term is a smoothness constraint which encourages q b to take a similar value as its nearby samples' class probabilities, whereas the second term attempts to maintain its  ...</p><formula xml:id="formula_8">Aug′ 7 * ∘ ( ...</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6′</head><p>... memory-smoothed pseudo-labels, which are used as targets to train the class prediction on strongly-augmented images. A pseudo-label graph with self-loop is constructed to measure the similarity between samples, which is used to train an embedding graph such that images with similar pseudo-labels have similar embeddings. sg means stop-gradient.</p><p>original class prediction. a k measures the affinity between the current sample and the k-th sample in the memory, and is computed using similarity in the embedding space:</p><formula xml:id="formula_9">a k = exp(z w b · z w k /t) K k=1 exp(z w b · z w k /t) ,<label>(7)</label></formula><p>where t is a scalar temperature parameter.</p><p>Since a k is normalized (i.e. a k sums to one), the minimizer for J(q b ) can be derived as:</p><formula xml:id="formula_10">q b = αp w b + (1 − α) K k=1 a k p w k .<label>(8)</label></formula><p>Graph-based contrastive learning aims to learn representations guided by a pseudo-label graph. Given the pseudolabels {q b } µB b=1 for the batch of unlabeled samples, we build the pseudo-label graph by constructing a similarity matrix W q of size µB × µB:</p><formula xml:id="formula_11">W q bj =      1 if b = j q b · q j if b = j and q b · q j ≥ T 0 otherwise<label>(9)</label></formula><p>Samples with similarity lower than a threshold T are not connected, and each sample is connected to itself with the strongest edge of value 1 (i.e. self-loop).</p><p>The pseudo-label graph serves as the target to train an embedding graph. To construct the embedding graph, we first perform two strong augmentations on each unlabeled sample u b ∈ U, and obtain their embeddings</p><formula xml:id="formula_12">z b = g • f (Aug s (u b )), z b = g • f (Aug s (u b )</formula><p>). Then we build the embedding graph W z as:</p><formula xml:id="formula_13">W z bj = exp(z b · z b /t) if b = j exp(z b · z j /t) if b = j<label>(10)</label></formula><p>We aim to train the encoder f and the projection head g such that the embedding graph has the same structure as the pseudo-label graph. To this end, we first normalize W q and W z withŴ bj = W bj / j W bj , so that each row of the similarity matrix sums to 1. Then we minimize the cross-entropy between the two normalized graphs. The contrastive loss is defined as:</p><formula xml:id="formula_14">L ctr u = 1 µB µB b=1 H(Ŵ q b ,Ŵ z b )<label>(11)</label></formula><formula xml:id="formula_15">H(Ŵ q b ,Ŵ z b )</formula><p>can be decomposed into two terms:</p><formula xml:id="formula_16">−Ŵ q bb log( exp(z b ·z b /t) µB j=1Ŵ z bj ) − µB j=1,j =bŴ q bj log( exp(z b ·z j /t) µB j=1Ŵ z bj )<label>(12)</label></formula><p>The first term is a self-supervised contrastive loss that comes from the self-loops in the pseudo-label graph. It encourages the model to produce similar embeddings for different augmentations of the same image, which is a form of consistency regularization. The second term encourages samples with similar pseudo-labels to have similar embeddings. It gathers samples from the same class into clusters, which achieves entropy minimization.</p><p>During training, a natural curriculum would occur from CoMatch. The model would start with producing low-confidence pseudo-labels, which leads to a sparse pseudolabel graph. As training progresses, samples are gradually clustered, which in turns leads to more confident pseudolabels and more connections in the pseudo-label graph.</p><p>Another advantage of CoMatch appears in open-set semi-supervised learning, where the unlabeled data contains out-of-distribution (ood) samples. Due to the smoothness constraint, ood samples would have low-confidence pseudo-labels. Therefore, they are less connected to indistribution samples, and will be pushed further away from in-distribution samples by the proposed contrastive loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Scalable learning with an EMA model</head><p>In order to build a meaningful pseudo-label graph, the unlabeled batch of data should contain a sufficient number of samples from each class. While this requirement can be easily satisfied for datasets with a small number of classes (e.g. CIFAR-10), it becomes difficult for large datasets with more classes (e.g. ImageNet) because a large unlabeled batch would exceed the memory capacity of 8 commodity GPUs (e.g. NVIDIA V100). Therefore, we improve CoMatch for SSL on large-scale datasets.</p><p>Inspired by MoCo <ref type="bibr" target="#b13">[15]</ref> and Mean Teacher <ref type="bibr" target="#b33">[35]</ref>, we introduce an EMA model {f ,ḡ,h} whose parametersθ are the moving-average of the original model's parameters θ:</p><formula xml:id="formula_17">θ ← mθ + (1 − m)θ.<label>(13)</label></formula><p>The advantage of the EMA model is that it can evolve smoothly as controlled by the momentum parameter m.</p><p>We also introduce a momentum queue which stores the pseudo-labels and the strongly-augmented embeddings for the past K unlabeled samples: MQ = {(q k ,z k =ḡ • f (Aug s (u k )))} K k=1 , whereq k andz k are produced using the EMA model. Different from the memory bank, the momentum queue only contains unlabeled samples.</p><p>We modify the pseudo-label graph W q to have a size of µB × K. It defines the similarity between each sample in the current batch and each sample in the momentum queue (which also contains the current batch). Different from eqn.(9), the similarity is now calculated asq b ·q j , where b = {1, ..., µB} and j = {1, ..., K}.</p><p>The embedding graph W z is also modified to have a size of µB × K, where the similarity is calculated using the model's output embedding z b and the momentum embeddingz j : W z bj = exp(z b ·z j /t). Since gradient only flows back through z b , we can use a large K with only a small increase in GPU memory usage and computation time.</p><p>Besides the contrastive loss, we also leverage the EMA model for memory-smoothed pseudo-labeling, by forwarding the weakly-augmented samples through the EMA model instead of the original model. A graphical illustration of the memory bank and the momentum queue is given in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">CIFAR-10 and STL-10</head><p>First, we conduct experiments on CIFAR-10 and STL-10 datasets. CIFAR-10 contains 50,000 images of size 32 × 32 from 10 classes. We vary the amount of labeled data and focus on the label-scarce scenario where few labels are available. We evaluate on 5 runs with different random seeds. STL-10 contains 5,000 labeled images of size 96 × 96 from 10 classes and 100,000 unlabeled images including ood samples. We evaluate on the 5 pre-defined folds. Following <ref type="bibr" target="#b1">[3,</ref><ref type="bibr" target="#b32">34]</ref>, we report the performance of an EMA model. Baseline methods. For fair comparison, we improve the current state-of-the-art method FixMatch <ref type="bibr" target="#b32">[34]</ref> with distribution alignment <ref type="bibr" target="#b0">[2]</ref> to build a stronger baseline. We also compare with the original FixMatch and MixMatch <ref type="bibr" target="#b1">[3]</ref>. We omit previous methods such as Π-model <ref type="bibr" target="#b30">[32]</ref>, Pseudo-Labeling <ref type="bibr" target="#b19">[21]</ref>, and Mean Teacher <ref type="bibr" target="#b33">[35]</ref> due to their poorer performance as reported in <ref type="bibr" target="#b32">[34]</ref>. Following <ref type="bibr" target="#b27">[29]</ref>, we reimplemented the baselines and performed all experiments using the same model architecture, the same codebase (Py-Torch <ref type="bibr" target="#b29">[31]</ref>), and the same random seeds. Implementation details. For CIFAR-10, we use a Wide ResNet-28-2 <ref type="bibr" target="#b37">[39]</ref>. For STL-10, we use a ResNet-18 <ref type="bibr" target="#b15">[17]</ref> due to its lower computation cost compared to the WRN-37-2 used in <ref type="bibr" target="#b32">[34]</ref> 1 . The projection head is a 2-layer MLP which outputs 64-dimensional embeddings. The models are trained using SGD with a momentum of 0.9 and a weight decay of 0.0005. We follow the original papers <ref type="bibr" target="#b1">[3,</ref><ref type="bibr" target="#b32">34]</ref> and train the baselines for 1024 epochs, using an learning rate of 0.03 with a cosine decay schedule. We train CoMatch for only 512 epochs to demonstrate its efficiency in learning. For the hyperparameters in CoMatch that also exist in <ref type="bibr" target="#b32">[34]</ref>, we follow <ref type="bibr" target="#b32">[34]</ref> and set λ cls = 1, τ = 0.95, µ = 7, B = 64. For other hyperparameters, we fix α = 0.9, K = 2560, t = 0.2, T = 0.8, and λ ctr = 1 for all CIFAR-10 experiments, and only changes λ ctr to 5 for STL-10. Augmentations. CoMatch uses one "weak" augmentation Aug w , and two "strong" augmentations Aug s and Aug s . The weak augmentation for all experiments is the standard crop-and-flip. For strong augmentations, we follow <ref type="bibr" target="#b32">[34]</ref> and uses RandAugment <ref type="bibr" target="#b7">[9]</ref> as Aug s . For Aug s , we follow the augmentation strategy in SimCLR <ref type="bibr" target="#b4">[6]</ref> which applies random color jittering and grayscale conversion. Results. <ref type="table" target="#tab_1">Table 1</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">ImageNet</head><p>We evaluate CoMatch on ImageNet ILSVRC-2012 to verify its efficacy on large-scale datasets. Following <ref type="bibr" target="#b38">[40,</ref><ref type="bibr" target="#b4">6]</ref>, we randomly sample 1% or 10% of images with labels in a class-balanced way (13 or 128 samples per-class, respectively), while the rest of images are unlabeled. Our results are not sensitive to different random seeds hence we use a fixed random seed. Baseline methods.</p><p>The baselines include (1) semisupervised learning methods and (2) self-supervised pretraining followed by fine-tuning. Furthermore, we construct a state-of-the-art baseline which combines FixMatch (w. DA) with self-supervised pre-training using MoCov2 <ref type="bibr" target="#b6">[8]</ref> (pre-trained for 800 epochs). Self-supervised methods require additional model parameters during training due to the projection network. We count the number of training parameters as those that require gradient update. We also report the performance of SimCLRv2 <ref type="bibr" target="#b5">[7]</ref>. However, the best model from SimCLRv2 uses substantially (33×) larger pre-trained teacher models to produce high-quality pseudolabels for distillation. Hence CoMatch should not be directly compared to SimCLRv2. Implementation details. We use a ResNet-50 <ref type="bibr" target="#b15">[17]</ref> model as the encoder. Following <ref type="bibr" target="#b6">[8,</ref><ref type="bibr" target="#b4">6]</ref>, the projection head is a 2-layer MLP which outputs 128-dimensional embeddings. We train the model using SGD with a momentum of 0.9 and a weight decay of 0.0001. The learning rate is 0.1, which follows a cosine decay schedule for 400 epochs. For models that are initialized with MoCov2, we use a smaller learning rate of 0.03. The momentum parameter is set as m = 0.996. Other hyperparameters are shown in appendix A. We use the same strong augmentation for Aug s and Aug s , which applies crop-and-flip followed by color distortion. For fair comparison with baselines, we report the original model's performance instead of the EMA model's.</p><p>Results. <ref type="table" target="#tab_3">Table 2</ref>    best baseline (MoCov2 followed by FixMatch w. DA), Co-Match achieves 6.1% improvement with 3× less training time. With the help of MoCov2 pre-training, the performance of CoMatch can further improve to 67.1% on 1% of labels, and 73.7% on 10% of labels. In <ref type="figure">Figure 3</ref>, we further show that CoMatch produces pseudo-labels that are more confident and accurate. Pre-training with MoCov2 helps speed up the convergence rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study.</head><p>We perform extensive ablation study to examine the effect of different components in CoMatch. We use ImageNet with 1% labels as the main experiment. Due to the number of experiments in our ablation study, we report the top-1 accuracy after training for 100 epochs, where the default setting of CoMatch achieves 57.1%. Graph connection threshold. The threshold T in eqn. <ref type="bibr" target="#b7">(9)</ref> controls the sparsity of edges in the pseudo-label graph. <ref type="figure">Figure 4</ref>(a) presents the effect of T . As T increases, samples whose pseudo-labels have lower similarity are disconnected. Hence their embeddings are pushed apart by our contrastive loss. When T = 1, the proposed graph-based contrastive loss downgrades to the self-supervised loss in eqn. <ref type="bibr" target="#b0">(2)</ref> where the only connections are the self-loops. Us-ing the self-supervised contrastive loss decreases the performance by 2.8%. Contrastive loss weight. We vary the weight λ ctr for the contrastive loss L ctr u and report the result in <ref type="figure">Figure 4</ref>(b), where λ ctr = 10 gives the best performance. With 10% of ImageNet labels, λ ctr = 2 yields better performance. We find that in general, fewer labeled samples require a larger λ ctr to strengthen the graph regularization. Prediction weight in pseudo-labels.</p><p>Our memorysmoothed pseudo-labeling uses α to control the balance between the EMA model's prediction and smoothness constraint. <ref type="figure">Figure 4(c)</ref> shows its effect, where α = 0.9 results in the best performance. When α = 1, the pseudo-labels are purely generated by the EMA model, which reduces to the Mean-Teacher <ref type="bibr" target="#b33">[35]</ref> method. The accuracy decreases by 2.1% due to confirmation bias. When α &lt; 0.9, the pseudolabels are over-smoothed. A potential improvement is to apply sharpening <ref type="bibr" target="#b1">[3]</ref> to pseudo-labels with smaller α, but is not studied here due to the need of an extra hyperparameter. Size of memory bank and momentum queue. K controls both the size of the memory bank for pseudo-labeling and the size of the momentum queue for contrastive learning. A larger K considers more samples to enforce a structural constraint on the label space and the embedding space. As   shown in <ref type="figure">Figure 4(d)</ref>, the performance increases as K increases from 10k to 30k, but plateaus afterwards. We would also like to highlight that the memory bank and the momentum queue only introduce a small computation overhead because (1) low-dimensional embeddings are stored, (2) gradients are not computed w.r.t to the embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Transfer of Learned Representations</head><p>We further evaluate the quality of the representations learned by CoMatch by transferring it to other tasks. Following <ref type="bibr" target="#b10">[12,</ref><ref type="bibr" target="#b20">22]</ref>, We first perform linear classification on two datasets: PASCAL VOC2007 <ref type="bibr" target="#b9">[11]</ref> for object classification and Places205 <ref type="bibr" target="#b39">[41]</ref> for scene recognition. We train linear SVMs using fixed representations from ImageNet pretrained models. We preprocess all images by resizing them to 256 pixels along the shorter side and taking a 224×224 center crop. The SVMs are trained on the global average pooling features of ResNet-50. To study the transferability of the representations in few-shot scenarios, we vary the number of samples per-class (k) in the downstream datasets. <ref type="table" target="#tab_5">Table 3</ref> shows the results. We compare CoMatch with standard supervised learning on labeled ImageNet and selfsupervised learning (MoCov2 <ref type="bibr" target="#b6">[8]</ref> and SwAV <ref type="bibr" target="#b2">[4]</ref>) on unlabeled ImageNet. CoMatch with 10% labels achieves higher performance on both datasets. It is interesting to observe that self-supervised learning methods do not perform well in few-shot transfer, and only catch up with supervised learning when k increases.</p><p>In <ref type="table" target="#tab_6">Table 4</ref>, we also show that compared to supervised and self-supervised learning, CoMatch learns a better CNN backbone for object detection and instance segmentation on COCO <ref type="bibr" target="#b22">[24]</ref>. We follow the exact same setting as <ref type="bibr" target="#b13">[15]</ref> to fine-tune a Mask-RCNN model <ref type="bibr" target="#b14">[16]</ref> for 1× or 2× schedule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>To conclude, the success of CoMatch can be attributed to three contributions: (1) co-training of class probabilities and image embeddings, (2) memory-smoothed pseudolabeling to mitigate confirmation bias, (3) graph-based contrastive learning to learn better representations. We believe that CoMatch will help enable machine learning to be deployed in domains where labels are expensive to acquire.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Experiment Details</head><p>In <ref type="table" target="#tab_7">Table 5</ref>, we show the complete set of hyperparameters in our semi-supervised learning experiments. <ref type="bibr">Dataset</ref> B µ λcls α K t τ T λctr CIFAR-10 64 7 1 0.9 2560 0.2 0.95 0.8 1 STL-10 5</p><p>ImageNet 1% labels 160 4 10 0.9 30000 0.1 0.6 0.3 10 ImageNet 10% labels 0.5 0.2 2 The strong augmentation Aug s on ImageNet unlabeled data uses color distortion in addition to the standard cropand-flip. A pseudo-code for the color distortion in PyTorch is as follows:  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>6 3 Figure 2 :</head><label>632</label><figDesc>Framework of the proposed CoMatch. Given a batch of unlabeled images, their weakly-augmented images are used to produce</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Plots of different methods as training progresses on ImageNet with 1% labels. (a) Accuracy of the confident pseudo-labels w.r.t to the ground-truth labels of the unlabeled samples. (b) Ratio of the unlabeled samples with confident pseudo-labels that are included in the unsupervised classification loss. (3) Top-1 accuracy on the test data. Plots of various ablation studies on CoMatch. The default hyperparameter setting achieves 57.1% (ImageNet with 1% labels, trained for 100 epochs). (a) Varying the threshold T which controls the sparsity of edges in the pseudo-label graph. T = 1 reduces to self-supervised contrastive learning. (b) Varying the weight λctr for the contrastive loss. (c) Varying α, the weight of the EMA model's prediction in generating pseudo-labels. α = 1 reduces to pseudo-labeling with mean teacher [35]. (d) Varying K, the number of samples in both the memory bank and the momentum queue.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>from torchvision import transforms as T color jitter = T.ColorJitter(0.4,0.4,0.4,0.1) transforms.Compose([ T.RandomApply([color jitter], p=0.8) T.RandomGrayscale(p=0.2)]) Appendix B. MB and MQ in CoMatch</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 Figure 5 : 24 L</head><label>5524</label><figDesc>illustrates how the EMA model is utilized in CoMatch to construct the memory bank (MB) and the momentum queue (MQ). The memory bank contains the class probability and the low-dimensional embeddings for both weakly-augmented labeled samples and weakly-augmented unlabeled samples. The momentum queue contains the pseudo-labels for the unlabeled samples and their stronglyaugmented embeddings. Illustration of the memory bank and the momentum queue. U is the batch of unlabeled data, X is the batch of labeled data.f ,h, andḡ refer to the EMA version of the encoder, the classification head, and the projection head, respectively.1 if b = j q b · qj if b = j and q b · qj ≥ T 0 otherwise b · z b /t) if b = j exp(z b · zj/t) if b = j 17 end 18Ŵ q = Normalize(W q ) 19Ŵ z = Normalize(W z ) y b , p(y|Augw(x b ))) 22 L cls u = 1 µB µB b=1 1(max q b ≥ τ )H(q b ,p(y|Augs(u b ))) = Lx + λ cls L cls u + λctrL ctr u 25 update f , h, g with SGD to minimize L.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>shows the results. CoMatch outperforms the best baseline across all settings. The improvement is more substantial when fewer labeled samples are available. For example, CoMatch achieves an average accuracy of 93.09% on CIFAR-10 with only 4 labels per class, whereas FixMatch (w. DA) has a lower accuracy of 86.98% and a 84±10.63 51.90±11.76 80.79±1.28 88.97±0.85 38.02±8.29 FixMatch [34] 82.32±9.77 86.12±3.53 92.06±0.88 94.90±0.67 65.38±0.42 FixMatch [34] w. DA [2] 83.81±9.35 86.98±3.40 92.29±0.86 94.95±0.66 66.53±0.39 CoMatch 87.67±8.47 93.09±1.39 93.97±0.62 95.09±0.33 79.80±0.38</figDesc><table><row><cell>Method</cell><cell>20 labels</cell><cell>CIFAR-10 40 labels 80 labels</cell><cell>250 labels</cell><cell>STL-10 1000 labels</cell></row><row><cell>MixMatch [3]</cell><cell>27.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Accuracy for CIFAR-10 and STL-10 on 5 different folds. All methods are tested using the same data and codebase.</figDesc><table><row><cell>larger variance. On STL-10, CoMatch also improves Fix-</cell></row><row><cell>Match (w. DA) by 13.27%.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>shows the result, where CoMatch achieves state-of-the-art performance. CoMatch obtains a top-1 accuracy of 66.0% on 1% of labels. Compared to the the</figDesc><table><row><cell>Self-supervised Pre-training</cell><cell>Method</cell><cell>#Epochs</cell><cell>#Paramters (train/test)</cell><cell cols="4">Top-1 Label fraction Label fraction Top-5 1% 10% 1% 10%</cell></row><row><cell></cell><cell>Supervised baseline [40]</cell><cell>∼20</cell><cell>25.6M / 25.6M</cell><cell>25.4</cell><cell>56.4</cell><cell>48.4</cell><cell>80.4</cell></row><row><cell></cell><cell>Pseudo-label [21, 40]</cell><cell>∼100</cell><cell>25.6M / 25.6M</cell><cell>-</cell><cell>-</cell><cell>51.6</cell><cell>82.4</cell></row><row><cell></cell><cell cols="2">VAT+EntMin. [27, 13, 40] -</cell><cell>25.6M / 25.6M</cell><cell>-</cell><cell>68.8</cell><cell>-</cell><cell>88.5</cell></row><row><cell>None</cell><cell>S4L-Rotation [40] UDA (RandAug) [38]</cell><cell>∼200 -</cell><cell>25.6M / 25.6M 25.6M / 25.6M</cell><cell>--</cell><cell>53.4 68.8</cell><cell>--</cell><cell>83.8 88.5</cell></row><row><cell></cell><cell cols="2">FixMatch (RandAug) [34] ∼300</cell><cell>25.6M / 25.6M</cell><cell>-</cell><cell>71.5</cell><cell>-</cell><cell>89.1</cell></row><row><cell></cell><cell>FixMatch w. DA</cell><cell>∼400</cell><cell>25.6M / 25.6M</cell><cell>53.4</cell><cell>70.8</cell><cell>74.4</cell><cell>89.0</cell></row><row><cell></cell><cell>CoMatch</cell><cell>∼400</cell><cell>30.0M / 25.6M</cell><cell>66.0</cell><cell>73.6</cell><cell>86.4</cell><cell>91.6</cell></row><row><cell>PIRL [26]</cell><cell></cell><cell>∼800</cell><cell>26.1M / 25.6M</cell><cell>30.7</cell><cell>60.4</cell><cell>57.2</cell><cell>83.8</cell></row><row><cell>PCL [22]</cell><cell></cell><cell>∼200</cell><cell>25.8M / 25.6M</cell><cell>-</cell><cell>-</cell><cell>75.3</cell><cell>85.6</cell></row><row><cell>SimCLR [6]</cell><cell>Fine-tune</cell><cell>∼1000</cell><cell>30.0M / 25.6M</cell><cell>48.3</cell><cell>65.6</cell><cell>75.5</cell><cell>87.8</cell></row><row><cell>BYOL [14]</cell><cell></cell><cell>∼1000</cell><cell>37.1M / 25.6M</cell><cell>53.2</cell><cell>68.8</cell><cell>78.4</cell><cell>89.0</cell></row><row><cell>SwAV [4]</cell><cell></cell><cell>∼800</cell><cell>30.4M / 25.6M</cell><cell>53.9</cell><cell>70.2</cell><cell>78.5</cell><cell>89.9</cell></row><row><cell></cell><cell>Fine-tune</cell><cell>∼800</cell><cell>30.0M / 25.6M</cell><cell>49.8</cell><cell>66.1</cell><cell>77.2</cell><cell>87.9</cell></row><row><cell>MoCov2 [8]</cell><cell>FixMatch w. DA</cell><cell>∼1200</cell><cell>30.0M / 25.6M</cell><cell>59.9</cell><cell>72.2</cell><cell>79.8</cell><cell>89.5</cell></row><row><cell></cell><cell>CoMatch</cell><cell>∼1200</cell><cell>30.0M / 25.6M</cell><cell>67.1</cell><cell>73.7</cell><cell>87.1</cell><cell>91.4</cell></row><row><cell>SimCLRv2* [7]</cell><cell>Fine-tune Fine-tune+Distillation</cell><cell>∼800 &gt;1200</cell><cell cols="2">34.2M / 29.8M 829.2M / 29.8M 73.9 57.9</cell><cell>68.4 77.5</cell><cell>82.5 91.5</cell><cell>89.2 93.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Accuracy for ImageNet with 1% and 10% of labeled examples. SimCLRv2* [7] uses larger models for training and test.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>51±2.12 79.60±0.61 82.75±0.34 85.55±0.12 87.12 47±2.18 76.74±0.87 80.61±0.53 84.60±0.11 86.83 SwAV [4] 400 68.04±2.39 75.06±0.73 79.46±0.55 84.24±0.13 86.86 SwAV* [4] 800 64.27±2.13 73.19±0.68 78.87±0.46 85.07±0.20 88.10</figDesc><table><row><cell>Method</cell><cell cols="2">#ImageNet labels #Pre-train epochs</cell><cell>k=4</cell><cell>k=8</cell><cell>k=16</cell><cell>k=64</cell><cell>Full</cell></row><row><cell cols="8">Supervised 73.MoCov2 [8] 100% 90 0% 800 70.CoMatch 1% 400 72.81±1.50 79.18±0.51 82.30±0.46 85.65±0.17 87.66</cell></row><row><cell>CoMatch</cell><cell>10%</cell><cell>400</cell><cell cols="5">74.56±2.04 80.60±0.31 83.24±0.43 86.07±0.16 87.91</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(a) VOC07</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="2">#ImageNet labels #Pre-train epochs</cell><cell>k=4</cell><cell>k=8</cell><cell>k=16</cell><cell>k=64</cell><cell>k=256</cell></row><row><cell>Supervised</cell><cell>100%</cell><cell>90</cell><cell cols="5">27.20±0.41 32.08±0.45 35.95±0.21 41.81±0.17 45.74±0.14</cell></row><row><cell>MoCov2 [8]</cell><cell></cell><cell>800</cell><cell cols="5">25.34±0.51 30.64±0.39 35.08±0.34 42.18±0.10 46.96±0.06</cell></row><row><cell>SwAV [4]</cell><cell>0%</cell><cell>400</cell><cell cols="5">25.32±0.46 31.00±0.47 35.65±0.28 42.60±0.11 47.51±0.20</cell></row><row><cell>SwAV* [4]</cell><cell></cell><cell>800</cell><cell cols="5">27.07±0.60 33.26±0.38 38.38±0.22 46.01±0.10 51.00±0.17</cell></row><row><cell>CoMatch</cell><cell>1%</cell><cell>400</cell><cell cols="5">27.15±0.42 32.36±0.37 36.56±0.33 42.97±0.11 47.32±0.18</cell></row><row><cell>CoMatch</cell><cell>10%</cell><cell>400</cell><cell cols="5">28.11±0.33 33.05±0.46 36.98±0.28 43.06±0.22 47.10±0.11</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(b) Places</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Linear classification on VOC07 and Places using models pre-trained on ImageNet. We vary the number of examples per-class (k) on the down-stream datasets. We report the average result with std across 5 runs. SwAV* uses multi-crop augmentation.</figDesc><table><row><cell></cell><cell>#ImageNet</cell><cell></cell><cell></cell><cell cols="2">1× schedule</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">2× schedule</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>labels</cell><cell cols="2">AP bb AP bb 50</cell><cell>AP bb 75</cell><cell cols="2">AP mk AP mk 50</cell><cell>AP mk 75</cell><cell cols="2">AP bb AP bb 50</cell><cell>AP bb 75</cell><cell cols="2">AP mk AP mk 50</cell><cell>AP mk 75</cell></row><row><cell>Supervised</cell><cell>100%</cell><cell>38.9</cell><cell>59.6</cell><cell>42.7</cell><cell>35.4</cell><cell>56.5</cell><cell>38.1</cell><cell>40.6</cell><cell>61.3</cell><cell>44.4</cell><cell>36.8</cell><cell>58.1</cell><cell>39.5</cell></row><row><cell>MoCo [15]</cell><cell>0%</cell><cell>38.5</cell><cell>58.9</cell><cell>42.0</cell><cell>35.1</cell><cell>55.9</cell><cell>37.7</cell><cell>40.8</cell><cell>61.6</cell><cell>44.7</cell><cell>36.9</cell><cell>58.4</cell><cell>39.7</cell></row><row><cell>CoMatch</cell><cell>1%</cell><cell>39.7</cell><cell>61.2</cell><cell>43.1</cell><cell>36.1</cell><cell>57.8</cell><cell>38.5</cell><cell>41.2</cell><cell>62.2</cell><cell>44.9</cell><cell>37.3</cell><cell>59.0</cell><cell>39.9</cell></row><row><cell>CoMatch</cell><cell>10%</cell><cell>40.5</cell><cell>61.5</cell><cell>44.2</cell><cell>36.7</cell><cell>58.3</cell><cell>39.2</cell><cell>41.5</cell><cell>62.5</cell><cell>45.4</cell><cell>37.6</cell><cell>59.5</cell><cell>40.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Transfer the pre-trained models to object detection and instance segmentation on COCO, by fine-tuning Mask-RCNN with R50-FPN on train2017. We evaluate bounding-box AP (AP bb ) and mask AP (AP mk ) on val2017.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Hyperparameters for CoMatch in the semi-supervised learning experiments.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The forward-pass GFLOPs/image is 0.34 for ResNet-18 and 2.58 for WRN-37-2. Compared to ResNet-18, WRN-37-2 takes 3×GPU memory and 7×training time per epoch.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C. Pseudo-code of CoMatch</head><p>Algorithm 1 presents the pseudo-code of CoMatch.</p><p>Algorithm 1: Pseudo-code of CoMatch (one iteration). </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Remixmatch: Semi-supervised learning with distribution alignment and augmentation anchoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mixmatch: A holistic approach to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09882</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Data-efficient semi-supervised learning by reliable edge mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peibin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuchang</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="9189" to="9198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Big self-supervised models are strong semi-supervised learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10029</idno>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<title level="m">Improved baselines with momentum contrastive learning</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="702" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (VOC) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">M</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Scaling and benchmarking self-supervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6391" to="6400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semi-supervised learning by entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altché</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><forename type="middle">H</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><forename type="middle">Daniel</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilal</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Valko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07733</idno>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Label propagation for deep semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmet</forename><surname>Iscen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgos</forename><surname>Tolias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5070" to="5079" />
		</imprint>
	</monogr>
	<note>Yannis Avrithis, and Ondrej Chum</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images. Mater&apos;s thesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Temporal ensembling for semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Hyun</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop on Challenges in Representation Learning</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.04966</idno>
		<title level="m">Prototypical contrastive learning of unsupervised representations</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Density-aware graph for deep semi-supervised visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suichan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13397" to="13406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Microsoft COCO: common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Smooth neighbors on teacher graphs for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yucen</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengxi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Self-supervised learning of pretext-invariant representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Virtual adversarial training: A regularization method for supervised and semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Shin-Ichi Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shin</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Deep Learning and Unsupervised Feature Learning</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Realistic evaluation of deep semi-supervised learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Ekin Dogus Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<editor>Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolò Cesa-Bianchi, and Roman Garnett</editor>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="3239" to="3250" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with ladder networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Rasmus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Berglund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikko</forename><surname>Honkala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tapani</forename><surname>Raiko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<editor>Corinna Cortes, Neil D. Lawrence, Daniel D. Lee, Masashi Sugiyama, and Roman Garnett, editors</editor>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="3546" to="3554" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Regularization with stochastic transformations and perturbations for deep semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehran</forename><surname>Javanmardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tolga</forename><surname>Tasdizen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<editor>Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett</editor>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1163" to="1171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fixmatch: Simplifying semisupervised learning with consistency and confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A survey on semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jesper E Van Engelen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Holger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page" from="373" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Unsupervised data augmentation for consistency training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.12848</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<editor>Richard C. Wilson, Edwin R. Hancock, and William A. P. Smith</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">S4l: Self-supervised semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1476" to="1485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning deep features for scene recognition using places database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Àgata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="487" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Learning with local and global consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengyong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Thomas Navin Lal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schölkopf</surname></persName>
		</author>
		<editor>Sebastian Thrun, Lawrence K</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Saul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schölkopf</surname></persName>
		</author>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="321" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Semi-supervised learning using gaussian fields and harmonic functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<editor>Tom Fawcett and Nina Mishra</editor>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="912" to="919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Semi-supervised learning literature survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerry</forename><surname>Xiaojin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
		<respStmt>
			<orgName>University of Wisconsin-Madison Department of Computer Sciences</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
