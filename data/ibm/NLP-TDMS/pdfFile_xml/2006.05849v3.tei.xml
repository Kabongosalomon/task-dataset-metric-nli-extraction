<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-Supervised Relational Reasoning for Representation Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Patacchiola</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><surname>Storkey</surname></persName>
							<email>a.storkey@ed.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Self-Supervised Relational Reasoning for Representation Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In self-supervised learning, a system is tasked with achieving a surrogate objective by defining alternative targets on a set of unlabeled data. The aim is to build useful representations that can be used in downstream tasks, without costly manual annotation. In this work, we propose a novel self-supervised formulation of relational reasoning that allows a learner to bootstrap a signal from information implicit in unlabeled data. Training a relation head to discriminate how entities relate to themselves (intra-reasoning) and other entities (inter-reasoning), results in rich and descriptive representations in the underlying neural network backbone, which can be used in downstream tasks such as classification and image retrieval. We evaluate the proposed method following a rigorous experimental procedure, using standard datasets, protocols, and backbones. Self-supervised relational reasoning outperforms the best competitor in all conditions by an average 14% in accuracy, and the most recent state-of-the-art model by 3%. We link the effectiveness of the method to the maximization of a Bernoulli log-likelihood, which can be considered as a proxy for maximizing the mutual information, resulting in a more efficient objective with respect to the commonly used contrastive losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>arXiv:2006.05849v3 [cs.</head><p>LG] 10 Nov 2020 data. Differently from the canonical relational approach, which focuses on relations between objects in the same scene (Santoro et al., 2017), we focus on relations between views of the same object (intra-reasoning) and relations between different objects in different scenes (inter-reasoning), in doing so we allow the learner to acquire both intra-class and inter-class knowledge without the need of labeled data.</p><p>We evaluate our method following a rigorous experimental methodology, since comparing selfsupervised learning methods can be problematic <ref type="bibr" target="#b30">(Kolesnikov et al., 2019;</ref><ref type="bibr" target="#b40">Musgrave et al., 2020)</ref>. Gains may be largely due to the backbone and learning schedule used, rather than the self-supervised component. To neutralize these effects we provide a benchmark environment where all methods are compared using standard datasets (CIFAR-10, CIFAR-100, CIFAR-100-20, STL-10, tiny-ImageNet, SlimageNet), evaluation protocol <ref type="bibr" target="#b30">(Kolesnikov et al., 2019)</ref>, learning schedule, and backbones (both shallow and deep). Results show that our method largely outperforms the best competitor in all conditions by an average 14% accuracy and the most recent state-of-the-art method by 3%.</p><p>Main contributions: 1) we propose a novel algorithm based on relational reasoning for the selfsupervised learning of visual representations, 2) we show its effectiveness on standard benchmarks with an in-depth experimental analysis, outperforming concurrent state-of-the-art methods (code released with an open-source license 1 ), and 3) we highlight how the maximization of a Bernoulli log-likelihood in concert with a relation module, results in more effective and efficient objective functions with respect to the commonly used contrastive losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Overview</head><p>Following the terminology used in the self-supervised literature (Jing and Tian, 2020) we consider relational reasoning as a pretext task for learning useful representations in the underlying neural network backbone. Once the joint system (backbone + relation head) has been trained, the relation head is discarded, and the backbone used in downstream tasks (e.g. classification, image retrieval). To achieve this goal we provide a new formulation of relational reasoning. The canonical formulation defines it as the process of learning the ways in which entities are connected, using this knowledge to accomplish higher-order goals . The proposed formulation defines it as the process of learning the ways entities relate to themselves (intra-reasoning) and to other entities (inter-reasoning), using this knowledge to accomplish downstream goals.</p><p>Consider a set of objects O = {o 1 , . . . , o N }, the canonical approach is within-scene, meaning that all the elements in O belong to the same scene (e.g. fruits from a basket). The within-scene approach is not very useful in our case. Ideally, we would like our learner to be able to differentiate between objects taken from every possible scene. Therefore first we define between-scenes reasoning: the task of relating objects from different scenes (e.g. fruits from different baskets).</p><p>Starting from the between-scenes setting, consider the case where the learner is tasked with discriminating if two objects {o i , o j } ∼ O belong to the same category {o i , o j } → same, or to a different one {o i , o j } → different. Often a single attribute is informative enough to solve the task. For instance, in the pair {apple i , orange j } the color alone is a strong predictor of the class, it follows that the learner does not need to pay attention to other features, this results in poor representations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Learning useful representations from unlabeled data can substantially reduce dependence on costly manual annotation, which is a major limitation in modern deep learning. Toward this end, one solution is to develop learners able to self-generate a supervisory signal exploiting implicit information, an approach known as self-supervised learning <ref type="bibr" target="#b53">(Schmidhuber, 1987</ref><ref type="bibr" target="#b54">(Schmidhuber, , 1990</ref>. Humans and animals are naturally equipped with the ability to learn via an intrinsic signal, but how machines can build similar abilities has been material for debate <ref type="bibr" target="#b33">(Lake et al., 2017)</ref>. A common approach consists of defining a surrogate task (pretext) which can be solved by learning generalizable representations, then use those representations in downstream tasks, e.g. classification and image retrieval <ref type="bibr" target="#b28">(Jing and Tian, 2020)</ref>.</p><p>A key factor in self-supervised human learning is the acquisition of new knowledge by relating entities, whose positive effects are well established in studies of adult learning <ref type="bibr" target="#b14">(Gentner and Kurtz, 2005;</ref><ref type="bibr" target="#b17">Goldwater et al., 2018)</ref>. Developmental studies have shown something similar in children, who can build complex taxonomic names when they have the opportunity to compare objects <ref type="bibr" target="#b15">(Gentner and Namy, 1999;</ref><ref type="bibr" target="#b41">Namy and Gentner, 2002)</ref>. Comparison allows the learner to neglect irrelevant perceptual features and focus on non-obvious properties. Here, we argue that it is possible to exploit a similar mechanism in self-supervised machine learning via relational reasoning.</p><p>The relational reasoning paradigm is based on a key design principle: the use of a relation network as a learnable function to quantify the relationships between a set of objects. Starting from this principle, we propose a new formulation of relational reasoning which can be used as a pretext task to build useful representations in a neural network backbone, by training the relation head on unlabeled 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.</p><p>To solve the issue we alter the object o i via random augmentations A(o i ) (e.g. geometric transformation, color distortion) making between-scenes reasoning more complicated. The color of an orange can be randomly changed, or the shape resized, such that it is much more difficult to discriminate it from an apple. In this challenging setting, the learner is forced to take account of the correlation between a wider set of features (e.g. color, size, texture, etc.). However, it is not possible to create pairs of similar and dissimilar objects when labels are not given. To overcome the problem we bootstrap a supervisory signal directly from the (unlabeled) data, and we do so by introducing intra-reasoning and inter-reasoning. Intra-reasoning consists of sampling two random augmentations of the same object {A(o i ), A(o i )} → same (positive pair), whereas inter-reasoning consists of coupling two random objects {A(o i ), A(o \i )} → different (negative pair). This is like coupling different views of the same apple to build the positive pair, and coupling an apple with a random fruit to build the negative pair. In this work we show that it is possible to train a relation module via intra-reasoning and inter-reasoning, with the aim of learning useful representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Previous work</head><p>Relational reasoning. In the last decades there have been entire sub-fields interested in relational learning: e.g. reinforcement learning <ref type="bibr" target="#b13">(Džeroski et al., 2001)</ref> and statistics <ref type="bibr" target="#b31">(Koller et al., 2007)</ref>. However, only recently the relational paradigm has gained traction in the deep learning community with applications in question answering <ref type="bibr" target="#b48">Raposo et al., 2017)</ref>, graphs <ref type="bibr" target="#b3">(Battaglia et al., 2018)</ref>, sequential streams , deep reinforcement learning <ref type="bibr" target="#b62">(Zambaldi et al., 2019)</ref>, few-shot learning <ref type="bibr" target="#b57">(Sung et al., 2018)</ref>, and object detection <ref type="bibr" target="#b25">(Hu et al., 2018)</ref>. Our work differentiate from previous one in several ways: (i) previous work is based on labeled data, while we use relational reasoning on unlabeled data; (ii) previous work has focused on within-scene relations, here we focus on relations between different views of the same object (intra-reasoning) and between different objects in different scenes (inter-reasoning); (iii) in previous work training the relation head was the main goal, here is a pretext task for learning useful representations in the underlying backbone.</p><p>Solving pretext tasks. There has been a substantial effort in defining self-supervised pretext tasks which can be solved only if generalizable representations have been learned. Examples are: predicting the augmentation applied to a patch <ref type="bibr" target="#b12">(Dosovitskiy et al., 2014)</ref>, predicting the relative location of patches <ref type="bibr" target="#b11">(Doersch et al., 2015)</ref>, solving Jigsaw puzzles <ref type="bibr" target="#b42">(Noroozi and Favaro, 2016)</ref>, learning to count <ref type="bibr" target="#b43">(Noroozi et al., 2017)</ref>, spotting artifacts <ref type="bibr" target="#b26">(Jenni and Favaro, 2018)</ref>, predicting image rotations <ref type="bibr" target="#b16">(Gidaris et al., 2018)</ref>, or image channels <ref type="bibr" target="#b64">(Zhang et al., 2017)</ref>, generating color version of grayscale images <ref type="bibr" target="#b34">Larsson et al., 2016)</ref>, and generating missing patches <ref type="bibr" target="#b47">(Pathak et al., 2016)</ref>.</p><p>Metric learning. The aim of metric learning <ref type="bibr" target="#b6">(Bromley et al., 1994)</ref> is to use a distance metric to bring closer representations of similar inputs (positives), while moving away representations of dissimilar inputs (negatives). Commonly used losses are the contrastive loss <ref type="bibr" target="#b20">(Hadsell et al., 2006)</ref>, the triplet loss <ref type="bibr" target="#b60">(Weinberger et al., 2006)</ref>, the Noise-Constrative Estimation (NCE, <ref type="bibr" target="#b19">Gutmann and Hyvärinen 2010)</ref>, the margin <ref type="bibr" target="#b55">(Schroff et al., 2015)</ref> and magnet <ref type="bibr" target="#b49">(Rippel et al., 2016)</ref> losses. At a first glance relational reasoning and metric learning may seem related, however they are fundamentally different: (i) metric learning explicitly aims at organizing representations by similarity, self-supervised relational reasoning aims at learning a relation measure and, as a byproduct, learning useful representations; (ii) metric learning directly applies a distance metric over the representations, relational reasoning collects representations into a set, aggregates them, then estimates relations; (iii) the relational score is not a distance metric (see Section 3.3) but rather a learnable (probabilistic) similarity measure.</p><p>Contrastive learning. Metric learning methods based on contrastive loss and NCE are often referred to as contrastive learning methods. Contrastive learning via NCE has recently obtained the state of the art in self-supervised learning. However, one limiting factor is that NCE relies on a large quantity of negatives, which are difficult to obtain in mini-batch stochastic optimization. Recent work has used a memory bank to dynamically store negatives during training <ref type="bibr" target="#b61">(Wu et al., 2018)</ref>, followed by a plethora of other methods <ref type="bibr" target="#b22">(He et al., 2019;</ref><ref type="bibr" target="#b58">Tian et al., 2019;</ref><ref type="bibr" target="#b39">Misra and van der Maaten, 2019;</ref><ref type="bibr" target="#b65">Zhuang et al., 2019)</ref>. However, a memory bank has several issues, it introduces additional overhead and a considerable memory footprint. SimCLR <ref type="bibr" target="#b8">(Chen et al., 2020)</ref> tries to circumvent the problem by mining negatives in-batch, but this requires specialized optimizers to stabilize the training at scale. We compare relational reasoning and constrastive learning in Section 3.1 and Section 5.</p><p>Pseudo-labeling. Self-supervision can be achieved providing pseudo-labels to the learner, which are then used for standard supervised learning. A way to obtain pseudo-labels is to use the model itself, picking up the class which has the maximum predicted probability <ref type="bibr" target="#b35">(Lee, 2013;</ref><ref type="bibr" target="#b56">Sohn et al., 2020)</ref>. A neural network ensemble can also be used to provide the labels <ref type="bibr" target="#b18">(Gupta et al., 2020)</ref>. In DeepCluster <ref type="bibr" target="#b7">(Caron et al., 2018)</ref>, pseudo-labels are produced by running a k-means clustering algorithm, which can be forced to induce equipartition <ref type="bibr" target="#b1">(Asano et al., 2020)</ref>. Recent studies have shown that pseudo-labeling is not competitive against other methods <ref type="bibr" target="#b44">(Oliver et al., 2018)</ref>, since they are often prone to degenerate solutions with points assigned to the same label (or cluster).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>InfoMax.</head><p>A recent line of work has investigated the use of mutual information for unsupervised and self-supervised representation learning, following the InfoMax principle <ref type="bibr" target="#b37">(Linsker, 1988)</ref>. Mutual information is often maximized at different scales (global and local) on single views (Deep InfoMax, <ref type="bibr" target="#b24">Hjelm et al. 2019)</ref>, multi-views <ref type="bibr" target="#b27">Ji et al., 2019)</ref>, or sequentially <ref type="bibr" target="#b45">(Oord et al., 2018)</ref>. Those methods are often strongly dependent on the choice of feature extractor architecture <ref type="bibr" target="#b59">(Tschannen et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1:</head><p>Overview of the proposed method. The mini-batch B is augmented K times (e.g. via random flip and crop-resize) and passed through a neural network backbone f θ to produce the representations Z (1) , . . . , Z <ref type="bibr">(K)</ref> . An aggregation function a joins positives (representations of the same images) and negatives (randomly paired representations) through a commutative operator. The relation module r φ estimates the relational score y, which must be 1 for positives and 0 for negatives. The model is optimized minimizing the binary cross-entropy (BCE) between prediction and target t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Description of the method</head><p>Consider an unlabeled dataset D = {x n } N n=1 and a non-linear function f θ (·) parameterized by a vector of learnable weights θ, modeled as a neural network (backbone). A forward pass generates a vector f θ (x n ) = z n (representation), which can be collected in a set Z = {z n } N n=1 . The notation A(x n ) is used to express the probability distribution of instances generated by applying stochastic data augmentation to x n , while x (i) n ∼ A(x n ) is the i-th sample from this distribution (a particular augmented version of the input instance), and</p><formula xml:id="formula_0">D (i) = {x (i) n } N n=1 the i-th set of random augmentations over all instances. Likewise z (i) n = f θ (x (i) n ) is grouped in Z (i) = {z (i) n } N n=1 .</formula><p>Let K indicate the total number of augmentations D (1) , . . . , D (K) and their representations Z (1) , . . . , Z (K) . Now, let us define a relation module r φ (·), as a non-linear function approximator parameterized by φ, which takes as input a pair of aggregated representations and returns a relation score y. Indicating with a(·, ·) an aggregation function and with L(y, t) the loss between the score and a target value t, the complete learning objective can be specified as</p><formula xml:id="formula_1">argmin θ,φ N n=1 K i=1 K j=1 L r φ a(z (i) n , z (j) n ) , t = 1 intra-reasoning + L r φ a(z (i) n , z (j) \n ) , t = 0 inter-reasoning , with z n = f θ (x n ),</formula><p>(1) where \n is an index randomly sampled from {1, . . . , N } \ {n}. In practice (1) can be framed as a standard binary classification problem (see Section 3.4), and minimized by stochastic gradient descent sampling a mini-batch B ∼ D with pairs built by repeatedly applying K augmentations to B. Positives can be obtained pairing two encodings of the same input (intra-reasoning term), and negatives by randomly coupling representations of different inputs (inter-reasoning term), relying on the assumption that in common settings this yields a very low probability of false negatives. An overview of the model is given in <ref type="figure">Figure 1</ref> and the pseudo-code in Appendix C.</p><p>Mutual information. Following the recent work of <ref type="bibr" target="#b4">Boudiaf et al. (2020)</ref> we can interpret (1) in terms of mutual information. Let us define the random variables Z|X and T |Z, representing embeddings and targets. Now consider the generative view of mutual information</p><formula xml:id="formula_2">I(Z; T ) = H(Z) − H(Z|T ).<label>(2)</label></formula><p>Intra-reasoning is a tightening factor which can be expressed as a bound over the conditional entropy H(Z|T ). Inter-reasoning is a scattering factor which can be linked to the entropy of the representations H(Z). In other words, each representation is pushed towards a positive neighborhood (intra-reasoning) and repelled from a complementary set of negatives (inter-reasoning). Under this interpretation (1) can be considered as a proxy for maximizing Equation (2). We refer the reader to <ref type="bibr" target="#b4">Boudiaf et al. (2020)</ref> for a more detailed analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Inputs augmentation</head><p>Given a random mini-batch of M input instances B ∼ D, recursively apply data augmentation K times B (1) , . . . , B (K) then propagate through f θ with a forward pass, to generate the corresponding representations Z (1) , . . . , Z <ref type="bibr">(K)</ref> . Representations are coupled across augmentations to generate positive and negative tuples</p><formula xml:id="formula_3">∀i, j ∈ {1, . . . , K} Z (i) , Z (j) positives and Z (i) ,Z (j) negatives ,<label>(3)</label></formula><p>whereZ indicates random assignment of each representation z (i) n to a different element z (j) \n . In practice, we discard identical pairs (identity mapping is learned across augmentations) and take just one of the symmetrical tuples (z (i) , z (j) ) and (z (j) , z (i) ) (the aggregation function ensures commutation, see Section 3.2). If a certain amount of data in D is labeled (semi-supervised setting), then positive pairs include representations of different augmented inputs belonging to the same category.</p><p>Computational cost. Having defined M as the number of inputs in the mini-batch B, and K as the number of augmentations, the total number of pairs P (positive and negative) is given by</p><formula xml:id="formula_4">P = M (K 2 − K).<label>(4)</label></formula><p>The number of comparisons P scales quadratically with the number of augmentations K, and linearly with the size of the mini-batch M ; whereas in recent constrastive learning methods <ref type="bibr" target="#b8">(Chen et al., 2020)</ref>, they scale as P = (M K) 2 , which is quadratic in both augmentations and mini-batch size.</p><p>Augmentation strategy. Here, we consider the particular case where the input instances are color images. Following previous work <ref type="bibr" target="#b8">(Chen et al., 2020)</ref> we focus on two augmentations: random crop-resize and color distortion. Crop-resize enforces comparisons between views: global-to-global, global-to-local, and local-to-local. Since augmentations are sampled from the same color distribution, the color alone may suffice to distinguish positives and negatives. Color distortion enforces colorinvariant encodings and neutralizes learning shortcuts. Additional details about the augmentations used in this work are reported in Section 4 and Appendix A.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Aggregation function</head><p>Relation networks operate over sets. To avoid a combinatorial explosion due to an increasing cardinality, a commutative aggregation function is applied. Given f θ (x i ) = z i and f θ (x j ) = z j , there are different possible choices for the aggregation function</p><formula xml:id="formula_5">a sum (z i , z j ) = z i + z j , a max (z i , z j ) = max(z i , z j ), a cat (z i , z j ) = z i , z j ,<label>(5)</label></formula><p>where sum and max are applied elementwise. Concatenation a cat is not commutative, but it has been previously used when the cardinality is small <ref type="bibr" target="#b25">(Hu et al., 2018;</ref><ref type="bibr" target="#b57">Sung et al., 2018)</ref>, like in our case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Relation module</head><p>The relation module is a function r φ (·) parameterized by a vector of learnable weights φ, modeled as a multi-layer perceptron (MLP). Given a pair of representations z i and z j , the module takes as input the aggregated pair and produce a scalar y (relation score)</p><formula xml:id="formula_6">r φ a(z i , z j ) = y.<label>(6)</label></formula><p>The relational score respects two properties: (i) r(a(z i , z j )) ∈ [0, 1]; (ii) r(a(z i , z j )) = r(a(z j , z i )).</p><p>It is crucial to not misinterpret the relational score for a pairwise distance metric. Given a set of input vectors {v i , v j , v k } the distance metric d(·, ·) respects four properties:</p><formula xml:id="formula_7">(i) d(v i , v j ) ≥ 0; (ii) d(v i , v j ) = 0 ↔ v i = v j ; (iii) d(v i , v j ) = d(v j , v i ); (iv) d(v i , v k ) ≤ d(v i , v j ) + d(v j , v k ).</formula><p>Note that the relational score does not satisfies all the conditions of a distance metric and therefore the relational score is not a distance metric, but rather a probabilistic estimate (see Section 3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Definition of the loss</head><p>The learning objective (1) can be framed as a binary classification problem over the P representation pairs. Under this interpretation, the relation score y represents a probabilistic estimate of representation membership, which can be induced through a sigmoid activation function. It follows that the objective reduces to the maximization of a Bernoulli log-likelihood, or similarly, the minimization of a binary cross-entropy loss</p><formula xml:id="formula_8">L(y, t, γ) = 1 P P i=1 −w i t i · log y i + (1 − t i ) · log(1 − y i ) ,<label>(7)</label></formula><p>with target t i = 1 for positives and t i = 0 for negatives. The optional weight w i is a scaling factor</p><formula xml:id="formula_9">w i = 1 2 (1 − t i ) · y i + t i · (1 − y i ) γ ,<label>(8)</label></formula><p>where γ ≥ 0 defines how sharp the weight should be. This factor gives more importance to uncertain estimations and it is also known as the focal loss <ref type="bibr" target="#b36">(Lin et al., 2017)</ref>. Note that, a binary estimator has been previously used in the context of correlation minimization for independent component analysis <ref type="bibr" target="#b5">(Brakel and Bengio, 2017)</ref> and in information maximization for representation learning . <ref type="bibr" target="#b24">Hjelm et al. (2019)</ref> did not find any major benefit in using a binary loss (the Jensen-Shannon estimator), but similarly to us they observed a low sensitivity to the number of negative samples, outperforming NCE as the number of negatives became smaller (see Section 5 for a discussion).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Evaluating self-supervised methods is problematic because of substantial inconsistency in the way methods have been compared <ref type="bibr" target="#b30">(Kolesnikov et al., 2019;</ref><ref type="bibr" target="#b40">Musgrave et al., 2020)</ref>. We provide a standardized environment implemented in Pytorch using standard datasets (CIFAR-10, CIFAR-100, CIFAR-100-20, STL-10, tiny-ImageNet, SlimageNet), different backbones (shallow and deep), same learning schedule (epochs), and well know evaluation protocols <ref type="bibr" target="#b30">(Kolesnikov et al., 2019)</ref>. In most conditions our method show superior performance.</p><p>Implementation. Hyperparameters (relation learner): mini-batch of 64 images (K = 16 for ResNet-32 on tiny-ImageNet, K = 25 for ResNet-34 on STL-10, K = 32 for the rest), Adam optimizer with learning rate 10 −3 , binary cross-entropy loss with focal factor (γ = 2). Relation module: MLP with 256 hidden units (batch-norm + leaky-ReLU) and a single output unit (sigmoid). Aggregation: we used concatenation as it showed to be more effective (see Appenidx B.8, <ref type="table" target="#tab_0">Table 13</ref>). Augmentations: horizontal flip (50% chance), random crop-resize, conversion to grayscale (20% chance), and color jitter (80% chance). Backbones: Conv-4, ResNet-8/32/56 and ResNet-34 <ref type="bibr" target="#b23">(He et al., 2016)</ref>. Baselines: DeepCluster <ref type="bibr" target="#b7">(Caron et al., 2018)</ref>, RotationNet <ref type="bibr" target="#b16">(Gidaris et al., 2018)</ref>, Deep InfoMax , and SimCLR <ref type="bibr" target="#b8">(Chen et al., 2020)</ref>. Those are recent (hard) baselines, with SimCLR being the current state-of-the-art in self-supervised learning. As upper bound we include the performance of a fully supervised learner (it has access to the labels), and as lower bound a network initialized with random weights, evaluated training only the linear classifier. All results are the average over three random seeds. Additional details in Appendix A.</p><p>Linear evaluation. We follow the linear evaluation protocol defined by <ref type="bibr" target="#b30">Kolesnikov et al. (2019)</ref> training the backbone for 200 epochs using the unlabeled training set, and then training for 100 epochs a linear classifier on top of the backbone features (without backpropagation in the backbone weights). The accuracy of this classifier on the test set is considered as the final metric to asses the quality of the representations. Our method largely outperforms other baselines with an accuracy of 46.2% (CIFAR-100) and 30.5% (tiny-Imagenet), which is an improvement of +4.0% and +4.7% over the best competitor (SimCLR), see <ref type="table" target="#tab_0">Table 1</ref>. Best results are also obtained with the Conv-4 backbone on all datasets. Only in CIFAR-10/ResNet-32 SimCLR is doing better, with a score of 77% against 75% of our method, see Appendix B.1. In the appendix we report the results on the challenging SlimageNet dataset used in few-shot learning <ref type="bibr" target="#b0">(Antoniou et al., 2020)</ref>: 160 low-resolution images for each one of the 1000 classes in ImageNet. On SlimageNet our method has the highest accuracy (15.8%, K = 16), being better than RotationNet (7.2%) and SimCLR (14.3%).</p><p>Domain transfer. We evaluate the performance of all methods in transfer learning by training on the unlabeled CIFAR-10 with linear evaluation on the labeled CIFAR-100 (and viceversa). Our  method outperforms once again all the others in every condition. In particular, it is very effective in generalizing from a simple dataset (CIFAR-10) to a complex one (CIFAR-100), obtaining an accuracy of 41.5%, which is a gain of +5.3% over SimCLR and +7.5% over the supervised baseline (with linear transfer). For results see <ref type="table" target="#tab_0">Table 1</ref> and Appendix B.2.</p><p>Grain. Different methods produce different representations, some may be better on datasets with a small amount of labels (coarse-grained), others may be better on datasets with a large amount of labels (fine-grained). To investigate the granularity of the representations we train on unlabeled CIFAR-100, then perform linear evaluation using the 100 labels (fine grained; e.g. apple, fox, bee, etc) and the 20 super-labels (coarse grained; e.g. fruits, mammals, insects, etc). Also in this case our method is superior in all conditions with an accuracy of 52.4% on CIFAR-100-20, see <ref type="table" target="#tab_0">Table 1</ref> and Appendix B.3. In comparison, the method does better in the fine-grained case, indicating that it is well suited for datasets with a large amount of classes.</p><p>Finetuning. We used the STL-10 dataset <ref type="bibr" target="#b9">(Coates et al., 2011)</ref> which provides a set of unlabeled data coming from a similar but different distribution from the labeled data. Methods have been trained for 300 epochs on the unlabeled set (100K images), finetuned for 20 epochs on the labeled set (5K images), and finally evaluated on the test set (8K images). We used a mini-batch of 64 with K = 25 and a ResNet-34. Implementation details are reported in Appendix A.6. Results in <ref type="table" target="#tab_0">Table 1</ref> show that our method obtains the highest accuracy: 89.67% (best seed 90.04%). Moreover a wider comparison reported in Appendix B.4 shows that the method outperforms strong supervised baselines and the previous self-supervised state-of-the-art (88.80%, <ref type="bibr" target="#b27">Ji et al., 2019</ref>).</p><p>Depth of the backbone. In Appendix B.5 we report an extensive comparison on four backbones of increasing depth: Conv-4, ResNet-8, ResNet-32, and ResNet-56. We tested the three best methods (RotationNet, SimCLR, and Relational Reasoning) on CIFAR-10/100 linear evaluation, grain, and domain transfer for a total of 24 conditions. Results show that our method has the highest accuracy on 21 of those conditions, with SimCLR performing better on CIFAR-10 linear evaluation with ResNet backbones. A distilled version of those results is reported in <ref type="figure">Figure 2a</ref>. The figure shows the gain in accuracy from using a ResNet-32 instead of a Conv-4 backbone for datasets of increasing complexity (10, 100, and 200 classes). As the complexity of the dataset raises our method performs increasingly better than the others. The relative gain against SimCLR gets larger: −2.6% (CIFAR-10), +1.1% (CIFAR-100), +3.3% (tiny-ImageNet). The relative gain against RotationNet is even more evident: +8.7%, +11.2%, +11.9%.</p><p>Additional experiments. <ref type="figure">Figure 2b</ref> and Appendix B.6 show the difference in accuracy between K = 2 and K ∈ {4, 8, 16, 32} mini-batch augmentations for a fixed mini-batch size. There is a clear positive correlation between the number of augmentations and the performance of our model, while the same does not hold for a self-supervised algorithm (RotationNet) and the supervised baseline. <ref type="figure">Figure 2c</ref> and Appendix B.7 show the accuracy obtained via linear evaluation in the semi-supervised setting, when the number of available labels is gradually increased (0%, 1%, 10%, 25%, 50%, 100%), in both CIFAR-10 and CIFAR-100 (ResNet-32). The accuracy is positively correlated with the proportion of labels available, approaching the supervised upper bound when 100% of labels are available (supervised case).</p><p>Ablations. In Appendix B.8 we report the results of ablation studies on the aggregation function and relation head. We compare four aggregation functions: sum, mean, maximum, and concatenation. Results show that concatenation and maximum are respectively the most and less effective functions. Concatenation may favor backpropagation improving the quality of the representations, as supported by similar results in previous work <ref type="bibr" target="#b57">(Sung et al., 2018)</ref>. Ablations of the relation head have followed two directions: (i) removing the head, and (ii) replacing the relation module with an encoder. In the first condition we removed the head and replace it with a simple dot product between representation pairs (BCE-focal loss). In the second condition we followed an approach similar to SimCLR <ref type="bibr" target="#b8">(Chen et al., 2020)</ref>, replacing the relation head with an encoder and applying the dot product to representations at the higher level (BCE-focal loss). The second condition differs from SimCLR for the loss type (BCE vs Contrastive) and total number of mini-batch augmentations (K = 32 vs K = 2). In both conditions we observe a severe degradation of the performance with respect to the complete model (from a minimum of −3% to a maximum of −23%), confirming that the relation module is a fundamental component in the pipeline (see discussion in Section 5).</p><p>Qualitative analysis. In Appendix B.9 is presented a qualitative comparison between the proposed method and RotationNet, on an image retrieval downstream task. Given a random query image (not cherry-picked) the top-10 most similar images in representation space are retrieved. Our method shows better distinction between categories which are hard to separate (e.g. ships vs planes, trucks vs cars). The lower sample variance and the higher similarity with the query, confirm the fine-grained organization of the representations, which account for color, texture, and geometry. An analysis of retrieval errors in Appendix B.10 shows that the proposed method is superior in accuracy across all categories while being more robust against misclassification, with a top-10 retrieval accuracy of 67.8% against 47.7% of RotationNet. In Appendix B.11 we report a qualitative analysis of the representations (ResNet-32, CIFAR-10) using t-SNE <ref type="bibr" target="#b38">(Maaten and Hinton, 2008)</ref>. Relational reasoning is able to aggregate the data in a more effective way, and to better capture high level relations with lower scattering (e.g. vehicles vs animals super-categories).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion and conclusions</head><p>Self-supervised relational reasoning is effective on a wide range of tasks in both a quantitative and qualitative manner, and with backbones of different size (ResNet-32, ResNet-56 and ResNet-34, with 0.5 × 10 6 , 0.9 × 10 6 and 21.3 × 10 6 parameters). Representations learned through comparison can be easily transferred across domains, they are fine-grained and compact, which may be due to the direct correlation between accuracy and number of augmentations. An instance is pushed towards a positive neighborhood (intra-reasoning) and repelled from a complementary set of negatives (inter-reasoning). The number of augmentations may have a primary role in this process affecting the quality of the clusters. The possibility to exploit an high number of augmentations, by generating them on the fly, could be decisive in the low-data regime (e.g. unsupervised few-shot/online learning) where self-supervised relational reasoning has the potential to thrive. Those are factors that require further consideration and investigation.</p><p>From self-supervised to supervised. Recent work has showed that contrastive learning can be used in a supervised setting with competitive results <ref type="bibr" target="#b29">(Khosla et al., 2020)</ref>. In our experiments we have observed a similar trend, with relational reasoning approaching the supervised performance when all the labels are available. However, we have obtained those results using the same hyperparameters and augmentations used in the self-supervised case, while there may be alternatives that are more effective. Learning by comparison could help in disentangling fine-grained differences in a fully supervised setting with high number of classes, and be decisive to build complex taxonomic representations, as pointed out in cognitive studies <ref type="bibr" target="#b15">(Gentner and Namy, 1999;</ref><ref type="bibr" target="#b41">Namy and Gentner, 2002)</ref>.</p><p>Comparison with contrastive methods. We have compared relational reasoning to a state-of-the-art contrastive learning method (SimCLR) using the same backbone, head, augmentation strategy, and learning schedule. Relational reasoning outperforms SimCLR (+3% on average) using a lower number of pairs, being more efficient. Given a mini-batch of size 64, relational reasoning uses 6.35 × 10 4 (K = 32) and 1.5 × 10 4 (K = 16) pairs, against 6.55 × 10 4 of SimCLR with mini-batch 128. Contrastive losses needs a large number of negatives, which can be gathered by increasing M the size of the mini-batch, or increasing K the number of augmentations (both solutions incur a quadratic cost, see Section 3.1). High quality negatives can only be gathered following the first solution, since the second provides lower sample variance. A typical mini-batch in SimCLR encloses 98% negatives and 2% positives, in our method 50% negatives and 50% positives. The larger set of positives could be one of the reasons why relational reasoning is more effective in disentangling fine-grained representations. In addition to the difference in loss type, there is an important structural difference between the two approaches: in SimCLR pairs are allocated in the loss space and then compared via dot product, while in relational reasoning they are aggregated in the space of transferable representations and compared through a relation head. Ablation studies in Section 4 have shown that this structural difference is fundamental for obtaining higher performances, but the way it influences the learning dynamics and the optimization process is not clear and requires further investigation.</p><p>Why does cross-entropy work so well? We argue that in the context of recent state-of-the-art methods, cross-entropy has been overlooked in favor of contrastive losses. Our experiments show that cross-entropy is a more efficient and effective objective function with respect to the commonly used contrastive losses. Based on the results of the ablation studies, we hypothesize that the difference in performance is mainly due to the use of a relation module in conjunction with the binary cross-entropy loss. When the BCE is split from the relation head and applied directly to the representations there is a drastic drop in performance; applying the BCE to surrogate representations in a second encoding stage (like in SimCLR) is equally ineffective. Therefore, the use of BCE on its own does not provide any advantage but in concert with the relation head it becomes effective. A more thorough analysis is necessary to substantiate these findings, which is left for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>The motivation behind this work is to build systems able to exploit a large amount of unlabeled data. Applications that could benefit from the proposed method span from standard supervised classifiers to medical diagnostic systems. Therefore, there is a large number of individuals who may benefit or be harmed from this research. This requires putting some effort into selecting the data source, especially when the system is scaled.</p><p>In most cases a large body of unlabeled images can be easily gathered from the internet; to avoid biases those images should be representative of different categories. Our method does not guarantee unbiased predictions, therefore it should be used with caution in critical applications. Individuals who may want to use it should consider the particular source of data at hand and evaluate how it could impact the system performance after the final deployment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Implementation details A.1 Datasets</head><p>For datasets with low/medium number of categories we used CIFAR-10 and CIFAR-100 <ref type="bibr" target="#b32">(Krizhevsky et al., 2009)</ref>, which are composed of 32 × 32 RGB images, with 10 and 100 classes respectively. In addition we used the 20 super-classes of CIFAR-100 (naming this CIFAR-100-20), which consists of broader categories (e.g. fruits, mammals, insects, etc). In the finetuning experiments we used the STL-10 dataset <ref type="bibr" target="#b9">(Coates et al., 2011)</ref> which provides 100K RGB images of size 96 × 96 in the unlabeled set, 5K images in the labeled set, and 8K images in the test set.</p><p>For datasets with an high number of categories we used the tiny-ImageNet and SlimageNet <ref type="bibr" target="#b0">(Antoniou et al., 2020)</ref> datasets, both of them derived from ImageNet <ref type="bibr" target="#b50">(Russakovsky et al., 2015)</ref>. Tiny-ImageNet consists of 200 different categories, with 500 training images (64 × 64, 100K in total), 50 validation images (10K in total), and 50 test images (10K in total). SlimageNet consists of 64 × 64 RGB images, 1000 categories with 160 training images (160K in total), 20 validation images (20K in total), and 20 test images (20K in total). Both of them are considered more challenging than ImageNet because of the lower resolution of the images and lower number of training samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Backbones</head><p>We use off-the-shelf Pytorch implementations of ResNets as described in the original paper <ref type="bibr" target="#b23">(He et al., 2016)</ref>. Some of these networks have quite different structure, with ResNet-8/32/56 based on three hyper-blocks (ResNet-32 has 0.5 × 10 6 total parameters) and ResNet-34 based on four hyper-blocks (21.3 × 10 6 total parameters). The Conv-4 backbone is based on three blocks (8, 16, 32 feature maps), each one performing: convolution (kerne-size=3, stride=1, padding=1), BatchNorm, ReLU, average pooling (kerne-size=2, stride=2). The fourth block (64 feature maps) performed the same operations but with an adaptive average pooling to squeeze the maps to unit shape in the spatial dimension. We used standard fan-in/fan-out weight initialization, and set BatchNorm weights to 1 and bias to 0. For Conv-4 and ResNet-8/32/56 the size of the representations is 64, whereas for ResNet-34 is 512.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Augmentations</head><p>During the self-supervised training phase of our method we used a set of augmentations which is similar to the one adopted by <ref type="bibr" target="#b8">Chen et al. (2020)</ref>. We apply horizontal flip (50% chance), random crop-resize, conversion to grayscale (20% chance), and color jitter (80% chance). Random crop-resize consists of cropping the given image (from 0.08 to 1.0 of the original size), changing the aspect ratio (from 3/4 to 4/3 of the original aspect ratio), and finally resizing to input shape using a bilinear interpolation. Color jitter consists of sampling from a uniform distribution [0, max] a jittering value for: brightness (max = 0.8), contrast (max = 0.8), saturation (max = 0.8), and hue (max = 0.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Computing infrastructure</head><p>All the experiments have been performed on a workstation with 20 cores, 187 GB of RAM, and with 8 NVIDIA GeForce RTX 2080 Ti GPUs (11 GB of internal RAM). All the methods could fit on a single one of those GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Other methods</head><p>Supervised. This baseline consists of standard supervised training. We used standard data augmentation (horizontal flip and random crop) and learning schedule (SGD optimizer with initial learning rate of 0.1 divided by 10 at 50% and 75% of total epochs). It represents an upper bound. When evaluated for the number of augmentations (Appendix B.6) the same strategy adopted in our method (Appendix A.3) has been used to augment the input mini-batch (size 128) K times with coherent labels.</p><p>Random weights. This baseline consists of initializing the weights of the backbone via standard fan-in/fan-out, then perform linear evaluation optimizing the last linear layer (without backpropagation on the backbone). It represents a lower bound since the backbone is not trained.</p><p>DeepCluster <ref type="bibr" target="#b7">(Caron et al., 2018)</ref>. We adapted the open-source implementation provided by the authors 2 . Clustering has been performed at the beginning of each epoch by using the k-means algorithm available in Scikitlearn. We performed whitening over the features before the clustering step, as suggested by the authors. We used a number of cluster one order of magnitude larger than the number of classes in the dataset, as recommended by the authors to improve the performance. We also used an MLP head (256 hidden units with leaky-ReLU and BatchNorm) instead of a linear layer, since in our tests this showed to slightly boost the performance. The MLP weights have been reset at the beginning of each epoch as in the original code. We optimized the model minimizing the cross-entropy loss between the pseudo-labels provided by the clustering and the network outputs. We used Adam optimizer with learning rate 10 −3 .</p><p>RotationNet <ref type="bibr" target="#b16">(Gidaris et al., 2018)</ref>. Given the simplicity of the method, this has been reproduced locally following the instructions of the authors. Labels are provided by 4 rotations (0 • , 90 • , 180 • , 270 • ), those are the one providing the highest accuracy according to the authors. The input mini-batch of size 128, has been augmented adding 4 rotations for each image (resulting in a mini-batch of size 128 × 4). This is in line with the best performing strategy reported by the authors. In all experiments the cross-entropy loss between the network output and the labels provided by the rotation has been minimized (Adam optimizer, learning rate 10 −3 ). When evaluated for the number of augmentations (Appendix B.6) the same strategy used in our method has been applied (Appendix A.3), augmenting the input mini-batch (size 128) K times with coherent self-supervised rotation labels. In order to keep the size of the mini-batch manageable the additional 4 rotations for image have not been included, since this would increase the size to 4 × K and not fit on the available hardware.</p><p>Deep InfoMax  The code has been adapted from open-source implementations available online (see code for details) and from the code provided by the authors 3 . The local version of the algorithm has been used (α = 0, β = 1.0, γ = 0.1), as reported by the authors this is the one with the best performance. The capacity of the discriminator networks has been partially reduced to fit the available hardware and to speedup the training, this did not affected significantly the results. We used Adam optimizer with learning rate 10 −4 as in the original paper.</p><p>SimCLR <ref type="bibr" target="#b8">(Chen et al., 2020)</ref> The code has been adapted from the implementation provided by the authors 4 and other open-source implementations (see code for details). To have a fair comparison with our method we used the same MLP head, the same data augmentation strategy, and optimizer (Adam with learning rate10 −3 ). We used a temperature of 0.5 in all the experiments, this was reported as the consistent optimal value regardless of the batch sizes in the original paper. We could not replicate the original setup reported by the authors on very large mini-batches, since it is computationally expensive, requiring 32 to 128 cores on Tensor Processing Units (TPUs). We adapted the setup to our available hardware (described in Appendix A.4), and we guaranteed a fair comparison by using a comparable number of pairs. In particular, we used a mini-batch of 128 images, which results in 6.5 × 10 4 pairs, this is similar (or superior) to the number of pairs compared by our method which is 6.3 × 10 4 for K = 32, 3.8 × 10 4 for K = 25, and 1.5 × 10 4 for K = 16.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 Finetuning experiments</head><p>All methods are trained for 300 epochs on the unlabeled portion of the STL-10 dataset, using the same hyperparameters and augmentations described before and a ResNet34 backbone. In the finetuning stage the pretrained backbone is coupled with a linear classifier and both of them are trained using Adam optimizer for 100 epochs with mini-batch of size 32. A lower learning rate for the backbone (10 −4 ) respect to the linear classifier (10 −3 ) has been used. Both learning rates are divided by 10 at 50% and 75% of total epochs. The same augmentations of <ref type="bibr" target="#b27">Ji et al. (2019)</ref> have been used for the finetuning stage. Those consists of affine transformations (50% chance) sampled from a uniform distribution [min, max]: random rotation (min = −18 • , max = 18 • ), scale (min = 0.9, max = 1.1), translation (min = 0, max = 0.1), shear (min = −10, max = 10), and bilinear interpolation, cutout (50% chance) with 32 × 32 patches. Same schedule and augmentations have also been used to train (from scratch) the supervised baseline on the labeled set of data (100 epochs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7 Semi-supervised experiments</head><p>We adapted our method to the semi-supervised case by coupling instances sampled from the same category. Those instances represented a portion of the total number of pairs in the mini-batch depending on the percentage of available labels. Results for each conditions are the average of three seeds. We used the same hyperparameters described in the linear evaluation phase. We did not prevent possible collisions between classes during the allocation of negative pairs. Collisions are unlikely in datasets with medium/high number of classes, but a slight performance improvement could be obtained if negatives are paired without collisions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.8 Qualitative analysis experiments</head><p>For the qualitative analysis we compared the representations generated by the supervised baseline, Rotation Net, and our method on CIFAR-10 with a ResNet-32 backbone at the end of the training (200 epochs). The query images were randomly sampled and the representations compared using Euclidean distance. For the t-SNE analysis we used the Scikit implementation of the algorithms and set the hyperparameters as follows: 1000 iterations, Euclidean metric, random init, perplexity 30, learning rate 200.          Supervised n/a n/a n/a n/a n/a 90.87±0.41 Relational Reasoning (ours) 74.99±0.07 <ref type="bibr">76.55±0.27 80.14±0.35 85.30±0.28 89.35±0.11 90.66±0.23</ref>  Supervised n/a n/a n/a n/a n/a 65.32±0.22 Relational Reasoning (ours) <ref type="bibr">46.17±0.17 46.10±0.29 49.55±0.36 54.44±0.58 58.52±0.70 58.96±0.28</ref> B.8 Ablations <ref type="table" target="#tab_0">Table 13</ref>: Ablation of the aggregation function. Training with relational self-supervision on unlabeled CIFAR-10 and CIFAR-100, and linear evaluation on labeled datasets . Mean accuracy (percentage) and standard deviation over three runs on a validation set (obtained sampling 20% of the images from the training set). Best results highlighted in bold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Additional results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Linear evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Aggregation</head><p>Analytical form CIFAR-10 CIFAR-100   <ref type="figure">Figure 3</ref>: Ablation of the relation head (graphical illustration). Comparison between the two ablations in (a) and <ref type="bibr">(b)</ref>, and the full model in (c). In (a) the head is removed and the dot product z 1 , z 2 is used to compare the representations pair. In (b) the relation head is replaced with an encoder g φ that projects each representation in another latent space where the dot product is performed. In (c) is showed the full model, with the relation module r φ taking in input the aggregated pair. In all cases is minimized the binary cross-entropy loss (BCE) over positive and negative pairs. The query is the leftmost image (red frame), followed by the top-10 most similar images (Euclidean distance) in representation space. Comparison between (a) self-supervised relational reasoning (ours), and (b) self-supervised rotation prediction <ref type="bibr" target="#b16">(Gidaris et al., 2018)</ref>. Our method shows better distinction between categories which are hard to separate, (e.g. ships vs planes in row 4, trucks vs cars in row 12). Moreover, the lower sample variance and the higher similarity with the query, indicates a fine-grained organization in representation space (e.g. red sport cars in row 1, long white trucks in rows 11 and 12, deer with snow in row 16, blue car in row 22, dog breeds in row 25).      <ref type="bibr" target="#b16">(Gidaris et al., 2018)</ref>. Our method shows a lower scattering, with clusters which are more distinct.  <ref type="figure">Figure 7</ref>: Visualization of t-SNE embeddings for the 10K test points in CIFAR-10 divided in two super-categories: vehicles (plane, car, ship, truck), and animals (bird, cat, deer, dog, frog, horse). ResNet-32 backbone trained via (a) supervised learning, (b) self-supervised relational reasoning (ours), and (c) self-supervised rotation prediction <ref type="bibr" target="#b16">(Gidaris et al., 2018)</ref>. Our method shows a better split, lower scattering, and a minor overlap between the two super-categories.</p><formula xml:id="formula_10">Sum a sum (z i , z j ) = z i + z j 57.</formula><p>C Pseudo-code of the method Algorithm 1 Self-supervised relational learning: training function and shuffling without collisions. Require: D = {x n } N n=1 unlabeled training set; A(·) augmentation distribution; θ parameters of f θ (neural network backbone); φ parameters of r φ (relation module); aggregation function a(·, ·); α and β learning rate hyperparameters; K number of augmentations; M mini-batch size; for i = 1 to K − 1 do 10:</p><p>for j = i + 1 to K do 11:</p><p>P ← a(Z (i) , Z (j) ), t = 1 Aggregating and appending positive pairs 12:Z (j) = SHUFFLE(Z (j) ) Shuffling without collisions 13:</p><p>P ← a(Z (i) ,Z (j) ), t = 0 Aggregating and appending negative pairs </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) Difference in accuracy using the deeper backbone (Conv4→ResNet-32, linear evaluation). As the complexity of the dataset raises our method performs increasingly better than the others. (b) Correlation between validation accuracy (3 seeds, Conv-4, CIFAR-10) and number of mini-batch augmentations. Only in our method the accuracy is positively correlated with the number of augmentations. (c) Semi-supervised accuracy with an increasing percentage of labels (ResNet-32).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Image retrieval given 25 random queries (not cherry-picked) on CIFAR-10 with ResNet-32.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Visualization of t-SNE embeddings for the 10K test points in CIFAR-10. ResNet-32 backbone trained via (a) supervised learning, (b) self-supervised relational reasoning (ours), and (c) self-supervised rotation prediction</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>1:function TRAIN(D, α, β, M , K, θ, φ)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison on various benchmarks. Mean accuracy (percentage) and standard deviation over three runs. Best results in bold. Linear Evaluation: training on unlabeled data and linear evaluation on labeled data. Domain Transfer: training on unlabeled CIFAR-10 and linear evaluation on labeled CIFAR-100 (10→100), and viceversa (100→10). Grain: training on unlabeled CIFAR-100, linear evaluation on coarse-grained. Finetune: training on the unlabeled set of STL-10, finetuning on the labeled set (ResNet-34).</figDesc><table><row><cell></cell><cell cols="2">Linear Evaluation</cell><cell cols="2">Domain Transfer</cell><cell>Grain</cell><cell>Finetune</cell></row><row><cell>Method</cell><cell cols="2">CIFAR-100 tiny-ImgNet</cell><cell>10→100</cell><cell>100→10</cell><cell>CIFAR-100-20</cell><cell>STL-10</cell></row><row><cell>Supervised (upper bound)</cell><cell cols="4">65.32±0.22 50.09±0.32 33.98±0.71 71.01±0.44</cell><cell>76.35±0.57</cell><cell>69.82±3.36</cell></row><row><cell>Random Weights (lower bound)</cell><cell>7.65±0.44</cell><cell>3.24±0.43</cell><cell cols="2">7.65±0.44 27.47±0.83</cell><cell>16.56±0.48</cell><cell>n/a</cell></row><row><cell>DeepCluster (Caron et al., 2018)</cell><cell cols="4">20.44±0.80 11.64±0.21 18.37±0.41 43.39±1.84</cell><cell>29.49±1.36</cell><cell>73.37±0.55</cell></row><row><cell>RotationNet (Gidaris et al., 2018)</cell><cell cols="4">29.02±0.18 14.73±0.48 27.02±0.20 52.22±0.70</cell><cell>40.45±0.39</cell><cell>83.29±0.44</cell></row><row><cell cols="5">Deep InfoMax (Hjelm et al., 2019) 24.07±0.05 17.51±0.15 23.73±0.04 45.05±0.24</cell><cell>33.92±0.34</cell><cell>76.03±0.37</cell></row><row><cell>SimCLR (Chen et al., 2020)</cell><cell cols="4">42.13±0.35 25.79±0.35 36.20±0.16 65.59±0.76</cell><cell>51.88±0.48</cell><cell>89.31±0.14</cell></row><row><cell>Relational Reasoning (ours)</cell><cell cols="4">46.17±0.17 30.54±0.42 41.50±0.35 67.81±0.42</cell><cell>52.44±0.47</cell><cell>89.67±0.33</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Linear evaluation. Self-supervised training on unlabeled data and linear evaluation on labeled data. Comparison between three datasets (CIFAR-10, CIFAR-100, tiny-ImageNet) for a shallow (Conv-4) and a deep (ResNet-32) backbone. Mean accuracy (percentage) and standard deviation over three runs. Best results highlighted in bold.</figDesc><table><row><cell></cell><cell>Conv-4</cell><cell></cell><cell></cell><cell>ResNet-32</cell><cell></cell></row><row><cell>Method</cell><cell cols="5">CIFAR-10 CIFAR-100 tiny-ImageNet CIFAR-10 CIFAR-100 tiny-ImageNet</cell></row><row><cell>Supervised (upper bound)</cell><cell>80.46±0.39 49.29±0.85</cell><cell>36.47±0.36</cell><cell cols="2">90.87±0.41 65.32±0.22</cell><cell>50.09±0.32</cell></row><row><cell>Random Weights (lower bound)</cell><cell>32.92±1.88 10.79±0.59</cell><cell>6.19±0.13</cell><cell>27.47±0.83</cell><cell>7.65±0.44</cell><cell>3.24±0.43</cell></row><row><cell>DeepCluster (Caron et al., 2018)</cell><cell>42.88±0.21 21.03±1.56</cell><cell>12.60±1.23</cell><cell cols="2">43.31±0.62 20.44±0.80</cell><cell>11.64±0.21</cell></row><row><cell>RotationNet (Gidaris et al., 2018)</cell><cell>56.73±1.71 27.45±0.80</cell><cell>18.40±0.95</cell><cell cols="2">62.00±0.79 29.02±0.18</cell><cell>14.73±0.48</cell></row><row><cell cols="2">Deep InfoMax (Hjelm et al., 2019) 44.60±0.27 22.74±0.21</cell><cell>14.19±0.13</cell><cell cols="2">47.13±0.45 24.07±0.05</cell><cell>17.51±0.15</cell></row><row><cell>SimCLR (Chen et al., 2020)</cell><cell>60.43±0.26 30.45±0.41</cell><cell>20.90±0.15</cell><cell cols="2">77.02±0.64 42.13±0.35</cell><cell>25.79±0.40</cell></row><row><cell>Relational Reasoning (ours)</cell><cell>61.03±0.23 33.38±1.02</cell><cell>22.31±0.19</cell><cell cols="2">74.99±0.07 46.17±0.16</cell><cell>30.54±0.42</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Linear evaluation on SlimageNet<ref type="bibr" target="#b0">(Antoniou et al., 2020)</ref>. This dataset is more challenging than ImageNet, since it only has 160 low-resolution (64 × 64) color images for each one of the 1000 classes of ImageNet. Below is reported the linear evaluation accuracy on labeled data with a ResNet-32 backbone, after training on unlabeled data. Mean accuracy (percentage) and standard deviation over three runs. Best result highlighted in bold.</figDesc><table><row><cell>Method</cell><cell>SlimageNet</cell></row><row><cell>Supervised (upper bound)</cell><cell>33.94±0.21</cell></row><row><cell>Random Weights (lower bound)</cell><cell>0.79±0.09</cell></row><row><cell>RotationNet (Gidaris et al., 2018)</cell><cell>7.25±0.28</cell></row><row><cell>SimCLR (Chen et al., 2020)</cell><cell>14.32±0.24</cell></row><row><cell>Relational Reasoning (ours)</cell><cell>15.81±0.72</cell></row><row><cell>B.2 Domain transfer</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Domain transfer. Training with self-supervision on unlabeled CIFAR-10 linear evaluation on CIFAR-100 (10 → 100), and viceversa (100 → 10). ∆ indicates the difference between the accuracy in the standard setting (unsupervised train and linear evaluation on the same dataset) and the accuracy in the transfer setting (unsupervised train on first dataset and linear evaluation on the second dataset). Mean accuracy (percentage) and standard deviation over three runs. Best results highlighted in bold.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Conv-4</cell><cell></cell><cell></cell><cell cols="2">ResNet-32</cell><cell></cell></row><row><cell>Method</cell><cell>10 → 100</cell><cell>∆</cell><cell>100 → 10</cell><cell>∆</cell><cell>10 → 100</cell><cell>∆</cell><cell>100 → 10</cell><cell>∆</cell></row><row><cell>Supervised (upper bound)</cell><cell cols="8">32.06±0.63 -17.23 64.00±1.07 -16.46 33.98±0.70 -31.34 71.01±0.44 -19.86</cell></row><row><cell>Random Weights (lower bound)</cell><cell>10.79±0.59</cell><cell>n/a</cell><cell>32.92±1.89</cell><cell>n/a</cell><cell>7.65±0.44</cell><cell>n/a</cell><cell>27.47±0.83</cell><cell>n/a</cell></row><row><cell>DeepCluster (Caron et al., 2018)</cell><cell cols="8">19.68±1.23 -1.35 43.59±1.31 +0.71 18.37±0.41 -2.07 43.39±1.84 +0.08</cell></row><row><cell>RotationNet (Gidaris et al., 2018)</cell><cell cols="8">26.06±0.09 -1.39 51.86±0.36 -4.87 27.02±0.20 -2.00 52.22±0.70 -9.78</cell></row><row><cell cols="9">Deep InfoMax (Hjelm et al., 2019) 22.35±0.12 -0.39 43.30±0.15 -1.30 23.73±0.04 -0.34 45.05±0.24 -2.08</cell></row><row><cell>SimCLR (Chen et al., 2020)</cell><cell cols="8">29.20±0.08 -1.25 54.73±0.60 -5.70 36.21±0.16 -5.92 65.59±0.76 -11.43</cell></row><row><cell>Relational Reasoning (ours)</cell><cell cols="8">31.84±0.23 -1.54 57.30±0.26 -3.73 41.50±0.35 -4.67 67.81±0.42 -7.18</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Grain. Training with self-supervision on unlabeled CIFAR-100, and linear evaluation on labeled CIFAR-100 Fine-Grained (100 classes) and CIFAR-100-20 Coarse-Grained (20 super-classes). Mean accuracy (percentage) and standard deviation over three runs. Best results highlighted in bold.</figDesc><table><row><cell></cell><cell cols="2">Conv-4</cell><cell cols="2">ResNet-32</cell></row><row><cell>Method</cell><cell cols="4">Fine-Grain Coarse-Grain Fine-Grain Coarse-Grain</cell></row><row><cell>Supervised (upper bound)</cell><cell>49.29±0.85</cell><cell>59.91±0.62</cell><cell>65.32±0.22</cell><cell>76.35±0.57</cell></row><row><cell>Random Weights (lower bound)</cell><cell>10.79±0.59</cell><cell>19.94±0.31</cell><cell>7.65±0.44</cell><cell>16.56±0.48</cell></row><row><cell>DeepCluster (Caron et al., 2018)</cell><cell>21.03±1.56</cell><cell>30.07±2.06</cell><cell>20.44±0.80</cell><cell>29.49±1.36</cell></row><row><cell>RotationNet (Gidaris et al., 2018)</cell><cell>27.45±0.80</cell><cell>35.49±0.17</cell><cell>29.02±0.19</cell><cell>40.45±0.39</cell></row><row><cell cols="2">Deep InfoMax (Hjelm et al., 2019) 22.74±0.21</cell><cell>32.36±0.43</cell><cell>24.07±0.05</cell><cell>33.92±0.34</cell></row><row><cell>SimCLR (Chen et al., 2020)</cell><cell>30.45±0.41</cell><cell>37.72±0.14</cell><cell>42.13±0.35</cell><cell>51.88±0.48</cell></row><row><cell>Relational Reasoning (ours)</cell><cell>33.38±1.02</cell><cell>40.86±1.03</cell><cell>46.17±0.17</cell><cell>52.44±0.47</cell></row><row><cell>B.4 Finetuning</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Finetuning. Comparison with other results reported in the literature on unsupervised training and finetuning on the STL-10 dataset. Best result in bold. Local refers to our local reproduction of the method, with results reported as best (mean ± std) on three runs with different seeds. Note that backbone and learning schedule may differ. The ResNet-34 backbone is much larger than ResNet-32 (21.3 × 10 6 vs 0.47 × 10 6 ), showing that the proposed method can be effectively scaled.</figDesc><table><row><cell>Method</cell><cell>Reference</cell><cell>Backbone</cell><cell>Accuracy</cell></row><row><cell>Supervised (crop + cutout)</cell><cell>DeVries and Taylor (2017)</cell><cell>WideResnet-16-8</cell><cell>87.30</cell></row><row><cell>Supervised (scattering)</cell><cell>Oyallon et al. (2017)</cell><cell>Hybrid-WideResnet</cell><cell>87.60</cell></row><row><cell>Exemplars (Dosovitskiy et al., 2014)</cell><cell>Dosovitskiy et al. (2014)</cell><cell>Conv-3</cell><cell>72.80</cell></row><row><cell>Artifacts (Jenni and Favaro, 2018)</cell><cell>Jenni and Favaro (2018)</cell><cell>Custom</cell><cell>80.10</cell></row><row><cell>ADC (Haeusser et al., 2018)</cell><cell>Ji et al. (2019)</cell><cell>ResNet-34</cell><cell>56.70</cell></row><row><cell>DeepCluster (Caron et al., 2018)</cell><cell>Ji et al. (2019)</cell><cell>ResNet-34</cell><cell>73.40</cell></row><row><cell>Deep InfoMax (Hjelm et al., 2019)</cell><cell>Ji et al. (2019)</cell><cell>AlexNet</cell><cell>77.00</cell></row><row><cell>Invariant Info Clustering (Ji et al., 2019)</cell><cell>Ji et al. (2019)</cell><cell>ResNet-34</cell><cell>88.80</cell></row><row><cell>Supervised (affine + cutout)</cell><cell>Local</cell><cell>ResNet-34</cell><cell>72.04 (69.82 ± 3.36)</cell></row><row><cell>DeepCluster (Caron et al., 2018)</cell><cell>Local</cell><cell>ResNet-34</cell><cell>74.00 (73.37 ± 0.55)</cell></row><row><cell>RotationNet (Gidaris et al., 2018)</cell><cell>Local</cell><cell>ResNet-34</cell><cell>83.77 (83.29 ± 0.44)</cell></row><row><cell>Deep InfoMax (Hjelm et al., 2019)</cell><cell>Local</cell><cell>ResNet-34</cell><cell>76.45 (76.03 ± 0.37)</cell></row><row><cell>SimCLR (Chen et al., 2020)</cell><cell>Local</cell><cell>ResNet-34</cell><cell>89.44 (89.31 ± 0.14)</cell></row><row><cell>Relational Reasoning (ours)</cell><cell>Local</cell><cell>ResNet-34</cell><cell>90.04 (89.67 ± 0.33)</cell></row><row><cell cols="2">B.5 Performance with different backbones</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Comparison on different backbones: linear evaluation. Comparison between four backbones of different depth for baselines and the three best performing methods. Training with self-supervision on unlabeled CIFAR-10 and CIFAR-100, and linear evaluation on labeled version of the same datasets. Mean accuracy (percentage) and standard deviation over three runs. Best results highlighted in bold.<ref type="bibr" target="#b16">Gidaris et al., 2018)</ref> 56.73±1.71 27.45±0.80 62.73±0.94 32.09±0.87 62.00±0.79 29.02±0.18 61.66±1.11 28.24±0.23 SimCLR (Chen et al., 2020) 60.43±0.26 30.45±0.41 69.85±0.58 36.23±0.15 77.02±0.64 42.13±0.35 78.75±0.24 44.33±0.48 Relational Reasoning (ours) 61.03±0.23 33.38±1.02 67.97±0.58 38.18±0.63 74.99±0.07 46.17±0.17 77.51±0.00 47.90±0.27</figDesc><table><row><cell></cell><cell>Conv-4</cell><cell>ResNet-8</cell><cell cols="2">ResNet-32</cell><cell>ResNet-56</cell></row><row><cell>Method</cell><cell cols="5">CIFAR-10 CIFAR-100 CIFAR-10 CIFAR-100 CIFAR-10 CIFAR-100 CIFAR-10 CIFAR-100</cell></row><row><cell>Supervised (upper bound)</cell><cell cols="5">80.46±0.39 49.29±0.85 87.08±0.17 59.41±1.15 90.87±0.41 65.32±0.22 91.40±0.30 67.54±0.32</cell></row><row><cell>Random Weights (lower bound)</cell><cell cols="3">32.92±1.89 10.79±0.59 35.94±1.39 13.08±0.91 27.47±0.83</cell><cell>7.65±0.44</cell><cell>13.53±3.66</cell><cell>1.88±0.14</cell></row><row><cell>RotationNet (</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Comparison on different backbones: grain. Comparison between four backbones of different depth for baselines and the three best performing methods. Training with self-supervision on unlabeled CIFAR-100 and linear evaluation on labeled version of the same datasets with 100 labels (fine) or 20 super-labels (coarse). Mean accuracy (percentage) and standard deviation over three runs. Best results highlighted in bold.<ref type="bibr" target="#b16">Gidaris et al., 2018)</ref> 27.45±0.80 35.49±0.17 32.09±0.87 41.21±0.94 29.02±0.18 40.45±0.39 28.24±0.23 39.16±0.35 SimCLR (Chen et al., 2020) 30.45±0.41 37.72±0.14 36.23±0.15 43.78±0.92 42.13±0.35 51.87±0.48 44.33±0.48 54.09±0.15 Relational Reasoning (ours) 33.38±1.02 40.86±1.03 38.18±0.63 45.36±0.55 46.17±0.17 52.44±0.47 47.90±0.27 54.90±0.07</figDesc><table><row><cell></cell><cell></cell><cell>Conv-4</cell><cell></cell><cell>ResNet-8</cell><cell cols="2">ResNet-32</cell><cell cols="2">ResNet-56</cell></row><row><cell>Method</cell><cell>Fine</cell><cell>Coarse</cell><cell>Fine</cell><cell>Coarse</cell><cell>Fine</cell><cell>Coarse</cell><cell>Fine</cell><cell>Coarse</cell></row><row><cell>Supervised (upper bound)</cell><cell cols="8">49.29±0.85 59.91±0.62 59.41±1.15 70.12±0.33 65.32±0.22 76.35±0.57 67.54±0.32 77.60±0.43</cell></row><row><cell>Random Weights (lower bound)</cell><cell cols="7">10.79±0.59 19.94±0.31 13.08±0.91 23.12±0.90 7.65±0.44 16.56±0.48 1.88±0.14</cell><cell>6.88±0.35</cell></row><row><cell>RotationNet (</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9</head><label>9</label><figDesc></figDesc><table><row><cell cols="9">: Comparison on different backbones: domain transfer. Comparison between four backbones</cell></row><row><cell cols="9">of different depth for baselines and the three best performing methods. Training with self-supervision</cell></row><row><cell cols="9">on unlabeled CIFAR-10 linear evaluation on CIFAR-100 (10 → 100), and viceversa (100 → 10).</cell></row><row><cell cols="9">Mean accuracy (percentage) and standard deviation over three runs. Best results highlighted in bold.</cell></row><row><cell></cell><cell cols="2">Conv-4</cell><cell cols="2">ResNet-8</cell><cell cols="2">ResNet-32</cell><cell cols="2">ResNet-56</cell></row><row><cell>Method</cell><cell>10→100</cell><cell>100→10</cell><cell>10→100</cell><cell>100→10</cell><cell>10→100</cell><cell>100→10</cell><cell>10→100</cell><cell>100→10</cell></row><row><cell>Supervised (upper bound)</cell><cell cols="8">32.06±0.63 64.00±1.07 36.83±0.36 71.20±0.18 33.98±0.70 71.01±0.44 33.92±0.50 71.97±0.17</cell></row><row><cell>Random Weights (lower bound)</cell><cell cols="8">10.79±0.59 32.92±1.89 13.08±0.91 35.94±1.39 7.65±0.44 27.47±0.83 1.88±0.14 13.53±3.66</cell></row><row><cell cols="9">RotationNet (Gidaris et al., 2018) 26.06±0.09 51.86±0.36 31.60±0.54 56.85±0.13 27.02±0.20 52.22±0.70 27.25±0.62 51.82±0.58</cell></row><row><cell>SimCLR (Chen et al., 2020)</cell><cell cols="8">29.20±0.08 54.73±0.60 34.46±0.78 61.34±0.24 36.21±0.16 65.59±0.76 36.79±0.45 66.19±0.80</cell></row><row><cell>Relational Reasoning (ours)</cell><cell cols="8">31.84±0.23 57.30±0.26 36.07±0.35 63.24±0.52 41.50±0.35 67.81±0.42 42.19±0.28 68.66±0.21</cell></row><row><cell cols="2">B.6 Number of augmentations</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 :</head><label>10</label><figDesc>Accuracy with respect to the number of augmentations K . Methods have been trained on CIFAR-10 with a Conv-4 backbone for 100 epochs. The input mini-batch has been augmented K times then given as input. Results are the average accuracy (linear evaluation) of three runs on the validation set. Only the relational reasoning accuracy is positively correlated with K.<ref type="bibr" target="#b16">Gidaris et al., 2018)</ref> 51.58±0.49 51.51±1.02 52.62±0.68 52.85±1.24 52.25±1.06</figDesc><table><row><cell>Method</cell><cell>K = 2</cell><cell>K = 4</cell><cell>K = 8</cell><cell>K = 16</cell><cell>K = 32</cell></row><row><cell>Supervised</cell><cell cols="5">79.61±0.47 79.76±0.54 79.96±0.71 79.56±0.49 80.00±0.45</cell></row><row><cell>RotationNet (Relational Reasoning (ours)</cell><cell cols="5">55.31±0.58 58.05±0.67 59.24±0.51 60.26±0.59 60.33±0.36</cell></row><row><cell cols="2">B.7 Semi-supervised and supervised</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 11 :</head><label>11</label><figDesc>Test accuracy on CIFAR-10 with respect to the percentage of labeled data available. Methods have been trained with a ResNet-32 backbone (200 epochs), followed by linear evaluation on the entire labeled dataset (100 epochs). The quality of the representations improves with the number of labeled data available.</figDesc><table><row><cell>Method</cell><cell>0%</cell><cell>1%</cell><cell>10%</cell><cell>25%</cell><cell>50%</cell><cell>100%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 12 :</head><label>12</label><figDesc>Test accuracy on CIFAR-100 with respect to the percentage of labeled data available. Methods have been trained with a ResNet-32 backbone (200 epochs), followed by linear evaluation on the entire labeled dataset (100 epochs). The quality of the representations improves with the number of labeled data available.</figDesc><table><row><cell>Method</cell><cell>0%</cell><cell>1%</cell><cell>10%</cell><cell>25%</cell><cell>50%</cell><cell>100%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 14</head><label>14</label><figDesc>Relation module corresponds to the proposed method where encodings are aggregated (concatenation) and passed through an MLP for binary classification. All the other factors are kept constant for a fair comparison (e.g. augmentation strategy, mini-batch size). Best results in bold.</figDesc><table><row><cell cols="5">: Ablation of the relation head. The models have been trained on unlabeled CIFAR-10 and</cell></row><row><cell cols="5">CIFAR-100 and tested on various benchmarks with a ResNet32 backbone for 200 epochs (mean</cell></row><row><cell cols="5">accuracy and standard deviation of 3 runs). We consider three head types: (a) dot product between the</cell></row><row><cell cols="5">pairs encoded through the backbone, followed by BCE loss; (b) Encoder + dot product, aggregation</cell></row><row><cell cols="5">is not performed, for each encoded representation an MLP performs a second encoding, then dot</cell></row><row><cell cols="5">product is applied between pairs and the BCE loss minimized (similar to SimCLR, Chen et al.</cell></row><row><cell cols="2">2020); (c) Linear Evaluation</cell><cell cols="2">Domain Transfer</cell><cell>Grain</cell></row><row><cell>Head type</cell><cell>CIFAR-10 CIFAR-100</cell><cell>10→100</cell><cell>100→10</cell><cell>CIFAR-100-20</cell></row><row><cell>(a) dot product</cell><cell cols="3">72.74±0.22 28.77±0.44 18.19±0.10 51.9±0.50</cell><cell>45.05±1.07</cell></row><row><cell cols="4">(b) Encoder + dot product 59.44±0.59 29.91±1.28 28.29±0.90 53.65±0.85</cell><cell>36.94±1.30</cell></row><row><cell cols="4">(c) Relation module (ours) 74.99±0.07 46.17±0.17 41.50±0.35 67.81±0.42</cell><cell>52.44±0.47</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head></head><label></label><figDesc>Data loader img_list = list () if self . transform is not None : for _ in range ( self . K ) : img_transformed = self . transform ( pic . copy () ) ( torch . ones ( size , dtype = torch . float32 ) ) targets_list . append ( torch . zeros ( size , dtype = torch . float32 ) ) shifts_counter +=1 if ( shifts_counter &gt;= size ) : shifts_counter =1 # avoid identity pairs relation_pairs = torch . cat ( relation_pairs_list , 0) targets = torch . cat ( targets_list , 0) return relation_pairs , targets def train ( self , tot_epochs , train_loader ) : optimizer = torch . optim . Adam ([ { ' params ': self . backbone . parameters () } , { ' params ': self . relation_head . parameters () }]) BCE = torch . nn . True ) model . train ( tot_epochs =200 , train_loader = train_loader ) torch . save ( model . backbone . state_dict () , ' ./ backbone . tar ')</figDesc><table><row><cell>14: 15: 16: 17: 18: 19: 20: 21: 22: end function end for end for y = r φ (P) L = BCE(y, t) θ ← θ − α∇ θ L φ ← φ − β∇ φ L end while return θ, φ 23: function SHUFFLE(Z) 24:Z = Z 25: for m = 1 to M do 26:m ∼ {1, . . . , M } \ {m} 27:Z m ← Zm 28: end for 29: returnZ 30: end function D Essential PyTorch code of the method Assigning a random representation with indexm Forward pass in the relation module Estimating the Binary Cross-Entropy loss Updating backbone Updating relation module Returning the learned weights Copying the input set Sampling an indexm = m Returning the shuffled set else : img_list = img D.1 img_list . append ( img_transformed ) return img_list , target D.2 Augmentations import torchvision . transforms as transforms normalize = transforms . Normalize ( mean =[0.491 , 0.482 , 0.447] , std =[0.247 , 0.243 , 0.262]) # CIFAR10 color_jitter = transforms . ColorJitter ( brightness =0.8 , contrast =0.8 , saturation =0.8 , hue =0.2) rnd_color_jitter = transforms . RandomApply ([ color_jitter ] , p =0.8) rnd_gray = transforms . RandomGrayscale ( p =0.2) rnd_rcrop = transforms . RandomResizedCrop ( size =32 , scale =(0.08 , 1.0) , interpolation =2) rnd_hflip = transforms . RandomHorizontalFlip ( p =0.5) train_transform = transforms . Compose ([ rnd_rcrop , rnd_hflip , rnd_color_jitter , rnd_gray , transforms . ToTensor () , normalize ]) D.3 Self-supervised relational reasoning import torch class RelationalReasoning ( torch . nn . Module ) : def __init__ ( self , backbone ) : super ( RelationalReasoning , self ) . __init__ () feature_size = 64*2 # multiply by 2 since aggregation = ' cat ' self . backbone = backbone self . relation_head = torch . nn . Sequential ( nn . Linear ( feature_size , 256) , nn . BatchNorm1d (256) , nn . LeakyReLU () , nn . Linear (256 , 1) ) def aggregate ( self , features , K ) : relation_pairs_list = list () targets_list = list () size = int ( features . shape [0] / K ) shifts_counter =1 for index_1 in range (0 , size *K , size ) : for index_2 in range ( index_1 + size , size *K , size ) : # Using the ' cat ' aggregation function by default pos_pair = torch . cat ([ features [ index_1 : index_1 + size ] , features [ index_2 : index_2 + size ]] , 1) # Shuffle by rolling the mini -batch ( negatives ) neg_pair = torch . cat ([ features [ index_1 : index_1 + size ] , torch . roll ( features [ index_2 : index_2 + size ] , shifts = shifts_counter , dims =0) ] , 1) relation_pairs_list . append ( pos_pair ) relation_pairs_list . append ( neg_pair ) self . backbone . train () self . relation_head . train () for epoch in range ( tot_epochs ) : # the real target is discarded ( unsupervised ) for i , ( data_augmented , _ ) in enumerate ( train_loader ) : K = len ( data_augmented ) # tot augmentations x = torch . cat ( data_augmented , 0) optimizer . zero_grad () # forward pass ( backbone ) features = self . backbone ( x ) # aggregation function relation_pairs , targets = self . aggregate ( features , K ) # forward pass ( relation head ) score = self . relation_head ( relation_pairs ) . squeeze () # cross -entropy loss and backward loss = BCE ( score , targets ) loss . backward () optimizer . step () # estimate the accuracy predicted = torch . round ( torch . sigmoid ( score ) ) correct = predicted . eq ( targets . view_as ( predicted ) ) . sum () accuracy = (100.0 * correct / float ( len ( targets ) ) ) if ( i %100==0) : print ( ' Epoch [{}][{}/{}] loss : {:.5 f }; accuracy : {:.2 f }% ' \ . format ( epoch +1 , i +1 , len ( train_loader ) +1 , loss . item () , accuracy . item () ) ) D.4 Main def main () : backbone = Conv4 () # it should be a CNN with 64 linear output units model = RelationalReasoning ( backbone ) train_set = MultiCIFAR10 ( K =4 , # it should be K =32 for CIFAR10 /100 root = ' data ' , train = True , transform = train_transform , download = True ) train_loader = torch . utils . data . DataLoader ( train_set , batch_size =64 , targets_list . append BCEWithLogitsLoss () shuffle =</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/mpatacchiola/self-supervised-relational-reasoning</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/facebookresearch/deepcluster</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/rdevon/DIM 4 https://github.com/google-research/simclr</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Disclosure of Funding</head><p>MP and AS would like to thank anonymous reviewers for useful comments and suggestions; the BayesWatch team for feedback and discussion, in particular Elliot J. Crowley, Luke Darlow, and Joseph Mellor. MP would like to thank the Becchi team for revising the preliminary version of the manuscript, in particular Valerio Biscione, Riccardo Polvara, and Luca Surace.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Defining benchmarks for continual few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Antoniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Patacchiola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ochal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Storkey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.11967</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Self-labelling via simultaneous clustering and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning representations by maximizing mutual information across views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Buchwalter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Faulkner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
		<title level="m">Relational inductive biases, deep learning, and graph networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Boudiaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">M</forename><surname>Ziko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Granger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Piantanida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">B</forename><surname>Ayed</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.08983</idno>
		<title level="m">Metric learning: cross-entropy vs. pairwise losses</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Learning independent features with adversarial nets for non-linear ica</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.05050</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Signature verification using a &quot;siamese&quot; time delay neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bromley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Säckinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An analysis of single-layer networks in unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Discriminative unsupervised feature learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Relational reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Džeroski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>De Raedt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Driessens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="7" to="52" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Relational categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gentner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">;</forename><surname>Kurtz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wk Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goldstone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Love</surname></persName>
		</author>
		<editor>AB Markman, &amp; PW Wolff</editor>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="151" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Comparison in the development of categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gentner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">L</forename><surname>Namy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive development</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="487" to="513" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Relational discovery in category learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Don</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Krusche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Livesey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">147</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised clustering using pseudo-semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kwatra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sivathanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Noise-contrastive estimation: A new estimation principle for unnormalized statistical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Associative deep clustering: Training a classification network with no labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haeusser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Plapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Aljalbout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05722</idno>
		<title level="m">Momentum contrast for unsupervised visual representation learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Relation networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Self-supervised feature learning by learning to spot artifacts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jenni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Invariant information clustering for unsupervised image classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Self-supervised visual feature learning with deep neural networks: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.11362</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Supervised contrastive learning</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Revisiting self-supervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Introduction to statistical relational learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Džeroski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pfeffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-F</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Heckerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Meek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Building machines that learn and think like people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Ullman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioral and brain sciences</title>
		<imprint>
			<biblScope unit="page">40</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning representations for automatic colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on challenges in representation learning, ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Self-organization in a perceptual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Linsker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="105" to="117" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Self-supervised learning of pretext-invariant representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.01991</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Musgrave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-N</forename><surname>Lim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.08505</idno>
		<title level="m">A metric learning reality check</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Making a silk purse out of two sow&apos;s ears: Young children&apos;s use of comparison in category learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">L</forename><surname>Namy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gentner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">131</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Representation learning by learning to count</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Realistic evaluation of deep semi-supervised learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Scaling the scattering transform: Deep hybrid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Oyallon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Belilovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.05068</idno>
		<title level="m">Discovering objects and their relations from entangled scene representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Metric learning with adaptive density discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Rippel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Relational recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Faulkner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chrzanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A simple neural network module for relational reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Evolutionary principles in self-referential learning, or on learning how to learn: The meta-meta-... hook. Diplomarbeit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
		</imprint>
		<respStmt>
			<orgName>Technische Universität München</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Making the world differentiable: On using self-supervised fully recurrent neural networks for dynamic reinforcement learning and planning in non-stationary environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.07685</idno>
		<title level="m">Fixmatch: Simplifying semi-supervised learning with consistency and confidence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05849</idno>
		<title level="m">Contrastive multiview coding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">On mutual information maximization for representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Djolonga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Rubenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Distance metric learning for large margin nearest neighbor classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning with relational inductive biases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tuyls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Reichert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lockhart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Split-brain autoencoders: Unsupervised learning by cross-channel prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Similar class can be defined for other datasets ( e . g . CIFAR100 ) . &quot;&quot;&quot; def __init__ ( self , K , ** kwds ) : super () . __init__ (** kwds ) self . K = K # tot number of augmentations def __getitem__ ( self , index ) : img , target = self</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yamins</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>import torchvision from PIL import Image class MultiCIFAR10 ( torchvision . datasets . CIFAR10 ) : &quot;&quot;&quot; Override torchvision CIFAR10 for multi -image management. data [ index ] , self . targets [ index ] pic = Image . fromarray ( img</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
