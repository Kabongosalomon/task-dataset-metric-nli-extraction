<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Double Graph Based Reasoning for Document-level Relation Extraction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Zeng</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Key Laboratory of Computational Linguistics</orgName>
								<orgName type="laboratory" key="lab2">MOE</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Software and Microelectronics</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runxin</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Key Laboratory of Computational Linguistics</orgName>
								<orgName type="laboratory" key="lab2">MOE</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Key Laboratory of Computational Linguistics</orgName>
								<orgName type="laboratory" key="lab2">MOE</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Software and Microelectronics</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">ByteDance AI Lab</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Double Graph Based Reasoning for Document-level Relation Extraction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Document-level relation extraction aims to extract relations among entities within a document. Different from sentence-level relation extraction, it requires reasoning over multiple sentences across a document. In this paper, we propose Graph Aggregation-and-Inference Network (GAIN) featuring double graphs. GAIN first constructs a heterogeneous mention-level graph (hMG) to model complex interaction among different mentions across the document. It also constructs an entitylevel graph (EG), based on which we propose a novel path reasoning mechanism to infer relations between entities. Experiments on the public dataset, DocRED, show GAIN achieves a significant performance improvement (2.85 on F1) over the previous state-of-the-art. Our code is available at https://github.com/ DreamInvoker/GAIN. * Equal contribution. † Corresponding author.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Elias Brown</head><p>[1] Elias Brown (May 9, 1793-July 7, 1857 was a U.S. Representative from Maryland. [2] Born near Baltimore, Maryland, Brown attended the common schools. … [7] He died near Baltimore, Maryland, and is interred in a private cemetery near Eldersburg, Maryland.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The task of identifying semantic relations between entities from text, namely relation extraction (RE), plays a crucial role in a variety of knowledge-based applications, such as question answering <ref type="bibr" target="#b18">(Yu et al., 2017)</ref> and large-scale knowledge graph construction. Previous methods <ref type="bibr" target="#b20">(Zeng et al., 2014;</ref><ref type="bibr" target="#b19">Zeng et al., 2015;</ref><ref type="bibr" target="#b15">Xiao and Liu, 2016;</ref><ref type="bibr">Zhang et al., 2017;</ref><ref type="bibr" target="#b7">Zhang et al., 2018;</ref><ref type="bibr">Baldini Soares et al., 2019)</ref> focus on sentence-level RE, which predicts relations among entities in a single sentence. However, sentence-level RE models suffer from an inevitable limitation -they fail to recognize relations between entities across sentences. Hence, extracting relations at the document-level is necessary for a holistic understanding of knowledge in text.</p><p>There are several major challenges in effective relation extraction at the document-level. Firstly, the subject and object entities involved in a relation <ref type="figure">Figure 1</ref>: An example document and its desired relations from DocRED <ref type="bibr" target="#b16">(Yao et al., 2019)</ref>. Entity mentions and relations involved in these relation instances are colored. Other mentions are underlined for clarity. may appear in different sentences. Therefore a relation cannot be identified based solely on a single sentence. Secondly, the same entity may be mentioned multiple times in different sentences. Crosssentence context information has to be aggregated to represent the entity better. Thirdly, the identification of many relations requires techniques of logical reasoning. This means these relations can only be successfully extracted when other entities and relations, usually spread across sentences, are identified implicitly or explicitly. As <ref type="figure">Figure 1</ref> shows, it is easy to recognize the intra-sentence relations (Maryland, country, U.S.), (Baltimore, located in the administrative territorial entity, Maryland), and (Eldersburg, located in the administrative territorial entity, Maryland), since the subject and object appear in the same sentence. However, it is non-trivial to predict the inter-sentence relations between Baltimore and U.S., as well as Eldersburg and U.S., whose mentions do not appear in the same sentence and have long-distance dependencies. Besides, the identification of these two relation instances also   <ref type="bibr" target="#b16">(Yao et al., 2019)</ref>, with 1150 bad cases in total.</p><p>requires logical reasoning. For example, Eldersburg belongs to U.S. because Eldersburg is located in Maryland, which belongs to U.S.. Recently, <ref type="bibr" target="#b16">Yao et al. (2019)</ref> proposed a largescale human-annotated document-level RE dataset, DocRED, to push sentence-level RE forward to document-level and it contains massive relation facts. <ref type="figure">Figure 1</ref> shows an example from DocRED. We randomly sample 100 documents from the Do-cRED dev set and manually analyze the bad cases predicted by a BiLSTM-based model proposed by <ref type="bibr" target="#b16">Yao et al. (2019)</ref>. As shown in <ref type="table" target="#tab_1">Table 1</ref>, the error type of inter-sentence and that of logical reasoning take up a large proportion of all bad cases, with 53.5% and 21.0% respectively. Therefore, in this paper, we aim to tackle these problems to extract relations from documents better.</p><p>Previous work in document-level RE do not consider reasoning <ref type="bibr">(Gupta et al., 2019;</ref><ref type="bibr">Jia et al., 2019;</ref><ref type="bibr" target="#b16">Yao et al., 2019)</ref>, or only use graph-based or hierarchical neural network to conduct reasoning in an implicit way <ref type="bibr" target="#b2">(Peng et al., 2017;</ref><ref type="bibr" target="#b5">Sahu et al., 2019;</ref><ref type="bibr" target="#b0">Nan et al., 2020)</ref>. In this paper, we propose a Graph Aggregation-and-Inference Network (GAIN) for document-level relation extraction. It is designed to tackle the challenges mentioned above directly. GAIN constructs a heterogeneous Mention-level Graph (hMG) with two types of nodes, namely mention node and document node, and three different types of edges, i.e., intra-entity edge, inter-entity edge and document edge, to capture the context information of entities in the document. Then, we apply Graph Convolutional Network (Kipf and Welling, 2017) on hMG to get a document-aware representation for each mention. Entity-level Graph (EG) is then constructed by merging mentions that refer to the same entity in hMG, on top of which we propose a novel path reasoning mechanism. This reasoning mechanism allows our model to infer multi-hop relations between entities.</p><p>In summary, our main contributions are as fol-lows:</p><p>• We propose a novel method, Graph Aggregation-and-Inference Network (GAIN), which features a double graph design, to better cope with document-level RE task.</p><p>• We introduce a heterogeneous Mention-level Graph (hMG) with a graph-based neural network to model the interaction among different mentions across the document and offer document-aware mention representations.</p><p>• We introduce an Entity-level Graph (EG) and propose a novel path reasoning mechanism for relational reasoning among entities.</p><p>We evaluate GAIN on the public DocRED dataset. It significantly outperforms the previous state-of-the-art model by 2.85 F1 score. Further analysis demonstrates the capability of GAIN to aggregate document-aware context information and to infer logical relations over documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task Formulation</head><p>We formulate the document-level relation extraction task as follows. Given a document comprised of N sentences D = {s i } N i=1 and a va-</p><formula xml:id="formula_0">riety of entities E = {e i } P i=1 , where s i = {w j } M j=1</formula><p>refers to the i-th sentence consisting of M words, e i = {m j } Q j=1 and m j refers to a span of words belonging to the j-th mention of the i-th entity, the task aims to extract the relations between different entities in E, namely {(e i , r ij , e j )|e i , e j ∈ E, r ij ∈ R}, where R is a pre-defined relation type set.</p><p>In our paper, a relation r ij between entity e i and e j is defined as inter-sentential, if and only if S e i ∩ S e j = ∅, where S e i denotes those sentences containing mentions of e i . Instead, a relation r ij is defined as intra-sentential, if and only if S e i ∩S e j = ∅. We also define K-hop relational reasoning as predicting relation r ij based on a K-length chain of existing relations, with e i and e j being the head and tail of the reasoning chain, i.e., e i   <ref type="figure">Figure 2</ref>: The overall architecture of GAIN. First, A context encoder consumes the input document to get a contextualized representation of each word. Then, the Mention-level Graph is constructed with mention nodes and a document node. After applying GCN, the graph is transformed into Entity-level Graph, where the paths between entities are identified for reasoning. Finally, the classification module predicts target relations based on the above information. Different entities are in different colors. The number i in the mention node denotes that it belongs to the i-th sentence. module (Sec. 3.3), classification module (Sec. 3.4), as is shown in <ref type="figure">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Encoding Module</head><p>In the encoding module, we convert a document D = {w i } n i=1 containing n words into a sequence of vectors {g i } n i=1 . Following <ref type="bibr" target="#b16">Yao et al. (2019)</ref>, for each word w i in D, we first concatenate its word embedding with entity type embedding and coreference embedding:</p><formula xml:id="formula_1">x i = [E w (w i ); E t (t i ); E c (c i )]<label>(1)</label></formula><p>where E w (·) , E t (·) and E c (·) denote the word embedding layer, entity type embedding layer and coreference embedding layer, respectively. t i and c i are named entity type and entity id. We introduce None entity type and id for those words not belonging to any entity. Then the vectorized word representations are fed into an encoder to obtain the context sensitive representation for each word:</p><p>[g 1 , g 2 , . . . , g n ] = Encoder([x 1 , x 2 , . . . , x n ])</p><p>(2) where the Encoder can be LSTM or other models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Mention-level Graph Aggregation Module</head><p>To model the document-level information and interactions between mentions and entities, a heterogeneous Mention-level Graph (hMG) is constructed. hMG has two different kinds of nodes: mention node and document node. Each mention node denotes one particular mention of an entity. And hMG also has one document node that aims to model the overall document information. We argue that this node could serve as a pivot to interact with different mentions and thus reduce the long distance among them in the document.</p><p>There are three types of edges in hMG:</p><p>• Intra-Entity Edge: Mentions referring to the same entity are fully connected with intraentity edges. In this way, the interaction among different mentions of the same entity could be modeled.</p><p>• Inter-Entity Edge: Two mentions of different entities are connected with an inter-entity edge if they co-occur in a single sentence. In this way, interactions among entities could be modeled by co-occurrences of their mentions.</p><p>• Document Edge: All mentions are connected to the document node with the document edge.</p><p>With such connections, the document node can attend to all the mentions and enable interactions between document and mentions. Besides, the distance between two mention nodes is at most two with the document node as a pivot. Therefore long-distance dependency can be better modeled.</p><p>Next, we apply Graph Convolution Network (Kipf and Welling, 2017) on hMG to aggregate the features from neighbors. Given node u at the l-th layer, the graph convolutional operation can be defined as:</p><formula xml:id="formula_2">h (l+1) u = σ   k∈K v∈N k (u) W (l) k h (l) v + b (l) k   (3) where K are different types of edges, W (l) k ∈ R d×d and b (l) k ∈ R d are trainable parameters. N k (u)</formula><p>denotes neighbors for node u connected in k-th type edge. σ is an activation function (e.g., ReLU).</p><p>Different layers of GCN express features of different abstract levels, and therefore in order to cover features of all levels, we concatenate hidden states of each layer to form the final representation of node u:</p><formula xml:id="formula_3">m u = [h (0) u ; h (1) u ; . . . ; h (N ) u ]<label>(4)</label></formula><p>where h</p><p>u is the initial representation of node u. For a mention ranging from the s-th word to the t-th word in the document, h</p><formula xml:id="formula_5">(0) u = 1 t−s+1</formula><p>t j=s g j and for document node, it is initialized with the document representation output from the encoding module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Entity-level Graph Inference Module</head><p>In this subsection, we introduce Entity-level Graph (EG) and path reasoning mechanism. First, mentions that refer to the same entity are merged to entity node so as to get the nodes in EG. Note that we do not consider document node in EG. For i-th entity node e i mentioned N times, it is represented by the average of its N mention representations:</p><formula xml:id="formula_6">e i = 1 N n m n<label>(5)</label></formula><p>Then, we merge all inter-entity edges that connect mentions of the same two entities so as to get the edges in EG. The representation of directed edge from e i to e j in the EG is defined as :</p><formula xml:id="formula_7">e ij = σ (W q [e i ; e j ] + b q )<label>(6)</label></formula><p>where W q and b q are trainable parameters, and σ is an activation function (e.g., ReLU). Based on the vectorized edge representation, the i-th path between head entity e h and tail entity e t passing through entity e o is represented as:</p><formula xml:id="formula_8">p i h,t = [e ho ; e ot ; e to ; e oh ]<label>(7)</label></formula><p>Note that we only consider two-hop paths here, while it can easily extend to multi-hop paths. We also introduce attention mechanism <ref type="bibr">(Bahdanau et al., 2015)</ref>, using the entity pair (e h , e t ) as query, to fuse the information of different paths between e h and e t .</p><formula xml:id="formula_9">s i = σ([e h ; e t ] · W l · p i h,t ) (8) α i = e s i j e s j (9) p h,t = i α i p i h,t<label>(10)</label></formula><p>where α i is the normalized attention weight for i-th path. Consequently, the model will pay more attention to useful paths. σ is an activation function. With this module, an entity can be represented by fusing information from its mentions, which usually spread in multiple sentences. Moreover, potential reasoning clues are modeled by different paths between entities. Then they can be integrated with the attention mechanism so that we will take into account latent logical reasoning chains to predict relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Classification Module</head><p>For each entity pair (e h , e t ), we concatenate the following representations: (1) the head and tail entity representation e h and e t derived in the Entity-level Graph, with the comparing operation <ref type="bibr">(Mou et al., 2016)</ref> to strengthen features, i.e., absolute value of subtraction between the representation of two entities, |e h − e t |, and element-wise multiplication, e h e t ; (2) the representation of document node in Mention-level Graph, m doc , as it can help aggregate cross-sentence information and provide document-aware representation; (3) the comprehensive inferential path information p h,t .</p><p>I h,t = [e h ; e t ; |e h − e t |; e h e t ; m doc ; p h,t ] (11) Finally, we formulate the task as multi-label classification task and predict relations between entities:</p><formula xml:id="formula_10">P (r|e h , e t ) = sigmoid (W b σ(W a I h,t + b a ) + b b )<label>(12)</label></formula><p>where W a , W b , b a , b b are trainable parameters, σ is an activation function (e.g., ReLU). We use binary cross entropy as the classification loss to train our model in an end-to-end way:</p><formula xml:id="formula_11">L = − D∈S h =t r i ∈R I (r i = 1) log P (r i |e h , e t ) + I (r i = 0) log (1 − P (r i |e h , e t ))<label>(13)</label></formula><p>where S denotes the whole corpus, and I (·) refers to indication function.  <ref type="bibr" target="#b16">(Yao et al., 2019)</ref>. we follow the standard split of the dataset, 3, 053 documents for training, 1, 000 for development and 1, 000 for test. For more detailed statistics about DocRED, we recommend readers to refer to the original paper <ref type="bibr" target="#b16">(Yao et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Settings</head><p>In our GAIN implementation, we use 2 layers of GCN and set the dropout rate to 0.6, learning rate to 0.001. We train GAIN using AdamW (Loshchilov and Hutter, 2019) as optimizer with weight decay 0.0001 and implement GAIN under PyTorch <ref type="bibr" target="#b1">(Paszke et al., 2017)</ref> and DGL <ref type="bibr" target="#b14">(Wang et al., 2019b)</ref>. We implement three settings for our GAIN. GAIN-GloVe uses GloVe (100d) and BiLSTM (256d) as word embedding and encoder. GAIN-BERT base and GAIN-BERT large use BERT base and BERT large as encoder respectively and the learning rate is set to 1e −5 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baselines and Evaluation Metrics</head><p>We use the following models as baselines. <ref type="bibr" target="#b16">Yao et al. (2019)</ref> proposed models to encode the document into a sequence of hidden state vector {h i } n i=1 using CNN <ref type="bibr">(Fukushima, 1980)</ref>, LSTM (Hochreiter and Schmidhuber, 1997), and BiL-STM <ref type="bibr" target="#b6">(Schuster and Paliwal, 1997)</ref> as their encoder, and predict relations between entities with their representations. Other pre-trained models like <ref type="bibr">BERT (Devlin et al., 2019)</ref>, <ref type="bibr">RoBERTa (Liu et al., 2019)</ref>, and CorefBERT <ref type="bibr" target="#b17">(Ye et al., 2020)</ref> are also used as encoder <ref type="bibr" target="#b12">(Wang et al., 2019a;</ref><ref type="bibr" target="#b17">Ye et al., 2020)</ref> to document-level RE task.</p><p>Context-Aware, also proposed by <ref type="bibr" target="#b16">Yao et al. (2019)</ref> on DocRED adapted from <ref type="bibr" target="#b8">(Sorokin and Gurevych, 2017)</ref>, uses an LSTM to encode the text, but further utilizes attention mechanism to absorb the context relational information for predicting.</p><p>BERT-Two-Step base , proposed by <ref type="bibr" target="#b12">Wang et al. (2019a)</ref> on DocRED. Though similar to BERT-RE base , it first predicts whether two entities have a relationship and then predicts the specific target relation.</p><p>HIN-GloVe/HIN-BERT base , proposed by <ref type="bibr" target="#b9">Tang et al. (2020)</ref>. Hierarchical Inference Network (HIN) aggregate information from entity-level, sentence-level, and document-level to predict target relations, and use GloVe <ref type="bibr" target="#b3">(Pennington et al., 2014)</ref> or BERT base for word embedding.</p><p>LSR-GloVe/LSR-BERT base , proposed by Nan et al. <ref type="formula" target="#formula_4">(2020)</ref>   <ref type="bibr">, 2019)</ref>. We also include their results.</p><p>Following <ref type="bibr" target="#b16">Yao et al. (2019)</ref>, we use the widely used metrics F1 and AUC in our experiment. We also use Ign F1 and Ign AUC, which calculate F1 and AUC excluding the common relation facts in the training and dev/test sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results</head><p>We show GAIN's performance on the DocRED dataset in <ref type="table" target="#tab_5">Table 2</ref>, in comparison with other baselines.</p><p>Among the models not using BERT or BERT variants, GAIN-GloVe consistently outperforms all sequential-based and graph-based strong baselines by 0.9 ∼ 12.82 F1 score on the test set. Among the models using BERT or BERT variants, GAIN-BERT base yields a great improvement of F1/Ign F1 on dev and test set by 2.22/6.71 and 2.19/2.03, respectively, in comparison with the strong baseline LSR-BERT base . GAIN-BERT large also improves 2.85/2.63 F1/Ign F1 on test set compared with   <ref type="bibr" target="#b0">(Nan et al., 2020)</ref>. Results with † are based on our implementation.</p><p>previous state-of-the-art method, CorefRoBERTa-RE large . It suggests that GAIN is more effective in document-level RE tasks. We can also observe that LSR-BERT base improves F1 by 3.83 and 4.87 on dev and test set with GloVe embedding replaced with BERT base . In comparison, our GAIN-BERT base yields an improvement by 5.93 and 6.16, which indicates GAIN can better utilize BERT representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Study</head><p>To further analyze GAIN, we also conduct ablation studies to illustrate the effectiveness of different modules and mechanisms in GAIN. We show the results of the ablation study in <ref type="table" target="#tab_7">Table 3</ref>. First, we remove the heterogeneous Mentionlevel Graph (hMG) of GAIN. In detail, we initialize an entity node in Entity-level Graph (EG) with Eq. 5 but replace m n with h (0) n , and apply GCN to EG instead. Features in different layers of GCN are concatenated to obtain e i . Without hMG, the performance of GAIN-GloVe/GAIN-BERT base sharply drops by 2.08/2.02 Ign F1 score on dev set. This drop shows that hMG plays a vital role in capturing interactions among mentions belonging to the same and different entities and document-aware features.</p><p>Next, we remove the inference module. To be specific, the model abandon the path information between head and tail entity p h,t obtained in Entitylevel Graph, and predict relations only based on entity representation, e h and e t , and document node representation, m doc . The inference module's removal results in poor performance across all metrics, for instance, 2.21/2.17 Ign F1 score decrease on the dev set for GAIN-GloVe/GAIN-BERT base . It suggests that our path inference mechanism helps capture the potential K-hop inference paths to infer relations and, therefore, improve document-level RE performance.</p><p>Moreover, taking away the document node in hMG leads to 2.19/1.88 Ign F1 decrease on the dev set for GAIN-GloVe/GAIN-BERT base . It helps GAIN aggregate the document information and works as a pivot to facilitate the information exchange among different mentions, especially those far away from each other within the document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Analysis &amp; Discussion</head><p>In this subsection, we further analyze both intersentential and inferential performance on the development set. The same as <ref type="bibr" target="#b0">Nan et al. (2020)</ref>, we report Intra-F1/Inter-F1 scores in <ref type="table">Table 4</ref>, which only consider either intra-or inter-sentence relations respectively. Similarly, in order to evaluate the inference ability of the models, Infer-F1 scores are reported in <ref type="table" target="#tab_9">Table 5</ref>, which only considers relations that engaged in the relational reasoning process . For example, we take into account the golden relation facts r 1 , r 2 , and r 3 if there exist   <ref type="table">Table 4</ref>: Intra-and Inter-F1 results on dev set of Do-cRED. Results with * are reported in <ref type="bibr" target="#b0">(Nan et al., 2020</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Infer-F1.</head><p>As <ref type="table">Table 4</ref> shows, GAIN outperforms other baselines not only in Intra-F1 but also Inter-F1, and the removal of hMG leads to a more considerable decrease in Inter-F1 than Intra-F1, which indicates our hMG do help interactions among mentions, especially those distributed in different sentences with long-distance dependency.</p><p>Besides, <ref type="table" target="#tab_9">Table 5</ref> suggests GAIN can better handle relational inference. For example, GAIN-BERT base improves 5.11 Infer-F1 compared with RoBERTa-RE base . The inference module also plays an important role in capturing potential inference chains between entities, without which GAIN-BERT base would drop by 1.78 Infer-F1.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Case Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Previous approaches focus on sentence-level relation extraction <ref type="bibr" target="#b20">(Zeng et al., 2014;</ref><ref type="bibr" target="#b19">Zeng et al., 2015;</ref><ref type="bibr" target="#b13">Wang et al., 2016;</ref><ref type="bibr">Zhou et al., 2016;</ref><ref type="bibr" target="#b15">Xiao and Liu, 2016;</ref><ref type="bibr">Zhang et al., 2017;</ref><ref type="bibr">Feng et al., 2018;</ref><ref type="bibr">Zhu et al., 2019)</ref>. But sentence-level RE models face an inevitable restriction in practice, where many realworld relation facts can only be extracted across sentences. Therefore, many researchers gradually shift their attention into document-level relation extraction. Several approaches <ref type="bibr" target="#b2">Peng et al., 2017;</ref><ref type="bibr">Gupta et al., 2019;</ref><ref type="bibr" target="#b7">Song et al., 2018;</ref><ref type="bibr">Figure 3</ref>: The case study of our proposed GAIN and baseline models. The models take the document as input and predict relations among different entities in different colors. We only show a part of entities within the documents and the according sentences due to the space limitation. <ref type="bibr">Jia et al., 2019)</ref> leverage dependency graph to better capture document-specific features, but they ignore ubiquitous relational inference in document. Recently, many models are proposed to address this problem. <ref type="bibr" target="#b9">Tang et al. (2020)</ref> proposed a hierarchical inference network by considering information from entity-level, sentence-level, and documentlevel. However, it conducts relational inference implicitly based on a hierarchical network while we adopt the path reasoning mechanism, which is a more explicit way. <ref type="bibr" target="#b5">(Christopoulou et al., 2019)</ref> is one of the most powerful systems on document-level RE tasks recently. Compared to <ref type="bibr" target="#b5">(Christopoulou et al., 2019)</ref> and other graph-based approaches to relation extraction, our architecture features many different designs with different motivations behind them. First, the ways of graph construction are different. We create two separate graphs of different levels to capture long-distance document-aware interactions and entity path inference information, respectively. While Christopoulou et al. (2019) put mentions and entities in the same graph. Moreover, they do not conduct graph node representation learning like GCN to aggregate interactive information on the constructed graph, only using the features from BiLSTMs to represent nodes. Second, the processes of path inference are different. Christopoulou et al. (2019) use a walkbased method to iteratively generate a path for every entity pair, which requires the extra overhead of hyper-parameter tuning to control the process of inference. Instead, we use an attention mechanism to selectively fuse all possible path information for the entity pair while without extra overhead.</p><p>When we were writing this paper, <ref type="bibr" target="#b0">(Nan et al., 2020</ref>) make their work public as preprints, which adopt the dependency tree to capture the semantic information in the document. They put mention and entity nodes in the same graph and conduct inference implicitly by using GCN. Unlike their work, our GAIN presents mention node and entity node in different graphs to better conduct inter-sentence information aggregation and infer relations more explicitly.</p><p>Some other attempts <ref type="bibr" target="#b11">(Verga et al., 2018;</ref><ref type="bibr" target="#b5">Sahu et al., 2019;</ref><ref type="bibr" target="#b5">Christopoulou et al., 2019</ref>) study document-level RE in a specific domain like biomedical RE. However, the datasets they use usually contain very limited relation types and entity types. For instance, <ref type="bibr">CDR (Li et al., 2016)</ref> only has one type of relation and two types of entities, which may not be the ideal testbed for relational reasoning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Extracting inter-sentence relations and conducting relational reasoning are challenging in documentlevel relation extraction.</p><p>In this paper, we introduce Graph Aggregationand-Inference Network (GAIN) to better cope with document-level relation extraction, which features double graphs in different granularity. GAIN utilizes a heterogeneous Mention-level Graph to model the interaction among different mentions across the document and capture document-aware features. It also uses an Entity-level Graph with a proposed path reasoning mechanism to infer relations more explicitly.</p><p>Experimental results on the large-scale humanannotated dataset, DocRED, show GAIN outperforms previous methods, especially in intersentence and inferential relations scenarios. The ablation study also confirms the effectiveness of different modules in our model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Hyperparameter settings</head><p>We use development set to manually tune the optimal hyperparameters for GAIN, based on the Ign F1 score. Hyperparameter settings for GAIN-GloVe, GAIN-BERT base and GAIN-BERT large are listed in <ref type="table" target="#tab_12">Table 6</ref>, 7 and 8, respectively. The value of hyperparameters we finally adopted are in bold. Note that we do not tune all the hyperparameters.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>recently. They construct a graph based on the dependency tree and predict relations by latent structure induction and GCN.<ref type="bibr" target="#b0">Nan et al. (2020)</ref> also adapted four graph-based state-of-the-art RE models to DocRED, including GAT<ref type="bibr" target="#b10">(Velickovic et al., 2017)</ref>,GCNN (Sahu et al., 2019), EoG  (Christopoulou et al., 2019, and AGGCN (Guo et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3</head><label>3</label><figDesc>also shows the case study of our proposed model GAIN, in comparison with other baselines. As is shown, BiLSTM can only identify two rela-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>arXiv:2009.13752v1 [cs.CL] 29 Sep 2020</figDesc><table><row><cell>Error Type</cell><cell>Count</cell></row><row><cell>Intra-sentence</cell><cell>535</cell></row><row><cell>Inter-sentence</cell><cell>615</cell></row><row><cell>Logical Reasoning</cell><cell>242</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Statistics of bad cases in randomly sampled 100 documents from DocRED dev set for BiLSTM</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>He died near Baltimore, Maryland, and is interred in a private cemetery near Eldersburg, Maryland.</figDesc><table><row><cell cols="10">[1] Elias Brown … was a U.S. Representative from Maryland. [2] Born near Baltimore, Maryland, …</cell></row><row><cell cols="10">2 [7] Path Info 7 2 Maryland Baltimore U.S</cell></row><row><cell>Encoder</cell><cell></cell><cell></cell><cell>1</cell><cell>7</cell><cell>7</cell><cell>7</cell><cell>GCNs</cell><cell>Maryland</cell><cell>MLP</cell><cell>country</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Eldersburg</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>1</cell><cell>U.S.</cell><cell></cell><cell></cell><cell cols="2">Baltimore</cell><cell>Eldersburg</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Mention-level Graph</cell><cell></cell><cell></cell><cell cols="2">Entity-level Graph</cell><cell>Classifier</cell></row><row><cell>1</cell><cell>2</cell><cell>7 1</cell><cell cols="2">Mention Node</cell><cell cols="2">Document Node</cell><cell cols="3">Intra-Entity Edge</cell><cell>Inter-Entity Edge</cell><cell>Document Edge</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">3 Graph Aggregation and Inference</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Network (GAIN)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">GAIN mainly consists of 4 modules: encoding</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">module (Sec. 3.1), mention-level graph aggrega-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">tion module (Sec. 3.2), entity-level graph inference</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Performance on DocRED. Models above the first double line do not use pre-trained model. Results with * are reported in their original papers. Results with ‡ are performances of graph-based state-of-the-art RE models implemented in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Performance of GAIN with different embeddings and submodules.</figDesc><table><row><cell>Model</cell><cell cols="2">Intra-F1 Inter-F1</cell></row><row><cell>CNN  *</cell><cell>51.87</cell><cell>37.58</cell></row><row><cell>LSTM  *</cell><cell>56.57</cell><cell>41.47</cell></row><row><cell>BiLSTM  *</cell><cell>57.05</cell><cell>43.49</cell></row><row><cell>Context-Aware  *</cell><cell>56.74</cell><cell>42.26</cell></row><row><cell>LSR-GloVe  *</cell><cell>60.83</cell><cell>48.35</cell></row><row><cell>GAIN-GloVe</cell><cell>61.67</cell><cell>48.77</cell></row><row><cell>-hMG</cell><cell>59.72</cell><cell>46.49</cell></row><row><cell>BERT-RE  *  base</cell><cell>61.61</cell><cell>47.15</cell></row><row><cell>RoBERTa-RE base</cell><cell>65.65</cell><cell>50.09</cell></row><row><cell>BERT-Two-Step  *  base LSR-BERT  *  base</cell><cell>61.80 65.26</cell><cell>47.28 52.05</cell></row><row><cell>GAIN-BERT base</cell><cell>67.10</cell><cell>53.90</cell></row><row><cell>-hMG</cell><cell>66.15</cell><cell>51.42</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Infer-F1 results on dev set of DocRED. P: Precision, R: Recall. tions within the first sentence. Both BERT-RE base and GAIN-BERT base can successfully predict Without Me is part of The Eminem Show. But only GAIN-BERT base is able to deduce the performer and publication date of Without Me are the same as those of The Eminem Show, namely Eminem and May 26, 2002, where it requires logical inference across sentences.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 2335-2344. Yuhao Zhang, Peng Qi, and Christopher D. Manning. 2018. Graph convolution over pruned dependency trees improves relation extraction. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2205-2215. Yuhao Zhang, Victor Zhong, Danqi Chen, Gabor Angeli, and Christopher D. Manning. 2017. Positionaware attention and supervised data improve slot filling. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 35-45. Yankai Lin, Zhiyuan Liu, Jie Fu, Tat-Seng Chua, and Maosong Sun. 2019. Graph neural networks with generated parameters for relation extraction. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1331-1339.</figDesc><table><row><cell>Peng Zhou, Wei Shi, Jun Tian, Zhenyu Qi, Bingchen Li,</cell></row><row><cell>Hongwei Hao, and Bo Xu. 2016. Attention-based</cell></row><row><cell>bidirectional long short-term memory networks for</cell></row><row><cell>relation classification. In Proceedings of the 54th</cell></row><row><cell>Annual Meeting of the Association for Computa-</cell></row><row><cell>tional Linguistics (Volume 2: Short Papers), pages</cell></row><row><cell>207-212.</cell></row><row><cell>Hao Zhu,</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 6 :</head><label>6</label><figDesc>Settings for GAIN-GloVe.</figDesc><table><row><cell>Hyperparameter</cell><cell>Value</cell></row><row><cell>Batch Size</cell><cell>5</cell></row><row><cell>Learning Rate</cell><cell>0.001</cell></row><row><cell>Activation Function</cell><cell>ReLU, Tanh</cell></row><row><cell>Positive v.s. Negative Ratio</cell><cell>1, 0.5, 0.25</cell></row><row><cell>Entity Type Embedding Size</cell><cell>20</cell></row><row><cell>Coreference Embedding Size</cell><cell>20</cell></row><row><cell>Dropout</cell><cell>0.2, 0.6, 0.8</cell></row><row><cell>Layers of GCN</cell><cell>1, 2, 3</cell></row><row><cell>GCN Hidden Size</cell><cell>808</cell></row><row><cell>Weight Decay</cell><cell>0.0001</cell></row><row><cell>Numbers of Parameters</cell><cell>217M</cell></row><row><cell>Hyperparameter Search Trials</cell><cell>20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 7 :</head><label>7</label><figDesc>Settings for GAIN-BERT base .</figDesc><table><row><cell>Hyperparameter</cell><cell>Value</cell></row><row><cell>Batch Size</cell><cell>5</cell></row><row><cell>Learning Rate</cell><cell>0.001</cell></row><row><cell>Activation Function</cell><cell>ReLU, Tanh</cell></row><row><cell>Positive v.s. Negative Ratio</cell><cell>1, 0.5, 0.25</cell></row><row><cell>Entity Type Embedding Size</cell><cell>20</cell></row><row><cell>Coreference Embedding Size</cell><cell>20</cell></row><row><cell>Dropout</cell><cell>0.2, 0.6, 0.8</cell></row><row><cell>Layers of GCN</cell><cell>1, 2, 3</cell></row><row><cell>GCN Hidden Size</cell><cell>1064</cell></row><row><cell>Weight Decay</cell><cell>0.0001</cell></row><row><cell>Numbers of Parameters</cell><cell>512M</cell></row><row><cell>Hyperparameter Search Trials</cell><cell>20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 8 :</head><label>8</label><figDesc>Settings for GAIN-BERT large .</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank the anonymous reviewers for their thoughtful and constructive comments and ByteDance AI Lab for providing the computational resources for this work. This paper is supported in part by the National Key R&amp;D Program of China under Grand No.2018AAA0102003, the National Science Foundation of China under Grant No.61876004 and 61936012.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Reasoning with latent structure refinement for document-level relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoshun</forename><surname>Nan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Sekulic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.141</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1546" to="1557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS-W</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cross-sentence n-ary relation extraction with graph LSTMs. Transactions of the Association for</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="101" to="115" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1162</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction beyond the sentence boundary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1171" to="1182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Inter-sentence relation extraction with document-level graph convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fenia</forename><surname>Sunil Kumar Sahu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Christopoulou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophia</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ananiadou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4309" to="4316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
		<idno type="DOI">10.1109/78.650093</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">N-ary relation extraction using graphstate LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1246</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2226" to="2235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Contextaware representations for knowledge base relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniil</forename><surname>Sorokin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1188</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1784" to="1789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">HIN: hierarchical inference network for documentlevel relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengzhu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangxia</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Yin</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-47426-3_16</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Knowledge Discovery and Data Mining -24th Pacific-Asia Conference</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-05-11" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="197" to="209" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Simultaneously self-attending to all mentions for full-abstract biological relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Verga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1080</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="872" to="884" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Fine-tune bert for docred with two-step process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christfried</forename><surname>Focke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Sylvester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nilesh</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11898</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Relation classification via multi-level attention CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linlin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhu</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>De Melo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1123</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1298" to="1307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Gai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mufei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qipeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<title level="m">Deep graph library: Towards efficient and scalable deep learning on graphs. ICLR Workshop on Representation Learning on Graphs and Manifolds</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semantic relation classification via hierarchical recurrent neural network with attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minguang</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1254" to="1263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">DocRED: A large-scale document-level relation extraction dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1074</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="764" to="777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaju</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.06870</idno>
		<idno>abs/2004.06870</idno>
		<title level="m">Coreferential reasoning learning for language representation</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improved neural relation detection for knowledge base question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kazi Saidul Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Cicero Dos Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1053</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="571" to="581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction via piecewise convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1203</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1753" to="1762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Relation classification via convolutional deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
