<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Looking for the Devil in the Details: Learning Trilinear Attention Sampling Network for Fine-grained Image Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heliang</forename><surname>Zheng</surname></persName>
							<email>1zhenghl@mail.ustc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Rochester</orgName>
								<address>
									<settlement>Rochester</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Looking for the Devil in the Details: Learning Trilinear Attention Sampling Network for Fine-grained Image Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning subtle yet discriminative features (e.g., beak and eyes for a bird) plays a significant role in fine-grained image recognition. Existing attention-based approaches localize and amplify significant parts to learn fine-grained details, which often suffer from a limited number of parts and heavy computational cost. In this paper, we propose to learn such fine-grained features from hundreds of part proposals by Trilinear Attention Sampling Network (TASN) in an efficient teacher-student manner. Specifically, TASN consists of 1) a trilinear attention module, which generates attention maps by modeling the inter-channel relationships, 2) an attention-based sampler which highlights attended parts with high resolution, and 3) a feature distiller, which distills part features into an object-level feature by weight sharing and feature preserving strategies. Extensive experiments verify that TASN yields the best performance under the same settings with the most competitive approaches, in iNaturalist-2017, CUB-Bird, and Stanford-Cars datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Fine-grained visual categorization (FGVC) focuses on distinguishing subtle visual differences within a basic-level category (e.g., bird <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b33">34]</ref> and car <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b35">36]</ref>). Although the techniques of convolutional neural network (CNN) <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b24">25]</ref> for general image recognition <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b22">23]</ref> have become increasingly practical, FGVC is still a challenging task where discriminative details are too subtle to be wellrepresented by traditional CNN. Thus the majority of efforts in the fine-grained community focuses on learning better representation for such subtle yet discriminative details.</p><p>Existing attention/part-based methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b39">40]</ref>   and amplifying the attended parts, and concatenating part features for recognition. Although promising performance has been achieved, there are several critical issues in such a pipeline. Specifically, 1) the number of attention is limited and pre-defined, which restricts the effectiveness and flexibility of the model. 2) Without part annotations, it is difficult to learn multiple consistent (i.e., attending on the same part for each sample) attention maps. Although a well-designed initialization <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b39">40]</ref> can benefit the model training, it is not robust and cannot handle the cases with uncommon poses. Moreover, 3) training CNNs for each part is not efficient. Such problems evolve as bottlenecks for the study on attention-based methods.</p><p>To address the above challenges, we propose a trilinear attention sampling network (TASN) which learns finegrained details from hundreds of part proposals and efficiently distills the learned features into a single convolu-tional neural network. The proposed TASN consists of a trilinear attention module, an attention-based sampler, and a feature distiller. First, the trilinear attention module takes as input feature maps and generates attention maps by selftrilinear product, which integrates feature channels with their relationship matrix. Since each channel of feature maps is transformed into an attention map, hundreds of part proposals can be extracted. Second, attention-based sampler takes as input an attention map as well as an image, and highlights attended parts with high resolution. Specifically, for each iteration, the attention-based sampler generates a detail-preserved image based on a randomly selected attention map, and a structure-preserved image based on an averaged attention map. The former learns fine-grained feature for a specific part, and the latter captures global structure and contains all the important details. Finally, A partnet and a master-net are further formulated as "teacher" and "student," respectively. Part-net learns fine-grained features from the detail-preserved image and distills the learned features into master-net. And the master-net takes as input the structure-preserved image and refines a specific part (guided by the part-net) in each iteration. Such distillation is achieved by weight sharing and feature preserving strategies. Note that we adopt knowledge distilling introduced in [10] instead of concatenating part features, because the part number is large and not pre-defined.</p><p>Since the feature distiller transfers the knowledge from part-net into master-net via optimizing the parameters, 1) stochastic details optimization (i.e., randomly optimize one part in each iteration) can be achieved, which makes it practical to learn details from hundreds of part proposals, and 2) efficient inference can be obtained as we can use master-net to perform recognition in the testing stage. To the best of our knowledge, this work makes the first attempt to learn fine-grained features from hundreds of part proposals and represent such part features with a single convolutional neural network. Our contributions are summarized as follows:</p><p>• We propose a novel trilinear attention sampling network (TASN) to learn subtle feature representations from hundreds of part proposals for fine-grained image recognition. • We propose to optimize TASN in a teacher-student manner, in which fine-grained features can be distilled into a single master-net with high-efficiency. • We conduct extensive experiments on three challenging datasets (iNaturalist, CUB Birds and Stanford Cars), and demonstrate that TASN outperforms partensemble models even with a single stream. The remainder of the paper is organized as follows. We describe related work in Section 2, and introduce our proposed TASN model in Section 3. An evaluation on three widely-used datasets is presented in Section 4, followed by conclusions in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Attention Mechanism: As subtle yet discriminative details play an important role for Fine-Grained Image Recognition, learning to attend on discriminative parts is the most popular and promising direction. Thus various of attention mechanisms have been proposed in recent years <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b39">40]</ref>. DT-RAM <ref type="bibr" target="#b18">[19]</ref> proposed a dynamic computational time model for recurrent visual attention, which can attend on the most discriminative part in dynamic steps. RA-CNN <ref type="bibr" target="#b6">[7]</ref> proposed a recurrent attention convolutional neural network to recurrently learn attention maps in multiple (i.e., 3) scales. And MA-CNN <ref type="bibr" target="#b39">[40]</ref> takes one step further to generate multiple (i.e., 4) consistency attention maps in a single scale by designing a channel grouping module. However, the attention numbers (i.e., 1, 3, 4, respectively) are pre-defined, which counts against the effectiveness and flexibility of the model.</p><p>Meanwhile, high-order attention methods are proposed in visual question answering (VQA) and video classification. Specifically, BAN <ref type="bibr" target="#b11">[12]</ref> proposed a bilinear attention module to handle the relationship between image regions and the words in question, and Non-local <ref type="bibr" target="#b29">[30]</ref> calculates the dot production of features to represent the spatial and temporary relationship in video frames. Different from these works, our trilinear attention module conducts bilinear pooling to obtain the relationship among feature channels, which is further utilized to integrate such features to obtain third-order attention maps.</p><p>Adaptive Image Sampling: To preserve fine-grained details for recognition, high input resolution (448 × 448 v.s. 224 × 224) is widely adopted <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b39">40]</ref> and it can significantly improve the performance <ref type="bibr" target="#b4">[5]</ref>. However, high resolution brings large computational cost. More importantly, the importance of different regions are various, while directly zooming in images cannot promise different regions with different resolutions. STN <ref type="bibr" target="#b10">[11]</ref> proposed a nonuniformed sampling mechanism which performs well on MNIST datasets <ref type="bibr" target="#b16">[17]</ref>. But without explicit guidance, it is hard to learn non-uniformed sampling parameters for sophisticated tasks such as fine-grained recognition, thus they finally learned two parts without non-uniformed sampling. SSN <ref type="bibr" target="#b21">[22]</ref> firstly proposed to use saliency maps as the guidance of non-uniformed sampling and obtained significant improvements. Different from them, our attention sampler 1) conduct non-uniformed sampling based on trilinear attention maps, and 2) decomposes attention maps into two dimensions to reduce spatial distortion effects.</p><p>Knowledge Distilling: Knowledge distilling is firstly proposed by Hinton et al. <ref type="bibr" target="#b9">[10]</ref> to transfer knowledge from an ensemble or from a large highly regularized model into a smaller, distilled model. The main idea is using soft targets (i.e., the predicted distribution of ensemble/large model) to optimize the small model, for it contains more information than the one-hot label. Such a simple yet effective idea inspires many researchers and has been further studied by <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b37">38]</ref>. In this paper, we adopt this technique to distill the learned details from part-net into master-net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we introduce the proposed Trilinear Attention Sampling Network (TASN), which is able to represent rich fine-grained features by a single convolutional neural network. TASN contains three modules, i.e., a trilinear attention module for details localization, an attentionbased sampler for details extraction, and a feature distiller for details optimization.</p><p>An overview of the proposed TASN is shown in <ref type="figure" target="#fig_2">Figure 2</ref>. Given an input image in (a), we first take it through several convolutional layers to extract feature maps, which is further transformed into attention maps by the trilinear attention module in (b). To learn fine-grained features for a specific part, we randomly select an attention map and conduct attention sampling over the input image using the selected attention map. The sampled image in (e) is named as detailpreserved image since it can preserve a specific detail with high resolution. Moreover, to capture global structure and contain all the important details, we average all the attention maps and again conduct attention sampling, such a sampled image in (d) is called structure-preserved image. We further formulate a part-net to learn fine-grained representation for detail-preserved images, and a master-net to learn the features for the structure-preserved image. Finally, the part-net generates soft targets to distill the fine-grained features into master-net via soft target cross entropy <ref type="bibr" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Details Localization by Trilinear Attention</head><p>In this subsection, we introduce our trilinear attention module, which transfers convolutional feature maps into attention maps. As shown in previous work <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b38">39]</ref>, each channel of the convolutional features corresponds to a visual pattern, however, such feature maps cannot act as attention maps due to the lack of consistency and robustness <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b39">40]</ref>. Inspired by <ref type="bibr" target="#b39">[40]</ref>, we transform feature maps into attention maps by integrating feature channels according to their spatial relationship. Note that such a process can be implemented in a trilinear formulation, thus we call it trilinear attention module.</p><p>Given an input image I, we extract convolutional features by feeding it into multiple convolutional, batch normalization, ReLU, and pooling layers. Specifically, we use resnet-18 <ref type="bibr" target="#b7">[8]</ref> as backbone. To obtain high-resolution feature maps for precise localization, we remove the two downsampling processes from original resnet-18 by changing convolutional stride. Moreover, to improve the robustness of convolutional response, we increase the field of views <ref type="bibr" target="#b2">[3]</ref> by appending two sets of dilated convolutional layers with multiple dilate rates. In the training stage, we added a softmax classifier to optimize such convolutional features.</p><p>Assume that the feature maps is a tube with a dimension of c × h × w, where c, h and w indicate channel numbers, height, and width respectively. We reshape this feature into a matrix with a shape of c × hw, which is denoted as X ∈ R c×hw . Then our trilinear function can be basically formulated as:</p><formula xml:id="formula_0">M b (X) := (XX T )X,<label>(1)</label></formula><p>where XX T is the bilinear feature, which indicates the spa-  <ref type="figure">Figure 3</ref>. An illustration the trilinear product. X indicates convolutional feature maps, and we can obtain inter-channel relationships by XX T . After that, we integrate each feature map with its related ones to get trilinear attention maps via conducting dot production over XX T and X. tial relationship among channels. Specifically, X i is the i th channel of feature maps, and XX T i,j indicates the spatial relationship between channel i and channel j. To make feature maps more consistency and robust, we further integrate spatial relationship into feature maps by conducting dot production over XX T and X, thus trilinear attention maps can be obtained (which is shown in <ref type="figure">Figure 3</ref>).</p><p>We further studied different normalization methods to improve the effectiveness of trilinear attention, and a detailed discussion can be found in Section 4.2. To the end, we adopt the following normalized trilinear attention:</p><formula xml:id="formula_1">M(X) := N (N (X)X T )X,<label>(2)</label></formula><p>where N (·) indicates sof tmax normalization over the second dimension of a matrix. Note that these two normalization functions have different meanings: The first one N (X) is spatial normalization which keeps each channel of feature maps within the same scale. And the second one is relationship normalization which is conducted over each relationship vector (N (X)X T ) i . We denote the output of the trilinear function in Equation 2 as M ∈ R c×hw , i.e., M = M(X). Finally, we reshape M into the shape of c × h × w, thus each channel of M indicates an attention map M i ∈ R h×w .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Details Extraction by Attention Sampling</head><p>In this subsection, we introduce our attention-based sampler, which takes as input an image as well as trilinear attention maps, and generates a structure-preserved image and a detail-preserved image. The structure-preserved image captures the global structure and contains all the important details. Compared to the original image, the structurepreserved one removed the regions without fine-grained details, thus the discriminative parts can be better represented with high resolution. The detail-preserved image focuses on a single part, which can preserve more fine-grained details.</p><p>Given an image I, we obtain structure-preserved image I s and detail-preserved image I d by conducting non-uniform sampling over different attention maps:</p><formula xml:id="formula_2">I s = S(I, A(M)), I d = S(I, R(M)),<label>(3)</label></formula><p>where M is the attention maps, S(·) indicates the nonuniform sampling function, A(·) indicates average pooling over channels, and R(·) indicates randomly selecting a channel from the input. We calculate the average of all attention maps to guide structure-preserved sampling, because such an attention map takes all the discriminative parts into consideration. And we randomly select one attention map for detail-preserved sampling, thus it can preserve the fine-grained details of this attended area with high resolution.</p><p>With the training process going on, all attention maps have the opportunity to be selected, thus different fine-grained details can be asynchronously refined.</p><p>Our basic idea for attention-based sampling is considering the attention map as probability mass function, where the area with large attention value is more likely to be sampled. Inspired by the inverse-transform <ref type="bibr" target="#b5">[6]</ref>, we implement the sampling by calculating the inverse function of the distribution function. Moreover, we decompose attention maps into two dimensions to reduce spatial distortion effects.</p><p>Taking structure-preserved sampling for example, we first calculate the integral of the structure-preserved attention map A(M) over x and y axis:</p><formula xml:id="formula_3">F x (n) := n j=1 max 1≤i≤w A(M) i,j , F y (n) := n i=1 max 1≤j≤h A(M) i,j ,<label>(4)</label></formula><p>where w and h are the width and height of the attention map, respectively. Note that we use max(·) function to decompose the attention map into two dimensions, because it is more robust than the alternative sum(·). We can further obtain the sampling function by:</p><formula xml:id="formula_4">S(I, A(M)) i,j = I F −1 x (i),F −1 y (j) .<label>(5)</label></formula><p>where F −1 (·) indicates the inverse function of F(·). In a word, the attention map is used to calculate the mapping function between the coordinates of the original image and the sampled image. Such a sampling mechanism is illustrated in <ref type="figure" target="#fig_4">Figure 4</ref>. Given an attention map in (a), we first decompose the map into two dimensions by calculating the max values over x axis (b1) and y axis (b2). Then the integral of (b1) and (b2) are obtained and shown in (c1) and (c2), respectively. We further calculate the inverse function of (c1) and (c2) in a digital manner, i.e., we uniformly sample points over the y axis, and follow the red arrow (shown in (c1) and (c2)), and the blue arrow to obtain the values over x axis. (d) shows the sampling points by blue dots, and we can observe that the regions with large attention values are allocated with more sampling points. Finally, (e) shows the result of the sampled image. Note that the example in <ref type="figure" target="#fig_4">Figure 4</ref> is a structurepreserved sampling case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Details Optimization by Knowledge Distilling</head><p>In this subsection, we introduce our details distiller, which takes as input a detail-preserved image and a structure-preserved image, and transfers the learned details from part-net to master-net in a teacher-student manner.</p><p>Specifically, for each iteration, the attention-based sampler introduced in Section 3.2 can provide a structurepreserved image (denoted as I s ) and a detail-preserved one (denoted as I d ). We first obtain the fully connected (fc) outputs by feeding these two images into the same backbone CNN (e.g., Resnet-50 <ref type="bibr" target="#b7">[8]</ref>). The fc outputs are denoted as z s and z d , respectively. Then the "softmax" classifier converts z s and z d into a probability vector q s and q d , which indicates the predicted probability over each class. Taking z s for example:</p><formula xml:id="formula_5">q (i) s = exp(z (i) s /T ) j exp(z (j) s /T ) ,<label>(6)</label></formula><p>where T is a parameter namely temperature, which is normally set to 1 for classification tasks. While in knowledge distilling, a large value for T is important as it can produce a soft probability distribution over classes. We obtain the soft target cross entropy <ref type="bibr" target="#b9">[10]</ref> for the master-net as:</p><formula xml:id="formula_6">L sof t (q s , q d ) = − N i=1 q (i) d logq (i) s ,<label>(7)</label></formula><p>where N denotes the class number. Finally, the objective function of the master-net can be drived by: where L cls represents the classification loss function, y is a one hot vector which indicates the class label and λ denotes loss weight of the two terms. The soft target cross entropy aims to distill the learned feature for fine-grained details and transfer such information to the master-net. As the attention-based sampler randomly select one part in each iteration, all the fine-grained details can be distilled to the master-net in training process. Note that the convolutional parameters are shared for part-net and master-net, which is important for distilling, while the sharing of fully connected layers is optional.</p><formula xml:id="formula_7">L(I s ) = L cls (q s , y) + λL sof t (q s , q d ),<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experiment setup</head><p>Datasets: To evaluate the effectiveness of our proposed TASN, we conducted experiments on three extensive and competitive datasets, namely Caltech-UCSD Birds (CUB-200-2011) <ref type="bibr" target="#b33">[34]</ref>, Stanford Cars <ref type="bibr" target="#b12">[13]</ref> and iNaturalist-2017 <ref type="bibr" target="#b26">[27]</ref>, respectively. The detailed statistics with category numbers and the standard training/testing splits can be found in <ref type="table" target="#tab_0">Table 1</ref>. iNaturalist-2017 is the largest dataset for the fine-grained task. Compared with other datasets for this task, it contains 13 superclasses. Such a data distribution can provide a more convincing evaluation for the generalization ability of a model.</p><p>Baselines: We compared our method to the following baselines due to their state-of-the-art performance and high relevance. Note that for a fair comparison, we did not include methods using 1) additional data (from the web or other datasets), 2) human-annotated part locations and 3) hierarchical labels (i.e., species, genus, and family). And all of the compared methods in each table share the same backbone unless specified otherwise.</p><p>• FCAN <ref type="bibr" target="#b20">[21]</ref>: Fully convolutional attention network, which adaptively selects multiple attentions by reinforcement learning. • MDTP <ref type="bibr" target="#b30">[31]</ref>: Mining discriminative triplets of patches, which utilize geometric constraints to improve the accuracy of patch localization. • DT-RAM <ref type="bibr" target="#b18">[19]</ref>: Dynamic computational time model for recurrent visual attention, which attends on the most discriminative parts by dynamic steps. • SSN <ref type="bibr" target="#b21">[22]</ref>: Saliency-based sampling networks, which conduct non-uniformed sampling based on saliency map in an end-to-end way. which is a novel self-supervision mechanism to effectively localize informative regions without the need of bounding-box/part annotations. • iSQRT-COV <ref type="bibr" target="#b17">[18]</ref>: Towards faster training of global covariance pooling networks by iterative matrix square root normalization. Implementation: We used open-sourced MXNet <ref type="bibr" target="#b3">[4]</ref> as our code-base, and trained all the models on 8 Tesla P-100 GPUs. The backbones are are pre-trained on Imagenet <ref type="bibr" target="#b22">[23]</ref>, and all of the performances are single-crop testing results for a single model unless specially stated. We used SGD optimizer without momentum and weight decay, and the batch size was set to 96. The temperature in Equation 6 is 10, and the loss weight λ in Equation 8 is 2. More implementation details can be referred to our code https://github.com/researchmm/tasn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation and analysis on CUB-200-2011</head><p>Trilinear attention. <ref type="table" target="#tab_1">Table 2</ref> shows the impact of different normalization functions for the part-net in term of recognition accuracy. Specifically, we randomly select a channel of attention maps in each iteration in training stage, and conduct average pooling over attention maps for testing. All the models use Resnet-50 as the backbone with an input resolution of 224. It can be observed that trilinear attention maps can significantly outperform the original feature maps. Both the attention functions of N (X)X T X and N (XX T )X can improve the gain of trilinear attention. N (X)N (X) T X and N (XX T X) bring a drop of performance, because such normalization functions is harmful for preserving spatial information. To this end, we adopt the last setting (of <ref type="table" target="#tab_1">Table 2</ref>) in our TASN. Note that in the term N (X)X T , N (X) indicates the region that a channel is focusing on and X T denotes the feature of that region. We further compared our trilinear attention module with "self-attention" <ref type="bibr" target="#b27">[28]</ref>. Specifically, we followed <ref type="bibr" target="#b27">[28]</ref> to obtain attention maps by X T X, and the results show that the trilinear attention module can outperform self-attention module with 0.7% points increases.</p><p>Attention-based sampler. To demonstrate the effectiveness of our attention-based sampling mechanism, we compared our sampling mechanism with 1) uniformed sampling (by binarizing the attention maps) and 2) sampling operation introduced in SSN <ref type="bibr" target="#b21">[22]</ref>. We set the input attention maps to be same when comparing sampling mechanisms, and experiments were conducted on two cases, i.e., with and without part-net. All the models use Resnet-50 as the backbone and the input resolution is set to 224. The result in <ref type="table" target="#tab_2">Table 3</ref> shows that our sampling mechanism remarkably outperforms the baselines. SSN sampler obtains a better result than uniformed sampler without part-net, while the further improvements are limited when added part-net. These observations show that the spatial distortion caused by SSN sampler is harmful for preserving subtle details.</p><p>Knowledge distilling. <ref type="table">Table 4</ref> reveals the impact of details distilling module with different input resolutions. We can observe consistency improvements by details distilling. The performance of Resnet-50 <ref type="bibr" target="#b7">[8]</ref> is saturated with 85.6%, and 448 input can not further improve the accuracy. Without distiller (i.e., master-net only), the performance is slightly dropped with 392 input (compared to 336 input), since it is difficult to optimize each detail with large feature reso-  lutions (a similar drop can also be observed on Resnet-50 with 672 inputs). Moreover, to study the attention selection strategy (i.e., ranking selection vs. random selection), we ranked attention maps by their response, and sample high response ones with large possibility, while the recognition performance dropped from 87.0% to 86.8%. The reason is that ranking makes some parts rarely picked, while such parts can also benefit details learning. We also conducted experiments on distilling two parts in each iteration, and the result is the same as distilling one part each time.</p><p>Compared to sampling-based methods. We compare our TASN with three sampling-based methods: 1) uniformed sampling with high resolution (i.e., zoom in), 2) uniformed sampling with attention (i.e., crop) and 3) nonuniformed sampling proposed in SSN <ref type="bibr" target="#b21">[22]</ref>. As shown in <ref type="table" target="#tab_3">Table 5</ref>, higher resolution can significantly improve finegrained recognition performance by 4.9% relatively. However, 448 input increases the computational cost (i.e., flops) by four times compared to 224 input. SSN <ref type="bibr" target="#b21">[22]</ref> obtains a better results than DT-RAM <ref type="bibr" target="#b18">[19]</ref>, and our TASN can further obtain 2.9% relative improvement. Such improvements mainly come from two aspects: 1) a better sampling mechanism considering spatial distortion (1.2%), and 2) a better fine-grained details optimizing strategy (1.7%).</p><p>Compared to attention-based part methods. In Table 6, we compare our TASN to attention-based parts methods. For a fair comparison, 1) high-resolution input is adopted by all methods and 2) the same backbone numbers are used. It can be observed that for VGG based methods, our TASN outperforms all the baselines even with only one backbone. Moreover, after ensembling three backbones (trained with different parameter settings), TASN can improve the performance by 1.9% over the best 3 parts model MA-CNN <ref type="bibr" target="#b39">[40]</ref>. Moreover, our 3 streams result can also outperform 6 streams MA-CNN (86.5%) with a margin of 0.7%. We do not ensemble more streams as the model ensemble is beyond this work. For Resnet-50 based method: compared with the state-of-the-art single-stream MAMC <ref type="bibr" target="#b25">[26]</ref>, our TASN achieves a remarkable improvement by 1.6%. Moreover, although NTSN <ref type="bibr" target="#b36">[37]</ref> (K = 2) concatenates global feature with two part features, our singlestream TASN still can achieve 0.6% points increases.</p><p>Combining with second-order feature learning methods. In <ref type="table" target="#tab_5">Table 7</ref>, we exhibit that our TASN learns a strong first-order representation, which can further improve the performance of second-order feature methods. Specifically, compared to the best second-order methods iSQRT-COV <ref type="bibr" target="#b17">[18]</ref>, our TASN 2k first-order feature outperforms their 8k feature with an improvement by 0.7%, which shows the effectiveness of our TASN. Moreover, we transfer their released code to our framework and obtain an accuracy of 89.1%, which shows the compatibility of these two methods. Note that for a fair comparison, we follow their settings and predict the label of a test image by averaging prediction scores of the image and its horizontal flip.  <ref type="table">Table 8</ref> shows the result of VGG-19 baseline, our masternet, a single TASN model, and TASN ensemble results. We can observe 1.9% relative improvements by structure preserved sampling and further improvements of 2.3% by the full model. <ref type="table">Table 9</ref> compares TASN with attention-based parts methods. Specifically, TASN with single VGG-19 achieves comparable results with 3 streams part methods. And our ensembled 3 streams TASN outperforms the best 3 streams part learning methods MA-CNN <ref type="bibr" target="#b39">[40]</ref>. Compared to their 5 streams result (92.8%), our result is still better. For Resnet-50 based method, we compare our TASN to the state-of-the-art method MAMC <ref type="bibr" target="#b25">[26]</ref>, and achieve 1.1% improvements. Moreover, our single-stream TASN can achieve slightly better performance than NTSN <ref type="bibr" target="#b36">[37]</ref>, which concatenates a global feature with two part features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation and analysis on Stanford-Car</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Evaluation and analysis on iNaturalist 2017</head><p>We also conduct our TASN on the largest fine-grained dataset, i.e., iNaturalist 2017. We compare to Resnet <ref type="bibr" target="#b7">[8]</ref>  baseline and the best sampling method SSN <ref type="bibr" target="#b21">[22]</ref>. All the models use Resnet-101 as the backbone with an input resolution of 224. As there are 13 superclasses in this dataset, we re-implement SSN <ref type="bibr" target="#b21">[22]</ref> with their released code to obtain the performance on each superclass. The results are shown in <ref type="table" target="#tab_0">Table 10</ref>, and we can observe that TASN outperforms Resnet baseline and SSN on every superclass. It is notable that compared to Resnet-101, TASN significantly improves the performance, especially on Reptilia (improved by 24.0%, relatively) and Aves (improved by 21.8%, relatively), which indicates that such superclasses contain more fine-grained details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we proposed a trilinear attention sampling network for fine-grained image recognition, which can learn rich feature representations from hundreds of part proposals. Instead of ensembling multiple part CNNs, we adopted knowledge distilling method to integrate fine-grained features into a single stream, which is not only efficient but also effective. Extensive experiments in CUB-Bird, iNaturalist 2017 and Stanford-Car demonstrate that TASN is able to outperform part-ensemble models even with a single stream. In the future, we will further study the proposed TASN in the following directions: 1) attention selection strategy, i.e., learning to select which details should be learned and distilled instead of randomly selecting, 2) conduct attention-based sampling over convolutional features instead of only over images, and 3) extend our work to other vision tasks, e.g., object detection and segmentation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>try to solve this problem by learning part detectors, cropping * This work was performed when Heliang Zheng was visiting Microsoft Research as a research intern. † Corresponding author. TASN (b) rich details learning (c) fine-grained feature (a) input image</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>An illustration of learning discriminative details by TASN for a "bule jay." As shown in (b), TASN learns such subtle details by up-sampling each detail into high-resolution. And the white concentric circles in (c) indicates fine-grained details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Overview of the proposed Trilinear Attention Sampling Network (TASN). The trilinear attention module in (b) takes as input convolutional feature maps (denoted as "conv"), and generates attention maps (denoted as "att"). The attention sampling module in (c) further takes as input an attention map as well as the original image to obtain sampled images. Specifically, average pooling and random selection (in each iteration) are conducted over attention maps to obtain structure preserved image in (d) and detail preserved image in (e), respectively. The part-net (in green) learns fine-grained features from (e) and generates a soft target to distill such features into the master-net (in blue) via soft target cross entropy<ref type="bibr" target="#b9">[10]</ref>. [Best viewed in color]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>An example of attention-based non-uniform sampling. (a) is an attention map with Gaussian distribution. (b1) and (b2) are the marginal distributions over x and y axis, respectively. (c1) and (c2) are the integrals of marginal distributions. (d) shows the sampling points by the blue dot, and (e) illustrates the sampled image. [Best viewed in color with zoom-in.]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>A comparison of feature maps X in (a) and trilinear attention maps N (N (X)X T )X in (b). Each column shows the same channel of feature maps and trilinear attention maps, and we randomly select nine channels for comparison. Compared to first-order feature maps, each channel of the trilinear attention maps focus on a specific part, without attending on background noises. [Best viewed in color]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Detailed statistics of the three datasets used in this paper.</figDesc><table><row><cell>Dataset</cell><cell># Class</cell><cell># Train</cell><cell># Test</cell></row><row><cell>CUB-200-2011 [34]</cell><cell>200</cell><cell>5,994</cell><cell>5,794</cell></row><row><cell>Stanford-Car [13]</cell><cell>196</cell><cell>8,144</cell><cell>8,041</cell></row><row><cell>iNaturalist-2017 [27]</cell><cell>5,089</cell><cell cols="2">579,184 95,986</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation experiments on attention module in terms of recognition accuracy on the CUB-200-2011 dataset.</figDesc><table><row><cell>Attention</cell><cell>Description</cell><cell>Accuracy</cell></row><row><cell>X</cell><cell>feature maps</cell><cell>83.5</cell></row><row><cell>XX T X</cell><cell>trilinear attention</cell><cell>84.9</cell></row><row><cell>N (X)X T X</cell><cell>spacial norm</cell><cell>85.2</cell></row><row><cell>N (X)N (X) T X</cell><cell>spacial norm</cell><cell>84.3</cell></row><row><cell>N (XX T X)</cell><cell>spacial norm</cell><cell>84.5</cell></row><row><cell>N (XX T )X</cell><cell>relation norm</cell><cell>85.0</cell></row><row><cell>N (N (X)X T )X</cell><cell>spacial + relation</cell><cell>85.3</cell></row><row><cell cols="3">• MG-CNN [29]: Multiple granularity descriptors,</cell></row><row><cell cols="3">which leverage the hierarchical labels to generate com-</cell></row><row><cell cols="2">prehensive descriptors.</cell><cell></cell></row><row><cell cols="3">• STN [11]: Spatial transformer network, which con-</cell></row><row><cell cols="3">ducts parameterized spatial transformation to obtain</cell></row><row><cell cols="2">zoomed in or pose normalized objects.</cell><cell></cell></row><row><cell cols="3">• RA-CNN [7]: Recurrent attention CNN, which recur-</cell></row><row><cell cols="3">rently attends on discriminative parts in multi-scale.</cell></row><row><cell cols="3">• MA-CNN [40]: Multiple attention CNN, which at-</cell></row><row><cell cols="3">tends on multiple parts by their proposed channel</cell></row><row><cell cols="3">grouping module in a weakly-supervised way.</cell></row><row><cell cols="3">• MAMC [26]: Multi-attention multi-class constraint</cell></row><row><cell cols="3">network, which learns multiple attentions by conduct-</cell></row><row><cell cols="3">ing multi-class constraint over attended features.</cell></row><row><cell cols="3">• NTSN [37]: Navigator-Teacher-Scrutinizer Network,</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Ablation experiments on sampling module in term of classification accuracy on the CUB-200-2011 dataset.</figDesc><table><row><cell>Approach</cell><cell></cell><cell cols="3">master-net TASN</cell></row><row><cell cols="2">Resnet-50 [8]</cell><cell>81.6</cell><cell></cell><cell>81.6</cell></row><row><cell cols="2">uniformed sampler</cell><cell>84.1</cell><cell></cell><cell>85.8</cell></row><row><cell cols="2">sampler in SSN [22]</cell><cell>84.8</cell><cell></cell><cell>85.3</cell></row><row><cell>our sampler</cell><cell></cell><cell>85.5</cell><cell></cell><cell>87.0</cell></row><row><cell cols="5">Table 4. Ablation experiments on distilling module with different</cell></row><row><cell>input resolutions.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Resolution</cell><cell>224</cell><cell>280</cell><cell>336</cell><cell>392</cell></row><row><cell cols="5">Resnet-50 [8] 81.6 83.3 85.0 85.6</cell></row><row><cell>master-net</cell><cell cols="4">85.5 86.6 87.0 86.8</cell></row><row><cell>TASN</cell><cell cols="4">87.0 87.3 87.9 87.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Comparison with sampling-based methods in terms of classification accuracy on the CUB-200-2011 dataset.</figDesc><table><row><cell>Approach</cell><cell cols="2">Resolution Accuracy</cell></row><row><cell>Resnet-50 [8]</cell><cell>224</cell><cell>81.6</cell></row><row><cell>Resnet-50 [8]</cell><cell>448</cell><cell>85.6</cell></row><row><cell>DT-RAM [19]</cell><cell>224</cell><cell>82.8</cell></row><row><cell>SSN [22]</cell><cell>227</cell><cell>84.5</cell></row><row><cell>TASN (ours)</cell><cell>224</cell><cell>87.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .</head><label>6</label><figDesc>Comparison with part-based methods (all the results are reported in high-resolution setting) in terms of classification accuracy on the CUB-200-2011 dataset.</figDesc><table><row><cell>Approach</cell><cell>Backbone</cell><cell>Accuracy</cell></row><row><cell>MG-CNN [29]</cell><cell>3×VGG-16</cell><cell>81.7</cell></row><row><cell>ST-CNN [11]</cell><cell>3×Inception-v2</cell><cell>84.1</cell></row><row><cell>RA-CNN [7]</cell><cell>3×VGG-19</cell><cell>85.3</cell></row><row><cell>MA-CNN [40]</cell><cell>3×VGG-19</cell><cell>85.4</cell></row><row><cell>TASN (ours)</cell><cell>1×VGG-19</cell><cell>86.1</cell></row><row><cell>TASN (ours)</cell><cell>3×VGG-19</cell><cell>87.1</cell></row><row><cell>MAMC [26]</cell><cell>1×Resnet-50</cell><cell>86.5</cell></row><row><cell>NTSN [37]</cell><cell>3×Resnet-50</cell><cell>87.3</cell></row><row><cell>TASN (ours)</cell><cell>1×Resnet-50</cell><cell>87.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 .</head><label>7</label><figDesc>Extensive experiments on combining second-order feature learning methods.</figDesc><table><row><cell>Approach</cell><cell></cell><cell cols="2">Dimension Accuracy</cell></row><row><cell>iSQRT-COV [18]</cell><cell></cell><cell>8k</cell><cell>87.3</cell></row><row><cell>iSQRT-COV [18]</cell><cell></cell><cell>32k</cell><cell>88.1</cell></row><row><cell>TASN (ours)</cell><cell></cell><cell>2k</cell><cell>87.9</cell></row><row><cell cols="2">TASN + iSQRT-COV</cell><cell>32k</cell><cell>89.1</cell></row><row><cell cols="4">Table 8. Component analysis in terms of classification accuracy on</cell></row><row><cell>the Stanford-Car dataset.</cell><cell></cell><cell></cell></row><row><cell>Approach</cell><cell></cell><cell>Backbone</cell><cell>Accuracy</cell></row><row><cell>Baseline</cell><cell></cell><cell>1×VGG-19</cell><cell>88.6</cell></row><row><cell>master-net</cell><cell></cell><cell>1×VGG-19</cell><cell>90.3</cell></row><row><cell>TASN</cell><cell></cell><cell>1×VGG-19</cell><cell>92.4</cell></row><row><cell cols="3">TASN (ensemble) 2×VGG-19</cell><cell>93.1</cell></row><row><cell cols="3">TASN (ensemble) 3×VGG-19</cell><cell>93.2</cell></row><row><cell cols="4">Table 9. Comparison in terms of classification accuracy on the</cell></row><row><cell>Stanford-Car dataset.</cell><cell></cell><cell></cell></row><row><cell>Approach</cell><cell></cell><cell>Backbone</cell><cell>Accuracy</cell></row><row><cell>FCAN [21]</cell><cell cols="2">3×VGG-16</cell><cell>91.3</cell></row><row><cell>MDTP [31]</cell><cell cols="2">3×VGG-16</cell><cell>92.5</cell></row><row><cell>RA-CNN [7]</cell><cell cols="2">3×VGG-19</cell><cell>92.5</cell></row><row><cell>MA-CNN [40]</cell><cell cols="2">3×VGG-19</cell><cell>92.6</cell></row><row><cell>TASN (ours)</cell><cell cols="2">1×VGG-19</cell><cell>92.4</cell></row><row><cell>TASN (ours)</cell><cell cols="2">3×VGG-19</cell><cell>93.2</cell></row><row><cell>MAMC [26]</cell><cell cols="2">1×Resnet-50</cell><cell>92.8</cell></row><row><cell>NTSN [37]</cell><cell cols="2">3×Resnet-50</cell><cell>93.7</cell></row><row><cell>TASN (ours)</cell><cell cols="2">1×Resnet-50</cell><cell>93.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 10 .</head><label>10</label><figDesc>Comparison in terms of classification accuracy on the iNaturalist 2017 dataset.</figDesc><table><row><cell>Super Class</cell><cell cols="4"># Class Resnet [8] SSN [22] TASN</cell></row><row><cell>Plantae</cell><cell>2101</cell><cell>60.3</cell><cell>63.9</cell><cell>66.6</cell></row><row><cell>Insecta</cell><cell>1021</cell><cell>69.1</cell><cell>74.7</cell><cell>77.6</cell></row><row><cell>Aves</cell><cell>964</cell><cell>59.1</cell><cell>68.2</cell><cell>72.0</cell></row><row><cell>Reptilia</cell><cell>289</cell><cell>37.4</cell><cell>43.9</cell><cell>46.4</cell></row><row><cell>Mammalia</cell><cell>186</cell><cell>50.2</cell><cell>55.3</cell><cell>57.7</cell></row><row><cell>Fungi</cell><cell>121</cell><cell>62.5</cell><cell>64.2</cell><cell>70.3</cell></row><row><cell>Amphibia</cell><cell>115</cell><cell>41.8</cell><cell>50.2</cell><cell>51.6</cell></row><row><cell>Mollusca</cell><cell>93</cell><cell>56.9</cell><cell>61.5</cell><cell>64.7</cell></row><row><cell>Animalia</cell><cell>77</cell><cell>64.8</cell><cell>67.8</cell><cell>71.0</cell></row><row><cell>Arachnida</cell><cell>56</cell><cell>64.8</cell><cell>73.8</cell><cell>75.1</cell></row><row><cell>Actinopterygii</cell><cell>53</cell><cell>57.0</cell><cell>60.3</cell><cell>65.5</cell></row><row><cell>Chromista</cell><cell>9</cell><cell>57.6</cell><cell>57.6</cell><cell>62.5</cell></row><row><cell>Protozoa</cell><cell>4</cell><cell>78.1</cell><cell>79.5</cell><cell>79.5</cell></row><row><cell>Total</cell><cell>5089</cell><cell>59.6</cell><cell>65.2</cell><cell>68.2</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Birdsnap: Large-scale fine-grained visual categorization of birds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiongxin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seung</forename><surname>Woo Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michelle</forename><forename type="middle">L</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2011" to="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bird species categorization using pose normalized deep convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grant</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianjun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.01274</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Large scale fine-grained categorization and domain-specific transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4109" to="4118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sample-based non-uniform random variate generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Devroye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSC</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1986" />
			<biblScope unit="page" from="260" to="265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Look closer to see better: Recurrent attention convolutional neural network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heliang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="4438" to="4446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Knowledge distillation with adversarial samples supporting decision boundary. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeongho</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsik</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><forename type="middle">Young</forename><surname>Choi</surname></persName>
		</author>
		<idno>abs/1805.05532</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">stat</title>
		<imprint>
			<biblScope unit="volume">1050</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bilinear attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Hwa</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehyun</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byoung-Tak</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1571" to="1581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">3D object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshop</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">The cifar-10 dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="http://www.cs.toronto.edu/kriz/cifar.html" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fine-grained recognition as hsnet search for informative image parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Behrooz</forename><surname>Mahasseni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinisa</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6497" to="6506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Towards faster training of global covariance pooling networks by iterative matrix square root normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangtao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qilong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilin</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="947" to="955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dynamic computational time for visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Largescale vehicle re-identification in urban surveillance videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huadong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyuan</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Fully convolutional attention networks for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanqing</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06765</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning to zoom: a saliencybased sampling layer for neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adria</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Kellnhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Stent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Matusik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="51" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ImageNet Large Scale Visual Recognition Challenge. IJCV</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Neural activation constellations: Unsupervised part model discovery with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcel</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Rodner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1143" to="1151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1409" to="1556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multiattention multi-class constraint for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="805" to="821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">The inaturalist species classification and detection dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Grant Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multiple granularity descriptors for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Mining discriminative triplets of patches for finegrained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonghyun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1163" to="1172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Selective convolutional descriptor aggregation for fine-grained image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu-Shen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Hao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2868" to="2881" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Mask-cnn: Localizing parts and selecting descriptors for fine-grained bird species categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu-Shen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Wei</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="704" to="714" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Caltech-UCSD Birds 200</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<idno>CNS-TR-2010-001</idno>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The application of two-level attention models in deep convolutional neural network for fine-grained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianjun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuiyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="842" to="850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A large-scale car dataset for fine-grained categorization and verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3973" to="3981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning to navigate for fine-grained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiange</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="420" to="435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A gift from knowledge distillation: Fast optimization, network minimization and transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junho</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donggyu</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihoon</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junmo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4133" to="4141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Picking deep filter responses for finegrained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkai</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1134" to="1142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning multi-attention convolutional neural network for finegrained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heliang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="5209" to="5217" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
