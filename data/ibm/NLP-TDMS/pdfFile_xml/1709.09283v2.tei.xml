<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fast Shadow Detection from a Single Image Using a Patched Convolutional Neural Network*</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepideh</forename><surname>Hosseinzadeh</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moein</forename><surname>Shakeri</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Zhang</surname></persName>
						</author>
						<title level="a" type="main">Fast Shadow Detection from a Single Image Using a Patched Convolutional Neural Network*</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In recent years, various shadow detection methods from a single image have been proposed and used in vision systems; however, most of them are not appropriate for the robotic applications due to the expensive time complexity. This paper introduces a fast shadow detection method using a deep learning framework, with a time cost that is appropriate for robotic applications. In our solution, we first obtain a shadow prior map with the help of multi-class support vector machine using statistical features. Then, we use a semanticaware patch-level Convolutional Neural Network that efficiently trains on shadow examples by combining the original image and the shadow prior map. Experiments on benchmark datasets demonstrate the proposed method significantly decreases the time complexity of shadow detection, by one or two orders of magnitude compared with state-of-the-art methods, without losing accuracy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Dealing with shadows is one of the most fundamental issues in image processing, computer vision and robotics. Shadows are omnipresent in outdoor applications and must be taken into account in the solutions to standard computer vision and robotics problems such as image segmentation <ref type="bibr" target="#b0">[1]</ref>, change detection <ref type="bibr" target="#b1">[2]</ref>, place recognition <ref type="bibr" target="#b2">[3]</ref>, background subtraction <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, visual robot localization <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref> and navigation <ref type="bibr" target="#b7">[8]</ref>. Unfortunately in most of these cases images are strongly influenced by shadow at different times of a day, making it difficult to interpret or understand a scene. Although in some applications people have attempted to address this challenge by relying on robust features with impressive results, these features often do not provide sufficient invariance to shadow.</p><p>The problem of detecting shadow is a well-studied research topic, and many methods have been proposed <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>. Existing methods can be categorized into two major groups. The first group of methods alleviate or remove the effect of shadow by providing an invariant representation of the image <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>. Most of these methods model the process of image formation to build shadow-free images. Although these methods are effective to some extent, all of them have the limitation in terms of dealing with non-Plankian source of light, narrow-band color camera and *This paper was supported by the Natural Science and Engineering Research Council (NSERC) through the NSERC Canadian Field Robotics Network (NCFRN) and by Alberta Innovates Technology Future (AITF). <ref type="bibr" target="#b0">1</ref>  environment calibration. They also tend to lose information in the shadow-free representation that can be important for scene understanding. The other group of methods to deal with shadow rely on a learning framework <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>. These methods specifically focus on shadow detection while attempting to keep the original color and intensity of images. Unfortunately, these methods still have difficulty in robotics applications that we will elaborate in the next section. State-of-the-art shadow detection methods come from the second group above and are based on convolutional neural networks (CNN). In this paper we present a novel and fast method, also based on CNN. Our method detects shadows of an image without any assumptions about image quality or the camera and it is therefore appropriate for the robotics applications. To develop an efficient shadow-detection algorithm, rather than labeling individual pixels, we work with super-pixels, obtained through segmentation. Subsequently, we extract color and texture features from each super-pixel and, with the help of a trained SVM, compute a shadow prior in terms of the probability of a super-pixel being shadow. Then, we use the combination of the original image and the obtained shadow prior as the input to a patch-level CNN to compute the improved shadow probability map of the image. The edge pixels between the super-pixels are further refined by running the same patch-level CNN a second time, to produce the final shadow detection result. We will show that the proposed method can provide comparable results with existing deep shadow detection methods due to the use of the combination of texture features and deep neural networks, but works much faster in both training and detection phases than existing CNN based methods, due to the use of superpixels. This method enables us to detect shadow in an image in robotic applications, a task that was not possible before due to the high time complexity.</p><p>The remainder of this paper is organized as follows. In Sections II, we discuss related works in shadow detection in further detail. In Section III, we introduce our novel method to this problem, focusing on improving the efficiency of an existing deep neural network based solution. Comparative experimental results on benchmark datasets are described in Section IV, and finally Section V summarizes our method and concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head><p>The importantce of detecting shadow in a single image has been well investigated in computer vision and robotics community. One common approach in robotics community  <ref type="figure">Fig. 1</ref>. Proposed method pipeline. For obtaining shadow prior map, input image is segmented by mean shift algorithm, then for each segment, we obtain the confidence of being shadow using SVM. Color and texture features are input of SVM. This shadow prior (P) is attached to the RGB image for using RGBP patched-CNN. In detection of shadow algorithm, we have two steps region and edge based predictions to obtain the final shadow probability map. The segmentation information obtained by mean shift is used in region based prediction step.</p><p>computes "intrinsic images" <ref type="bibr" target="#b13">[14]</ref> by decomposing an image into its reflectance and illuminance constituents <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b11">[12]</ref>. As discussed in the previous section, these methods have restrictive assumptions on illumination and sensor. To relax these assumptions, data-driven methods have been proposed, which work with images in grayscale or color space based on a learning framework to learn shadow in different situations <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>. Zhu et al. <ref type="bibr" target="#b17">[18]</ref> proposed a method that classifies regions based on statistics of intensity, gradient, and texture, computed over local neighborhoods, and refines shadow labels by exploiting spatial continuity within a conditional random field (CRF) framework <ref type="bibr" target="#b26">[27]</ref>. Lalonde et al. <ref type="bibr" target="#b14">[15]</ref> find shadow boundaries by comparing the color and texture of neighboring regions and employing a CRF to encourage boundary continuity. To benefit from global information, Guo et al. <ref type="bibr" target="#b15">[16]</ref> proposed a region based method, which can model long-range interaction between pairs of regions of the same material, with two types of pairwise classifiers, under similar/different illumination conditions. Then, they incorporated the pairwise classifier and a shadow region classifier into a conditional random field (CRF) via graph-cut optimization <ref type="bibr" target="#b27">[28]</ref>. Vicente et al. <ref type="bibr" target="#b16">[17]</ref> proposed a multi-kernel model to train a shadow support vector machine (SVM). Their multi-kernel model is a summation of base kernels, one for each type of local features. The main limitation of this model is the assumption of equal importance for all features. To avoid this assumption, they proposed leave-one-out kernel optimization <ref type="bibr" target="#b20">[21]</ref> in which the parameters of the kernel and the classifier are jointly learned. They also used least square SVM (LSSVM), which has a closed form solution and therefore is faster than SVM. However, their approach is still computationally expensive. Although these methods reviewed above provide good accuracy in some cases, they run on the order of many minutes or seconds, and are not applicable in robotics due to this high time complexity.</p><p>Recently, some end-to-end deep learning frameworks have been proposed to learn the most relevant features for shadow detection. They outperform the state-of-the-art methods that use hand crafted features. The first method in this category is proposed by Khan et al. <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>. They train two CNNs, one for detecting shadow regions and the other for detecting shadow boundaries. They also train a unary classifier by combining the two CNNs, and the per-pixel predictions are then fed to a CRF for enforcing local consistency. <ref type="bibr" target="#b21">[22]</ref> proposes a structured deep edge detection for shadows and shows that using structured label information, local consistency over pixel labels can be improved. More recently, Vicente et al. <ref type="bibr" target="#b25">[26]</ref> propose a method by combining an image level fully connected network (FCN) and a patch-based CNN. They train the FCN for semantically aware shadow prediction and use the output of the FCN as shadow prior with the corresponding input RGB image to train the patched-CNN from a random initialization. This method produces excellent result but is unsatisfactory in terms of its time complexity.</p><p>In this paper, we propose a novel method based on deep learning with a shadow prior. Like <ref type="bibr" target="#b21">[22]</ref>, our method can detect shadow from a single image, but at a much lower time complexity than all existing methods based on deep learning. The key insight of our method is that it performs shadow detection on a per super-pixel basis. Initial result from such a detection method would produce boundary effects between super-pixels. We overcome such artifacts with a post-processing step using edge refinement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED METHOD</head><p>In this section we describe our proposed method to detect shadows from a single image. Our method uses two steps to learn shadow from training images. First we obtain a prior map that we call shadow prior using a trained SVM on color and texture features, and then we train a patched-CNN using the original images and their shadow priors obtained from the first step. These two steps will be detailed in Section III-A and III-B, respectively. For the detection of shadows in a given image, we compute its shadow prior first with the SVM and use the prior and the image as input to the trained patched-CNN, considering only the center pixel of each super-pixel of the input as the representative in order to reduce the computational time. In doing so, however, the super-pixels near object and shadow boundaries tend to produce unreliable "edge effects". To overcome this problem, we refine prediction labels for the edge pixels along superpixel boundaries, using the patch-CNN again. In other words, algorithm makes use of the trained patched-CNN twice. The output of the second patched-CNN provides the final detected shadow areas. The edge refinement step of our algorithm will be discussed in Section III-C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Computing Shadow Prior</head><p>Shadow prior computation involves steps shown in the first row of <ref type="figure">Fig. 1</ref>. We first segment the image using mean shift algorithm <ref type="bibr" target="#b22">[23]</ref> to obtain superpixels. Second, using a trained classifier on texture and color features, we estimate the shadow probability of each region. Segmentation enables us to estimate the shadow probability for each region instead of each pixel and therefore, the computation time is significantly decreased.</p><p>In general, a shadowed region is darker and less textured than a non-shadow region <ref type="bibr" target="#b17">[18]</ref>. Therefore, the color and texture of a region can help to predict whether it is in shadow. We exploit this observation and represent color with a histogram in L*a*b space, with 21 bins per channel, as was successfully applied in <ref type="bibr" target="#b15">[16]</ref>. We represent texture with the texton histogram provided by <ref type="bibr" target="#b23">[24]</ref>. We train our SVM classifier with the color and texture features with a χ 2 kernel and slack parameter C = 1 <ref type="bibr" target="#b24">[25]</ref>. We define the shadow prior of each super-pixel, as the log likelihood output of this trained classifier. In the subsequent step, we use this shadow prior as a critical input to our patched-CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Training Patched-CNN with the Shadow Prior</head><p>In the next step of our shadow detection pipeline, we employ a patch-wise CNN to predict shadow. Current research in <ref type="bibr" target="#b25">[26]</ref> showed that using patches of an image with a specific size has two benefits in the case of shadow recognition. First, these patches include enough local pattern of the image and more global information in a large-range of neighborhood pixels than pixel-based methods. Secondly, using patches we are able to provide more training samples with different patterns from a limited number of labeled images. As discussed, one of the challenging problems in the case of shadow detection is the number of training samples, which can significantly affect the accuracy of a deep neural network. Unfortunately, available shadow benchmark datasets are small due to the high cost of shadow annotation. This patch-wise structure enables us to provide a huge number of patches of shadow and non-shadow areas for training that can increase the overall accuracy of the network.</p><p>We utilize the network architecture used in <ref type="bibr" target="#b25">[26]</ref>. The deep network has six convolutional layers, two pooling layers, and one fully connected layer. The input of this network is a 32×32 RGBP patch selected from combining RGB image and shadow prior image P. The output is the shadow probability map of the patch. We select equal number of patches for training in three classes as follows.</p><p>• Shadow patches: Since we are going to learn shadow areas, we first select patches from shadow regions. • Non-shadow patches: We select patches from nonshadow image locations randomly to include patches of various textures and colors. Also, these selected patches prevent overfitting. • Shadow-Edge patches: We also select patches on edges between shadow and non-shadow regions, to learn the shadow boundaries. Since the ground-truth is binary, locations of all shadow edges can be extracted accurately. Using this strategy, we are able to provide millions of patches from thousands of images. The loss function of the network is the average negative log-likelihood of the prediction of every pixel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Edge Refinement of Super-Pixel Labels</head><p>For the detection of shadows in a given image, only the patches centered in the super-pixel center are used and the average value of each predicted patch is assigned to all pixels of the super-pixel. These predictions made by the patched-CNN are local, and the prediction results near shadow boundaries are poor. To improve the accuracy of our detection algorithm, higher level interaction between the regions is needed. Therefore, in this final step we process the edge pixels between the regions by patched-CNN once again, shown in the bottom row of <ref type="figure">Fig. 1</ref>. We only process those pixels that are on edges between the segments, labeled as R(S) and defined as:</p><formula xml:id="formula_0">R(S) = {s i ∈ S|s i ≥ α max 1≤i≤m (s i )}<label>(1)</label></formula><p>R(S) contains those segments with the higher probability than a threshold of maximum shadow probability in the regionbased prediction P . α is a constant threshold (equal to 0.2 in our implementation) and m is the number of superpixels or regions in the image or the region-based prediction P . Absolute non-shadow regions always provide a very low shadow probability in the shadow prior map, and (1) only filters out those regions. This thresholding step will reduce the number of pixels to be refined and the total time of this step. For each boundary pixel (x, y) between segments that is included in R(S), a window patch with size 32×32 surrounding the pixel (x, y) from its shadow prior and corresponding original image are given to the patched-CNN to predict the shadow probability for that patch. Then we set the edge pixel (x, y) and its 8 neighbor pixels' probability values to be average probability value of these 9 pixels.</p><p>This step can integrate the segmented probability maps obtained in previous step and the final shadow probability map becomes smooth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS</head><p>In this section we perform a set of experiments to evaluate our proposed method and compare it with other state-of-theart methods. We first use three challenging available datasets "UCF" <ref type="bibr" target="#b17">[18]</ref>, "UIUC" <ref type="bibr" target="#b15">[16]</ref>, and "SBU" <ref type="bibr" target="#b25">[26]</ref> for shadow detection to evaluate quantitatively the proposed method. The number of images in each of dataset in our experiments is as follows.</p><p>• UCF Dataset: This dataset contains 355 images with manually labeled pixel-based ground truth.</p><p>• UIUC Dataset: This dataset contains 108 images (32 train images and 76 test images) with pixel-based ground truth.</p><p>• SBU Dataset: This new dataset contains 4,727 images (4,089 train images and 638 test images) with pixelbased ground truth.</p><p>• Combined Dataset: Both UCF and UIUC datasets include an insufficient number of images, and to evaluate the propose method we need to select a portion of these datasets as training samples. Since our proposed method works on patches, we are able to create many patches from the datasets for the training phase. However, to be fair in comparison with other methods, we combine UCF, UIUC, and SBU datasets to train for all methods. The combined dataset includes 5,078 images. We ran-domly selected 25% of the images for testing, and the rest for training. In addition, we use two other datasets "UACampus" and "St. Lucia" <ref type="bibr" target="#b28">[29]</ref> to obtain qualitative results of detecting shadows on roads, one of the common problems in robot applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Evaluation Metrics</head><p>To evaluate the proposed method in terms of detection accuracy, we use three evaluation metrics as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ShadowAccuracy =</head><p>T P all shadow pixels</p><p>Non − shadowAccuracy = T N all non − shadow pixels</p><p>(3)</p><formula xml:id="formula_2">TotalAccuracy = T P + T N all pixels<label>(4)</label></formula><p>For comparison in terms of computational efficiency, we simply use the execution time of shadow detection in a single image as the performance metric. Therefore, a total of four performance metrics, three for accuracy and one for efficiency, are considered in our experimental evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Results on Benchmark Datasets</head><p>In this section we evaluate our proposed method and compare it with Stacked-CNN <ref type="bibr" target="#b25">[26]</ref> and Unary-Pairwise <ref type="bibr" target="#b15">[16]</ref>. We select Stacked-CNN as a recent shadow detection method based on deep learning framework that uses a shadow prior map. We choose the unary-pairwise method since it is one of the best statistical methods to detect shadows from a single image. <ref type="figure" target="#fig_1">Fig. 2</ref> shows example results of these methods and ours.</p><p>In the third row, although the unary-pairwise method provides acceptable results in some cases, it completely fails in the first, third, fifth, and sixth columns. The fourth row of <ref type="figure" target="#fig_1">Fig. 2</ref> shows the results of Stacked-CNN, which in the  most cases are comparable with our method (the fifth row). The last row shows the binary shadow mask of the proposed method by a constant threshold. <ref type="table" target="#tab_1">Table I</ref> shows the quantitative results on SBU dataset. The values shown are the average of the performance metrics on all test images. Although the total accuracy of the proposed method is not the best, with respect to shadow accuracy, our method outperforms the other methods. The goal of the proposed method is providing a fast shadow detection method without losing the accuracy. Thereore, in <ref type="table" target="#tab_1">Table II</ref> we show the execution time of training and testing phases of the proposed method. Results in both <ref type="table" target="#tab_1">Tables I and II</ref> illustrate that the proposed method works an order of magnitude faster than the statistical methods and two orders of magnitude faster than the deep learning competing method with the comparable accuracy. This is almost an entirely a result of predicting shadow/non-shadow labels on super-pixels rather than pixels, even with the additional cost to pay for refining the boundary pixels.   We also evaluate the proposed method and compare it with the other methods on the combined dataset to investigate the effect of increasing the number of images for training and testing. <ref type="table" target="#tab_1">Table III</ref> shows that our method is still comparable with other methods in terms of accuracy, and <ref type="table" target="#tab_1">Table IV</ref> shows that the execution time of our method is significantly less than the other two methods, as was the case on individual datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Shadow Detection in Road Detection Application</head><p>To illustrate the utility of our method in a real application, we consider the problem of detecting shadow on roads in this section. Cast shadows on roads can cause difficulty or mistakes in the scene interpretation or segmentation for this application. To determine the performance of our method in this application, we apply the proposed method on St. Lucia and UACampus datasets to show the potential of the method in detecting shadows on roads. <ref type="figure">Figs. 3 and 4</ref> show the results of our method in terms of shadow probability maps of sample images. It is clear from these examples that our method is able to generate a probability map of high accuracy, and can serve as a useful building block for road detection in, for example, autonomous driving.</p><p>We also apply the proposed method on aerial images to detect shadows, a common problem for many applications that rely on the aerial images. <ref type="figure" target="#fig_3">Fig. 5</ref> shows the qualitative results of the proposed method to detect shadows in the aerial images. Once again, the results are highly accurate, and our method can directly contribute to solutions in aerial imaging applications. V. CONCLUSION In this paper, we have presented a method for accurately detecting shadow in a single image. Our method combines traditional color and texture features and deep learning in a novel way, and achieves start-of-the-art performance in terms of detection accuracy and out-performance state-of-the-art in terms of computational efficiency. Our method uses color and texture features to compute a shadow prior map by training an SVM. The prior map and the original input image are then used as input to a patched-CNN to compute shadow probability map, one for each super-pixel, to achieve the desired computational efficiency. In the final step, we refine the prediction result of the patched-CNN by re-estimating the class labels of boundary pixels between super-pixels with the same patched-CNN. Extensive experimental results demonstrate that the proposed method works significantly faster than the existing deep or statistical methods, by one to two orders of magnitude, without losing the accuracy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Comparison of our qualitative results with the results of other methods. Rows from top to bottom: input images, ground truths, results of unary-pairwise method, results of stacked-CNN, obtained probanility map of our method, binary mask of shadows based on the probability map of our method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .Fig. 4 .</head><label>34</label><figDesc>Qualitative results of the proposed method on smaple images of the St.Lucia dataset. Qualitative results of the proposed method on smaple images of the UACampus dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Qualitative results of the proposed method on aerial images. Last row shows the binary mask of the shadow probability map.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I EVALUATION</head><label>I</label><figDesc>OF SHADOW DETECTION METHODS ON SBU DATASET</figDesc><table><row><cell>Method</cell><cell>Accuracy/std</cell><cell>Sh-Acc/std</cell><cell>Non-sh-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Acc/std</cell></row><row><cell>Stacked-CNN</cell><cell cols="3">0.8850 / 0.13 0.8609 / 0.23 0.9059 / 0.15</cell></row><row><cell>Unary-Pairwise</cell><cell cols="3">0.8639 / 0.14 0.5636 / 0.35 0.9357 / 0.12</cell></row><row><cell>Our method</cell><cell cols="3">0.8664 / 0.14 0.8987 / 0.20 0.8773 / 0.15</cell></row><row><cell></cell><cell cols="2">TABLE II</cell><cell></cell></row><row><cell cols="4">TIME COMPLEXITY OF SHADOW DETECTION METHODS ON SBU</cell></row><row><cell></cell><cell cols="2">DATASET, USING GPU</cell><cell></cell></row><row><cell>Method</cell><cell>Testing</cell><cell>Testing</cell><cell>Training</cell></row><row><cell></cell><cell>(hours)</cell><cell>(sec/image)</cell><cell>(hours)</cell></row><row><cell>Stacked-CNN</cell><cell>25.08</cell><cell>141.56</cell><cell>9.4+TrFCN</cell></row><row><cell>Unary-Pairwise</cell><cell>9.13</cell><cell>51.56</cell><cell>-</cell></row><row><cell>Our method</cell><cell>0.33</cell><cell>1.87</cell><cell>3.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III EVALUATION</head><label>III</label><figDesc></figDesc><table><row><cell cols="4">OF SHADOW DETECTION METHODS ON COMBINED</cell></row><row><cell></cell><cell cols="2">DATASET</cell><cell></cell></row><row><cell>Method</cell><cell>Accuracy/std</cell><cell>Sh-Acc/std</cell><cell>Non-sh-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Acc/std</cell></row><row><cell>Stacked-CNN</cell><cell cols="3">0.9044 / 0.12 0.8614 / 0.18 0.9140 / 0.13</cell></row><row><cell>Unary-Pairwise</cell><cell cols="3">0.8835 / 0.13 0.6374 / 0.32 0.9366 / 0.11</cell></row><row><cell>Our method</cell><cell cols="3">0.9103 / 0.11 0.8527 / 0.20 0.9248 / 0.11</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV TIME</head><label>IV</label><figDesc></figDesc><table><row><cell cols="4">COMPLEXITY OF SHADOW DETECTION METHODS ON COMBINED</cell></row><row><cell></cell><cell cols="2">DATASET, USING GPU</cell><cell></cell></row><row><cell>Method</cell><cell>Testing</cell><cell>Testing</cell><cell>Training</cell></row><row><cell></cell><cell>(hours)</cell><cell>(sec/image)</cell><cell>(hours)</cell></row><row><cell>Stacked-CNN</cell><cell>27.77</cell><cell>78.75</cell><cell>9.08+TrFCN</cell></row><row><cell>Unary-Pairwise</cell><cell>20.77</cell><cell>58.87</cell><cell>-</cell></row><row><cell>Our method</cell><cell>0.55</cell><cell>1.55</cell><cell>3.05</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fast and inexpensive color image segmentation for interactive robots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Balch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Veloso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Robots and Systems (IROS), IEEE/RSJ International Conference on. 200</title>
		<imprint>
			<biblScope unit="page">2066</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Moving object detection in time-lapse or motion trigger image sequences using low-rank and invariant sparse decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shakeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">5131</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Visual place recognition: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lowry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sunderhauf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leonard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Corke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Milford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Robot</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">119</biblScope>
			<date type="published" when="2015-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Detection of small moving objects using a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shakeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Robots and Systems (IROS), IEEE/RSJ International Conference on</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2777" to="2782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">COROLA: a sequential solution to moving object detection using low-rank approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shakeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding journal (CVIU)</title>
		<imprint>
			<biblScope unit="volume">146</biblScope>
			<biblScope unit="page" from="27" to="39" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dealing with shadows: Capturing intrinsic scene appearance for image-based outdoor localisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Corke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Churchill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Robots and Systems (IROS), IEEE/RSJ International Conference on</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">2092</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Online loop-closure detection via dynamic sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shakeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Field and Service Robotics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="125" to="139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Illumination invariant imaging: Applications in robust vision-based localisation, mapping and classification for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Maddern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mcmanus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Upcroft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Churchill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Visual Place Recognition in Changing Environments, (ICRA)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Entropy minimization for shadow removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Finlayson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Drew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">57</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">On the removal of shadows from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Finlayson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Hordley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Drew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">68</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Shadow detection based on colour segmentation and estimated illumination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Schofield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Wyatt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Illumination invariant representation of natural images for visual place recognition&quot; Intelligent Robots and Systems (IROS)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shakeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page">472</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Recovering intrinsic scene characteristics from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Barrow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision Systems</title>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1978" />
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Recovering intrinsic scene characteristics from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Barrow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
		<editor>Computer Vision Systems, A. Hanson and E. Riseman</editor>
		<imprint>
			<date type="published" when="1978" />
			<publisher>Academic Press</publisher>
			<biblScope unit="page" from="3" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Detecting ground shadows in outdoor consumer photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-F</forename><surname>Lalonde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narasimhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV) 2010</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">335</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Single-image shadow detection and removal using paired regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), IEEE Conference on</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">2040</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Single image shadow detection using multiple cues in a supermodular mrf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F Y</forename><surname>Vicente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-P</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning to recognize shadows in monochromatic natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Masood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Tappen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), IEEE Conference on</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">230</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Automatic feature learning for robust shadow detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Togneri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), IEEE Conference on</title>
		<imprint>
			<date type="published" when="1946" />
			<biblScope unit="page">1939</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Automatic shadow detection and removal from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Togneri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page">446</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Leave-one-out Kernel Optimization for Shadow Detection and Removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F Y</forename><surname>Vicente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page">695</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Shadow optimization from structured deep edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Wee</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Leman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">2074</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Mean shift: A robust approach toward feature space analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page">619</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning to detect natural image boundaries using local brightness, color, and texture cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page">549</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Libsvm: a library for support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM transactions on intelligent systems and technology (TIST)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Large-scale training of shadow detectors with noisily-annotated shadow examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F Y</forename><surname>Vicente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">832</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">18th International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">282</biblScope>
			<biblScope unit="page">289</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fast approximate energy minimization via graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Veksler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zabih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page">1239</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">FAB-MAP+RatSLAM: Appearance-based SLAM for Multiple Times of Day</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Glover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Maddern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Milford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wyeth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
