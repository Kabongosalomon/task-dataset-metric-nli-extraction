<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">KERCNNS: BIOLOGICALLY INSPIRED LATERAL CONNECTIONS FOR CLASSIFICATION OF CORRUPTED IMAGES</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-10-18">18 Oct 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noemi</forename><surname>Montobbio</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dipartimento di Matematica</orgName>
								<orgName type="institution">Università di Bologna</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">CAMS Center of Mathematical Analysis</orgName>
								<orgName type="institution" key="instit2">CNRS-EHESS</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Bonnasse-Gahot</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">CAMS Center of Mathematical Analysis</orgName>
								<orgName type="institution" key="instit2">CNRS-EHESS</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanna</forename><surname>Citti</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dipartimento di Matematica</orgName>
								<orgName type="institution">Università di Bologna</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>AND</roleName><forename type="first">Alessandro</forename><surname>Sarti</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">CAMS Center of Mathematical Analysis</orgName>
								<orgName type="institution" key="instit2">CNRS-EHESS</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">KERCNNS: BIOLOGICALLY INSPIRED LATERAL CONNECTIONS FOR CLASSIFICATION OF CORRUPTED IMAGES</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-10-18">18 Oct 2019</date>
						</imprint>
					</monogr>
					<note>1 KERCNNS 2</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CNNs</term>
					<term>Image classification</term>
					<term>Visual cortex</term>
					<term>Correlation kernels</term>
					<term>Neuroge- ometry</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The state of the art in many computer vision tasks is represented by Convolutional Neural Networks (CNNs). Although their hierarchical organization and local feature extraction are inspired by the structure of primate visual systems, the lack of lateral connections in such architectures critically distinguishes their analysis from biological object processing. The idea of enriching CNNs with recurrent lateral connections of convolutional type has been put into practice in recent years, in the form of learned recurrent kernels with no geometrical constraints. In the present work, we introduce biologically plausible lateral kernels encoding a notion of correlation between the feedforward filters of a CNN: at each layer, the associated kernel acts as a transition kernel on the space of activations. The lateral kernels are defined in terms of the filters, thus providing a parameter-free approach to assess the geometry of horizontal connections based on the feedforward structure. We then test this new architecture, which we call KerCNN, on a generalization task related to global shape analysis and pattern completion: once trained for performing basic image classification, the network is evaluated on corrupted testing images. The image perturbations examined are designed to undermine the recognition of the images via local features, thus requiring an integration of context information -which in biological vision is critically linked to lateral connectivity. Our KerCNNs turn out to be far more stable than CNNs and recurrent CNNs to such degradations, thus validating this biologically inspired approach to reinforce object recognition under challenging conditions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Convolutional Neural Networks (CNNs) are a powerful tool that provides outstanding performances on image classification tasks. Major advances have been made since their introduction in the 1980s <ref type="bibr" target="#b14">(Fukushima, 1980)</ref>, thanks to the availability of large-scale datasets, as well as efficient GPU implementations and new regularization schemes. A notable example in this respect is the huge improvement of the state of the art performance reached by <ref type="bibr" target="#b25">Krizhevsky et al. (2012)</ref> on the ImageNet 2012 classification benchmark. However, there is still little insight into how the learning process of these algorithms develops and how the features of the data are encoded in the network structure. Some visualization techniques have been developed, such as the "deconvolution"-based projection of activations onto the pixel space proposed by <ref type="bibr" target="#b49">Zeiler and Fergus (2014)</ref>, to identify the input stimuli that excite each feature map at a given layer of the network. Although this may provide some intuition on internal operations and simplify the diagnosis of the limitations of the models, there is still much to be understood about how exactly image information is coded in CNNs, and notably about how their functioning is related to human object processing. Indeed, although CNN models were initially inspired <ref type="bibr" target="#b14">(Fukushima, 1980;</ref><ref type="bibr" target="#b28">LeCun et al., 1989</ref>) by the hierarchical model of the visual system of <ref type="bibr" target="#b21">Hubel and Wiesel (1962)</ref>, they display critical discrepancies w.r.t. biological vision in both structure and feature analysis.</p><p>In <ref type="bibr" target="#b4">Baker et al. (2018)</ref> the authors show that, unlike in human vision, global shapes have surprisingly little impact on the classification output of the net: that is, CNNs turn out to learn mostly from local features. As such, CNN architectures are very unstable to small local perturbations, even when the global structure of the image is preserved and its content is still easily recognizable by a human observer. Along the same lines, it has been recently shown by <ref type="bibr" target="#b8">Brendel and Bethge (2019)</ref> that a very good classification accuracy on the ImageNet dataset can be reached through a model that only relies on the occurrences of local features, with no information on their spatial location in the image.</p><p>Another key strand in unraveling the shortcomings in the internal processing of CNNs is the one related to "adversarial attacks": it has been shown <ref type="bibr" target="#b45">(Szegedy et al., 2014</ref>) that a CNN can be caused to completely misclassify an image when it is perturbed in a way imperceptible to humans, but specifically designed to maximize the prediction error. A similar study is presented in <ref type="bibr" target="#b35">Nguyen et al. (2015)</ref>, where the authors produce images that are unrecognizable to the human eye, but get labeled as one specific object class with high confidence by state-of-the-art CNNs.</p><p>Besides, although the overall convolutional architecture has much in common with the process of feature extraction carried out in the visual pathways, its structure implements a purely feedforward mechanism. On the contrary, the human visual system is well known to rely on both lateral (intra-layer) and feedback (top-down) recurrent connections for processes that are critical for object recognition, such as contour integration or figure-ground segregation <ref type="bibr" target="#b15">(Gilbert et al., 1996;</ref><ref type="bibr" target="#b18">Grossberg and Mingolla, 1985;</ref><ref type="bibr" target="#b34">Neumann and Mingolla, 2001;</ref><ref type="bibr" target="#b27">Layton et al., 2014)</ref>. In recent years, several models have been proposed in which CNN architectures were enriched with some recurrent mechanism inspired by biological visual systems. In <ref type="bibr" target="#b46">Tang et al. (2018)</ref>, pre-trained feedforward models were augmented with a Hopfield-like recurrent mechanism acting on the activation of the last layer, to improve their performance in pattern completion: partially visible objects converge to fixed attractor points dictated by the original whole objects. <ref type="bibr" target="#b30">Liang and Hu (2015)</ref> introduced a "Recurrent CNN" architecture, where lateral connections of convolutional type are inserted in a regular feedforward CNN. A systematic analysis of the effect of adding lateral and/or feedback connections has been carried out by <ref type="bibr" target="#b44">Spoerer et al. (2017)</ref>, where the resulting architectures are trained and tested on a task of classification of cluttered digits.</p><p>In Recurrent CNNs, lateral connections are learned, and no geometrical prior (apart from the ones given by the convolutional structure) is inserted. As such, these connections are determined by additional parameters that are completely independent of the feedforward architecture.</p><p>In this work, we propose to modify the classical CNN architecture by inserting lateral connections defined by structured kernels, containing precise geometric information specific of each layer. The new architecture will be referred to as KerCNN. The kernel associated to a convolutional layer implements a measure of correlation between neurons of that layer, inspired by the connectivity model of <ref type="bibr">Montobbio et al. (2019a,b)</ref>, and acts as a transition kernel on the corresponding activation. As will be discussed in Section 3.1, the lateral contribution is defined by an iterative update rule similar to the recurrent mechanism of <ref type="bibr" target="#b30">Liang and Hu (2015)</ref>, although carefully modified to implement a biologically plausible propagation of neural activity. Most importantly, the lateral kernels themselves are not learned, but rather they are constructed to allow diffusion in the metric defined by the learned filters. In particular, they establish a link between the geometrical properties of feedforward connections and horizontal connectivity, being defined as a function of the convolutional filters. This also implies that such kernels do not depend on any additional trainable parameters: therefore, their insertion does not increase the original network's complexity in terms of number of parameters, which allows a fair comparison in performance.</p><p>The main point that we wish to make is that the insertion of these connections allows the networks to spontaneously implement perceptual mechanisms of global shape analysis and completion. Therefore, we shall examine the ability of the models to generalize an image classification task to data corrupted by a variety of different perturbations: these include occlusions (as in <ref type="bibr" target="#b46">Tang et al., 2018)</ref>, local contour disruption (as in <ref type="bibr" target="#b4">Baker et al., 2018)</ref> and adversarial attacks via the Fast Gradient Sign Method (FGSM) of <ref type="bibr" target="#b17">Goodfellow et al. (2015)</ref>.</p><p>We stress that the data perturbations are only inserted in the testing phase -that is, the models are not optimized to classify corrupted images.</p><p>We will fix a base 2-layer CNN model and modify it by inserting our structured lateral connections in one or both layers. We will then compare the performance of the base CNN with the one of the different KerCNN models, obtained by varying the number of iterations of the update rule for each layer. We first present an extensive analysis of our results for the classical MNIST dataset <ref type="bibr" target="#b29">(LeCun et al., 1998)</ref>. As will be shown in Section 4.1, KerCNNs turn out to improve the base CNN's classification accuracy on degraded images by up to ∼ 25 points, while preserving the same performance on the original (not corrupted) testing images. We also compare the KerCNN models with the "RecCNN" obtained by adding recurrent connections to the base model as in <ref type="bibr" target="#b44">Spoerer et al. (2017)</ref> -where the number of parameters of the networks is matched by decreasing the size of feedforward filters, to compensate for the additional recurrent parameters. In particular, for each task we inspect the performance of the best KerCNN and RecCNN architectures (i.e. the ones with the optimal number of iterations), and our results show that our biologically inspired model outperforms the recurrent one in practically all experiments, see Section 4.1.6. We will conclude the paper by giving a synthetic account on the same study carried out on different datasets, namely Kuzushiji-MNIST <ref type="bibr" target="#b11">(Clanuwat et al., 2018)</ref>, Fashion-MNIST <ref type="bibr" target="#b48">(Xiao et al., 2017)</ref> and <ref type="bibr">CIFAR-10 (Krizhevsky, 2009)</ref>. The choice of the two MNIST-like datasets was driven by their being very homogeneous, allowing for a meaningful interpretation of our results in terms of the characterizing features of the images. On the other hand, our tests on CIFAR-10 show that this technique can be extended to richer datasets, and notably to natural images.</p><p>A noteworthy feature of our model is that it somewhat links two approaches to image treatment that are classically seen as opposites, namely "geometrical" methods and "datadriven" methods. The former rely on a priori assumptions based on mathematical modeling either of the structure of the data or of the task, e.g. variational techniques for inpainting <ref type="bibr" target="#b2">(Ambrosio and Masnou, 2003;</ref><ref type="bibr" target="#b6">Bertalmio et al., 2000)</ref>; the latter instead are designed to learn patterns and convenient representations from the statistics of a dataset through optimization of a loss function related to the task. In KerCNNs these two aspects coexist, since the metric structure that we define on each layer of the network is directly induced by the learned filters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminaries</head><p>In this section, we shall give an overview on some biological notions and computational methods that will be of interest throughout the paper. CNNs are a particular kind of deep neural network architecture, designed in analogy with information processing in biological visual systems <ref type="bibr" target="#b14">(Fukushima, 1980;</ref><ref type="bibr" target="#b28">LeCun et al., 1989)</ref>. In addition to the hierarchical organization typical of deep architectures, translation invariance is enforced in CNNs by local convolutional windows shifting over the spatial domain. This structure was inspired by the localized receptive profiles of neurons in the early visual areas, and by the approximate translation invariance in their tuning. Although the analogy with biological vision is strong, the feedforward mechanism implemented in CNNs is a simplified one, and it does not take into account all of the processes contributing towards the interpretation of a visual scene.</p><p>In the following, we first describe some structures of the visual pathways with a focus on the primary visual cortex (V1), and review some mathematical models of vision; we then recall the main features of feedforward convolutional architectures, and finally outline the Recurrent CNN models of <ref type="bibr" target="#b30">Liang and Hu (2015)</ref> and <ref type="bibr" target="#b44">Spoerer et al. (2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Feedforward and lateral connectivity of V1. The primary visual cortex (V1)</head><p>implements the first stage of cortical processing of a visual stimulus. It receives the retinal signal after a first subcortical processing stage, and it sends information to "higher" cortical areas performing further processing. These junctions form a connectivity of feedforward type, since they link the zones of the visual pathways in a sequential way, generating a hierarchy starting from the retina.</p><p>Through the above-mentioned connections, each visual neuron is linked to a specific domain D of the retina which is referred to as its receptive field (RF). The reaction of a cell to a punctual luminous stimulation applied at a point (x, y) ∈ D can be of excitatory or inhibitory type, with different modulation: this can be described by a function ψ : D → R, called the receptive profile (RP) of the cell, whose values are positive when the cell is excited and negative when it is inhibited. The RPs of certain types of visual neurons are shown to act, at least to a first approximation, as linear filters on the optic signal. This means that the response of the cell to a visual stimulus I, defined as a function on the retina, is given by the integral of I against the profile ψ of the neuron, computed over its receptive field D:</p><p>h := D I(x, y)ψ(x, y)dxdy.</p><p>(1)</p><p>In these cases, the shape of the RP contains information about the features that it extracts from a visual signal. For example, the local support 1 of ψ makes it sensitive to position,</p><p>i.e. the neuron only responds to stimuli in a localized region of the image. Or again, a receptive profile with an elongated shape will be sensitive to a certain orientation, i.e. it will respond strongly to stimuli consisting of bars aligned with this shape. This is the case for simple cells, a class of neurons of V1 showing orientation selectivity due to their strongly anisotropic RPs, first discovered by <ref type="bibr" target="#b21">Hubel and Wiesel (1962)</ref>.</p><p>The processing performed by the human visual system allows to efficiently group local items into extended contours, and to segregate a path of elements from its background.</p><p>This implies that the perception of a local edge element in a visual stimulus is influenced by the perception of the surrounding oriented components: this perceptual phenomenon has been described through the concept of association field <ref type="bibr" target="#b13">(Field et al., 1993)</ref>, characterizing the geometry of the mutual influences between local oriented elements in the perceived image, depending on their orientation and reciprocal position. These psychophysical experiments suggest that a local analysis is not sufficient to correctly interpret a visual scene:</p><p>from the physiological point of view, this means that the activity of V1 neurons is not only influenced by the feedforward signal received from the preceding visual areas, but also by intracortical connections with surrounding V1 cells. In fact, the reciprocal influences described by association fields are thought to be neurally implemented in V1 through a kind of long-range connections referred to as lateral (or horizontal ), whose orientation specificity and spatial extent is compatible with association fields. Indeed, V1 horizontal connections show facilitatory influences for cells that are similarly oriented; moreover, the connections departing from each neuron spread anisotropically, concentrating along the axis of its preferred orientation (see e.g. <ref type="bibr" target="#b7">Bosking et al., 1997)</ref>.</p><p>1 The support of a function ψ is defined as the closure of the set {x : ψ(x) = 0}.</p><p>The functional architecture of V1 and the related perceptual phenomena have been described in a variety of mathematical models. The set of RPs of simple cells is typically represented by a bank of linear filters {ψ p } p∈G ⊆ L 2 (R 2 ), where G is a set of indices parameterizing the family. Each p ∈ G can be thought of as representing the features extracted by the filter ψ p : in these terms, we can refer to G as the feature space associated to the bank of filters {ψ p } p∈G . This set often has the product form G = R 2 × F, where the parameters (x, y) ∈ R 2 determine the retinal location of the RF, while f ∈ F represents the other local image features extracted by each filter. This is the case for the work of <ref type="bibr" target="#b9">Bressloff and Cowan (2003)</ref>, where each V1 cell is labeled by a spatial index and a "feature index", and the evolution in time t → a(p, t) of the activity of the neural population at p ∈ R 2 × F is assumed to satisfy a Wilson-Cowan equation <ref type="bibr" target="#b47">(Wilson and Cowan, 1972)</ref>:</p><formula xml:id="formula_0">∂ t a(p, t) = −α a(p, t) + s φ(p, p )a(p , t)dp + h(p, t) .<label>(2)</label></formula><p>Here, s is a nonlinear activation function; α is a decay rate; h is the feedforward input corresponding to the response of the simple cells in presence of a visual stimulus, as in (1); and the kernel φ weights the strength of horizontal connections between p and p . A possible way to obtain a measure of this connectivity is by means of differential geometry tools. A breakthrough idea in this direction has been that of viewing the feature space G = R 2 × F as a fiber bundle with basis R 2 and fiber F. This approach first appeared in the works of <ref type="bibr" target="#b23">Koenderink and van Doom (1987)</ref> and <ref type="bibr" target="#b20">Hoffman (1989)</ref>. It was then further developed by <ref type="bibr" target="#b37">Petitot and Tondut (1999)</ref> and <ref type="bibr" target="#b10">Citti and Sarti (2006)</ref>. In the latter work, the model is written in the Lie group R 2 × S 1 by requiring the invariance under roto-translations:</p><p>here, the feature index explicitly represents a local orientation θ. More generally, it can also contain information about other variables such as scale, curvature or even velocity (see e.g. <ref type="bibr" target="#b41">Sarti et al., 2008;</ref><ref type="bibr" target="#b0">Abbasi-Sureshjani et al., 2018;</ref><ref type="bibr" target="#b5">Barbieri et al., 2014)</ref>. Another strand of research is linked to statistics of natural images (see e.g. <ref type="bibr" target="#b3">August and Zucker, 2000;</ref><ref type="bibr" target="#b26">Kruger, 1998;</ref><ref type="bibr" target="#b43">Sigman et al., 2001;</ref><ref type="bibr" target="#b39">Sanguinetti et al., 2010)</ref>. In <ref type="bibr" target="#b39">Sanguinetti et al. (2010)</ref>, the statistics of edge co-occurrence in natural images are fitted to a Fokker-Planck kernel in R 2 × S 1 ; such kernel has been proposed as a connectivity weight φ to insert in (2) by <ref type="bibr" target="#b40">Sarti and Citti (2015)</ref>.</p><p>2.1.1. A kernel model for lateral connectivity. A different connectivity kernel, induced by a structure of metric space associated to the RPs of simple cells, has been introduced in <ref type="bibr" target="#b31">Montobbio et al. (2019a)</ref>. The core of the model is the definition of a kernel describing the interactions between local elements, which determines a metric structure directly induced by the shape of the RPs of V1 simple cells. This local correlation kernel is then propagated through an iterative procedure, to yield a wider kernel modeling long-range connections.</p><p>The starting point is a family of filters {ψ p } p ∈ G ⊆ L 2 (R 2 ) modeling the set of V1 simple cells. The local connectivity of V1 is represented by the following generating kernel on G × G:</p><formula xml:id="formula_1">K(p, p 0 ) := Re ψ p , ψ p 0 L 2 = Re R 2 ψ p (x, y) ψ p 0 (x, y) dx dy ∀p, p 0 ∈ G.<label>(3)</label></formula><p>The kernel K is constructed to provide a measure of correlation between RPs. In fact, if the filters are normalized to have squared L 2 -norm equal to some number η &gt; 0, then</p><formula xml:id="formula_2">d(p, p 0 ) := ψ p − ψ p 0 2 L 2 = 2 η − K(p, p 0 ) .</formula><p>This means that K expresses the correlation w.r.t. the L 2 distance between the filters.</p><p>Note that this also defines a metric d onto the feature space.</p><p>The generating kernel has a local sense, since it only describes the reciprocal influences between simple cells with overlapping RFs. The action of K is then iterated to model the long-range connectivity. Given a starting point p 0 , the local kernel around it is first passed through a nonlinear activation function ν and a normalization operator N (see also <ref type="bibr" target="#b12">Coifman and Lafon, 2006)</ref>, thus defining:</p><formula xml:id="formula_3">K p 0 1 (p) := N [ν(K)](p, p 0 ).<label>(4)</label></formula><p>The iterative procedure yielding the propagation is then given by</p><formula xml:id="formula_4">K p 0 n (p) := G N [ν(K)](p, q) K p 0 n−1 (q)dµ(q) ∀n &gt; 0.<label>(5)</label></formula><p>Here, µ is the spherical Hausdorff measure <ref type="bibr" target="#b19">(Hausdorff, 1918)</ref> associated to the distance d on G.</p><p>The geometrical structure encoded in this kernel is shown in <ref type="bibr" target="#b31">Montobbio et al. (2019a)</ref> to be compatible with the properties of V1 horizontal connections, and with the perceptual principles synthesized by association fields. Results are also shown for a bank of filters arising from an unsupervised learning algorithm: this shows that meaningful information of the geometry of horizontal connections can be recovered from numerically known filters, thus motivating the present work.</p><p>We conclude this section with an important remark on the action of the correlation kernel of <ref type="bibr" target="#b31">Montobbio et al. (2019a)</ref> as an operator acting on functions defined on G. Given a function</p><formula xml:id="formula_5">F 0 : G −→ R,</formula><p>the action of the propagated kernel onto F can then be expressed by</p><formula xml:id="formula_6">H n [F ](p 0 ) := G K p 0 n (p) F 0 (p)dµ(p).<label>(6)</label></formula><p>Note that, by substituting Eq. (5) into Eq. (6), we get:</p><formula xml:id="formula_7">H n [F ](p 0 ) = H 1 . . . H 1 n [F ](p 0 ).</formula><p>This means that applying the n-th step kernel to F 0 is equivalent to applying n times the local kernel to F 0 . In the following, we will take as functions F 0 the activations obtained by mapping a signal to a feature space. In the case of V1, this signal is a retinal image I and the activation in presence of I is a function of the cortical coordinates p ∈ G:</p><formula xml:id="formula_8">F 0 (p) := s I(x, y)ψ p (x, y)dxdy ,</formula><p>where s is a nonlinear activation function. Updating this activation through the connectivity kernel means taking into account the contextual influences in modeling the response of V1 to the image I.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.2.</head><p>CNNs for image classification. In order to fix the notations that will be used in later sections, we recall here the typical structure of a CNN, with a focus on image classification tasks. We refer to <ref type="bibr" target="#b38">Rawat and Wang (2017)</ref> for an exhaustive review on this topic. As mentioned in Section 2.1, the processing of early visual areas is classically modeled as the mapping of an image to a feature space through a bank of filters with a localized support. The first convolutional layer of a CNN implements an analogous mechanism, typically defined as follows:</p><formula xml:id="formula_9">h 1 (i, j, k) = s   i ,j ,c ψ 1 k (i , j , c) · I(i − i , j − j , c) + b 1 (k)   ,<label>(7)</label></formula><p>where s is a nonlinear activation function. A popular choice for it in recent literature is the Rectified Linear Unit (ReLU) s(z) := max(0, z) <ref type="bibr" target="#b33">(Nair and Hinton, 2010;</ref><ref type="bibr" target="#b25">Krizhevsky et al., 2012)</ref>. Here, the input image I is an H 0 × W 0 × n 0 tensor, where H 0 and W 0 denote the height and width of the image in pixels, while n 0 is the number of channels:</p><formula xml:id="formula_10">n 0 = 1 if I is a grayscale image, n 0 = 3 if it is RGB. The bank of filters {ψ 1 k } k=1,...,n 1 of the first layer is a d 1 × d 1 × n 1 × n 0 tensor: d 1 × d 1</formula><p>is the spatial size of the filters, n 1 is the number of filters and n 0 is the number of channels of each filter, matching the number of channels of the input. The convolution between I and the filters ψ 1 k gives an H 1 × W 1 × n 1 tensor, to which a bias vector b 1 ∈ R n 1 is added along the third component, to obtain the output h 1 of the layer. Note that the number of filters n 1 defines the number of channels of the output of the layer. Written in a more compact notation, Eq. (7) reads:</p><formula xml:id="formula_11">h 1 = s(ψ 1 * I + b 1 ).</formula><p>The subsequent convolutional layers are defined similarly: for each l ∈ {1, . . . , L} we have a bank of filters {ψ l k } k=1,...,n l defined by a d l × d l × n l × n l−1 tensor, and a bias vector b l of length n l . The number of channels of the filters varies across layers according to the number of channels of the inputs they receive. In particular, since the output h l−1 of the (l−1)-th layer has n l−1 channels, each of the filters ψ l k applied to it must have n l−1 channels as well. The activation of the l-th layer in terms of the output h l−1 of the preceding layer is given by</p><formula xml:id="formula_12">h l = s(ψ l * h l−1 + b l ).</formula><p>Another layer that can optionally be interposed between convolutional layers consists of the application of a pooling operator P: this performs a downsampling of its input over the spatial variables (i.e. the "depth" dimension remains unchanged), typically by taking the maximum or by averaging over small neighborhoods. For instance, if a pooling layer is applied to an activation h l of size H l × W l × n l over 2×2 squares, then the output P(h l ) will be an H l 2 × W l 2 × n l tensor. This downsampling operation reduces the dimensionality and introduces invariance to small shifts and distortions. The insertion of pooling layers has a neural motivation as well: the receptive fields of visual neurons tend to get wider and wider moving towards higher cortical layers, and subsampling the spatial dimension of a feature space is equivalent to taking filters with a wider support in the next layer.</p><p>The final layer of the network is typically fully connected : the output h L of the last convolutional layer, which is a tensor of size H L × W L × n L , is "flattened" to a vectorh L of length S = H L · W L · n L and transformed as follows:</p><formula xml:id="formula_13">Ψ ·h L + b,</formula><p>where Ψ is an S × n matrix of trainable weights and b is a bias vector of length n. This yields a vector of length n as output: in the case of multiclass classification, n must be the number of classes. It is also not uncommon to have multiple fully connected layers, with nonlinear activation functions interposed between them -in this case, only the length of the last output vector needs to match the number of classes. A softmax function ρ is then typically applied to the final output vector:</p><formula xml:id="formula_14">ρ(v) i = e v i j e v j .</formula><p>The softmax function gives a vector whose entries are real numbers between 0 and 1 that sum to 1: this can be interpreted as a vector of probabilities, where each entry represents the "score" of the corresponding class.</p><p>The most common loss function for multiclass classification is the cross entropy between the output y = y(I) and the target vector T = T (I) containing the "true" probabilities associated to the input I:</p><formula xml:id="formula_15">L(y, T ) = − n i=1 T i log(y i ) = − log(y i(I) ),<label>(8)</label></formula><p>where i(I) is the correct class for I. The last equality holds since T i(I) = 1 and T i = 0 for each i = i(I).</p><p>2.3. Recurrent CNNs (RecCNNs). The human visual system, as outlined in Section 2.1, relies not only on a hierarchical transmission of signals, but also on a horizontal spreading of information. On the other hand, the sequential structure of a CNN implements a purely feedforward mechanism: the output of each layer only depends on the activation of the preceding layer. Recurrent CNNs <ref type="bibr" target="#b30">(Liang and Hu, 2015;</ref><ref type="bibr" target="#b44">Spoerer et al., 2017)</ref>, referred to as RecCNNs in the following, are a modification of this kind of architecture, where lateral connections of convolutional type are added to a regular CNN, yielding an equation analogous to <ref type="formula" target="#formula_0">(2)</ref>. This means that the network includes not only connections from one layer to the next one, but also connections from a layer to itself, ruled by "horizontal" connectivity weights. As in (2), this is described through an evolution in time. The activation of the l-th hidden layer at time t, which we denote by h t l , is a function of:</p><p>• h t l−1 (the output of the preceding layer at the same time step t); • h t−1 l (the output of the same layer at time t − 1). Specifically, following the notations introduced for CNNs, we have:</p><formula xml:id="formula_16">h t l = s φ l * h t−1 l lateral + ψ l * h t l−1 feedforward + b l<label>(9)</label></formula><p>for all t, l &gt; 0, where h t 0 = I for all t &gt; 0, and h 0 l ≡ 0 for all l &gt; 0. Here, φ l denotes the bank of convolutional filters defining the lateral connections at the l-th layer. Note that their introduction results in an additional set of parameters in the architecture w.r.t. a standard CNN.</p><p>Recurrent neural networks (RNNs) are often employed to process sequential inputs, e.g. audio recordings, video or text. In such cases, a new input I t = h t 0 is fed into the network at each time step. On the contrary, in RecCNNs the input image I is static, i.e. it is kept fixed at each time step: the time variable only affects the processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Kernel CNNs (KerCNNs)</head><p>The lateral connections in RecCNNs are completely learned and independent of the feedforward ones. As such, the inclusion of these connections in a CNN increases its complexity in terms of trainable parameters. We propose a different modification of a CNN, obtained by introducing convolutional lateral connections with kernels constructed according to the connectivity model of <ref type="bibr">Montobbio et al. (2019a,b</ref>) (see Section 2.1.1). We shall refer to this architecture as KerCNN. In the following, we first outline the proposed network architecture; we then introduce a testing framework to analyze the performance of the networks in three tasks of classification of corrupted images, focusing on types of image degradation where mechanisms of perceptual completion and global object analysis are required for correct classification.</p><p>3.1. Network architecture. The idea of the KerCNN architecture is to transpose the notion of connectivity of <ref type="bibr">Montobbio et al. (2019a,b)</ref>, and notably the propagation of Eq.</p><p>(6), into the structure of a CNN. The lateral kernel K l associated to the l-th convolutional layer is defined as follows. First, a correlation kernelK l is computed by taking the L 2 scalar product between the filters ψ l :</p><formula xml:id="formula_17">K l (i, j, f, g) = ν i ,j ,c ψ l f (i , j , c) · ψ l g (i − i , j − j , c) ,<label>(10)</label></formula><p>where ν is the sigmoidal activation function</p><formula xml:id="formula_18">ν(z) = 1 1 + e −z .</formula><p>The indices in the sum are let vary as long as the product ψ l</p><formula xml:id="formula_19">f (i , j , c) · ψ l g (i − i , j − j , c) does not vanish. Therefore, if the size of the bank of filters ψ l is d l × d l × n l × n l−1 , then the size of the kernel is obtained as (2d l − 1) × (2d l − 1) × n l × n l . The final kernel is then obtained as K l (i, j, f, g) = N [K l ](i, j, f, g),</formula><p>where N is the same normalization operator introduced by <ref type="bibr" target="#b12">Coifman and Lafon (2006)</ref> and appearing in <ref type="bibr" target="#b31">Montobbio et al. (2019a)</ref>, see Eq. (4). Specifically, in the current case of a discrete, translation-invariant kernel K (i, j, f ), (0, 0, g) =K l (i, j, f, g), the operator reads:</p><formula xml:id="formula_20">N [K l ](i, j, f, g) :=K l (1) (i, j, f, g) i ,j ,g K l (1) (i , j , f, g ) , whereK l (1) (i, j, f, g) =K l (i, j, f, g) i ,j ,f K l (i , j , f , g) i ,j ,g K l (i , j , f, g ) .</formula><p>The update rule of a KerCNN layer is inspired by the iterative procedure outlined in the preceding section, designed to model the propagation of neural activity in V1:</p><formula xml:id="formula_21">     h 1 l = s(ψ l * h T l−1 l−1 + b l ) h t l = 1 2 K l * h t−1 l + h t−1 l for 1 &lt; t ≤ T l .<label>(11)</label></formula><p>The output of the (l − 1)-th layer is first mapped to the l-th feature space through a feedforward step, yielding an activation h 1 l , which is then updated through convolution with the kernel K l , as in <ref type="formula" target="#formula_6">(6)</ref>. The new output h 2 l is defined by averaging between this updated activation K l * h 1 l and the original activation h 1 l . Note again an analogy with Eq. (2). The same procedure is repeated, yielding a sequence of activations h t l , until a fixed stopping time T l is reached. Note that each layer has its own stopping time: this yields a different KerCNN architecture for each combination of the stopping times (T 1 , . . . , T L ) of the layers. If all stopping times are 1, the model coincides with the base CNN. We remark that convolutions with the kernel K l are taken with appropriate zero padding, so that the size of h t l is preserved at every iteration. The intuitive idea here is that K l behaves like a "transition kernel" on the feature space of the l-th layer, slightly modifying its output according to the correlation between its filters: the activation of a filter encourages the activation of other filters highly correlated with it.</p><p>3.2. Task: stability to corrupted images. We will show that the insertion of such structured lateral connections improves the performance of a CNN in tasks related to perceptual mechanisms of global shape analysis and integration. In particular, we focus on classification of corrupted images. Given a labeled image dataset, each model is trained in a supervised way to perform classification. No corruption is applied to the images during the training phase. The actual experiment consists in analyzing the ability of the models to generalize the classification to the degraded images, by comparing their classification accuracy on corrupted testing images. We examine the following different kinds of image corruption.</p><p>(1) Gaussian patches occluding the image, similar to the ones in <ref type="bibr" target="#b46">Tang et al. (2018)</ref>.</p><p>(2) Disruption of local contours, in analogy with the study presented by <ref type="bibr" target="#b4">Baker et al. (2018)</ref>, obtained by subdividing the image into horizontal or vertical strips and by shifting each of these strips by a random number of pixels d ∈ {0, . . . , D}.</p><p>(3) Adversarial attacks through the Fast Gradient Sign Method (FGSM) of <ref type="bibr" target="#b17">Goodfellow et al. (2015)</ref>. FGSM, one of the most popular attack methods, simply adjusts the input image by taking a gradient ascent step to maximize the loss function.</p><p>Precisely, the perturbed image I is obtained as</p><formula xml:id="formula_22">I = I + ε · sign ∇ I L(y(I), T )<label>(12)</label></formula><p>where L is the loss function, as in Eq. (8).</p><p>In all three cases, the amount of degradation can be quantified by one parameter: the standard deviation of the Gaussian patches, the maximum displacement D of the strips, and the step ε of the FGSM. The more stable a model is to these perturbations, the slower the drop in performance w.r.t. the degradation parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>In this section, we first provide a complete analysis of the results obtained on the MNIST   4.1.2. Training details. All the models were trained with validation-based early stopping, for a maximum of 150 epochs. Adam optimizer was employed with the standard parameters indicated in <ref type="bibr" target="#b22">Kingma and Ba (2015)</ref>, a batch size of 50 and the Xavier initialization scheme <ref type="bibr" target="#b16">(Glorot and Bengio, 2010)</ref>; L 2 regularization with λ = .0005 was used. In the models including lateral connections (of any kind), recurrent dropout <ref type="bibr" target="#b42">(Semeniuta et al., 2016)</ref> with .2 probability was applied to the "horizontal" contributions. In RecCNNs, local response normalization (LRN) was applied after recurrent convolutional layers as in Liang and Hu <ref type="formula" target="#formula_0">(2015)</ref>  4.1.3. Gaussian patches. We first consider testing images corrupted by occlusions in the form of Gaussian "bubbles" at random locations over the image, similar to the ones considered by <ref type="bibr" target="#b46">Tang et al. (2018)</ref>. Specifically, the image I obtained by modifying the original input I through a patch centered at (u 1 , u 2 ) was implemented as:</p><formula xml:id="formula_23">I (u) = (I(u) − b) · (1 − g(u)) + b,</formula><p>where g(u) := 1 2πγ 2 exp (u 1 −u 1 ) 2 +(u 2 −u 2 ) 2 2γ 2 and b is the "background color", chosen to be the value at the upper left angle of each image. See <ref type="figure" target="#fig_1">Figure 1b</ref>. The number of patches per image was kept fixed to 4. In the following, we show the results of comparing the classification accuracy of the CNN and KerCNN models for varying amounts of image degradation (i.e. standard deviation γ of the Gaussian bubbles, expressed in pixels) and</p><p>for different stopping times of KerCNN.</p><p>We first examine the KerCNN defined by inserting lateral connections in the first layer of the base CNN. <ref type="figure" target="#fig_4">Figure 3a</ref> stopping times T 1 = 1, 2, 3. The chance level accuracy (10%) is displayed as well (dashed blue line). For T 1 = 1, the model is the standard CNN with no lateral connections. The mean performance of these three nets on the original testing set (γ = 0) is almost identical (99.0 ± 0.1%). On the other hand, for increasingly degraded images the performance drops dramatically for the CNN (T 1 = 1, blue curve), while decaying much more slowly for increasing values of T 1 . Note that the difference in classification accuracy between the CNN and the best KerCNN reaches ∼ 25 points. After reaching its optimal value (T 1 = 2 for γ ≤ 5 and T 1 = 3 for greater values), the performance drops again by taking further steps. For the sake of legibility, we displayed in the left plot only the curves up to the optimal value of T 1 . The behavior of classification accuracy w.r.t. T 1 ∈ {1, . . . , 6} can be best appreciated in the right plot of <ref type="figure" target="#fig_4">Figure 3a</ref>, displaying a curve for each value of the standard deviation γ: for every γ, the accuracy increases w.r.t. T 1 until a maximum is reached, and then decreases again.</p><p>We now analyze the performance of the KerCNN models with lateral connections:</p><p>• only in the second layer;</p><p>• in both layers.</p><p>Analogous to the preceding case, the optimal stopping time for the net with lateral connections in the second layer is T 2 = 2 for the original images, T 2 = 4 for a small degradation (γ = 5) and T 2 = 5 for greater values of standard deviation. <ref type="figure" target="#fig_4">Figure 3b</ref>(left) plots the accuracy against the level of degradation: we display the curves for T 2 = 1, . . . , 5; the accuracy w.r.t. stopping times T 2 ∈ {1, . . . , 6} is plotted in <ref type="figure" target="#fig_4">Figure 3b</ref>(right), where each curve corresponds to a level of image degradation. The results show the same pattern as before, although with a smaller improvement (up to ∼ 15 points between the base CNN and the model with optimal T 2 ).</p><p>It is interesting to note that the optimal number of iterations shifts towards higher values (for both layers) as the size of the occlusions increases. As mentioned before, the kernel K l can be thought of as an anisotropic transition kernel on the space of activations of the l-th layer. As such, the repeated application of the lateral contribution given by these kernels may be interpreted as a spreading of activation, around each spatial location, along those orientations that are most activated at that point. Intuitively, this "compensates" for the gaps in the activation caused by the occlusions: the wider the gap, the higher the number of iterations of the kernel needed for the image to be consistently completed.</p><p>We finally study the combinatorics of stopping times T 1 , T 2 ∈ {1, . . . , 6} in the two layers: <ref type="figure" target="#fig_5">Figure 4</ref> displays the results for different levels of image degradation. For each combination of T 1 (x-axis) and T 2 (y-axis), the mean accuracy over all trials (color-coded) is displayed.</p><p>Note that the highest values of accuracy lie on a diagonal that shifts towards higher values of both T 1 and T 2 as the level of degradation increases. It is interesting to observe that, for γ = 15, 20, 25, the optimal couple (T 1 , T 2 ), highlighted by a red star, is one involving lateral connections in both layers.</p><p>4.1.4. Local contour disruption. In <ref type="bibr" target="#b4">Baker et al. (2018)</ref>, evidence is provided that the feature extraction performed by deep CNNs mostly relies on local edge relations, rather than on global object shapes. Their experiments showed that, conversely to human vision, the networks' performance was much more robust to global shape changes preserving local features, than to a disruption of local contours preserving the global information. We hypothesized that the insertion of structured lateral connections in CNNs could make the models more robust to these local perturbations.</p><p>To automatically create a "local scrambling" of pixel information, we subdivided the images into horizontal strips and shifted each of these strips by a number of pixels d, randomly picked in {0, . . . , D}; we then repeated the procedure by subdividing the modified image into vertical strips and by shifting them as well. For a small displacement (D = 1), this produces a local degradation analogous to the one considered by <ref type="bibr" target="#b4">Baker et al. (2018)</ref>,</p><p>where the local contours are corrupted but the connected components are preserved. For increasing values of D, the image is more and more disrupted, yet still roughly preserving its global structure. See <ref type="figure" target="#fig_1">Figure 1c</ref>. As before, we compare the classification accuracy of the models for an increasing amount of degradation, given in this case by the maximum displacement D, which was kept the same for both horizontal and vertical strips. In the present experiments, D varies from 0 to 4 pixels. In this case, the performance of the models for D ≥ 2 turns out to rise for increasing stopping times up to T 2 = 6 for the models with lateral connections in the second layer, while there is a peak in performance at T 1 = 5 for the ones with lateral connections in the first layer: see <ref type="figure" target="#fig_6">Figure 5</ref>. A similar situation can be observed when analyzing the combinatorics of stopping times for the first and second layers, as shown in <ref type="figure" target="#fig_7">Figure 6</ref>: the optimal couple of values (T 1 , T 2 ) shifts towards the maximum as the displacement D increases, and the best accuracy is reached at (T 1 , T 2 ) = (6, 6) above a certain amount of degradation.</p><p>4.1.5. Adversarial attacks. Finally, we tested the robustness of our model to adversarial attacks via FGSM. <ref type="figure" target="#fig_1">Figure 1d</ref> shows some examples of images obtained through (12) applied to the base CNN for MNIST, for increasing values of ε. For sufficiently small ε, this perturbation results in an image that is almost identical to the original one to the human eye; however, these images are misclassified by the network.</p><p>Again, we first examine the performance of the models with lateral connections in one layer at a time, for varying T 1 and T 2 respectively. <ref type="figure" target="#fig_8">Figure 7</ref> displays the classification accuracies of these models for T 1 ∈ {1, . . . , 6} and T 2 = 1 (A) and for T 1 = 1 and T 2 ∈ {1, . . . , 6} (B). As before, the left figure plots the accuracy against the amount of degradation, with a curve for each stopping time T i , while the right figure plots the accuracy against the stopping time T i , with a curve for each value of ε. Finally, <ref type="figure" target="#fig_9">Figure 8</ref>  accuracy values lie on a diagonal. However, while in that case the optimal combination was clearly located around a single spot, two peaks develop in the current case, corresponding to either high values of T 1 and low values of T 2 , or vice versa. We will summarize the main results obtained for all datasets in <ref type="table" target="#tab_0">Table 1</ref>, showing the difference in mean percent accuracy between the base CNN and the optimal KerCNN model, along with the corresponding combination of stopping times (T 1 , T 2 ). A possible concern about our approach is the fact that we do not identify a combination that is optimal for all tasks, thus raising the issue of how to choose the stopping times when the amount of degradation is not known a priori. We nonetheless remark that, although the optimal combination of (T 1 , T 2 ) varies, the KerCNNs with T 1 , T 2 ∈ {2, 3} outperform the base CNN in practically all the tasks.</p><p>4.1.6. Comparison with learned kernels. We now compare our model with the RecCNN architectures described above. Here, recurrent convolutional connections as described in Section 2.3, with weights φ 1 of size 4 × 4 × 16 × 16, have been added in the first (resp. second) layer; the size of the feedforward weights of the second layer has been decreased to 3 × 3 × 16 × 16 to make the number of parameters match with the base CNN (as in <ref type="bibr" target="#b44">Spoerer et al., 2017)</ref>. The performance of these RecCNN models on the tasks examined before has been compared to the one of the base CNN, as well as with the corresponding KerCNNs.</p><p>In most experiments, the RecCNN model did not reach better accuracies than the base CNN on corrupted images, although in some cases a pattern similar to the one seen for KerCNNs could be observed: in such cases, the performance increased until an optimal stopping time. However, the improvement in accuracy w.r.t. the CNN turned out to be much smaller than the one obtained by KerCNN models. Moreover, the geometric content of these learned lateral kernels is not evident and the iterative steps taken according to <ref type="formula" target="#formula_16">(9)</ref> do not seem to implement a kind of propagation -a hint of this lies in the fact that the optimal stopping time for RecCNNs never depends on the amount of degradation of the testing images.</p><p>In <ref type="figure" target="#fig_11">Figure 9</ref>, we compare the accuracies of the KerCNN and RecCNN architectures for the corresponding optimal stopping times for each task. In all plots, the filled curves refer to KerCNN models, while the accuracy of RecCNNs is displayed by dashed curves. The color of each curve matches the one used for the corresponding stopping time in all the plots throughout the paper.</p><p>Note that, in <ref type="figure" target="#fig_11">Figure 9a</ref> examined. It is interesting to note that the only case in which RecCNNs show a higher accuracy than KerCNNs for some values of degradation (only for lateral connections in the first layer) is when the images are perturbed via FGSM for ε &gt; 0.2. This suggests that, although the recurrent structure of RecCNNs may help improve the stability to "noiselike" perturbations, the absence of a geometric prior prevents them from implementing any mechanism of completion or contour integration. It is worth noting that, in the study carried out by <ref type="bibr" target="#b44">Spoerer et al. (2017)</ref>, the networks were trained and tested to recognize the networks are facing nuisances for which they were not specifically optimized. For such generalization task, our structured lateral connections inducing a geometric prior turn out to be much more effective. 4.2. Other datasets. In this last section, we provide a synthetic report of our results on some different datasets, namely Kuzushiji-MNIST <ref type="bibr" target="#b11">(Clanuwat et al., 2018)</ref>, Fashion-MNIST <ref type="bibr" target="#b48">(Xiao et al., 2017)</ref> and <ref type="bibr">CIFAR-10 (Krizhevsky, 2009)</ref>. We then illustrate our results through a summary table, which exhibits the improvement in accuracy obtained with the optimal (T 1 , T 2 ) w.r.t. the base CNN as an index of effectiveness of KerCNNs.  (T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, Ankle boot).</p><p>Both datasets are made up of 70000 images of size 28×28, with the same training-testing split as in MNIST. <ref type="figure" target="#fig_1">Figure 10</ref> displays, for each of these two datasets, some representatives of their 10 classes, as well as a few testing images corrupted by the three types of degradation examined. These have been implemented exactly as for MNIST, except for some changes in the range of degradation values considered (see Section 4.2.3).</p><p>Again, we considered a CNN with 2 hidden layers as a base model; the architecture is the same, except for the number of filters of the second layer which was set to 32 instead of 16, so that the total number of parameters becomes 14538. The training options were kept the same as before, except for the L 2 regularization parameter for Kuzushiji-MNIST which was set to λ = .001. With these choices, the mean accuracy of the base CNN is 93.13% on Kuzushiji-MNIST and 89.86% on Fashion-MNIST. 4.2.2. CIFAR-10. The CIFAR-10 dataset consists of 60000 32×32 color images in 10 classes (0:Airplane, 1:Automobile, 2: <ref type="bibr">Bird,</ref><ref type="bibr">3:Cat,</ref><ref type="bibr">4:Deer,</ref><ref type="bibr">5:Dog,</ref><ref type="bibr">6:Frog,</ref><ref type="bibr">7:Horse,</ref><ref type="bibr">8:Ship,</ref><ref type="bibr">9:Truck)</ref>.</p><p>In contrast with MNIST-like datasets, CIFAR-10 poses the significantly harder problem of recognizing objects in natural scene images. The dataset includes 50000 training images and 10000 test images. We extracted 10000 images from the training set to use for validationbased early stopping -so that in our experiments the models were trained on 40000 samples, validated on 10000 samples and tested on 10000 samples. <ref type="figure" target="#fig_1">Figure 11</ref> shows some examples of (original as well as perturbed) testing images from CIFAR-10. The perturbations have been applied to the images by simply extending the former methods to three channels. Our base model is a 2-layer CNN with the same architecture as before, but with 64 and 128 filters respectively in the first and second convolutional layers. Moreover, since the images are RGB, the filters of the first layer have three channels in this case. The models were trained with early stopping for a maximum of 300 epochs. Stochastic gradient descent was employed with an initial learning rate of .01, which was automatically decreased by 1/10 when validation accuracy stopped increasing for 10 epochs. We used a batch size of 64 samples and an L 2 regularization parameter λ = .001. Also, dropout with .5 probability was employed in the last layer. The rest of the settings were kept the same as for the other datasets. Due to the longer training times, the results displayed for each architecture are obtained by averaging over 3 networks, instead of 10, trained with different random seeds. Moreover, we let vary the stopping times T i only in {1, 2, 3, 4}. We remark that we are employing a rather small CNN (the total number of parameters is 214922), and no data augmentation is used. With these settings, the mean accuracy of the base CNN on CIFAR-10 is 75.64%. We stress that our aim is to determine the improvement brought by our lateral kernel: in order to better assess its effect, we thought it best to consider a simple network as a base model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3.">Results overview.</head><p>Our results on all considered datasets are summarized in <ref type="table" target="#tab_0">Table   1</ref>. For each dataset, the three row blocks correspond to the three types of perturbation examined. For each type of image degradation and each level of corruption, the table displays the mean percent accuracies of the base CNN and the best KerCNN, as well as their difference. The combination (T 1 , T 2 ) leading to the best KerCNN performance is shown next to the corresponding accuracy value. In the event when the optimal performance is reached by the CNN, the best (T 1 , T 2 ) = (1, 1) is displayed.</p><p>For what concerns Kuzushiji-MNIST, the best performance improvement for images occluded by Gaussian patches is comparable to the one obtained for MNIST. However, a greater contribution of the second layer's kernel can be observed: that is, the optimal combinations of stopping times display larger values of T 2 for this type of degradation. This may be due to the more frequent occurrence of complex patterns requiring a "higher order" analysis (such as crossings and loops) w.r.t. MNIST. On the other hand, on images subject to local displacement, the values of T 1 and T 2 bringing to the best accuracy are overall smaller, also leading to significantly smaller differences in performance relative to MNIST.</p><p>In fact, the abundance of small details in such characters makes this kind of perturbation far more disruptive than it is for images like MNIST's digits: even a small displacement may completely destroy some tiny yet characterizing features. Finally, the results for adversarial attacks with small values of ε are analogous to the ones obtained for digits, although with a faster decay in accuracy. On the other hand, although a configuration different from MNIST is observed for ε ≥ .2, the accuracy values are around (or even below) chance level in these cases, which makes somewhat pointless to speculate about them.</p><p>Let us now examine the results obtained for the Fashion-MNIST dataset. As for the images occluded by Gaussian patches, the slightly increased contribution of the second layer w.r.t. MNIST is again probably due to the heterogeneity of features characterizing these images, including both extended contours and tiny, intricate line patterns. For this type of perturbation, the improvement provided by our lateral connections is more moderate than it is for the preceding datasets, reaching a maximum accuracy difference of ∼10%. This may depend upon such images being largely composed by "solid color" areas rather than lines.</p><p>Intuitively, when an occlusion falls in the middle of one such area, it does not interrupt a curve or a contour: therefore, the activation values of filters sensitive to local orientation is very low at these locations and consequently the action of the kernel on them is less relevant. As for CIFAR-10, the performance of CNNs and KerCNNs on images corrupted by Gaussian patches is comparable for all values of γ, with a slight advantage for KerCNNs for occlusions large enough (γ &gt; 5). In our view, such "insensitivity" of lateral kernels to this type of perturbation may be linked to the increased difficulty of dealing with color images -indeed, this aspect certainly requires further investigation. On the other hand, the improvement obtained by KerCNNs w.r.t. CNNs for images subject to edge disruption and adversarial attacks is still consistent (up to ∼15%). Note that the value of ε for adversarial attacks in this case was let vary in {0, .005, .01, .015, .02, .025} (again due to the faster decay in accuracy w.r.t. ε).</p><p>Overall, we believe that the global results are very promising, both for what concerns the effectiveness of the model for image recognition under challenging conditions, and from the point of view of its interpretation linked to biological vision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this article we introduced KerCNN, a modification of a CNN architecture given by the addition of biologically inspired lateral connections. Such connections are determined by convolutional kernels iteratively applied to the output of each convolutional layer, and defined by a notion of correlation between the filters of that layer, as in the cortical connectivity model of <ref type="bibr">Montobbio et al. (2019a,b)</ref>. This allows to establish a link between the geometry of feedforward and lateral connections, as the latter are defined in terms of the former. Moreover, since the lateral kernels are a deterministic function of the convolutional filters, the number of parameters of the original CNN is left unchanged -thus allowing a fair comparison between a base CNN architecture and the KerCNNs obtained from it.</p><p>The models were compared on their ability to generalize a learned image classification task to unseen corrupted inputs. The types of perturbation applied to the images were chosen to disrupt discriminative local information, so as to "force" the nets to perform an integration of context data to correctly recognize the corrupted input. The biological reason for choosing this testing framework was the close bond between anatomical lateral connections and perceptual phenomena linked to global shape analysis. In fact, our study revealed that the insertion of the proposed lateral connections in a 2-layer CNN critically enhanced its stability to all types of perturbation examined. Moreover, such improvement was not observed when introducing learned lateral kernels as in <ref type="bibr" target="#b30">Liang and Hu (2015)</ref> and <ref type="bibr" target="#b44">Spoerer et al. (2017)</ref>. This suggests that the geometric information encoded in our lateral kernel has a meaningful role in implementing mechanisms of pattern completion and contour integration, to compensate for the missing information in the corrupted testing images. We remark that such mechanisms are "spontaneous" to the effect that that they are not enforced during the training stage: indeed, the networks were only trained to classify uncorrupted images.</p><p>The main analysis was carried out on the MNIST dataset, and then extended to a few more image datasets. Notably, promising results were obtained on natural images from the CIFAR-10 dataset. As a future development, we intend to test our model on bigger images and on richer datasets. It would also be interesting to examine the connectivity kernels obtained for non-image data and for different tasks: as an example, the regularity enforced by our lateral kernels may be helpful for problems of sound source separation.</p><p>Another natural advancement would be to consider deeper architectures. Indeed, although the proposed architecture was motivated by a model for early visual areas, its flexibility could make it suitable for recovering patterns in higher level processing as well.</p><p>An analysis of the different feature information encoded in the kernels associated to each layer may help gain better insight into the analysis carried out by the networks at each stage of their processing.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>dataset<ref type="bibr" target="#b29">(LeCun et al., 1998)</ref>: we compare the performance of a 2-layer CNN model with the ones of the corresponding KerCNN and RecCNN models, for varying stopping times T 1 and T 2 and for different types and amounts of image degradation (as outlined in Section 3.2). We then give a more synthetic report on the same study carried out on the<ref type="bibr" target="#b36">(Paszke et al., 2017)</ref>. 4.1. MNIST. We start by considering the MNIST dataset<ref type="bibr" target="#b29">(LeCun et al., 1998)</ref>, consisting of 70000 labeled 28 × 28 grayscale images of handwritten digits from 0 to 9: see the sample inFigure 1a. The default train-test split is 60000/10000. We retained a part of the images from the training set for validation-based early stopping, so that the final dataset used consisted of 50000 training samples, 10000 validation samples and 10000 testing samples. We trained the networks on the original training images, and we tested them on corrupted testing images, according to the three types of degradation mentioned above. Some examples are displayed inFigure 1b-d.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>(a) A sample from the MNIST dataset. (b) A testing image corrupted by a Gaussian patch of increasing standard deviation. (c) A testing image corrupted by an increasing amount of local contour disruption D. (d) Testing images perturbed by applying FGSM to the base CNN, with increasing values of ε. Below each image, we display the classified label, as well as the correct label (in brackets).Apart from the unperturbed one (ε = 0), all the images are misclassified by the CNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Our KerCNN model with structured lateral connections defined by kernels K 1 and K 2 . 4.1.1. Base model. Our base model is a CNN with 2 hidden layers. We take 16 filters of size 5×5 in the first convolutional layer and 16 filters of size 5×5×16 in the second convolutional layer, each followed by ReLU activation and max pooling, and a fully connected last layer followed by softmax activation. The total number of trainable parameters is 7482. We then compare this model with the one obtained from it by inserting the structured lateral connections. SeeFigure 2for a description of the model. The lateral kernels in this case have size 9 × 9 × 16 × 16. We also analyze the performance of the model obtained from the CNN by inserting recurrent connections according to the RecCNN model, i.e. through the update rule (9). As said before, lateral connections given by the kernels K l do not introduce new parameters in the starting CNN. On the other hand, the insertion of learned lateral connections results in a model with more parameters than the base CNN: for example, the introduction of learned kernels of size 4 × 4 × 16 × 16 in the first layer of the base model would add 4096 new parameters to the original 7482. In the following, we consider a 7482parameter version of the RecCNN, obtained by decreasing the size of feedforward filters in order to compensate for the extra recurrent parameters, as in<ref type="bibr" target="#b44">Spoerer et al. (2017)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc><ref type="bibr" target="#b44">and Spoerer et al. (2017)</ref>. The training and testing images were z-score normalized according to the mean and standard deviation computed across the whole training set. For each architecture (i.e. each combination of stopping times T 1 and T 2 ), 10 nets initialized with different random seeds were trained. The results displayed in the following are obtained by testing all 10 nets and averaging the classification accuracy over trials. Note that the testing itself introduces a further element of randomness over trials, since the perturbations are applied to the images at each evaluation, yielding possibly different results. Error bars (95% confidence intervals) are shown in the plots to keep track of the variability across initialization seeds and image perturbations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>(left) shows its classification accuracy for varying values of standard deviation γ of the Gaussian patches. The three graphs displayed refer to different Results for MNIST testing images corrupted through Gaussian patches, for KerCNN with lateral connections in the first (A), resp. second layer (B). Left plots: accuracy (y-axis) at increasing values of γ (x-axis), for stopping time T 1 = 1, 2, 3 (A), resp. T 2 = 1, . . . , 5 (B). Right plots: accuracy (y-axis) for increasing values (x-axis) of T 1 (A), resp. T 2 (B), for different values of degradation. Each curve refers to a value of γ, specified in red in correspondence of the curve.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Classification accuracy (color-coded) for KerCNN for all combinations of T 1 , T 2 ∈ {1, . . . , 6}, displayed for γ = 0, . . . , 30. The maximum value of accuracy is marked by a red star onto the corresponding cell.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Results for MNIST testing images corrupted through local contour disruption, for KerCNN with lateral connections in the first (A), resp. second layer (B). Left plots: accuracy at increasing values of displacement D, for stopping time T 1 = 1, . . . , 5 (A), resp. T 2 = 1, . . . , 5 (B). Right plots: accuracy for increasing values of T 1 (A), resp. T 2 (B), for different values of degradation. Each curve refers to a value of D, displayed in red in correspondence of the curve.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>Classification accuracy (color-coded) for KerCNN for all combinations of T 1 , T 2 ∈ {1, . . . , 6}, displayed for D = 0, . . . , 4. The maximum value of accuracy is marked by a red star onto the corresponding cell.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 .</head><label>7</label><figDesc>displays the analysis of the combinatorics of T 1 and T 2 . Similarly to the case of Gaussian patches, the highest Results for MNIST testing images perturbed via FGSM, for KerCNN with lateral connections in the first (A), resp. second layer (B). Left plots: accuracy at increasing values of the FGSM parameter ε, for stopping time T 1 = 1, . . . , 6 (A), resp. T 2 = 1, . . . , 6 (B). Right plots: accuracy for increasing values of T 1 (A), resp. T 2 (B), for different values of degradation. Each curve refers to a value of ε, displayed in red in correspondence of the curve.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 .</head><label>8</label><figDesc>Classification accuracy (color-coded) for KerCNN for all combinations of T 1 , T 2 ∈ {1, . . . , 6}, displayed for ε = 0, . . . , 0.25. The maximum value of accuracy is marked by a red star onto the corresponding cell.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>(top), curves for KerCNN with both T 1 = 2 and T 1 = 3 are displayed. Although the KerCNN model with stopping time T 1 = 3 (orange curve) widely outperforms the optimal RecCNN for all values of standard deviation above 10, the Rec-CNN displays a higher accuracy with small occlusions. However, for these smaller patches the optimal stopping time for KerCNN is T 1 = 2 (green curve), and this model outperforms the best RecCNN for all values of degradation. A similar situation can be observed in Figure 9a(middle) for local edge disruption, where both T 1 = 3 and T 1 = 5 curves are displayed for the KerCNN model. To sum up, the KerCNN model clearly outperforms the corresponding RecCNN architecture, when comparing the two for their respective best stopping times, for almost all tasks</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 .</head><label>9</label><figDesc>cluttered digits: in their experiments, RecCNNs significantly outperform the purely convolutional architectures, thus showing the benefits of recurrence in learning challenging tasks. On the other hand, our study shows that this does not extend to the case where Comparison between optimal KerCNN and optimal RecCNN. Top: Gaussian patches; middle: local edge disruption; bottom: adversarial attacks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>4.2.1. Kuzushiji-MNIST and Fashion-MNIST. In order to analyze the effect of our lateral connections on different images while keeping most of our settings unchanged, we examined two MNIST-like datasets: the Kuzushiji-MNIST dataset, containing 10 phonetic letters of hiragana, one of the components of the Japanese writing system; and the Fashion-MNIST dataset, consisting of Zalando's article images subdivided into 10 item categories</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 10 .</head><label>10</label><figDesc>Examples from the Kuzushiji-MNIST (left) and Fashion-MNIST (right) datasets. For each database, the images display: (a) A sample from the dataset. Each row corresponds to a class. (b) A testing image corrupted by a Gaussian patch of increasing standard deviation. (c) A testing image corrupted by an increasing amount of local contour disruption D. (d) Testing images from different classes, perturbed by applying the FGSM to the base CNN with increasing values of ε. Below each image, we display the classified label, as well as the correct label (in brackets). Apart from the unperturbed image (ε = 0), all the images are misclassified by the CNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 11 .</head><label>11</label><figDesc>(a) A sample from the CIFAR-10 dataset. Each row corresponds to a class. (b) A testing image corrupted by a Gaussian patch of increasing standard deviation. (c) A testing image corrupted by an increasing amount of local contour disruption D. (d) Testing images from different classes, perturbed by applying the FGSM to the base CNN with increasing values of ε. Below each image, we display the classified label, as well as the correct label (in brackets). Apart from the unperturbed image (ε = 0), all the images are misclassified by the CNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Overview of the results for MNIST, Kuzushiji-MNIST, Fashion-MNIST and CIFAR-10. For each degradation value, the accuracy of the base CNN is compared to the one of the best KerCNN. The optimal (T 1 , T 2 ) = (1, 1) is also shown for each case.On the other hand, the perturbation obtained by shifting horizontal and vertical strips does not affect constant areas, while it consistently disrupts the image edges. Moreover, differently from Kuzushiji-MNIST's characters, global shapes rather than local details are markedly characterizing for discriminating between Fashion-MNIST classes. This makes our lateral connections particularly suited to manage this kind of perturbation. Indeed, a far greater improvement in the CNN performance can be observed w.r.t. Kuzushiji-MNIST in this case, especially for large values of the displacement D: as an example, for D = 4, the ∼35% accuracy obtained by the base CNN rises to ∼60% with the optimal KerCNN model. Finally, for what concerns adversarial attacks, we considered values of ε varying in a smaller range, since the decay in performance for this dataset turned out to be much faster; namely, we took ε ∈ {0, .02, .04, .06, .08, .1}. Again, up to this rescaling, the results are analogous to the other datasets.</figDesc><table><row><cell>MNIST</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abbasi-Sureshjani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Favali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Citti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Ter Haar Romeny</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Curvature integration in a 5d kernel for extracting vessel connections in retinal images</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Image Process</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="606" to="621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A direct variational approach to a problem arising in image reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ambrosio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Masnou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Interfaces and Free Boundaries</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="63" to="81" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The curve indicator random field: Curve organization via edge correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>August</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Zucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Kluwer International Series in Engineering and Computer Science</title>
		<editor>Boyer, K. and Sarkar, S.</editor>
		<imprint>
			<biblScope unit="volume">546</biblScope>
			<date type="published" when="2000" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
	<note>Perceptual Organization for Artificial Vision Systems</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep convolutional networks do not classify based on global object shape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Erlikhman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Kellman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS Computational Biology</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1" to="43" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A cortical-inspired geometry for contour perception and motion integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Barbieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cocci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Citti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sarti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Math Imaging Vis</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="511" to="529" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Image inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bertalmio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Caselles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ballester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH &apos;00</title>
		<meeting>the 27th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH &apos;00<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press/Addison-Wesley Publishing Co</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="417" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Orientation selectivity and the arrangement of horizontal connections in tree shrew striate cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bosking</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schoenfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fitzpatrick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Neurosci</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="2112" to="2127" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Approximating CNNs with bag-of-local-features models works surprisingly well on ImageNet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The functional geometry of local and long-range connections in a model of V1</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Bressloff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Cowan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Physiol Paris</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="221" to="236" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A cortical based model of perceptual completion in the rototranslation space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Citti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sarti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Imaging and Vision archive</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="307" to="326" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep learning for classical Japanese literature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Clanuwat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bober-Irizar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kitamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Machine Learning for Creativity and Design. NIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Diffusion maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Coifman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lafon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl Comput Harmon Anal</title>
		<imprint>
			<biblScope unit="page">21</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Contour integration by the human visual system: evidence for a local association field</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Field</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">F</forename><surname>Hess</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Res</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="173" to="193" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fukushima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological Cybernetics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="193" to="202" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Spatial integration and cortical dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kapadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Westheimer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Academy of Sciences USA</title>
		<meeting>the National Academy of Sciences USA</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="615" to="622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ICLR</title>
		<meeting>the ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Neural dynamics of perceptual grouping: Textures, boundaries, and emergent segmentations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Grossberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mingolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception &amp; Psychophysics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="141" to="171" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hausdorff</surname></persName>
		</author>
		<title level="m">Dimension undäusseres Mass. Mathematische Annalen</title>
		<imprint>
			<date type="published" when="1918" />
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="157" to="179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The visual cortex is a contact bundle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hoffman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl Math Comput</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="137" to="167" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Receptive fields, binocular interaction and functional architecture in the cat visual cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Hubel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Wiesel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Physiol</title>
		<imprint>
			<biblScope unit="volume">160</biblScope>
			<biblScope unit="page" from="106" to="154" />
			<date type="published" when="1962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adam: a method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Representation of local geometry in the visual system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Koenderink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Van Doom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biol. Cybern</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="367" to="375" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Neural Information Processing Systems</title>
		<meeting>the 25th International Conference on Neural Information Processing Systems<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
	<note>NIPS&apos;12</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Collinearity and parallelism are statistically significant second order relations of complex cell responses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kruger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Processing Letters</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="117" to="129" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Neural dynamics of feedforward and feedback processing in figure-ground segregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">W</forename><surname>Layton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mingolla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazdanbakhsh</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front Psychol</title>
		<imprint>
			<biblScope unit="issue">972</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Recurrent convolutional neural network for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">From receptive profiles to a metric model of V1</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Montobbio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Citti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sarti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Comput Neurosci</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="257" to="277" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">A metric model for the functional architecture of the visual cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Montobbio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Citti</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>under revision</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted Boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on International Conference on Machine Learning, ICML&apos;10</title>
		<meeting>the 27th International Conference on International Conference on Machine Learning, ICML&apos;10</meeting>
		<imprint>
			<publisher>USA. Omnipress</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Computational neural models of spatial integration in perceptual grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mingolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Segmentation and grouping in vision</title>
		<editor>Shipley, T. F. and Kellman, P. J.</editor>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="page" from="353" to="400" />
		</imprint>
	</monogr>
	<note>Advances in psychology</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clune</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<title level="m">Deep neural networks are easily fooled: High confidence predictions for unrecognizable images. 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="427" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS-W</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Vers une neuro-géométrie. Fibrations corticales, structures de contact et contours subjectifs modaux</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Petitot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tondut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mathématiques , Informatique et Sciences Humaines</title>
		<imprint>
			<publisher>CAMS, EHESS</publisher>
			<date type="published" when="1999" />
			<biblScope unit="volume">145</biblScope>
			<biblScope unit="page" from="5" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks for image classification: A comprehensive review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Rawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2352" to="2449" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A model of natural image edge cooccurrence in the rototranslation group</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sanguinetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Citti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sarti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Vis</title>
		<imprint>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The constitution of visual perceptual units in the functional architecture of V1</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Citti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Neuroscience</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="285" to="300" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The symplectic structure of the primary visual cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Citti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Petitot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biol. Cybern</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="33" to="48" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Recurrent dropout without memory loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Semeniuta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Barth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1757" to="1766" />
		</imprint>
	</monogr>
	<note>The COLING 2016 Organizing Committee</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">On a common circle: Natural scenes and Gestalt rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cecchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">O</forename><surname>Magnasco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Academy of Sciences</title>
		<meeting>the National Academy of Sciences</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page" from="1935" to="1940" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural networks: a better model of biological object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Spoerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mcclure</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kriegeskorte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in psychology</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">1551</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fergus</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
		<idno>abs/1312.6199</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schrimpf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lotter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Moerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">O</forename><surname>Caro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hardesty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kreiman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>PNAS</publisher>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="8835" to="8840" />
		</imprint>
	</monogr>
	<note>Recurrent computations for visual pattern completion</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Excitatory and inhibitory interactions in localized populations of model neurons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Cowan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biophys J</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vollgraf</surname></persName>
		</author>
		<idno>arXiv:cs.LG/1708.07747</idno>
		<title level="m">Fashion-MNIST: a novel image dataset for benchmarking machine learning algorithms</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<editor>Fleet, D., Pajdla, T., Schiele, B., and Tuytelaars, T.</editor>
		<imprint>
			<biblScope unit="volume">8689</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
