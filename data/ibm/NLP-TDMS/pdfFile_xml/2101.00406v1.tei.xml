<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cross-Document Language Modeling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avi</forename><surname>Caciularu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Bar-Ilan University</orgName>
								<address>
									<settlement>Ramat-Gan</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Cohan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Allen Institute for Artificial Intelligence</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Allen Institute for Artificial Intelligence</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
							<email>matthewp@allenai.orgarie.cattan@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Allen Institute for Artificial Intelligence</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arie</forename><surname>Cattan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Bar-Ilan University</orgName>
								<address>
									<settlement>Ramat-Gan</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
							<email>dagan@cs.biu.ac.il</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Bar-Ilan University</orgName>
								<address>
									<settlement>Ramat-Gan</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Cross-Document Language Modeling</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a new pretraining approach for language models that are geared to support multi-document NLP tasks. Our crossdocument language model (CD-LM) improves masked language modeling for these tasks with two key ideas. First, we pretrain with multiple related documents in a single input, via cross-document masking, which encourages the model to learn cross-document and long-range relationships. Second, extending the recent Longformer model, we pretrain with long contexts of several thousand tokens and introduce a new attention pattern that uses sequence-level global attention to predict masked tokens, while retaining the familiar local attention elsewhere. We show that our CD-LM sets new state-of-the-art results for several multi-text tasks, including crossdocument event and entity coreference resolution, paper citation recommendation, and documents plagiarism detection, while using a significantly reduced number of training parameters relative to prior works 1 . * Work partly done as an intern at AI2.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The majority of NLP research addresses a single text, typically at the sentence or document level. This has been the case for both infrastructure language analysis tasks, such as syntactic, semantic and discourse analysis, as well as applied tasks, such as question answering (and its reading comprehension variant <ref type="bibr" target="#b33">(Xu et al., 2019)</ref>), information extraction, sentiment analysis etc., where the system output is typically extracted from a single document. Yet, there are important applications which are concerned with aggregated information spread across multiple texts, e.g., multidocument summarization <ref type="bibr" target="#b11">(Fabbri et al., 2019a)</ref>, cross-document coreference resolution <ref type="bibr" target="#b6">(Cybulska and Vossen, 2014a)</ref>, and multi-hop question answering <ref type="bibr" target="#b36">(Yang et al., 2018)</ref>. While providing stateof-the-art results for cross-document tasks, current pretraining methods, developed for a single text, are not geared to fully address the needs of crossdocument tasks. As an alternate approach, we propose Cross-Document Language Model (CD-LM), a new language model (LM) that is trained in a cross-document manner. We show that this significantly outperforms previous approaches, resulting in state-of-the-art results for event and entity crossdocument coreference resolution, paper citation recommendation, and documents plagiarism detection.</p><p>Tasks that consider multiple documents typically require mapping or linking between pieces of information across documents. Such input documents usually contain overlapping information, e.g., Doc 1 and 2 in <ref type="figure">Fig. 1</ref>. Desirably, LMs should be able to align between overlapping elements across these related documents. For example, one would expect a competent model to correctly align the events around "name" and "nominates" in Doc 1 and Doc 2, effectively recognizing their relation even when they are in separate documents. Yet, existing LM pretraining methods do not expose the model to learn such information. Here, we propose a scheme to integrate cross-document knowledge already in pretraining, thus allowing the LM to learn to encode relevant cross-document relationships implicitly.</p><p>To allow our CD-LM to address large contexts across multiple documents, we leverage the recent appealing architecture of the Longformer model <ref type="bibr" target="#b2">(Beltagy et al., 2020)</ref>, designed to address long inputs. Specifically, we leverage its global attention mechanism, originally utilized only during task-specific fine-tuning, and extend its use already in pretraing, enabling the model to consider cross-Doc 1: "... President Obama will name Dr. Regina Benjamin as U.S. Surgeon General in a Rose Garden announcement late this morning. ..." Doc 2: "... Obama nominates new surgeon general: MacArthur "genius grant" fellow Regina Benjamin. ..." <ref type="figure">Figure 1</ref>: Various document examples from the ECB+ dataset. In Doc 1 and Doc 2, underlined words represent coreferering events and the same color represents a coreference cluster: The entity clusters are ("Dr. Regina Benjamin", "MacArthur "genius grant" fellow Regina Benjamin") and ("President Obama","Obama"), and the single event cluster is <ref type="bibr">("name","nominates")</ref>. These examples are adopted from <ref type="bibr" target="#b4">Cattan et al. (2020)</ref>. document, as well as long-range within-document, information. While using this mechanism, we introduce a cross-document masking approach. This approach considers as input multiple-documents containing related, partly overlapping, information. The model is then challenged to unmask the masked tokens while attending to information in both the same as well as the related documents. This way, the model is encouraged to "peek" at other documents and map cross-document information, in order to yield better unmasking. Our pretraining procedure yields a generic cross-document language model, which may be leveraged for various cross-document downstream tasks that would need to map information across related texts. As mentioned above, our experiments assess utility of our CD-LM for a range of cross-document tasks, resulting with significant improvements, suggesting its appeal for future work in the cross-document setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Transformer-based language models (LMs) <ref type="bibr" target="#b9">(Devlin et al., 2019;</ref><ref type="bibr" target="#b18">Yang et al., 2019)</ref> have led to significant performance gains in various natural language understanding tasks, mainly for within-document-related tasks. They use multiple self-attention layers in order to learn to produce high-quality token representations. They were shown to incorporate contextual knowledge by assigning a representation that is an attentive function of the entire input context. Such models are trained using the Masked Language Modeling (MLM) objective (known as the pretraining phase) -given a piece of text, a model uses the context words surrounding a masked token to try to predict it, and by that, maximizing the likelihood of the input words.</p><p>These models have significantly advanced the state-of-the-art in various NLP tasks, mostly using post-pretraining, finetuning approaches, e.g., question answering , coreference resolution , such as those of the GLUE benchmark . Importantly, pretrained LMs eliminate the need for many heavily-engineered and hand-crafted task-specific architectures for downstream tasks. Additionally, <ref type="bibr" target="#b5">Clark et al. (2019)</ref> show that BERT's <ref type="bibr" target="#b9">(Devlin et al., 2019)</ref> attention heads encode a substantial amount of linguistic knowledge, such as the ability to represent within-document coreference relations. This enables better performance over downstream tasks, with limited resources of labeled training data. Despite such models' success in within-document tasks, due to memory and time constraints, they limit the input size and are only able to support a rather small context. Thus, they cannot be readily applied in cross-document tasks where the input size is large.</p><p>Recently, several models were suggested to handle these issues and bypass the length constraint, by employing techniques for dealing with the computational and memory obstacles <ref type="bibr" target="#b28">(Tay et al., 2020)</ref>. Examples to such architectures include the Longformer <ref type="bibr" target="#b2">(Beltagy et al., 2020)</ref> BigBird <ref type="bibr" target="#b39">(Zaheer et al., 2020)</ref>, and LinFormer , which were introduced to extend the range of context that can be used, both for the pretraining and fine-tuning stages. Specifically for the Longformer model, which we utilize in this work, a localized sliding window-based attention, termed local attention, was proposed for reducing computation and extending the previous LMs to support longer sequences. This enabled the handling of long context processing by removing the restrictions of long inputs. In addition, the authors introduced the global attention mode, which allows the LM to build representations based on the full input sequence for prediction, and is used during fine-tuning only. Both the local attention and the global attention modes rely on the known self-attentive score <ref type="bibr" target="#b29">(Vaswani et al., 2017)</ref> which is given by:</p><formula xml:id="formula_0">Attention (Q, K, V ) = softmax QK T √ d k V,</formula><p>where the learned linear projection matrices Q, K, V are partitioned into two distinct sets; θ l = {Q l , K l , V l } and θ g = {Q g , K g , V g }, for the local and the global attention modes, respectively. During pretraining, the Longformer assigns the local attention mode for all tokens to optimize the MLM objective. Before task-specific finetuning, the attention mode is predetermined for each input token, assigning global attention to few targeted tokens (e.g., special tokens) to avoid computational inefficiency.</p><p>We hypothesize that the global attention mechanism is useful for learning meaningful representations for modeling cross-document relationships. We propose augmenting the pretraining phase to exploit the global attention mode, rather than using it only for finetuning, as further described in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Cross-Document Language Model</head><p>Documents that describe the same event, e.g., different news articles that discuss the same story, usually contain overlapping information. Accordingly, many cross-document tasks may leverage from LM infrastructure that encodes information regarding alignment and mapping across texts. For example, for the cross-document coreference resolution task, consider the underlined predicate examples in <ref type="figure">Fig. 1</ref>. One would expect a model to correctly align the events around "name" and "nominates", effectively recognizing their coreference relation even when they are in separate documents.</p><p>Our approach to cross-document language modeling is based on training a Transformer-based LM on sets (clusters) of documents, all describing the same event. Such document clusters are readily available in a variety of existing datasets for cross-document benchmarks, such as summarization (e.g., MultiNews <ref type="bibr" target="#b12">(Fabbri et al., 2019b)</ref>), crossdocument coreference resolution (e.g., ECB+ (Cybulska and Vossen, 2014b)) and cross-documents alignment benchmarks <ref type="bibr" target="#b43">(Zhou et al., 2020)</ref>. Training the LM over a set of related documents provides the potential to learn cross-text mapping and alignment capabilities, as part of the contextualization process. Indeed, we show that this cross-document pretraining strategy directs the model to utilize information across documents for predicting masked tokens, and helps in multiple cross-document downstream tasks.</p><p>To support contextualizing information across multiple documents, we need to use efficient Transformer models that scale linearly with input length.</p><p>Thus, we base our model on the Longformer (Beltagy et al., 2020). As described in Sec. 2, this is an efficient Transformer model for long sequences that uses a combination of local attention (selfattention restricted to a local sliding window) and global attention (a small set of pre-specified input locations with direct global attention access).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-Document Masking</head><p>In pretraining, we concatenate the document set using new special document separator tokens, &lt;doc-s&gt; and &lt;\doc-s&gt;, for marking document boundaries. We apply a similar masking procedure as in BERT: For each training example, we chose randomly a sample of tokens (15%) to be masked 2 ; for each of those, our proposed model tries to predict it considering the full document set, by assigning them global attention. This allows the global attention parameters of the Longformer to contextualize encompassing both information across documents, and within-document long dependencies. The nonmasked tokens use local attention, as usual.</p><p>An illustration of the full cross-document masking procedure is depicted in <ref type="figure" target="#fig_0">Fig. 2</ref>, where the masked token associated with "nominates" (colored in orange) globally attends to the whole sequence, and the non-masked token hiding the word "new" (colored in blue) attends to the local context. With regard to the example in <ref type="figure">Fig. 1</ref>, this masking approach aims to implicitly compel the model to learn to correctly predict the word "nominates" by looking at the second document, optimally at the phrase "name", and thus enforce the alignment between the events.</p><p>The loss function induced by the above masking method requires a MLM objective which accounts for the entire sequence, namely, the concatenated documents. We mimic the LM bidirectional conditioning from BERT <ref type="bibr" target="#b9">(Devlin et al., 2019)</ref> but instead of using constant model weights for all tokens, we assign the global attention weights θ g for the masked tokens, so the model can predict the target token in a multi-document context. The unmasked tokens use the local attention weights, θ l . We dub this method Cross-document masked language modeling (CD-MLM). The resulting model includes the following new components: new pretrained special document separator, and pretrained sets of both global and local attention weights that form the cross-document language model (CD-  LM). The document separator tokens can be useful for downstream tasks for marking the document bounderies while the global attention weights provide better encoding of cross-document selfattentive information.</p><p>Finetuning During finetuning on downstream cross-document tasks, we utilize our model by concatenating the tokens of relevant input documents using the document separator tokens, along with the classification token (referred to as CLS) at the beginning of the input sequence. Moreover, for token-level tasks such as coreference resolution, we assign global attention to several explicit spans of text, as described in Section 5.2. Using global attention on at least one token ensures that the distribution of the data during finetuning is similar to the distribution during pretraining, which avoids pretraining-finetuning discrepancy. Note that this method is much simpler than existing task-specific cross-document models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CD-LM Implementation</head><p>In this section, we provide experimental details used for pretraining our CD-LM model, and detail the ablations we used.</p><p>Corpus data We use the Multi-News dataset <ref type="bibr" target="#b11">(Fabbri et al., 2019a)</ref> as the source of related documents for the pretraining. This large-scale dataset contains 44,972 training documents-summary clusters which are originally intended for multi-document summarization. The number of related source documents (that describe the same event) per summary varies from 2 to 10, as detailed in Appendix A.1. We discard the summaries and consider each cluster of related documents, of at least 3 documents, for our cross-document pretraining scheme. We compiled the training corpus by concatenating related documents that were sampled randomly from each cluster, until reaching the input sequence length limit of 4,096 tokens per sample. The average input contains 2.5k tokens and the 90th percentile of input lengths is 3.8K tokens.</p><p>Training and hyperparameters We pretrain the model according to our CD-MLM strategy described in Section 3. To that end, we employ the Longformer-base model <ref type="bibr" target="#b2">(Beltagy et al., 2020)</ref> 3 and continue its pretraining for additional 25k steps. We use the same hyperparameters and follow the exact setting as in <ref type="bibr" target="#b2">Beltagy et al. (2020)</ref>: Input sequences are of the length of 4,096, effective batch size of 64 (using gradient accumulation and batch size of 8), a maximum learning rate of 3e-5, and a linear warmup of 500 steps, followed by a power 3 polynomial decay. For speeding up the training and reducing memory consumption, we used the mixedprecision (16-bits) training mode. <ref type="bibr">4</ref> The rest of the hyperparameters are the same as for RoBERTa .</p><p>Baseline Language Models In addition to our proposed CD-LM model and the state-of-the-art models detailed in the next sections, we considered the following LM variations in our evaluations, as ablations for our model:</p><p>• The plain LONGFORMER-base model, without further pretraining.</p><p>• The RAND CD-LM model based on the Longformer-base model, with the additional CD-MLM pretraining but using random, unrelated documents from various clusters. The amounts of data and pretraining hypyerparameters are the same as the ones of CD-LM. This baseline model can asses whether pretraining using related documents is beneficial.</p><p>When finetuning each one of the above models, we restrict each input segment (document/abstract/passage) to include a maximal length of 2,047 tokens, so that the entire input length, including the CLS token, will have no more than 4,096 input tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluations and Results</head><p>This section presents the intrinsic and extrinsic experiments conducted to evaluate our CD-LM. For the intrinsic evaluation we measure the perplexity of the models, while for extrinsic evaluations we considered the event and entity cross-document coreference resolution, paper citation recommendation, and the document plagiarism detection tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Cross-Document Perplexity</head><p>First, we conduct a cross-document perplexity experiment, in a task-independent manner, to asses the contribution of the pretraining process. We used the MultiNews validation and test sets, each of them containing 5,622 documents-summary clusters, to construct the evaluation corpora. Then we followed the same protocol from the pretraining phase -random 15% of the input tokens are masked and assigned with global attention, and the challenge is to predict the masked token given all documents in the input sequence. The perplexity is then measured by computing exponentiation of the loss.</p><p>The results are depicted in <ref type="table" target="#tab_2">Table 1</ref>. The CD-LM model outperforms the baselines. In particular, the advantage over RAND CD-LM, which was pretrained equivalently over an equivalent amount of (unrelated) cross-document data, confirms that cross-document masking, in pretraining over related documents, indeed helps for cross-document masked token prediction across such document. The CD-LM is encouraged to look at the full sequence, when predicting a masked token. Therefore, it exploits related information in other documents as well, and not just local context. The RAND CD-LM is inferior since, in its pretraining phase, it was not exposed to such overlapping useful information. The plain LONGFORMER model, which is reported just as a reference point, is expected to have difficulty to predict cross-document tokens, in addition to the reason above, since the document separators we used are not part of its embedding set and are randomly initialized during this task. Moreover, recall that the CD-LM and the RAND CD-LM models have two pretrained  sets of linear projection weights -one for local attention and one for global attention. The plain LONGFORMER model uses the same weights for the two modes, and therefore it is reasonable that it will fail at long-range mask prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Cross-Document Coreference Resolution</head><p>Cross-document (CD) coreference resolution deals with identifying and clustering together textual mentions across multiple documents that refer to the same concept (see examples in Doc 1 and Doc 2 in <ref type="figure">Fig. 1</ref>). The considered mentions can be both entity mentions, usually noun phrases, e.g., "Obama" and "President Obama", and event mentions, which are mostly verbs or nominalizations that appear in the text, e.g., "name" and "nominates".</p><p>Benchmark. For assessing our CD-LM on CD coreference resolution, we utilized it for an evaluation over the ECB+ corpus <ref type="bibr" target="#b6">(Cybulska and Vossen, 2014a)</ref>, which is the most commonly used dataset for the task. ECB+ consists of within-and crossdocument coreference annotations for entities and events. The ECB+ dataset statistics are described in Appendix A.2. Following previous work, for comparison, we conduct our experiments on gold event and entity mentions. For evaluating the performance of coreference clustering we follow the standard coreference resolution evaluation metrics: MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998), CEAFe <ref type="bibr" target="#b21">(Luo, 2005)</ref>, their average CoNLL F1, and the more recent LEA metric (Moosavi and Strube, 2016).</p><p>Algorithm. Recent approaches for CD coreference resolution train a pairwise scorer to learn the likelihood that two mentions are coreferring. At inference time, an agglomerative clustering based on the pairwise scores is applied, to form the coreference clusters. Next, we detail our proposed modifications for the pairwise scorer. The current state-ofthe-art models <ref type="bibr" target="#b40">(Zeng et al., 2020;</ref><ref type="bibr" target="#b38">Yu et al., 2020)</ref> train the pairwise scorer by including only the local contexts (containing sentences) of the candidate mentions. They concatenate the two input sentences and feed them into a transformer-based LM. Then, part of the resulting tokens representations are aggregated into a single feature vector which is passed into an additional MLP-based scorer to produce the coreference probability estimate. To accommodate our proposed CD-LM model, we modify this modeling, as illustrated in <ref type="figure">Fig 3.</ref> We include the entire documents containing the two candidate mentions, instead of just their containing sentences. We concatenate the relevant documents using the special document separator tokens, then encode them using our CD-LM along with the &lt;s&gt; token (corresponding to the CLS token) at the beginning of this sequence, as suggested in Section 3. For within-document coreference candidate examples, we use just the single containing document with the document separator. Inspired by <ref type="bibr" target="#b38">Yu et al. (2020)</ref>, we use candidate mention marking: we wrap the mentions with special tokens &lt;m&gt; and &lt;\m&gt; in order to direct the model to specifically pay attention to the candidates representations. Additionally, we assign global-attention to &lt;s&gt;, &lt;m&gt;, &lt;\m&gt;, and the mention tokens themselves, according to the finetuning strategy proposed in Section 3. Our final pairwise-mention representation is formed like in <ref type="bibr" target="#b40">Zeng et al. (2020)</ref> and <ref type="bibr" target="#b38">Yu et al. (2020)</ref>: We concatenate the cross-document contextualized representation vectors for the t th sample:</p><formula xml:id="formula_1">m t (i, j) = s t , m i t , m j t , m i t • m j t ,</formula><p>where [·] denotes the concatenation operator, s t is the cross-document contextualized representation vector of the CLS token, and each of m i t and m j t is the sum of candidate tokens of the corresponding mentions (i and j). Then, we train the pairwise scorer according to the suggested finetuning scheme. At test time, similar to most recent works, we apply agglomerative clustering to merge the most similar cluster pairs. The hyperparameters and further details are elaborated in Appendix B.1.</p><p>Baselines. We consider recent, state-of-the-art baselines that reported results over the ECB+ benchmark. The following baselines were used for both event and entity coreference resolution:</p><p>Same Head-Lemma is a simple baseline that merges mentions sharing the same syntactic headlemma into the same coreference cluster.  <ref type="figure">Figure 3</ref>: CD-coreference resolution pairwise mention representation, using the CD-LM. m i t , m j t and s t are the cross-document contextualized representation vectors for mentions i and j, and of the CLS token, respectively. m i t • m j t is the element-wise product between m i t and m j t . m t (i, j) is the final produced pairwisemention representation. The tokens colored in yellow represent global attention, and tokens colored in blue represent local attention. <ref type="bibr" target="#b1">Barhom et al. (2019)</ref> is a model trained jointly for solving both event and entity coreference as a single task. <ref type="bibr" target="#b4">Cattan et al. (2020)</ref> is a model trained in an endto-end manner (jointly learning mention detection and coreference), employing the RoBERTa-large model to encode separately each document and to train a pair-wise scorer on top of these representations.</p><p>The following baselines were used only for event coreference resolution. They all integrate external linguistic information as additional features to the model: <ref type="bibr" target="#b22">Meged et al. (2020)</ref> is an extension of <ref type="bibr" target="#b1">Barhom et al. (2019)</ref>, leveraging additional side-information acquired by a paraphrase resource <ref type="bibr" target="#b26">(Shwartz et al., 2017)</ref>. <ref type="bibr" target="#b40">Zeng et al. (2020)</ref> is an end-to-end model, encoding the concatenated two sentences containing the two mentions, by the BERT-large model. Similarly to our proposed algorithm, they feed a MLP-based pairwise scorer with the CLS contextualized token representation and an attentive function of the contextualized representation vectors of the candidate mentions. <ref type="bibr" target="#b38">Yu et al. (2020)</ref> is an end-to-end model similar to <ref type="bibr" target="#b40">Zeng et al. (2020)</ref>, but uses rather RoBERTa-large and does not consider the CLS contextualized token representation for the pairwise classification. This is a non-attentive version of Zeng et al's mechanism for paraphrase detection.   Results. The results on event and entity CD coreference resolution are depicted in <ref type="table" target="#tab_4">Tables 2 and  3</ref>. All results are statistically significant using bootstrap and permutation tests with p &lt; 0.001 <ref type="bibr" target="#b10">(Dror et al., 2018)</ref>. Our CD-LM outperforms the sentence based models <ref type="bibr" target="#b40">(Zeng et al., 2020;</ref><ref type="bibr" target="#b38">Yu et al., 2020)</ref> on event coreference (+1.2 CoNLL F1) and largely surpasses state-of-the-art results on entity coreference (+9.8 CoNLL F1), even though these models utilize external linguistic argument information, 5 and include many more parameters (large models vs our base model). Finally, the RAND CD-LM is inferior to the plain LONGFORMER model, despite the fact that it already has pretrained document separator embeddings. This emphasizes the requirement of pretraining on related documents rather than random ones, which allows better alignment and paraphrasing capabilities, required for coreference detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Paper Citation Recommendation &amp; Plagiarism Detection</head><p>We evaluate our CD-LM over citation recommendation and plagiarism detection benchmarks <ref type="bibr" target="#b43">Zhou et al. (2020)</ref>, a recently released benchmark for cross-document tasks. These tasks share the same objective -categorizing whether a particular relationship holds between two input documents, and therefore, correspond to binary classification problems. Citation recommendation deals with detecting whether one reference document should cite the other one, while the plagiarism detection task infers whether one document plagiarizes the other one. To compare with recent state-of-the-art models, we utilized the setup and data selection from <ref type="bibr" target="#b43">Zhou et al. (2020)</ref>, which provides three datasets for citation recommendation and one for plagiarism detection.</p><p>Benchmarks. For citation recommendation, we used the ACL Anthology Network Corpus (AAN; <ref type="bibr" target="#b25">Radev et al., 2013)</ref>, the Semantic Scholar Open Corpus (OC; <ref type="bibr" target="#b3">Bhagavatula et al., 2018)</ref>, and the Semantic Scholar Open Research Corpus (S2ORC; <ref type="bibr" target="#b20">Lo et al., 2020)</ref>. For plagiarism detection, we used the Plagiarism Detection Challenge (PAN; <ref type="bibr" target="#b24">Potthast et al., 2013)</ref>.</p><p>AAN is composed of computational linguistics papers which were published on the ACL Anthology from 2001 to 2014, OC is composed of computer science and neuroscience papers, S2ORC is composed of open access papers across broad domains of science, and PAN is composed of web documents that contain several kinds of plagiarism phenomena. For further dataset prepossessing details and statistics, see Appendix A.3.</p><p>Algorithm. For our models we added the CLS token at the beginning of the input sequence and   Baselines. We consider the reported results of the following recent baselines: SMASH <ref type="bibr" target="#b14">(Jiang et al., 2019)</ref> is an attentive hierarchical RNN model, used for tasks related to long-document.</p><p>SMITH  is a BERTbased hierarchical model, similar to the previously suggested hierarchical attentive networks (HANs <ref type="bibr" target="#b37">(Yang et al., 2016)</ref>).</p><p>BERT-HAN+CDA <ref type="bibr" target="#b43">(Zhou et al., 2020)</ref> is a cross-document attentive mechanism (CDA) built on top of Hierarchical Attention Networks (HANs), based on BERT. For more details, see Section 6. We report the results of their finetuned model over the datasets <ref type="bibr">(Zhou et al., 2020, Section 5.2)</ref>.</p><p>Note that both SMASH and SMITH reported results only over the ANN benchmark. In addition, they used a slightly different version of the AAN dataset, and included the full documents, unlike the dataset that BERT-HAN+CDA used, which we utilized as well, that considers only the documents' abstracts.</p><p>Results. The results on the citation recommendation over the AAN dataset are depicted in <ref type="table" target="#tab_7">Table 4</ref>. We observe that even though several baselines reported results using the full documents, our model outperforms them, using the partial version of the dataset, as in <ref type="bibr" target="#b43">(Zhou et al., 2020)</ref>. Moreover, unlike our model, the CDA is task-specific since it trains new cross-document weights for each task, yet it is still inferior to our model. The results on the rest of the benchmarks are reported in <ref type="table" target="#tab_8">Table 5</ref>, and as can be seen, our CD-LM consistently outperforms both the prior baseline as well as the LONGFORMER and RAND CD-LM models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Recently, several works proposed equipping LMs with cross-document processing capabilities, mostly by harnessing sequence-to-sequence architectures. <ref type="bibr" target="#b17">Lewis et al. (2020)</ref> suggested to pretrain a LM by means of reconstructing a document, given, and conditioned on, related documents. They showed that this technique forced the model to learn how to paraphrase the original reconstructed document, leading to significant performance gains on multi-lingual document summarization and retrieval. This work considers a basic retrieval model, that does not consider cross-document interactions at all.  proposed an end-to-end architecture for improving abstractive summarization. Unlike standard LMs, in their pretraining, several sentences (and not just tokens) are removed from documents, and the model's task is to recover them. A similar approach was also suggested for single document summarization . The advantage of such self-supervision approaches is that they were proved to produce high-quality summaries without any human annotation, often the bottleneck in purely supervised summarization systems. While these approaches advanced the stateof-the-art sequence-to-sequence tasks, the encoders they employed support the encoding of a single document at a time. In our work, we allow inputs comprised of multiple documents in each sample, to support cross-document contextualization. Nevertheless, the main drawback of such sequence-tosequence architectures is that they require a massive amount of data and training time in order to obtain a plausibly trained model, while we used a relatively small corpus.</p><p>The closest work to our proposed model is the recent Cross-Document Attention model (CDA) <ref type="bibr" target="#b43">(Zhou et al., 2020)</ref>. They introduced a crossdocument component, that enables document-todocument and sentence-to-document alignments. This model is set on top of existing hierarchical document encoding models <ref type="bibr" target="#b27">(Sun et al., 2018;</ref><ref type="bibr" target="#b18">Liu and Lapata, 2019;</ref><ref type="bibr" target="#b13">Guo et al., 2019)</ref>, that do not consider information across documents by themselves. CDA suggests influencing the document and sentence representations, by those of other documents, without considering word-to-word information across documents (which might require an additional quadratic number of parameters). This makes such modeling unsuitable for token-level alignment tasks, such as cross-document coreference resolution. Moreover, unlike our proposed model, which employs a generic cross-document pretraining, the CDA mechanism requires learning from scratch the cross-document parameters for each downstream task. Further, they support crossdocument attention between two documents, while our method does not restrict the number of input documents, as long as they fit the input length of the Longformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We presented a novel pretraining strategy and technique for cross-document language modeling, providing better encoding for cross-document downstream tasks. Our primary contributions include cross-document masking over clusters of related documents, driving the model to encode crossdocument relationships. This was achieved by extending the use of the global attention mechanism of the Longformer model <ref type="bibr" target="#b2">(Beltagy et al., 2020)</ref> in pretraining, attending to long-range information across and within documents. Our experiments assess that leveraging our cross-document language model yields new state-of-the-art results over several cross-document benchmarks, including the fundamental task of cross-document entity and event coreference, while, in fact, employing substantially smaller models. We suggest the attractiveness of our CD-LM for neural encoding in cross-document tasks, and propose future research to extend this framework to support sequence-to-sequence crossdocument tasks, such as multi-document abstractive summarization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Dataset Statistics and Details</head><p>In this section, we provide more details about the datasets of the corpus and benchmarks we used during our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 MultiNews Corpus</head><p>In <ref type="table" target="#tab_10">Table 6</ref> we list the number of related documents articles per cluster. This follows the original dataset construction. Note that the datasets and the statistics are taken from <ref type="bibr" target="#b11">Fabbri et al. (2019a</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 ECB+ Dataset</head><p>In <ref type="table" target="#tab_12">Table 7</ref> we list the statistics about training, development and test splits regarding the topics, documents, mentions and coreference clusters. We follow the data split by previous works <ref type="bibr" target="#b8">(Cybulska and Vossen, 2015;</ref><ref type="bibr" target="#b16">Kenyon-Dean et al., 2018;</ref><ref type="bibr">Barhom et al., 2019): Training topics: 1, 3, 4, 6-11, 13-17, 19-20, 22, 24-33;</ref><ref type="bibr">Validation topics: 2, 5, 12, 18, 21, 23, 34, 35;</ref><ref type="bibr"></ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Paper Citation Recommendation &amp; Plagiarism Detection Datasets</head><p>In <ref type="table" target="#tab_14">Table 8</ref> we list the statistics about training, development and test splits for each benchmark, and in <ref type="table" target="#tab_15">Table 9</ref> we list the document and examples counts for each benchmark. The statistics are taken from <ref type="bibr" target="#b43">Zhou et al. (2020)</ref>. The preprocessing of the datasets performed by <ref type="bibr" target="#b43">Zhou et al. (2020)</ref> includes the following steps:   For AAN, only pairs of documents that include abstracts are considered, and only their abstracts are used. For OC, only one citation per paper is considered and the dataset was downsampled significantly. For S2ORC, formed pairs of citing sections and the corresponding abstract in the cited paper are included, and the dataset was downsampled significantly. For PAN, pairs of relevant segments out of the test were extracted. For all the datasets, negative pairs are sampled randomly. Then, a standard preprocessing that includes filtering out characters that are not digits, letters, punctuation, or white space in the texts was performed. The complete dataset statistics are described in Appendix A.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Hyperparameters Setting and Training Details</head><p>In this section, we elaborate the hyperparameter choices for our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Cross-Document Coreference Resolution</head><p>We adopt the same protocol as suggested in <ref type="bibr">Cattan et al. (2020) 7</ref> : Our training set is composed of positive instances which consist of all the pairs of mentions that belong to the same coreference cluster, while the negative examples are randomly sampled. We fine-tune our models for 10 epochs, with an effective batch size of 128. The feature vector is passed through a MLP pairwise scorer</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>CD-LM pretraining: The input consists of concatenated documents, separated by the new special document separator tokens. The token colored in yellow represents global attention, and tokens colored in blue represent local attention. The goal is to predict the masked token given the output representation x i , based on the global context, i.e, the entire set of documents in the sample.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Cross-document perplexity evaluation on the validation set and test set of MultiNews. The lower is better.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Results on event cross-document coreference on ECB+ test set.</figDesc><table><row><cell>Model</cell><cell></cell><cell>MUC</cell><cell></cell><cell>B 3</cell><cell></cell><cell>CEAF e</cell><cell></cell><cell>LEA</cell><cell>CoNLL</cell></row><row><cell></cell><cell>R</cell><cell>P F1</cell><cell>R</cell><cell>P F1</cell><cell>R</cell><cell>P F1</cell><cell>R</cell><cell>P F1</cell><cell>F1</cell></row><row><cell>Same Head-Lemma</cell><cell cols="8">71.3 83.0 76.7 53.4 84.9 65.6 70.1 52.5 60.0 40.6 69.1 51.1</cell><cell>67.4</cell></row><row><cell>Barhom et al. (2019)</cell><cell cols="8">81.0 80.8 80.9 66.8 75.5 70.9 62.5 62.8 62.7 53.5 63.8 58.2</cell><cell>71.5</cell></row><row><cell>Cattan et al. (2020)</cell><cell cols="8">85.7 81.7 83.6 70.7 74.8 72.7 59.3 67.4 63.1 56.8 65.8 61.0</cell><cell>73.1</cell></row><row><cell>LONGFORMER</cell><cell cols="8">84.3 91.8 87.9 82.7 81.7 82.2 70.9 71.1 71.0 72.5 73.1 72.8</cell><cell>80.4</cell></row><row><cell>RAND CD-LM</cell><cell cols="8">84.4 91.2 87.7 80.1 80.8 80.4 74.1 71.0 72.5 72.1 75.2 73.6</cell><cell>80.2</cell></row><row><cell>CD-LM</cell><cell cols="8">88.1 91.8 89.9 82.5 81.7 82.1 81.2 72.9 76.8 76.4 73.0 74.7</cell><cell>82.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Results on entity cross-document coreference on ECB+ test set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Accuracy and F 1 scores of various baselines on the AAN test set. * indicates using a different version of the dataset 6 .</figDesc><table><row><cell></cell><cell>OC</cell><cell>S2ORC</cell><cell>PAN</cell></row><row><cell>Model</cell><cell>Acc F1</cell><cell>Acc F1</cell><cell>Acc F1</cell></row><row><cell cols="4">BERT-HAN+CDA 86.7 87.2 85.1 86.3 56.8 64.6</cell></row><row><cell>LONGFORMER</cell><cell cols="3">93.2 93.4 95.7 95.8 81.2 80.4</cell></row><row><cell>RAND CD-LM</cell><cell cols="3">93.4 93.5 94.6 94.6 81.3 79.4</cell></row><row><cell>CD-LM</cell><cell cols="3">95.4 95.3 96.5 96.5 83.9 82.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Accuracy and F 1 scores of various models on OC, S2ORC and PAN test sets.</figDesc><table><row><cell>concatenated the pair of texts together, according to</cell></row><row><cell>the finetuning setup discussed in Section 3. The hy-</cell></row><row><cell>perparameters are further detailed in Appendix B.2.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>MultiNews training set statistics.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Test topics: 36-45.</figDesc><table><row><cell></cell><cell>Train</cell><cell>Validation</cell><cell>Test</cell></row><row><cell>Topics</cell><cell>25</cell><cell>8</cell><cell>10</cell></row><row><cell>Docs</cell><cell>594</cell><cell>196</cell><cell>206</cell></row><row><cell cols="4">Mentions 3808/4758 1245/1476 1780/2055</cell></row><row><cell cols="4">Clusters 411/472 129/125 182/196</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 :</head><label>7</label><figDesc>ECB+ dataset statistics. The slash numbers for Mentions and Clusters represent event/entity statistics.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 8 :</head><label>8</label><figDesc>Document-to-Document benchmarks statistics: Details regrading the training, validation, and test splits.</figDesc><table><row><cell>Dataset</cell><cell># of doc pairs</cell><cell># of docs</cell></row><row><cell>AAN</cell><cell>132K</cell><cell>13K</cell></row><row><cell>OC</cell><cell>300K</cell><cell>567K</cell></row><row><cell>S2ORC</cell><cell>190K</cell><cell>270K</cell></row><row><cell>PAN</cell><cell>34K</cell><cell>23K</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 9 :</head><label>9</label><figDesc>Document-to-Document benchmarks statistics: The reported numbers are the count of document pairs and the count of unique documents.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">For more details, see the masking procedure of BERT<ref type="bibr" target="#b9">(Devlin et al., 2019)</ref> </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">HuggingFace implmentation, https://github. com/huggingface/transformers.4  The pretraining took 8 days, using eight 48GB RTX8000 GPUs.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">They utilized semantic role labeling to add features related to the arguments of each event mention.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">Following the most recent work of<ref type="bibr" target="#b43">Zhou et al. (2020)</ref>, we evaluate our model on their version of the dataset. We also quote the results of SMASH and SMITH methods, even though they used a somewhat different version of this dataset, hence their results are not fully comparable to the results of our model and those of BERT-HAN+CDA.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">we used the implementation taken from https:// github.com/ariecattan/cross_encoder</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">we used the script https://github.com/ XuhuiZhou/CDA/blob/master/BERT-HAN/run_ ex_sent.sh</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Yoav Goldberg and Luke Zettlemoyer for fruitful discussions and helpful feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Algorithms for scoring coreference chains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Bagga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Breck</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The first international conference on language resources and evaluation workshop on linguistics coreference</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="563" to="566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Revisiting joint modeling of cross-document entity and event coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shany</forename><surname>Barhom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vered</forename><surname>Shwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Eirew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bugert</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1409</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4179" to="4189" />
		</imprint>
	</monogr>
	<note>Nils Reimers, and Ido Dagan</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Longformer: The long-document transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Content-based citation recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Power</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1022</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="238" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Streamlining crossdocument coreference resolution: Evaluation and modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arie</forename><surname>Cattan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Eirew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Stanovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<idno>abs/2009.11032</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">What does BERT look at? an analysis of BERT&apos;s attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Urvashi</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-4828</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</title>
		<meeting>the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="276" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Using a sledgehammer to crack a nut? lexical diversity and event coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Cybulska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piek</forename><surname>Vossen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="4545" to="4552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Using a sledgehammer to crack a nut? lexical diversity and event coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Cybulska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piek</forename><surname>Vossen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC&apos;14)</title>
		<meeting>the Ninth International Conference on Language Resources and Evaluation (LREC&apos;14)<address><addrLine>Reykjavik, Iceland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="4545" to="4552" />
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">bag of events&quot; approach to event coreference resolution. supervised classification of event templates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Cybulska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piek</forename><surname>Vossen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Linguistics Appl</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The hitchhiker&apos;s guide to testing statistical significance in natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rotem</forename><surname>Dror</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gili</forename><surname>Baumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Segev</forename><surname>Shlomov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1128</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1383" to="1392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-news: A large-scale multi-document summarization dataset and abstractive hierarchical model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Fabbri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irene</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1102</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1074" to="1084" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-news: A large-scale multi-document summarization dataset and abstractive hierarchical model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Fabbri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irene</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1102</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1074" to="1084" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hierarchical document encoder for parallel corpus mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandy</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heming</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Hsuan</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Strope</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ray</forename><surname>Kurzweil</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-5207</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Conference on Machine Translation</title>
		<meeting>the Fourth Conference on Machine Translation<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="64" to="72" />
		</imprint>
	</monogr>
	<note>Research Papers)</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semantic text matching for long-form documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jyun-Yu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bendersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadav</forename><surname>Golbandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Najork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
		<imprint>
			<publisher>WWW</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">BERT for coreference resolution: Baselines and analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Weld</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1588</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5803" to="5808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Resolving event coreference with supervised representation learning and clusteringoriented regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kian</forename><surname>Kenyon-Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jackie Chi Kit</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/S18-2001</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics</title>
		<meeting>the Seventh Joint Conference on Lexical and Computational Semantics<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Pre-training via paraphrasing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gargi</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armen</forename><surname>Aghajanyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hierarchical transformers for multi-document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1500</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5070" to="5081" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">RoBERTa: A robustly optimized BERT pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">S2ORC: The semantic scholar open research corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><forename type="middle">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodney</forename><surname>Kinney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Weld</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.447</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4969" to="4983" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On coreference resolution performance metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing</title>
		<meeting>the conference on Human Language Technology and Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Paraphrasing vs coreferring: Two sides of the same coin</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehudit</forename><surname>Meged</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avi</forename><surname>Caciularu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vered</forename><surname>Shwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.440</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4897" to="4907" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Which coreference evaluation metric do you trust? a proposal for a link-based entity aware metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadat</forename><surname>Nafise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Moosavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Strube</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1060</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="632" to="642" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Overview of the 5th international competition on plagiarism detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Gollub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Tippmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Kiesel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstathios</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Multilingual and Multimodal Information Access Evaluation (CLEF)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The acl anthology network corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Dragomir R Radev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vahed</forename><surname>Muthukrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amjad</forename><surname>Qazvinian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Abu-Jbara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language Resources and Evaluation</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="919" to="944" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Acquiring predicate paraphrases from news tweets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vered</forename><surname>Shwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Stanovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/S17-1019</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017)</title>
		<meeting>the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017)<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="155" to="160" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Stance detection with hierarchical attention network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingying</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaoming</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics<address><addrLine>Santa Fe, New Mexico, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2399" to="2409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.04006</idno>
		<title level="m">Long range arena: A benchmark for efficient transformers</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A modeltheoretic coreference scoring scheme</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Vilain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Aberdeen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th conference on Message understanding</title>
		<meeting>the 6th conference on Message understanding</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1995" />
			<biblScope unit="page" from="45" to="52" />
		</imprint>
	</monogr>
	<note>Dennis Connolly, and Lynette Hirschman</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">GLUE: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-5446</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 EMNLP Workshop Black-boxNLP: Analyzing and Interpreting Neural Networks for NLP</title>
		<meeting>the 2018 EMNLP Workshop Black-boxNLP: Analyzing and Interpreting Neural Networks for NLP<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="353" to="355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Linformer: Selfattention with linear complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Belinda</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04768</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">BERT post-training for review reading comprehension and aspect-based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1242</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2324" to="2335" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Beyond 512 tokens: Siamese multi-depth transformer-based hierarchical encoder for long-form document matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bendersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Najork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Information &amp; Knowledge Management (CIKM)</title>
		<meeting>the ACM International Conference on Information &amp; Knowledge Management (CIKM)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">XLNet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Russ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">HotpotQA: A dataset for diverse, explainable multi-hop question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1259</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2369" to="2380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1174</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Paired representation learning for event and entity coreference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.12808</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Big bird: Transformers for longer sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guru</forename><surname>Guruganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kumar Avinava Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Ontanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifan</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Event coreference resolution with their paraphrases and argument-aware embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saiping</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics<address><addrLine>Barcelona, Spain (Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3084" to="3094" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">PEGASUS: Pre-training with extracted gap-sentences for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">HI-BERT: Document level pre-training of hierarchical bidirectional transformers for document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1499</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5059" to="5069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Multilevel text alignment with crossdocument attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuhui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.407</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5012" to="5025" />
		</imprint>
	</monogr>
	<note type="report_type">Online</note>
	<note>that is composed of one hidden layer of the size of 1024, followed by the Tanh activation</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Paper Citation Recommendation &amp; Plagiarism Detection We fine-tune our models and BERT-HAN + CDA for 8 epochs, using a batch size of 32, and used the same hyperparameter setting from Zhou et al. (2020, Section 5.2) 8 . We used the mixed-precision training mode</title>
		<imprint/>
	</monogr>
	<note>to reduce time and memory consuption</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
