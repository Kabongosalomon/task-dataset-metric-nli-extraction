<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Multi-Target Domain Adaptation Through Knowledge Distillation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">T</forename><surname>Nguyen-Meidine</surname></persName>
							<email>le-thanh.nguyen-meidine.1@ens.etsmtl.ca</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Belal</surname></persName>
							<email>abelal@myamu.ac.in</email>
							<affiliation key="aff1">
								<orgName type="institution">Aligarh Muslim University</orgName>
								<address>
									<settlement>Aligarh</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kiran</surname></persName>
							<email>mkiran@livia.etsmtl.ca</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dolz</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L-A</forename><surname>Blais-Morin</surname></persName>
							<email>lablaismorin@genetec.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Genetec Inc</orgName>
								<address>
									<settlement>Montreal</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Granger</surname></persName>
							<email>eric.granger@etsmtl.ca</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">LIVIA</orgName>
								<address>
									<addrLine>École de technologie supérieure</addrLine>
									<settlement>Montreal</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Multi-Target Domain Adaptation Through Knowledge Distillation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Unsupervised domain adaptation (UDA) seeks to alleviate the problem of domain shift between the distribution of unlabeled data from the target domain w.r.t. labeled data from the source domain. While the singletarget UDA scenario is well studied in the literature, Multi-Target Domain Adaptation (MTDA) remains largely unexplored despite its practical importance, e.g., in multi-camera video-surveillance applications. The MTDA problem can be addressed by adapting one specialized model per target domain, although this solution is too costly in many real-world applications. Blending multiple targets for MTDA has been proposed, yet this solution may lead to a reduction in model specificity and accuracy. In this paper, we propose a novel unsupervised MTDA approach to train a CNN that can generalize well across multiple target domains. Our Multi-Teacher MTDA (MT-MTDA) method relies on multi-teacher knowledge distillation (KD) to iteratively distill target domain knowledge from multiple teachers to a common student. The KD process is performed in a progressive manner, where the student is trained by each teacher on how to perform UDA for a specific target, instead of directly learning domain adapted features. Finally, instead of combining the knowledge from each teacher, MT-MTDA alternates between teachers that distill knowledge, thereby preserving the specificity of each target (teacher) when learning to adapt to the stu-dent. MT-MTDA is compared against state-of-the-art methods on several challenging UDA benchmarks, and empirical results show that our proposed model can provide a considerably higher level of accuracy across multiple target domains. Our code is available at: https://github.com/LIVIAETS/MT-MTDA.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep Learning (DL) models, and in particular Convolutional Neural Networks (CNNs), have achieved state-of-the-art performance in many visual recognition applications such as image classification, detection and segmentation <ref type="bibr" target="#b9">[10]</ref>. Despite their success, several factors limit their deployment in real-world industrial applications. Among these factors is the problem of domain shift, where the distribution of original training data (source domain) diverges w.r.t data from the operational environment (target domain). This problem often translates to a notable decline in performance once the DL model has been deployed in the target domain.</p><p>To address this problem, DL models for domain adaptation have been proposed to align a discriminant source model with the target domain using data captured from the target domain <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b28">27]</ref>. In unsupervised domain adaptation (UDA), a large amount of unlabeled data is often assumed to have been collected from the target domain to avoid the costly task of annotating data. Currently, several conventional and DL models have been proposed for the single target domain adaptation (STDA) setting, using unlabeled data that is collected from a single target domain. These models rely on different approaches, ranging from the optimization of a statistical criterion to the integration of adversarial losses, in order to learn robust domain-invariant representations from source and target domain data. However, despite multi-target domain adaptation (MTDA) scenario, i.e. multiple unlabeled target domains, has many realworld applications, it remains virtually unexplored. For instance, in video-surveillance applications, each camera of a distributed network corresponds to a different non-overlapping viewpoint (target domain). A DL model for person re-identification <ref type="bibr" target="#b20">[20]</ref> should normally be adapted to multiple different camera viewpoints.</p><p>Extension of STDA techniques to the MTDA setting is not straightforward, and they may perform poorly on multiple target domains. Although MTDA problems can be solved by producing one model per target domain, this approach becomes costly and impractical in applications with a growing number of target domains. In such cases, a MTDA approach should ideally yield a common DL model that is compact and has been adapted to perform accurately across all target domains. To adapt a common multi-target DL model, one recent MTDA approach considers the problem of MTDA without domain labels, and proposes an approach to blend all the target domains together, which may lead to a reduction in model specificity and accuracy <ref type="bibr" target="#b4">[5]</ref>. While the current approach provides an interesting direction in adapting a common model to multiple target domains, we argue that directly adapting a model to multiple target domains can affect the performance since there are limitations on a model's capacity to learn and generalize in diverse target domains. Other works on MTDA have focused on the problem of unshared categories between target domains <ref type="bibr" target="#b31">[30]</ref>, nevertheless, this scenario has not been considered since it is outside the scope of this work.</p><p>In this paper, a novel MTDA learning strategy referred to as Multi-Teacher MTDA (MT-MTDA) is proposed to train a common CNN to perform well across multiple target domains. Our strategy relies on knowledge distillation to efficiently transfer information from several different target domains, each one associated with a specialized teacher, to a single common multi-target model. <ref type="figure">Figures 1(a)</ref>-(c) illustrate the different MTDA strategies from literature, evolving from strategies that adapt a single CNN per target domain, to strategies that adapt a common CNN across all target domains. Our novel MT-MTDA approach (illustrated in <ref type="figure">Figure 1(d)</ref>) is inspired by a common education scenario, where each teacher is responsible for a single subject (i.e. target domain), and these teachers sequentially educate a student to learn all the subjects.</p><p>In our MT-MTDA approach: (1) Since only the student performance is important after training, we can resort to complex architecture for the teacher model;</p><p>(2) These complex teachers can provide a higher capacity to generalize toward a single target domain instead of having one model learning multiple target domains; (3) The student model learns compressed knowledge from teachers across target domains, instead of directly learning to generalize on multiple domains; and (4) MT-MTDA can benefit from different STDA algorithms since each teacher adapt to only one target.</p><p>We also propose an efficient alternative for the fusion of knowledge from multiple teachers. State-of-theart techniques for multi-teacher knowledge distillation rely on average fusion (sum operations) to directly combine the information derived by teachers <ref type="bibr" target="#b25">[24]</ref>. To preserve the specificity of individual teachers, we let our student model learn to adapt from each teacher separately and sequentially from teacher to teacher. We argue that having better preservation of target specificity leads to higher accuracy.</p><p>Finally, the proposed MT-MTDA is compared extensively to state-of-the-art strategies on widely used UDA benchmarks (OfficeHome, Office31, and Digits-5), and show that MT-MTDA consistently achieves a high level of accuracy across multiple target domains with different backbone network architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work:</head><p>Single Target Domain Adaptation. STDA is an unsupervised transfer learning task that focuses on <ref type="figure">Figure 1</ref>: Illustration of different STDA and MTDA strategies for training CNNs across multiple target domains. S is the labelled source dataset, while T i are the unlabelled target datasets for i = 1, 2..., n.</p><p>adapting a model such that it can generalize well on an unlabeled target domain data while using a labeled source domain dataset. DL models for UDA seek to learn discriminant and domain-invariant representations from source and target data <ref type="bibr" target="#b27">[26]</ref>. They are either based on either adversarial- <ref type="bibr" target="#b7">[8]</ref>, discrepancy- <ref type="bibr" target="#b17">[17]</ref>, or reconstruction-based approaches <ref type="bibr" target="#b6">[7]</ref>. Taking advantage of adversarial training, several methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b26">25,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b2">3]</ref> have been proposed using either gradient reversal <ref type="bibr" target="#b7">[8]</ref> or a combination of feature extractor and domain classifier to encourage domain confusion. Discrepancy-based approaches <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b14">14]</ref> rely on measures between source and target distributions that can be minimized to generalize on the target domain. In <ref type="bibr" target="#b17">[17]</ref>, authors minimize the Maximum Mean Discrepancy (MMD) between target and source features to find domain invariant features. On the other hand, <ref type="bibr" target="#b14">[14]</ref> assumes that task knowledge is already learned and the domain adaptation is done on the batch normalization layer to correct the domain shift. Lastly, another set of domain adaptation techniques focuses on the mapping of the source domain to target domain data or vice versa <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13]</ref>. These techniques are often based on the use of Generative Adversarial Network (GAN) in order to find a mapping between source and target.</p><p>Knowledge Distillation (KD). KD techniques allow for model compression by transferring knowledge from a teacher model, usually complex, to a smaller compact student model. The two main approaches of transferring knowledge between teacher and student models consist in minimizing the difference between logits <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">15]</ref>, and between features maps <ref type="bibr" target="#b30">[29,</ref><ref type="bibr" target="#b24">23,</ref><ref type="bibr" target="#b10">11]</ref>. Techniques from the first approach focus on measuring logits obtained from a temperature-based softmax and then minimize the distance between the logits of the teacher and the student <ref type="bibr" target="#b11">[12]</ref>. More recently, techniques like <ref type="bibr" target="#b10">[11]</ref> minimize the distance between the intermediate feature maps of the teacher and student using a partial L2 distance. In contrast with other techniques, these features are obtained using a margin ReLU that accounts for negative values of the feature map. KD has been also recently employed in STDA <ref type="bibr" target="#b25">[24,</ref><ref type="bibr" target="#b22">21]</ref>. For example, in <ref type="bibr" target="#b25">[24]</ref>, the authors use multiple teachers and employ a fusion scheme that sums the output of each teacher as distillation strategy. In a similar work <ref type="bibr" target="#b22">[21]</ref>, STDA is performed during compression using knowledge distillation. While their approach is limited to a single-target scenario, we extend this approach to an MTDA setting by leveraging multi-teacher distillation into a single common student.</p><p>Multi Target Domain Adaptation. MTDA is a set of domain adaptation techniques that improves upon the single target domain adaptation by adapting a single model to teacher target domains. Currently, MTDA still remains largely unexplored with many open research questions. The few existing MTDA approaches follow two main directions: MTDA either with target domain labels <ref type="bibr" target="#b8">[9]</ref> or without target domain labels <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b23">22]</ref>. The work in <ref type="bibr" target="#b8">[9]</ref> proposes an approach that can adapt a model to multiple target domains by maximizing the mutual information between domain labels and domain-specific features while minimizing the mutual information between the shared features. Recently, <ref type="bibr" target="#b4">[5]</ref> proposed to blend multiple target domains together and minimize the discrepancy between the source and the blended targets. Additionally, the authors employ an unsupervised meta-learner in combination with a meta target domain discriminator in order to blend the target domains. In <ref type="bibr" target="#b16">[16]</ref>, authors use a curriculum domain adaptations strategy combined with an augmentation of the representation based on features from source domain to handle multiple-target domains. While these methods achieve good performance, they fail to take advantage of existing STDA techniques, which have been extensively studied. Another important common point to existing methods is that they try to capture the representation of all the target domains using a common feature extractor directly from the data, which can degrade the final accuracy because of the limited capacity of the common model. In our paper, we overcome this issue by performing UDA separately on different models, and then distilling compressed knowledge to a common model. In addition, our experiments show that current mixed-target approaches still struggle with blending target domains in the feature space. We can gain more by preserving each domain specificity using STDA on different models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Domain Adaptation of Teachers:</head><p>In this paper, the RevGrad <ref type="bibr" target="#b7">[8]</ref> technique is employed since it is the basis for many popular methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4]</ref>, although it can be easily replaced by other STDA techniques. Let us define the source domain as S = {x s , y s } where x s is input pattern, and y s its corresponding label. The set of target domains is defined as T = {T 1 , T 2 , ...T n }, each one defined as</p><formula xml:id="formula_0">T i = {x i t }.</formula><p>For each target domain T i , we define a teacher model Φ i , and each of these teachers will be adapted to a corresponding target domain using the UDA technique proposed in <ref type="bibr" target="#b7">[8]</ref>. The domain adaptation of the teacher relies on a domain classifier, a gradient reversal layer (GRL), and the domain confu-sion loss:</p><formula xml:id="formula_1">L DC (φ i , S, T i ) = 1 Ns + N ti x∈S∪T i L CE (D i (φ i (x)), d l ) (1)</formula><p>where φ i (x) is the output from the feature extractor of teacher network Φ i , before the fully connected layers, D i is the domain classifier for the corresponding teacher network, d l the domain label (source or target), N s is the number of samples in the source domain S, and N ti is the number of samples in the target domain T i . The final domain adaptation loss is then defined as:</p><formula xml:id="formula_2">L DA (Φ i , S, T i ) = 1 Ns xs,ys∈S L CE (Φ i (xs), ys) + γ · L DC (φ i , T i )<label>(2)</label></formula><p>The first term (cross-entropy loss) allows the supervised training of the teacher model on the source domain that ensures the consistency of domain confusion. The second term is controlled by a hyper-parameter γ that regulates the importance of the domain confusion loss which is maximized using a gradient reversal layer. <ref type="figure">Figure 2</ref> illustrates how GRL is applied for UDA. <ref type="figure">Figure 2</ref>: Illustration of GRL applied on a teacher model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Teacher to Student Knowledge Distillation:</head><p>In this paper, we employ knowledge distillation based on logits as in [12] 1 . The <ref type="figure" target="#fig_0">Figure 3</ref> illustrates the overall process of distillation on both target and source domains. Logits from a teacher/student model are fed to a temperature-based softmax function, in combination with a KL divergence loss on both the teacher and student outputs:</p><formula xml:id="formula_3">L Source KD (Φ i , Θ, S) = 1 Ns xs,ys∈S L KL (Φ i (xs, τ ), Θ(xs, 1)) +α · L CE (Θ(xs, 1), ys)<label>(3)</label></formula><p>where Θ represents our student model with τ the temperature hyper-parameter the softmax, and α the hyper-parameter to regulate the importance of the cross-entropy term. Even though the second term of Eq. 3 may perform well with data from the source domain because it has labels, we add the domain confusion loss (Eq. 1) on the target domain to provide consistency during target distillation: </p><formula xml:id="formula_4">L T arget KD (Φ i , Θ, T i ) = 1 N ti x∈T i L KL (Φ i (x, τ ), Θ(x, 1)) +α · L DC (Θ, T i ) (4)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Multi-Teacher Multi-Target DA:</head><p>For progressive UDA of teacher models and transfer of knowledge from teacher to the student model, we adapt an exponential growing rate to gradually transfer the importance of UDA to KD in a similar way to <ref type="bibr" target="#b22">[21]</ref>. The growth rate is defined as:</p><formula xml:id="formula_5">g = log(f /s) Ne (5)</formula><p>where s is the starting value, f the final value, and N e the number of total epochs. This growth rate is used to calculate β = s · exp{g · e} with e the current epoch in the overall loss function for optimization of one teacher:</p><formula xml:id="formula_6">L(Φ i , Θ, S, T i ) = (1 − β)L DA (Φ i , T i )+ β(L Source KD (Φ i , Θ, S) + L T arget KD (Φ i , Θ, T i ))<label>(6)</label></formula><p>1 Note that our method can work with any other technique.</p><p>With β, the value that balances between the importance of the domain adaptation loss and the distillation loss. Our approach, MT-MTDA, instead of using deterministic fusion functions, such as average fusion, employs an alternative learning scheme for knowledge distillation from multiple teachers. This alternative scheme is performed by sequentially looping through each teacher at batch level (see Algorithm 1).</p><formula xml:id="formula_7">Algorithm 1: Multi-Teacher Multi-Target Do- main adaptation (MT-MTDA) input</formula><p>: A source domain dataset S, a set of target dataset T 0 , T 1 , ...Tn output : A student model adapted to n targets Initialize a set of teachers models Φ = {Φ 0 , Φ 1 , ...Φn} Initialize a student model Θ for e ← 1 to Ne do for xs ∈ S and Xt ∈ {T 0 , ...Tn} do Get the set of data of target domains Xt for   <ref type="figure" target="#fig_2">Figure 4</ref> illustrates the overall pipeline for our MT-MTDA approach. While all teachers share the same source dataset, the figure shows that they each teacher has its own target dataset with their own domain adaptation loss.</p><formula xml:id="formula_8">x i t ∈ Xt and Φ i ∈ Φ do Optimize (1 − β)L DA (2) for Φ i using xs, x i t</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Optimize the loss of equation βL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets:</head><p>To the best of our knowledge, no specific dataset has been created for the for the MTDA task. For validation, we rely on datasets that are commonly used in other MTDA research <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9]</ref>, which are described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Digits:</head><p>This dataset regroups a set of digits datasets: MNIST (mt), MNIST-M (mm), SVHN (sv), USPS (up) and Synthetic Digits (sy). Each one has each 10 classes that represent all the digits. For the evaluation on this dataset, we follow the same protocol as in <ref type="bibr" target="#b4">[5]</ref> for a fair comparison.</p><p>2) Office31: It has 3 subsets -Amazon (A), DSLR (D) and Webcam (W). These datasets all have 31 common classes. Images are taken respectively from the Amazon website, a DSLR camera and a webcam. We followed the standard evaluation protocol, a domain is chosen as a source, and the rest as targets.</p><p>3) OfficeHome: This dataset contains 4 subsets: Art (Ar), Clip Art (Cl), Real World (Rw) and Product (Pr). It has a total of 15,500 images for 65 object categories that are usually found in office or home settings. We follow the same evaluation protocol of Office31.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) PACS:</head><p>While this dataset is often used for domain generalization, <ref type="bibr" target="#b8">[9]</ref> used it for MTDA. It contains 4 subsets: Art painting (Ap), Photo (P), Cartoon (Cr) and Sketch (S).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details:</head><p>In MT-MTDA, we use the same number of optimizers as teacher models, which are responsible for the UDA of each teacher. Additionally, we add another optimizer for the knowledge distillation of the student. MT-MTDA is compared to 1) a lower bound, which is only trained on source and tested on target, 2) the current state-of-the-art in MTDA with domain labels such as MTDA-ITA <ref type="bibr" target="#b8">[9]</ref> and 3) MTDA without domain labels such as AMEANS <ref type="bibr" target="#b4">[5]</ref>. Additionally, we compare to baseline methods such as RevGrad <ref type="bibr" target="#b7">[8]</ref> which is the basis of our MTDA method. We also use other baselines like DAN <ref type="bibr" target="#b17">[17]</ref> or ADDA <ref type="bibr" target="#b26">[25]</ref> in some cases for additional comparison, to show the advantages of MTDA algorithms. These baselines are domain adapted to directly to an ensemble of target domains similar to <ref type="bibr" target="#b4">[5]</ref>. For the Digits-five dataset, we employ a LeNet backbone with ResNet50 as teacher. As for the comparison on Office31 and OfficeHome, we use AlexNet backbone with ResNet50 as teacher models, and as for the comparison on the ResNet50 backbone, we use a ResNext101 as teachers. Our backbone CNNs follows the choices in <ref type="bibr" target="#b4">[5]</ref>. All these models start with pre-trained weights from ImageNet, except for LeNet.</p><p>When comparing with AMEANS <ref type="bibr" target="#b4">[5]</ref> and DADA <ref type="bibr" target="#b23">[22]</ref>, we add MT-MTDA Mixed -our method when it employs mixed target domains without target domain labels. Specifically, we mix data from all the target domains, split them into subset of equal size without using domain labels, and then directly used them for UDA of respective teachers. For a fair comparison with AMEANS <ref type="bibr" target="#b4">[5]</ref>, we chose the same number of clusters, which corresponds to the number of target domains.</p><p>We selected the models' hyper-parameters based on their overall result in cross-validation in all the scenarios, instead of having a set of dedicated hyperparameters for each scenario. Details of our hyperparameters can be found in the Suppl. Material. We report the average classification accuracy obtained by all implemented models over 3 replications, from all the target domains. For other baselines, we report their best published result for fair comparison. Additional results (with OCDA <ref type="bibr" target="#b16">[16]</ref>, DADA <ref type="bibr" target="#b23">[22]</ref>) and ablation studies (fusion methods, number of splits, etc.) are shown and analysed, along with a weighted average accuracy version of MT-MTDA in the Supplementary Material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results and Discussion:</head><p>Comparison without domain labels <ref type="table" target="#tab_0">Table 1</ref> shows the average classification accuracy of the MT- MTDA versus baseline and state-of-the-art methods on the Digits-Five dataset. We observe that our technique provides a higher level of accuracy, on average than the other approaches. In the first scenario, where our method performs poorly, further analysis on separate target domains (in Suppl. Material) indicates that our teacher models did not adapt well to the mm and sy datasets. This is mainly due to our selection of hyper-parameters, which was based on the all-scenario setting instead of individual cases. This explains why, in our first scenario, the result lags behind current baselines. It is possible, however, to overcome this problem by optimizing each scenario with a different set of hyper-parameters, including each teacher.</p><p>Comparing the performance between the MT-MTDA Mixed (without domain labels) and MT-MTDA (with domain labels), the former improved slightly which might be due to the fact that the Mixed version is less susceptible to the sharing of hyper-parameters. Finally, an upper bound is included, i.e., trained and tested on target domain data, to show the gap between with a supervised model.  <ref type="table" target="#tab_1">Tables 2 and 3</ref> present the average classification accuracy of the MT-MTDA versus baseline and stateof-the-art methods on Office31 and OfficeHome data, respectively. In both cases, we observe that MT-MTDA, Mixed or with target domain labels, typically outperform the current state-of-the-art methods. With the AlexNet backbone, the improvements are significant, which can be explained by the advantage of using KD from multiple complex teachers, leading to a better generalization on a single target domain. We can observe that on Office31, AMEANS performs slightly better that MT-MTDA with the larger ResNet50 backbone. We believe that this is due to the limitations of domain adaptation on teacher models with MT-MTDA. We further discuss this point in the ablation study where we compare and discuss the performance of teacher and student (Sec. 4.4). Using our Mixed version on both these datasets, we often achieve similar results to the version with target domain labels. Comparison with domain labels From <ref type="table" target="#tab_3">Table 4</ref>, we observe that, in a comparison with another MTDA technique that uses domain labels <ref type="bibr" target="#b8">[9]</ref>, our method can have an improvement up to 15-16%. Similar to other comparisons, the boost in our performance is due to the use of teacher models with higher capacity to generalize then distilled to the student.</p><p>Overall, our MT-MTDA technique outperforms both the baselines and state-of-the-art techniques. From Tables 1, 2, 3 and 4, we noticed that our model generally provides the highest accuracy on a compact backbone CNN, mainly because of the teacher's complexity and our knowledge distillation process. This is further confirmed by a comparison with the baseline RevGrad <ref type="bibr" target="#b7">[8]</ref> technique adapted directly on multi-target domains. Additionally, the improvements   in accuracy that our methods brings decrease when the complexity gap between the teachers and student is smaller. In this case, the performance bottleneck is the teacher and the distillation algorithm. We further discuss this point in the ablation study, when comparing between student and teacher models. Finally, the difference between our versions is very small, meaning that our technique can perform well without domain labels and still preserve a high level of accuracy. This indicates that our teacher models can generalize well on multiple sub-mixtures of target domains without reducing performance. From <ref type="figure" target="#fig_3">Figure 5</ref> 2 , we observe that features learned with MT-MTDA better separates Office31 features, compared to other reference methods. Furthermore, MT-MTDA also separates class samples from different target domains in a better way than AMEANS. For comparison purposes, representations of other baselines are provided in the Suppl. Material. We noted that in current state-of-the-art method, the target domains do not blend well with each other since the feature extractor can still differentiate them quite well based on the t-SNE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>Detailed Comparison of Each Target Domain. For this experiment, we compare MT-MTDA in a setting where each target domain has a specific model. Our result on separate target domains is compared with RevGrad <ref type="bibr" target="#b7">[8]</ref>, but trained on a single target domain adaptation task. We also compared with the current best STDA algorithm, to our knowledge, in order to evaluate the effect of having a better STDA for the teacher model in our method.   <ref type="table" target="#tab_4">Table 5</ref> shows that while our algorithm does not perform as well as the state-of-the-art in STDA, it requires less computational power and memory since it only uses one model for all the target models instead of having a specific model for each target domain, i.e. two models in this case. This means that MTDA methods would typically scale better than current STDA methods since the number of models does not depend on the number of target models. Additionally, DM-ADA <ref type="bibr" target="#b29">[28]</ref> shows that our algorithm can still be further improved since we can replace the STDA algorithm we are using on the teacher models (RevGrad <ref type="bibr" target="#b7">[8]</ref>) with almost any other STDAs, i.e. CDAN <ref type="bibr" target="#b18">[18]</ref>. The table also shows the cosine distance in order to quantify the domain shift between source and target features for each UDA problem. Results show that the UDA problems in Office31 have a similar level of difficulty.</p><formula xml:id="formula_9">AlexNet A − → D,W D − → A W W − → A D A − → D A − → W Avg D − → A D − → W Avg W − → A W − → D Avg</formula><p>Teachers vs Student Performance. We now compare the performances of our teachers with the student in order to explore the impact of knowledge distillation. For this experiment, we select the accuracy of each separate teacher on their respective target domain and compare it with the result of our student. This comparison was performed on the Of-fice31 dataset 4 . From <ref type="table" target="#tab_6">Table 6</ref>, the gap in accuracy is small and the student is almost at the same level as the teachers, except for the case of A − → D,W.</p><p>3 These results were obtained on the model provided in the authors's github, hence it is slightly different from the result reported in the original paper used in <ref type="table" target="#tab_1">Table 2</ref> 4 Additional results on OfficeHome can be found in the Supp. Material This indicates that our student model has learned how to adapt to multi-target domains from each separate teacher without an explicit fusion scheme. The first scenario of A − → D,W shows a particular case where knowledge distillation helps improving domain adaptation. This behavior is also found when using ResNet50 as backbone architecture and seems to happen when the gap between the teachers and student is very small. This also suggests that knowledge from other teachers also help increasing accuracy. Additionally, from the ResNet50 backbone, we can see that the bottleneck can be found on teacher models and its domain adaptation since the student is stuck with a very similar accuracy as the teachers. Consistency on Target Knowledge Distillation.</p><p>In this section, we evaluate the impact of the consistency loss on the target knowledge distillation 4. To this end, we removed the second term of the target knowledge distillation, Eq. 4, completely and we run our algorithm with the same settings as before on the scenario with an AlexNet as backbone on Office31 dataset. From the <ref type="table" target="#tab_9">Table 9</ref>, it seems that having a consistency term on the target distillation loss only brings a small boost in performance. This aligns with our main results since the hyper-parameter that controls this consistency term is set to a small value.</p><p>Single Teacher vs Multiple Teachers. We compare the scenario of having multiple teachers, each in a different target domain versus a scenario with one teacher adapting on a mixed target domain similar to <ref type="bibr" target="#b22">[21]</ref>. In this setting, we merge all the target domains into a single target domain, where a single teacher is then assigned. We run this study on the Office31 dataset with the same hyper-parameters as in the main experiment. From the results of <ref type="table" target="#tab_0">Table 10</ref>, we can observe that the accuracy of a mixed target domain using similar algorithm to <ref type="bibr" target="#b22">[21]</ref> is significantly lower than the results with a multi-teacher approach.   This suggests that even with a complex teacher network, a good generalization on a mixed target domain is hard to achieve and a multi-teacher scenario is preferable. Impact of Number of Target Domains. We now investigate the impact of increasing the number of domains on the student model. This experiment starts with a STDA setting of our algorithm and slowly increases the number of domains until reaching the maximum. We decided to do this experiment on the scenario with Ar dataset as source in OfficeHome dataset since it has more than two target domains and the dataset is bigger than Digits-Five. From <ref type="table" target="#tab_7">Table 7</ref>, we can see that while the performance degrades on the target domain Pr, we notice a slight increase in accuracy of the other cases. This means that, with our method, training multiple target domains together can boost the performance of some separate target domains. The decrease of performance in the case of Pr also indicates that there might be a saturation in learning capacity. In this case, we can say that the target domains Cl and Rw improved at the expense of Pr.</p><p>Order of Target Domains. We evaluate whether the order of target domains impacts the performance of the final model. Similarly to the previous experiment, we used the scenario with Ar as the source domain in OfficeHome dataset since there are more than 2 target domains. <ref type="table" target="#tab_8">Table 8</ref> reports the results of individual target domains when their orders are different. These results indicate that even though the order of the domains leads to different average results, the difference between the configurations is marginal, of nearly 0.3%, with a standard deviation equal to 0.1. These results indicate that the order of target domains has little impact, if any, on the final result of the trained models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, an avenue is unexplored for MTDA, relying on multiple teachers in order to distill knowledge from multiple target domains into a single student. Results from our experiment show that our method outperforms the current state-of-the-art, especially when using compact models, which can facilitate the use in numerous real-time applications. From our experiment, we identify several bottlenecks that can impede generalization of a compact model to multiple domains: 1) the STDA algorithm determines the accuracy of teacher models and 2) the transfer of target domain knowledge which needs to be improved when the student model is compact. Since STDA is a popular research area, our future work will focus on how to transfer target domain knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material A Experimental Methodology</head><p>A.1 Hyper-parameters</p><p>From <ref type="table" target="#tab_0">Table 11</ref>, you can find all the hyper-parameters that was used for different datasets and backbones. We selected these hyper-parameters based on a standard cross-validation process. These hyperparameters are selected based on the overall result in all the scenarios instead of each scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Evaluation metrics</head><p>In the paper of <ref type="bibr" target="#b4">[5]</ref>, the authors first proposed an accuracy metrics that take in account the different sizes of each target domain in order to have a balanced accuracy score at the end. This accuracy is defined as:</p><formula xml:id="formula_10">Acc = n i=0 w i Acc i<label>(7)</label></formula><p>With w i calculated as w i = Ni n j=0 Nj . The problem with this accuracy is that it can hide the poor performance of a target domain that is small. The authors from the same paper proposed to use another accuracy which is the same one we used in our main paper where the same weight is used for each target domain. This is also referred as the equal-weight classification accuracy in the paper of <ref type="bibr" target="#b4">[5]</ref>. This is accuracy is calculated as:</p><formula xml:id="formula_11">Acc EQ = 1 n n i=0 Acc i<label>(8)</label></formula><p>Additionally, we also give our result based on the Equation 7 and on each target domain in order to highlight where our algorithm can fail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Results and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Digits-Four</head><p>In this comparison, we compare our results with both MTDA-ITA <ref type="bibr" target="#b8">[9]</ref> and <ref type="bibr" target="#b16">[16]</ref> on the same scenario as in <ref type="bibr" target="#b16">[16]</ref> on Digits. For a fair comparison with <ref type="bibr" target="#b16">[16]</ref>, which is an open domain adaptation algorithm, we only takes in account the compound domains results of OCDA (close domain adaptation) since they are part of the closed domain of OCDA which is similar to our setting.</p><p>From Table12, we observe that, while our base techniques (with RevGrad) perform slightly worse than OCDA, it still performs better than most of the other techniques. As for our version that uses CDAN <ref type="bibr" target="#b18">[18]</ref>, our technique perform even better than state-of-the-art technique <ref type="bibr" target="#b16">[16]</ref> by around 1%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Comparison with DADA[22] on</head><p>Office-Caltech 10</p><p>In this experiment, we provide an additional comparison with DADA <ref type="bibr" target="#b23">[22]</ref> on the Office-Caltech10 dataset with AlexNet and ResNet101 as backbones similar to DADA <ref type="bibr" target="#b23">[22]</ref>. For this, we use similar hyper-parameters as the Office31 dataset. From <ref type="table" target="#tab_0">Table 13</ref>, we can see that both version of our techniques perform better than DADA <ref type="bibr" target="#b23">[22]</ref>. Similar to previous comparison, we think that our technique achieves state-of-the-art performance by taking advantages of higher generalization capacity of teacher models and transfer it to a common student.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Further analysis on Digits-Five</head><p>As indicated in our results for <ref type="table" target="#tab_0">Table 1</ref>, we analyzed the accuracy of each target domain in order to show where's our drop in accuracy.</p><p>From <ref type="table" target="#tab_0">Table 14</ref>, we can see that the drop in performance in the scenario previously noted in the main paper is due to the decline of the domain adaptation on both mt − → mm and mt − → sy. Further analysis of these two domain adaptation shows that our common hyper-parameters do no work well for these two cases since we can get better performance using other hyper-parameters. However, these parameters would yield lower performance on other scenarios therefore we choose to remain on the same hyperparameters as before. This indicates that in order to have better performance, it is best to have a different    hyper-parameters set for each scenario and even each teacher.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Number of splits for mixed target domains</head><p>In this experiment, we evaluate the impact of the number of splits with our technique on the Office31 dataset with AlexNet as backbone. We gradually increase the number of splits starting from two splits, since one splits is the same as the scenario of singleteacher with a mixture of targets.</p><p>From <ref type="table" target="#tab_0">Table 15</ref>, we noticed that an increase in the number of splits and the number of teachers can result in a slight increase in the overall accuracy. The results also show that our teacher model is capable of adapting to randomly split sub-mixture of target domains and then transfer the knowledge to a single student model, independently from the number of splits. In addition, these results also show the robustness of our algorithm since the sub-mixture target domains are always obtained randomly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 MT-MTDA using another STDA technique</head><p>In this experiment, we evaluate our algorithm with another domain adaptation technique, namely <ref type="bibr" target="#b18">[18]</ref>, in order to show that our algorithm is agnostic w.r.t domain adaptation technique. We will also do the same for distillation, where we use <ref type="bibr" target="#b10">[11]</ref> distillation.</p><p>From <ref type="table" target="#tab_0">Table 16</ref>, we noticed that the version with CDAN <ref type="bibr" target="#b18">[18]</ref>, while it does not present the same performance gap as in the STDA case as shown in <ref type="bibr" target="#b18">[18]</ref>, it still performs better than the version with RevGrad of around 1%. Taking in account result from 12, we can see that our student model is reaching its limit in term of generalization across multiple domains for the Office31 dataset. Lastly, results in both of these tables also show the improvement our algorithm can achieve when employing state-of-the-art UDA technique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.6 Comparison with Other Fusion</head><p>Methods.</p><p>To demonstrate the benefits of the proposed feature fusion strategy, we compare our alternative fusion scheme with other baselines fusion methods, e.g., the sum or the mean of the output. The hyper-parameters for all the cases remain the same to those of the main experiment, with the only difference being the output of all the teachers is summed/averaged and then distill to the student. <ref type="table" target="#tab_0">Table 17</ref> shows that the proposed alternative distillation works better than either fusion by sum or average. This means that the proposed alternative scheme transfers learned knowledge better than the baseline methods in the particular case of MTDA. In addition, this shows that the student does not need an explicit fusion scheme in order to learn target domain knowledge from multiple teachers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.7 Weighted Accuracy</head><p>In this section, we present our average accuracy using Equation <ref type="bibr" target="#b6">7</ref>. We compare with the weighted accuracy reported in the paper of <ref type="bibr" target="#b4">[5]</ref> for a fair comparison. From <ref type="table" target="#tab_0">Tables 18, 19</ref>, 20, our weighted results are still consistent with our equal-weight results in the main paper. Our method performs better than current state-of-the-art method in all cases except on Office31 with ResNet50. These results show that our method does not improve upon state-of-the-art by having a good accuracy on an easy case of domain adaptation with huge amount of data but it improves in more general manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.8 Additional Comparison on Each Target</head><p>As mentioned in the main paper, we present more results on each separate target domain comparing to a standard STDA baseline <ref type="bibr" target="#b7">[8]</ref> on OfficeHome using AlexNet.</p><p>From <ref type="table" target="#tab_0">Table 21</ref>, we can draw a similar conclusion of the main paper. Our method performs in average better than multiple STDA on different target domains. This shows that we can have one model  handling different target domains without sacrificing computational power or memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.9 TSNE Visualization</head><p>In this section, we add the TSNE of RevGrad <ref type="bibr" target="#b7">[8]</ref> and DAN <ref type="bibr" target="#b17">[17]</ref> and provide a higher resolution of the previous TSNE. From <ref type="figure">Figure 6</ref>, we can see that features between different target domains can be mixed together even when there's a blending mechanism like in <ref type="bibr" target="#b4">[5]</ref>.     <ref type="figure">Figure 6</ref>: T-SNE visualization of all baselines methods versus MT-MTDA (ours)</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Illustration of proposed KD for domain adaptation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>for Φ i and Θ using xs Optimize the loss of equation βL T arget KD (4) for Φ i and Θ using x i t end Update β = s · exp g·e end Evaluate the model end</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Illustration of the proposed learning technique.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>T-SNE visualization of Office31 data, where features are learned using MT-MTDA and AMEANS. Best viewed in color</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Accuracy of MT-MTDA and reference methods on the Digits-Five dataset.</figDesc><table><row><cell>Models</cell><cell cols="5">mt − → mm, sv, mm − → mt, sv, sv − → mt, mm, sy − → mt, mm up − → mt, sv, Avg</cell></row><row><cell></cell><cell>up, sy</cell><cell>up, sy</cell><cell>up, sy</cell><cell>up, sv</cell><cell>mm, sy</cell></row><row><cell>Lower-bound: Superv. (source only)</cell><cell>36.6</cell><cell>57.3</cell><cell>67.1</cell><cell>74.9</cell><cell>36.9 54.6</cell></row><row><cell>ADDA[25]</cell><cell>52.5</cell><cell>58.9</cell><cell>46.4</cell><cell>67.0</cell><cell>34.8 51.9</cell></row><row><cell>DAN[17]</cell><cell>38.8</cell><cell>53.5</cell><cell>55.1</cell><cell>65.8</cell><cell>27.0 48.0</cell></row><row><cell>RevGrad[8]</cell><cell>60.2</cell><cell>66.0</cell><cell>64.7</cell><cell>69.2</cell><cell>44.3 60.9</cell></row><row><cell>DADA[22]</cell><cell>39.4</cell><cell>61.1</cell><cell>80.1</cell><cell>83.7</cell><cell>47.2 62.3</cell></row><row><cell>AMEANS[5]</cell><cell>61.2</cell><cell>66.9</cell><cell>67.2</cell><cell>73.3</cell><cell>47.5 63.2</cell></row><row><cell>MT-MTDA Mixed (ours)</cell><cell>59.5</cell><cell>71.5</cell><cell>69.9</cell><cell>78.3</cell><cell>49.6 65.8</cell></row><row><cell>MT-MTDA (ours)</cell><cell>58.6</cell><cell>71.0</cell><cell>67.6</cell><cell>75.6</cell><cell>51.0 64.7</cell></row><row><cell>Upper-bound: Superv. (targets)</cell><cell>88.1</cell><cell>90.2</cell><cell>93.0</cell><cell>90.3</cell><cell>89.1 90.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Accuracy of MT-MTDA and reference methods on Alexnet and ResNet50 as backbone(student) on the Office31.</figDesc><table><row><cell>Models</cell><cell cols="3">A − → D,W D − → A,W W − → A,D Avg</cell></row><row><cell cols="3">Teacher: ResNet50 -Student: AlexNet</cell><cell></cell></row><row><cell>Superv. (source only)</cell><cell>62.7</cell><cell>73.3</cell><cell>74.4 70.1</cell></row><row><cell>DAN[17]</cell><cell>68.2</cell><cell>71.4</cell><cell>73.2 70.9</cell></row><row><cell>RevGrad[8]</cell><cell>74.1</cell><cell>72.1</cell><cell>73.4 73.2</cell></row><row><cell>AMEANS[5]</cell><cell>74.9</cell><cell>74.9</cell><cell>76.3 75.4</cell></row><row><cell>MT-MTDA Mixed (Ours)</cell><cell>80.3</cell><cell>76.3</cell><cell>78.0 78.2</cell></row><row><cell>MT-MTDA (Ours)</cell><cell>82.5</cell><cell>74.9</cell><cell>77.6 78.3</cell></row><row><cell cols="3">Teacher: ResNext101 -Student: ResNet50</cell><cell></cell></row><row><cell>Superv. (source only)</cell><cell>68.7</cell><cell>79.6</cell><cell>80.0 76.1</cell></row><row><cell>DAN[17]</cell><cell>77.9</cell><cell>75.0</cell><cell>80.0 77.6</cell></row><row><cell>RevGrad[8]</cell><cell>79.0</cell><cell>81.4</cell><cell>82.3 80.9</cell></row><row><cell>AMEANS[5]</cell><cell>89.8</cell><cell>84.6</cell><cell>84.3 86.2</cell></row><row><cell>MT-MTDA Mixed (Ours)</cell><cell>85.5</cell><cell>84.0</cell><cell>84.4 84.6</cell></row><row><cell>MT-MTDA (Ours)</cell><cell>87.9</cell><cell>83.7</cell><cell>84.0 85.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Accuracy of proposed and reference methods on OfficeHome dataset.</figDesc><table><row><cell>Models</cell><cell cols="4">Ar − → Cl, Pr, Rw Cl − → Ar, Pr, Rw Pr − → Ar, Cl, Rw Rw − → Ar, Cl, Pr Avg</cell></row><row><cell></cell><cell cols="2">Teacher: ResNet50 -Student: AlexNet</cell><cell></cell><cell></cell></row><row><cell>Superv. (Source only)</cell><cell>33.4</cell><cell>35.3</cell><cell>30.6</cell><cell>37.9 34.3</cell></row><row><cell>DAN[17]</cell><cell>39.7</cell><cell>41.6</cell><cell>37.8</cell><cell>46.8 41.5</cell></row><row><cell>RevGrad[8]</cell><cell>42.2</cell><cell>43.8</cell><cell>39.9</cell><cell>47.7 43.4</cell></row><row><cell>AMEANS[5]</cell><cell>44.6</cell><cell>45.6</cell><cell>41.4</cell><cell>49.3 45.2</cell></row><row><cell>MT-MTDA Mixed (Ours)</cell><cell>48.6</cell><cell>46.6</cell><cell>41.1</cell><cell>52.1 47.1</cell></row><row><cell>MT-MTDA (Ours)</cell><cell>48.8</cell><cell>48.7</cell><cell>42.9</cell><cell>55.8 49.1</cell></row><row><cell></cell><cell cols="2">Teacher: ResNext101 -Student: ResNet50</cell><cell></cell><cell></cell></row><row><cell>Superv. (Source only)</cell><cell>47.6</cell><cell>41.8</cell><cell>43.4</cell><cell>51.7 46.1</cell></row><row><cell>DAN[17]</cell><cell>55.6</cell><cell>55.1</cell><cell>47.8</cell><cell>56.6 53.8</cell></row><row><cell>RevGrad[8]</cell><cell>58.4</cell><cell>57.0</cell><cell>52.0</cell><cell>63.0 57.6</cell></row><row><cell>AMEANS[5]</cell><cell>64.3</cell><cell>64.2</cell><cell>59.0</cell><cell>66.4 63.5</cell></row><row><cell>MT-MTDA Mixed (Ours)</cell><cell>64.8</cell><cell>65.3</cell><cell>60.5</cell><cell>67.5 64.5</cell></row><row><cell>MT-MTDA (Ours)</cell><cell>64.6</cell><cell>66.4</cell><cell>59.2</cell><cell>67.1 64.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Comparison with [9] on PACS dataset.</figDesc><table><row><cell>LeNet</cell><cell></cell><cell cols="2">P − → Ap, Cr, S</cell><cell></cell><cell>Ap − → Cr, S, P</cell><cell></cell></row><row><cell></cell><cell cols="6">P − → Ap P − → Cr P − → S Avg Ap − → Cr Ap − → S Ap − → P Avg</cell></row><row><cell>ADDA [25]</cell><cell>24.3</cell><cell>20.1</cell><cell>22.4 22.3</cell><cell>17.8</cell><cell>18.9</cell><cell>32.8 23.2</cell></row><row><cell>MTDA-ITA [9]</cell><cell>31.4</cell><cell>23.0</cell><cell>28.2 27.6</cell><cell>27.0</cell><cell>28.9</cell><cell>35.7 30.5</cell></row><row><cell>MT-MTDA (Ours)</cell><cell>24.6</cell><cell>32.2</cell><cell>33.8 30.2</cell><cell>46.6</cell><cell>57.5</cell><cell>35.6 46.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Accuracy of STDA methods for individual targets vs MTDA methods on Office31 dataset using AlexNet</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Comparison with of teachers accuracy vs students</figDesc><table><row><cell></cell><cell>Models</cell><cell cols="4">A − → D,W D − → A,W W − → A,D Average</cell></row><row><cell>Student</cell><cell>AlexNet</cell><cell>82.5</cell><cell>74.9</cell><cell>77.6</cell><cell>78.3</cell></row><row><cell cols="2">Teachers ResNet50</cell><cell>77.6</cell><cell>79.9</cell><cell>80.0</cell><cell>79.2</cell></row><row><cell>Student</cell><cell>ResNet50</cell><cell>87.9</cell><cell>83.7</cell><cell>84.0</cell><cell>85.2</cell></row><row><cell cols="3">Teachers ResNext101 85.9</cell><cell>83.6</cell><cell>84.3</cell><cell>84.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Comparison of student's accuracy on separate domains with gradual increase in domains. The order in the target domains indicates the order in which they were integrated into the training.</figDesc><table><row><cell>AlexNet</cell><cell cols="5">Ar − → Cl Ar − → Pr Ar − → Rw Ar − → Pr, Cl Ar − → Pr, Rw Ar − → Rw, Cl Ar − → Cl, Pr, Rw</cell></row><row><cell>Acc on Cl</cell><cell>34.0</cell><cell>33.3</cell><cell></cell><cell>33.0</cell><cell>34.1</cell></row><row><cell>Acc on Pr</cell><cell>55.3</cell><cell>50.1</cell><cell>50.0</cell><cell></cell><cell>52.6</cell></row><row><cell>Acc on Rw</cell><cell>59.0</cell><cell></cell><cell>57.9</cell><cell>57.7</cell><cell>59.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Accuracy of each target domain and standard deviation between these accuracies</figDesc><table><row><cell>AlexNet</cell><cell cols="7">Ar − → Cl,Pr,Rw Ar − → Cl,Rw,Pr Ar − → Pr,Cl,Rw Ar − → Pr,Rw,Cl Ar − → Rw,Cl,Pr Ar − → Rw,Pr,Cl STDev</cell></row><row><cell>Acc on Cl</cell><cell>34.1</cell><cell>33.7</cell><cell>34.0</cell><cell>34.7</cell><cell>33.6</cell><cell>34.6</cell><cell>0.4</cell></row><row><cell>Acc on Pr</cell><cell>52.6</cell><cell>52.5</cell><cell>53.1</cell><cell>52.5</cell><cell>52.5</cell><cell>53.1</cell><cell>0.3</cell></row><row><cell>Acc on Rw</cell><cell>59.7</cell><cell>60.6</cell><cell>59.8</cell><cell>59.5</cell><cell>60.4</cell><cell>59.5</cell><cell>0.4</cell></row><row><cell>Average Acc</cell><cell>48.8</cell><cell>48.9</cell><cell>48.9</cell><cell>48.9</cell><cell>48.8</cell><cell cols="2">49.1 0.1 | 0.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Accuracy of proposed method with target distillation consistency vs without</figDesc><table><row><cell>Models</cell><cell cols="4">A − → D,W D − → A,W W − → A,D Average</cell></row><row><cell>MT-MTDA without CST</cell><cell>82.1</cell><cell>73.8</cell><cell>74.3</cell><cell>76.7</cell></row><row><cell>MT-MTDA with CST</cell><cell>82.5</cell><cell>74.9</cell><cell>77.6</cell><cell>78.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>Accuracy of proposed method using a single teacher vs multiple teachers</figDesc><table><row><cell>Models</cell><cell cols="4">A − → D,W D − → A,W W − → A,D Average</cell></row><row><cell>Single Teacher MTDA</cell><cell>75.3</cell><cell>64.0</cell><cell>67.0</cell><cell>68.8</cell></row><row><cell>MT-MTDA</cell><cell>82.5</cell><cell>74.9</cell><cell>77.6</cell><cell>78.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 :</head><label>11</label><figDesc>Hyper-parameters for our algorithms for each backbone and dataset</figDesc><table><row><cell cols="6">Hyper parameters Digits-Five LeNet Office31 Alexnet OfficeHome Alexnet Office31 ResNet50 OfficeHome Resnet50</cell></row><row><cell>N e</cell><cell>100</cell><cell>100</cell><cell>200</cell><cell>100</cell><cell>200</cell></row><row><cell>batch size</cell><cell>64</cell><cell>16</cell><cell>8</cell><cell>16</cell><cell>8</cell></row><row><cell>τ</cell><cell>20</cell><cell>20</cell><cell>20</cell><cell>20</cell><cell>20</cell></row><row><cell>α</cell><cell>0.5</cell><cell>0.3</cell><cell>0.5</cell><cell>0.3</cell><cell>0.5</cell></row><row><cell>s</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell></row><row><cell>f</cell><cell>0.8</cell><cell>0.8</cell><cell>0.5</cell><cell>0.8</cell><cell>0.5</cell></row><row><cell>γ</cell><cell>0.5</cell><cell>0.5</cell><cell>0.5</cell><cell>0.5</cell><cell>0.5</cell></row><row><cell>UDA Learning Rate</cell><cell>0.0005</cell><cell>0.001</cell><cell>0.0001</cell><cell>0.001</cell><cell>0.0001</cell></row><row><cell>KD Learning Rate</cell><cell>0.0005</cell><cell>0.01</cell><cell>0.001</cell><cell>0.01</cell><cell>0.001</cell></row><row><cell>weight decay</cell><cell>0.0005</cell><cell>0.0005</cell><cell>0.0005</cell><cell>0.0005</cell><cell>0.0005</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 12 :</head><label>12</label><figDesc>Accuracy of proposed and reference methods on Digits-Four dataset</figDesc><table><row><cell>Source → Targets</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 13 :</head><label>13</label><figDesc>Accuracy of MT-MTDA and reference methods on Alexnet and Resnet101 as backbone(student) on the Office-Caltech</figDesc><table><row><cell>Models</cell><cell cols="5">A − → C,D,W C − → A,D,W D − → A,C,W W − → A,C,D Average</cell></row><row><cell></cell><cell cols="3">Teacher: ResNet50 -Student: AlexNet</cell><cell></cell><cell></cell></row><row><cell>Source only</cell><cell>83.1</cell><cell>88.9</cell><cell>86.7</cell><cell>82.2</cell><cell>85.2</cell></row><row><cell>RevGrad[8]</cell><cell>85.9</cell><cell>90.5</cell><cell>88.6</cell><cell>90.4</cell><cell>88.9</cell></row><row><cell>DADA[22]</cell><cell>86.3</cell><cell>91.7</cell><cell>89.9</cell><cell>91.3</cell><cell>89.8</cell></row><row><cell>MT-MTDA Mixed (Ours)</cell><cell>92.8</cell><cell>93.4</cell><cell>89.2</cell><cell>90.8</cell><cell>91.6</cell></row><row><cell>MT-MTDA (Ours)</cell><cell>93.3</cell><cell>93.9</cell><cell>90.1</cell><cell>91.2</cell><cell>92.1</cell></row><row><cell cols="4">Teacher: ResNext101 -Student: ResNet101</cell><cell></cell><cell></cell></row><row><cell>Source only</cell><cell>90.5</cell><cell>94.3</cell><cell>88.7</cell><cell>82.5</cell><cell>89.0</cell></row><row><cell>RevGrad[8]</cell><cell>91.5</cell><cell>94.3</cell><cell>90.5</cell><cell>86.3</cell><cell>90.6</cell></row><row><cell>DADA[22]</cell><cell>92.0</cell><cell>95.1</cell><cell>91.3</cell><cell>93.1</cell><cell>92.9</cell></row><row><cell>MT-MTDA Mixed (Ours)</cell><cell>94.9</cell><cell>97.9</cell><cell>94.7</cell><cell>95.3</cell><cell>95.7</cell></row><row><cell>MT-MTDA (Ours)</cell><cell>96.1</cell><cell>98.1</cell><cell>96.3</cell><cell>96.4</cell><cell>96.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 14 :</head><label>14</label><figDesc>Accuracy of each target domain on Digits-Five dataset with LeNet as Backbone.</figDesc><table><row><cell>Lenet</cell><cell cols="5">mt − → mm, sv, up, sy mm − → mt, sv, up, sy sv − → mt, mm, up, sy sy − → mt, mm, up, sv up − → mt, sv, mm, sy</cell></row><row><cell>Student Acc on mt</cell><cell>-</cell><cell>96.3</cell><cell>69.1</cell><cell>85.8</cell><cell>87.0</cell></row><row><cell>Student Acc on mm</cell><cell>46.6</cell><cell>-</cell><cell>48.1</cell><cell>55.5</cell><cell>40.3</cell></row><row><cell>Student Acc on sv</cell><cell>53.8</cell><cell>43.9</cell><cell>-</cell><cell>75.3</cell><cell>30.7</cell></row><row><cell>Student Acc on sy</cell><cell>57.7</cell><cell>82.9</cell><cell>83.7</cell><cell>-</cell><cell>48.4</cell></row><row><cell>Student Acc on up</cell><cell>77.3</cell><cell>61.1</cell><cell>69.4</cell><cell>85.8</cell><cell>-</cell></row><row><cell>Average</cell><cell>58.85</cell><cell>71.05</cell><cell>67.575</cell><cell>75.6</cell><cell>51.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 15 :</head><label>15</label><figDesc>Accuracy of MT-MTDA Mixed on different number of splits of sub-targets</figDesc><table><row><cell>AlexNet</cell><cell cols="4">A − → D,W D − → A,W W − → A,D Average</cell></row><row><cell>MT-MTDA Mixed 2</cell><cell>80.3</cell><cell>76.3</cell><cell>78.0</cell><cell>78.2</cell></row><row><cell>MT-MTDA Mixed 3</cell><cell>81.2</cell><cell>76.2</cell><cell>78.2</cell><cell>78.5</cell></row><row><cell>MT-MTDA Mixed 4</cell><cell>82.1</cell><cell>76.6</cell><cell>78.8</cell><cell>79.2</cell></row><row><cell>MT-MTDA Mixed 10</cell><cell>81.3</cell><cell>76.9</cell><cell>78.5</cell><cell>78.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 16 :</head><label>16</label><figDesc>Accuracy of MT-MTDA with RevGrad vs CDAN</figDesc><table><row><cell>Models</cell><cell cols="4">A − → D,W D − → A,W W − → A,D Average</cell></row><row><cell cols="3">Teacher: ResNet50 -Student: AlexNet</cell><cell></cell><cell></cell></row><row><cell>MT-MTDA Mixed (Ours)</cell><cell>80.3</cell><cell>76.3</cell><cell>78.0</cell><cell>78.2</cell></row><row><cell>MT-MTDA (Ours)</cell><cell>82.5</cell><cell>74.9</cell><cell>77.6</cell><cell>78.3</cell></row><row><cell>MT-MTDA CDAN Mixed (Ours)</cell><cell>84.3</cell><cell>75.4</cell><cell>77.8</cell><cell>79.2</cell></row><row><cell>MT-MTDA CDAN (Ours)</cell><cell>84.5</cell><cell>76.9</cell><cell>78.0</cell><cell>79.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 17 :</head><label>17</label><figDesc>Accuracy of proposed method with different fusions</figDesc><table><row><cell>Models</cell><cell cols="4">A − → D,W D − → A,W W − → A,D Average</cell></row><row><cell>MT-MTDA Mean</cell><cell>75.1</cell><cell>65.4</cell><cell>67.1</cell><cell>69.2</cell></row><row><cell>MT-MTDA Sum</cell><cell>78.3</cell><cell>66.9</cell><cell>69.8</cell><cell>71.6</cell></row><row><cell>MT-MTDA</cell><cell>82.5</cell><cell>74.9</cell><cell>77.6</cell><cell>78.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 18 :</head><label>18</label><figDesc>Weighted accuracy of proposed and baseline methods on Digits-Five dataset with AlexNet as Backbone.</figDesc><table><row><cell>Models</cell><cell cols="6">mt − → mm, sv, up, sy mm − → mt, sv, up, sy sv − → mt, mm, up, sy sy − → mt, mm, up, sv up − → mt, sv, mm, sy Average</cell></row><row><cell>Source only</cell><cell>26.9</cell><cell>56.0</cell><cell>67.2</cell><cell>73.8</cell><cell>36.9</cell><cell>52.2</cell></row><row><cell>ADDA</cell><cell>43.7</cell><cell>55.9</cell><cell>40.4</cell><cell>66.1</cell><cell>34.8</cell><cell>48.2</cell></row><row><cell>DAN</cell><cell>31.3</cell><cell>53.1</cell><cell>48.7</cell><cell>63.3</cell><cell>27.0</cell><cell>44.7</cell></row><row><cell>RevGrad</cell><cell>52.4</cell><cell>64.0</cell><cell>65.3</cell><cell>66.6</cell><cell>44.3</cell><cell>58.5</cell></row><row><cell>AMEANS</cell><cell>56.2</cell><cell>65.2</cell><cell>67.3</cell><cell>71.3</cell><cell>47.5</cell><cell>61.5</cell></row><row><cell>MT-MTDA Mixed (ours)</cell><cell>51.6</cell><cell>69.2</cell><cell>79.7</cell><cell>76.0</cell><cell>61.5</cell><cell>67.6</cell></row><row><cell>MT-MTDA (ours)</cell><cell>54.3</cell><cell>73.4</cell><cell>67.1</cell><cell>73.1</cell><cell>64.0</cell><cell>66.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 19 :</head><label>19</label><figDesc>Weighted accuracy of proposed and baseline methods on Office31 dataset.</figDesc><table><row><cell>Models</cell><cell>A − → D,W D − → A,W W − → A,D Average</cell></row><row><cell></cell><cell>Teacher: ResNet50 -Student: AlexNet</cell></row><row><cell>Source only</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 21 :</head><label>21</label><figDesc>Average accuracy of proposed and baseline STDA methods for individual and overall target datasets on OfficeHome dataset using AlexNet</figDesc><table><row><cell>Alexnet</cell><cell></cell><cell cols="2">Ar − → Cl, Pr, Rw</cell><cell></cell><cell></cell><cell cols="2">Cl − → Ar, Pr, Rw</cell><cell></cell><cell></cell><cell cols="2">Pr − → Ar, Cl, Rw</cell><cell></cell><cell>Rw − → Ar, Cl, Pr</cell></row><row><cell></cell><cell cols="3">Ar − → Cl Ar − → Pr Ar − → Rw</cell><cell cols="4">Avg Cl − → Ar Cl − → Pr Cl − → Rw</cell><cell cols="4">Avg Pr − → Ar Pr − → Cl Pr − → Rw</cell><cell cols="2">Avg Rw − → Ar Rw − → Cl Rw − → Pr</cell><cell>Avg</cell></row><row><cell>RevGrad STDA</cell><cell>36.4</cell><cell>45.2</cell><cell cols="2">54.7 45.4</cell><cell>35.2</cell><cell>51.8</cell><cell cols="2">55.1 47.4</cell><cell>31.6</cell><cell>39.7</cell><cell cols="2">59.3 43.5</cell><cell>45.7</cell><cell>46.4</cell><cell>65.9 52.6</cell></row><row><cell>AMEANS</cell><cell>-</cell><cell>-</cell><cell cols="2">-44.6</cell><cell>-</cell><cell>-</cell><cell cols="2">-45.6</cell><cell>-</cell><cell>-</cell><cell cols="2">-41.4</cell><cell>-</cell><cell>-</cell><cell>-49.3</cell></row><row><cell>MT-MTDA</cell><cell>34.1</cell><cell>52.6</cell><cell cols="2">59.7 48.8</cell><cell>40.7</cell><cell>52.0</cell><cell cols="2">53.5 48.7</cell><cell>36.5</cell><cell>33.7</cell><cell cols="2">58.6 42.9</cell><cell>55.0</cell><cell>42.0</cell><cell>70.3 55.7</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">See a higher resolution image in Suppl. Material</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This research was supported in part by the NSERC, Compute Canada, and MITACS.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation to improve image segmentation quality both in the source and target domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Aike</forename><surname>Bolte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Kamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonia</forename><surname>Breuer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silviu</forename><surname>Homoceanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Schlicht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Huger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Lipinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Fingscheidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Unsupervised pixel-level domain adaptation with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<idno>abs/1612.05424</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Adversarial-learned loss for fdomain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.01046</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Domain adaptive faster r-cnn for object detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Blending-target domain adaptation by adversarial meta-adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziliang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyu</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Dual-triplet metric learning for unsupervised domain adaptation in video-based face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Ekladious</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Lemoine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Granger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaveh</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Moudache</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Geometry-consistent generative adversarial networks for one-sided unsupervised domain mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaohui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kayhan</forename><surname>Batmanghelich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Unsupervised multi-target domain adaptation: An information theoretic approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pritish</forename><surname>Behnam Gholami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ognjen</forename><surname>Sahu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Rudovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pavlovic</surname></persName>
		</author>
		<idno>abs/1810.11547</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<ptr target="http://www.deeplearningbook.org" />
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A comprehensive overhaul of feature distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeongho</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeesoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyojin</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nojun</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><forename type="middle">Young</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Deep Learning and Representation Learning Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixiang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Conditional generative adversarial network for structured domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Revisiting batch normalization for practical domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodi</forename><surname>Hou</surname></persName>
		</author>
		<idno>abs/1603.04779</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Learning from noisy labels with distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuncheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yale</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangliang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<idno>abs/1703.02391</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Open compound domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqi</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingang</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohang</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Conditional adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangjie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<editor>S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors</editor>
		<imprint>
			<biblScope unit="page" from="1640" to="1650" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep unsupervised domain adaptation for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zimeng</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiani</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FG 2018</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Unsupervised domain adaptation in the dissimilarity space for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djebril</forename><surname>Mekhazni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amran</forename><surname>Bhuiyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Ekladious</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Granger</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eccv</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Joint progressive knowledge distillation and unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Le Thanh Nguyen-Meidine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madhu</forename><surname>Granger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Kiran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Antoine</forename><surname>Dolz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blais-Morin</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Domain agnostic learning with disentangled representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingchao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ximeng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<editor>Kamalika Chaudhuri and Ruslan Salakhutdinov</editor>
		<meeting>the 36th International Conference on Machine Learning<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="9" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6550</idno>
		<title level="m">Fitnets: Hints for thin deep nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Knowledge adaptation: Teaching to adapt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parsa</forename><surname>Ghaffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">G</forename><surname>Breslin</surname></persName>
		</author>
		<idno>abs/1702.02052</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Deep visual domain adaptation: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
		<idno>abs/1802.03601</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Improving face recognition with domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaguan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">287</biblScope>
			<biblScope unit="page" from="45" to="51" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Adversarial domain adaptation with domain mixup</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.01805</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A gift from knowledge distillation: Fast optimization, network minimization and transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junho</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donggyu</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihoon</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junmo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Multitarget unsupervised domain adaptation without exactly shared categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanhuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songcan</forename><surname>Chen</surname></persName>
		</author>
		<idno>abs/1809.00852</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Weighted accuracy of proposed and baseline methods on OfficeHome dataset. Models Ar -&gt;Cl, Pr, Rw Cl -&gt;Ar, Pr, Rw Pr -&gt;Ar, Cl, Rw Rw -&gt;Ar, Cl</title>
	</analytic>
	<monogr>
		<title level="m">Pr Average Teacher: ResNet50 -Student: AlexNet</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
