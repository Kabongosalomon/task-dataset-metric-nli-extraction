<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DIRV: Dense Interaction Region Voting for End-to-End Human-Object Interaction Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao-Shu</forename><surname>Fang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Xie</surname></persName>
							<email>xieyichen@sjtu.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dian</forename><surname>Shao</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
							<email>lucewu@sjtu.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghai</forename><surname>Jiao</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Tong University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DIRV: Dense Interaction Region Voting for End-to-End Human-Object Interaction Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent years, human-object interaction (HOI) detection has achieved impressive advances. However, conventional two-stage methods are usually slow in inference. On the other hand, existing one-stage methods mainly focus on the union regions of interactions, which introduce unnecessary visual information as disturbances to HOI detection. To tackle the problems above, we propose a novel one-stage HOI detection approach DIRV in this paper, based on a new concept called interaction region for the HOI problem. Unlike previous methods, our approach concentrates on the densely sampled interaction regions across different scales for each human-object pair, so as to capture the subtle visual features that is most essential to the interaction. Moreover, in order to compensate for the detection flaws of a single interaction region, we introduce a novel voting strategy that makes full use of those overlapped interaction regions in place of conventional Non-Maximal Suppression (NMS). Extensive experiments on two popular benchmarks: V-COCO and HICO-DET show that our approach outperforms existing state-of-the-arts by a large margin with the highest inference speed and lightest network architecture. Our code is publicly available at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human-object interaction (HOI) detection aims to recognize and localize the interactions between human-object pairs (e.g. sitting on a chair, riding a horse, eating an apple, etc.). As a fundamental task of image semantic understanding, it plays a vital role in many other computer vision fields such as image captioning <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">22]</ref>, visual question answering <ref type="bibr">Figure 1</ref>. Union Regions vs Interaction Regions: Conventional approaches usually pays attention to the union region (dashed yellow), which contains too much redundant information. Instead, we propose a method focusing on interaction regions (solid violet) with different scales. In above two figures, despite distinct human/object poses, interaction regions cover the most critical segments containing the cups, hands or arms, when detecting holding a cup. <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b24">25]</ref> and action understanding <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b27">28]</ref>.</p><p>For HOI detection, almost all previous methods emphasized the importance of the union regions of an interaction, which covers the whole human, object and intermediate context. For instance, existing two-stage algorithms commonly crop the union region of a human-object pair and then embed its visual features <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b16">17]</ref>, while recent onestage methods aim to regress this union region with keypoints <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b30">31]</ref> or anchor boxes <ref type="bibr" target="#b11">[12]</ref> and use it to associate the target human and object.</p><p>However, we find that such emphasis on union region is counter-intuitive for human beings. In practice, it is not necessary to observe the whole union region before making decisions in most situations. For instance, when asked to determine whether a man is holding a cup, we only need to notice his hands but never care about where his feet are. That's to say, humans can easily target the human-object pair of an HOI, without the needs of being told the union regions. Based on these observations, we propose a new recognition unit for HOI detection, called interaction region. The interaction region denotes the region that covers the minimal area of human and object crucial for recognizing the interaction. An example is given in <ref type="figure">Fig. 1</ref>. In this case, an upper-body region that contains a cup and hand would be more distinguishable than the union region.</p><p>To this end, we propose a novel one-stage HOI detector that concentrates on the interaction regions of humanobject interactions. We hypothesize that these regions are highly informative to determine the interaction category and human-object relative spatial configuration. To fully utilize the interaction regions for HOI detection, three main technical challenges identified as follows need to be addressed beforehand.</p><p>Challenge 1: How do we decide the interaction regions? Although recent work provided part-level action labels <ref type="bibr" target="#b15">[16]</ref>, we tend to seek a more general and simpler HOI detector without the need for extra annotations. Empirically, we consider that those human parts closer to the object are more likely to take an indispensable effect on the interaction, and so are the object parts. For simplicity, we consider some rectangle regions, which cover both some parts of the human and object, as interaction regions. A natural idea comes by applying the dense anchor boxes in one-stage object detection models to represent these regions. To achieve that, we set three overlapping thresholds between anchor boxes and human bounding boxes, object bounding boxes as well as union regions. We apply a dense interaction region selection manner, where all anchors satisfying these three thresholds are regarded as interaction regions.</p><p>Challenge 2: An anchor box may be regarded as the interaction region for multiple different HOIs. Unlike object detection, this situation appears frequently in HOI detection. Under this condition, the anchor box needs to predict multiple HOI labels and corresponding object locations, where the number is unfixed. This poses extra challenges for network design and final result association. Therefore, we match each anchor box with only one unique interaction. In addition, there inevitably exists some missed positive interactions within the popular datasets. We develop a novel ignorance loss based on classical focal loss <ref type="bibr" target="#b19">[20]</ref> to address these problems.</p><p>Challenge 3: Single interaction region may lead to ambiguity or misrepresentation. HOI recognition relies on very subtle visual cues in interaction regions. Some visual features are even ambiguous, leading to the fragile result from a single anchor. For this reason, we propose a novel voting strategy. Each anchor only contributes a little to the final location and classification prediction. For each interaction type, a probability distribution is established for the relative location between each human-object pair by fusing the prediction results of different anchors. This dense anchor voting strategy can remarkably elevate the fault-tolerance of each anchor and achieve a robust final prediction.</p><p>Extensive experiments show that our one-stage approach, DIRV (Dense Interaction Region Voting), outperforms existing state-of-the-art models on two popular benchmarks, achieving both higher accuracy and faster speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Human-object interaction (HOI) detection is formally defined as retrieving human, verb, object triplets from images. Previous methods mainly employed a two-stage strategy. In the first stage, a pre-trained object detector <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b26">27]</ref> localized both humans and objects within the image. In the second stage, a classification network recognized the interaction categories for each human-object pair. Most work focused on the improvement of the second stage. Some early work <ref type="bibr" target="#b9">[10]</ref> simply extracted features from each human or object instance. This method suffered from lack of contextual information. Afterwards, more information was taken into account rather than instance appearance, including spatial location <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b25">26]</ref>, human pose <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b32">33]</ref>, word embedding <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b23">24]</ref>, segmentations <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b3">4]</ref> and human part label <ref type="bibr" target="#b15">[16]</ref>. Yet, these two-stage methods typically need to detect all human-object pairs, making their inference time grow quadratically with instance number. Furthermore, these approaches usually adopted a heavy network for classification, which led to considerable computation overhead.</p><p>To tackle these drawbacks, some recent work developed one-stage HOI detectors. Liao et.al. <ref type="bibr" target="#b17">[18]</ref> and Wang et.al. <ref type="bibr" target="#b30">[31]</ref> posed HOI detection as a keypoint detection and grouping problem. Despite their impressive efficiency and accuracy, the interaction keypoints had no apparent characteristics in visual patterns so the networks were not easy to train. Kim et.al. <ref type="bibr" target="#b11">[12]</ref> designed an anchor-based one-stage algorithm to regress the union region of human and object. However, as aforementioned, union region prediction is not straight-forward and single anchor's prediction is fragile.</p><p>Unlike all the above methods, our method makes full use of visual patterns within interaction regions across different scales, allowing a promising accuracy without the help of any other proposals or annotations. The one-stage strategy and concise network architecture also bring greatly improved running time and space efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>In this section, we introduce our proposed DIRV (Dense Interaction Regions Voting) framework for human-object interaction (HOI) detection. The problem formulation is firstly explained in Sec. 3.1. Then, we present the network architecture of our detector in Sec. 3.2. Afterwards, the inference protocol based on voting strategy is shown in Sec. 3.3. Finally, we demonstrate how to train our deep neural network model in Sec. 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Formulation</head><p>Typically, HOI detection aims to fetch a b h , v, b o triplet for each interaction within a single image x, where b h , b o denote the bounding box of human h and object o separately, while v denotes the human action. Without considering external input like human poses <ref type="bibr" target="#b5">[6]</ref>, conventional twostage HOI detectors formulate the problem as</p><formula xml:id="formula_0">H, O = d(f x ), v i = g(b h , b o , f x ), ∀h ∈ H, ∀o ∈ O,<label>(1)</label></formula><p>where d(·) is a vanilla object detector, g(·) is the verb classifier for a human-object pair, f x is the appearance feature of the whole image x and H,O are detected humans and objects. Since the input of g(·) relies on the output of d(·), these two processes cannot run in parallel and g(·) would face the combinatorial explosion problem. On the contrary, we reformulate HOI detection as</p><formula xml:id="formula_1">H, O = d(f x ), T (b h ), v, T (b o ) = g(f x ), h ∈ H, o ∈ O,<label>(2)</label></formula><p>where T (·) is a target indicator that links the verb to a detected human-object pair. By doing so, we can run these two processes simultaneously. Further, we do not adopt the common practice of Non-Maximum Suppression (NMS) when retrieving the</p><formula xml:id="formula_2">T (b h ), v, T (b o )</formula><p>. In contrast, we propose a different strategy, voting, to handle the prediction of different interaction regions. Predictions based on every anchor's visual features are fully utilized instead of being suppressed. The final HOI prediction comes from the combination of each interaction region through voting. To sum up, our algorithm is formulated as Eq. 3:</p><formula xml:id="formula_3">H, O = d(f x ), T (b i h ), v i , T (b i o ) = g(f a i x ), i ∈ {1, 2, . . . , N }, T (b h ), v, T (b o ) = vote({ T (b i h ), v i , T (b i o ) } i∈{1,...,N } ), (3) where T (b i h ), v i , T (b i o )</formula><p>is the prediction based on anchor a i . N is the number of interaction regions for this interaction. We show how we obtain H, O and T (b i h ), v i , T (b i o ) for each anchor in Sec. 3.2. vote(·) is the voting strategy, which is elaborated in Sec. 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Dense Interaction Region Detector</head><p>Our network structure is illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>. The model is composed of two components: an instance detector and an interaction detector. Each of them contains three parallel sub-branches, which share the feature map of the Feature Pyramid Network. We first explain the instance detector for H, O and then the interaction detector for</p><formula xml:id="formula_4">T (b i h ), v i , T (b i o ) .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Instance Detector</head><p>The instance detector mainly helps instance localization and supports the detection of none object actions, e.g. walking. It contains three sub-branches: instance classification branch, instance regression branch and instance action classification branch.</p><p>The instance regression and classification branches follow the standard setting in most object detection networks, which regress instance bounding boxes based on anchors as well as classify these instances. Interactions are not considered in these two branches.</p><p>Beyond these two branches, an instance action classification branch plays an auxiliary role in interaction classification. It predicts the action scores of humans and objects, helping the association of human-verb-object pair. The actions of humans and objects are treated separately, e.g., hold and be held are classified as two different actions. If there are C h human actions and C o object actions, the classification gives two scores s act h ∈ R C h and s act o ∈ R Co . The anchor settings follow standard object detection and only those positive anchors involved in at least one interaction are taken into account when calculating loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Interaction Detector</head><p>The interaction detector serves as the key of our proposed architecture, DIRV. It directly predicts the interaction v i and the target T (b i h ), T (b i o ) that indicates the corresponding human-object pair from the subtle visual features in interaction regions. We first clarify our methodology, followed by two key learning techniques: interaction region decision and ignorance loss.</p><formula xml:id="formula_5">Methodology: To retrieve the T (b i h ), v i , T (b i o )</formula><p>triplet, we design three parallel sub-branches: interaction classification branch, human target branch and object target branch for predicting v i , T (b i h ), and T (b i o ) separately. The interaction classification branch classifies the interaction type v i within the interaction region (i.e. the anchor). It obtains an interaction score prediction s inter ai ∈ R C for each interaction region a i , where C is the number of interaction categories.</p><p>For human and object targets T (b i h ) and T (b i o ), it is difficult to directly link the verb to the detected human and object given by the instance detector since the detection branch run in parallel. Thus, we propose an intuitive yet effective solution. The  </p><formula xml:id="formula_6">(b i h ) and T (b i o ).</formula><p>We can easily link the verb v i to the detected human and object box b i h , b i o during inference via simple post processing (e.g., IoU matching), which is introduced in Sec.3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interaction Region Decision:</head><p>As explained before, the interaction regions should cover both parts of interacting human and object. With different scales, these regions may provide important visual features of different levels. Interestingly, we find that such a setting naturally matches the characteristic of anchor boxes A. An anchor box a j ∈ A serves as an interaction region of interaction I i so long as it satisfies the following overlapping requirement:</p><formula xml:id="formula_7">O j i = 1 IoU (a j ,b i u ) &gt; t u · 1 a j ∩b i ĥ b i h &gt; t h · 1 a j ∩b i ô b i o &gt; t o<label>(4)</label></formula><p>whereb i h ,b i o are the ground-truth human/object bounding box of a possible interaction pair I i .b i u is the union region box of interaction I i , which is the smallest box that completely covers bothb i h ,b i o . t u , t h , t o are three thresholds. We set them as t u = t h = t o = 0.25, which is analyzed in ablation study.</p><p>With the requirement above, single anchor box may serve as the interaction region of multiple interactions, which impedes the human/object regression. Thus, we define a overlapping level metric to ensure that an anchor box corresponds to at most a unique interaction, i.e.,</p><formula xml:id="formula_8">O j i = IoU (a j ,b i u ) + a j ∩b i ĥ b i h · a j ∩b i ô b i o .<label>(5)</label></formula><p>If multiple interactions are matched with the same anchor box, it will associate with interaction I k whereÔ j k = max i Ô j i |O j i = 1 so each anchor has at most one groundtruth in regression.</p><p>Ignorance Loss: For human/object target branch, we just follow many anchor-based object detection methods to apply the standard smooth L 1 loss between predicted b ai h,inter /b ai o,inter and ground-truthb i h /b i o on their loss functions L reg,h /L reg,o for interaction region a i .</p><p>Yet, standard focal loss is not applicable for interaction classification branch because of the following two reasons: Firstly, the receptive field of an anchor may contain multiple different interactions. Secondly, HOI detection datasets have much more missed positive samples than object detection datasets. These cause serious confusion during training.</p><p>We propose a novel ignorance loss based on vanilla focal loss <ref type="bibr" target="#b19">[20]</ref> to address both difficulties above. We eliminate the influence of missed unlabelled interactions by removing the background loss i.e. anchors associated with none interactions don't take effect in learning.</p><p>Further, as a solution to the multiple interactions problem, we modify the ground-truth targets of foreground anchors as below. For anchor a j , if there exist multiple interactions {I i } within current anchor where O j i = 1, we set the target label as</p><formula xml:id="formula_9">t c j =      1 I c k = 1,Ô j k = max i {Ô j i |O j i = 1} 0 I c i = 0, ∀i, O j i = 1 ignored others (6)</formula><p>where t c j is the target label of interaction category c for anchor a j . I c i = 1 denotes interaction I i is positive for category c, else I c i = 0. The above equation means that we ignore the classification loss for those interaction categories exist but not dominant in an anchor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Voting Based Model Inference</head><p>Our model makes inference by combining the prediction results of different interaction regions. Each interaction region contributes to the final interaction recognition with the weighted localization score as weight. The inference process is divided into three steps as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Parallel Inference</head><p>All six sub-branches work in parallel during inference, which dramatically reduces the inference time. ∈ R C is the interaction classification score for each interaction region. Here, we should have C = C h = C o after eliminating interactions with none objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Object Location Estimation</head><p>We retrieve the b h , v, b o triplet in a human-centric manner. For each interaction region a j , we first try to match it with a human instance h aj ∈ H based on the overlapping metric, that is</p><formula xml:id="formula_10">IoU (a j , b aj h ) = max h IoU (a j , b h ), h ∈ H, a j ∩ b h b h &gt; t h<label>(7)</label></formula><p>where b h is the human bounding box and t h is the threshold same as that in Eq. 4. If no human instance meets the requirement, this interaction region is abandoned. After matching the interaction region to a detected human instance, we then search its corresponding object instance. A natural thinking is to match the object like Eq.7. However, we found that the location of object is usually not accurate enough. To improve the robustness, we build a probability distribution for the object location based on the prediction result. Referring to <ref type="bibr" target="#b7">[8]</ref>, we model it with a 2-d Gaussian distribution:</p><formula xml:id="formula_11">p aj (x o , y o ) = e − || v a j o|h −µ a j o|h || 2 2·σ 2<label>(8)</label></formula><p>where v </p><p>and the standard deviation σ is a hyper-parameter, which is set as 0.9 in our experiments. As analyzed in the supplementary material, our method is insensitive to σ. After obtaining the object location distribution, we weight it by interaction classification score s inter aj as below. </p><p>where (x o , y o ) is the center of object bounding box. Until now, we obtain the weighted localization scores s loc aj (x, y) ∈ R C for all C interaction categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Voting Based Region Fusion</head><p>By fusing weighted localization scores of interaction regions associated with same human instance b h , a humancentric object location distribution s f use h is computed with our voting strategy:</p><formula xml:id="formula_14">s f use h (x, y) = aj ∈A h s loc aj (x, y),<label>(11)</label></formula><p>where A h = {a j } h a j =h is set of interaction regions associated with human instance h. We visualize some examples of the fused distribution in <ref type="figure" target="#fig_4">Fig. 3</ref>. Finally, we are now able to score a human-object pair using this distribution. For each interaction region, we first associate it with a detected object instance o aj , like Eq. 7.   A h,o is the set consisting of all interaction regions associated with any interactive human-object pairs. The size is not very large and it is easy to compute in parallel, so only a little CPU overhead is introduced.</p><formula xml:id="formula_15">p aj (x aj o , y aj o ) = max o p aj (x o , y o ), o aj ∈ O, a j ∩ b o b o &gt; t o .<label>(12)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Model Training</head><p>During training, the backbone, feature pyramid network and instance classification/regression branches are frozen with COCO pre-trained weight <ref type="bibr" target="#b29">[30]</ref>. The final loss is the sum of loss functions for other four sub-branches in <ref type="figure" target="#fig_0">Fig. 2</ref>. <ref type="bibr" target="#b14">(15)</ref> In interaction detector, L reg,h , L reg,o are the smooth L 1 losses for human and object target branches separately. L inter cls is our ignorance loss for interaction classification branch. We follow focal loss <ref type="bibr" target="#b19">[20]</ref> to set α = 0.25, γ = 2.0. In instance detector, L inst cls is standard binary cross-entropy loss for instance action classification branch.</p><formula xml:id="formula_16">L = L reg,h + L reg,o + L inter cls + L inst cls</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we carry out comprehensive experiments to demonstrate the superiority of our proposed DIRV. Firstly, we introduce two benchmarks in Sec </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset and Metric</head><p>Dataset We evaluate our method on two popular datasets: V-COCO <ref type="bibr" target="#b9">[10]</ref> and HICO-DET <ref type="bibr" target="#b2">[3]</ref>. V-COCO dataset is a subset of COCO <ref type="bibr" target="#b20">[21]</ref> with extra interaction labels. It contains 10,346 images (2,533 for training, 2867 for validation and 4,946 for testing). Each person in these images is annotated with 29 action categories, 4 of which (stand, smile, walk, run) have no object. HICO-DET is a large dataset for HOI detection by augmenting HICO dataset <ref type="bibr" target="#b2">[3]</ref> with instance bounding box annotations. This dataset includes 38,118 images for training and 9,658 images for testing. It is labelled with 600 HOI types over 117 verbs and 80 object categories.</p><p>Metric We adopt the popular evaluation metric for HOI detection: mean average precision (mAP). A prediction is true positive only when the HOI classification result is accurate as well as bounding boxes of human and object both have IoUs larger than 0.5 with reference to ground-truth. Specifically, we follow prior works to report Scenario 1 role mAP on V-COCO dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>For HOI detection, we use EfficientDet-d3 <ref type="bibr" target="#b29">[30]</ref> as the backbone due to its effectiveness and efficiency. The backbone is pre-trained on COCO dataset. The instance classification and regression branches are also initialized with the COCO pre-trained weight, which is frozen during training. We apply random flip and random crop data augmentation approaches to our model. Adam optimizer <ref type="bibr" target="#b12">[13]</ref> is employed to optimize the loss function. We set the learning rate as 1e-4 with a batch size of 32. All experiments are carried out on NVIDIA RTX2080Ti GPUs. <ref type="table">Table 1</ref>. Results on V-COCO:Proposal shows whether it needs object detection beforehand. For Additional, P,B,L denotes human pose, human body part states and language priors respectively, which are utilized in prior methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Proposal Additional mAP role Inference Time Params  <ref type="bibr" target="#b6">[7]</ref> 44.7 204 ms 89 M Zhou et.al. <ref type="bibr" target="#b34">[35]</ref> 48.9 -620 M VSGNet <ref type="bibr" target="#b31">[32]</ref> 51.8 312 ms 59 M UnionDet <ref type="bibr" target="#b11">[12]</ref> 47.5 78 ms 44 M IP-Net <ref type="bibr" target="#b30">[31]</ref> 51.0 --DIRV (ours) 56.1 68 ms 12 M </p><formula xml:id="formula_17">RP D C D [</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results and Comparison</head><p>We compare our proposed DIRV with other state-of-theart methods on V-COCO (Tab. 1) and HICO-DET (Tab. 2) datasets. It is noticeable that many state-of-the-art models utilize other additional features like human poses and language priors. These methods require additional data, annotations or models, which are quite exhaustive to collect. For fairness, we do not take them (gray ones in both Tab. 1,2) into account in our comparison. What's more, unlike many existing two-stage approaches, our method does not rely on object proposals, which significantly elevates its compatibility.</p><p>For V-COCO dataset (Tab. 1), we follow prior works to ignore the class point since it has too few samples. Compared to prior arts, our approach outperforms them in accuracy significantly. It also has a fastest inference speed and a least parameter number.</p><p>For HICO-DET dataset (Tab. 2), we report the results on two different settings: Default and Known Objects. The interaction classification branch only classifies verb categories e.g. eating, which are associated with object cate-gories e.g. apple based on the results of instance classification branch, as in <ref type="bibr" target="#b11">[12]</ref>. This classification strategy brings a more promising performance than directly recognizing the verb-object pair. The reason may be that it reduces the number of categories in interaction classification branch, which elevates the accuracy. What's more, it also saves the space overhead, allowing a larger batch size during training and improving the training stability. The results also demonstrate that our approach has a superiority in time and space complexity.</p><p>Two prior arts share some common insights with us. In-teractNet <ref type="bibr" target="#b7">[8]</ref> localizes objects based on single human appearance. UnionDet <ref type="bibr" target="#b11">[12]</ref> is another anchor-based one-stage HOI detection approach, focusing on union regions. However, we surpass their performance by a large margin on both datasets, which proves the effectiveness of our concentration on interaction regions and our dense interaction region voting strategy.</p><p>In the supplementary materials, we show some qualitative results of our network, which is further analyzed with visualization. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>In this section, we dig into the influence of different modules in our DIRV. For simplicity, all results here are for V-COCO dataset. Analysis of more components are available in the supplementary materials. <ref type="table">Table 3</ref>. Interaction Region Overlapping Thresholds: tu, t h , to denote the thresholds in Eq. 4. The interaction regions become denser as these three thresholds decrease. Interaction Regions Overlapping Thresholds We set interaction regions in a dense manner for human-object pairs. The overlapping thresholds in Eq. 4 is examined in this part. Results in Tab. 3 certificate this dense manner, which can make full use of the visual features.</p><p>Voting Strategy We examine the superiority of our voting strategy by adding a NMS module for interaction regions, which weakens the effect of voting. In <ref type="figure" target="#fig_6">Fig. 4</ref>, we set different IoU thresholds for NMS and the performance drops as the value of those thresholds decreases (when IoU threshold is 1, NMS takes no effect). It reveals that interaction regions of different scales all contribute to the final detection though some of their classification scores may not be very high. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss Function mAP role</head><p>Focal Loss <ref type="bibr" target="#b19">[20]</ref> 54.8 Foreground Loss <ref type="bibr" target="#b11">[12]</ref> 54.0 Ignore Loss (ours) 56.1</p><p>Ignorance Loss We look into the effect of loss function in interaction classification branch. We test the performance with vanilla focal loss, foreground loss in <ref type="bibr" target="#b11">[12]</ref> and our proposed ignorance loss. Results in Tab. 4 verify our superiority since it can help dealing with region overlapping and missed positive labels.</p><p>Backbone We apply a novel backbone <ref type="bibr" target="#b29">[30]</ref> to our model, which has never been utilized for HOI detection. We separately carry out experiments with EfficientDet-d1, d2, d3 and d4. To our surprise, we find that the heavier backbone doesn't certainly lead to better HOI detection performance, according to the results in Tab. 5.</p><p>We also reproduce another anchor-based one-stage algorithm UnionDet <ref type="bibr" target="#b11">[12]</ref> with EfficientDet-d3 backbone. Results in Tab. 5 reveals that our DIRV surpasses it because of our novel design in methodology, instead of the backbone improvement. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we present a novel one-stage HOI detection framework. It detects HOI in an intuitive manner by concentrating on the interaction regions. To compensate for the detection flaws of single interaction region, a voting strategy is applied as an alternative to conventional NMS. Our method outperforms all existing approaches without any additional features or proposals. Due to the one-stage structure and simple network architecture, our method reaches a very high efficiency with least model parameters compared to other state-of-the-art approaches. In the future, we will try to incoporate the part-level knowledge <ref type="bibr" target="#b14">[15]</ref> into our framework. In this supplement, we provide more analysis and experiments not included in the main paper due to space limitation. They are listed as follows:</p><p>• Analysis of performance and efficiency is given in Sec. A. We compare our method with other existing ones.</p><p>• We show some qualitative results of our proposed interaction regions in Sec. B</p><p>• More ablation studies are conducted to examine some components of our DIRV in Sec. C.</p><p>• We visualize some examples of HOI detection in various cases to analyze the generality of our DIRV in Sec. D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Performance and Efficiency</head><p>As mentioned in the main paper, our DIRV surpasses other state-of-the-art approaches in accuracy with both fewer parameters and faster inference speed.</p><p>For parameter counting, we follow the estimation strategy in <ref type="bibr" target="#b0">[1]</ref> to calculate the parameter number of iCAN <ref type="bibr" target="#b6">[7]</ref> and TIN <ref type="bibr" target="#b16">[17]</ref>. Similar estimation is also applied to Union-Det <ref type="bibr" target="#b11">[12]</ref>, InteractNet <ref type="bibr" target="#b7">[8]</ref> and IP-Net <ref type="bibr" target="#b30">[31]</ref> since the authors did not provide open-source codes. For VSGNet <ref type="bibr" target="#b31">[32]</ref> and PMFNet <ref type="bibr" target="#b32">[33]</ref>, parameters are counted based on the opensource codes.</p><p>For time estimation, we consider the sum of the object detection time and HOI detection time for those two-stage approaches, including iCAN <ref type="bibr" target="#b6">[7]</ref>, TIN <ref type="bibr" target="#b16">[17]</ref>, InteractNet <ref type="bibr" target="#b7">[8]</ref> and VSGNet <ref type="bibr" target="#b31">[32]</ref>. We run different models on a NVIDIA RTX2080Ti GPU and some results are referred from other published work <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b17">18]</ref>.</p><p>In <ref type="figure" target="#fig_8">Fig. 5</ref>, we illustrate the performance of different models versus inference time and parameter number separately on V-COCO dataset. It is apparent that our DIRV outperforms others remarkably with a significant superiority in both time and space efficiency. <ref type="figure">Figure 6</ref>. Examples of Interaction Regions: Red and blue rectangles respectively denote the interacting humans and objects. Interaction regions are drawn in purple. During training, interaction regions are actually much denser than these illustrations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Analysis of Interaction Regions</head><p>We provide more examples of interaction regions with different scales in <ref type="figure">Fig. 6</ref>, where each interaction region is associated with a specific human-object interaction. These interaction regions are composed of parts of the human, object and context, containing visual features essential for HOI detection.  We mark positive (with interactions) and negative (w/o interactions) pairs with different colors. There are two hints in this image. Firstly, positive pairs are predicted with notably higher interaction classification scores in most interaction regions since our interaction regions capture the most crucial visual features for interactions. Secondly, object location probability for positive regions is not certainly very high. The relative spatial relationship is reflected from some very subtle visual features, which are hard to be completely discovered from a single interaction region. This corroborates the necessity of our voting strategy.</p><p>As a supplement, we also illustrate interaction detector results of some single interaction regions in <ref type="figure">Fig. 8</ref>. Most single regions can fetch satisfying classification results but there exist clear errors in object localization. However, since the errors are distributed in all directions uniformly, they are counterbalanced through voting. <ref type="figure">Figure 8</ref>. Detection Result of Single Interaction Region: Yellow dotted rectangles denote the interaction regions. We visualize the object location distribution and list the classification results for each interaction region. Note that here hit-instr means hit with instrument in the upper line, whose target object is the racket.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. More Ablation Studies</head><p>We add three extra ablation studies on V-COCO dataset here. Firstly, we verify the significance of our interaction detector, which serves as the key of our DIRV. Then, we examine the effect of scores from instance detector. Finally, we consider different values for the standard deviation σ of the 2-d Gaussian distribution in Eq. 8 of the main paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Interaction Detector</head><p>Since the results can be derived from the instance detector alone, we try to eliminate the whole interaction detector, i.e. set s f use h,o = 1. In this case, we have</p><formula xml:id="formula_18">S h,o = s h · s o · (s act h + s act o )<label>(16)</label></formula><p>in place of Eq. 14 in the main paper. Results in Tab. 6 witness a dramatic drop of 15.5 mAP, which verifies the indispensability of our novel interaction detector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Instance Scores</head><p>In Eq. 14 of the main paper, the instance classification scores s h , s o and instance action classification scores s act h , s act o from the instance detector have an auxiliary influence on the final HOI score of a human-object pair. We analyze their exact roles by separately setting them as a fixed value 1 when making the prediction. The results are shown in Tab. 6. Although these scores are beneficial for the prediction, their effect is limited especially compared to the scores s f use h,o from the interaction detector. This result reversely verifies the effectiveness of our interaction detector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Standard Deviation for Location Distribution</head><p>In this part, we analyze the hyper-parameter σ for the Gaussian distribution of the relative object location (Eq. 8 in the main paper). We find that the model performance is not very sensitive to this standard deviation, as is shown in <ref type="figure" target="#fig_11">Fig. 9</ref>. It shows the reliability and robustness of our interaction region prediction and voting strategy. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Detection Visualization</head><p>We present some visualization of DIRV results in <ref type="figure" target="#fig_12">Fig. 10</ref>. For each human, the corresponding interaction labels and target objects are displayed in the same color. We mainly pay attention to examples with different characteristics. In these examples, our proposed DIRV deals with various situations very well, despite their special difficulties as follows.</p><p>For humans and objects vary in different sizes, our dense interaction regions can easily capture visual features of different scales, resulting in high confidence and accuracy.</p><p>For objects remote from humans, there exist less interactive clues in interaction regions, which makes the prediction harder than close human-object pairs. Despite the overall great performance, several ambiguous interactions (e.g. catch and throw) share some common features, bringing possible detection flaws. Multiple interactions of same or different humans may share overlapping interaction regions, which generates potential confusion during training. Yet, our proposed DIRV solved these problems well, obtaining satisfying performance in these cases.</p><p>Since our interaction regions focus on parts of humans or objects most essential for interaction, incomplete human or object instances can hardly have any negative influence on the detection.</p><p>All the examples above verify the strong generality of our proposed approach. We are looking forward to its wide application in different practical applications. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Overview of our DIRV Framework: It is composed of two components: Interaction Detector and Instance Detector. For each interaction region, a relative spatial vector is obtained by regressing the human and object bounding boxes. During inference, results of interaction regions vote for an object location distribution, from which HOI score is derived.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>s loc aj (x o , y o ) = s inter aj · p aj (x o , y o )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Object Location Distribution: we visualize the target object location distribution for some human instances of several categories. Our voting strategy accurately localizes the objects in these interactions.Then, Eq. 11 is rewritten for each specific human-object pair.s f use h,o = aj ∈A h,o s loc aj (x o , y o ) (13) where A h,o denotes all the interaction regions {a j } associated human-object pair (b h , b o ) where (b h , b o ) = (b aj h , b aj o</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>. 4.1 and model implementation details in Sec. 4.2. Then, we compare the performance of our model with other state-of-the-art approaches in Sec. 4.3. Finally, effect of some crucial configurations are examined with ablation study in Sec. 4.4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 .</head><label>4</label><figDesc>Ablation Study for Voting Strategy: The mAP role increases as the IoU threshold for NMS grows. There is actually no NMS when IoU threshold is 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 .</head><label>5</label><figDesc>mAP versus Inference Time/Parameter Number on V-COCO dataset: Our proposed DIRV reaches a new state-ofthe-art 56.1 mAP role with fastest inference time (68 ms) and fewest parameters (13M) compared with previous methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 .</head><label>7</label><figDesc>Scores of Interaction Regions: It shows object location probability pa j (x a j o , y a j o ) and interaction classification scores s inter a j of interaction regions for different human-object pairs. Interaction regions of positive and negative pairs are marked in orange and blue separately.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 7</head><label>7</label><figDesc>visualizes the interaction classification scores s inter aj versus object location probability p aj (x aj o , y aj o ) of different interaction regions for some human-object pairs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 .</head><label>9</label><figDesc>Ablation Study for Standard Deviation σ of Object Location Distribution</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 10 .</head><label>10</label><figDesc>(a) large humans or objects (b) small humans or objects (c) humans remote from target objects (d) humans close to target objects (e) humans interacting with multiple objects (f) different HOI pairs overlapping with each other (g) incomplete humans or objects Visualization of Detection Results</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>From instance detector, a set of human H and object O (H ⊂ O) candidates are generated after NMS. For each human instance, we get its bounding box b h ∈ R 4 , instance classification score s h ∈ R and instance action classification score s act Similarly, we obtain bounding box b o ∈ R 4 , instance classification score s o ∈ R and instance action classification score s act</figDesc><table /><note>h ∈ R C h . s h ∈ R is a scalar since an instance can only be classified as a unique object category with highest score (here is human) while s acth ∈ R C h is a C h -d vector.o ∈ R Co for each object. In interaction detector, it fetches a triplet of (b ai h,inter , s interai , b ai o,inter ) from each interaction region ai , where b ai h,inter , b ai o,inter ∈ R 4 are the human/object target bounding boxes and s inter ai</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>When no object is involved, we simply define S h = s h · s act h . The HOI scores are not normalized because we only care about their relative value for the same interaction category.The time complexity of voting is O(|A pos |), where A pos = ∪</figDesc><table><row><cell></cell><cell>).</cell></row><row><cell cols="2">Thus, the final HOI score for a human-object pair (b h , b o )</cell></row><row><cell>can be derived as</cell><cell></cell></row><row><cell>S h,o = s h · s o · (s act h + s act o ) · s f use h,o</cell><cell>(14)</cell></row><row><cell cols="2">where s h , s o , s act h , s act o have been explained in section Par-</cell></row><row><cell>allel Inference. h,o</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>Results on HICO-DET: Proposal shows whether it needs object detection beforehand. For Additional, P,B,L denotes human pose, human body part states and language priors respectively, which are utilized in prior methods.</figDesc><table><row><cell>Method</cell><cell>Proposal Additional</cell><cell>Full</cell><cell cols="3">Default Rare Non-Rare Full</cell><cell cols="2">Known Object Rare Non-Rare</cell><cell cols="2">Inference Time Params</cell></row><row><cell>RP D C D [17]</cell><cell>P</cell><cell cols="2">17.03 13.42</cell><cell>18.11</cell><cell cols="2">19.17 15.51</cell><cell>20.26</cell><cell>513 ms</cell><cell>64 M</cell></row><row><cell>PMFNet [33]</cell><cell>P</cell><cell cols="2">17.46 15.65</cell><cell>18.00</cell><cell cols="2">20.34 17.47</cell><cell>21.20</cell><cell>253 ms</cell><cell>179 M</cell></row><row><cell>MLCNet [29]</cell><cell>P+B+L</cell><cell cols="2">17.95 16.62</cell><cell>18.35</cell><cell cols="2">22.28 20.73</cell><cell>22.74</cell><cell>-</cell><cell>-</cell></row><row><cell>ConsNet [23]</cell><cell>P+L</cell><cell cols="2">22.15 17.12</cell><cell>23.65</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>InteractNet [8]</cell><cell></cell><cell>9.94</cell><cell>7.16</cell><cell>10.77</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>145 ms</cell><cell>72 M</cell></row><row><cell>iCAN [7]</cell><cell></cell><cell cols="2">14.84 10.45</cell><cell>16.15</cell><cell cols="2">16.26 11.33</cell><cell>17.73</cell><cell>204 ms</cell><cell>89 M</cell></row><row><cell>UnionDet [12]</cell><cell></cell><cell cols="2">17.58 11.72</cell><cell>19.33</cell><cell cols="2">19.76 14.68</cell><cell>21.27</cell><cell>78 ms</cell><cell>50 M</cell></row><row><cell>IP-Net [31]</cell><cell></cell><cell cols="2">19.56 12.79</cell><cell>21.58</cell><cell cols="2">22.05 15.77</cell><cell>23.92</cell><cell>-</cell><cell>-</cell></row><row><cell>PPDM-Hourglass [18]</cell><cell></cell><cell cols="2">21.73 13.78</cell><cell>24.10</cell><cell cols="2">24.58 16.65</cell><cell>26.84</cell><cell>71 ms</cell><cell>195 M</cell></row><row><cell>DIRV (ours)</cell><cell></cell><cell cols="2">21.78 16.38</cell><cell>23.39</cell><cell cols="2">25.52 20.84</cell><cell>26.92</cell><cell>68 ms</cell><cell>13 M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Loss Function for Interaction Classification</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Ablation Study for Backbones: We compare the performance of our DIRV and another anchor-based method UnionDet<ref type="bibr" target="#b11">[12]</ref> with different backbones.</figDesc><table><row><cell>Method</cell><cell>Backbone (Params)</cell><cell>mAP role</cell></row><row><cell>UnionDet</cell><cell>ResNe50-FPN (34M) EfficientDet-d3 (12M)</cell><cell>47.5 49.2</cell></row><row><cell></cell><cell>EfficientDet-d1 (6.6M)</cell><cell>46.8</cell></row><row><cell>DIRV (ours)</cell><cell>EfficientDet-d2 (8.1M) EfficientDet-d3 (12M)</cell><cell>51.3 56.1</cell></row><row><cell></cell><cell>EfficientDet-d4 (21M)</cell><cell>54.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 .</head><label>6</label><figDesc>Effect of Each Score for HOI Prediction: We examine the role of each score (s h , so, s act h , s act o , s f use h,o ) in Eq. 14. The lack of interaction detector brings significant performance drop, while removing instance classification scores or instance action classification scores only leads to limited performance drop.</figDesc><table><row><cell>Method</cell><cell>Scores s h , s o s act h , s act o</cell><cell>s f use h,o</cell><cell>Method</cell></row><row><cell></cell><cell></cell><cell></cell><cell>40.6</cell></row><row><cell>DIRV</cell><cell></cell><cell></cell><cell>53.8 53.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell>56.1</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Detecting human-object interactions via functional generalization. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Rambhatla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1904" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning to detect human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 ieee winter conference on applications of computer vision (wacv)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="381" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">HICO: A benchmark for recognizing human-object interactions in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Weakly and semi supervised human body part parsing via poseguided knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="70" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pairwise body-part attention for recognizing human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="51" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Rmpe: Regional multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2334" to="2343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">ican: Instance-centric attention network for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.10437</idno>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Detecting and recognizing human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Aligning linguistic words and visual semantic units for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.04474</idno>
		<title level="m">Visual semantic role labeling</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">No-frills human-object interaction detection: Factorization, layout encodings, and training techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Uniondet: Unionlevel detector towards real-time human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Conference on Learning Representations</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Relation-aware graph attention network for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.06539</idno>
		<title level="m">Hake: Human activity knowledge engine</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pastanet: Toward human activity knowledge engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Transferable interactiveness knowledge for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Ppdm: Parallel point detection and matching for real-time human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<idno>abs/1612.03144</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<idno>abs/1708.02002</idno>
		<imprint>
			<date type="published" when="2008" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Contextaware visual policy network for sequence-level image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<idno>abs/1808.05864</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">ConsNet: Learning Consistency Graph for Zero-Shot Human-Object Interaction Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.06254</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Visual relationship detection with language priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="852" to="869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Learning conditioned graph structures for interpretable visual question answering. CoRR, abs/1806.07243</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Norcliffe-Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vafeias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Parisot</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning human-object interactions by graph parsing neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="401" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1506.01497</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Finegym: A hierarchical video dataset for fine-grained action understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Human object interaction detection via multi-level conditioned network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 International Conference on Multimedia Retrieval, ICMR &apos;20</title>
		<meeting>the 2020 International Conference on Multimedia Retrieval, ICMR &apos;20</meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="26" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Efficientdet: Scalable and efficient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Learning human-object interaction detection using interaction points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Vsgnet: Spatial attention network for detecting human object interactions using graph convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ulutan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S M</forename><surname>Iftekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Manjunath</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pose-aware multi-level feature network for human object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning actor relation graphs for group activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Cascaded human-object interaction recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="4263" to="4272" />
		</imprint>
	</monogr>
	<note>(a) accuracy vs speed (b) accuracy vs size</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
