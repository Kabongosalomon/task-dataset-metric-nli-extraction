<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Graph WaveNet for Deep Spatial-Temporal Graph Modeling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
							<email>zonghan.wu-3@student.uts.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">Centre for Artificial Intelligence</orgName>
								<orgName type="institution" key="instit1">FEIT</orgName>
								<orgName type="institution" key="instit2">University of Technology Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirui</forename><surname>Pan</surname></persName>
							<email>shirui.pan@monash.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Faculty of Information Technology</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Long</surname></persName>
							<email>guodong.long@uts.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">Centre for Artificial Intelligence</orgName>
								<orgName type="institution" key="instit1">FEIT</orgName>
								<orgName type="institution" key="instit2">University of Technology Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
							<email>jing.jiang@uts.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">Centre for Artificial Intelligence</orgName>
								<orgName type="institution" key="instit1">FEIT</orgName>
								<orgName type="institution" key="instit2">University of Technology Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
							<email>chengqi.zhang@uts.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">Centre for Artificial Intelligence</orgName>
								<orgName type="institution" key="instit1">FEIT</orgName>
								<orgName type="institution" key="instit2">University of Technology Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Graph WaveNet for Deep Spatial-Temporal Graph Modeling</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Spatial-temporal graph modeling is an important task to analyze the spatial relations and temporal trends of components in a system. Existing approaches mostly capture the spatial dependency on a fixed graph structure, assuming that the underlying relation between entities is pre-determined. However, the explicit graph structure (relation) does not necessarily reflect the true dependency and genuine relation may be missing due to the incomplete connections in the data. Furthermore, existing methods are ineffective to capture the temporal trends as the RNNs or CNNs employed in these methods cannot capture long-range temporal sequences. To overcome these limitations, we propose in this paper a novel graph neural network architecture, Graph WaveNet, for spatial-temporal graph modeling. By developing a novel adaptive dependency matrix and learn it through node embedding, our model can precisely capture the hidden spatial dependency in the data. With a stacked dilated 1D convolution component whose receptive field grows exponentially as the number of layers increases, Graph WaveNet is able to handle very long sequences. These two components are integrated seamlessly in a unified framework and the whole framework is learned in an end-to-end manner. Experimental results on two public traffic network datasets, METR-LA and PEMS-BAY, demonstrate the superior performance of our algorithm.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Spatial-temporal graph modeling has received increasing attention with the advance of graph neural networks. It aims to model the dynamic node-level inputs by assuming interdependency between connected nodes, as demonstrated by <ref type="figure">Figure 1</ref>. Spatial-temporal graph modeling has wide applications in solving complex system problems such as traffic speed forecasting <ref type="bibr" target="#b4">[Li et al., 2018b]</ref>, taxi demand prediction <ref type="bibr" target="#b9">[Yao et al., 2018]</ref>, human action recognition [Yan * Corresponding Author. <ref type="figure">Figure 1</ref>: Spatial-temporal graph modeling. In a spatial-temporal graph, each node has dynamic input features. The aim is to model each node's dynamic features given the graph structure. <ref type="bibr">et al., 2018]</ref>, and driver maneuver anticipation <ref type="bibr" target="#b3">[Jain et al., 2016]</ref>. For a concrete example, in traffic speed forecasting, speed sensors on roads of a city form a graph where the edge weights are judged by two nodes' Euclidean distance. As the traffic congestion on one road could cause lower traffic speed on its incoming roads, it is natural to consider the underlying graph structure of the traffic system as the prior knowledge of inter-dependency relationships among nodes when modeling time series data of the traffic speed on each road.</p><p>A basic assumption behind spatial-temporal graph modeling is that a node's future information is conditioned on its historical information as well as its neighbors' historical information. Therefore how to capture spatial and temporal dependencies simultaneously becomes a primary challenge. Recent studies on spatial-temporal graph modeling mainly follow two directions. They either integrate graph convolution networks (GCN) into recurrent neural networks (RNN) <ref type="bibr" target="#b7">[Seo et al., 2018;</ref><ref type="bibr" target="#b4">Li et al., 2018b]</ref> or into convolution neural networks (CNN) . While having shown the effectiveness of introducing the graph structure of data into a model, these approaches face two major shortcomings.</p><p>First, these studies assume the graph structure of data reflects the genuine dependency relationships among nodes. However, there are circumstances when a connection does not entail the inter-dependency relationship between two nodes and when the inter-dependency relationship between two nodes exists but a connection is missing. To give each circumstance an example, let us consider a recommendation system. In the first case, two users are connected, but they may have distinct preferences over products. In the second case, two users may share a similar preference, but they are not linked together.  used attention mechanisms to address the first circumstance by adjusting the dependency weight between two connected nodes, but they failed to consider the second circumstance.</p><p>Second, current studies for spatial-temporal graph modeling are ineffective to learn temporal dependencies. RNNbased approaches suffer from time-consuming iterative propagation and gradient explosion/vanishing for capturing longrange sequences <ref type="bibr" target="#b7">[Seo et al., 2018;</ref><ref type="bibr" target="#b4">Li et al., 2018b;</ref>. On the contrary, CNN-based approaches enjoy the advantages of parallel computing, stable gradients and low memory requirement . However, these works need to use many layers in order to capture very long sequences because they adopt standard 1D convolution whose receptive field size grows linearly with an increase in the number of hidden layers.</p><p>In this work, we present a CNN-based method named Graph WaveNet, which addresses the two shortcomings we have aforementioned. We propose a graph convolution layer in which a self-adaptive adjacency matrix can be learned from the data through an end-to-end supervised training. In this way, the self-adaptive adjacency matrix preserves hidden spatial dependencies. Motivated by WaveNet <ref type="bibr" target="#b6">[Oord et al., 2016]</ref>, we adopt stacked dilated casual convolutions to capture temporal dependencies. The receptive field size of stacked dilated casual convolution networks grows exponentially with an increase in the number of hidden layers. With the support of stacked dilated casual convolutions, Graph WaveNet is able to handle spatial-temporal graph data with long-range temporal sequences efficiently and effectively. The main contributions of this work are as follows:</p><p>• We construct a self-adaptive adjacency matrix which preserves hidden spatial dependencies. Our proposed self-adaptive adjacency matrix is able to uncover unseen graph structures automatically from the data without any guidance of prior knowledge. Experiments validate that our method improves the results when spatial dependencies are known to exist but are not provided. • We present an effective and efficient framework to capture spatial-temporal dependencies simultaneously. The core idea is to assemble our proposed graph convolution with dilated casual convolution in a way that each graph convolution layer tackles spatial dependencies of nodes' information extracted by dilated casual convolution layers at different granular levels. • We evaluate our proposed model on traffic datasets and achieve state-of-the-art results with low computation costs. The source codes of Graph WaveNet are publicly available from https://github.com/ nnzhan/Graph-WaveNet.</p><p>2 Related Works</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Graph Convolution Networks</head><p>Graph convolution networks are building blocks for learning graph-structured data <ref type="bibr" target="#b8">[Wu et al., 2019]</ref>. They are widely applied in domains such as node embedding , node classification <ref type="bibr" target="#b3">[Kipf and Welling, 2017]</ref>, graph classification <ref type="bibr" target="#b9">[Ying et al., 2018]</ref>, link prediction  and node clustering <ref type="bibr" target="#b7">[Wang et al., 2017]</ref>. There are two mainstreams of graph convolution networks, the spectralbased approaches and the spatial-based approaches. Spectralbased approaches smooth a node's input signals using graph spectral filters <ref type="bibr" target="#b1">[Bruna et al., 2014;</ref><ref type="bibr">Defferrard et al., 2016;</ref><ref type="bibr" target="#b3">Kipf and Welling, 2017]</ref>. Spatial-based approaches extract a node's high-level representation by aggregating feature information from neighborhoods <ref type="bibr" target="#b0">[Atwood and Towsley, 2016;</ref><ref type="bibr" target="#b2">Gilmer et al., 2017;</ref><ref type="bibr" target="#b3">Hamilton et al., 2017]</ref>. In these approaches, the adjacency matrix is considered as prior knowledge and is fixed throughout training. <ref type="bibr" target="#b5">Monti et al. [2017]</ref> learned the weight of a node's neighbor through Gaussian kernels. Velickovic et al.</p><p>[2017] updated the weight of a node's neighbor via attention mechanisms. Liu et al. <ref type="bibr">[2019]</ref> proposed an adaptive path layer to explore the breadth and depth of a node's neighborhood. Although these methods assume the contribution of each neighbor to the central node is different and need to be learned, they still rely on a predefined graph structure. <ref type="bibr" target="#b4">Li et al. [2018a]</ref> adopted distance metrics to adaptively learn a graph's adjacency matrix for graph classification problems. This generated adjacency matrix is conditioned on nodes' inputs. As inputs of a spatialtemporal graph are dynamic, their method is unstable for spatial-temporal graph modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Spatial-temporal Graph Networks</head><p>The majority of Spatial-temporal Graph Networks follows two directions, namely, RNN-based and CNN-based approaches. One of the early RNN-based methods captured spatial-temporal dependencies by filtering inputs and hidden states passed to a recurrent unit using graph convolution <ref type="bibr" target="#b7">[Seo et al., 2018]</ref>. Later works adopted different strategies such as diffusion convolution <ref type="bibr" target="#b4">[Li et al., 2018b]</ref> and attention mechanisms  to improve model performance. Another parallel work used node-level RNNs and edge-level RNNs to handle different aspects of temporal information <ref type="bibr" target="#b3">[Jain et al., 2016]</ref>. The main drawbacks of RNN-based approaches are that it becomes inefficient for long sequences and its gradients are more likely to explode when they are combined with graph convolution networks. CNN-based approaches combine a graph convolution with a standard 1D convolution . While being computationally efficient, these two approaches have to stack many layers or use global pooling to expand the receptive field of a neural network model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>In this section, we first give the mathematical definition of the problem we are addressing in this paper. Next, we describe two building blocks of our framework, the graph convolution layer (GCN) and the temporal convolution layer (TCN). They work together to capture the spatial-temporal dependencies. Finally, we outline the architecture of our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Definition</head><p>A graph is represented by G = (V, E) where V is the set of nodes and E is the set of edges. The adjacency matrix derived from a graph is denoted by</p><formula xml:id="formula_0">A ∈ R N ×N . If v i , v j ∈ V and (v i , v j ) ∈ E, then A ij is one otherwise it is zero.</formula><p>At each time step t, the graph G has a dynamic feature matrix X (t) ∈ R N ×D . In this paper, the feature matrix is used interchangeably with graph signals. Given a graph G and its historical S step graph signals, our problem is to learn a function f which is able to forecast its next T step graph signals. The mapping relation is represented as follows</p><formula xml:id="formula_1">[X (t−S):t , G] f − → X (t+1):(t+T ) ,<label>(1)</label></formula><p>where</p><formula xml:id="formula_2">X (t−S):t ∈ R N ×D×S and X (t+1):(t+T ) ∈ R N ×D×T .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Graph Convolution Layer</head><p>Graph convolution is an essential operation to extract a node's features given its structural information. <ref type="bibr" target="#b3">Kipf et al. [2017]</ref> proposed a first approximation of Chebyshev spectral filter <ref type="bibr">[Defferrard et al., 2016]</ref>. From a spatial-based perspective, it smoothed a node's signal by aggregating and transforming its neighborhood information. The advantages of their method are that it is a compositional layer, its filter is localized in space, and it supports multi-dimensional inputs. LetÃ ∈ R N ×N denote the normalized adjacency matrix with self-loops, X ∈ R N ×D denote the input signals , Z ∈ R N ×M denote the output, and W ∈ R D×M denote the model parameter matrix, in <ref type="bibr" target="#b3">[Kipf and Welling, 2017]</ref> the graph convolution layer is defined as</p><formula xml:id="formula_3">Z =ÃXW.<label>(2)</label></formula><p>Li et al.</p><p>[2018b] proposed a diffusion convolution layer which proves to be effective in spatial-temporal modeling. They modeled the diffusion process of graph signals with K finite steps. We generalize its diffusion convolution layer into the form of Equation 2, which results in,</p><formula xml:id="formula_4">Z = K k=0 P k XW k ,<label>(3)</label></formula><p>where P k represents the power series of the transition matrix.</p><p>In the case of an undirected graph, P = A/rowsum(A).</p><p>In the case of a directed graph, the diffusion process have two directions, the forward and backward directions, where the forward transition matrix P f = A/rowsum(A) and the backward transition matrix</p><formula xml:id="formula_5">P b = A T /rowsum(A T ).</formula><p>With the forward and the backward transition matrix, the diffusion graph convolution layer is written as</p><formula xml:id="formula_6">Z = K k=0 P k f XW k1 + P k b XW k2 .<label>(4)</label></formula><p>Self-adaptive Adjacency Matrix: In our work, we propose a self-adaptive adjacency matrixÃ adp . This self-adaptive adjacency matrix does not require any prior knowledge and is learned end-to-end through stochastic gradient descent. In doing so, we let the model discover hidden spatial dependencies by itself. We achieve this by randomly initializing two node embedding dictionaries with learnable parameters E 1 , E 2 ∈ R N ×c . We propose the self-adaptive adjacency matrix asÃ adp = Sof tM ax(ReLU (E 1 E T 2 )).</p><p>(5) We name E1 as the source node embedding and E2 as the target node embedding. By multiplying E1 and E2, we derive the spatial dependency weights between the source nodes and the target nodes. We use the ReLU activation function to eliminate weak connections. The SoftMax function is applied to normalize the self-adaptive adjacency matrix. The normalized self-adaptive adjacency matrix, therefore, can be considered as the transition matrix of a hidden diffusion process. By combining pre-defined spatial dependencies and self-learned hidden graph dependencies, we propose the following graph convolution layer</p><formula xml:id="formula_7">Z = K k=0 P k f XW k1 + P k b XW k2 +Ã k apt XW k3 .<label>(6)</label></formula><p>When the graph structure is unavailable, we propose to use the self-adaptive adjacency matrix alone to capture hidden spatial dependencies, i.e.,</p><formula xml:id="formula_8">Z = K k=0Ã k apt XW k .<label>(7)</label></formula><p>It is worth to note that our graph convolution falls into spatial-based approaches. Although we use graph signals interchangeably with node feature matrix for consistency, our graph convolution in Equation 7 indeed is interpreted as aggregating transformed feature information from different orders of neighborhoods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Temporal Convolution Layer</head><p>We adopt the dilated causal convolution <ref type="bibr" target="#b10">[Yu and Koltun, 2016]</ref> as our temporal convolution layer (TCN) to capture a node's temporal trends. Dilated causal convolution networks allow an exponentially large receptive field by increasing the layer depth. As opposed to RNN-based approaches, dilated casual convolution networks are able to handle longrange sequences properly in a non-recursive manner, which facilitates parallel computation and alleviates the gradient explosion problem. The dilated causal convolution preserves the temporal causal order by padding zeros to the inputs so that predictions made on the current time step only involve historical information. As a special case of standard 1Dconvolution, the dilated causal convolution operation slides over inputs by skipping values with a certain step, as illustrated by <ref type="figure">Figure 2</ref>. Mathematically, given a 1D sequence input x ∈ R T and a filter f ∈ R K , the dilated causal convolution operation of x with f at step t is represented as</p><formula xml:id="formula_9">x f (t) = K−1 s=0 f (s)x(t − d × s),<label>(8)</label></formula><p>where d is the dilation factor which controls the skipping distance. By stacking dilated causal convolution layers with dilation factors in an increasing order, the receptive field of a </p><formula xml:id="formula_10">h = g(Θ 1 X + b) σ(Θ 2 X + c),<label>(9)</label></formula><p>where Θ 1 , Θ 2 , b and c are model parameters, is the element-wise product, g(·) is an activation function of the outputs, and σ(·) is the sigmoid function which determines the ratio of information passed to the next layer. We adopt Gated TCN in our model to learn complex temporal dependencies. Although we empirically set the tangent hyperbolic function as the activation function g(·), other forms of Gated TCN can be easily fitted into our framework, such as an LSTM-like Gated <ref type="bibr">TCN [Kalchbrenner et al., 2016]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Framework of Graph WaveNet</head><p>We present the framework of Graph WaveNet in <ref type="figure" target="#fig_0">Figure 3</ref>. It consists of stacked spatial-temporal layers and an output layer. A spatial-temporal layer is constructed by a graph convolution layer (GCN) and a gated temporal convolution layer (Gated TCN) which consists of two parallel temporal convolution layers (TCN-a and TCN-b). By stacking multiple spatial-temporal layers, Graph WaveNet is able to handle spatial dependencies at different temporal levels. For example, at the bottom layer, GCN receives short-term temporal information while at the top layer GCN tackles long-term temporal information. The inputs h to a graph convolution layer in practice are three-dimension tensors with size [N,C,L] where N is the number of nodes, and C is the hidden dimension, L is the sequence length. We apply the graph convolution layer to each of h[:, :, i] ∈ R N ×C .</p><p>We choose to use mean absolute error (MAE) as the training objective of Graph WaveNet, which is defined by</p><formula xml:id="formula_11">L(X (t+1):(t+T ) ; Θ) = 1 T N D i=T i=1 j=N j=1 k=D k=1 |X (t+i) jk −X (t+i) jk | (10)</formula><p>Unlike previous works such as <ref type="bibr" target="#b4">[Li et al., 2018b;</ref>, our Graph WaveNet outputsX (t+1):(t+T ) as a whole rather than generatingX (t) recursively through T steps. It addresses the problem of inconsistency between training and  testing due to the fact that a model learns to make predictions for one step during training and is expected to produce predictions for multiple steps during inference. To achieve this, we artificially design the receptive field size of Graph WaveNet equals to the sequence length of the inputs so that in the last spatial-temporal layer the temporal dimension of the outputs exactly equals to one. After that we set the number of output channels of the last layer as a factor of step length T to get our desired output dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We verify Graph WaveNet on two public traffic network datasets, METR-LA and PEMS-BAY released by <ref type="bibr" target="#b4">Li et al. [2018b]</ref>. METR-LA records four months of statistics on traffic speed on 207 sensors on the highways of Los Angeles County. PEMS-BAY contains six months of traffic speed information on 325 sensors in the Bay area. We adopt the same data pre-processing procedures as in <ref type="bibr" target="#b4">[Li et al., 2018b]</ref>. The readings of the sensors are aggregated into 5-minutes windows. The adjacency matrix of the nodes is constructed by road network distance with a thresholded Gaussian kernel <ref type="bibr" target="#b7">[Shuman et al., 2012]</ref>. Z-score normalization is applied to inputs. The datasets are split in chronological order with 70% for training, 10% for validation and 20% for testing. Detailed dataset statistics are provided in <ref type="table" target="#tab_1">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Baselines</head><p>We compare Graph WaveNet with the following models.</p><p>• ARIMA. Auto-Regressive Integrated Moving Average model with Kalman filter <ref type="bibr" target="#b4">[Li et al., 2018b]</ref>.  <ref type="bibr" target="#b4">[Li et al., 2018b]</ref> 3.44 6.30 9.60% 3.77 7.23 10.90% 4.37 8.69 13.20% WaveNet <ref type="bibr" target="#b6">[Oord et al., 2016]</ref> 2.99 5.89 8.04% 3.59 7.28 10.25% 4.45 8.93 13.62% DCRNN <ref type="bibr" target="#b4">[Li et al., 2018b]</ref> 2.77 5.38 7.30% 3.15 6.45 8.80% 3.60 7.60 10.50% GGRU  2.71 5.24 6.99% 3.12 6.36 8.56% 3.64 7.65 10.62% STGCN  2.88 5.74 7.62% 3.47 7.24 9.57% 4.59 9.40 12.70% Graph WaveNet 2.69 5.15 6.90% 3.07 6.22 8.37% 3.53 7.37 10.01%  • FC-LSTM Recurrent neural network with fully connected LSTM hidden units <ref type="bibr" target="#b4">[Li et al., 2018b]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PEMS</head><p>• WaveNet. A convolution network architecture for sequence data <ref type="bibr" target="#b6">[Oord et al., 2016]</ref>.</p><p>• DCRNN. Diffusion convolution recurrent neural network <ref type="bibr" target="#b4">[Li et al., 2018b]</ref>, which combines graph convolution networks with recurrent neural networks in an encoder-decoder manner.</p><p>• GGRU. Graph gated recurrent unit network . Recurrent-based approaches. GGRU uses attention mechanisms in graph convolution.</p><p>• STGCN. Spatial-temporal graph convolution network , which combines graph convolution with 1D convolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Setups</head><p>Our experiments are conducted under a computer environment with one Intel(R) Core(TM) i9-7900X CPU @ 3.30GHz and one NVIDIA Titan Xp GPU card. To cover the input sequence length, we use eight layers of Graph WaveNet with a sequence of dilation factors 1, 2, 1, 2, 1, 2, 1, 2. We use Equation 4 as our graph convolution layer with a diffusion step K = 2. We randomly initialize node embeddings by a uniform distribution with a size of 10. We train our model using Adam optimizer with an initial learning rate of 0.001. Dropout with p=0.3 is applied to the outputs of the graph convolution layer. The evaluation metrics we choose include mean absolute error (MAE), root mean squared error (RMSE), and mean absolute percentage error (MAPE). Missing values are excluded both from training and testing.  It outperforms temporal models including ARIMA, FC-LSTM, and WaveNet by a large margin. Compared to other spatial-temporal models, Graph WaveNet surpasses the previous convolution-based approach STGCN significantly and excels recurrent-based approaches DCRNN and GGRU at the same time. In respect of the second best model GGRU as suggested in <ref type="table" target="#tab_4">Table 2</ref>, Graph WaveNet achieves small improvement over GGRU on the 15-minute horizons; however, realizes bigger enhancement on the 60-minute horizons. We think this is because our architecture is more capable of detecting spatial dependencies at each temporal stage. GGRU uses recurrent architectures in which parameters of the GCN layer are shared across all recurrent units. In contrast, Graph WaveNet employs stacked spatial-temporal layers which contain separate GCN layers with different parameters. Therefore each GCN layer in Graph WaveNet is able to focus on its own range of temporal inputs. We plot 60-minutes-ahead predicted values v.s real values of Graph WaveNet and WaveNet on a snapshot of the test data in <ref type="figure" target="#fig_1">Figure 4</ref>. It shows that Graph WaveNet generates more stable predictions than WaveNet. In particular, there is a red sharp spike produced by WaveNet, which deviates far from real values. On the contrary, the curve of Graph WaveNet goes in the middle of real values all the time.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of the Self-Adaptive Adjacency Matrix</head><p>To verify the effectiveness of our proposed adaptive adjacency matrix, we conduct experiments with Graph WaveNet using five different adjacency matrix configurations. <ref type="table" target="#tab_7">Table 3</ref> shows the average score of MAE, RMSE, and MAPE over 12 prediction horizons. We find that the adaptive-only model works even better than the forward-only model with mean MAE. When the graph structure is unavailable, Graph WaveNet would still be able to realize a good performance. The forward-backward-adaptive model achieves the lowest scores on all three evaluation metrics. It indicates that if graph structural information is given, adding the self-adaptive adjacency matrix could introduce new and useful information to the model. In <ref type="figure" target="#fig_3">Figure 5</ref>, we further investigate the learned self-adaptive adjacency matrix under the configuration of the forward-backward-adaptive model trained on the METR-LA dataset. According to <ref type="figure" target="#fig_3">Figure 5a</ref>, some columns have more high-value points than others such as column 9 in the left box compared to column 47 in the right box. It suggests that some nodes are influential to most nodes in a graph while other nodes have weaker impacts. <ref type="figure" target="#fig_3">Figure 5b</ref> confirms our observation. It can be seen that node 9 locates nearby the intersection of several main roads while node 47 lies in a single road.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Computation Time</head><p>We compare the computation cost of Graph WaveNet with DCRNN and STGCN on the METR-LA dataset in <ref type="table" target="#tab_9">Table 4</ref>.  Graph WaveNet runs five times faster than DCRNN but two times slower than STGCN in training. For inference, we measure the total time cost of each model on the validation data. Graph WaveNet is the most efficient of all at the inference stage. This is because that Graph WaveNet generates 12 predictions in one run while DCRNN and STGCN have to produce the results conditioned on previous predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we present a novel model for spatial-temporal graph modeling. Our model captures spatial-temporal dependencies efficiently and effectively by combining graph convolution with dilated casual convolution. We propose an effective method to learn hidden spatial dependencies automatically from the data. This opens up a new direction in spatialtemporal graph modeling where the dependency structure of a system is unknown but needs to be discovered. On two public traffic network datasets, Graph WaveNet achieves state-ofthe-art results. In future work, we will study scalable methods to apply Graph WaveNet on large-scale datasets and explore approaches to learn dynamic spatial dependencies.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>The framework of Graph WaveNet. It consists of K spatial-temporal layers on the left and an output layer on the right. The inputs are first transformed by a linear layer and then passed to the gated temporal convolution module (Gated TCN) followed by the graph convolution layer (GCN). Each spatial-temporal layer has residual connections and is skip-connected to the output layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Comparison of prediction curves between WaveNet and Graph WaveNet for 60 minutes ahead prediction on a snapshot of the test data of METR-LA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>The learned self-adaptive adjacency matrix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Summary statistics of METR-LA and PEMS-BAY.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison of Graph WaveNet and other baseline models. Graph WaveNet achieves the best results on both datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>compares the performance of Graph WaveNet and</cell></row><row><cell>baseline models for 15 minutes, 30 minutes and 60 minutes</cell></row><row><cell>ahead prediction on METR-LA and PEMS-BAY datasets.</cell></row><row><cell>Graph WaveNet obtains the superior results on both datasets.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Experimental results of different adjacency matrix configurations. The forward-backward-adaptive model achieves the best results on both datasets. The adaptive-only model achieves nearly the same performance with the forward-only model.</figDesc><table><row><cell>(a) The heatmap of the learned</cell></row><row><cell>self-adaptive adjacency matrix</cell></row><row><cell>for the first 50 nodes.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>The computation cost on the METR-LA dataset.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was funded by the Australian Government through the Australian Research Council (ARC) under grants 1) LP160100630 partnership with Australia Government Department of Health and 2) LP150100671 partnership with Australia Research Alliance for Children and Youth (ARACY) and Global Business College Australia (GBCA).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Diffusion-convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Towsley ; James</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Don</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Towsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Michaël Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
	<note>NIPS</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Gilmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hamilton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.10099</idno>
	</analytic>
	<monogr>
		<title level="m">Neural machine translation in linear time</title>
		<editor>Kalchbrenner et al., 2016] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Koray Kavukcuoglu</editor>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5308" to="5317" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Diffusion convolutional recurrent neural network: Data-driven traffic forecasting</title>
	</analytic>
	<monogr>
		<title level="m">Graph neural networks with adaptive receptive paths</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3546" to="3553" />
		</imprint>
	</monogr>
	<note>AAAI</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Monti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5115" to="5124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning graph embedding with adversarial training methods</title>
		<idno type="arXiv">arXiv:1609.03499</idno>
	</analytic>
	<monogr>
		<title level="m">Guodong Long, Jing Jiang, and Chengqi Zhang</title>
		<editor>Shirui Pan, Ruiqi Hu, Sai-fu Fung</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>IJCAI</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains</title>
		<idno type="arXiv">arXiv:1211.0053</idno>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="889" to="898" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>CIKM</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sijie Yan, Yuanjun Xiong, and Dahua Lin. Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<idno type="arXiv">arXiv:1901.00596</idno>
	</analytic>
	<monogr>
		<title level="m">A comprehensive survey on graph neural networks</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3482" to="3489" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>AAAI</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<editor>Ying, Jiaxuan You, Christopher Morris, Xiang Ren, Will Hamilton, and Jure Leskovec</editor>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4800" to="4810" />
		</imprint>
	</monogr>
	<note>NIPS</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multiscale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Spatio-temporal graph convolutional networks: A deep learning framework for traffic forecasting</title>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3634" to="3640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Link prediction based on graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen ; Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5165" to="5175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<idno type="arXiv">arXiv:1803.07294</idno>
		<title level="m">Gaan: Gated attention networks for learning on large and spatiotemporal graphs</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
