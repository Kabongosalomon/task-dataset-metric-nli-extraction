<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Glimpse Clouds: Human Activity Recognition from Unstructured Feature Points</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabien</forename><surname>Baradel</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Univ Lyon</orgName>
								<orgName type="institution" key="instit2">INSA-Lyon</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">LIRIS</orgName>
								<address>
									<postCode>F-69621</postCode>
									<settlement>Villeurbanne</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Wolf</surname></persName>
							<email>christian.wolf@liris.cnrs.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Univ Lyon</orgName>
								<orgName type="institution" key="instit2">INSA-Lyon</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">LIRIS</orgName>
								<address>
									<postCode>F-69621</postCode>
									<settlement>Villeurbanne</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">INRIA</orgName>
								<orgName type="institution" key="instit2">CITI Laboratory</orgName>
								<address>
									<settlement>Villeurbanne</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mille</surname></persName>
							<email>julien.mille@insa-cvl.fr</email>
							<affiliation key="aff2">
								<orgName type="laboratory">Laboratoire d&apos;Informatique de l&apos;</orgName>
								<orgName type="institution" key="instit1">Université de Tours</orgName>
								<orgName type="institution" key="instit2">INSA Centre Val de Loire</orgName>
								<address>
									<postCode>41034</postCode>
									<settlement>Blois</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
							<email>gwtaylor@uoguelph.ca</email>
							<affiliation key="aff3">
								<orgName type="department">School of Engineering</orgName>
								<orgName type="institution">University of Guelph</orgName>
								<address>
									<settlement>Guelph</settlement>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">Vector Institute</orgName>
								<address>
									<settlement>Toronto</settlement>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Glimpse Clouds: Human Activity Recognition from Unstructured Feature Points</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a method for human activity recognition from RGB data which does not rely on any pose information during test time, and which does not explicitly calculate pose information internally. Instead, a visual attention module learns to predict glimpse sequences in each frame. These glimpses correspond to interest points in the scene which are relevant to the classified activities. No spatial coherence is forced on the glimpse locations, which gives the module liberty to explore different points at each frame and better optimize the process of scrutinizing visual information.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Tracking and sequentially integrating this kind of unstructured data is a challenge, which we address by separating the set of glimpses from a set of recurrent tracking/recognition workers. These workers receive the glimpses, jointly performing subsequent motion tracking and prediction of the activity itself. The glimpses are softassigned to the workers, optimizing coherence of the assignments in space, time and feature space using an external memory module. No hard decisions are taken, i.e. each glimpse point is assigned to all existing workers, albeit with different importance. Our methods outperform state-ofthe-art methods on the largest human activity recognition dataset available to-date; NTU RGB+D Dataset, and on a smaller human action recognition dataset Northwestern-UCLA Multiview Action 3D Dataset. Our code is publicly available 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>We address the problem of human activity recognition in settings where activities are complex and diverse, performed by an individual or involving multiple people interacting. These activities may even include people interacting with objects or the environment. The usage of RGB-D cameras is very popular in this case, as it allows the use of articulated pose (skeletons) delivered in real time and relatively cheaply by some middleware. The exclusive usage of pose makes it possible to work on gesture and activity recognition without being a specialist in vision, and with significantly reduced dimensionality of the input data. The combined usage of pose and raw depth and/or RGB images can often boost performance over a solution that uses a single modality.</p><p>In this paper we propose a method which only uses raw RGB images at test time. We avoid the usage of articulated pose essentially for two reasons: (i) depth data is not always available, for example, in applications involving smaller or otherwise resource-constrained robots; and (ii) the question whether articulated pose is the optimal intermediate representation for activity recognition is unclear. We explore an alternative strategy, which consists of learning a local representation of the video through a visual attention process.</p><p>We conjecture that the replacement of articulated pose should keep one important property, which is its collection of local entities, which can be tracked over time and whose motion is relevant to the activity at hand. Instead of fixing the semantic meaning of these entities to the definition of a subset of the joints in the human body, we learn it discriminatively. In our strategy, the attention process is completely free to attend to arbitrary locations at each time instant. In particular, we do not impose any constraints on spatiotemporal coherence of glimpse locations, which allows the  <ref type="figure">Figure 1</ref>. We recognize human activities from unstructured collections of spatio-temporal glimpses with distributed recurrent tracking/recognition and soft-assignment among glimpse points and trackers.</p><p>model to vary its focus within and across frames. Certain similarities can be made to human gaze patterns which saccade to different points in a scene.</p><p>Activities are highly correlated with motion, and therefore tracking the motion of specific points of visual interest is essential, yielding a distributed representation of the collection of glimpses. Appearance and motion features need to be collected over time from local points and integrated into a sequential decision model. However, tracking a set of glimpse points, whose location is not spatio-temporally smooth and whose semantic meaning can change from frame to frame, is a challenge. Our objective is to match new glimpses with past ones of the same (or a nearby) location in the scene. Due to the unconstrained nature of the attention mechanism, we do not know when a point in the scene has been last scrutinized, or if it has been attended to in the past.</p><p>We solve this issue by separating the problem into two distinct parts: (i) selecting a distributed and local representation of G glimpse points through a sequential recurrent attention model, and (ii) tracking the set of glimpses by a set of C recurrent workers which sequentially integrate features, and participate in the final recognition of the activity ( <ref type="figure">Fig. 1)</ref>. In general, G can be different from C, and the assignment between glimpses and workers is soft. Each worker is potentially assigned to all glimpses, albeit to a varying degree. This assignment attention distribution is calculated with external memory based on regularities in space, time and feature space.</p><p>We summarize the main contributions of our paper as follows:</p><p>• We present a method for human activity recognition which does not require articulated pose during testing and which models activities two attentional processes;</p><p>one extracting a set of glimpses per frame and one reasoning about entities over time.</p><p>• This unstructured "cloud" of glimpses produced by the attention process are tracked over time using a set of trackers/recognizers, which are soft-assigned using external memory. Each tracker can potentially track multiple glimpses.</p><p>• Articulated pose is used during training time as an additional target, encouraging the attention process to focus on human structures.</p><p>• All attentional mechanisms are executed in feature space which is calculated jointly with a global model processing the full input image.</p><p>• We evaluate our method on the NTU RGB-D dataset, the largest available human activity dataset, where we outperform the state-of-the-art by a large margin.</p><p>• We also show state-of-the-art results on a smaller human action recognition dataset: the Northwestern-UCLA Multiview Action 3D Dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Activities, gestures and multimodal data -Recent gesture and human activity recognition methods dealing with several modalities typically process 2D+T RGB and/or depth data as 3D. Sequences of frames are stacked into volumes and fed into convolutional layers at the first stages <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b53">54]</ref>. When additional pose data is available <ref type="bibr" target="#b35">[36]</ref>, the 3D joint positions are typically fed into a separate network. Preprocessing pose is reported to improve performance in some situations, e.g. augmenting coordinates with velocities and acceleration <ref type="bibr" target="#b57">[58]</ref>. Fusing pose and raw video modalities is traditionally done as late fusion <ref type="bibr" target="#b38">[39]</ref>, or early through fusion layers <ref type="bibr" target="#b53">[54]</ref>.</p><p>In contrast, our method does not require pose during testing and only leverages it during training for regularization.</p><p>Recurrent architectures for action recognition -Recurrent neural networks (or their variants) are employed in much of the contemporary work on activity recognition, and a recent trend is to make recurrent models local. Partaware LSTMs <ref type="bibr" target="#b42">[43]</ref> separate the memory cell of an LSTM network <ref type="bibr" target="#b16">[17]</ref> into part-based sub-cells and let the network learn long-term representations individually for each part, fusing the parts for output. Similarly, Du et al <ref type="bibr" target="#b10">[11]</ref> use bi-directional LSTM layers which fit an anatomical hierarchy. Skeletons are split into anatomically-relevant parts (legs, arms, torso, etc.) and let subnetworks specialize on them. Lattice LSTMs partition the latent space over a grid which is aligned with the spatial input space <ref type="bibr" target="#b47">[48]</ref>.</p><p>Our method, on the other hand, soft-assigns parts of the scene over multiple recurrent workers, where each worker can potentially integrate all points of the scene.</p><p>Tracking and distributed recognition -Structural RNNs <ref type="bibr" target="#b19">[20]</ref> bear a certain resemblance to our work. They handle the temporal evolution of tracked objects in videos with a set of RNNs, each of which correspond to cliques in a graph which models the spatio-temporal relationships between these objects. However, this graph is hand-crafted manually for each application, and the tracking of the objects is done using external trackers, which are not integrated into the neural model.</p><p>Our model, on the other hand, does not rely on external trackers and does not require the manual creation of a graph, since the assignments between objects (here, glimpses) and trackers are learned automatically.</p><p>Attention mechanisms and external memory -attention mechanisms focus selectively on parts of the scene which are the most relevant to the target task. Two classes of attention have emerged in recent years. Soft attention weights each part of the observation dynamically <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b23">24]</ref>. The objective function is usually differentiable, allowing gradient-based optimization. Soft attention was proposed for image <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b54">55]</ref> and video understanding <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b55">56]</ref> with spatial, temporal and spatio-temporal variants.</p><p>Towards action recognition in particular, Sharma et al <ref type="bibr" target="#b44">[45]</ref> proposed a recurrent mechanism from RGB data, which integrates convolutional features from different parts of a space-time volume. Song et al <ref type="bibr" target="#b45">[46]</ref> propose separate spatial and temporal attention networks for action recognition from pose. At each frame, the spatial attention model gives more importance to the joints most relevant to the current action, whereas the temporal model selects frames.</p><p>On the other hand, hard attention, which is the principle we adopt in this work, takes hard decisions when choosing parts of the input data. In a seminal paper, Mnih et al <ref type="bibr" target="#b37">[38]</ref> proposed visual hard attention for image classification built around an RNN. The model selects the next loca-tion to focus on, based on past information. Similar hard attention was used in multiple object recognition <ref type="bibr" target="#b1">[2]</ref>, object localization <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b21">22]</ref>, saliency map generation <ref type="bibr" target="#b25">[26]</ref>, or action detection <ref type="bibr" target="#b56">[57]</ref>. While the early hard attention glimpses were not differentiable, implying reinforcement learning, the DRAW algorithm <ref type="bibr" target="#b13">[14]</ref> and spatial transformer networks (STN) <ref type="bibr" target="#b18">[19]</ref> provide attention crops which are fully differentiable and can thus be learned using gradient-based optimization.</p><p>Besides attention-based modules, the addition of external memory proved to increase the capacity of neural networks, by storing long-term information from past observations. This was mainly popularized by memory networks <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b26">27]</ref>. In <ref type="bibr" target="#b0">[1]</ref>, a Fully Convolutional Network is coupled with an attention-based memory module to perform context selection and refinement, for semantic segmentation. In <ref type="bibr" target="#b49">[50]</ref>, visual memory is used to learn a spatio temporal representation of moving objects in a scene. The memory is implemented as a convolutional GRU with a 2D spatial hidden state. In <ref type="bibr" target="#b33">[34]</ref>, the ST-LSTM method of <ref type="bibr" target="#b32">[33]</ref> is extended with a global context memory for skeleton-based action recognition. Multiple attention iterations are performed to optimize the global context memory, which is used for the final classification. In <ref type="bibr" target="#b48">[49]</ref>, an LSTM-based memory network is used for RGB and optical flow-based action recognition.</p><p>Our attention process is different from previously published work in that it produces an unstructured Glimpse Cloud in a spatio temporal cube. The attention process in unconstrained, which we show to be an important design choice. In our work, the external memory module is trainable in reading only, and provides a way to remember past soft-assignments of glimpses to recurrent workers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Dynamic sequential attention</head><p>We first introduce the following notation. We want to map our input video sequence X ∈ R T ×H×W ×3 to a corresponding activity label y where H, W , T denote, respectively, the height, the width and the number of time steps. The sequence X is a set of RGB input images X t ∈ R H×W ×3 with t = 1...T . We do not assume any other kind of prior information on the input data. We do not use any external information during testing such as pose data nor depth nor motion. However, if pose data is available during training time, our method is capable of integrating it in the form of additional inputs, which we show increases the performance of the system (see section 5). Most of the RGB-only state-of-the-art methods, which do not use pose data, extract features at a frame level by feeding the entire video frame to a pre-trained deep network. This leads to global features, which do not capture local information well, which is relevant to the activities at hand. Reasoning at a local level has, up till now, been achieved us-ing pose features, or attention processes which were limited to attention maps (e.g. <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b31">32]</ref>). Here, we propose an alternative approach, where an attention process runs statically over each time instant and over time, creating sequences of sets of glimpse points, from which features are extracted.</p><p>Our model processes videos using several key components, also illustrated in figure 1: i) a recurrent spatial attention model that extracts features from different local glimpses v t,g following an attention path in each video over frames t and multiple glimpses g in each frame; and ii) recurrent soft-tracking workers (indexed by c) which process these spatial features sequentially. The input data being unstructured, the spatial glimpses are soft-assigned to the workers, such that no hard decisions are taken at any point. To this end, iii) an external memory module keeps track of the glimpses seen in the past, their features, as well as of past soft-assignments, and produces new soft-assignments optimizing spatio-temporal consistency. Our approach is fully-differentiable, such that the full model is trained endto-end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Joint global/local feature space</head><p>We recognize activities based on global and local features jointly. In order to speed up calculations and to avoid extracting redundant calculations, we use a single feature space computed by a global model. In particular, we map an input sequence X to a spatio-temporal feature map Z ∈ R T ×H ×W ×C using a deep neural network f (·) with 3D convolutions. Pooling is performed on the spatial dimensions but, not in time. This allows retention of the original temporal scale of the video, and thus access to features in each frame. It should, however, be noted, that due to the 3D convolutions used, the temporal receptive field of a single "temporal" slice of the feature map is greater than a single frame. This is intended, as it allows the attention process to utilize motion. In an abuse of terminology, we will still use the term frame to specify the slice Z t of a feature map with a temporal length of 1. More information on the architecture of f (·) is given in section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">A recurrent model of spatial attention</head><p>Inspired by human behavior when scrutinizing a scene, we extract a fixed number of features from a series of G glimpses within each frame. The process of moving from one glimpse to another is achieved with a recurrent model. Glimpses are indexed by index g=1 . . . G, and each glimpse Z t,g corresponds to a sub-region of Z t using coordinates and scale l t,g = x g , y g , s x g , s y g t output by a differentiable glimpse function, which will be defined in section 3.3. Features are extracted from the glimpse region Z t,g using global average pooling, resulting in a 1D feature vector z t,g :</p><formula xml:id="formula_0">z t,g = Γ(Z t,g ) = 1 H W m n Z t,g (m, n)<label>(1)</label></formula><p>where W × H is the size of the glimpse region. The glimpse locations and scales l g for g=1 . . . G are predicted by a recurrent network, which runs over glimpses. As illustrated in <ref type="figure">Fig. 1</ref>, the model predicts a fixed-length sequence of glimpse points for each frame. It runs over the video, i.e. it is not restarted/reinitialized after each frame. The hidden state thus carries information across frames and creates a globally coherent scrutinization process over the video. In equations 2 and 3 we index glimpses with a linear index g.</p><p>The recurrent model is given as follows (we use GRUs <ref type="bibr" target="#b9">[10]</ref>, for notational simplicity we omit gates and biases in the rest of the equations):</p><formula xml:id="formula_1">h g = Ω(h g−1 , [z g−1 , r t ] |θ) (2) l g = W l [h g , c t ]<label>(3)</label></formula><p>where h denotes the hidden state of the RNN running over glimpses g, c t is a frame context vector for making the process aware of frame transitions (described in section 4.4) and r t carries information about the high level classification task. In essence, r t corresponds to the global hidden state of the recurrent workers performing the actual recognition, as described in section 4.1, equation <ref type="bibr" target="#b6">(7)</ref>. Note, that the recurrence runs over glimpses g. The index t here corresponds to the frame associated to current glimpse g.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Differentiable glimpse module</head><p>In order to create a model which can be trained end-toend, we use a simple version of spatial transformer networks (STN) <ref type="bibr" target="#b18">[19]</ref> to perform a differentiable crop operation on each feature map. Given an input feature map Z t ∈ R H×W ×C and the glimpse parameters l g = x g , y g , s x g , s y g where (x g , y g ) is the central focus point and (s x g , s y g ) corresponds to the scale, we output a feature map Z t,g ∈ R H ×W ×C . Note that the output size can differ from the input size.</p><p>We constrain the STN to implement a simple 2D affine transformation A lg which allows cropping, translation and isotropic scaling on a regular grid point x t i , y t i according to the given glimpse parameters l g :</p><formula xml:id="formula_2">x s i y s i = A lg   x t i y t i 1   = s x g 0 x g 0 s y g y g   x t i y t i 1   (4)</formula><p>where x t i , y t i are the target coordinates of the regular grid in the output feature map Z t,g and x s i , y s i are the source coordinates in the input feature map that define the sample points. We must define a sampler which takes the set of sampling points (x s i , y s i ), along with the input feature map Z t and produces the sampled output feature map Z t,g . We employ bilinear interpolation which implements the following mapping:</p><formula xml:id="formula_3">Z t,g (x s i , y s i ) = H n W m Z t (m, n) max(0, 1−|x s i −n|) max(0, 1−|y s i −m|).</formula><p>The STN is differentiable, which allows us to train the parameters W l for the prediction of focus point parameters l g together with the rest of the network using gradient descent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Distributed Reasoning on Unstructured Glimpse Clouds</head><p>The attended points (glimpses) predicted in each frame Z t have a semantic meaning in the input video (e.g. a patch around the hands or shoulders; an object held or pointed at by a person etc.). The goal is to reason about their positions, motion, changes in appearance, relationships or other properties. This is made difficult by the sequential attention process described in section 3.2, which can provide very different glimpse sequences for each frame, since we avoid any direct supervision. This is intentional, in order to give the spatial attention process complete freedom. In particular, it can choose to jump to different glimpse points at each frame, and/or decide to revisit certain glimpses attended to in the past. Since frame features Z t also encode motion due to the 3D convolutions in f (·), the attention process can learn to revisit attended points, compensating for their motion. In section 7 we describe experiments performed which justify this kind of attention process compared to an alternate choice of spatio-temporally coherent attention.</p><p>As a consequence, extracting motion cues from semantic points in the scene requires associating glimpse points from different frames over time. Due to the freedom of the attention process and fixed number of glimpses, subsequent glimpses of the same point in the scene are generally not in subsequent frames, which excludes conventional tracking mechanisms known from the computer vision literature. Instead, we avoid hard tracking and hard assignments between glimpse points in a temporal manner. We propose a soft associative model for automatically associating similar spatial features over time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Distributed soft-tracking workers</head><p>As given in eq. (1), we denote by z t,g the features extracted from the g th glimpse in feature map Z t for g = 1...G and t = 1...T . We are interested in a joint encoding of spatial dimensions and feature dimensions and employ "what" and "where" features v t,g introduced in <ref type="bibr" target="#b27">[28</ref></p><formula xml:id="formula_4">] defined by: v t,g = z t,g ⊗ Λ(l t,g |θ Λ )<label>(5)</label></formula><p>where ⊗ is the Hadamard product and Λ(l t,g |θ Λ ) is a network which provides an embedding of the spatial patch coordinates into a space which is of the same dimensionality as the features z t,g . The vector v t,g contains joint cues about motion and appearance, but also the spatial localization of those features.</p><p>Evolution over time of this information is modeled with a number (C) of so-called soft-tracking workers Ψ c for c = 1...C. Each worker corresponds to a recurrent model capable of tracking entities over time. We never hard assign glimpses to workers. Inputs to each individual worker correspond to weighted contributions from all of the G glimpses. In general, the number of glimpse points G can be different from the number of workers C. At each time instant, focal points are thus soft-assigned to the workers on the fly but changing the weights of the contributions, which will be described further below.</p><p>A worker Ψ c is a recurrent network following the usual update equations based on the past state r t−1,c and its input v t,c :</p><formula xml:id="formula_5">r t,c = Ψ c (r t−1,c ,ṽ t,c |θ Ψc )<label>(6)</label></formula><formula xml:id="formula_6">r t = c r t,c<label>(7)</label></formula><p>where Ψ c is a GRU and r t is carrying global information about the current state (needed as input of the recurrent model of spatial attention). The inputṽ t,c to each worker Ψ c is a linear combination of the different glimpses {v t,g }, g = 1 . . . G weighted by a soft attention distribution p t,c = {p t,g,c }, g = 1 . . . G:</p><formula xml:id="formula_7">v t,c = V t p t,c<label>(8)</label></formula><p>where V t is a matrix whose rows are the different glimpse features v t,g . Workers are independent from each other in the sense that they do not share parameters θ Ψc . This can potentially lead to specialization of the workers on types of tracked and integrated scene entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Soft-assignment using External Memory</head><p>The role of the attention distribution p t,c is to give higher weights to glimpses which have been soft-assigned to this worker in the past. Thus workers extract different kinds of features from each other. To do so, we employ an external memory bank denoted M = {m k } which is common to all workers. In particular, M is a fixed-length array of K entries m k each capable of storing a feature vector v t,g . Even if the external memory is common to each worker, they have their own ability to extract information from it. Each worker Ψ c has its own weight bank denoted W c = {w c,k }. The scalar w c,k holds the importance of the entry m c,k for worker Ψ c . Hence the overall external memory is defined by the set {M , W 1 , . . . W c }.</p><p>Attention from memory reads -The attention distribution p t,c is a distribution over glimpses g, i.e. p t,c = {p t,c,g }, 0 ≤ p t,c,g ≤ 1 and g p t,c,g =1. We want the glimpses to get distributed appropriately across the workers, and encourage worker specialization. In particular, at each timestep we want to assign a glimpse high importance to a worker if this worker has been soft-assigned similar glimpses in the past with high importance. To this end, we define a fully trainable distance function φ(., .) which is implemented as a quadratic form:</p><formula xml:id="formula_8">φ(x, y) = (x − y) D(x − y)<label>(9)</label></formula><p>where D is a learned weight matrix. Within each batch we normalize φ(·, ·) by min-max normalization to scale it to lie between 0 and 1. A glimpse g is soft-assigned to a given worker c with a higher weight p t,c,g if v t,g is similar to vectors m k from the memory bank M which had a high importance for the worker in the past Ψ c :</p><formula xml:id="formula_9">p t,c,g = σ α k e −t m k × w c,k [1 − φ(v t,g , m k )] (10)</formula><p>where σ is the softmax function over the G glimpses and e −t m k is an exponential rate over time to give higher importance to recent feature vectors compared to those in the distant past. t m k is the corresponding timestep of the memory bank m k . In practice we add a temperature term α to the softmax function σ. When α → 0 the output vector is sparser. The negative factor multiplied with φ is justified by the fact that φ is initially pre-trained as a Mahalanobis distance by setting D to the inverse covariance matrix of the glimpse data. The factor therefore transforms the distance into a similarity. After pre-training, D is trained end-toend.</p><p>The attention distribution p t,c is computed for each worker Ψ c . Thus each glimpse g potentially contributes to each worker Ψ c through its input vectorṽ t,c (c.f. equation <ref type="bibr" target="#b7">(8)</ref>), albeit with different weights.</p><p>Memory writesfor each frame, the feature representations v t,g are stored in the memory bank M . However, the attention distribution p t,c = {p t,c,g } is used to weight these entries for each worker Ψ c . If a glimpse feature v t,g is stored in a slot m k , then its importance weight w c,k for worker Ψ c is set to p t,c,g . The only limitation is the size K of the memory bank. When the memory is full, we delete the oldest memory slot. More flexible storing processes, e.g. trained mappings, are left for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Recognition</head><p>Since workers proceed in a independent manner through time, we need an aggregation strategy to perform classification. Each worker Ψ c has its own hidden state {r t,c } t=1...T and is responsible for its own classification through a fullyconnected layer. The final classification is done by averaging logits of the workers:</p><formula xml:id="formula_10">q c = W c · r c (11) y = softmax C c q c<label>(12)</label></formula><p>whereŷ is the probability vector of assigning the input video X to each class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Context vector</head><p>In order to make the spatial attention process (section 3.2) aware of frame transitions, we introduce a context vector c t which contains high level information about humans present in the current frame t. c t is obtained by global average pooling over the spatial domain of the penultimate feature maps of a given timestep. We regress the 2D pose coordinates of humans from the context vector c t using the following mapping:</p><formula xml:id="formula_11">y p t = W p c t<label>(13)</label></formula><p>Pose y p t is linked to ground truth pose (during training only) using a supervised term described in section 5. This leads to incorporate hierarchical feature learning in a sense that the penultimate feature maps have to detect human joints present in each frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Training</head><p>We train the model end-to-end with a sum of a collection of loss terms, which are explained in the following paragraphs:</p><formula xml:id="formula_12">L = L D (ŷ, y) + L P (ŷ p , y p ) + L G (l, y p )<label>(14)</label></formula><p>Supervision -L D (ŷ, y) is a supervised loss term (cross-entropy loss on activity labels y).</p><p>Pose prediction -articulated pose y p is available for many datasets. Our goal is to not depend on pose during testing; however, its usage during training can provide additional information to the learning process and reduce the tendency of activity recognition methods to memorize individual elements in the data for recognition. We therefore add an additional term L P (ŷ p , y p ), which encourages the model to perform pose regression during training only from intermediate feature maps (described in section 4.4). Pose regression over time leads to a faster convergence of the overall model.</p><p>Attracting glimpses to humans -L G (l, y p ) is a loss encouraging the glimpse points to be as sparse as possible within a frame but by the same time close to humans in the scene. Recall that l t,g = x t,g , y t,g , s x t,g , s y t,g</p><formula xml:id="formula_13">T , so L G is defined by: L t G1 (l, y p ) = 1 1 + G g1 G g2 ||l t,g1 , l t,g2 ||<label>(15)</label></formula><p>L t G2 (l, y p ) = G g min j ||l t,g , y p j || (16)</p><formula xml:id="formula_14">L G (l, y p ) = T t L t G1 (l, y p ) + L t G2 (l, y p )<label>(17)</label></formula><p>where y p j denotes the 2D coordinates of joints j, and Euclidean distance on l t,g is computed using the central focus point (x t,g , y t,g ) only. L G1 encourages diversity between glimpses within a frame. L G2 ensures that all the glimpses are not taken too far away from humans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Architectures -Pretraining</head><p>We designed the 3D convolutional network f (·) computing the global feature maps in section 3.1 such that the temporal dimension is maintained, i.e. without any temporal subsampling. We proceed from the Resnet-50 network <ref type="bibr" target="#b15">[16]</ref> and inflate the 2D spatial convolutional kernels into 3D kernels, artificially creating new a temporal dimension, as described by Carreira et al <ref type="bibr" target="#b7">[8]</ref>. This allows us to take advantage of the 2D kernels learned by pre-training on image classification on the Imagenet dataset. The Inflated ResNet f (·) is then trained as a first step by minimizing the loss L D + L P . The supervised loss L D on the global model is applied on a path attached to global average pooling on the last feature maps followed by a fully-connected layer which is subsequently removed.</p><p>The recurrent spatial attention module Ω(·) is a GRU with a hidden state of size 1024; Λ(·) is an MLP with a single hidden layer of size 256 and a ReLU activation; the soft-trackers Ψ c are GRUs with a hidden state of size 512. There is no parameter sharing between them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Experimental results</head><p>The proposed method has been evaluated on two human action recognition datasets: NTU RDB+D Dataset <ref type="bibr" target="#b42">[43]</ref> and Northwestern-UCLA Multiview Action 3D Dataset <ref type="bibr" target="#b51">[52]</ref>. NTU RDB+D Dataset (NTU) -NTU has been acquired with a Kinect v2 sensor and contains more than 56K videos and 4 million frames with 60 different activities including individual activities, interactions between 2 people and health related events. The actions have been performed by 40 subjects and from 80 viewpoints. We follow the crosssubject and cross-view split protocol from <ref type="bibr" target="#b42">[43]</ref>. Due to the large amount of videos, this dataset is highly suitable for deep learning modeling.</p><p>Northwestern-UCLA Multiview Action 3D Dataset (N-UCLA) -This dataset <ref type="bibr" target="#b51">[52]</ref> contains 1494 sequences, covering ten action categories, such as drop trash or sit down. Each sequence is captured simultaneously by 3 Kinect v1 cameras. RGB, depth, and human pose are available for each video, and each action is performed one to six times by ten different subjects. Most actions involve human-object interaction, making this dataset challenging. We followed the cross-view protocol defined by <ref type="bibr" target="#b51">[52]</ref>, and we trained our method on samples from two camera views, and tested it on samples from the remaining view. This produced three possible cross-view combinations: V 3 1,2 , V 2 1,3 , V 1 2,3 . The combination V 3 1,2 means that samples from view 1 and 2 are used for training, and samples from view 3 are used for testing.</p><p>Implementation details -Following <ref type="bibr" target="#b42">[43]</ref>, we cut videos into sub-sequences of 8 frames and sample subsequences. During training, a single sub-sequence is sampled. During testing, 5 sub-sequences and logits are averaged. RGB videos are rescaled to 256 × 256 and random cropping of size 224 × 224 is done during training and testing.</p><p>Training is performed using the Adam Optimizer <ref type="bibr" target="#b24">[25]</ref> with an initial learning rate of 0.0001. We use minibatches of size 40 on 4 GPUs. Following <ref type="bibr" target="#b42">[43]</ref>, we sample 5% of the initial training set as a validation set, which is used for hyper-parameter optimization and for early stopping. All hyperparameters have been optimized on the validation sets of the respective datasets. We used the model trained on NTU as a pre-trained model and fine-tuned it on N-UCLA.</p><p>Comparison with the state of the art -Our method outperforms state-of-the-art methods on NTU and N-UCLA  by a large margin, and this also includes several methods which use multiple modalities among RGB, depth and pose. <ref type="table">Table 2</ref> and 7 provide detailed results compared to the stateof-the-art on the NTU dataset.</p><p>Ablation study - <ref type="table">Table 3</ref> shows several experiments to study the effect of our design choices. Classification from the Global Model alone (Inflated-Resnet-50) is clearly inferior to the distributed recognition strategy using the set of workers (+1.9 points on NTU and +4.4 points on N-UCLA). The bigger gap obtained on N-UCLA can be explained by the larger portion of the frame occupied by people and therefore higher efficiency of a local representation. The additional loss predicting pose during training helps, even though pose is not used during testing. An impor- Importance of losses - <ref type="table">Table 3</ref> also shows importances of our three loss functions. Cross-entropy only L D gives 89.1%. Adding pose prediction L P we gain 0.6 points and adding pose attraction L G we gain 0.4 points, which are complementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Unstructured vs. coherent attention -we also evaluated the choice of unstructured attention, i.e. the decision to give the attention process complete freedom to attend to a new (and possibly unrelated) set of scene points in each frame. We compared this with an alternative choice, where glimpses are axis-aligned space-time tubes over the whole temporal length of the video. In this baseline, the attention process is not aligned with time. At each iteration, a new tube is attended in the full space-time volume, and no tracking or soft-assignment to worker modules is necessary. As indicated in <ref type="table" target="#tab_1">Table 4</ref>, this choice is sub-optimal. We conjecture that tubes cannot cope with moving objects and object parts in the video.</p><p>Attention vs. saliency vs. random -we evaluated whether a sequential attention process contributes to performance, or whether the gain is solely explained from the sampling of local features in the space-time volume. We compared our choice with two simpler baselines: (i) complete random sampling of local features, which leads to a drop of more than 6 points. The location of the glimpses is clearly important. (ii) with a saliency model, which predicts glimpse locations in parallel through different outputs of the location network. This is not a full attention process, in that a glimpse prediction does not depend on what the model has seen in the past. This choice is also sub-optimal.</p><p>Learned weight matrix -Random initialization and fine-tuning of D matrix in equation 9 loses 0.4 points and leads to slower convergence by a factor of 1.5. Fixing D (to inverse covariance) w/o any training loses 0.8 points.</p><p>The Joint encoding -("what and where" features) are important in order to correctly weight their respective contribution. Plainly adding concatenating coordinates and features loses 1.1 points.</p><p>Hyper-parameters C, G, T -Number of glimpses and workers: C and G were selected by cross-validation on the validation set by varying them from 1 to 4, giving an optimum of G=C=3 over all 16 combinations. More leads the model to overfit. The size of the memory bank K is set to T where T =8 is the length of the sequence. Runtime -the model has been trained on a GPU cluster with a single job spread over 4 Titan Xp GPUs. Pretraining the global model on the NTU dataset takes 16h. Training the Glimpse Cloud model end-to-end then takes a further 12h. A single forward pass over the full model takes 97ms on 1 GPU. The method has been implemented in Py-Torch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>We proposed a method for human activity recognition which does not rely on depth images or articulated pose, though it is able to leverage pose information during training. The method achieves state-of-the-art performance on the NTU and N-UCLA datasets even when compared to methods that use pose, depth or both at test time. An attention process over space and time produces an unstructured Glimpse Cloud, which are soft-assigned to a set of tracking/recognition workers. In or experiments, we showed that this distributed recognition outperforms a global convolutional model and also local models with simpler baselines for the localization of glimpses. Future work will investigate more complex procedures for fusing the decisions of the set of workers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>3 Figure 2 .</head><label>32</label><figDesc>An external memory module determines an attention distribution over workers (a soft assignment) for each new glimpse vt,g based on similarities with past glimpses M and their past attention probabilities w. Shown for a single glimpse and 3 workers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>An illustration of the glimpse distribution for several sequences of the NTU dataset. Here we set 3 glimpses per frame (G=3, Red: first glimpse, Blue: second glimpse, Yellow: third one).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Results on the Northwestern-UCLA Multiview Action 3D dataset with Cross-View Setting (accuracy as a percent). V, D, and P mean Visual (RGB), Depth, and Pose, respectively.</figDesc><table><row><cell>Methods</cell><cell>Data</cell><cell>V 3 1,2</cell><cell>V 2 1,3</cell><cell>V 1 2,3</cell><cell>Avg</cell></row><row><cell>DVV [31]</cell><cell>D</cell><cell cols="4">58.5 55.2 39.3 51.0</cell></row><row><cell>CVP [60]</cell><cell>D</cell><cell cols="4">60.6 55.8 39.5 52.0</cell></row><row><cell>AOG [52]</cell><cell>D</cell><cell>45.2</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>HPM+TM [42]</cell><cell>D</cell><cell cols="4">91.9 75.2 71.9 79.7</cell></row><row><cell>Lie group [51]</cell><cell>P</cell><cell>74.2</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>HBRNN-L [12]</cell><cell>P</cell><cell>78.5</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Enhanced viz. [35]</cell><cell>P</cell><cell>86.1</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Ensemble TS-LSTM [29]</cell><cell>P</cell><cell>89.2</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Hankelets [30]</cell><cell>V</cell><cell>45.2</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>nCTE [15]</cell><cell>V</cell><cell cols="4">68.6 68.3 52.1 63.0</cell></row><row><cell>NKTM [41]</cell><cell>V</cell><cell cols="4">75.8 73.3 59.1 69.4</cell></row><row><cell>Global model</cell><cell>V</cell><cell cols="4">85.6 84.7 79.2 83.2</cell></row><row><cell>Glimpse Clouds</cell><cell>V</cell><cell cols="4">90.1 89.5 83.4 87.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 4 .</head><label>4</label><figDesc>Results on the NTU: different attention and alternative strategies.</figDesc><table><row><cell></cell><cell cols="2">Pose RGB</cell><cell>CS</cell><cell>CV Avg</cell></row><row><cell>Lie Group [51]</cell><cell></cell><cell>-</cell><cell cols="2">50.1 52.8 51.5</cell></row><row><cell>Skeleton Quads [13]</cell><cell></cell><cell>-</cell><cell cols="2">38.6 41.4 40.0</cell></row><row><cell>Dynamic Skeletons [18]</cell><cell></cell><cell>-</cell><cell cols="2">60.2 65.2 62.7</cell></row><row><cell>HBRNN [11]</cell><cell></cell><cell>-</cell><cell cols="2">59.1 64.0 61.6</cell></row><row><cell>Deep LSTM [43]</cell><cell></cell><cell>-</cell><cell cols="2">60.7 67.3 64.0</cell></row><row><cell>Part-aware LSTM [43]</cell><cell></cell><cell>-</cell><cell cols="2">62.9 70.3 66.6</cell></row><row><cell>ST-LSTM + TrustG. [33]</cell><cell></cell><cell>-</cell><cell cols="2">69.2 77.7 73.5</cell></row><row><cell>STA-LSTM [46]</cell><cell></cell><cell>-</cell><cell cols="2">73.2 81.2 77.2</cell></row><row><cell>Ensemble TS-LSTM [29]</cell><cell></cell><cell>-</cell><cell cols="2">74.6 81.3 78.0</cell></row><row><cell>GCA-LSTM [34]</cell><cell></cell><cell>-</cell><cell cols="2">74.4 82.8 78.6</cell></row><row><cell>JTM [53]</cell><cell></cell><cell>-</cell><cell cols="2">76.3 81.1 78.7</cell></row><row><cell>MTLN [23]</cell><cell></cell><cell>-</cell><cell cols="2">79.6 84.8 82.2</cell></row><row><cell>VA-LSTM [59]</cell><cell></cell><cell>-</cell><cell cols="2">79.4 87.6 83.5</cell></row><row><cell>View-invariant [35]</cell><cell></cell><cell>-</cell><cell cols="2">80.0 87.2 83.6</cell></row><row><cell>DSSCA -SSLM [44]</cell><cell></cell><cell></cell><cell>74.9</cell><cell>-</cell><cell>-</cell></row><row><cell>STA-Hands [5]</cell><cell>X</cell><cell>X</cell><cell cols="2">82.5 88.6 85.6</cell></row><row><cell>Hands Attention [6]</cell><cell></cell><cell></cell><cell cols="2">84.8 90.6 87.7</cell></row><row><cell>C3D †</cell><cell>-</cell><cell></cell><cell cols="2">63.5 70.3 66.9</cell></row><row><cell>Resnet50+LSTM †</cell><cell>-</cell><cell></cell><cell cols="2">71.3 80.2 75.8</cell></row><row><cell>Glimpse Clouds</cell><cell>-</cell><cell></cell><cell cols="2">86.6 93.2 89.9</cell></row><row><cell cols="5">Table 2. Results on the NTU RGB+D dataset with Cross-Subject</cell></row><row><cell cols="5">and Cross-View settings (accuracies in %); ( † indicates method</cell></row><row><cell>has been re-implemented).</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">tant question is whether the Glimpse Cloud could be in-</cell></row><row><cell cols="5">tegrated with an easier mechanism than a soft-assignment.</cell></row><row><cell cols="5">We tested a baseline which sums glimpse features for each</cell></row><row><cell cols="5">time step and which integrates them temporally (row #3).</cell></row><row><cell cols="5">This gave only a very small improvement over the global</cell></row><row><cell cols="5">model. Distributed recognition from Glimpse Clouds with</cell></row><row><cell cols="5">soft-assignment clearly outperforms the simpler baselines.</cell></row><row><cell cols="5">Adding the global model does not gain any improvement.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/fabienbaradel/glimpse_ clouds</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Acknowledgements</head><p>This work was funded under grant ANR Deepvision (ANR-15-CE23-0029), a joint French/Canadian call by ANR and NSERC.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Episodic camn: Contextual attention-based memory networks with iterative feedback for scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abdulnabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Winkler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multiple object recognition with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sequential deep learning for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baccouche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mamalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baskurt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HBU</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Human action recognition: Pose-based attention draws focus to hands</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Baradel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Pose-conditioned spatiotemporal attention for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Baradel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mille</surname></persName>
		</author>
		<idno>arxiv:1703.10106</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Pre-print</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hierarchical object detection with deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bellver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Giro-I Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Torres</surname></persName>
		</author>
		<idno>Decem- ber 2016. 3</idno>
	</analytic>
	<monogr>
		<title level="m">Deep Reinforcement Learning Workshop, NIPS</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Describing multimedia content using attention-based encoder-decoder networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1875" to="1886" />
		</imprint>
	</monogr>
	<note type="report_type">IEEE-T-Multimedia</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">On the properties of neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.05738</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Encoder-Decoder approaches. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Skeletal quads:human action recognition using joint quadruples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Evangelidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Horaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="4513" to="4518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">DRAW: A recurrent neural network for image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">3d pose from motion for cross-view action recognition via non-linear circulant temporal encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Jointly learning heterogeneous features for rgb-d activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5344" to="5352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Structural-RNN: Deep Learning on Spatio-Temporal Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">3d convolutional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="221" to="231" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Treestructured reinforcement learning for sequential object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A new representation of skeleton sequences for 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Boussaid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Structured attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Recurrent attentional networks for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3668" to="3677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Ask me anything: Dynamic memory networks for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>English</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pierce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning to combine foveal glimpses with a third-order Boltzmann machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1243" to="1251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Ensemble deep learning for skeleton-based action recognition using temporal sliding lstm networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Cross-view activity recognition using hankelets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Camps</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sznaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Discriminative virtual views for crossview action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zickler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Vide-oLSTM convolves, attends and flows for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gavrilyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Snoek</surname></persName>
		</author>
		<idno>2017. 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Spatio-Temporal LSTM with Trust Gates for 3D Human Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="816" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Global context-aware attention LSTM networks for 3D action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Enhanced skeleton visualization for view invariant human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="346" to="362" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">2d/3d pose estimation and action recognition using multitask deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luvizon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tabia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Reinforcement learning for visual object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mathe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pirinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Recurrent models of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2204" to="2212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Online detection and classification of dynamic hand gestures with recurrent 3d convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4207" to="4215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Moddrop: adaptive multi-modal gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nebout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1692" to="1706" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning a non-linear knowledge transfer model for cross-view action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">3d action recognition from novel viewpoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">NTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1010" to="1019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Deep multimodal feature analysis for action recognition in rgb+d videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<idno>arXiv</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Action recognition using visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">ICLR Workshop</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">An Endto-End Spatio-Temporal Attention Model for Human Action Recognition from Skeleton Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conf. on AI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Endto-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2440" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Lattice long short-term memory for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Lattice long short-term memory for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Learning video object segmentation with visual memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05737</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Human action recognition by representing 3d skeletons as points in a lie group</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Arrate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="588" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Cross-view action modeling, learning, and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xiaohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Song-Chun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Action Recognition Based on Joint Trajectory Maps with Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Deep dynamic neural networks for multimodal gesture segmentation and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pigou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-J</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dambre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Odobez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1583" to="1597" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Every moment counts: Dense detailed labeling of actions in complex videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.05738</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">End-toend Learning of Action Detection from Frame Glimpses in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">The moving pose: An efficient 3d kinematics descriptor for low-latency action recognition and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Leordeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2752" to="2759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">View adaptive recurrent neural networks for high performance human action recognition from skeleton data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Cross-view action recognition via a continuous virtual path</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
