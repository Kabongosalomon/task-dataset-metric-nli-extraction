<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CSPN++: Learning Context and Resource Aware Convolutional Spatial Propagation Networks for Depth Completion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinjing</forename><surname>Cheng</surname></persName>
							<email>chengxinjing@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Robotics and Auto-driving Lab (RAL)</orgName>
								<address>
									<country>Baidu Research</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
							<email>wangpeng54@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Robotics and Auto-driving Lab (RAL)</orgName>
								<address>
									<country>Baidu Research</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenye</forename><surname>Guan</surname></persName>
							<email>guanchenye@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Robotics and Auto-driving Lab (RAL)</orgName>
								<address>
									<country>Baidu Research</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
							<email>yangruigang@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Robotics and Auto-driving Lab (RAL)</orgName>
								<address>
									<country>Baidu Research</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CSPN++: Learning Context and Resource Aware Convolutional Spatial Propagation Networks for Depth Completion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Depth Completion deals with the problem of converting a sparse depth map to a dense one, given the corresponding color image. Convolutional spatial propagation network (CSPN) is one of the state-of-the-art (SoTA) methods of depth completion, which recovers structural details of the scene. In this paper, we propose CSPN++, which further improves its effectiveness and efficiency by learning adaptive convolutional kernel sizes and the number of iterations for the propagation, thus the context and computational resource needed at each pixel could be dynamically assigned upon requests. Specifically, we formulate the learning of the two hyper-parameters as an architecture selection problem where various configurations of kernel sizes and numbers of iterations are first defined, and then a set of soft weighting parameters are trained to either properly assemble or select from the pre-defined configurations at each pixel. In our experiments, we find weighted assembling can lead to significant accuracy improvements, which we referred to as "contextaware CSPN", while weighted selection, "resource-aware CSPN" can reduce the computational resource significantly with similar or better accuracy. Besides, the resource needed for CSPN++ can be adjusted w.r.t. the computational budget automatically. Finally, to avoid the side effects of noise or inaccurate sparse depths, we embed a gated network inside CSPN++, which further improves the performance. We demonstrate the effectiveness of CSPN++ on the KITTI depth completion benchmark, where it significantly improves over CSPN and other SoTA methods 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Image guided depth completion, or depth completion for short in this paper, is the task of converting a sparse depth map from devices such as LiDAR or algorithms such as structure-from-motion (SfM) and simultaneously localization and mapping (SLAM) to a per-pixel dense depth map with the help of reference images. The technique has a wide range of applications for the perception of indoor/outdoor moving robots such as self-driving vehicles, home/indoor robots, or applications such as augmented reality.</p><p>One of the state-of-the-art (SoTA) methods for this task is CSPN <ref type="bibr" target="#b2">(Cheng, Wang, and Yang 2018a)</ref>, which is an efficient local linear propagation model with learned affinity from a convolutional neural network (CNN). CSPN claims three important properties should be considered for the depth completion task, 1) depth preservation, where the depth value at sparse points should be maintained, 2) structure alignment, where the detailed structures, such as edges and object boundaries in estimated depth map, should be aligned with the given image, and 3) transition smoothness, where the depth transition between sparse points and their neighborhoods should be smooth.</p><p>In real applications, depths from devices like LiDAR, or algorithms such as SfM or SLAM could be noisy (Van Gansbeke et al. 2019) due to system or environmental errors. Datasets like KITTI adopt stereo and multiple frames to compensate the errors for evaluation. Here in this paper, we do not assume that the sparse depth map is the ground truth, rather, we consider that it may include errors as well. So the depth value at sparse points should be conditionally maintained with respect to its accuracy. Secondly, all pixels are considered equally in CSPN, while intuitively the pixels at geometrical edges and object boundaries should be more focused for structure alignment and transition smoothness. Therefore, in CSPN++, we propose to find a proper propa-gation context, to further improve the performance of depth completion.</p><p>To be specific, as illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, in CSPN++, numerous configurations of convolutional kernel size and number of iteration are first defined for each pixel x, then we utilize α x to weight different proposals of kernel size, and use λ x to weight outputs after different iterations. Based on these hyper-parameters, we induce context-aware and resource-aware variants for CSPN++. In context-aware CSPN (CA-CSPN), we propose to assemble the outputs, and CSPN++ is structurally similar to networks such as In-ceptionNet <ref type="bibr" target="#b20">(Szegedy et al. 2016)</ref> or DenseNet <ref type="bibr" target="#b5">(Huang et al. 2017a)</ref>, where gradient from the final output can be directly back-propagated to earlier propagation stages. We find the model learns stronger representation yielding significant performance boost comparing to CSPN.</p><p>In resource-aware CSPN (RA-CSPN), CSPN++ sequentially selects one convolutional kernel and one number of iteration for each pixel by minimizing the computational resource usage, where the learned computational resource allocation speeds up CSPN significantly (2ˆ"5ˆin our experiments) with improvements of accuracy. In addition, RA-CSPN can also be automatically adapted to a provided computational budget with the awareness of accuracy through a budget rounding operation during the training and inference.</p><p>In summary, our contribution lies in two aspects:</p><p>1. Base on the observation of error sparse depths, we propose a gate network to guide the depth preservation, and make the output more robust to noisy sparse depths.</p><p>2. We propose an effective method to adapt the kernel sizes and iteration number for each pixel with respect to image content for CSPN, which induces two variants, named as context-aware and resource-aware CSPN. The former significantly improves its performance, and the later speeds up the algorithm and makes the CSPN++ adapt to computational budgets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Depth estimation, completion, enhancement/refinement and models for dense prediction with dynamic context and compression have been center problems in computer vision and robotics for a long time. Here we summarize those works in several aspects without enumerating them all due to space limits, and we majorly clarify their core relationships with CSPN++ proposed in this paper. Depth Completion. The task of depth completion <ref type="bibr" target="#b22">(Uhrig et al. 2017)</ref> recently attracts lots of interests in robotics due to its wide application for enhancing 3D perception for robotics <ref type="bibr" target="#b12">(Liao et al. 2017)</ref>. The provided depths are usually from LiDAR, SfM or SLAM, yielding a map with valid depth partially available in some of the pixels. Within this field, some works directly convert sparse 3D points to dense ones without image guidance <ref type="bibr" target="#b22">(Uhrig et al. 2017)</ref>, which produce impressive results with deep learning. However, conventionally, jointly considering the structures from reference images for guiding depth completion/enhancement <ref type="bibr" target="#b13">(Liu and Gong 2013)</ref> yields better results. With the rising the deep learning for depth estimation from a single image , researchers adopt similar strategies to image guided depth completion. For example, <ref type="bibr" target="#b14">(Ma and Karaman 2018)</ref> propose to treat sparse depth map as an additional input to a ResNet based depth predictor <ref type="bibr" target="#b9">(Laina et al. 2016)</ref>, producing superior results than the depth output from CNN with solely image inputs. Later works are further proposed by focusing on improving the efficiency <ref type="bibr" target="#b8">(Ku, Harakeh, and Waslander 2018)</ref>, separately modeling the features from image and sparse depths <ref type="bibr" target="#b21">(Tang et al. 2019)</ref>, recovering the structural details of depth maps <ref type="bibr" target="#b2">(Cheng, Wang, and Yang 2018a)</ref>, combing with multi-level CRF <ref type="bibr" target="#b27">(Xu et al. 2018)</ref> or adopting auxiliary training losses using normal <ref type="bibr" target="#b29">(Zhang and Funkhouser 2018)</ref> or 3D representation  from self-supervised learning strategy <ref type="bibr" target="#b15">(Ma, Cavalheiro, and Karaman 2019)</ref>. Among all of these works, we treat CSPN <ref type="bibr" target="#b2">(Cheng, Wang, and Yang 2018a)</ref> as our baseline strategy due to its clear motivation and good theoretical guarantee in the stability of training and inference, and our resulted CSPN++ provides a significant boost both on its effectiveness and efficiency.</p><p>Context Aware Architectures. Assembling multiple contexts inside a network for dense predictions has been an effective component for recognition tasks in computer vision. In our perspective, the assembling strategies could be horizontal or vertical. Horizontal strategies assemble outputs from multiple branches in a single layer of a network, which include modules of Inception/Xception <ref type="bibr" target="#b20">(Szegedy et al. 2016)</ref>, pyramid spatial pooling (PSP) <ref type="bibr" target="#b30">(Zhao et al. 2016)</ref>, atrous spatial pyramid pooling (ASPP) <ref type="bibr" target="#b0">(Chen et al. 2017)</ref>, and vertical strategies assemble outputs from different layers include modules of HighwayNet <ref type="bibr" target="#b18">(Srivastava, Greff, and Schmidhuber 2015)</ref>, DenseNet <ref type="bibr" target="#b5">(Huang et al. 2017a</ref>), etc. Some recent works combine these two strategies together such as networks of HRNet <ref type="bibr" target="#b19">(Sun et al. 2019)</ref> or models of DenseASPP <ref type="bibr" target="#b28">(Yang et al. 2018)</ref>. Most recently, to make the context to be better conditioned on each pixel or provided image, attention mechanism with the cost of additional computation is further induced inside the network for context selection such as skipnet ), non-local networks  or context deformation such as spatial transformer networks <ref type="bibr" target="#b7">(Jaderberg et al. 2015)</ref> or deformable networks .</p><p>In the field of depth completion, <ref type="bibr" target="#b3">(Cheng, Wang, and Yang 2018b)</ref> propose the atrous convolutional spatial pyramid fusion (ACSF) module which extends ASPP by additionally adding affinity for each pixel, yielding stronger performance, which can be treated as a case of combining horizontal assembling with attention from affinity values. In our case, CA-CSPN of CSPN++ extends context assembling idea into CSPN with both horizontal and vertical strategies via attention. Horizontally, it assembles multiple kernel sizes, and vertically it assembles the outputs from different iteration stages as illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>. Here we would like to note that although mathematically in forward process, performing one step CA-CSPN with kernels of 7ˆ7, 5ˆ5, 3ˆ3 together is equivalent to performing CSPN with a single 7ˆ7 kernel since the full process are linear, the backward learn-Figure 2: Framework of our networks for depth completion with resource and context aware CSPN (best view in color). At the end of the network, we generate the depth confidence for each sparse point, affinity matrix for CSPN, and weighting variables α x and λ x for model assembling and selection.</p><p>ing process is different due to the auxiliary parameters (α x , λ x ), and our results are significantly better.</p><p>Resource Aware Inference. In addition, the dynamic context intuition can be also applied for efficient prediction by stopping the computation after obtained a proper context, which is also known as adaptive inference. Specifically, the relevant strategies have been adopted in image classification such as a multi-scale dense network (MSDNet) <ref type="bibr" target="#b7">(Huang et al. 2018)</ref>, object detection such as trade-off balancing <ref type="bibr" target="#b6">(Huang et al. 2017b)</ref> or semantic segmentation such as regional convolution network (RCN) treating each pixel differently <ref type="bibr" target="#b11">(Li et al. 2017)</ref>.</p><p>In RA-CSPN of CSPN++, we first embed such an idea in depth completion, and adopt functionality of RCN in CSPN for efficient inference. To minimize the computation, each pixel chooses one kernel size and then one number of iterations sequentially from the proposed configurations. Besides, we can easily add a provided computation budget, such as latency or memory constraints, into our optimization target, which could be back-propagated for operation selection similar to resource constraint architecture search algorithms <ref type="bibr" target="#b0">(Cai, Zhu, and Han 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preliminaries</head><p>To make the paper self-contained, we first briefly review CSPN <ref type="bibr" target="#b3">(Cheng, Wang, and Yang 2018b)</ref>, and then demonstrate how we extend it with context and resource awareness. Given one depth map D o P R mˆn that is output from a network taken input as an image I P R mˆn , CSPN updates the depth map to a new depth map D n . Without loss of generality, we follow their formulation by embedding depth to a hidden representation H P R mˆnˆc , and the updating equation for one step propagation can be written as,</p><formula xml:id="formula_0">H x,t`1 " φ CSP pH x,t , H x,0 |kq " κ x pxq d H x,0`ÿ xnPN k κ x px n q d H xn,t where, κ x px n q "κ x px n q{ ÿ xnPN |κ x px n q|, κ x pxq " 1´ÿ xnPN κ x px n q<label>(1)</label></formula><p>where φ CSP pq represents one step CSPN given a predefined size of convolutional kernel k. N k is the neighborhood pixels in a kˆk kernel, and the affinities output from a network κ x pq are properly normalized which guarantees the stability of the module. The whole process will iterate N times to obtain the final results. Here, k, N needs to be tuned in the experiments, which impacts the final performance significantly in their paper. For depth completion, CSPN preserves the depth value at those valid pixels in a sparse depth map D s by adding a replacement operation at the end of each step. Formally, let H s to be the corresponding embedding for D s , the replacement step after performing Eq. <ref type="formula" target="#formula_0">(1)</ref> is,</p><formula xml:id="formula_1">H x,t`1 " p1´m x qH x,t`1`mx H s x ,<label>(2)</label></formula><p>where m x " Ipd s x ą 0q is an indicator for the validity of sparse depth at x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context and Resource Aware CSPN</head><p>In this section, we elaborate how CSPN++ enhances CSPN by learning a proper configuration for each pixel by introducing additional parameters to predict. Specifically, predicting α x " tα x pkqu for weighting various convolutional kernel size and λ x " tλ x pk, tqu for weighting different number of iterations given a kernel size k. As shown in <ref type="figure">Fig.  2</ref>, both variables are image content dependent, and are predicted from a shared backbone with CSPN affinity and estimated depths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context-Aware CSPN</head><p>Given the provided α x and λ x , context-aware CSPN (CA-CSPN) first assembles the results from different steps. Formally, the propagation from t to t`1 could be written as,</p><formula xml:id="formula_2">Hx ,t`1,k " λ x pk, t`1q˚φ CSP pH t , H 0 |x, kq`Hx ,t,k λ x pk, tq " σpλ x pk, tqq{ ÿ tPt1¨N u σpλ x pk, tqq<label>(3)</label></formula><p>where, σpq is the sigmoid function, andλ x is the outputs from the network. In the process, Hx ,t`1,k progressively aggregates the output from each step of CSPN based on λ x . Finally, we assemble different outputs from various kernels after N iterations,</p><formula xml:id="formula_3">Hx ,N " ÿ kPK α x pkqHx ,N,k α x pkq " σpα x pkqq{ ÿ kPK σpα x pkqq (4)</formula><p>Here, both α x and λ x are properly normalized with their l 1 norm, so that our output Hx ,N maintains the stabilization property of CSPN for training and inference. When there are sparse points available, CSPN++ adopts a confidence variable g x predicted at each valid depth in the sparse depth map, which is output from the shared backbone in our framework <ref type="figure">(Fig. 2)</ref>. Therefore, the replacement step for CSPN++ can be modified accordingly,</p><formula xml:id="formula_4">Hx ,t`1 " p1´g x qHx ,t`1`g x H s x ,<label>(5)</label></formula><p>where g x " Ipd s x ą 0qσpĝ x q, whereĝ x is predicted from a network after a convolutional layer.</p><p>Complexity and computational resource analysis. From CSPN, we know that theoretically with sufficient amount of GPU cores and large memory storage, the overall complexity for CSPN with a kernel size of k and iteration N is Oplogpk 2 qN q. In CA-CSPN, with induced K convolutional kernels, the computation complexity is Oplogpk 2 max qN q, where k max is the maximum kernel size since all branch can be performed simultaneously.</p><p>However, in the real application, the expected computational resource is limited and latency of memory request with large convolutional kernel could be time consuming. Therefore, we need to utilize a better metric for estimating the cost. Here, we adopt the popularly used memory cost and Mult-Adds/FLOPs as an indicator of latency or computational resource usage on a device. Specifically, based on the CUDA implementation of convolution with im2col, performing CSPN with a kernel k would require memory cost of Opk 2 q, and FLOPs of OpN k 2 q, given a single feature block with a size of hˆwˆc. In summary, given K kernels, the latency from big O estimation for CA-CSPN would be OpN k 2 max q. Finally, we would like to note that the memory and computational configuration varies with given devices, so does the latency estimation. A better strategy would be directly testing over the target device as proposed in <ref type="bibr" target="#b0">(Cai, Zhu, and Han 2019)</ref>. Here, we just provide a reasonable estimation with the commonly used GPU. Network architectures. As illustrated in <ref type="figure">Fig. 2</ref>, for the backbone network, we adopt the same ResNet-34 structure proposed in <ref type="bibr" target="#b15">(Ma, Cavalheiro, and Karaman 2019)</ref>. The only modification is at the end of the network, it outputs the per-pixel estimation of assembling parameters λ x , α x , noisy guidance for replacement g x and affinity matrix κ x using a convolutional layer with a 3ˆ3 kernel. For handling the affinity values for various propagation kernels, we use a shared affinity matrix since the affinity between different pixels should be irrelevant to the context of propagation, which saves the memory cost inside the network. Training context-aware CSPN. Given the proposed architecture, based on our computational resource analysis w.r.t. latency, we add additional regularization term inside the general optimization target, which minimizes the expected  <ref type="figure">Figure 3</ref>: The proposed regional im2col and conv. operation for efficient testing. Here, let the regions of green ( 1 ), red ( 2 ) and blue ( 3 ) have kernel size of 3, 7, 5 and iteration number of t, t+1, t+1 respectively. We convert each region to a matrix of |f i |ˆ|R i | for performing parallel conv. through im2col, where |f i | " k iˆkiˆc is the feature dimension, and |R i | is the number of pixels in the corresponding region. If pixels belong to a region does not need propagation (i.e. region 1 at time step t as illustrated), we direct copy its feature to next step.</p><p>computational cost c by treating α x , λ x as probabilities of configuration selection. It is shown to be effective in improving the final performance in our experiments. Formally, the overall target for training CA-CSPN can be written as,</p><formula xml:id="formula_5">min w L train pD, D˚|wq`η 1 }w} 2 2`η2 Epc|tα x , λ x uq L train pD, D˚|wq " }D´D˚} 2 2 Epc|tα x , λ x uq " 1 hw ÿ x Epc x |α x , λ x q (6) Epc x |α x , λ x q " 1 pN k 2 max q ÿ k,t λ x pk, tqα x pkqtk 2</formula><p>where w is the network parameters, and }w} 2 2 is weight decay regularization. Epc|¨q is the expected computational cost given the assembling variables based on our analysis. h, w are height and width of the feature respectively. D and D˚is the output depth map from CA-CSPN and ground truth depth map correspondingly. Here, our system can be trained end-to-end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Resource Aware Configuration</head><p>As introduced in our complexity analysis, CSPN with large kernel size and long time propagation is time consuming. Therefore, to accelerate it, we further propose resourceaware CSPN (RA-CSPN), which selects the best kernel size and number of iteration for each pixel based on the estimated α x , λ x . Formally, its propagation step can be written as,</p><formula xml:id="formula_6">H x,t`1 " φ CSP pH t , H 0 |x, k˚q</formula><p>where k˚" arg max k α x pkq, t ď arg max t λ x pk, tq <ref type="formula">(7)</ref> Here each pixel is treated differently by selecting a best learned configuration, and we follow the same process of replacement as Eq. (2) for handling depth completion.</p><p>Computational resource analysis. Given the selected configuration of convolutional kernel and number of iteration at each pixel, the latency estimation for each image that we proposed in Sec. is changed to Opk 2t q, wherê k " 1 hw ř</p><p>x kx andt " 1 hw ř x tx are the average iteration step and kernel size in the image respectively. Both of the numbers are guaranteed to be smaller than the maximum number of iteration N and kernel size k max .</p><p>Training RA-CSPN. In our case, training RA-CSPN does not need to modify the multi-branch architecture shown in <ref type="figure" target="#fig_0">Fig. 1, but</ref> switches from the weighted average assembling as described in Eq. (3) and Eq. (4) to max selection that only one path is adopted for each pixel. In addition, we need modify our loss function in Eq. (6) by changing the expected computational cost as,</p><formula xml:id="formula_7">Epc x |α x , λ x q " pkxq 2 tx{pN k 2 max q where k˚" arg max k α x pkq, t˚" arg max t λ x pk˚, tq (8)</formula><p>In practice, to implement configuration selection, we can reuse the same training pipeline as CA-CSPN via converting the obtained soft weighting values in α x and λ x to one-hot representation through an argmax operation.</p><p>Efficient testing. Practically, there are two issues we need to handle when making the algorithm efficient at testing: 1) how to perform different convolution simultaneously at different pixels, and 2) how to continue the propagation for pixels whose neighborhood pixels stop their diffusion/propagation process. To handle these issues, we follow the idea of regional convolution <ref type="bibr" target="#b11">(Li et al. 2017)</ref>.</p><p>Specifically, as shown in <ref type="figure">Fig. 3</ref>, to tackle the first one, we group pixels to multiple regions based on our predicted kernel size, and prepare corresponding matrix before convolution for each group using region-wise im2col. Then, the generated matrix can be processed simultaneously at each pixel using region-wise convolution. To tackle the second issue, when the propagation of one pixel x stops at time step t, we directly copy the feature of x to the next step t`1 for computing convolution at later stages. In summary, RA-CSPN can be performed in a single forward pass with less resource usage.</p><p>Learning with provided computational budget. Finally, in real applications, rather than providing an optimal computational resource, usually there is a hard constraint for a deployed model, either the memory or latency of inference. Thanks to the adaptive resource usage of CSPN++, we are able to directly put the required budget into our optimization target during training. Formally, given a target memory cost C m and a latency cost C l for resource-aware CSPN, our optimization target in Eq. (6) could be modified as, min</p><formula xml:id="formula_8">w L train pD, D˚|wq`η 1 }w} 2 2`η2 Epc|tα x , λ x uq s.t. Epcm|tα x , λ x uq ď C m , Epc|tα x , λ x uq ď C l (9)</formula><p>where Epcm|t¨,¨uq "</p><formula xml:id="formula_9">1 hwk 2 max ř</formula><p>x pkxq 2 is the expected memory cost, and Epc|t¨,¨uq is the expected latency cost defined in Eq. (8). The two constraints can be added to our target easily with Lagrange multiplier. Formally, our optimization target with resource budges is, min w L train pD, D˚|wq`η 1 }w} 2 2`( 10) η 1 2 rEpc|t¨,¨u´C l s``η 3 rEpcm|t¨,¨uq´C m sẁ here the hinge loss rxs`" maxpx, 0q is adopted as our surrogate function for satisfying the constraints.</p><p>Last but not the least, since our primal problem, i.e. optimization with deep neural network, is highly non-convex, thus during training, there is no guarantee that all samples will satisfy the constraints. In addition, during testing, the predicted configuration might also violate the given constraints, e.g. Epc|t¨,¨uq´C l ą 0. Therefore, for these cases, we propose a resource rounding strategy to hard constraint its overall computation within the budgets. Specifically, we calculate the average cost at each pixel, and for the pixels violating the cost, as illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, we are are able to find the Pareto optimal frontier (Mock 2011) that satisfying the constraint, and we pick the one with largest iteration since it obtains the largest reception field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>In this section, we will first introduce the dataset, metrics and our implementation details. Then, extensive ablation study of CSPN++ is conducted on the validation set to verify our insight of each proposed components. Finally, we provide qualitative comparison of CSPN++ versus other SoTA method on testing set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental setup</head><p>KITTI Depth Completion dataset. The KITTI Depth Completion benchmark is a large self-driving real-world dataset with street views from a driving vehicle. It consists 86k training, 7k validation and 1k testing depth maps with corresponding raw LiDAR scans and reference images. We use the official 1k validation images in as our validation set while merge the remained images to training set. The sparse depth maps are obtained by projecting the raw Li-DAR points through the view of camera, and the ground truth dense depth maps are generated by first projecting the accumulated LiDAR scans of multiple timestamps, and then removing outliers depths from occlusion and moving objects through comparing with stereo depths from image pairs. NYU v2 dataset. The NYU-Depth-v2 dataset consists of RGB and depth images collected from 464 different indoor scenes. We use the official split of data, where 249 scenes are used for training and we sample 50K images out of the training set with the same manner as <ref type="bibr" target="#b2">(Cheng, Wang, and Yang 2018a)</ref>. For testing, following the standard setting <ref type="bibr" target="#b2">(Cheng, Wang, and Yang 2018a)</ref>, the small labeled test set with 654 images is used the final performance. The original image of size 640ˆ480 are first downsampled to half and then centercropped, producing a network input size of 304ˆ228. Metrics. We adopt error metrics same as KITTI depth completion benchmark, including root mean square error (RMSE), mean abosolute error (MAE), inverse RMSE (iRMSE) and inverse MAE (iMAE), where inverse indicates inverse depth representation, i.e.converting d x to 1.0{d x .  Implementation details. For kitti dataset, we train our network with four NVIDIA Tesla P40, and use batchsize of 8. In all our experiments, we adopt kernel sizes of 3ˆ3, 5ˆ5 and 7ˆ7, and sample outputs after 3, 6, 9, 12 times of propagation. All our models are trained with Adam optimizer with β 1 " 0.9, β 2 " 0.999. The learning rate start from 10´5 and reduce by half for every 5 epochs. Here, for training context-aware CSPN in Eq. (6), the parameter for weight decay, i.e. η 1 , is set to 0.0005, and the parameter for resource regularization, i.e. η 2 is set to 0.1. For training resourceaware CSPN in Eq. (8), we set η 1 2 " 1.0 and η 3 " 1.0. All our parameters are induced for balancing value scale of different losses without exhaustively tuning. While training on NYU dataset, we keep the same configuration with CSPN <ref type="bibr" target="#b2">(Cheng, Wang, and Yang 2018a)</ref>, and adopt same kernel and iteration configuration with kitti dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation studies</head><p>Ablation study of context-aware CSPN (CA-CSPN).</p><p>Here, we conduct experiments to verify each module adopted in our framework, including our baselines, i.e. CSPN with spatial pyramid pooling(SPP), and our newly proposed modules in context-aware CSPN. Specifically, to make the validation efficient, we only train each network 10 epochs to obtain its results. For SPP, we adopt pooling sizes of 12, 6, 4, 2 and for CSPN, we use the kernel size of 7ˆ7 and set the number of iteration as 12. As shown in Tab 1, by adding SPP and CSPN module to the baseline from <ref type="bibr" target="#b15">(Ma, Cavalheiro, and Karaman 2019)</ref>, we can significantly reduce the depth error due to the induced pyramid context in SPP and refined structure with CSPN. With additional confidence guided replacement(GR) (Eq. (5)), our module better handles the noisy sparse depths, and the RMSE is signifi-cantly reduced from 765.78 to 756.27. Then, at rows with 'assemble kernel', we add the component of learning to horizontally assemble predictions from different kernel size via the learned α x . It further reduce the error from 756.27 to 732.46. At rows with 'assemble iter.', we include the component of learning to vertically assemble outputs after different iterations via the learned λ x . Finally, at rows with 'LR', we add our proposed latency regularization term (Eq. (6)) into the training losses, yielding the best results of our contextaware CSPN.</p><p>In <ref type="figure" target="#fig_2">Fig. 4</ref>, we visualize the learned configurations of α x and λ x at each pixel. Typically, we find majority pixels on ground and walls only need small kernel and few iterations for recovery, while pixels further away and around object and surface boundary need large kernels and more iterations to obtain larger context for reconstruction. This agrees with our intuition since in real cases, sparse points are denser close by and the structure is simpler in planar regions, thus it is easier for depth estimation. Ablation study of resource-aware CSPN (RA-CSPN). To verify the efficiency of our proposed RA-CSPN, we study the computational improvement w.r.t. vanilla CSPN and CA-CSPN. As list in Tab 2, at row 'CSPN', we list its memory cost and latency on device. At row 'CA-CSPN', although the memory cost and latency are in practice larger, but the expected kernel size Epkq and iteration steps Eptq are much smaller using our latency regularization terms. This indicates that most pixels only need small kernel and few iteration for obtaining better results. At row of 'RA-CSPN', we train with resource-aware objective as in Eq. (8), and show that RA-CSPN not only outperforms CSPN for efficiency (almost 3ˆfaster), but also improves RMSE from 756.27 to 732.32. More importantly, we can train RA-CSPN with 2: Comparison of efficiency between CSPN and CSPN++. Epkq is the expected kernel size and Eptq is the expected number of iterations using learned α x and λ x . Cm is the real cost of memory and Cl is the real time latency on device. m.c. is short for memory constraints and l.c. is short for latency constraints. Both constraints and expected values are normalized by the corresponding resource used in the CSPN baseline. Note here the number of memory cost is not proportion to Epkq since the majority is taken by affinity matrix in our case. Here, we set a minimum cost of using kernel size of 3ˆ3 and propagation steps of 3, and one may achieve additional acceleration by dropping the minimum cost. computational budget to fit different devices as proposed in Eq. (10). At the last row, with a hard constrain that the m.c. and l.c. is less than 35% of the vanilla CSPN, we found that, our method will adjust kernel sizes and iteration actively. In this case, the Eptq reduce from 0.439 to 0.303 but Epkq increase from 0.268 to 0.333, which means that the network chooses larger kernel sizes with less iteration automatically to satisfied our hard constraints, while still produces better results and demonstrate the effectiveness of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments on NYU v2 dataset</head><p>To verify the generalization capability of our method in indoor scenes, we adopt same experiments on NYU v2 dataset. As shown in Tab. 2, we draw similar conclusions with the KITTI dataset, which shows the effectiveness of our method again.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparisons against other methods</head><p>Finally, to compare against other SoTA methods for depth estimation accuracy, we use our best obtained model from CA-CSPN, and finetune it with another 30 epochs before submitting the results to KITTI test server. As summarized in Tab. 3, CA-CSPN outperforms all other methods signifi-cantly and currently rank 2nd on the bench mark. However, our results are better in three out of the four metrics. Here, we would like to note that our results are also better than methods adopted additional dataset, e.g. DeepLiDAR <ref type="bibr" target="#b17">(Qiu et al. 2019</ref>) uses CARLA <ref type="bibr" target="#b4">(Dosovitskiy et al. 2017</ref>) to better learn dense depth and surface normal tasks jointly, and FusionNet (Van Gansbeke et al. 2019) used semantic pretrained segmentation models on CityScape <ref type="bibr" target="#b3">(Cordts et al. 2016</ref>). Our plain model only trained on KITTI dataset and outperforms all other methods. In <ref type="figure">Fig. 5</ref>, we qualitatively compare the dense depth maps estimated from our proposed mehtod with UberATG-FuseNet ) together with the corresponding error maps. We found our results are better at detailed scene structure recovery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we propose CSPN++ for depth completion, which outperforms previous SoTA strategy CSPN <ref type="bibr" target="#b3">(Cheng, Wang, and Yang 2018b</ref>) by a large margin. Specifically, we elaborate two variants using the same framework of model selection, i.e. context-aware CSPN and resourceaware CSPN. The former significantly reduces estimation error, while the later achieves much better efficiency with comparable accuracy with the former. We hope CSPN++ could motivate researchers to better adopt data-driven strategies for effective learning hyper-parameters in various tasks.</p><p>In the future, we would like merge the two variants, and consider replacing more modules in network with CSPN for multiple tasks such as segmentation and detection.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Output assembling or selection over an unrolled CSPN. The color of each dot indicates the computational resources need at the point, where blue indicates low resource usage while red indicates high resource usage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Framework of our networks for depth completion with resource and context aware CSPN(best view in color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Ablation Study on KITTI Depth Completion validation dataset. 'GR' stands for guided replacement.'LR' stands for latency regularization for the model. 'CPSN++' is our proposed strategies.</figDesc><table><row><cell>Method</cell><cell>SPP</cell><cell>CSPN configuration Normal assemble kernel assemble iter.</cell><cell>GR LR</cell><cell cols="2">Results (Lower the better) RMSE(mm) MAE(mm)</cell></row><row><cell>(Ma, Cavalheiro, and Karaman 2019)</cell><cell></cell><cell></cell><cell></cell><cell>799.08</cell><cell>265.98</cell></row><row><cell>(Ma, Cavalheiro, and Karaman 2019)</cell><cell></cell><cell></cell><cell></cell><cell>788.23</cell><cell>247.55</cell></row><row><cell>CSPN</cell><cell></cell><cell></cell><cell></cell><cell>765.78</cell><cell>213.,54</cell></row><row><cell>CSPN</cell><cell></cell><cell></cell><cell></cell><cell>756.27</cell><cell>215.21</cell></row><row><cell>CA-CSPN</cell><cell></cell><cell></cell><cell></cell><cell>732.46</cell><cell>210.61</cell></row><row><cell>CA-CSPN</cell><cell></cell><cell></cell><cell></cell><cell>732.34</cell><cell>209.20</cell></row><row><cell>CA-CSPN</cell><cell></cell><cell></cell><cell></cell><cell>725.43</cell><cell>207.88</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Qualitative comparison with UberATG-FuseNet on KITTI test set, where the zoom regions show that our method recover better and detailed structure.</figDesc><table><row><cell>DataSet</cell><cell>Method</cell><cell>kernel</cell><cell>iter.</cell><cell>m. c. l. c.</cell><cell>Epkq</cell><cell cols="3">Lower the Better Eptq Cm(MB) Cl(ms) RMSE(mm)</cell></row><row><cell></cell><cell>CSPN</cell><cell>7x7</cell><cell>12</cell><cell></cell><cell>1.0</cell><cell>1.0</cell><cell>829</cell><cell>28.88</cell><cell>756.27</cell></row><row><cell></cell><cell cols="2">CA-CSPN assemble</cell><cell>12</cell><cell></cell><cell>0.680</cell><cell>1.0</cell><cell>2125</cell><cell>67.23</cell><cell>732.46</cell></row><row><cell>KITTI</cell><cell cols="3">CA-CSPN assemble assemble</cell><cell></cell><cell cols="2">0.316 0.446</cell><cell>2125</cell><cell>67.23</cell><cell>725.43</cell></row><row><cell></cell><cell>RA-CSPN</cell><cell>select</cell><cell>select</cell><cell></cell><cell cols="2">0.268 0.439</cell><cell>626.29</cell><cell>10.03</cell><cell>732.32</cell></row><row><cell></cell><cell>RA-CSPN</cell><cell>select</cell><cell>select</cell><cell cols="3">0.35 0.35 0.333 0.303</cell><cell>625.30</cell><cell>9.84</cell><cell>742.17</cell></row><row><cell></cell><cell>CSPN</cell><cell>7x7</cell><cell>12</cell><cell></cell><cell>1.0</cell><cell>1.0</cell><cell>628</cell><cell>21.03</cell><cell>121.49</cell></row><row><cell>NYU v2</cell><cell cols="3">CA-CSPN assemble assemble</cell><cell></cell><cell cols="2">0.373 0.451</cell><cell>1691</cell><cell>50.47</cell><cell>115.73</cell></row><row><cell></cell><cell>RA-CSPN</cell><cell>select</cell><cell>select</cell><cell cols="3">0.40 0.40 0.386 0.395</cell><cell>531.27</cell><cell>10.03</cell><cell>118.70</cell></row><row><cell>Figure 5:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparisons against state-of-the-art methods on KITTI Depth Completion benchmark.</figDesc><table><row><cell>Method</cell><cell>iRMSE (1/km)</cell><cell>iMAE (1/km)</cell><cell>RMSE (mm)</cell><cell>MAE (mm)</cell></row><row><cell>SC (Uhrig et al. 2017)</cell><cell>4.94</cell><cell>1.78</cell><cell>1601.33</cell><cell>481.27</cell></row><row><cell>CSPN (Cheng, Wang, and Yang 2018a)</cell><cell>2.93</cell><cell>1.15</cell><cell>1019.64</cell><cell>279.46</cell></row><row><cell>NC (Eldesokey, Felsberg, and Khan 2019)</cell><cell>2.60</cell><cell>1.03</cell><cell>829.98</cell><cell>233.26</cell></row><row><cell>StD (Ma, Cavalheiro, and Karaman 2019)</cell><cell>2.80</cell><cell>1.21</cell><cell>814.73</cell><cell>249.95</cell></row><row><cell>FN (Van Gansbeke et al. 2019)</cell><cell>2.19</cell><cell>0.93</cell><cell>772.87</cell><cell>215.02</cell></row><row><cell>DL (Qiu et al. 2019)</cell><cell>2.56</cell><cell>1.15</cell><cell>758.38</cell><cell>226.25</cell></row><row><cell>Uber (Chen et al. 2019)</cell><cell>2.34</cell><cell>1.14</cell><cell>752.88</cell><cell>221.19</cell></row><row><cell>CA-CSPN</cell><cell>2.07</cell><cell>0.90</cell><cell>743.69</cell><cell>209.28</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Proxylessnas: Direct neural architecture search on target task and hardware. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han ;</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
		<ptr target="CoRRabs/1706.05587" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Rethinking atrous convolution for semantic image segmentation</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<title level="m">Learning joint 2d-3d representations for depth completion. ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Depth estimation via affinity learned with convolutional spatial propagation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang ;</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="103" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning depth with convolutional spatial propagation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang ;</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1810.02695</idno>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>The cityscapes dataset for semantic urban scene understanding</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Confidence propagation through cnns for guided sparse depth regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dosovitskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.03938</idno>
	</analytic>
	<monogr>
		<title level="m">Carla: An open urban driving simulator</title>
		<imprint>
			<publisher>TPAMI</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Speed/accuracy trade-offs for modern convolutional object detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7310" to="7311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-scale dense networks for resource efficient image classification. arxiv preprint arxiv: 170309844. ICLR</title>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
	<note>Spatial transformer networks</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">In defense of classical image processing: Fast depth completion on the cpu</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harakeh</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CRV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="16" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deeper depth prediction with fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Laina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<title level="m">Fourth International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="239" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Not all pixels are equal: Difficulty-aware semantic segmentation via deep layer cascade</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3193" to="3202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Parse geometry from a line: Monocular depth estimation with partial laser observation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kodagoda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ICRA</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Guided depth enhancement via anisotropic diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific-Rim Conference on Multimedia</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="408" to="417" />
		</imprint>
	</monogr>
	<note>Liu and Gong</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Sparseto-dense: Depth prediction from sparse depth samples and a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karaman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Ma and Karaman. ICRA</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Self-supervised sparse-to-dense: Selfsupervised depth completion from lidar and monocular camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cavalheiro</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">V</forename><surname>Cavalheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karaman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3288" to="3295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Mock</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="808" to="809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deeplidar: Deep surface normal guided depth prediction for outdoor scene from sparse lidar data and single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3313" to="3322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greff</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00387</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">Highway networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
	<note>Sun et al. 2019</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning guided convolutional network for depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.01238</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sparsity invariant cnns. 3DV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gansbeke</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.05356</idno>
		<title level="m">Sparse and noisy lidar completion with rgb guidance and uncertainty</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Surge: Surface regularized geometry estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="172" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Skipnet: Learning dynamic routing in convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="409" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Structured attention guided convolutional neural fields for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3917" to="3925" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Denseaspp for semantic segmentation in street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3684" to="3692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep depth completion of a single rgb-d image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="175" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deformable convnets v2: More deformable, better results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="9308" to="9316" />
		</imprint>
	</monogr>
	<note>Pyramid scene parsing network</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
