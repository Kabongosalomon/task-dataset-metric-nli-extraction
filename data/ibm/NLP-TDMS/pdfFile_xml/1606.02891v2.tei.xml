<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Edinburgh Neural Machine Translation Systems for WMT 16</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2016-06-27">27 Jun 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
							<email>bhaddow@inf.ed.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
							<email>a.birch@ed.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Edinburgh Neural Machine Translation Systems for WMT 16</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2016-06-27">27 Jun 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We participated in the WMT 2016 shared news translation task by building neural translation systems for four language pairs, each trained in both directions:</p><p>English↔Czech, English↔German, English↔Romanian and English↔Russian. Our systems are based on an attentional encoder-decoder, using BPE subword segmentation for open-vocabulary translation with a fixed vocabulary. We experimented with using automatic back-translations of the monolingual News corpus as additional training data, pervasive dropout, and target-bidirectional models. All reported methods give substantial improvements, and we see improvements of 4.3-11.2 BLEU over our baseline systems. In the human evaluation, our systems were the (tied) best constrained system for 7 out of 8 translation directions in which we participated. 1 2</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We participated in the WMT 2016 shared news translation task by building neural translation systems for four language pairs: English↔Czech, English↔German, English↔Romanian and English↔Russian.</p><p>Our systems are based on an attentional encoder-decoder <ref type="bibr" target="#b0">(Bahdanau et al., 2015)</ref>, using BPE subword segmentation for open-vocabulary translation with a fixed vocabulary <ref type="bibr" target="#b9">(Sennrich et al., 2016b)</ref>. We experimented with using automatic backtranslations of the monolingual News corpus as <ref type="bibr">1</ref> We have released the implementation that we used for the experiments as an open source toolkit: https://github.com/rsennrich/nematus 2 We have released scripts, sample configs, synthetic training data and trained models: https://github.com/rsennrich/wmt16-scripts additional training data <ref type="bibr" target="#b8">(Sennrich et al., 2016a)</ref>, pervasive dropout <ref type="bibr" target="#b3">(Gal, 2015)</ref>, and targetbidirectional models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Baseline System</head><p>Our systems are attentional encoder-decoder networks <ref type="bibr" target="#b0">(Bahdanau et al., 2015)</ref>. We base our implementation on the dl4mt-tutorial 3 , which we enhanced with new features such as ensemble decoding and pervasive dropout.</p><p>We use minibatches of size 80, a maximum sentence length of 50, word embeddings of size 500, and hidden layers of size 1024. We clip the gradient norm to 1.0 <ref type="bibr" target="#b6">(Pascanu et al., 2013)</ref>. We train the models with Adadelta (Zeiler, 2012), reshuffling the training corpus between epochs. We validate the model every 10 000 minibatches via BLEU on a validation set (newstest2013, new-stest2014, or half of newsdev2016 for EN↔RO). We perform early stopping for single models, and use the 4 last saved models (with models saved every 30 000 minibatches) for the ensemble results. Note that ensemble scores are the result of a single training run. Due to resource limitations, we did not train ensemble components independently, which could result in more diverse models and better ensembles.</p><p>Decoding is performed with beam search with a beam size of 12. For some language pairs, we used the AmuNMT C++ decoder 4 as a more efficient alternative to the theano implementation of the dl4mt tutorial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Byte-pair encoding (BPE)</head><p>To enable open-vocabulary translation, we segment words via byte-pair encoding (BPE) 5 <ref type="bibr" target="#b9">(Sennrich et al., 2016b)</ref>. BPE, originally devised as a compression algorithm <ref type="bibr" target="#b2">(Gage, 1994)</ref>, is adapted to word segmentation as follows:</p><p>First, each word in the training vocabulary is represented as a sequence of characters, plus an end-of-word symbol. All characters are added to the symbol vocabulary. Then, the most frequent symbol pair is identified, and all its occurrences are merged, producing a new symbol that is added to the vocabulary. The previous step is repeated until a set number of merge operations have been learned.</p><p>BPE starts from a character-level segmentation, but as we increase the number of merge operations, it becomes more and more different from a pure character-level model in that frequent character sequences, and even full words, are encoded as a single symbol. This allows for a trade-off between the size of the model vocabulary and the length of training sequences. The ordered list of merge operations, learned on the training set, can be applied to any text to segment words into subword units that are in-vocabulary in respect to the training set (except for unseen characters).</p><p>To increase consistency in the segmentation of the source and target text, we combine the source and target side of the training set for learning BPE. For each language pair, we learn 89 500 merge operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Features</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Synthetic Training Data</head><p>WMT provides task participants with large amounts of monolingual data, both in-domain and out-of-domain.</p><p>We exploit this monolingual data for training as described in <ref type="bibr" target="#b8">(Sennrich et al., 2016a)</ref>. Specifically, we sample a subset of the available target-side monolingual corpora, translate it automatically into the source side of the respective language pair, and then use this synthetic parallel data for training. For example, for EN→RO, the back-translation is performed with a RO→EN system, and vice-versa. <ref type="bibr" target="#b8">Sennrich et al. (2016a)</ref> motivate the use of monolingual data with domain adaptation, reducing overfitting, and better modelling of fluency. We sample monolingual data from the News Crawl corpora 6 , which is in-domain with respect to the test set. The amount of monolingual data backtranslated for each translation direction ranges from 2 million to 10 million sentences. Statistics about the amount of parallel and synthetic training data are shown in <ref type="table" target="#tab_0">Table 1</ref>. With dl4mt, we observed a translation speed of about 200 000 sentences per day (on a single Titan X GPU).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Pervasive Dropout</head><p>For English↔Romanian, we observed poor performance because of overfitting. To mitigate this, we apply dropout to all layers in the network, including recurrent ones.</p><p>Previous work dropped out different units at each time step. When applied to recurrent connections, this has the downside that it impedes the information flow over long distances, and <ref type="bibr" target="#b7">Pham et al. (2014)</ref> propose to only apply dropout to non-recurrent connections.</p><p>Instead, we follow the approach suggested by <ref type="bibr" target="#b3">Gal (2015)</ref>, and use the same dropout mask at each time step. Our implementation differs from the recommendations by <ref type="bibr" target="#b3">Gal (2015)</ref> in one respect: we also drop words at random, but we do so on a token level, not on a type level. In other words, if a word occurs multiple times in a sentence, we may drop out any number of its occurrences, and not just none or all.</p><p>In our English↔Romanian experiments, we drop out full words (both on the source and target side) with a probability of 0.1. For all other layers, the dropout probability is set to 0.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Target-bidirectional Translation</head><p>We found that during decoding, the model would occasionally assign a high probability to words based on the target context alone, ignoring the source sentence. We speculate that <ref type="bibr" target="#b8">(Sennrich et al., 2016a)</ref>, which was randomly sampled from News Crawl 2007-2014.  this is an instance of the label bias problem <ref type="bibr">(Lafferty et al., 2001)</ref>.</p><p>To mitigate this problem, we experiment with training separate models that produce the target text from right-to-left (r2l), and re-scoring the nbest lists that are produced by the main (left-toright) models with these r2l models. Since the right-to-left model will see a complementary target context at each time step, we expect that the averaged probabilities will be more robust. In parallel to our experiments, this idea was published by <ref type="bibr" target="#b5">Liu et al. (2016)</ref>.</p><p>We increase the size of the n-best-list to 50 for the reranking experiments.</p><p>A possible criticism of the l-r/r-l reranking approach is that the gains actually come from adding diversity to the ensemble, since we are now using two independent runs. However experiments in <ref type="bibr" target="#b5">(Liu et al., 2016)</ref> show that a l-r/r-l reranking systems is stronger than an ensemble created from two independent l-r runs. <ref type="table" target="#tab_2">Table 2</ref> shows results for English↔German. We observe improvements of 3.4-5.7 BLEU from training with a mix of parallel and synthetic data, compared to the baseline that is only trained on parallel data. Using an ensemble of the last 4 checkpoints gives further improvements (1.3-1.7 BLEU). Our submitted system includes reranking of the 50-best output of the left-to-right model with a right-to-left model -again an ensemble of the last 4 checkpoints -with uniform weights. This yields an improvements of 0.6-1.1 BLEU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">English↔German</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">English↔Czech</head><p>For English→Czech, we trained our baseline model on the complete WMT16 parallel training set (including CzEng 1.6pre <ref type="bibr" target="#b1">(Bojar et al., 2016)</ref>), until we observed convergence on our heldout set (newstest2014). This took approximately 1M minibatches, or 3 weeks. Then we continued training the model on a new parallel corpus, comprising 8.2M sentences back-translated from the Czech monolingual news2015, 5 copies of news-commentary v11, and 9M sentences sampled from Czeng 1.6pre. The model used for backtranslation was a neural MT model from earlier experiments, trained on WMT15 data. The training on this synthetic mix continued for a further 400,000 minibatches.</p><p>The right-left model was trained using a similar process, but with the target side of the parallel corpus reversed prior to training. The resulting model had a slightly lower BLEU score on the dev data than the standard left-right model. We can see in <ref type="table" target="#tab_4">Table 3</ref> that back-translation improves performance by 2.2-2.8 BLEU, and that the final system (+r2l reranking) improves by 0.7-1.0 BLEU on the ensemble of 4, and 4.3-4.9 on the baseline.</p><p>For Czech→English the training process was similar to the above, except that we created the synthetic training data (back-translated from samples of news2015 monolingual English) in batches of 2.5M, and so were able to observe the effect of increasing the amount of synthetic data. After training a baseline model on all the WMT16 parallel set, we continued training with a parallel corpus consisting of 2 copies of the 2.5M sentences of back-translated data, 5 copies of newscommentary v11, and a matching quantity of data sampled from Czeng 1.6pre. After training this to convergence, we restarted training from the baseline model using 5M sentences of back-translated data, 5 copies of news-commentary v11, and a matching quantity of data sampled from Czeng 1.6pre. We repeated this with 7.5M sentences from news2015 monolingual, and then with 10M sentences of news2015. The back-translations were, as for English→Czech, created with an earlier NMT model trained on WMT15 data. Our final Czech→English was an ensemble of 8 systems -the last 4 save-points of the 10M synthetic data run, and the last 4 save-points of the 7.5M run. We show this as ensemble8 in <ref type="table" target="#tab_4">Table 3</ref>, and the +synthetic results are on the last (i.e. 10M) synthetic data run.</p><p>We also show in <ref type="table" target="#tab_5">Table 4</ref> how increasing the amount of back-translated data affects the results. We see that most of the gain from back-translation   comes with the first batch, but increasing the amount of back-translated data does gradually improve performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">English↔Romanian</head><p>The results of our English↔Romanian experiments are shown in <ref type="table" target="#tab_7">Table 5</ref>. This language pair has the smallest amount of parallel training data, and we found dropout to be very effective, yielding improvements of 4-5 BLEU. <ref type="bibr">7</ref> We found that the use of diacritics was inconsistent in the Romanian training (and development) data, so for Romanian→English we removed diacritics from the Romanian source side, obtaining improvements of 1.3-1.4 BLEU.</p><p>Synthetic training data gives improvements of 4.1-5.1 BLEU. for English→Romanian, we found that the best single system outperformed the ensemble of the last 4 checkpoints on dev, and we thus submitted the best single system as primary system.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">English↔Russian</head><p>For English↔Russian, we cannot effectively learn BPE on the joint vocabulary because alphabets differ. We thus follow the approach described in <ref type="bibr" target="#b9">(Sennrich et al., 2016b)</ref>, first mapping the Russian text into Latin characters via ISO-9 transliteration, then learning the BPE operations on the concatenation of the English and latinized Russian training data, then mapping the BPE operations back into Cyrillic alphabet. We apply the Latin BPE operations to the English data (training data and input), and both the Cyrillic and Latin BPE operations to the Russian data.</p><p>Translation results are shown in <ref type="table" target="#tab_8">Table 6</ref>. As for the other language pairs, we observe strong improvements from synthetic training data (4-4.4 BLEU). Ensembles yield another 1.1-1.7 BLEU. <ref type="table">Table 7</ref> shows the ranking of our submitted systems at the WMT16 shared news translation task. Our submissions are ranked (tied) first for 5 out of 8 translation directions in which we participated: EN↔CS, EN↔DE, and EN→RO. They are also the (tied) best constrained system for EN→RU and RO→EN, or 7 out of 8 translation directions in total.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Shared Task Results</head><p>Our models are also used in QT21-HimL-SysComb <ref type="bibr" target="#b6">(Peter et al., 2016)</ref>, ranked direction BLEU rank human rank EN→CS 1 of 9 1 of 20 EN→DE 1 of 11 1 of 15 EN→RO 2 of 10 1-2 of 12 EN→RU 1 of 8 2-5 of 12 CS→EN 1 of 4 1 of 12 DE→EN 1 of 6 1 of 10 RO→EN 2 of 5 2 of 7 RU→EN 3 of 6 5 of 10 <ref type="table">Table 7</ref>: Automatic (BLEU) and human ranking of our submitted systems (uedinnmt) at WMT16 shared news translation task.</p><p>Automatic rankings are taken from http://matrix.statmt.org , only considering primary systems.</p><p>Human rankings include anonymous online systems, and for EN↔CS, systems from the tuning task.</p><p>1-2 for EN→RO, and in AMU-UEDIN <ref type="bibr" target="#b4">(Junczys-Dowmunt et al., 2016)</ref>, ranked 2-3 for EN→RU, and 1-2 for RU→EN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We describe Edinburgh's neural machine translation systems for the WMT16 shared news translation task. For all translation directions, we observe large improvements in translation quality from using synthetic parallel training data, obtained by back-translating in-domain monolingual target-side data. Pervasive dropout on all layers was used for English↔Romanian, and gave substantial improvements. For English↔German and English→Czech, we trained a right-to-left model with reversed target side, and we found reranking the system output with these reversed models helpful.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Amount of parallel and synthetic training data (number of sentences, in millions) for EN-* language pairs. For synthetic data, we separate the data according to whether the original monolingual language is English or not.</figDesc><table><row><cell>type</cell><cell>DE</cell><cell>CS RO RU</cell></row><row><cell>parallel</cell><cell cols="2">4.2 52.0 0.6 2.1</cell></row><row><cell cols="3">synthetic ( *  →EN) 4.2 10.0 2.0 2.0</cell></row><row><cell cols="2">synthetic (EN→  * ) 3.6</cell><cell>8.2 2.3 2.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>28.1 34.2 32.1 38.6</head><label></label><figDesc></figDesc><table><row><cell>system</cell><cell>EN→DE</cell><cell>DE→EN</cell></row><row><cell></cell><cell cols="2">dev test dev test</cell></row><row><cell>baseline</cell><cell cols="2">22.4 26.8 26.4 28.5</cell></row><row><cell>+synthetic</cell><cell cols="2">25.8 31.6 29.9 36.2</cell></row><row><cell>+ensemble</cell><cell cols="2">27.5 33.1 31.5 37.5</cell></row><row><cell>+r2l reranking</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>English↔German translation results (BLEU) on dev (newstest2015) and test (new-stest2016). Submitted system in bold.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell cols="2">: English↔Czech translation results</cell></row><row><cell cols="2">(BLEU) on dev (newstest2015) and test (new-</cell></row><row><cell cols="2">stest2016). Submitted system in bold.</cell></row><row><cell>system</cell><cell>best single ensemble4</cell></row><row><cell></cell><cell>dev test dev test</cell></row><row><cell>baseline</cell><cell>23.8 25.3 25.5 26.8</cell></row><row><cell cols="2">+2.5M synthetic 26.7 29.4 27.7 30.4</cell></row><row><cell>+5M synthetic</cell><cell>27.2 29.3 28.2 30.4</cell></row><row><cell cols="2">+7.5M synthetic 27.2 29.7 28.4 30.8</cell></row><row><cell cols="2">+10M synthetic 27.2 30.1 28.6 31.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>: Czech→English translation results</cell></row><row><cell>(BLEU) on dev (newstest2015) and test (new-</cell></row><row><cell>stest2016), after continued training with increas-</cell></row><row><cell>ing amounts of back-translated synthetic data. For</cell></row><row><cell>each row, training was continued from the baseline</cell></row><row><cell>model until convergence.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell cols="3">: English↔Romanian translation results</cell></row><row><cell cols="3">(BLEU) on dev (newsdev2016), and test (new-</cell></row><row><cell cols="3">stest2016). Submitted system in bold.</cell></row><row><cell>system</cell><cell>EN→RU</cell><cell>RU→EN</cell></row><row><cell></cell><cell cols="2">dev test dev test</cell></row><row><cell>baseline</cell><cell cols="2">21.3 20.3 22.7 22.5</cell></row><row><cell cols="3">+synthetic 25.8 24.3 27.1 26.9</cell></row><row><cell cols="3">+ensemble 27.0 26.0 28.3 28.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell>: English↔Russian translation results</cell></row><row><cell>(BLEU) on dev (newstest2015) and test (new-</cell></row><row><cell>stest2016). Submitted system in bold.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/nyu-dl/dl4mt-tutorial 4 https://github.com/emjotde/amunmt 5 https://github.com/rsennrich/subword-nmt</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">Due to recency effects, we expect last year's corpus to be most relevant, and sampled from News Crawl 2015 for EN-RO, EN-RU and EN-CS; for EN-DE, we re-used data from</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">We also tested dropout for EN→DE with 8 million sentence pairs of training data, but found no improvement after 10 days of training. We speculate that dropout could still be helpful for datasets of this size with longer training times and/or larger networks.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This project has received funding from the European Union's Horizon 2020 research and innovation programme under grant agreements 645452 (QT21), 644333 (TraMOOC) and 644402 (HimL).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural Machine Translation by Jointly Learning to Align and Translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bahdanau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">CzEng 1.6: Enlarged Czech-English Parallel Corpus with Processing Tools Dockered</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Bojar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text, Speech and Dialogue: 19th International Conference, TSD 2016</title>
		<meeting><address><addrLine>Brno, Czech Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="2016-09-12" />
		</imprint>
	</monogr>
	<note>September 12-16. In press</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A New Algorithm for Data Compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Gage</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">C Users J</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="23" to="38" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A Theoretically Grounded Application of Dropout in Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The AMU-UEDIN Submission to the WMT16 News Translation Task: Attention-based NMT Models as Feature Functions in Phrase-based SMT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Junczys-Dowmunt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth International Conference on Machine Learning</title>
		<editor>Lafferty et al.2001] John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira</editor>
		<meeting>the Eighteenth International Conference on Machine Learning<address><addrLine>Berlin, Germany; San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
	<note>Proceedings of the First Conference on Machine Translation (WMT16)</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Agreement on Target-bidirectional Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL HLT 16</title>
		<meeting><address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">; Jan-Thorsten</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamer</forename><surname>Alkhouli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning, ICML 2013</title>
		<meeting>the 30th International Conference on Machine Learning, ICML 2013<address><addrLine>Atlanta, GA, USA; Hermann Ney, Matthias Huck, Fabienne Braune, Alexander Fraser, Aleš Tamchyna, Ondřej Bojar, Barry Haddow, Rico Sennrich, Frédéric Blain, Lucia Specia; Alex Waibel, Alexandre Allauzen, Lauriane Aufrant, Franck Burlot, Elena Knyazeva, Thomas Lavergne, François Yvon, and Marcis Pinnis; Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-01" />
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
	<note>Proceedings of the First Conference on Machine Translation (WMT16)</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dropout Improves Recurrent Neural Networks for Handwriting Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th International Conference on Frontiers in Handwriting Recognition</title>
		<meeting><address><addrLine>Crete, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-09-01" />
			<biblScope unit="page" from="285" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improving Neural Machine Translation Models with Monolingual Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Sennrich et al.2016a</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural Machine Translation of Rare Words with Subword Units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016)</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016)<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Sennrich et al.2016b</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">ADADELTA: An Adaptive Learning Rate Method. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zeiler</surname></persName>
		</author>
		<idno>abs/1212.5701</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
