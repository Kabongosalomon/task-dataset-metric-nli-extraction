<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RELATIONAL NETWORK FOR SKELETON-BASED ACTION RECOGNITION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Zheng</surname></persName>
							<email>zheng-w10@foxmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Research on Intelligent Perception and Computing</orgName>
								<orgName type="institution">CASIA</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">CASIA</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Research on Intelligent Perception and Computing</orgName>
								<orgName type="institution">CASIA</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">CASIA</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
							<email>zhaoxiang.zhang@ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Research on Intelligent Perception and Computing</orgName>
								<orgName type="institution">CASIA</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">CASIA</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">CAS Center for Excellence in Brain Science and Intelligence Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Research on Intelligent Perception and Computing</orgName>
								<orgName type="institution">CASIA</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">CASIA</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Research on Intelligent Perception and Computing</orgName>
								<orgName type="institution">CASIA</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">CASIA</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">RELATIONAL NETWORK FOR SKELETON-BASED ACTION RECOGNITION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Skeleton-Based Action Recognition</term>
					<term>Re- current Relational Network</term>
					<term>Spatio-Temporal Modeling</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With the fast development of effective and low-cost human skeleton capture systems, skeleton-based action recognition has attracted much attention recently. Most existing methods use Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN) to extract spatio-temporal informa-â€  Equal contribution * Corresponding author Joints Lines ,1 t v ,2 t v ,3</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>tion embedded in the skeleton sequences for action recognition. However, these approaches are limited in the ability of relational modeling in a single skeleton, due to the loss of important structural information when converting the raw skeleton data to adapt to the input format of CNN or RNN. In this paper, we propose an Attentional Recurrent Relational Network-LSTM (ARRN-LSTM) to simultaneously model spatial configurations and temporal dynamics in skeletons for action recognition. We introduce the Recurrent Relational Network to learn the spatial features in a single skeleton, followed by a multi-layer LSTM to learn the temporal features in the skeleton sequences. Between the two modules, we design an adaptive attentional module to focus attention on the most discriminative parts in the single skeleton. To exploit the complementarity from different geometries in the skeleton for sufficient relational modeling, we design a two-stream architecture to learn the structural features among joints and lines simultaneously. Extensive experiments are conducted on several popular skeleton datasets and the results show that the proposed approach achieves better results than most mainstream methods.</p><p>Index Terms-Skeleton-Based Action Recognition, Recurrent Relational Network, Spatio-Temporal Modeling</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Action recognition provides a reasonable approach for video understanding and is under great demand, especially in the domains of intelligent surveillance and human-computer interaction. Traditional approaches are mainly based on the  <ref type="figure">Fig. 1</ref>. The expanded structure of Recurrent Relational Network (RRN), which is used to learn the spatial pattern in the single skeleton frame by modeling joints and lines separately. modeling of appearance and optical flow. However, the noise interference in RGB video dramatically obstructs the extraction of high-level features for action recognition.</p><p>Benefited from the advent of affordable depth sensors and efficient algorithms, dynamic human skeleton becomes an available and effective modality for action recognition. Meanwhile, compared with RGB video, the characteristics of high-level representation and robustness to viewpoints, appearances and background noise make skeletons have advantages in action recognition. As a result, many early skeletonbased methods were proposed and have showed encouraging improvements, such as <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>. However, these approaches were significantly limited in either the lack of exploring spatial structures <ref type="bibr" target="#b0">[1]</ref> or the dependence for hand-crafted features to analyze the spatial patterns <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>.</p><p>Recently, various deep learning based methods have been proposed to conduct skeleton-based action recognition. In general, these approaches are mainly based on CNN and RNN for capturing spatio-temporal information in skeletons. Specifically, the CNN-based methods utilize the powerful representation ability of CNN and achieve better performances than those hand-crafted feature based methods. And the RNN-based models have shown great advantages in capturing temporal dynamics in sequential skeletons. However, CNNs usually lose important structural information in the process of encoding skeletons into spatial-temporal images <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>, and RNNs have the same weakness when learn the spatial features in a single skeleton <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>. Thus, when converting the raw skeleton data to match with the CNN or RNN input format, the destruction of the original structures among the skeleton joints and lines leads to the difficulty in extracting robust spatial features in a single skeleton, which remains the main weakness of these frameworks.</p><p>In this paper, we propose an Attentional Recurrent Relational Network <ref type="bibr" target="#b10">[11]</ref>-LSTM (ARRN-LSTM) to model temporal dynamics and spatial configurations in skeletons for action recognition. Our approach is based on a two-stream architecture to learn sufficient relational information from both joints and lines in the skeleton. In each stream, we use the Recurrent Relational Network to learn the spatial patterns in a single skeleton and exploit a multi-layer LSTM to extract temporal information in skeleton sequences. Between the two modules, we design an adaptive attentional module for focusing on potential discriminative parts of a skeleton towards a certain action. Compared with other graph networks <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>, we believe Recurrent Relational Network is a better framework for learning spatial information in a single skeleton, as depicted in <ref type="figure">Fig.1</ref>, because it can ensure the flexible flow of information and build long-range dependencies among all joints or lines, which is important for learning robust features from graph structure data. Overall, our contributions can be summarized as follows:</p><p>â€¢ We introduce the Recurrent Relational Network to the domain of skeleton-based action recognition and prove that it is a very good framework to learn the spatial feature in the single skeleton.</p><p>â€¢ We design an organic framework, the two-stream ARRN-LSTM, to conduct skeleton-based action recognition, and achieve better results than most mainstream methods on popular skeleton datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Relational Network</head><p>Santoro et al. <ref type="bibr" target="#b13">[14]</ref> propose a simple plug-and-play neural network module for relational reasoning. With this module, a neural network gains the ability of handling unstructured inputs and inferring their hidden relationship, which achieves state-of-the-art results on visual question answering datasets. Based on this work, Palm et al. <ref type="bibr" target="#b10">[11]</ref> propose the Recurrent Relational Networks for complex relational reasoning, such as learning an iterative strategy to solve Sudoku.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Skeleton-based Action Recognition with Deep Networks</head><p>To utilize the powerful representation ability of CNN, skeletons are usually encoded into spatial-temporal images to fit the inputs of CNN. Hou et al. <ref type="bibr" target="#b3">[4]</ref> accumulate the raw skeleton frames directly and encode the color based on temporal information. Liu et al. <ref type="bibr" target="#b5">[6]</ref> exploit the 3DCNN to extract spatiotemporal features for avoiding the loss of information in projecting process. Du et al. <ref type="bibr" target="#b4">[5]</ref> divide the joints into five main parts according to human physical structures (four limbs and one trunk) and take the 3D coordinates of joints as 3 channels of RGB image. Specifically, Yan et al. <ref type="bibr" target="#b12">[13]</ref> use the Graph Convolutinal Network to form hierarchical representation of skeletons and achieve good results. RNN is good at processing sequential data due to the extraordinary ability of capturing structural information in sequences. Du et al. <ref type="bibr" target="#b6">[7]</ref> divide the human skeleton into five parts according to human physical structure and separately feed them into different RNNs. Song et al. <ref type="bibr" target="#b7">[8]</ref> modify RNN to design an attentional module and use multi-layer LSTM to learn spatial and temporal information. Shahroudy et al. <ref type="bibr" target="#b8">[9]</ref> propose a Part-Aware LSTM unit that builds full connections between all the memory cells and all the input features for acquiring richer information. Liu et al. <ref type="bibr" target="#b9">[10]</ref> transform the joints in the form of tree structure based on traversal and propose a spatio-temporal LSTM framework to learn spatio-temporal information in joint sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PROPOSED METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Pipeline Overview</head><p>The pipeline of our framework is depicted in <ref type="figure" target="#fig_2">Fig.2</ref>. Our framework consists of two streams for learning structural features from joints and lines separately. In each stream, an embedding operation is first performed on each joint or line to increase its dimension. Then, the embedding results of joints or lines are sent to RRN for capturing the spatial patterns in the single skeleton. To focus more attention on potential discriminative parts in the skeleton, we generate a learnable mask and then use it to perform point-wise multiplication with the outputs of RRN. After that, we use a multi-layer LSTM to learn temporal features in skeleton sequences. Finally, we take the weighted average operation as our fusion strategy to combine the predictions from both streams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">The Construction of the Two-Stream ARRN-LSTM</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Embedding</head><p>The raw skeleton is defined by a fixed number of joints in the form of 3D coordinates, with the number denoted as J. To improve the representation of joints and strengthen the discrimination among them, we use a fully-connected layer to map the 3D coordinates to a high dimensional space. Thus, given a joint c t,i = (c x t,i , c y t,i , c z t,i ) that means the 3D coordinate of the i-th joint in the t-th frame, the M -dimensional embedding result v t,i is:  With the exception of original joints, we believe that the lines between pair-wise joints are also important geometric structures in the skeleton and contain rich structural or relational information. Specifically, joints emphasize the absolute position, which can figure out discriminative moving parts of body in the action by analyzing the distribution or local density of joints. While lines emphasize relative position, which can build angles to help figure out specific poses and actions. Thus, there exists potential complementarity for action recognition between both geometric structures. We calculate lines between c t,i and other joints as:</p><formula xml:id="formula_0">v t,i = Emb(c t,i ) = Emb(c x t,i , c y t,i , c z t,i ) (1) Embedding Embedding ,4 t v ,1 t v ,3 t v ,2 t v ,2 i t w ,3 i t w ,1 i t w ,4</formula><formula xml:id="formula_1">v ,1 t v ,3 t v ,2 t v ,2 i t w ,3 i t w ,1 i t w ,</formula><formula xml:id="formula_2">l t,i = (c x t,i âˆ’ c x t,1 , c y t,i âˆ’ c y t,1 , c z t,i âˆ’ c z t,1 , c x t,i âˆ’ c x t,2 ..., c x t,i âˆ’ c x t,J , c y t,i âˆ’ c y t,J , c z t,i âˆ’ c z t,J )<label>(2)</label></formula><p>The l t,i denotes all lines that connect the joint c t,i and all the other joints, but these distances exclude the one between c t,i and itself. Thus, if the dimension of one joint is 3, the dimension of the corresponding lines is 3Ã—(J âˆ’1). Similarly, the line embedding process is:</p><formula xml:id="formula_3">v t,i = Emb(l t,i ) = Emb(l 1 t,i , l 2 t,i , l 3 t,i ..., l 3Ã—(Jâˆ’1) t,i )<label>(3)</label></formula><p>Because the output dimensions of both joint and line embeddings are equal, we use the same symbol v t,i to denote the embedding result of lines l t,i in following expressions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Recurrent Relational Network</head><p>In the domain of skeleton-based action recognition, some graph networks based frameworks <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref> have been proposed recently and achieved substantial improvements than mainstream methods. While these frameworks applied the notion of receptive fields to the graph structure and extracted features only from a small adjacent range. Compared with them, we believe Recurrent Relational Network is a better framework for learning spatial features in a single skeleton, because it can ensure the flexible flow of information among joints and build long-range dependencies among them, which could help to learn more robust features from graph structure data. Besides, the implementation of RRN is much easier than previous graph networks. Specifically, we use all joints or lines from a single skeleton to feed the RRN. In the process, each joint or line embedding v t,i will be the input to one node of RRN and the number of nodes in RRN is J, which can be denoted as:</p><formula xml:id="formula_4">w t = Rrn(v tÂ· ) = Rrn(v t,1 , v t,2 , v t,3 ..., v t,J )<label>(4)</label></formula><p>As for the detailed process, if we denote the states of node i and j in the e-th iteration of RRN as h e i and h e j , we can define the information flowing from node j to node i as:</p><formula xml:id="formula_5">m e i,j = f (h eâˆ’1 i ,h eâˆ’1 j )<label>(5)</label></formula><p>f is a message function. The messages from all neighbouring nodes to joint i are then summed up as:</p><formula xml:id="formula_6">m e iÂ· = jâˆˆN (i) m e i,j<label>(6)</label></formula><p>Then the ouput of node i can be updated by a trainable node function g as:</p><formula xml:id="formula_7">h e i = g(h eâˆ’1 i , v t,i , m e iÂ· )<label>(7)</label></formula><p>Given the number of iterations in RRN as E, the output of node i after the final iteration can be expressed as:</p><formula xml:id="formula_8">w t,i = h E i<label>(8)</label></formula><p>At this point, we calculate the relational modeling results of the joints or lines in a skeleton as w t = (w t,1 , w t,2 , w t,3 ..., w t,J ), in the dimension of (J, M ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">Attentional Module</head><p>For a certain action, humans usually recognize it by focusing the most discriminative parts. For example, the action of kicking can be identified through the legs, while the drinking action can be recognized by the arms. However, some different actions, such as flipping and reading a book, cannot be distinguished until the subtle differences in the hand part are identified. Thus, we design an attentional module to address these problems, with the module following the RRN. Specifically, we first generate a learnable mask, it could be a random vector of size J, and then we use it to perform pointwise multiplication with the node outputs of RRN. After that, we use a fully-connected layer to reduce the product dimension to avoid overfitting in following multi-layer LSTM. We express the process as follows:</p><formula xml:id="formula_9">p t = Att(w t ) = Fc(m 1 Â· w t,1 , m 2 Â· w t,2 ..., m J Â· w t,J ) (9)</formula><p>The mask m = (m 1 , m 2 , ..., m J ) is a J-dimensional vector and can be trained with back-propagation algorithm, which aims to emphasize the impacts of some joints or lines while neglect other unimportant ones by allocating different weights. The output p t can expressed as (p t,1 , p t,2 , p t,3 ..., p t,J ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4.">Multi-Layer LSTM</head><p>Following RRN, we exploit a multi-layer LSTM to learn the temporal dynamics in skeleton sequences. We first concatenate all joints or lines features from a single skeleton as the input of one cell in LSTM, and the number of cells in LSTM is equal to the skeleton sequence length T , so all frames in a skeleton sequence can exchange information with internal connections in LSTM. This process is denoted as follows:</p><p>(q 1 , q 2 , q 3 ..., q T ) = Multi lstm(p 1 , p 2 , p 3 ..., p T )</p><p>The number of layers in multi-layer LSTM is H and the dimension of q t is h.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.5.">Score Fusion</head><p>To get the final prediction from each stream, we connect a fully-connected layer after multi-layer LSTM to map the extracted spatial-temporal features to the categories of size K. Then we run a softmax operation on the output to obtain the predicted probabilities. We take joint stream as an example:</p><formula xml:id="formula_11">y j = softmax(Fc(q 1 , q 2 , q 3 ..., q T ))<label>(11)</label></formula><p>To exploit the complementarity between joints and lines, we take the weighted average as the fusion strategy and get the final prediction. We use y j and y l to denote the scores of joint and line stream, respectively. The final prediction y is:</p><formula xml:id="formula_12">y = Î± Â· y j + Î² Â· y l<label>(12)</label></formula><p>Î± and Î² are relative weights of the two stream predictions and their sum is equal to 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS AND RESULTS</head><p>In our experiments, firstly we perform detailed ablation study to validate our framework, showing the results in <ref type="table" target="#tab_0">Tables 1 and  2</ref>. Then we train the proposed two-stream Attentional RRN-LSTM model and test it on NTU RGB+D, Florence 3D, and MSRAction3D datasets, with results and comparisons shown in <ref type="table">Tables 3, 4</ref> and 5 separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>We normalize the joint coordinators by subtracting the average value of the 5 joints close to the hip joint. The lines are calculated according to the normalized joints. We perform zero padding on videos with frames less than T and random sampling on videos with frames more than T to fix all videos as T frames. According to the differences of frame numbers in different datasets, we set T = 100, 25, 20 for NTU-RGBD, Florrence3D and MSRAction3D, respectively. Besides, we conduct the embedding with a fully-connected layer and set the output size M = 50, 20, 20 for the three datasets based on cross-validation. The following RRN executes E = 5 iterations per frame with each node function g realized by an GRU unit and the message function f constructed with 3 fully-connected layers. The attentional module is built with a trainable mask m and a fully-connected layer for reducing the product to a 256-dimensional vector. We use H = 3 layers LSTM to extract temporal information in skeleton sequences, with input and output set as a 256-dimensional and h = 512dimensional vector separately. The mid-layer LSTM has an output size of 512,256,256 for NTU-RGBD, Florrence3D and MSRAction3D, respectively. Both Î± and Î² are set as the optimal values based on validation set in our experiments. We use Stochastic Gradient Descent to train our model from scratch on NTU-RGBD and set the initial learning rate as 0.01, we multiply the learning rate with 0.1 when the accuracy gets saturated. On other datasets, we use Adam optimizer to train our model from scratch. Our model is trained on a NVIDIA TITAN X GPU with PyTorch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Datasets</head><p>NTU RGB+D Dataset. This is the most popular and largest depth-based skeleton action recognition dataset currently, with more than 56 thousand video samples and 4 million frames collected from 40 different subjects. It consists of 60 different action classes. We evaluate our model according to the metrics proposed in <ref type="bibr" target="#b8">[9]</ref>, including Cross-Subject (CS) and Cross-View (CV) evaluations. Florence 3D. This dataset includes 215 action sequences performed by 10 subjects for 2 to 3 times. It is made up of 9 activities and each skeleton is represented by 15 joints. The difficulty of this dataset lies in its similarities between actions, such as drinking from a bottle, answering phone and reading <ref type="table">Table 1</ref>. Comparisons with Baselines on NTU RGB+D. <ref type="bibr">Methods</ref> CS CV 2-Layer LSTM <ref type="bibr" target="#b8">[9]</ref> 60.7 67.3 2-Layer P-LSTM <ref type="bibr" target="#b8">[9]</ref> 62.9 70.3 MT-3D-CNN <ref type="bibr" target="#b5">[6]</ref> 66.9 72.6 STA-LSTM <ref type="bibr" target="#b7">[8]</ref> 73.4 81.2 VA-LSTM <ref type="bibr" target="#b14">[15]</ref> 79.4 87.6 Joint-ARRN-LSTM 79.6 87.8 watch. We follow the standard metric, i.e., leave-one-subjectout cross validation, to evaluate our model.</p><p>MSRAction3D. This dataset contains 20 actions performed by seven subjects for three times, which totally consists of 4020 action samples. The dataset is divided into three subsets and each subset has 8 actions. In each subset, the samples of subjects 1, 3, 5, 7, 9 are used for training while the samples of subjects 2, 4, 6, 8, 10 are used for testing. Final accuracy is calculated as average accuracies of three subsets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>We examine the effectiveness of the proposed ARRN-LSTM framework and study the impact of each part by ablation study in this section, with results on NTU shown in <ref type="table">Table 1</ref> and 2.</p><p>To illustrate the effectiveness of RRN in learning spatial features in single skeleton, we compare the results of our joint stream with several baselines in <ref type="table">Table 1</ref>, and these methods also learn features from joints. Our method uses RRN and multi-layer LSTM to learn spatial and temporal information separately. Differently, 2-layer LSTM and 2-layer P-LSTM use pure LSTM or its variant to extract spatio-temporal features from joints, and our method outperforms it substantially. Furthermore, MT-3D-CNN <ref type="bibr" target="#b5">[6]</ref> uses CNN to build a two-stream model for learning spatial and temporal features from skeletons, while both STA-LSTM <ref type="bibr" target="#b7">[8]</ref> and VA-LSTM <ref type="bibr" target="#b14">[15]</ref> use multi-layer LSTM as backbone to learn spatial and temporal information from joints. But our method achieves much better results than these baselines, which validates that RRN can model spatial features in single skeleton effectively.</p><p>From <ref type="table" target="#tab_0">Table 2</ref>, we analyze the performance of each stream and attentional module. Each stream ARRN-LSTM can achieve relatively good performance, which validates the effectiveness of the basic ARRN-LSTM. For attentional module, it increases the accuracy by 2 âˆ¼ 5 points under both metrics, validating the insight of the attention mechanism. Fur- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Accuracy(%) Lie Group <ref type="bibr" target="#b1">[2]</ref> 90.88 Graph-Based <ref type="bibr" target="#b17">[18]</ref> 91.63 P-LSTM <ref type="bibr" target="#b8">[9]</ref> 95.35 STGCK <ref type="bibr" target="#b11">[12]</ref> 97.67 Deep STGCK <ref type="bibr" target="#b11">[12]</ref> 99.07 ARRN-LSTM 98.52 thermore, the combination of both streams can result in an increase of 2 âˆ¼ 3 points in final accuracy, proving the complementarity between joints and lines. Finally, the overall framework increases the accuracy by almost 6 âˆ¼ 7 points, which proves the reasonability and effectiveness of our framework. For embedding layer, we cannot ensure the convergence of the framework if we remove it, so we do not show the ablation study of this part. For fusion of two streams, we tried early fusion and several operations like max and multiply, but they are hard to converge or achieve good performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparisons with Mainstream Methods</head><p>NTU RGB+D Dataset. We compare our ARRN-LSTM model with mainstream methods on this dataset in <ref type="table">Table 3</ref>, it is clear that our ARRN-LSTM method achieves better results than those CNN or RNN based methods. Specifically, ST-GCN <ref type="bibr" target="#b12">[13]</ref> takes the graph convolution to learn the spatial and temporal features in skeleton sequences, while our method takes the RRN to perform relational modeling in the single skeleton and uses a multi-layer LSTM to obtain the temporal information in skeleton sequences. The results could prove the strong ability of the RRN in modeling spatial features in single skeleton and the effectiveness of the whole framework.</p><p>Florence 3D. As shown in <ref type="table">Table 4</ref>, the proposed ARRN-LSTM framework is superior to most mainstream methods that are based on LSTM, CNN and traditional algorithms. Compared with the state-of-the-art Deep STGCK <ref type="bibr" target="#b11">[12]</ref>, the performance of our model is very close to its result. Our model is also based on a graph network like Deep STGCK, but the multi-layer LSTM make model complexity much <ref type="table">Table 5</ref>. Comparison of Accuracies on MSRAction3D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Accuracy(%) Lie Group <ref type="bibr" target="#b1">[2]</ref> 92.5 HBRNN <ref type="bibr" target="#b6">[7]</ref> 94.5 ST-LSTM <ref type="bibr" target="#b9">[10]</ref> 94.8 Graph-Based <ref type="bibr" target="#b17">[18]</ref> 94.8 ST-NBNN <ref type="bibr" target="#b15">[16]</ref> 94.8 ST-NBMIM <ref type="bibr" target="#b15">[16]</ref> 95.3 ARRN-LSTM 95.0 larger than Deep STGCK, making it easy to be overfitting on small datasets. MSRAction3D. As shown in <ref type="table">Table 5</ref>, our method achieves 95.0% accuracy and outperforms most mainstream methods, and the result is also competitive with the state-ofthe-art method ST-NBMIM <ref type="bibr" target="#b15">[16]</ref>. Although our framework suffers overfitting on small datasets again, the result could still validate the effectiveness of our method on small datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>In this paper, we introduced the Recurrent Relational Network to the domain of skeleton-based action recognition, and we also designed an organic two-stream ARRN-LSTM framework and achieved better results than most mainstream methods. The experiments results proved the strong modeling ability of RRN in the single skeleton and the effectiveness of the whole framework. However, we believe there still exists possibility of modeling both single skeleton and skeleton sequences with relation network for action recognition, which may provide a brand new perspective to address this problem and a potential direction to achieve further progress.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Framework of the proposed two-stream ARRN-LSTM model. It is recommended to view the digital version.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>Results of Ablation Study on NTU RGB+D.</figDesc><table><row><cell>Model</cell><cell>CS</cell><cell>CV</cell></row><row><cell>Line-RRN-LSTM</cell><cell>74.5</cell><cell>83.3</cell></row><row><cell>Joint-RRN-LSTM</cell><cell>74.6</cell><cell>83.1</cell></row><row><cell>Line-ARRN-LSTM</cell><cell>76.4</cell><cell>87.2</cell></row><row><cell>Joint-ARRN-LSTM</cell><cell>79.6</cell><cell>87.8</cell></row><row><cell>Two-Stream RRN-LSTM</cell><cell>77.6</cell><cell>84.2</cell></row><row><cell>Two-Stream ARRN-LSTM</cell><cell>81.8</cell><cell>89.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>Comparison of Accuracies on NTU RGB+D. Comparison of Accuracies on Florence 3D.</figDesc><table><row><cell>Methods</cell><cell cols="2">CS(%) CV(%)</cell></row><row><cell>Lie Group [2]</cell><cell>50.1</cell><cell>52.8</cell></row><row><cell>Deep LSTM [9]</cell><cell>60.7</cell><cell>67.3</cell></row><row><cell>PA-LSTM [9]</cell><cell>62.9</cell><cell>70.3</cell></row><row><cell>ST-LSTM+TS [10]</cell><cell>69.2</cell><cell>77.7</cell></row><row><cell>ST-NBMIM [16]</cell><cell>80.0</cell><cell>84.2</cell></row><row><cell>Deep STGCK [12]</cell><cell>74.9</cell><cell>86.3</cell></row><row><cell>VA-LSTM [15]</cell><cell>79.4</cell><cell>87.6</cell></row><row><cell>C-CNN + MTLN [17]</cell><cell>79.6</cell><cell>84.8</cell></row><row><cell>ST-GCN [13]</cell><cell>81.5</cell><cell>88.3</cell></row><row><cell>ARRN-LSTM</cell><cell>81.8</cell><cell>89.6</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t v t,J v 1 2 h 1 3 h 1 J</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mining actionlet ensemble for action recognition with depth cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1290" to="1297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Human action recognition by representing 3d skeletons as points in a lie group</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raviteja</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felipe</forename><surname>Arrate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="588" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The moving pose: An efficient 3d kinematics descriptor for low-latency action recognition and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Leordeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2752" to="2759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Skeleton optical spectra based action recognition using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghong</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pichao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanqing</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Skeleton based action recognition with convolutional neural network,&quot; in ACPR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="579" to="583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Two-stream 3d convolutional neural network for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juanhui</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyuan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.08106</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1110" to="1118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An end-to-end spatio-temporal attention model for human action recognition from skeleton data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian-Tsong</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.02808</idno>
		<title level="m">Ntu rgb+ d: A large scale dataset for 3d human activity analysis</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Spatiotemporal lstm with trust gates for 3d human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="816" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Recurrent relational networks for complex relational reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rasmus</forename><surname>Berg Palm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Paquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ole</forename><surname>Winther</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.08028</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Spatio-temporal graph convolution for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaolong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenming</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.09834</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07455</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A simple neural network module for relational reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4974" to="4983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">View adaptive recurrent neural networks for high performance human action recognition from skeleton data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianru</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2136" to="2145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Discriminative spatio-temporal pattern discovery for 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwu</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoqun</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A new representation of skeleton sequences for 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuhong</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Bennamoun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4570" to="4579" />
		</imprint>
	</monogr>
	<note>Senjian An, Ferdous Sohel, and Farid Boussaid</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Graph based skeleton motion representation and similarity measurement for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunfeng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanning</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="370" to="385" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
