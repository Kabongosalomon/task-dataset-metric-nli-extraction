<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Temporal-Spatial Mapping for Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Song</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyu</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Sun</surname></persName>
						</author>
						<title level="a" type="main">Temporal-Spatial Mapping for Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Temporal-Spatial Mapping (TSM)</term>
					<term>action recog- nition</term>
					<term>deep learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep learning models have enjoyed great success for image related computer vision tasks like image classification and object detection. For video related tasks like human action recognition, however, the advancements are not as significant yet. The main challenge is the lack of effective and efficient models in modeling the rich temporal spatial information in a video. We introduce a simple yet effective operation, termed Temporal-Spatial Mapping (TSM), for capturing the temporal evolution of the frames by jointly analyzing all the frames of a video. We propose a video level 2D feature representation by transforming the convolutional features of all frames to a 2D feature map, referred to as VideoMap. With each row being the vectorized feature representation of a frame, the temporalspatial features are compactly represented, while the temporal dynamic evolution is also well embedded. Based on the VideoMap representation, we further propose a temporal attention model within a shallow convolutional neural network to efficiently exploit the temporal-spatial dynamics. The experiment results show that the proposed scheme achieves the state-of-the-art performance, with 4.2% accuracy gain over Temporal Segment Network (TSN), a competing baseline method, on the challenging human action benchmark dataset HMDB51.</p><p>Index Terms-Temporal-Spatial Mapping (TSM), action recognition, deep learning • We propose a simple yet effective operation, Temporal-Spatial Mapping, for jointly embedding the temporalspatial information of a video from per-frame features into a compact feature map, i.e., VideoMap. The pro-arXiv:1809.03669v1 [cs.CV]</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>A CTION recognition is an important yet challenging problem in computer vision, with many practical applications such as visual surveillance, human computer interaction, and video analyses <ref type="bibr" target="#b0">[1]</ref>. Recently, deep learning models like Convolutional Neural Networks (CNN) <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b4">[5]</ref> and Recurrent Neural Networks (RNN) <ref type="bibr" target="#b5">[6]</ref>- <ref type="bibr" target="#b10">[11]</ref> have been extensively employed to recognize actions in videos. Despite great efforts and rapid developments, the advancements are not as significant as those achieved in image related computer vision tasks such as image classification <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b13">[14]</ref> and object detection <ref type="bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b16">[17]</ref>. The main reason is that actions in a video involve not only the spatial information of each frame, but also its temporal evolution. Exploring this rich temporal-spatial information requires the deep learning model to be equipped with more parameters, trained with more video samples, and most importantly formulated with more effective architecture.</p><p>Previous attempts to address action recognition include the two-stream ConvNets <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b17">[18]</ref>, 2D ConvNets followed by Long Short-Term Memory (LSTM) networks <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b10">[11]</ref>, 3D ConvNets <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, and many others <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>. These models are continually pushing forward the state-of-theart performances of action recognition. Most of these models, however, suffer from limitations such as lack of joint temporalspatial learning <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b17">[18]</ref>, difficulties in model training <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>. Moreover, limited by the designs, most of those approaches cannot leverage more dense frames for obtaining further gains even though more frames can provide more temporal-spatial information <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>. That is because the statistics of the features/scores of frames are utilized to get the final prediction <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b22">[23]</ref>. Frames above a certain number (e.g., 25 frames) help very little once they are enough to estimate the statistics. To overcome these limitations, we need a network architecture which jointly and effectively learns the temporal-spatial feature representations and is capable of exploring the information of dense frames.</p><p>To this end, we present a simple yet effective operation, i.e., Temporal-Spatial Mapping (TSM), for joint temporal-spatial feature modeling. We represent the temporal-spatial features of an entire video compactly by a VideoMap, which is a rowwise layout of the per-frame vectorized ConvNet features as illustrated in the middle of <ref type="figure" target="#fig_0">Figure 1</ref>. This enables the "seeing" of dense frames at a glance and thus performing effective joint temporal-spatial analyses. The proposed TSM operation is general and can be used after any convolutional features for video-level temporal-spatial feature learning.</p><p>To deploy this TSM operation for action recognition, we first train a backbone 2D ConvNet model to extract convlutional features for each frame of a video sequence, then the TSM operation is performed on the features to generate the temporal-spatial VideoMap, which naturally encodes the temporal-spatial information in 2D feature map. Based on the compact VideoMap representation, we further propose a temporal attention model within a head ConvNet to extract effective video-level feature embeddings to predict the final action categories. Experiment results on two large benchmarks, HMDB51 and UCF101, demonstrate the effectiveness of the proposed network architecture and its state-of-the-art performances.</p><p>To summarize, the main contributions of this work are threefold: posed operation is general and can be applied to any CNN features to explore temporal dynamics. VideoMap representation provides a way to leverage dense frames for enhancing the performance. <ref type="bibr">•</ref> We propose a temporal attention model within a head ConvNet to further transform the temporal-spatial VideoMap to a more compact and effective video-level feature representation for classification, which can better exploit the temporal-spatial dynamics. • We present a deep architecture for action recognition which achieves significant performance improvement on the HMDB51 dataset. The source code and trained model will be released to facilitate the research in action recognition.</p><p>II. RELATED WORK Motivated by the outstanding performance of deep neural networks on image classification and detection, more and more works have extended CNN-based or CNN-LSTM-based architectures for video analysis.</p><p>The two-stream ConvNets approaches <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref> train two separate 2D ConvNets for both appearance in still images and stacks of optical flow, from several sparsely sampled frames. The temporal stream takes the short-term temporal information into account by means of optical flow and achieves superior performance than the spatial stream (still images). The Temporal Segment Networks (TSN) <ref type="bibr" target="#b2">[3]</ref> combine a sparse temporal sampling strategy and video-level supervision in training to explore temporal structure. Three frames which are randomly selected from three equally divided segments are jointly trained with frames combined by average pooling. However, these approaches involve temporal information by simply averaging/multiplying the scores across frames for the video level prediction. Such approaches still cannot accurately model the temporal dynamics <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b22">[23]</ref>. Some aggregation techniques like VLAD <ref type="bibr" target="#b24">[25]</ref>, Fisher Vector <ref type="bibr" target="#b25">[26]</ref> and dictionary learning <ref type="bibr" target="#b26">[27]</ref> have been used in action recognition for aggregating features of frames <ref type="bibr" target="#b27">[28]</ref>- <ref type="bibr" target="#b29">[30]</ref>. VLAD-based methods like ActionVLAD <ref type="bibr" target="#b27">[28]</ref> provide solutions to perform spatiotemporal aggregation of a set of action primitives. However, they do not model the time order of frames.</p><p>To extend convolution operations from 2D image to 3D video, the 3D ConvNets <ref type="bibr" target="#b19">[20]</ref> directly operates on the video for spatio-temporal feature learning by replacing 2D filters with 3D ones. So far, such approach however has shown limited benefit, probably due to the lack of training data, high complexity of training 3D convolution kernels, and not exploiting the optical flow stream explicitly. To reduce the number of parameters of 3D ConvNets, Sun et al. <ref type="bibr" target="#b30">[31]</ref> propose to factorize the original 3D convolution kernel learning as a sequential process of learning 2D spatial kernels followed by learning 1D temporal kernels. However, both approaches model the temporal dynamics by averaging the activations of the sub-clips of a video. There is still a lack of a global modeling of the action among the video sub-clips. To capture the relationships among the video sub-clips, the temporal linear encoding (TLE) <ref type="bibr" target="#b22">[23]</ref> encodes the aggregated information of K (i.e., K = 3) clips/frames into a video feature representation by performing element-wise multiplication of the features of the clips/frames. However, for a long video, the sampling of K clips/frames and the aggregation of them by multiplication results in information loss. Since the statistics of clips/frames are explored rather than their details, like TSN <ref type="bibr" target="#b2">[3]</ref>, the performance increases little when more frames are used.</p><p>In <ref type="bibr" target="#b5">[6]</ref>- <ref type="bibr" target="#b8">[9]</ref>, LSTM is utilized to explore the temporal evolution of the per-frame CNN features across a video. Shi et al. <ref type="bibr" target="#b31">[32]</ref> propose a sequential Deep Trajectory Descriptor (sDTD) to model long-term motion information in video and employe a three-stream CNN-LSTM architecture for action recognition. Wang et al. <ref type="bibr" target="#b32">[33]</ref> propose two-stream 3D Con-vNets Fusion to recognize actions of arbitrary size and length by using spatial temporal pyramid pooling (STPP) with a LSTM or CNN model to extract multi-size descriptions and learn global representation for each input video. Li et al. <ref type="bibr" target="#b33">[34]</ref> propose a unified Spatio-Temporal Attention Networks (STAN) using attention neural cell (AttCell) based on CNN-LSTM architecture to estimate attention on both spatial and temporal locations in a video. Compared with image-based approaches <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, LSTM-based approaches go one step further which can model the temporal dynamics of a video. However, applying the LSTM models to video based action recognition has so far only achieved similar performance as temporal pooling <ref type="bibr" target="#b5">[6]</ref>, likely attributed to the rigid structure of LSTM and the difficulties in training.</p><p>In this paper, we propose a general approach of temporalspatial mapping to facilitate the joint analysis of the dense frames/clips of a video, with the time order information embedded in the mapped VideoMap. Our approach provides an efficient way to explore the details of dense frames, enabling performance improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. TEMPORAL-SPATIAL MAPPING OPERATION</head><p>In a video, besides the spatial information in each image, the temporal evolution provides vital information for identifying an action. It is not trivial to find a video representation that encodes the dense frames together to facilitate the joint analysis of the entire video. The traditional powerful approach iDTs <ref type="bibr" target="#b23">[24]</ref> densely samples feature points in video frames and uses optical flow to track them to yield a good video representation. For action recognition from video, most deep learning based approaches which have good performance <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b17">[18]</ref> are still image based where they infer the final results by averaging/multiplying the scores/features of frames. The proposed Temporal-Spatial Mapping provides a way to jointly consider the dense sequential frames for inferencing the video label. <ref type="figure">Figure III</ref> shows the process. For each frame of a video sequence, ConvNet generates feature maps at each convolutional layer, where features of higher abstraction are captured by higher layers <ref type="bibr" target="#b34">[35]</ref>. Consider the output features of 2D ConvNet for T frames from a video. The features of the k th frame can be a feature vector of L−dimension, i.e., f k ∈ R L , e.g., the output of the global pooling layer of the TSN <ref type="bibr" target="#b2">[3]</ref>. They can be feature maps with high dimensions, i.e., S k ∈ R h×w×c , where h, w and c denote the height, width, and number of channels of the feature maps, e.g., the inception layer output of TSN <ref type="bibr" target="#b2">[3]</ref>. The high dimensionality of feature maps makes it challenging to jointly analyze the dense frames of a video. Thus, in that case, a spatial vectorization function V : S k → f k , is used to encode the feature maps to a low dimension vector of fixed length, i.e., f k = V (S k ), where f k ∈ R L . Then, we layout the feature vector of each frame as a row with the row identity corresponding to the time order of the frames to create a twodimensional temporal-spatial map, i.e., VideoMap as</p><formula xml:id="formula_0">M = [f 1 ; f 2 ; · · · ; f T ] ∈ R T ×L .<label>(1)</label></formula><p>The width of the map is equal to the total number of frames while the height is equal to the dimension of the feature vector. The map has embedded both the temporal and spatial information. This makes it possible to have a global observation of a video sequence and facilitates the exploration of the temporal dynamics. This TSM operation has the following characteristics and advantages.</p><p>Discussions. Here, we discuss the relation and differences of our method with several classical methods. We aim to explore the temporal dynamics from the dense frames of a video and jointly make a decision from them.</p><p>Two-stream based ConvNets: Our framework is compatible with the two-stream ConvNet based approaches <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>. TSN <ref type="bibr" target="#b2">[3]</ref> uses three temporal segmented frames to explore the longrange temporal structure in training. During testing, the scores from N (N = 25) frames are averaged to finally predict the action. However, the temporal dynamics are only weakly explored by the simple averaging of a few frames without considering the time order. In contrast, our temporal-spatial mapping to a VideoMap enables the joint exploration of many frames with time order retained.</p><p>3D Convolution: 3D convolution provides an elegant framework for exploring the spatial and temporal dynamics <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>. Without the practical constraints, such as on memory, labeled data, computational resource, it is expected to achieve excellent performance. However, such approaches so far have not demonstrated satisfactory performance in practice due to the difficult to train. In practice, 3D Convolution only covers a short range of the sequence for each input video sub-clip (e.g., 5-7 frames in <ref type="bibr" target="#b18">[19]</ref>, 16 frames in C3D <ref type="bibr" target="#b19">[20]</ref>). The scores of subclips are averaged to get the final prediction. The temporal dynamics among clips are not well explored by the simple averaging and the time order information among the sub-clips is lost.</p><p>CNN+LSTM: To tackle the not well solved temporal dynamic modeling problem, some works [6]- <ref type="bibr" target="#b8">[9]</ref> use the Recurrent Neural Network with Long-short Term Memory (LSTM) to model the temporal evolution. The RNN structure facilitates the exploration of temporal dynamics from the dense frames with time order considered. However, it has only achieved similar performance as temporal pooling <ref type="bibr" target="#b5">[6]</ref>. This might be attributed to the difficulty in training with the gradient vanishing for the long videos.</p><p>In contrast, we leverage the temporal-spatial mapping to obtain a VideoMap which embeds the information of temporal dynamics and time order. It facilitates the joint exploration of the dense frames of a video for a global decision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. TEMPORAL ATTENTION WITHIN HEAD CONVNET</head><p>Motivated by the success of using ConvNet for feature extraction in image classification, which jointly explores crosspixel correlations, we leverage a convolution neural network to jointly explore the cross-frame dynamics.</p><p>With the VideoMap as input, we design a temporal attention model within a head ConvNet for video level feature extraction and action recognition. <ref type="figure">Figure IV</ref> shows this network structure, which consists of a shallow ConvNet and a temporal attention module. Note that we refer to this shallow ConvNet as head ConvNet since it is the last sub-network specific to the task <ref type="bibr" target="#b35">[36]</ref>. The responses from the temporal attention module are incorporated into the head ConvNet to adjust the importance level of temporal features. Cross-entropy as used in <ref type="bibr" target="#b2">[3]</ref> is taken as the video level loss function.</p><p>To recognize the action class in a video, the importance level of different frames differs. Some frames are more likely to be irrelevant or less relevant to the action category and may hurt the final performance by introducing noise. Some other frames are more relevant to the action category. Take the action of handshake as an example, the frames with two people approaching are less relevant to the action, which could be shared by other action types, while the frames with two people's hands holding together give more discriminative information. Therefore, we introduce a temporal attention model for learning and determining the importance levels.</p><p>As illustrated in <ref type="figure">Figure IV</ref>, the feature mapsF i of the i th layer of the head ConvNet after enforcing the temporal attention are described as:</p><formula xml:id="formula_1">F i = A i (M) • F i (M),<label>(2)</label></formula><p>where F i (M) denotes the output feature maps of the i th layer of the head ConvNet, A i (M) is the attention map from the attention model, • denotes entrywise product. Here A i (M) = a i (M) ⊗ 1, where ⊗ is the Kronecker product and 1 denotes the all-ones vector, a i (M) is the learned attention vector with the dimension being related with the total number of frames.</p><p>In other words, A i (M) is the column-wise repeat of a i (M).</p><p>Note that for the 0 th layer, F 0 (M) = M, a 0 (M) ∈ R . a i (M) is the max pooling result of a i−1 (M) with a stride of 2. The detailed network designs will be described in Section VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. OVERALL FRAMEWORK</head><p>We take the two-stream based ConvNets as our backbone ConvNets and embed the proposed TSM operation followed by a head ConvNet with temporal attention model, for the videolevel classification. <ref type="figure">Figure 2</ref> shows the overall flowchart of our final framework.</p><p>Temporal Segment Networks (TSN) <ref type="bibr" target="#b2">[3]</ref> with BN-Inception structure <ref type="bibr" target="#b36">[37]</ref> provides superior performance on both the spatial and temporal streams. We take the TSN as our backbone ConvNets for frame feature extraction. The network contains two streams: spatial stream with RGB image as input, and temporal stream with optical flow as input. The results from the two streams are fused to predict the video label.</p><p>Without loss of generality, we take the spatial stream as example to describe our overall network structure. The temporal stream acts similarly. For successive of frames in a video, the backbone spatial ConvNet outputs feature maps for each frame. With the feature maps of each frame vectorized to a feature vector, the feature vectors of the successive frames are arranged row-by-row to form a VideoMap. The VideoMap goes through the head ConvNet with temporal attention and the class scores are generated. The Temporal-Spatial Mapping operation permits the end to end training of the entire network. Due to memory constraints, in practice, we train the networks in two stages. In the first stage, we train the backbone ConvNets. Then we train the the head ConvNet for VideoMap classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXPERIMENTS</head><p>We validate the effectiveness of the proposed framework on two benchmark datasets. We first describe the datasets and implementation details. Then we study the effects of different factors in our network. Finally, we compare our approach with many state-of-the-art approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>We conduct experiments on two popular human action recognition datasets, namely HMDB51 <ref type="bibr" target="#b37">[38]</ref> and UCF101 <ref type="bibr" target="#b38">[39]</ref>. The HMDB51 dataset is a very challenging dataset with higher intra-class variations and smaller inter-class variations. The videos are collected from movies and a variety of YouTube consumer videos. This dataset consists of 6,766 video clips from 51 action categories, with each category containing at least 100 clips. In each split, each action class has around 70 clips for training and 30 clips for testing. The UCF101 dataset consists of 13320 video clips in 101 categories. This dataset provides large diversity in terms of actions, variations in background, illumination, camera motion and viewpoints, as well as object appearance, scale and pose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>Two-stream Backbone ConvNets. We pre-train our backbone network of the TSN the same way as reported in <ref type="bibr" target="#b2">[3]</ref>. The size of input images or optical flow stacks is fixed at 256 × 340. In order to avoid over-fitting, we perform data augmentation, by cropping images with the width and height chosen from four different sizes, 256, 224, 192 and 168, from five spatial locations of the full image, i.e., one center and four corners. These cropped regions will be resized to 224 × 224 for feature extraction. Note that, to maintain spatial consistency across a video, the cropped size and location are the same across a video sample. Temporal-Spatial Mapping and head ConvNet. For the k th frame, the top inception module of the TSN outputs feature maps S k of size h×w ×c, where h = 7,w = 7, and c = 1024. An average pooling on each channel vectorizes them to a 1024dimension vector f k . For T sequential frames, a VideoMap M = [f 1 ; f 2 ; · · · ; f T ] is obtained.</p><p>Video level feature learning and classification based on the VideoMap is performed using our head ConvNet with temporal attention. We build the head ConvNet by stacking three convolution blocks. In each block, it consists of a convolutional layer with 5×5 kernels, a ReLU layer, followed by a pooling layer of 3×3 sized kernel of stride 2. We construct the temporal attention module by two such convolution blocks followed by a fully connected layer which outputs a T dimensional vector, representing the frame-wise attention responses for a video. Since the GPU memory is limited, we set the mini-batch to have 128 VideoMaps. We set the initial base learning rate to 0.01 and decrease it by a factor of 10 every 10,000 iterations. We stop the training process after 100 epochs. Vectorization of Feature Maps. For the high level convolution feature maps, the average pooling (e.g., BN-Inception <ref type="bibr" target="#b2">[3]</ref>) or full connection (e.g., AlexNet <ref type="bibr" target="#b1">[2]</ref>) of the feature maps has been done in the ConvNets structure, to convert them to a low dimensional feature vector. Therefore, we directly utilize such feature vectors from the sequential frames to form a VideoMap.</p><p>Given the feature maps of high dimension from a ConvNet layer, if the feature maps are not already transformed to a feature vector, a module for vectorization of the feature maps is needed to convert the feature maps to a low dimensional feature vector representation. There are many ways to perform to perform the vectorization, e.g., leveraging a ConvNet to encode the feature maps to a feature vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation Study</head><p>In this subsection, we will analyze the effectiveness of the proposed Temporal-Spatial Mapping component, discuss the design of the head ConvNet, analyze the effectiveness of the temporal attention module, and the influence of the height of the VideoMap (i.e., density of temporal sampling), respectively. Effectiveness of Temporal-Spatial Mapping. Aggregation of single-frame features of dense frames to a VideoMap provides the opportunity to jointly explore the temporal-spatial dynamics at the video level. We show the performance of our scheme in comparison with the baseline scheme on the HMDB51 dataset (Split 1) in <ref type="table" target="#tab_1">Table I</ref>. In <ref type="table" target="#tab_1">Table I</ref>, "TSN <ref type="bibr" target="#b2">[3]</ref>" denotes the results of the TSN approach <ref type="bibr" target="#b2">[3]</ref> with the BN-Inception structure, serving as our baseline scheme. We take its ConvNets as our backbone network to extract frame level features. "TSN+TSM (Ours)" denotes our scheme with the Temporal-Spatial Mapping operation and a head ConvNet for the joint temporal dynamics exploration from densely sampled frames. We can see that our scheme achieves 1.6% after the two-stream fusion. Note that in <ref type="table" target="#tab_1">Table I and Table II</ref>, TSN results are obtained from the original TSN model but ran based on a new Caffe version on Windows. <ref type="bibr" target="#b0">1</ref> We also evaluate the performance when using another backbone network, which takes two-stream ConvNets with VGG-16 network structure <ref type="bibr" target="#b4">[5]</ref> as the frame-level feature extractor. Similarly, TSM brings 1.8% improvement in accuracy. Comparisons on Network Designs. We have designed a head ConvNet for encoding the VideoMap for action recognition. The purpose of this network is to explore temporal dynamics among frames to learn efficient video feature representation. For the head ConvNet with VideoMap as input, we have tried three network structures: AlexNet, our designed 3-layer ConvNet, and 1-layer ConvNet. In addition, one alternative way for temporal modeling of the feature vectors is to use the Recurrent Neuron Network with LSTM. However, RNN suffers from the gradient vanishing problem even though LSTM suffers less than RNN. We show the results of these designs in <ref type="table" target="#tab_1">Table II</ref> with experiments conducted on the HMDB51 dataset (Split 1). We can see that our 3-layer ConvNet (TSN+TSM (3layer ConvNet)) achieves much superior performance than the LSTM based network. The performance of AlexNet is inferior to the 3-layer ConvNet since a deeper network with more parameters is prone to over-fitting. The 1-layer ConvNet does not converge in modeling the temporal-spatial dynamics. Effectiveness of Temporal Attention. Different frames generally have different importance levels for recognizing the action. The less relevant frames or irrelevant frames could be noise which hurts the final performance. We have designed a temporal attention module with hierarchical attentions as shown in <ref type="figure">Figure IV</ref> (which is <ref type="figure">Figure 3</ref> in the paper) and found that the hierarchical structure provides superior performance.  <ref type="bibr" target="#b2">[3]</ref>. In our scheme, the performance increases as the sampling density increases. In TSN, however, the performance does not increase as the sampling density increases.   <ref type="table" target="#tab_1">Table III</ref> shows the comparisons with the designs having fewer attention branches. "w Attn. (A 0 )" denotes that only the attention A 0 is applied in the input (see <ref type="figure">Figure IV)</ref>. "w Attn. (A 1 &amp; A 2 )" denotes the attentions A 1 and A 2 are applied after the first layer and the second layer of the shallow CNN. "w Attn. (A 0 &amp; A 1 &amp; A 2 )" denotes our final scheme with all the three branches of attentions included. We can that the hierarchical attention structure provides superior performance, and when our attention model is enabled (w/ Attn.) with experiments conducted on the HMDB51 dataset (Split 1), the performance can be improved by 0.8%, demonstrating the effectiveness of the attention mechanism.</p><p>Influence of the Density of Temporal Sampling. Videos of different lengths will generate VideoMaps of different height. We can aggregate the feature vectors of dense frames of a video to form a VideoMap. In practice, for the convenience of learning, we densely sample the video frames to have a fixed number of frames to form the VideoMap of fixed height. In the training, we set the number of frames as 256 in considering the average length of videos. In order to measure the influence of temporal frame sampling density during testing, we have compared the performance under different temporal sampling densities, i.e., 25, 32, 64, 128, and 256 frames per video on the HMDB51 dataset (Split 1) and show the results in <ref type="figure" target="#fig_2">Figure 5</ref> (a). We can see that the performance increases as the sampling density increases. This is consistent with the human perception. Our method provides a way to jointly explore the inter-frame dynamics. In contrast, for the TSN model <ref type="bibr" target="#b2">[3]</ref>, the increase of the number of frames in the test will not increase the performance, (b1)(b2) VideoMap. (c1)(c2) Visualization from our ConvNet of the Conv-3 layer using approach Grad-CAM <ref type="bibr" target="#b39">[40]</ref>. Note that images in (b1)(c1)(b2)(c2) are resized and horizontal axis denotes the time here. We can see that Grad-CAM map presents higher responses over temporal segments corresponding to the frames when the persons are doing the corresponding actions. and the performance saturates at 25 frames as shown in <ref type="figure" target="#fig_2">Figure  5</ref> (b). This is because the TSN-like approaches (e.g., TSN <ref type="bibr" target="#b2">[3]</ref>, TLE <ref type="bibr" target="#b17">[18]</ref>) explore the temporal dynamics by simply averaging/multiplying the scores/features of the frames. The statistical information rather than the per-frame detailed information is explored for recognition. Then, more frames above a certain number of frames help little when they are enough to represent the statistical information. While, the performance of our model increases with the increase of frame density (see <ref type="figure" target="#fig_2">Figure 5</ref> (a)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Comparison with the State-of-the-art</head><p>We compare our proposed scheme with the state-of-theart approaches for video action recognition in <ref type="table" target="#tab_1">Table IV</ref>. We evaluate the performance on the HMDB51 and the UCF101 dataset. For both datasets, we use the provided evaluation protocol and report the mean average accuracy over the three splits. We can see our scheme achieves the best performance, with 72.7% on the HMDB51 dataset and 94.3%, on the UCF101 dataset. In comparison with the TSN, we achieve 4.2% improvement on the HMDB51 dataset.</p><p>Compared with the HMDB51 dataset, the accuracy improve- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>HMDB51 UCF101 Slow Fusion CNN <ref type="bibr" target="#b40">[41]</ref> -65.4 Two-Stream CNN (VGG16) <ref type="bibr" target="#b4">[5]</ref> 58.5 91.4 Two-Stream CNN (AlexNet) <ref type="bibr" target="#b1">[2]</ref> 59.4 88.0 Key Volume Mining <ref type="bibr" target="#b3">[4]</ref> 63.3 93.1 Two-Stream CNN Fusion <ref type="bibr" target="#b4">[5]</ref> 65.4 92.5 Spatiotemporal ResNets <ref type="bibr" target="#b41">[42]</ref> 66.4 93.4 TSN (BN-Inception) <ref type="bibr" target="#b2">[3]</ref> 68.5 94.0 Spatiotemporal Multiplier Nets <ref type="bibr" target="#b42">[43]</ref> 68.9 94.2 Spatiotemporal Pyramid Net <ref type="bibr" target="#b21">[22]</ref> 68.9 94.6 Fusion+iDT <ref type="bibr" target="#b4">[5]</ref> 69.2 93.5 ActionVLAD (VGG16)+iDT <ref type="bibr" target="#b27">[28]</ref> 69 ment on the UCF101 dataset is smaller. The performance of the UCF101 dataset is approaching saturation (&gt;94%) and it becomes difficult to demonstrate the effectiveness of an approach. We will perform further study on more challenging datasets in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Visualization</head><p>We make performance comparison for all the categories on the HMDB51 dataset to get better insights. <ref type="figure" target="#fig_3">Figure 6</ref> shows the top-25 classes that our approach outperforms TSN. For some action classes such as "Draw sword" and "Eat", our scheme outperforms TSN even by 20%. In TSN, "Draw sword" is easy to be mistaken as "Sword" and "Wave" since these actions usually share some common states like waving. <ref type="figure">Figure 7</ref> shows such an example. With our scheme capable of looking at dense frames with time order embedded rather than several sparse frames, the accuracy is improved by 23% for the class of "Draw sword". Similarly, "Eat" and "Drink" are prone to be confused and we find the video samples of the two classes usually have frames of similar states once the cup is away from the mouth. For the classes with time order, e.g., "Stand" versus "Sit" as shown in <ref type="figure">Figure 7</ref>, our scheme can capture the time order well thanks to the VideoMap representation and outperforms TSN.</p><p>In addition, <ref type="figure">Figure 9</ref> and <ref type="figure" target="#fig_0">Figure 10</ref> show partial of the confusion matrix corresponding to some easily confused categories, as referred in Section 6.4. In <ref type="figure">Figure 9</ref>, for the approach of Temporal Segment Networks (TSN) <ref type="bibr" target="#b2">[3]</ref>, "Draw sword" is easy to be mistaken as "Sword" and "Wave" where these actions usually share some common states like waving. Our proposed scheme performs much better than TSN since it is capable of looking at all frames and jointly making decision. In <ref type="figure" target="#fig_0">Figure 10</ref>, the proposed TSN+TSM method can distinguish "Sit", and "Stand" much better since the time order information is embedded in the VideoMap representation. There are some failure cases of our TSM, For instance, one of action "Climb" is mistaken as "Jump", and one of "Smoke" is mistaken as "Eat". The video appearances in the confused classes are similar. And we show two examples in <ref type="figure" target="#fig_0">Figure 11</ref>. For instance, "Climb" is mistaken as "Jump", and "Smoke" is mistaken as "Eat". The appearances in the confused classes are similar.  <ref type="figure" target="#fig_0">Fig. 11</ref>. Examples on the failure cases of the proposed model on the HMDB51 dataset (Split 1). The main reason for these failures comes from the similar appearances in the confusing classes.</p><p>Furthermore, we adopt Grad-CAM visualization technique <ref type="bibr" target="#b39">[40]</ref> to analyze our head ConvNet. Grad-CAM is a classdiscriminative localization technique, which can provide vi-sual explanations from the learned ConvNet model without requiring architectural changes or re-training. In <ref type="figure">Figure 8</ref>, we can see that Grad-CAM map presents higher responses over temporal segments corresponding to the frames that persons are doing the corresponding actions. For instance, in <ref type="figure">Figure  8</ref> (a1)-(c1), For the action of "stand up", there are some unrelated frames before the acting of standing up. In <ref type="figure">Figure 8</ref> (a2)-(c2), for the action "kick ball", there are high responses at the first 2/3 of the temporal duration and lower responses at the remaining time duration. The temporal response characteristics are similar to that for object detection/classification, where the regions being highly correlated with the actions/objects/classes having higher responses.</p><p>VII. CONCLUSION To model the temporal-spatial evolution in video for action recognition, we propose a simple yet effective operation, Temporal-Spatial Mapping (TSM), to enable the joint analysis of the dense frames of a video. We propose a video level 2D feature representation by transforming the convolutional features of a sequence to a VideoMap, where the temporal dynamic evolution is well embedded. We leverage a head ConvNet with temporal attention model to further explore the temporal-spatial dynamics in the VideoMap and learn effective video-level feature representation for classification. The experiment results show that the proposed scheme achieves the state-of-the-art performance, 72.7% and 94.3% on the HMDB51 and UCF101 dataset, respectively.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Overview of our network structure. The Temporal-Spatial Mapping operation enables the effective joint temporal-spatial modeling by representing the temporal spatial features of an entire video by a 2D VideoMap. A temporal attention model in a head ConvNet further transforms the VideoMap to a compact video-level feature embedding for classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .Fig. 3 .</head><label>23</label><figDesc>The overall framework with our Temporal-Spatial Mapping operation followed by a head ConvNet for action recognition. Two-stream ConvNets extract features on each frame for the spatial stream (RGB) and temporal stream (Optical flow), respectively. The vectorized feature vectors of the sequential frames form a VideoMap for temporal-spatial representation. A head ConvNet with temporal attention makes action classification based on the VideoMap. Finally the class scores of the VideoMaps from two streams are fused to produce the video-level prediction. Illustration of the proposed Temporal-Spatial Mapping operation, which transforms a sequence of feature maps into a compact VideoMap.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>Comparison of performance (accuracy %) at different frame sampling densities (number of frames: 25, 32, 64, 128, 256, respectively) for our method and the TSN on the HMDB51 dataset (Split 1). (a) Ours; (b) TSN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .</head><label>6</label><figDesc>Comparisons of Accuracy (%) for the top-25 classes on the HMDB51 dataset (Split 1) between our approach and the TSN model. Our approach consistently outperforms the TSN model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .Fig. 8 .</head><label>78</label><figDesc>Examples that our scheme succeeds in recognizing the action while TSN<ref type="bibr" target="#b2">[3]</ref> fails. Our scheme perform well mainly due to the Temporal-Spatial Mapping operation enables the joint exploration of temporal frames and the consideration of time order. Visualizations on a video of action "stand" ((a1)-(c1)) and "kick ball" ((a2)-(c2)). (a1)(a2) Video frames over time (only some frames are shown).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 9 .Fig. 10 .</head><label>910</label><figDesc>Comparison of partial confusion matrix with related categories of "Draw sword", "Fencing", "Sword", and "Wave". (a) TSN [1]; (b) Ours. Comparison of partial confusion matrix with related categories of "Sit", "Stand", "Turn", and "Walk". (a) TSN [1]; (b) Ours.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I COMPARISON</head><label>I</label><figDesc>WITH TWO-STREAM BASED NETWORKS IN ACCURACY (%) ON THE HMDB51 DATASET (SPLIT 1). HERE "TWO-STREAM" [2] USES VGG-16 AS THE NETWORK STRUCTURE, AND "TSN [3]" USES BN-INCEPTION. "TSN+TSM(OURS)" IS OUR SCHEME WITH THE TEMPORAL-SPATIAL MAPPING (TSM) FOLLOWED BY A HEAD CONVNET.</figDesc><table><row><cell>Method</cell><cell cols="3">RGB Optical Flow Fusion</cell></row><row><cell>Two-stream [5]</cell><cell>42.2</cell><cell>55.0</cell><cell>58.5</cell></row><row><cell>Two-stream+TSM (Ours)</cell><cell>43.1</cell><cell>56.2</cell><cell>60.3</cell></row><row><cell>TSN [3]</cell><cell>54.6</cell><cell>62.6</cell><cell>70.8</cell></row><row><cell>TSN+TSM (Ours)</cell><cell>55.0</cell><cell>63.1</cell><cell>72.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II COMPARISON</head><label>II</label><figDesc>ON THE NETWORK DESIGNS FOR EXPLORING TEMPORAL DYNAMICS FROM FEATURE VECTORS OF DENSELY SAMPLED FRAMES (IN ACCURACY %) ON THE HMDB51 DATASET (SPLIT 1). WE TAKE TSN [3]AS OUR BACKBONE FOR EXTRACTING FRAME LEVEL FEATURES.</figDesc><table><row><cell>Architecture</cell><cell cols="2">RGB Optical Flow</cell></row><row><cell>TSN [3]</cell><cell>54.6</cell><cell>62.6</cell></row><row><cell>TSN+3-layer LSTM</cell><cell>51.1</cell><cell>59.6</cell></row><row><cell>TSN+TSM (AlexNet)</cell><cell>51.7</cell><cell>58.0</cell></row><row><cell>TSN+TSM (3-layerConvNet)</cell><cell>55.0</cell><cell>63.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III</head><label>III</label><figDesc></figDesc><table><row><cell cols="4">ACCURACY (%) OF TWO-STREAM BASED NETWORK TSN+TSM WITHOUT (W/O ATTN.) AND WITH (W/O ATTN.) TEMPORAL ATTENTION ON THE</cell></row><row><cell cols="3">HMDB51 DATASET (SPLIT 1).</cell><cell></cell></row><row><cell>Model</cell><cell>RGB</cell><cell>Optical Flow</cell><cell>Fusion</cell></row><row><cell>w/o Attn.</cell><cell>55.0</cell><cell>63.1</cell><cell>72.4</cell></row><row><cell>w Attn. (A0)</cell><cell>54.6</cell><cell>63.3</cell><cell>72.4</cell></row><row><cell>w Attn. (A1 &amp; A2)</cell><cell>55.0</cell><cell>63.3</cell><cell>72.7</cell></row><row><cell>w Attn. (A0 &amp; A1 &amp; A2)</cell><cell>55.2</cell><cell>63.3</cell><cell>73.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV PERFORMANCE</head><label>IV</label><figDesc>COMPARISONS (IN ACCURACY %) OF OUR METHOD WITH THE OTHER STATE-OF-THE-ART METHODS OVER ALL THE THREE SPLITS.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">• This TSM operation is a general operation which can be applied to the feature maps/features from ConvNets for encoding the temporal-spatial dynamics from a sequence of frames.• This TSM operation for obtaining a VideoMap can maintain the time order information of the dense frames, which helps distinguishing action categories related with occurrence order, e.g., "stand up" versus "sit down". • This TSM operation is simple yet effective. It does not involve complicated operations.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The results reported in website (http://yjxiong.me/others/tsn/#exp, spatial stream, temporal stream and the final fusion are 54.4%, 62.4% and 69.5%, respectively) of the original TSN are a little different from our ran results due to the difference of Caffe version on Windows.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Human activity analysis: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A key volume mining deep framework for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1991" to="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1933" to="1941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yue-Hei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4694" to="4702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Action recognition using visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.04119</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Videolstm convolves, attends and flows for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.01794</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-stream multiclass fusion of deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM on Multimedia Conference</title>
		<meeting>the ACM on Multimedia Conference</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="791" to="800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Hierarchical attention network for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>O&amp;apos;hare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06416</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.08242</idno>
		<title level="m">Yolo9000: Better, faster, stronger</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep temporal linear encoding networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Diba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2329" to="2338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">3d convolutional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="221" to="231" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Action recognition by learning deep multi-granular spatio-temporal video representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM on International Conference on Multimedia Retrieval</title>
		<meeting>the ACM on International Conference on Multimedia Retrieval</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="159" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Spatiotemporal pyramid network for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep temporal linear encoding networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Diba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3551" to="3558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Aggregating local image descriptors into compact codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">1704</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Image classification with the fisher vector: Theory and practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="222" to="245" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Two-stream dictionary learning architecture for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="567" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Actionvlad: Learning spatio-temporal aggregation for action classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3165" to="3174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A discriminative cnn video representation for event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1798" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Action recognition with stacked fisher vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="581" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Human action recognition using factorized spatio-temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4597" to="4605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sequential deep trajectory descriptor for action recognition with three-stream cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1510" to="1520" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Two-stream 3-d convnet fusion for action recognition in videos with arbitrary size and length</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="634" to="644" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unified spatio-temporal attention networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Hmdb: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2556" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Spatiotemporal residual networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3468" to="3476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Spatiotemporal multiplier networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4768" to="4777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Embedding sequential information into spatiotemporal features for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1110" to="1118" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
