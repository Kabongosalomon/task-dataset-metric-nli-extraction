<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">3D Multi-Object Tracking: A Baseline and New Evaluation Metrics</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinshuo</forename><surname>Weng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianren</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Held</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Kitani</surname></persName>
						</author>
						<title level="a" type="main">3D Multi-Object Tracking: A Baseline and New Evaluation Metrics</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>3D multi-object tracking (MOT) is an essential component for many applications such as autonomous driving and assistive robotics. Recent work on 3D MOT focuses on developing accurate systems giving less attention to practical considerations such as computational cost and system complexity. In contrast, this work proposes a simple real-time 3D MOT system. Our system first obtains 3D detections from a LiDAR point cloud. Then, a straightforward combination of a 3D Kalman filter and the Hungarian algorithm is used for state estimation and data association. Additionally, 3D MOT datasets such as KITTI evaluate MOT methods in the 2D space and standardized 3D MOT evaluation tools are missing for a fair comparison of 3D MOT methods. Therefore, we propose a new 3D MOT evaluation tool along with three new metrics to comprehensively evaluate 3D MOT methods. We show that, although our system employs a combination of classical MOT modules, we achieve state-of-the-art 3D MOT performance on two 3D MOT benchmarks (KITTI and nuScenes). Surprisingly, although our system does not use any 2D data as inputs, we achieve competitive performance on the KITTI 2D MOT leaderboard. Our proposed system runs at a rate of 207.4 FPS on the KITTI dataset, achieving the fastest speed among all modern MOT systems. To encourage standardized 3D MOT evaluation, our system and evaluation code are made publicly available at https://github.com/xinshuoweng/AB3DMOT.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>MOT is an essential component for many real-time applications such as autonomous driving <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref> and assistive robotics <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>. Due to advancements in object detection <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b7">[8]</ref>, there has been much progress on MOT. For example, for the car class on the KITTI <ref type="bibr" target="#b8">[9]</ref> 2D MOT benchmark, the MOTA (multi-object tracking accuracy) has improved from 57.03 <ref type="bibr" target="#b9">[10]</ref> to 84.04 <ref type="bibr" target="#b10">[11]</ref> in just two years! While we are encouraged by the progress, we observed that our focus on innovation and accuracy has come at the cost of practical factors such as computational efficiency and system simplicity. State-of-the-art methods typically require a large computational cost <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b14">[15]</ref> making real-time performance a challenge. Also, modern MOT systems are often very complex and it is not always clear which part of the system contributes the most to performance. For example, leading works <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b15">[16]</ref> have substantially different system pipelines but only minor differences in performance. In these cases, modular comparative analysis is quite challenging.</p><p>To provide a standard 3D MOT baseline for comparative analysis, we implement a classical approach which is both efficient and simple in design -the Kalman filter <ref type="bibr">[17] (1960)</ref> coupled with the Hungarian method <ref type="bibr" target="#b17">[18]</ref>  <ref type="bibr">(1955)</ref>. Specifically, our system employs an off-the-shelf 3D object detector to obtain 3D detections from the LiDAR point cloud <ref type="bibr" target="#b5">[6]</ref>. Then, a combination of the 3D Kalman filter (with a constant velocity model) and the Hungarian algorithm is used for state estimation and data association. Unlike other filterbased MOT systems which define the state space of the filter in the 2D space <ref type="bibr" target="#b18">[19]</ref> or bird's eye view <ref type="bibr" target="#b19">[20]</ref>, we extend the state space of the objects to the 3D space, including 3D location, 3D size, 3D velocity and heading orientation.</p><p>Our empirical results are alarming. While the combination of modules in our system is straightforward, we achieve state-of-the-art 3D MOT performance on standard 3D MOT datasets: KITTI and nuScenes. Surprisingly, although our system does not use any 2D data as inputs, we also achieve competitive performance on the KITTI 2D MOT leaderboard as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. We hypothesize that the strong 2D MOT performance of our 3D MOT system may be due to the fact that tracking in 3D can better resolve depth ambiguities and lead to fewer mismatches than tracking in 2D. Also, due to efficient design of our system, it runs at a rate of 207.4 FPS on the KITTI dataset, achieving the fastest speed among modern MOT systems. To be clear, the contribution of this work is not to innovate 3D MOT algorithms but to provide a more clear picture of modern 3D MOT systems in comparison to a most basic yet strong baseline, the results of which are important to share across the community.</p><p>In addition to the 3D MOT system, we also observed two issues in 3D MOT evaluation: (1) Standard MOT benchmarks such as the KITTI dataset only supports 2D MOT evaluation, i.e., evaluation on the image plane. A tool to evaluate 3D MOT systems in 3D space is not currently available. On the KITTI dataset, the convention to evaluate 3D MOT methods is to project the 3D MOT results to the image plane and then use the KITTI 2D MOT evaluation tool. However, we believe that this will hamper the future progress of 3D MOT systems as evaluation on the image plane cannot provide a fair comparison of 3D MOT methods, e.g., a system that achieves better tracking in 3D does not necessarily have higher performance in 2D MOT evaluation. To overcome the issue, we propose an MOT evaluation tool that evaluates MOT systems directly in 3D space using 3D metrics; <ref type="bibr" target="#b1">(2)</ref> Common MOT metrics such as MOTA and MOTP do not consider the confidence score of tracked objects. As a result, users must manually select a threshold and filter out tracked objects with lower scores. However, selecting the best threshold requires non-trivial efforts. Also, evaluation at a single threshold prevents us from understanding the full spectrum of accuracy and precision of a MOT system. To address the issue, we propose three new integral metrics to summarize the performance of MOT methods across many thresholds. We hope that our new evaluation tool including metrics will serve as a standard for future 3D MOT evaluation. Our contributions are summarized as follows:</p><p>1) We propose an accurate real-time 3D MOT system based on a 3D Kalman filter for online applications; 2) We propose a new 3D MOT evaluation tool along with three new metrics to standardize 3D MOT evaluation; 3) Our 3D MOT system achieves S.O.T.A. performance and the fastest speed on standard 3D MOT datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>2D Multi-Object Tracking. Recent 2D MOT systems can be split into batch and online methods based on data association. Batch methods attempt to find the global optimal association from the entire sequence. These methods often create a network flow graph and can be solved by the min-cost flow algorithms <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>. In contrast, online methods only require the information up to the current frame and are applicable for online applications. Online methods often formulate data association as a bipartite graph matching problem and solve it using the Hungarian algorithm <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>. Beyond using the Hungarian algorithm, modern online methods design deep association networks <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b22">[23]</ref> that can construct the association using neural networks. Our proposed system falls into the category of online methods. For simple design and real-time efficiency, we do not use neural networks and only adopt the Hungarian algorithm. To achieve data association, designing appropriate cost functions to measure similarity is crucial to a MOT system. Early work <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b23">[24]</ref> employs hand-crafted features such as spatial distance and color histograms as the cost function. Modern methods often use the motion model <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref> and the appearance feature <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>. For system simplicity, we only employ the simplest motion model, i.e., constant velocity, while not using any appearance cue.</p><p>3D Multi-Object Tracking. 3D MOT systems often share the same components as 2D MOT systems. The distinction lies in that the input detections are in the 3D space instead of the image plane. Therefore, 3D MOT systems can obtain the motion and appearance information in the 3D space without perspective distortion. <ref type="bibr" target="#b15">[16]</ref> proposed to estimate the distance of objects to the camera and their velocity in the 3D space as the motion cue. <ref type="bibr" target="#b19">[20]</ref> used an unscented Kalman filter to estimate the linear and angular velocity on the ground. <ref type="bibr" target="#b28">[29]</ref> proposed a 2D-3D Kalman filter to utilize the observation from the image and 3D world. Beyond using hand-crafted features, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b29">[30]</ref>- <ref type="bibr" target="#b31">[32]</ref> used neural networks to learn the 3D appearance and motion features from data. Unlike prior work uses various 3D features and has complex systems, we only use a 3D Kalman filter to obtain the 3D motion cue for simplicity and efficiency, with extending the state space of the filter to full 3D domain including 3D location, 3D velocity, 3D size and heading orientation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. APPROACH</head><p>The goal of 3D MOT is to associate 3D detections in a sequence. As our system is an online MOT system, at every timestamp, we only require detections in the current frame and associated trajectories from the previous frames. Our system pipeline is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>: (A) a 3D detection module is used to obtain 3D detections from the LiDAR point cloud; (B) a 3D Kalman filter predicts the state of associated trajectories from the previous frames to the current frame; (C) a data association module matches the predicted trajectories from Kalman filter and detections in the current frame; (D) the 3D Kalman filter updates the state of matched trajectories based on the matched detections; (E) a birth and death memory creates trajectories for new objects and deletes trajectories for disappeared objects. Except for the pre-trained 3D detection module, our 3D MOT system does not need any training and can be directly used for inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. 3D Object Detection</head><p>Thanks to advancements in 3D object detection, we have access to high-quality detections. Here, we experiment with <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b32">[33]</ref> on KITTI and <ref type="bibr" target="#b33">[34]</ref> on nuScenes. We directly use their pre-trained models on the corresponding dataset. In frame t, the output of 3D detection module is a set of detections D t = {D 1 t , D 2 t , · · · , D nt t } (n t is the number of detections). Each detection D j t , where j ∈ {1, 2, · · · , n t }, is represented as a tuple (x, y, z, θ, l, w, h, s), including location of the object center in the 3D space (x, y, z), object's 3D size (l, w, h), heading angle θ and confidence score s. We will show how different 3D detection modules affect the performance of our 3D MOT system in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. 3D Kalman Filter: State Prediction</head><p>To predict the state of object trajectories from the previous frames to the current frame, we approximate objects' interframe displacement using a constant velocity model independent of camera ego-motion. That means we do not explicitly estimate the ego-motion but rely on our motion model to accommodate both the ego-motion and motion of the other objects. We formulate the state of an object trajectory as a 11-dimensional vector T = (x, y, z, θ, l, w, h, s, v x , v y , v z ), where the additional variables v x , v y , v z represent the object velocity in the 3D space. Note that we do not include the angular velocity v θ in the state space for simplicity as  we empirically found that including the angular velocity does not really improve the performance. In every frame, the state of associated trajectories from the previous frame</p><formula xml:id="formula_0">T t−1 ={T 1 t−1 , T 2 t−1 , · · · , T mt−1 t−1 } (m t−1</formula><p>is the number of trajectories in the frame t-1) will be propagated to the frame t as T est , based on the constant velocity model:</p><formula xml:id="formula_1">x est = x + v x , y est = y + v y , z est = z + v z . (1) As a result, for every trajectory T i t−1 in T t−1 where i ∈ {1, 2, · · · , m t−1 }, the predicted state in the frame t is T i est = (x est , y est , z est , θ, l, w, h, s, v x , v y , v z ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Data Association</head><p>To match the predicted trajectories T est with the detections D t , we first construct the affinity matrix with a dimension of m t−1 ×n t by computing the 3D Intersection of Union (IoU) or negative center distance between every pair of the trajectory T i est and detection D j t . Then, the data association becomes a bipartite graph matching problem, which can be solved in polynomial time using the Hungarian algorithm <ref type="bibr" target="#b17">[18]</ref>. Also, we reject a matching if the 3D IoU is less than a threshold IoU min (or the center's distance is larger than a threshold dist max if using center distance to compute affinity matrix). The outputs of data association are as follows:</p><formula xml:id="formula_2">T match = {T 1 match , T 2 match , · · · , T wt match }, (2) D match = {D 1 match , D 2 match , · · · , D wt match },<label>(3)</label></formula><formula xml:id="formula_3">T unmatch = {T 1 unmatch , T 2 unmatch , · · · , T mt−1−wt unmatch },<label>(4)</label></formula><formula xml:id="formula_4">D unmatch = {D 1 unmatch , D 2 unmatch , · · · , D nt−wt unmatch },<label>(5)</label></formula><p>where T match and D match are the matched trajectories and detections and w t denotes the number of matches. Also, T unmatch and D unmatch are the unmatched trajectories and detections. Note that, T unmatch is the complementary set of T match in T est . Similarly, D unmatch is the complementary set of D match in D t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. 3D Kalman Filter: State Update</head><p>To account for the uncertainty of state prediction, we update the state of each trajectory in T match based on its corresponding detection in D match . As a result, we obtain the final associated trajectories in frame t as T t ={T 1 t , T 2 t , · · · , T wt t }. Following the Bayes rule, the updated state of each trajectory is nearly opposite to the orientation of the corresponding trajectory T k match , i.e., differ by π. Although we know that this is impossible because objects should move smoothly and cannot change the orientation by π in one frame (i.e., 0.1s in KITTI), the prediction of the orientation in either the detection or the trajectory can be wrong, making this scenario possible. As a result, if we follow the normal state update rule, the final trajectory T k t in this case will have an orientation somewhere in the middle of the orientation of D k match and T k match , which will lead to a low 3D IoU between the associated trajectory and the ground truth. To prevent this issue, we propose an orientation correction technique. When the difference of the orientation θ d between D k match and T k match is greater than π 2 , we add a π to the orientation in T k match so that θ d is always less than π 2 , i.e., the orientation D k match and T k match are roughly consistent without substantial change.</p><formula xml:id="formula_5">T k t =(x , y , z , θ , l , w , h , s , v x , v y , v z ), where k ∈ {1, 2, · · · , w t },</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Birth and Death Memory</head><p>As tracked objects might leave the scene and new objects might enter the scene, a module to manage the birth and death of the objects is necessary. On one hand, we consider all unmatched detections D unmatch as potential new objects entering the scene. However, to avoid creating false positive trajectories, a new trajectory T p new will not be created for the unmatched detection D p unmatch until D p unmatch has been continually matched in the next Bir min frames, where p ∈ {1, 2, · · · , n t − w t }. Once the new trajectory T p new is created, we initialize its state same as its most recent detection D p unmatch with zero velocity for v x , v y and v z . On the other hand, we consider all unmatched trajectories T unmatch as potential objects leaving the scene. However, to prevent deleting true positive trajectories that still exist in the scene but cannot find a match due to missing detection, we keep tracking each unmatched trajectory T q unmatch for Age max frames before ensuring T q unmatch is a disappeared trajectory T q lost , where q ∈ {1, 2, · · · , m t−1 − w t }, and deleting it from the set of associated trajectories. Ideally, true positive trajectories with missing detection can be interpolated by our 3D MOT system without being deleted, and only the trajectories that leave the scene are deleted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. NEW 3D MOT EVALUATION TOOL</head><p>As the pioneering 3D MOT benchmark, KITTI <ref type="bibr" target="#b8">[9]</ref> dataset is crucial to the progress of 3D MOT systems. Though the KITTI dataset provides 3D object trajectories but it only supports 2D MOT evaluation, i.e., evaluation on the image plane, and a tool to evaluate 3D MOT systems directly in 3D space is not currently available. On the KITTI dataset, the current convention of evaluating 3D MOT systems is to project the 3D tracking results to the image plane and then use the KITTI 2D MOT evaluation tool, which matches the projected tracking results with ground truth trajectories on the image plane using 2D IoU as the cost function. However, we believe this will hamper the future progress of 3D MOT systems as evaluating on the image plane cannot provide a fair comparison of 3D MOT systems. For example, a system that outputs 3D trajectories with wrong depth estimates and low 3D IoU with the ground truth can still obtain high performance in 2D MOT evaluation as long as the projection of 3D trajectory outputs on the image plane has high 2D IoU with the ground truth on the image plane.</p><p>To provide a fair comparison of 3D MOT systems, we implement an extension to the KITTI 2D MOT evaluation tool for 3D MOT evaluation. Specifically, we modify the cost function from 2D IoU to 3D IoU and match the 3D tracking results with 3D ground truth trajectories directly in 3D space. In this way, we no longer need to project our 3D tracking results to the image plane for evaluation. For every tracked object, its 3D IoU with ground truth is required to be above a threshold IoU thres (or center distance must be below a threshold Dist thres ) in order to be considered as a successful match. Although the extension of our 3D MOT evaluation tool is straightforward, we hope that it can serve as a standard to evaluate future 3D MOT systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. NEW MOT EVALUATION METRICS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Limitation of the CLEAR Metrics</head><p>Conventional MOT evaluation is based on CLEAR metrics <ref type="bibr" target="#b34">[35]</ref> such as MOTA (see Section VI-A for details), MOTP, FP, FN, Precision, F1 score, IDS, FRAG. However, none of these metrics explicitly consider the object's confidence score s. In other words, the CLEAR metrics consider all object trajectories having the same confidence s=1, which is an unreasonable assumption because there could be many false positive trajectories with low confidence scores. Therefore, to reduce the number of false positives and achieve a high MOTA 1 , users must manually select a threshold and filter out tracked objects with confidence scores lower than the threshold prior to submitting the results for evaluation.</p><p>Our observations to the above evaluation are two-fold: (1) selecting the best threshold for a 3D MOT system requires non-trivial efforts from the users and the confidence threshold <ref type="bibr" target="#b0">1</ref> MOTA is the primary metric for ranking in most MOT benchmarks.</p><p>can be significantly different if a 3D MOT system changes its input detections or is being evaluated on a different dataset. As a result, users must run extensive experiments on the validation set to tune the confidence threshold; (2) using a single confidence threshold for evaluation prevents us from understanding how the performance of 3D MOT systems changes as a function of the threshold. In fact, we observed that different confidence thresholds can significantly affect the performance of the CLEAR metrics. For example, we show the performance of our system on three metrics at different thresholds in <ref type="figure" target="#fig_2">Fig. 3</ref> using the data from the car subset of the KITTI MOT dataset. To generate the results, we first sort the tracking results based on the confidence score s 2 . Then, we define a set of confidence thresholds based on the recall of our system between 0 to 1 with an interval of 0.025. This results in 40 confidence thresholds excluding the confidence threshold which corresponds to the recall of 0. For each confidence threshold, we evaluate the results using only trajectories with confidence higher than the threshold. We show that, in <ref type="figure" target="#fig_2">Fig. 3 (a)</ref>, the confidence threshold should not be very small (recall not very high) because the number of false positives will increase drastically, especially when the recall reaches 0.95. Also, in <ref type="figure" target="#fig_2">Fig. 3 (b)</ref>, the confidence threshold should not be very large, i.e., recall should not be very small, as it results in a large number of false negatives. As a result, in <ref type="figure" target="#fig_2">Fig. 3 (c)</ref>, we observed that the highest MOTA value is achieved only when we choose a confidence threshold corresponding to the recall of 0.9 which balances the false positives and false negatives.</p><p>Based on the above observations, we believe that using a single confidence threshold for evaluation requires non-trivial efforts from users, and more importantly, prevents us from understanding full spectrum of accuracy of a MOT system. One consequence is that a MOT system with a high MOTA at a single threshold and low MOTA at other thresholds can be still ranked high on the leaderboard. But ideally, we should aim to develop MOT systems that achieve high MOTA across many thresholds, i.e., 3D MOT systems that achieve high performance when using different detections as inputs. Prior work <ref type="bibr" target="#b35">[36]</ref> shares the same spirit with us in that <ref type="bibr" target="#b35">[36]</ref> also believes it is important to understand the performance of MOT systems at many operating points. Specifically, <ref type="bibr" target="#b35">[36]</ref> computes a MOTA matrix at different recall and precision values, similar to our MOTA-over-recall curve. The distinction lies in that, we additionally propose integral metrics (see Sec. V-B) that summarize the performance at many operating points into a single scalar for easy comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Integral Metrics: AMOTA and AMOTP</head><p>To deal with the issue that current MOT evaluation metrics do not consider the confidence and only evaluate at a single threshold, we propose two integral metrics -AMOTA and AMOTP (average MOTA and MOTP) -to summarize the performance of MOTA and MOTP across many thresholds. The AMOTA and AMOTP are computed by integrating Similar to other integral metrics such as the average precision used in object detection, we approximate the integration with a summation over a discrete set of recall values. Specifically, given the original definition of the MOTA metric from <ref type="bibr" target="#b34">[35]</ref>:</p><formula xml:id="formula_6">MOTA = 1 − FP + FN + IDS num gt ,<label>(6)</label></formula><p>where num gt is the number of ground truth objects in all frames. The AMOTA is then defined as follows:</p><formula xml:id="formula_7">AMOTA = 1 L r∈{ 1 L , 2 L ,··· ,1} (1 − FP r + FN r + IDS r num gt ), (7)</formula><p>where FP r , FN r and IDS r are the number of false positives, false negatives and identity switches computed at a specific recall value r. Also, L is the number of recall values (number of confidence thresholds for integration). The higher L is, more accurate the approximate integration can be. However, a large L requires significant compute during evaluation. To balance the accuracy and speed, we use 40 recall values (i.e., from 0% to 100% with an interval of 2.5% excluding 0%), i.e., L=40. For a 3D MOT system which has a maximum recall of r m less than 100%, the MOTA values for integration beyond r m are 0. As a result, our proposed metrics are biased towards high-recall systems. We believe that this bias is acceptable as having a high recall is crucial to prevent collision for autonomous systems in practice. Note that our proposed AMOTA metric is similar to the PR-MOTA metric proposed in the independent work <ref type="bibr" target="#b36">[37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Scaled Accuracy Metric: sAMOTA</head><p>Conventionally, an integral metric such as average precision is a percentage ranging from 0% to 100% so that it is easy to measure the absolute performance of the system. To ensure that the integral metric has a range between 0% and 100%, the metric used at every operating point to compute the integral metric should also be between 0% and 100%. However, we observe in <ref type="figure" target="#fig_2">Fig. 3 (c)</ref> that the MOTA is likely to have a strict upper bound lower than 100% at many recall values. In fact, the upper bound of the MOTA at a specific recall value r is derived as follows:</p><formula xml:id="formula_8">MOTA r = 1 − FP r + FN r + IDS r num gt ≤ 1 − FN r num gt ≤ 1 − num gt × (1 − r) num gt = r.<label>(8)</label></formula><p>The first inequality is true because the false positives FP r and identity switches IDS r are always non-negative. Also, the second inequality uses the fact that FN r ≥ num gt ×(1−r) because if the recall is r that means that at least (1−r) of the total objects (num gt ) are not tracked. If r is the upper bound on MOTA r then it follows that the integral metric AMOTA is upper bounded by 50% (i.e., upper bound r creates a triangle in the MOTA vs Recall Curve).</p><p>To make the value of the integral metric AMOTA range from 0% to 100%, we need to scale the range of the MOTA r . From Eq. 8, we find that the reason why the MOTA r has a strict upper bound of r is due to the fact that FN r ≥ num gt × (1 − r). To adjust the MOTA r , we propose two new metrics, called sMOTA (scaled MOTA) and sAMOTA (scaled AMOTA), which are defined as follows:</p><formula xml:id="formula_9">sMOTA r = max(0, 1 − FP r +FN r +IDS r −(1−r)×num gt r×num gt ), (9) sAMOTA = 1 L r∈{ 1 L , 2 L ,··· ,1} sMOTA r ,<label>(10)</label></formula><p>with the number of objects num gt × (1 − r) being subtracted from the FN r in the numerator, the proposed sMOTA r is now upper bounded by 100%, leading to that the sAMOTA is upper bounded by 100% as well. Note that we also add a scalar factor r in the denominator as we think using the actual number of ground truth objects available at a recall value of r (i.e., r × num gt ) makes more sense than using the total number of objects num gt , some of which are not even available to be tracked at a recall of r. Additionally, we add a max operation over zero in Eq. 9, which is to adjust the lower bound of the sMOTA r to zero. Otherwise, sMOTA r can approach towards negative if there are many false positives or identity switches. As a result, the proposed sMOTA r in Eq. 9 can have a range between 0% and 100% as shown in <ref type="figure" target="#fig_2">Fig. 3 (d)</ref>, which also leads to the corresponding integral metric sAMOTA having a range between 0% and 100%. In summary, we believe that the proposed new integral metric -sAMOTA, AMOTA, AMOTP -are able to summarize performance of MOT systems across all thresholds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXPERIMENT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Settings</head><p>Evaluation Metrics. In addition to the proposed sAMOTA, AMOTA and AMOTP, we also evaluate on standard CLEAR metrics such as MOTA, MOTP (multi-object tracking precision), IDS (number of identity switches), FRAG (number of trajectory fragmentation), FPS (frame per second).</p><p>Datasets. We evaluate on the KITTI and nuScenes 3D MOT datasets, which provide LiDAR point cloud and 3D bounding box trajectories. As the KITTI test set only supports 2D MOT evaluation and its ground truth is not released to users, we have to use the KITTI val set for 3D MOT evaluation. Also, we are collaborating with nuTomony to use our proposed metrics to build 3D MOT evaluation on the nuScenes dataset. However, the first nuScenes 3D MOT challenge is not yet finished when this work was developed. As such, we use our evaluation tool to evaluate 3D MOT systems on the nuScenes val set to develop a temporary comparison. For future evaluation on the nuScenes dataset, we recommend users to use the evaluation code provided by nuScenes and primarily evaluate 3D MOT systems on the nuScenes test set for comparison, though our developed temporary comparison on the val set can still be used for reference. In terms of the data split, we follow <ref type="bibr" target="#b15">[16]</ref> on KITTI and use sequences <ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19</ref> as the val set and other sequences as the train set, through our 3D MOT system does not require training. For nuScenes, we use its default data split. Regarding object category, we follow the KITTI convention and show results on each category (Car, Pedestrian, Cyclist). For nuScenes, we first obtain results on each category and then compute the final performance by averaging over 7 categories (Car, Truck, Trailer, Pedestrian, Bicycle, Motorcycle, Bus). For matching criteria, we follow the convention in KITTI 3D object detection benchmark and use 3D IoU to determine a successful match. Specifically, we use 3D IoU threshold IoU thres of 0.25, 0.5 for Pedestrian and Cyclist, and IoU thres of 0.25, 0.5, 0.7 for Car. On nuScenes, we follow the criteria defined in the nuScenes challenge and use a center distance Dist thres of 2 meters.</p><p>Baselines. We compare against modern open-sourced 3D MOT systems such as FANTrack <ref type="bibr" target="#b14">[15]</ref> and mmMOT <ref type="bibr" target="#b29">[30]</ref>. We use the same 3D detections obtained by PointRCNN <ref type="bibr" target="#b5">[6]</ref> on KITTI and by Megvii <ref type="bibr" target="#b33">[34]</ref> on nuScenes for our proposed method and baselines <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b29">[30]</ref> that require 3D detections as inputs. For baseline <ref type="bibr" target="#b14">[15]</ref> that also requires the 2D detections as inputs, we use the 2D projection of 3D detections.</p><p>Implementation Details. For our best results in <ref type="table" target="#tab_2">Table I</ref>, III, II and IV, we use (x, y, z, θ, l, w, h, s, v x , v y , v z ) as the state space of our 3D Kalman filter without including the angular velocity v θ . We use F min =3 and Age min =2 in the birth and death memory module. For the threshold to reject a matching in the data association module, we empirically found that using IoU min =0.01 for Car, Dist max =1 for Pedestrian, Dist max =6 for Cyclist can obtain the best performance on the KITTI dataset. On the nuScenes dataset, we use Dist max =10 for all object categories. For other detailed hyper-parameters, please directly check our code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental Results</head><p>Results for Cars on the KITTI val set. We summarize the results in <ref type="table" target="#tab_2">Table I</ref>. Our proposed 3D MOT system consistently outperforms other modern 3D MOT systems in all metrics when using different matching criteria (e.g., 3D IoU thres = 0.25, 0.5, and 0.7). As a result, we establish new state-ofthe-art 3D MOT performance on the KITTI val set for Cars and achieve an impressive zero identity switch.</p><p>Results for Pedestrians and Cyclists. In addition to evaluate on cars, we also report our 3D MOT performance for other objects such as pedestrians and cyclists on the KITTI val set in <ref type="table" target="#tab_2">Table II</ref>. Although tracking of pedestrians and cyclists is more challenging than cars due to the small size of the objects, we show strong performance of our 3D MOT system.</p><p>Results for all objects on the nuScenes val set. In addition to evaluate on the KITTI dataset, we also report 3D MOT results on the nuScenes val set in <ref type="table" target="#tab_2">Table III</ref>. We emphasize that the nuScenes dataset is more challenging than KITTI due to sparse LiDAR point cloud inputs, complex scenes, and a low frame rate. Therefore, 3D detections on nuScenes are of significantly lower quality than 3D detections on KITTI, resulting in that all 3D MOT systems have a lower  <ref type="table" target="#tab_2">Table I</ref>.</p><p>Qualitative Comparison. We show qualitative comparison between our 3D MOT system and <ref type="bibr" target="#b14">[15]</ref> and in <ref type="figure">Fig. 4</ref>. The 3D tracking results are visualized on the image with colored 3D bounding boxes where the color represents the object identity. We can see that the results of FANTrack (left) contain a few identity switches and miss tracking for objects at the rightmost of the image while our system (right) does not have these issues on the example sequence. We provide more qualitative results of our 3D MOT system in our demo video, which demonstrates that (1) our system, requiring no training, does not have the over-fitting issue on the dataset and (2) our system often produces more stable results and has fewer identity switches and jittered bounding boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation Study</head><p>We conduct all ablative analysis for cars on the KITTI val set using the proposed 3D MOT evaluation tool along with the new metrics, which is summarized in <ref type="table" target="#tab_2">Table IV</ref>.</p><p>Effect of 3D Detection Quality. In Table IV (a), we switch the 3D detection module from <ref type="bibr" target="#b5">[6]</ref> to <ref type="bibr" target="#b32">[33]</ref>. The distinction lies in that <ref type="bibr" target="#b5">[6]</ref> requires a LiDAR point cloud as input while <ref type="bibr" target="#b32">[33]</ref> only requires a single image. As a result, the quality of 3D detections produced by the monocular 3D detector <ref type="bibr" target="#b32">[33]</ref> is much lower than the LiDAR-based 3D detector <ref type="bibr" target="#b5">[6]</ref> (see <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b32">[33]</ref> for details). We can see that the 3D MOT performance in (k) is also better than (a), suggesting that 3D detection quality is crucial to the performance of 3D MOT systems.</p><p>3D v.s. 2D Kalman Filter. We replace the 3D Kalman filter in our final model (k) with a 2D Kalman filter <ref type="bibr" target="#b18">[19]</ref> in (b). Specifically, we define the state space of an object trajectory T =(x, y, a, r, s, v x , v y , v a ), where (x, y) is the object's 2D location, a is the 2D box area, r is the aspect ratio and (v x , v y , v a ) denote the velocity in the 2D image plane. We observed that using the 3D Kalman filter in (k) reduces the IDS from 7 to 0 and FRAG from 43 to 15, which we believe it is due to the fact that tracking in the 3D space can help resolve the depth ambiguity that exists if tracking in the 2D image plane. Overall, the absolute sAMOTA, AMOTA and MOTA values are improved by 3% to 4%.</p><p>Effect of Angular Velocity v θ . We add v θ to the state space so that the state space of a trajectory T = (x, y, z, θ, l, w, h, s, v x , v y , v z , v θ ) in Table IV (c). We observed that, compared to (k), adding v θ improves sAMOTA and AMOTA by 0.01% and decreases AMOTP and MOTA by up to 0.08%. This shows that adding the angular velocity or not does not really have a clear impact on the performance for all metrics. Therefore, we simply do not include the angular velocity in the state space of our final system for simplicity.</p><p>Effect of Orientation Correction. As mentioned in Section III-D, we use an orientation correction technique in our final system in <ref type="table" target="#tab_2">Table IV (k)</ref>. Here, we experiment a variant without using the orientation correction in Table IV (d). We observed that the orientation correction helps improve the performance in all metrics, suggesting that this technique is useful to our proposed 3D MOT system.</p><p>Effect of Threshold IoU min . We change IoU min =0.01 in (k) to IoU min =0.1 in (e) and IoU min =0.25 in (f). We observed that increasing IoU min leads to a consistent drop in all metrics.</p><p>Effect of Bir min . We adjust Bir min =3 in (k) to Bir min =1 in (g) and Bir min =5 in (h). We show that using either Bir min =1 (i.e., creating a new trajectory immediately for an unmatched detection) or Bir min =5 (i.e., creating a new trajectory after an unmatched detection is matched in next five frames) leads to inferior performance in sAMOTA, AMOTP and MOTA, suggesting that using Bir min =3 is the best.</p><p>Effect of Age max . We verify the effect of Age max by decreasing it to Age max =1 in (i) and increasing it to Age max =3 in (j). We show that both (i) and (j) result in a drop in sAMOTA, AMOTA and MOTA, suggesting that Age max =2 (i.e. keep tracking the unmatched trajectories T unmatch in next two frames) in our final model (k) is the best choice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>We proposed an accurate, simple and real-time system for online 3D MOT. Also, a new 3D MOT evaluation tool along with three new metrics was proposed to standardize future 3D MOT evaluation. Through extensive experiments on the KITTI and nuScenes 3D MOT datasets, our system establishes new state-of-the-art 3D MOT performance while achieving the fastest speed. We hope that our system will serve as a solid baseline on which others can easily build on to advance the state-of-the-art in 3D MOT.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>MOTA of modern 2D and 3D MOT systems on the KITTI 2D MOT leaderboard. The higher and more right is better. Our 3D MOT system achieves competitive MOTA in 2D MOT evaluation while being the fastest.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Proposed System Pipeline: (A) a 3D detection module obtains 3D detections Dt from the LiDAR point cloud; (B) a 3D Kalman filter predicts the state of trajectories T t−1 to the current frame t as Test during the state prediction step; (C) the detections Dt and predicted trajectories Test are associated using the Hungarian algorithm; (D) the state of each matched trajectory in T match is updated by the 3D Kalman filter based on the corresponding matched detection in D match to obtain the final trajectories Tt; (E) a birth and death memory takes the unmatched detections D unmatch and unmatched trajectories T unmatch as inputs and creates new trajectories Tnew and deletes disappeared trajectories T lost from the associated trajectories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>(a)(b)(c) The effect of confidence threshold on the CLEAR metrics: MOTA, FN and FP. We evaluate our 3D MOT system on the KITTI dataset using the proposed 3D MOT evaluation tool. We show that, to achieve the highest MOTA, a proper confidence threshold needs to be selected, otherwise the performance of MOTA will be decreased significantly due to a large number of false positives or false negatives.(d) Effect of scale adjustment in MOTA: the proposed scaled accuracy sMOTA has an upper bounding of 100% at any recall value. MOTA and MOTP values over all recall values, e.g., area under the MOTA over recall curve for computing AMOTA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>is the weighted average between the state of T k match and D k match . The weights are determined by the state uncertainty of the matched trajectory T k match and detection D k match (please refer to the Kalman filter [17] for details). Also, we observe that directly applying the Bayes update rule to orientation θ does not work well. For example, there might be the case where the orientation of detection D k match</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I PERFORMANCE</head><label>I</label><figDesc>OF CAR ON THE KITTI VAL SET USING THE PROPOSED 3D MOT EVALUATION TOOL WITH NEW METRICS.</figDesc><table><row><cell>Method</cell><cell></cell><cell></cell><cell cols="5">Input Data Matching criteria sAMOTA↑</cell><cell>AMOTA↑</cell><cell cols="2">AMOTP↑ MOTA↑</cell><cell>MOTP↑ IDS↓ FRAG↓</cell><cell>FPS↑</cell></row><row><cell cols="3">mmMOT [30] (ICCV 19)</cell><cell>2D + 3D</cell><cell></cell><cell cols="2">IoU thres = 0.25</cell><cell>70.61</cell><cell>33.08</cell><cell>72.45</cell><cell>74.07</cell><cell>78.16</cell><cell>10</cell><cell>55</cell><cell>4.8 (GPU)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">IoU thres = 0.5</cell><cell>69.14</cell><cell>32.81</cell><cell>72.22</cell><cell>73.53</cell><cell>78.51</cell><cell>10</cell><cell>64</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">IoU thres = 0.7</cell><cell>63.91</cell><cell>24.91</cell><cell>67.32</cell><cell>51.91</cell><cell>80.71</cell><cell>24</cell><cell>141</cell></row><row><cell cols="3">FANTrack [15] (IV 20)</cell><cell>2D + 3D</cell><cell></cell><cell cols="2">IoU thres = 0.25</cell><cell>82.97</cell><cell>40.03</cell><cell>75.01</cell><cell>74.30</cell><cell>75.24</cell><cell>35</cell><cell>202</cell><cell>25.0 (GPU)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">IoU thres = 0.5</cell><cell>80.14</cell><cell>38.16</cell><cell>73.62</cell><cell>72.71</cell><cell>74.91</cell><cell>36</cell><cell>211</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">IoU thres = 0.7</cell><cell>62.72</cell><cell>24.71</cell><cell>66.06</cell><cell>49.19</cell><cell>79.01</cell><cell>38</cell><cell>406</cell></row><row><cell>Ours</cell><cell></cell><cell></cell><cell>3D</cell><cell></cell><cell cols="2">IoU thres = 0.25</cell><cell>93.28</cell><cell>45.43</cell><cell>77.41</cell><cell>86.24</cell><cell>78.43</cell><cell>0</cell><cell>15 207.4 (CPU)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">IoU thres = 0.5</cell><cell>90.38</cell><cell>42.79</cell><cell>75.65</cell><cell>84.02</cell><cell>78.97</cell><cell>0</cell><cell>51</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">IoU thres = 0.7</cell><cell>69.81</cell><cell>27.26</cell><cell>67.00</cell><cell>57.06</cell><cell>82.43</cell><cell>0</cell><cell>157</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE II</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">PERFORMANCE OF PEDESTRIAN AND CYCLIST ON KITTI VAL SET.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Category</cell><cell cols="4">Matching criteria sAMOTA↑</cell><cell cols="4">AMOTA↑ AMOTP↑ MOTA↑</cell><cell></cell><cell></cell></row><row><cell>Pedestrian</cell><cell></cell><cell>IoU thres = 0.25</cell><cell cols="2">75.85</cell><cell>31.04</cell><cell>55.53</cell><cell cols="2">70.90</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>IoU thres = 0.5</cell><cell cols="2">70.95</cell><cell>27.31</cell><cell>52.45</cell><cell cols="2">65.06</cell><cell></cell><cell></cell></row><row><cell>Cyclist</cell><cell></cell><cell>IoU thres = 0.25</cell><cell cols="2">91.36</cell><cell>44.34</cell><cell>79.18</cell><cell cols="2">84.87</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>IoU thres = 0.5</cell><cell cols="2">89.27</cell><cell>42.39</cell><cell>77.56</cell><cell cols="2">79.82</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE III</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">PERFORMANCE OVER ALL CATEGORIES ON THE NUSCENES VAL SET.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell></cell><cell cols="3">Matching criteria sAMOTA↑</cell><cell>AMOTA↑</cell><cell cols="3">AMOTP↑ MOTA↑</cell><cell></cell><cell></cell></row><row><cell cols="2">FANTrack [15]</cell><cell cols="2">Dist thres = 2</cell><cell>19.64</cell><cell>2.36</cell><cell>22.92</cell><cell cols="2">18.60</cell><cell></cell><cell></cell></row><row><cell>mmMOT [30]</cell><cell></cell><cell cols="2">Dist thres = 2</cell><cell>23.93</cell><cell>2.11</cell><cell>21.28</cell><cell cols="2">19.82</cell><cell></cell><cell></cell></row><row><cell>Ours</cell><cell></cell><cell cols="2">Dist thres = 2</cell><cell>39.90</cell><cell>8.94</cell><cell>29.67</cell><cell cols="2">31.40</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV ABLATION</head><label>IV</label><figDesc>STUDY FOR CAR ON THE KITTI VAL SET USING THE PROPOSED 3D MOT EVALUATION TOOL WITH NEW METRICS.Fig. 4. Qualitative comparison between FANTrack [15] (left) and our system (right) on the sequence 3 of the KITTI test set. absolute performance on nuScenes. Our 3D MOT system still outperforms other 3D MOT systems in all metrics.Inference Time. We compare inference time of all methods in the last column ofTable I. Our 3D MOT system (excluding the 3D detector part) runs at a rate of 207.4 FPS on the KITTI val set without the need of GPUs, achieving the fastest speed among other 3D MOT systems in</figDesc><table><row><cell>Method variants</cell><cell>Matching criteria</cell><cell cols="2">sAMOTA↑ AMOTA↑</cell><cell cols="2">AMOTP↑ MOTA↑</cell><cell cols="3">MOTP↑ IDS↓ FRAG↓</cell><cell>FP↓</cell><cell>FN↓</cell></row><row><cell>(a) replace detector with [33]</cell><cell>IoU thres = 0.25, same below</cell><cell>63.27</cell><cell>32.47</cell><cell>64.29</cell><cell>64.91</cell><cell>68.26</cell><cell>1</cell><cell>24</cell><cell>1045</cell><cell>1894</cell></row><row><cell>(b) change to 2D Kalman Filter</cell><cell></cell><cell>90.17</cell><cell>42.99</cell><cell>77.99</cell><cell>81.95</cell><cell>78.98</cell><cell>7</cell><cell>43</cell><cell>684</cell><cell>821</cell></row><row><cell>(c) add angular velocity v θ</cell><cell></cell><cell>93.29</cell><cell>45.44</cell><cell>77.40</cell><cell>86.16</cell><cell>78.39</cell><cell>0</cell><cell>16</cell><cell>365</cell><cell>795</cell></row><row><cell>(d) remove orientation correction</cell><cell></cell><cell>92.87</cell><cell>45.04</cell><cell>76.73</cell><cell>85.62</cell><cell>76.93</cell><cell>0</cell><cell>50</cell><cell>418</cell><cell>787</cell></row><row><cell>(e) IoU min = 0.1</cell><cell></cell><cell>92.43</cell><cell>45.25</cell><cell>77.44</cell><cell>85.63</cell><cell>78.47</cell><cell>0</cell><cell>18</cell><cell>366</cell><cell>838</cell></row><row><cell>(f) IoU min = 0.25</cell><cell></cell><cell>86.70</cell><cell>40.05</cell><cell>73.85</cell><cell>79.91</cell><cell>79.03</cell><cell>19</cell><cell>34</cell><cell>342</cell><cell>1322</cell></row><row><cell>(g) Bir min = 1</cell><cell></cell><cell>91.51</cell><cell>43.60</cell><cell>79.06</cell><cell>82.17</cell><cell>78.26</cell><cell>4</cell><cell>21</cell><cell>797</cell><cell>693</cell></row><row><cell>(h) Bir min = 5</cell><cell></cell><cell>90.56</cell><cell>43.49</cell><cell>75.46</cell><cell>84.89</cell><cell>78.69</cell><cell>0</cell><cell>13</cell><cell>278</cell><cell>988</cell></row><row><cell>(i) Age max = 1</cell><cell></cell><cell>90.89</cell><cell>43.60</cell><cell>75.86</cell><cell>83.96</cell><cell>78.90</cell><cell>0</cell><cell>43</cell><cell>380</cell><cell>964</cell></row><row><cell>(j) Age max = 3</cell><cell></cell><cell>91.26</cell><cell>44.48</cell><cell>77.17</cell><cell>84.75</cell><cell>78.21</cell><cell>0</cell><cell>13</cell><cell>503</cell><cell>775</cell></row><row><cell>(k) Ours</cell><cell></cell><cell>93.28</cell><cell>45.43</cell><cell>77.41</cell><cell>86.24</cell><cell>78.43</cell><cell>0</cell><cell>15</cell><cell>365</cell><cell>788</cell></row><row><cell>Frame 13 (FANTrack 2019)</cell><cell></cell><cell></cell><cell cols="2">Frame 13 (Ours)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Frame 28 (FANTrack 2019)</cell><cell></cell><cell></cell><cell cols="2">Frame 28 (Ours)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Frame 43 (FANTrack 2019)</cell><cell></cell><cell></cell><cell cols="2">Frame 43 (Ours)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Xinshuo Weng, Jianren Wang, David Held and Kris Kitani are with</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We define the confidence score of an object trajectory as the average of its confidence scores across all frames.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>This work was funded in part by the Department of Homeland Security award 2017-DN-077-ER0001. Also, we thank the authors of SORT <ref type="bibr" target="#b18">[19]</ref>, which inspired our work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Deep Reinforcement Learning for Autonomous Driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Weng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.11329</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Sequential Forecasting of 100,000 Points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.08376</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">When We First Met: Visual-Inertial Person Localization for Co-Robot Rendezvous</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kitani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09959</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Forecasting Time-to-Collision from Monocular Video: Feasibility, Dataset, and Challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Manglik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ohn-Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IROS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">PointRCNN: 3D Object Proposal Generation and Detection from Point Cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Visual Compiler: Synthesizing a Scene-Specific Pedestrian Detector and Pose Estimator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">N</forename><surname>Boddeti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Beainy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.05234</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Rotational Rectification Network: Enabling Pedestrian Detection for Mobile Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Beainy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kitani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>WACV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Are We Ready for Autonomous Driving? the KITTI Vision Benchmark Suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Online Multi-Object Tracking via Structural Constraint Event Aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Yoon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multiple Object Tracking with Attention to Appearance, Structure, Motion and Size</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Karunasekera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Beyond Pixels: Leveraging Geometry and Shape Cues for Online Multi-Object Tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Ansari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Krishna</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">ICRA</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Online Multi-Object Tracking Using Joint Domain Information in Traffic Scenarios</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">End-to-End Learning of Multi-Sensor 3D Tracking by Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICRA</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">FANTrack: 3D Multi-Object Tracking with Feature Association Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Baser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Balasubramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Czarnecki</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">IV</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Mono-Camera 3D Multi-Object Tracking Using Deep Learning Detections and PMBM Filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scheidegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Benjaminsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Granstr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">IV</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A New Approach to Linear Filtering and Prediction Problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kalman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Basic Engineering</title>
		<imprint>
			<date type="published" when="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The Hungarian Method for the Assignment Problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval Research Logistics Quarterly</title>
		<imprint>
			<date type="published" when="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Simple Online and Realtime Tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Upcroft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The H3D Dataset for Full-Surround 3D Multi-Object Detection and Tracking in Crowded Urban Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Malla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">ICRA</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Global Data Association for Multi-Object Tracking Using Network Flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Deep Network Flow for Multi-Object Tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vernaza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Man</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kitani</surname></persName>
		</author>
		<title level="m">GNN3DMOT: Graph Neural Network for 3D Multi-Object Tracking with 2D-3D Multi-Feature Learning</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Globally-Optimal Greedy Algorithms for Tracking a Variable Number of Objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Near-Online Multi-Target Tracking with Aggregated Local Flow Descriptor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">The Way They Move: Tracking Multiple Targets with Similar Appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dicle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">I</forename><surname>Camps</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sznaier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">ICCV</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Robust Online Multi-Object Tracking Based on Tracklet Confidence and Online Discriminative Appearance Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Yoon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Learning Shape Representations for Clothing Variations in Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kitani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.07340</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Combined Imageand World-Space Tracking in Traffic Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Osep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Mehner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICRA</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Robust Multi-Modality Multi-Object Tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">ICCV</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kitani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.07847</idno>
		<title level="m">Joint 3D Tracking and Forecasting with Graph Neural Network and Diversity Sampling</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Joint Detection and Multi-Object Tracking with Graph Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kitani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.13164</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Monocular 3D Object Detection with Pseudo-LiDAR Point Cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kitani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCVW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Class-Balanced Grouping and Sampling for Point Cloud 3D Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Evaluating Multiple Object Tracking Performance: The CLEAR MOT Metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bernardin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Journal on Image and Video Processing</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Towards the Evaluation of Reproducible Robustness in Tracking-by-Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Solera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AVSS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">UA-DETRAC: A New Benchmark and Protocol for Multi-Object Detection and Tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
