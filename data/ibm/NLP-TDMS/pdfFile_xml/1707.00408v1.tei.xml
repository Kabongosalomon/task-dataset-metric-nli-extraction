<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pedestrian Alignment Network for Large-scale Person Re-identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
							<email>zdzheng12@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Technology Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">·</forename><surname>Liang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Technology Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Technology Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Technology Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Technology Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">·</forename><surname>Liang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Technology Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Technology Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Technology Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Pedestrian Alignment Network for Large-scale Person Re-identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note type="submission">Received: date / Accepted: date</note>
					<note>Noname manuscript No. (will be inserted by the editor) 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Computer vision · Person re-identification · Person search · Person alignment · Image retrieval · Deep learning</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Person re-identification (person re-ID) is mostly viewed as an image retrieval problem. This task aims to search a query person in a large image pool. In practice, person re-ID usually adopts automatic detectors to obtain cropped pedestrian images. However, this process suffers from two types of detector errors: excessive background and part missing. Both errors deteriorate the quality of pedestrian alignment and may compromise pedestrian matching due to the position and scale variances. To address the misalignment problem, we propose that alignment can be learned from an identification procedure. We introduce the pedestrian alignment network (PAN) which allows discriminative embedding learning and pedestrian alignment without extra annotations. Our key observation is that when the convolutional neural network (CNN) learns to discriminate between different identities, the learned feature maps usually exhibit strong activations on the human body rather than the background. The proposed network thus takes advantage of this attention mechanism to adaptively locate and align pedestrians within a bounding box. Visual examples show that pedestrians are better aligned with PAN. Experiments on three large-scale re-ID datasets confirm that PAN improves the discriminative ability of the feature embeddings and yields competitive accuracy with the state-of-the-art methods. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Person re-identification (person re-ID) aims at spotting the target person in different cameras, and is mostly viewed as an image retrieval problem, i.e., searching for the query person in a large image pool (gallery). Recent progress mainly consists in the discriminatively learned embeddings using the convolutional neural network (CNN) on large-scale datasets. The learned embeddings extracted from the fine-tuned CNNs are shown to outperform the hand-crafted features <ref type="bibr" target="#b57">[Zheng et al., 2016b</ref><ref type="bibr" target="#b46">, Xiao et al., 2016</ref><ref type="bibr" target="#b60">, Zhong et al., 2017</ref>.</p><p>Among the many influencing factors, misalignment is a critical one in person re-ID. This problem arises due to the usage of pedestrian detectors. In realistic set-arXiv:1707.00408v1 [cs.CV] 3 Jul 2017 tings, the hand-drawn bounding boxes, existing in some previous datasets such as VIPER <ref type="bibr" target="#b12">[Gray et al., 2007]</ref>, CUHK01  and CUHK02 <ref type="bibr" target="#b22">[Li and Wang, 2013]</ref>, are infeasible to acquire when millions of bounding boxes are to be generated. So recent large-scale benchmarks such as CUHK03 , Mar-ket1501  and MARS <ref type="bibr" target="#b54">[Zheng et al., 2016a]</ref> adopt the Deformable Part Model (DPM) <ref type="bibr" target="#b9">[Felzenszwalb et al., 2008]</ref> to automatically detect pedestrians. This pipeline largely saves the amount of labeling effort and is closer to realistic settings. However, when detectors are used, detection errors are inevitable, which may lead to two common noisy factors: excessive background and part missing. For the former, the background may take up a large proportion of a detected image. For the latter, a detected image may contain only part of the human body, i.e., with missing parts (see <ref type="figure">Fig. 1</ref>).</p><p>Pedestrian alignment and re-identification are two connected problems. When we have the identity labels of the pedestrian bounding boxes, we might be able to find the optimal affine transformation that contains the most informative visual cues to discriminate between different identities. With the affine transformation, pedestrians can be better aligned. Furthermore, with superior alignment, more discriminative features can be learned, and the pedestrian matching accuracy can, in turn, be improved.</p><p>Motivated by the above-mentioned aspects, we propose to incorporate pedestrian alignment into an identification re-ID architecture, yielding the pedestrian alignment network (PAN). Given a pedestrian detected image, this network simultaneously learns to re-localize the person and categorize the person into pre-defined identities. Therefore, PAN takes advantage of the complementary nature of person alignment and re-identification.</p><p>In a nutshell, the training process of PAN is composed of the following components: 1) a network to predict the identity of an input image, 2) an affine transformation to be estimated which re-localizes the input image, and 3) another network to predict the identity of the re-localized image. For components 1) and 3), we use two convolutional branches called the base branch and alignment branch, to respectively predict the identity of the original image and the aligned image. Internally, they share the low-level features and during testing are concatenated at the fully-connected (FC) layer to generate the pedestrian descriptor. In component 2), the affine parameters are estimated using the feature maps from the high-level convolutional layer of the base branch. The affine transformation is later applied on the lower-level feature maps of the base branch. In this step, we deploy a differentiable localization network: spatial transformer network (STN) <ref type="bibr" target="#b18">[Jaderberg et al., 2015]</ref>. With STN, we can 1) crop the detected images which may contain too much background or 2) pad zeros to the borders of images with missing parts. As a result, we reduce the impact of scale and position variances caused by misdetection and thus make pedestrian matching more precise.</p><p>Note that our method addresses the misalignment problem caused by detection errors, while the commonly used patch matching strategy aims to discover matched local structures in well-aligned images. For methods that use patch matching, it is assumed that the matched local structures locate in the same horizontal stripe <ref type="bibr" target="#b49">, Yi et al., 2014</ref><ref type="bibr" target="#b51">, Zhao et al., 2013a</ref><ref type="bibr" target="#b25">, Liao et al., 2015</ref><ref type="bibr" target="#b5">, Cheng et al., 2016</ref> or square neighborhood <ref type="bibr" target="#b0">[Ahmed et al., 2015]</ref>. Therefore, these algorithms are robust to some small spatial variance, e.g., position and scale. However, when misdetection happens, due to the limitation of the search scope, this type of methods may fail to discover the matched structures, and the risk of part mismatching may be high. Therefore, regarding the problem to be solved, the proposed method is significantly different from this line of works <ref type="bibr" target="#b49">, Yi et al., 2014</ref><ref type="bibr" target="#b0">, Ahmed et al., 2015</ref><ref type="bibr" target="#b51">, Zhao et al., 2013a</ref><ref type="bibr" target="#b25">, Liao et al., 2015</ref><ref type="bibr" target="#b5">, Cheng et al., 2016</ref>. We speculate that our method is a good complementary step for those using part matching.</p><p>Our contributions are summarized as follows:</p><p>-We propose the pedestrian alignment network (PAN), which simultaneously aligns pedestrians within images and learns pedestrian descriptors. Except for the identity label, we do not need any extra annotation; -We observe that the manually cropped images are not as perfect as preassumed to be. We show that our network also improves the re-ID performance on the hand-drawn datasets which are considered to have decent person alignment. -We achieve competitive accuracy compared to the state-of-the-art methods on three large-scale person re-ID datasets (Market-1501 , CUHK03  and DukeMTMC-reID <ref type="bibr" target="#b60">[Zheng et al., 2017b]</ref>).</p><p>The rest of this paper is organized as follows. Section 2 reviews and discusses related works. Section 3 illustrates the proposed method in detail. Experimental results and comparisons on three large-scale person re-ID datasets are discussed in Section 4, followed by conclusions in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Our work aims to address two tasks: person re-identification (person re-ID) and person alignment jointly. In this section, we review the relevant works in these two domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Hand-crafted Systems for Re-ID</head><p>Person re-ID needs to find the robust and discriminative features among different cameras. Several pioneering approaches have explored person re-ID by extracting local hand-crafted features such as LBP <ref type="bibr" target="#b32">[Mignon and Jurie, 2012]</ref>, Gabor <ref type="bibr" target="#b33">[Prosser et al., 2010]</ref> and LOMO <ref type="bibr" target="#b25">[Liao et al., 2015]</ref>. In a series of works by <ref type="bibr" target="#b51">, Zhao et al., 2013a</ref><ref type="bibr" target="#b52">, Zhao et al., 2013b</ref>, the 32-dim LAB color histogram and the 128-dim SIFT descriptor are extracted from each 10 × 10 patches.  use color name descriptor for each local patch and aggregate them into a global vector through the Bagof-Words model. Approximate nearest neighbor search ] is employed for fast retrieval but accuracy compromise. <ref type="bibr" target="#b4">[Chen et al., 2017]</ref> also deploy several different hand-crafted features extracting from overlapped body patches. Differently, <ref type="bibr" target="#b6">[Cheng et al., 2011]</ref> localize the parts first and calculate color histograms for part-to-part correspondences. This line of works is beneficial from the local invariance in different viewpoints.</p><p>Besides finding robust feature, metric learning is nontrivial for person re-ID. <ref type="bibr" target="#b20">[Köstinger et al., 2012]</ref> propose "KISSME" based on Mahalanobis distance and formulate the pair comparison as a log-likelihood ratio test. Further, <ref type="bibr" target="#b25">[Liao et al., 2015]</ref> extend the Bayesian face and KISSME to learn a discriminant subspace with a metric. Aside from the methods using Mahalanobis distance, Prosser et al. apply a set of weak RankSVMs to assemble a strong ranker <ref type="bibr" target="#b33">[Prosser et al., 2010]</ref>. Gray and Tao propose using the AdaBoost algorithm to fuse different features into a single similarity function <ref type="bibr" target="#b13">[Gray and Tao, 2008]</ref>. <ref type="bibr" target="#b30">[Loy et al., 2010]</ref> propose a cross canonical correlation analysis for the video-based person re-ID.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Deeply-learned Models for Re-ID</head><p>CNN-based deep learning models have been popular since <ref type="bibr" target="#b21">[Krizhevsky et al., 2012]</ref> won ILSVRC'12 by a large margin. It extracts features and learns a classifier in an end-to-end system. More recent approaches based on CNN apply spatial constraints by splitting images or adding new patch-matching layers. <ref type="bibr" target="#b49">[Yi et al., 2014]</ref> split a pedestrian image into three horizontal parts and respectively trained three part-CNNs to extract features. Similarly, <ref type="bibr" target="#b5">[Cheng et al., 2016]</ref> split the convolutional map into four parts and fuse the part features with the global feature.  add a new layer that multiplies the activation of two images in different horizontal stripes. They use this layer to allow patch matching in CNN explicitly. Later, <ref type="bibr" target="#b0">[Ahmed et al., 2015]</ref> improve the performance by proposing a new part-matching layer that compares the activation of two images in neighboring pixels. Besides, <ref type="bibr" target="#b38">[Varior et al., 2016a]</ref> combine CNN with some gate functions, similar to long-short-term memory <ref type="bibr">(LSTM [Hochreiter and Schmidhuber, 1997]</ref>) in spirit, which aims to focus on the similar parts of input image pairs adaptively. But it is limited by the computational inefficiency because the input should be in pairs. Similarly, <ref type="bibr" target="#b27">[Liu et al., 2016a]</ref> propose a soft attention-based model to focus on parts and combine CNN with LSTM components selectively; its limitation also consists of the computation inefficiency.</p><p>Moreover, a convolutional network has the high discriminative ability by itself without explicit patch-matching. For person re-ID,  directly use a conventional fine-tuning approach on Market-1501  and their performance outperform other recent results. <ref type="bibr" target="#b45">[Wu et al., 2016c]</ref> combine the CNN embedding with hand-crafted features. <ref type="bibr" target="#b46">[Xiao et al., 2016]</ref> jointly train a classification model with multiple datasets and propose a new dropout function to deal with the hundreds of identity classes. <ref type="bibr" target="#b44">[Wu et al., 2016b]</ref> deepen the network and use filters of smaller size.  use person attributes as auxiliary tasks to learn more information. <ref type="bibr" target="#b59">[Zheng et al., 2016d]</ref> propose combining the identification model with the verification model and improve the fine-tuned CNN performance. <ref type="bibr" target="#b8">[Ding et al., 2015]</ref> and <ref type="bibr" target="#b15">[Hermans et al., 2017]</ref> use triplet samples for training the network which considers the images from the same people and the different people at the same time. Recent work by Zheng et al. combined original training dataset with GAN-generated images and regularized the model <ref type="bibr" target="#b60">[Zheng et al., 2017b]</ref>. In this paper, we adopt the similar convolutional branches without explicit part-matching layers. It is noted that we focus on a different goal on finding robust pedestrian embedding for person re-identification, and thus our method can be potentially combined with the previous methods to further improve the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Objective Alignment</head><p>Face alignment (here refer to the rectification of face misdetection) has been widely studied. <ref type="bibr" target="#b17">[Huang et al., 2007]</ref> propose an unsupervised method called funneled . It consists of two identification networks (blue) and an affine estimation network (orange). The base branch predicts the identities from the original image. We use the high-level feature maps of the base branch (Res4 Feature Maps) to predict the grid. Then the grid is applied to the low-level feature maps (Res2 Feature Maps) to re-localize the pedestrian (red star). The alignment stream then receives the aligned feature maps to identify the person again. Note that we do not perform alignment on the original images (dotted arrow) as previously done in <ref type="bibr" target="#b18">[Jaderberg et al., 2015]</ref> but directly on the feature maps. In the training phase, the model minimizes two identification losses. In the test phase, we concatenate two 1 × 1 × 2048 FC embeddings to form a 4096-dim pedestrian descriptor for retrieval.</p><p>image to align faces according to the distribution of other images and improve this method with convolutional RBM descriptor later <ref type="bibr" target="#b16">[Huang et al., 2012]</ref>. However, it is not trained in an end-to-end manner, and thus following tasks i.e., face recognition take limited benefits from the alignment. On the other hand, several works introduce attention models for task-driven object localization. Jadeburg et al. <ref type="bibr" target="#b18">[Jaderberg et al., 2015]</ref> deploy the spatial transformer network (STN) to fine-grained bird recognition and house number recognition. <ref type="bibr" target="#b19">[Johnson et al., 2015]</ref> combine faster-RCNN <ref type="bibr" target="#b11">[Girshick, 2015]</ref>, RNN and STN to address the localization and description in image caption. Aside from using STN, Liu et al. use reinforcement learning to detect parts and assemble a strong model for fine-grained recognition <ref type="bibr" target="#b29">[Liu et al., 2016c]</ref>.</p><p>In person re-ID, <ref type="bibr" target="#b1">[Baltieri et al., 2015]</ref> exploits 3D body models to the well-detected images to align the pose but does not handle the misdetection problem. Besides, the work that inspires us the most is "Pose-Box" proposed by <ref type="bibr" target="#b55">[Zheng et al., 2017a]</ref>. The PoseBox is a strengthened version of the Pictorial Structures proposed in <ref type="bibr" target="#b6">[Cheng et al., 2011]</ref>. PoseBox is similar to our work in that 1) both works aim to solve the misalignment problem, and that 2) the networks have two convolutional streams. Nevertheless, our work differs significantly from PoseBox in two aspects. First, Pose-Box employs the convolutional pose machines (CPM) to generate body parts for alignment in advance, while this work learns pedestrian alignment in an end-to-end manner without extra steps. Second, PoseBox can tackle the problem of excessive background but may be less effective when some parts are missing, because CPM fails to detect body joints when the body part is absent. However, our method automatically provides solutions to both problems, i.e., excessive background and part missing.</p><p>3 Pedestrian Alignment Network</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview of PAN</head><p>Our goal is to design an architecture that jointly aligns the images and identifies the person. The primary challenge is to develop a model that supports end-to-end training and benefits from the two inter-connected tasks. The proposed architecture draws on two convolutional branches and one affine estimation branch to simultaneously address these design constraints. <ref type="figure" target="#fig_0">Fig. 2</ref> briefly illustrates our model.</p><p>To illustrate our method, we use the ResNet-50 model <ref type="bibr" target="#b14">[He et al., 2016]</ref> as the base model which is applied on the Market-1501 dataset . Each Res i, i = 1, 2, 3, 4, 5 block in <ref type="figure" target="#fig_0">Fig. 2</ref> denotes several convolutional layers with batch normalization, ReLU, and optionally max pooling. After each block, the feature <ref type="figure">Fig. 3</ref> We visualize the Res4 Feature Maps in the base branch. We observe that high responses are mostly concentrated on the pedestrian body. So we use the Res4 Feature Maps to estimate the affine parameters.</p><p>maps are down-sampled to be half of the size of the feature maps in the previous block. For example, Res 1 down-samples the width and height of an image from 224 × 224 to 112 × 112. In Section 3.2 and Section 3.3, we first describe the convolutional branches and affine estimation branches of our model. Then in Section 3.4 we address the details of a pedestrian descriptor. When testing, we use the descriptor to retrieve the query person. Further, we discuss the re-ranking method as a subsequent processing in Section 3.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Base and Alignment Branches</head><p>Recent progress in person re-ID datasets allows the CNN model to learn more discriminative visual representations. There are two main convolutional branches exist in our model, called the base branch and the alignment branch. Both branches are classification networks that predict the identity of the training images. Given an originally detected image, the base branch not only learns to distinguish its identity from the others but also encodes the appearance of the detected image and provides the clues for the spatial localization (see <ref type="figure">Fig.  3</ref>). The alignment branch shares a similar convolutional network but processes the aligned feature maps produced by the affine estimation branch.</p><p>In the base branch, we train the ResNet-50 model <ref type="bibr" target="#b14">[He et al., 2016]</ref>, which consists of five down-sampling blocks and one global average pooling. We deploy the model pre-trained on ImageNet <ref type="bibr" target="#b7">[Deng et al., 2009</ref>] and remove the final fully-connected (FC) layer. There are K = 751 identities in the Market-1501 training set, so we add an FC layer to map the CNN embedding of size 1 × 1 × 2048 to 751 unnormalized probabilities. The alignment branch, on the other hand, is comprised of three ResBlocks and one average pooling layer. We also add an FC layer to predict the multi-class probabilities. The two branches do not share weight. We use W 1 and W 2 to denote the parameters of the two convolutional branches, respectively.</p><p>More formally, given an input image x, we use p(k|x) to denote the probability that the image x belongs to the class k ∈ {1...K}. Specifically,</p><formula xml:id="formula_0">p(k|x) = exp(z k ) K k=1 exp(zi) .</formula><p>Here z i is the outputted probability from the CNN model. For the two branches, the cross-entropy losses are formulated as:</p><formula xml:id="formula_1">l base (W 1 , x, y) = − K k=1 (log(p(k|x))q(k|x)),<label>(1)</label></formula><formula xml:id="formula_2">l align (W 2 , x a , y) = − K k=1 (log(p(k|x a ))q(k|x a )),<label>(2)</label></formula><p>where x a denotes the aligned input. It can be derived from the original input x a = T (x). Given the label y, the ground-truth distribution q(y|x) = 1 and q(k|x) = 0 for all k = y. If we discard the 0 term in Eq. 1 and Eq. 2, the losses are equivalent to:</p><formula xml:id="formula_3">l base (W 1 , x, y) = −log(p(y|x)),<label>(3)</label></formula><p>l align (W 2 , x a , y) = −log(p(y|x a )).</p><p>Thus, at each iteration, we wish to minimize the total entropy, which equals to maximizing the possibility of the correct prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Affine Estimation Branch</head><p>To address the problems of excessive background and part missing, the key idea is to predict the position of the pedestrian and do the corresponding spatial transform. When excessive background exists, a cropping strategy should be used; under part missing, we need to pad zeros to the corresponding image borders. Both strategies need to find the parameters for the affine transformation. In this paper, this function is implemented by the affine estimation branch. The affine estimation branch receives two input tensors of activations 14 × 14 × 1024 and 56 × 56 × 256 from the base branch. We name the two tensors the Res2 Feature Maps and the Res4 Feature Maps, respectively. The Res4 Feature Maps contain shallow feature maps of the original image and reflects the local pattern information. On the other hand, since the Res2 Feature Maps are closer to the classification layer, it encodes the attention on the pedestrian and semantic cues for aiding identification. The affine estimation branch contains one bilinear sampler and one small network called Grid Network. The Grid Network contains one Res-Block and one average pooling layer. We pass the Res4 Feature Maps through Grid Network to regress a set of 6-dimensional transformer parameters. The learned transforming parameters θ are used to produce the image grid. The affine transformation process can be written as below,</p><formula xml:id="formula_5">x s i y s i = θ 11 θ 12 θ 13 θ 21 θ 22 θ 23    x t i y t i 1    ,<label>(5)</label></formula><p>where (x t i , y t i ) are the target coordinates on the output feature map, and (x s i , y s i ) are the source coordinates on the input feature maps (Res2 Feature Maps). θ 11 , θ 12 , θ 21 and θ 22 deal with the scale and rotation transformation, while θ 13 and θ 23 deal with the offset. In this paper, we set the coordinates as: (-1,-1) refer to the pixel on the top left of the image, while on the output image is equal to that of (−0.9, −0.7) on the original map. We use a bilinear sampler to make up the missing pixels, and we assign zeros to the pixels located out of the original range. So we obtain an injective function from the original feature map V to the aligned output U . More formally, we can formulate the equation:</p><formula xml:id="formula_6">U c (m,n) = H x s W y s V c (x s ,y s ) max(0, 1−|x t −m|)max(0, 1−|y t −n|).<label>(6)</label></formula><p>U c (m,n) is the output feature map at location (m, n) in channel c, and V c (x s ,y s ) is the input feature map at location (x s , y s ) in channel c. If (x t , y t ) is close to (m, n), we add the pixel at (x s , y s ) according to the bilinear sampling.</p><p>In this work, we do not perform pedestrian alignment on the original image; instead, we choose to achieve an equivalent function on the shallow feature maps. By using the feature maps, we reduce the running time and parameters of the model. This explains why we apply re-localization grid on the feature maps. The bilinear sampler receives the grid, and the feature maps to produce the aligned output x a . We provide some visualization examples in <ref type="figure">Fig. 3</ref>. Res4 Feature maps are shown. We can observe that through ID supervision, we are able to re-localize the pedestrian and correct misdetections to some extent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Pedestrian Descriptor</head><p>Given the fine-tuned PAN model and an input image x i , the pedestrian descriptor is the weighted fusion of the FC features of the base branch and the alignment branch. That is, we are able to capture the pedestrian characteristic from the original image and the aligned image. In the Section 4.3, the experiment shows that the two features are complementary to each other and improve the person re-ID performance.</p><p>In this paper, we adopt a straightforward late fusion strategy, i.e., f i = g(f 1 i , f 2 i ). Here f 1 i and f 2 i are the FC descriptors from two types of images, respectively. We reshape the tensor after the final average pooling to a 1dim vector as the pedestrian descriptor of each branch. The pedestrian descriptor is then written as:</p><formula xml:id="formula_7">f i = α|f 1 i | T , (1 − α)|f 2 i | T T .<label>(7)</label></formula><p>The | · | operator denotes an l 2 -normalization step. We concatenate the aligned descriptor with the original descriptor, both after l 2 -normalization. α is the weight for the two descriptors. If not specified, we simply use α = 0.5 in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Re-ranking for re-ID</head><p>In this work, we first obtain the rank list N (q, n) = [x 1 , x 2 , ...x n ] by sorting the Euclidean distance of gallery images to the query. Distance is calculated as D i,j = (f i − f j ) 2 , where f i and f j are l 2 -normalized features of image i and j, respectively. We then perform reranking to obtain better retrieval results. Several reranking methods can be applied in person re-ID <ref type="bibr" target="#b48">[Ye et al., 2015</ref><ref type="bibr" target="#b34">, Qin et al., 2011</ref><ref type="bibr" target="#b60">, Zhong et al., 2017</ref>. Specifically, we follow a state-of-the-art re-ranking method proposed in <ref type="bibr" target="#b60">[Zhong et al., 2017]</ref>. Apart from the Euclidean distance, we additionally consider the Jaccard similarity. To introduce this distance, we first define a robust retrieval set for each image. The k-reciprocal nearest neighbors R(p, k) are comprised of such element that appears in the top-k retrieval rank of the query p while the query is in the top-k rank of the element as well.</p><formula xml:id="formula_8">R(p, k) = {x|x ∈ N (p, k), p ∈ N (x, k)}<label>(8)</label></formula><p>According to <ref type="bibr" target="#b60">[Zhong et al., 2017]</ref>, we extend the set R to R * to include more positive samples. Taking the advantage of set R * , we use the Jaccard similarity for re-ranking. When we use the correctly matched images to conduct the retrieval, we should retrieve a similar <ref type="figure">Fig. 4</ref> Sample images from Market-1501 , CUHK03  and DukeMTMC-reID <ref type="bibr" target="#b60">[Zheng et al., 2017b]</ref>. The three datasets are collected in different scenes with different detection bias.</p><p>rank list as we use the original query. The Jaccard distance can be simply formulated as:</p><formula xml:id="formula_9">D similarity = 1 − |R * (q, k) ∩ R * (x i , k)| |R * (q, k) ∪ R * (x i , k)| ,<label>(9)</label></formula><p>where | · | denotes the cardinality of the set. If R * (q, k) and R * (x i , k) share more elements, x i is more likely to be a true match. This usually helps us to distinguish some hard negative samples from the correct matches. During testing, this similarity distance is added to the Euclidean distance to re-rank the result. In the experiment, we show that re-ranking further improves our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we report the results on three large-scale datasets: Market-1501 , CUHK03  and DukeMTMC-reID <ref type="bibr" target="#b60">[Zheng et al., 2017b]</ref>. As for the detector, Market-1501 and CUHK03 (detected) datasets are automatically detected by DPM and face the misdetection problem. It is unknown if the manually annotated images after slight alignment would bring any extra benefit. So we also evaluate the proposed method on the manually annotated images of CUHK03 (labeled) and DukeMTMC-reID, which consist of hand-drawn bounding boxes. As shown in <ref type="figure">Fig. 4</ref>, the three datasets have different characteristics, i.e., scene variances, and detection bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>Market1501 is a large-scale person re-ID dataset collected in a university campus. It contains 19,732 gallery images, 3,368 query images and 12,936 training images collected from six cameras. There are 751 identities in the training set and 750 identities in the testing set without overlapping. Every identity in the training set has 17.2 photos on average. All images are automatically detected by the DPM detector <ref type="bibr" target="#b9">[Felzenszwalb et al., 2008]</ref>. The misalignment problem is common, and the dataset is close to the realistic settings. We use all the 12,936 detected images to train the network and follow the evaluation protocol in the original dataset. There are two evaluation settings. A single query is to use one image of one person as query, and multiple query means to use several images of one person under one camera as a query. CUHK03 contains 14,097 images of 1,467 identities . Each identity contains 9.6 images on average. We follow the new training/testing protocol proposed in <ref type="bibr" target="#b60">[Zhong et al., 2017]</ref> to evaluate the multishot re-ID performance. This setting uses a larger testing gallery and is different from the papers published earlier than <ref type="bibr" target="#b60">[Zhong et al., 2017]</ref>, such as <ref type="bibr" target="#b27">[Liu et al., 2016a]</ref> and <ref type="bibr" target="#b39">[Varior et al., 2016b]</ref>. There are 767 identities in the training set and 700 identities in the testing set (The former setting uses 1,367 IDs for training and the other 100 IDs for testing). Since we usually face a large-scale searching image pool cropped from surveillance videos, a larger testing pool is closer to the realistic image retrieval setting. In the "detected" set, all the bounding boxes are produced by DPM; in the "labeled" set, the images are all hand-drawn. In this paper, we evaluate our method on "detected" and "labeled" sets, respectively. If not specified, "CUHK03" denotes the detected set, which is more challenging.</p><p>DukeMTMC-reID is a subset of the DukeMTMC <ref type="bibr" target="#b35">[Ristani et al., 2016]</ref> and contains 36,411 images of 1,812 identities shot by eight high-resolution cameras. It is one of the largest pedestrian image datasets. The pedestrian images are cropped from hand-drawn bounding boxes. The dataset consists 16,522 training images of 702 identities, 2,228 query images of the other 702 identities and 17,661 gallery images. It is challenging because many pedestrians are in the similar clothes, and may be occluded by cars or trees. We follow the evaluation protocol in <ref type="bibr" target="#b60">[Zheng et al., 2017b]</ref>.</p><p>Evaluation Metrics. We evaluate our method with rank-1, 5, 20 accuracy and mean average precision (mAP). Here, rank-i accuracy denotes the probability whether one or more correctly matched images appear in topi. If no correctly matched images appear in the top-i  <ref type="table">Table 1</ref> Comparison of different methods on Market-1501, CUHK03 (detected), CUHK03 (labeled) and DukeMTMC-reID. Rank-1, 5, 20 accuracy (%) and mAP (%) are shown. Note that the base branch is the same as the classification baseline <ref type="bibr" target="#b57">[Zheng et al., 2016b]</ref>. We observe consistent improvement of our method over the individual branches on the three datasets.</p><p>of the retrieval list, rank-i = 0, otherwise rank-i = 1. We report the mean rank-i accuracy for query images.</p><p>On the other hand, for each query, we calculate the area under the Precision-Recall curve, which is known as the average precision (AP). The mean of the average precision (mAP) then is calculated, which reflects the precision and recall rate of the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>ConvNet. In this work, we first fine-tune the base branch on the person re-ID datasets. Then, the base branch is fixed while we fine-tune the whole network. Specifically, when fine-tuning the base branch, the learning rate decrease from 10 −3 to 10 −4 after 30 epochs.</p><p>We stop training at the 40th epoch. Similarly, when we train the whole model, the learning rate decrease from 10 −3 to 10 −4 after 30 epochs. We stop training at the 40th epoch. We use mini-batch stochastic gradient descent with a Nesterov momentum fixed to 0.9 to update the weights. Our implementation is based on the Matconvnet <ref type="bibr" target="#b40">[Vedaldi and Lenc, 2015]</ref> package. The input images are uniformly resized to the size of 224 × 224. In addition, we perform simple data augmentation such as cropping and horizontal flipping following <ref type="bibr" target="#b59">[Zheng et al., 2016d]</ref>. STN. For the affine estimation branch, the network may fall into a local minimum in early iterations. To stabilize training, we find that a small learning rate is useful. We, therefore, use a learning rate of 1 × 10 −5 for the final convolutional layer in the affine estimation branch. In addition, we set the all θ = 0 except that θ 11 , θ 22 = 0.8 and thus, the alignment branch starts training from looking at the center part of the Res2 Feature Maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation</head><p>Evaluation of the ResNet baseline. We implement the baseline according to the routine proposed in <ref type="bibr" target="#b57">[Zheng et al., 2016b]</ref>, with the details specified in Section 4.2. We report our baseline results in <ref type="table">Table 1</ref>. The rank-1 accuracy is 80.17%, 30.50%, 31.14% and 65.22% on Market1501, CUHK03 (detected), CUHK03 (labeled) and DukeMTMC-reID respectively. The baseline model is on par with the network in <ref type="bibr" target="#b59">[Zheng et al., 2016d</ref><ref type="bibr" target="#b57">,Zheng et al., 2016b</ref>. In our recent implementation, we use a smaller batch size of 16 and a dropout rate of 0.75. We have therefore obtained a higher baseline rank-1 accuracy 80.17% on Market-1501 than 73.69% in the <ref type="bibr" target="#b59">[Zheng et al., 2016d</ref><ref type="bibr" target="#b57">, Zheng et al., 2016b</ref>. For a fair comparison, we will present the results of our methods built on this new baseline. Note that this baseline result itself is higher than many previous works <ref type="bibr" target="#b2">[Barbosa et al., 2017</ref><ref type="bibr" target="#b38">, Varior et al., 2016a</ref><ref type="bibr" target="#b55">, Zheng et al., 2017a</ref><ref type="bibr" target="#b59">, Zheng et al., 2016d</ref>.</p><p>Base branch. vs. alignment branch To investigate how alignment helps to learn discriminative pedestrian representations, we evaluate the Pedestrian descriptors of the base branch and the alignment branch, respectively. Two conclusions can be inferred.</p><p>First, as shown in <ref type="table">Table 1</ref>, the alignment branch yields higher performance i.e., +3.64% / +4.15% on the two dataset settings (CUHK03 detected/labeled) and +3.14% on DukeMTMC-reID, and achieves a very similar result with the base branch on Market-1501. We speculate that Market-1501 contains more intensive detection errors than the other three datasets and thus, the effect of alignment is limited.</p><p>Second, although the CUHK (labeled) dataset and the DukeMTMC-reID dataset are manually annotated,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Single Query Multi. Query rank-1 mAP rank-1 mAP DADM <ref type="bibr" target="#b36">[Su et al., 2016]</ref> 39.4 19.6 49.0 25.8 BoW+kissme  44.42 20.76 --MR- <ref type="bibr">CNN [Ustinova et al., 2015</ref><ref type="bibr">]* 45.58 26.11 56.59 32.26 MST-CNN [Liu et al., 2016b</ref> 45.1 -55.4 -FisherNet <ref type="bibr" target="#b43">[Wu et al., 2016a]</ref>  <ref type="table">Table 2</ref> Rank-1 precision (%) and mAP (%) on Market-1501. We also provide results of the fine-tuned ResNet50 baseline which has the same accuracy with the base branch. * the respective paper is on ArXiv but not published.</p><p>the alignment branch still improves the performance of the base branch. This observation demonstrates that the manual annotations may not be good enough for the machine to learn a good descriptor. In this scenario, alignment is non-trivial and makes the pedestrian representation more discriminative.</p><p>The complementary of the two branches. As mentioned, the two descriptors capture the different pedestrian characteristic from the original image and the aligned image. We follow the setting in Section 3.4 and simply combine the two features to form a stronger pedestrian descriptor. The results are summarized in <ref type="table">Table 1</ref>. We observe a constant improvement on the three datasets when we concatenate the two branch descriptors. The fused descriptor improves +2.64%, +2.15%, +1.63% and 3.23% on Market-1501, CUHK03(detected), CUHK03(labeled) and DukeMTMC-reID, respectively. The two branches are complementary and thus, contain more meaningful information than a separate branch. Aside from the improvement of the accuracy, this simple fusion is efficient sine it does not introduce additional computation.</p><p>Parameter sensitivity. We evaluate the sensitivity of the person re-ID accuracy to the parameter α. As shown in <ref type="figure" target="#fig_2">Fig. 5</ref>, we report the rank-1 accuracy and mAP when tuning the α from 0 to 1. We observe that the change of rank-1 accuracy and mAP are relatively Method rank-1 mAP BoW+kissme  25.13 12.17 LOMO+XQDA <ref type="bibr">[Liao et al., 2015] 30.75</ref> 17.04 Gan <ref type="bibr" target="#b60">[Zheng et al., 2017b]</ref> 67.68 47.13 OIM <ref type="bibr" target="#b47">[Xiao et al., 2017]</ref> 68.1 -APR  70.69 51.88 SVDNet <ref type="bibr" target="#b37">[Sun et al., 2017]</ref> 76.7 56.8 Basel. <ref type="bibr" target="#b60">[Zheng et al., 2017b]</ref> 65.22 44.99 Ours 71.59 51.51 Ours + re-rank 75.94 66.74 <ref type="table">Table 3</ref> Rank-1 accuracy (%) and mAP (%) on DukeMTMC-reID. We follow the evaluation protocol in <ref type="bibr" target="#b60">[Zheng et al., 2017b]</ref>. We also provide the result of the finetuned ResNet50 baseline for fair comparison.</p><p>small corresponding to the α. Our reported result simply use α = 0.5. α = 0.5 may not be the best choice for a particular dataset. But if we do not foreknow the distribution of the dataset, it is a simple and straightforward choice.</p><p>Comparison with the state-of-the-art methods. We compare our method with the state-of-the-art methods on Market-1501, CUHK03 and DukeMTMC-reID in <ref type="table">Table 2, Table 4 and Table 3</ref>, respectively. On Market-1501, we achieve rank-1 accuracy = 85.78%, mAP = 76.56% after re-ranking, which is the best result compared to the published paper, and the second</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Detected Labeled rank-1 mAP rank-1 mAP BoW+XQDA  6.36 6.39 7.93 7.29 LOMO+XQDA <ref type="bibr" target="#b25">[Liao et al., 2015]</ref> 12  <ref type="table">Table 4</ref> Rank-1 accuracy (%) and mAP (%) on CUHK03 using the new evaluation protocol in <ref type="bibr" target="#b60">[Zhong et al., 2017]</ref>. This setting uses a larger testing gallery and is different from the papers published earlier than <ref type="bibr" target="#b60">[Zhong et al., 2017]</ref>, such as <ref type="bibr" target="#b27">[Liu et al., 2016a]</ref> and <ref type="bibr" target="#b39">[Varior et al., 2016b]</ref>. There are 767 identities in the training set and 700 identities in the testing set (The former setting uses 1,367 IDs for training and the other 100 IDs for testing). Since we usually face a large-scale searching image pool cropped from surveillance videos, a larger testing pool is more challenging and closer to the realistic image retrieval setting. So we evaluate the proposed method on the "detected" and "labeled" subsets according to this new multi-shot protocol. We also provide the result of our fine-tuned ResNet50 baseline for fair comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 6</head><p>Sample retrieval results on the three datasets. The images in the first column are queries. The retrieved images are sorted according to the similarity score from left to right. For each query, the first row shows the result of baseline <ref type="bibr" target="#b57">[Zheng et al., 2016b]</ref>, and the second row denotes the results of PAN. The correct and false matches are in the blue and red rectangles, respectively. Images in the rank lists obtained by PAN demonstrate amelioration in alignment. Best viewed when zoomed in.</p><p>best among all the available results including the arXiv paper. Our model is also adaptive to previous models. One of the previous best results is based on the model regularized by GAN <ref type="bibr" target="#b60">[Zheng et al., 2017b]</ref>. We combine the model trained on GAN generated images and thus, achieve the state-of-the-art result rank-1 accuracy = 88.57%, mAP = 81.53% on Market-1501. On CUHK03, we arrive at a competitive result rank-1 accuracy = 36.3%, mAP=34.0% on the detected dataset and rank-1 accuracy = 36.9%, mAP = 35.0% on the labeled dataset. After re-ranking, we further achieve a state-of-the art result rank-1 accuracy = 41.9%, mAP=43.8% on the detected dataset and rank-1 accuracy = 43.9%, mAP = 45.8% on the labeled dataset. On DukeMTMC-reID, we also observe a state-of-the-art result rank-1 accuracy = 75.94% and mAP = 66.74% after re-ranking. Despite the visual disparities among the three datasets, i.e., scene variance, and detection bias, we show that our method consistently improves the re-ID performance. As shown in <ref type="figure">Fig. 6</ref>, we visualize some retrieval results on the three datasets. Images in the rank lists obtained by PAN demonstrate amelioration in alignment. Comparing to the baseline, true matches which are misaligned originally receive higher ranks, while false matches have lower ranks Visualization of the alignment. We further visualize the aligned images in <ref type="figure">Fig. 7</ref>. As aforementioned, the proposed network does not process the alignment on the original image. To visualize the aligned images, we extract the predicted affine parameters and then apply the affine transformation on the originally detected images manually. We observe that the network does not perform perfect alignment as the human, <ref type="figure">Fig. 7</ref> Examples of pedestrian images before and after alignment on three datasets (Market-1501, DukeMTMC-reID and CUHK03). Pairs of input images and aligned images are shown. By removing excessive background or padding zeros to image borders, we observe that PAN reduces the scale and location variance.</p><p>but it more or less reduces the scale and position variance, which is critical for the network to learn the representations. So the proposed network improves the performance of the person re-ID.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Pedestrian alignment and re-identification are two innerconnected problems, which inspires us to develop an attention-based system. In this work, we propose the pedestrian alignment network (PAN), which simultaneously aligns the pedestrians within bounding boxes and learns the pedestrian descriptors. Taking advantage of the attention of CNN feature maps to the human body, PAN addresses the misalignment problem and person re-ID together and thus, improves the person re-ID accuracy. Except for the identity label, we do not need any extra annotation. We also observe that the manually cropped images are not as perfect as preassumed to be. Our network also improves the re-ID performance on the datasets with hand-drawn bounding boxes. Experiments on three different datasets indicate that our method is competitive with the state-of-the-art methods. In the future, we will continue to investigate the attention-based model and apply our model to other fields i.e., car recognition.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2</head><label>2</label><figDesc>Architecture of the pedestrian alignment network (PAN)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(1,1) refer to the bottom right pixel. For example, if</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5</head><label>5</label><figDesc>Sensitivity of person re-ID accuracy to parameter α. Rank-1 accuracy(%) and mAP(%) on three datasets are shown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>80.17 91.69 96.59 59.14 30.50 51.07 71.64 29.04 31.14 52.00 74.21 29.80 65.22 79.13 87.75 44.99 Alignment 2,048 79.01 90.86 96.14 58.27 34.14 54.50 72.71 31.71 35.29 53.64 72.43 32.90 68.36 81.37 88.64 47.14 PAN 4,096 82.81 93.53 97.06 63.35 36.29 55.50 75.07 34.00 36.86 56.86 75.14 35.03 71.59 83.89 90.62 51.51</figDesc><table><row><cell cols="2">Methods dim</cell><cell>1</cell><cell>Market-1501 5 20 mAP</cell><cell>CUHK03 (detected) 1 5 20 mAP</cell><cell>1</cell><cell>CUHK03 (labeled) 5 20 mAP</cell><cell>1</cell><cell>DukeMTMC-reID 5 20 mAP</cell></row><row><cell>Base</cell><cell>2,048</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An improved deep learning architecture for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Mapping appearance descriptors on 3d body models for people re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baltieri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>IJCV</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Looking beyond appearances: Synthetic training data for deep cnns in re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barbosa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.03153</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Similarity learning with spatial constraints for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Exemplar-guided similarity learning on polynomial kernel feature map for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>IJCV</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Person re-identification by multichannel parts-based cnn with improved triplet loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Custom pictorial structures for re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep feature learning with relative distance comparison for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2993" to="3003" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A discriminatively trained, multiscale, deformable part model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Felzenszwalb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Deep transfer learning for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Geng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06765</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick ; Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Evaluating appearance models for recognition, reacquisition, and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PETS</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Viewpoint invariant pedestrian recognition with an ensemble of localized features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao ;</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hermans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
	</analytic>
	<monogr>
		<title level="m">defense of the triplet loss for person reidentification</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning to align from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised joint alignment of complex images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jaderberg</surname></persName>
		</author>
		<title level="m">Spatial transformer networks. In NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johnson</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07571</idno>
		<title level="m">Densecap: Fully convolutional localization networks for dense captioning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Large scale metric learning from equivalence constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Köstinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krizhevsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Locally aligned feature transforms across views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang ;</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Human reidentification with transferred metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deepreid: Deep filter pairing neural network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Person re-identification by local maximal occurrence representation and metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Improving person re-identification by attribute and identity learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07220</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">End-to-end comparative attention networks for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04404</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multi-scale triplet cnn for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Fully convolutional attention networks for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05244</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Loy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Time-delayed correlation analysis for multi-camera activity understanding</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pcca: A new approach for distance learning from sparse pairwise constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jurie</forename><surname>Mignon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mignon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Person re-identification by support vector ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Prosser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Hello neighbor: Accurate object retrieval with k-reciprocal nearest neighbors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Performance measures and a data set for multi-target, multi-camera tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ristani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCVW</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Deep attributes driven multi-camera person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Su</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.05693</idno>
		<idno>arXiv:1512.05300</idno>
		<title level="m">Multiregion bilinear convolutional neural networks for person re-identification</title>
		<editor>Ustinova et al., 2015. Ustinova, E., Ganin, Y., and Lempitsky, V.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Svdnet for pedestrian retrieval</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Gated siamese convolutional neural network architecture for human re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Varior</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A siamese long short-term memory architecture for human re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Varior</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lenc ;</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Matconvnet -convolutional neural networks for matlab</title>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Querydriven iterated neighborhood graph search for large scale indexing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Deep linear discriminant analysis on fisher networks: A hybrid architecture for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01595</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Personnet: Person re-identification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.07255</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">An enhanced deep feature representation for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning deep feature representations with domain guided dropout for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Joint detection and identification feature learning for person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1604.01850</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Ranking optimization for person reidentification via similarity and dissimilarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deep metric learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Learning a discriminative null space for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Person re-identification by salience matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Unsupervised salience learning for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning mid-level filters for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Mars: A video benchmark for large-scale person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Pose invariant embedding for deep person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07732</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Scalable person reidentification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02984</idno>
		<title level="m">Person re-identification: Past, present and future</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.02531</idno>
		<title level="m">Person re-identification in the wild</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">A discriminatively learned cnn embedding for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05666</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Unlabeled samples generated by gan improve the person re-identification baseline in vitro</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07717</idno>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Re-ranking person re-identification with kreciprocal encoding</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
