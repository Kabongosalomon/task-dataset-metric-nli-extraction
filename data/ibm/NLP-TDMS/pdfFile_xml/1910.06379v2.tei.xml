<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DUAL-PATH RNN: EFFICIENT LONG SEQUENCE MODELING FOR TIME-DOMAIN SINGLE-CHANNEL SPEECH SEPARATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuo</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft</orgName>
								<address>
									<addrLine>One Microsoft Way</addrLine>
									<settlement>Redmond</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuya</forename><surname>Yoshioka</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft</orgName>
								<address>
									<addrLine>One Microsoft Way</addrLine>
									<settlement>Redmond</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Columbia University</orgName>
								<address>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DUAL-PATH RNN: EFFICIENT LONG SEQUENCE MODELING FOR TIME-DOMAIN SINGLE-CHANNEL SPEECH SEPARATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Speech separation</term>
					<term>deep learning</term>
					<term>time domain</term>
					<term>recurrent neural networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent studies in deep learning-based speech separation have proven the superiority of time-domain approaches to conventional timefrequency-based methods. Unlike the time-frequency domain approaches, the time-domain separation systems often receive input sequences consisting of a huge number of time steps, which introduces challenges for modeling extremely long sequences. Conventional recurrent neural networks (RNNs) are not effective for modeling such long sequences due to optimization difficulties, while one-dimensional convolutional neural networks (1-D CNNs) cannot perform utterance-level sequence modeling when its receptive field is smaller than the sequence length. In this paper, we propose dual-path recurrent neural network (DPRNN), a simple yet effective method for organizing RNN layers in a deep structure to model extremely long sequences. DPRNN splits the long sequential input into smaller chunks and applies intra-and inter-chunk operations iteratively, where the input length can be made proportional to the square root of the original sequence length in each operation. Experiments show that by replacing 1-D CNN with DPRNN and apply sample-level modeling in the time-domain audio separation network (TasNet), a new state-of-the-art performance on WSJ0-2mix is achieved with a 20 times smaller model than the previous best system.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Recent progress in deep learning-based speech separation has ignited the interest of the research community in time-domain approaches <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref>. Compared with standard time-frequency domain methods, time-domain methods are designed to jointly model the magnitude and phase information and allow direct optimization with respect to both time-and frequency-domain differentiable criteria <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref>.</p><p>Current time-domain separation systems can be mainly categorized into adaptive front-end and direct regression approaches. The adaptive front-end approaches aim at replacing the short-time Fourier transform (STFT) with a differentiable transform to build a front-end that can be learned jointly with the separation network. Separation is applied to the front-end output as with the conventional time-frequency domain methods applying the separation processes to spectrogram inputs <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref>. Being independent of the traditional time-frequency analysis paradigm, these systems are able to have a much more flexible choice on the window size and the number of basis functions for the front-end. On the other hand, the direct regression approaches learn a regression function from an input mixture * Work done during internship at Microsoft Research. to the underlying clean signals without an explicit front-end, typically by using some form of one-dimensional convolutional neural networks (1-D CNNs) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b10">10]</ref>.</p><p>A commonality between the two categories is that they both rely on effective modeling of extremely long input sequences. The direct regression methods perform separation at the waveform sample level, while the number of the samples can usually be tens of thousands, or sometimes even more. The performance of the adaptive front-end methods also depend on selection of the window size, where a smaller window improves the separation performance at the cost of a significantly longer front-end representation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">11]</ref>. This poses an additional challenge as conventional sequential modeling networks, including RNNs and 1-D CNNs, have difficulty on learning such long-term temporal dependency <ref type="bibr" target="#b13">[12]</ref>. Moreover, unlike RNNs that have dynamic receptive fields, 1-D CNNs with fixed receptive fields that are smaller than the sequence length are not able to fully utilize the sequence-level dependency <ref type="bibr" target="#b14">[13]</ref>.</p><p>In this paper, we propose a simple network architecture, which we refer to as dual-path RNN (DPRNN), that organizes any kinds of RNN layers to model long sequential inputs in a very simple way. The intuition is to split the input sequence into shorter chunks and interleave two RNNs, an intra-chunk RNN and an inter-chunk RNN, for local and global modeling, respectively. In a DPRNN block, the intra-chunk RNN first processes the local chunks independently, and then the inter-chunk RNN aggregates the information from all the chunks to perform utterance-level processing. For a sequential input of length L, DPRNN with chunk size K and chunk hop size P contains S chunks, where K and S corresponds to the input lengths for the inter-and intra-chunk RNNs, respectively. When K ≈ S, the two RNNs have a sublinear input length (O( √ L)) as opposed to the original input length (O(L)), which greatly decreases the optimization difficulty that arises when L is extremely large.</p><p>Compared with other approaches for arranging local and global RNN layers, or more general the hierarchical RNNs that perform sequence modeling in multiple time scales <ref type="bibr" target="#b15">[14]</ref><ref type="bibr" target="#b16">[15]</ref><ref type="bibr" target="#b17">[16]</ref><ref type="bibr" target="#b18">[17]</ref><ref type="bibr" target="#b19">[18]</ref><ref type="bibr" target="#b20">[19]</ref>, the stacked DPRNN blocks iteratively and alternately perform the intra-and inter-chunk operations, which can be treated as an interleaved processing between local and global inputs. Moreover, the first RNN layer in most hierarchical RNNs still receives the entire input sequence, while in stacked DPRNN each intra-or inter-chunk RNN receives the same sublinear input size across all blocks. Compared with CNN-based architectures such as temporal convolutional networks (TCNs) that only perform local modeling due to the fixed receptive fields <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b21">20]</ref>, DPRNN is able to fully utilize global information via the inter-chunk RNNs and achieve superior performance with an even smaller model size. In Section 4 we will show that by simply replacing TCN by DPRNN in a previously proposed time-domain separation system <ref type="bibr" target="#b3">[4]</ref>, the model is able to achieve a 0.7 dB (4.6%) relative improvement with respect to scaleinvariant signal-to-noise ratio (SI-SNR) <ref type="bibr" target="#b7">[8]</ref> on WSJ0-2mix with a 49% smaller model size. By performing the separation at the waveform sample level, i.e. with window size of 2 samples and hop size of 1 sample, a new state-of-the-art performance is achieved with a 20 times smaller model than the previous best system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">DUAL-PATH RECURRENT NEURAL NETWORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Model design</head><p>A dual-path RNN (DPRNN) consists of three stages: segmentation, block processing, and overlap-add. The segmentation stage splits a sequential input into overlapped chunks and concatenates all the chunks into a 3-D tensor. The tensor is then passed to stacked DPRNN blocks to iteratively apply local (intra-chunk) and global (inter-chunk) modeling in an alternate fashion. The output from the last layer is transformed back to a sequential output with overlap-add method. <ref type="figure" target="#fig_0">Figure 1</ref> shows the flowchart of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1.">Segmentation</head><p>For a sequential input W ∈ R N ×L where N is the feature dimension and L is the number of time steps, the segmentation stage splits W into chunks of length K and hop size P . The first and last chunks are zero-padded so that every sample in W appears and only appears in K/P chunks, generating S equal size chunks Ds ∈ R N ×K , s = 1, . . . , S. All chunks are then concatenated together to form a 3-D tensor T = [D1, . . . , DS] ∈ R N ×K×S .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2.">Block processing</head><p>The segmentation output T is then passed to the stack of B DPRNN blocks. Each block transforms an input 3-D tensor into another tensor with the same shape. We denote the input tensor for block b = 1, . . . , B as T b ∈ R N ×K×S , where T1 = T. Each block contains two sub-modules corresponding to intra-and inter-chunk processing, respectively. The intra-chunk RNN is always bi-directional and is applied to the second dimension of T b , i.e., within each of the S blocks:</p><formula xml:id="formula_0">U b = [f b (T b [:, :, i]), i = 1, . . . , S]<label>(1)</label></formula><p>where</p><formula xml:id="formula_1">U b ∈ R H×K×S is the output of the RNN, f b (·)</formula><p>is the mapping function defined by the RNN, and T b [:, :, i] ∈ R N ×K is the sequence defined by chunk i. A linear fully-connected (FC) layer is then applied to transform the feature dimension of U b back to that</p><formula xml:id="formula_2">of T bÛ b = [GU b [:, :, i] + m, i = 1, . . . , S]<label>(2)</label></formula><p>whereÛ ∈ R N ×K×S is the transformed feature, G ∈ R N ×H and m ∈ R N ×1 are the weight and bias of the FC layer, respectively, and <ref type="bibr" target="#b22">[21]</ref> is then applied toÛ, which we empirically found to be important for the model to have a good generalization ability:</p><formula xml:id="formula_3">U b [:, :, i] ∈ R H×K represents chunk i in U b . Layer normalization (LN)</formula><formula xml:id="formula_4">LN (Û b ) =Û b − µ(Û b ) σ(Û b ) + z + r<label>(3)</label></formula><p>where z, r ∈ R N ×1 are the rescaling factors, is a small positive number for numerical stability, and denotes the Hadamard product. µ(·) and σ(·) are the mean and variance of the 3-D tensor defined as</p><formula xml:id="formula_6">µ(Û b ) = 1 N KS N i=1 K j=1 S s=1Û b [i, j, s] (5) σ(Û b ) = 1 N KS N i=1 K j=1 S s=1 (Û b [i, j, s] − µ(Û b )) 2<label>(6)</label></formula><p>A residual connection is then added between the output of LN operation and the input T b :</p><formula xml:id="formula_7">T b = T b + LN (Û b )<label>(7)</label></formula><p>T b is then served as the input to the inter-chunk RNN sub-module, where the RNN is applied to the last dimension, i.e. the aligned K time steps in each of the S blocks:</p><formula xml:id="formula_8">V b = [h b (T b [:, i, :]), i = 1, . . . , K] (8) where V b ∈ R H×K×S is the output of RNN, h b (·)</formula><p>is the mapping function defined by the RNN, andT b [:, i, :] ∈ R N ×S is the sequence defined by the i-th time step in all S chunks. As the intrachunk RNN is bi-directional, each time step inT b contains the entire information of the chunk it belongs to, which allows the inter-chunk RNN to perform fully sequence-level modeling. As with the intrachunk RNN, a linear FC layer and the LN operation are applied on top of V b . A residual connection is also added between the output andT b to form the output for DPRNN block b. For b &lt; B, the output is served as the input to the next block T b+1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3.">Overlap-Add</head><p>Denote the output of the last DPRNN block as TB+1 ∈ R N ×K×S . To transform it back to a sequence, the overlap-add method is applied to the S chunks to form output Q ∈ R N ×L .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Discussion</head><p>Consider the sum of the input sequence lengths for the intra-and inter-chunk RNNs in a single block denoted by K + S where the hop size is set to be 50% (i.e. P = K/2) as in <ref type="figure" target="#fig_0">Figure 1</ref>. It is simple to see that S = 2L/K + 1 where · is the ceiling function. To achieve minimum total input length K + S = K + 2L/K + 1, K should be selected such that K ≈ √ 2L, and then S also satisfies S ≈ √ 2L ≈ K. This gives us sublinear input length (O( √ L)) rather than the original linear input length (O(L)).</p><p>For tasks that require online processing, the inter-chunk RNN can be made uni-directional, scanning from the first up to the current chunks. The later chunks can still utilize the information from all previous chunks, and the minimal system latency is thus defined by the chunk size K. This is unlike standard CNN-based models that can only perform local processing due to the fixed receptive field or conventional RNN-based models that perform frame-level instead of chunk-level modeling. The performance difference between the online and offline settings, however, is beyond the scope of this paper. The segmentation stage splits a sequential input into chunks with or without overlaps and concatenates them to form a 3-D tensor. In our implementation, the overlap ratio is set to 50%. (B) Each DPRNN block consists of two RNNs that have recurrent connections in different dimensions. The intra-chunk bi-directional RNN is first applied to individual chunks in parallel to process local information. The inter-chunk RNN is then applied across the chunks to capture global dependency. Multiple blocks can be stacked to increase the total depth of the network. (C) The 3-D output of the last DPRNN block is converted back to a sequential output by performing overlap-add on the chunks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTAL PROCEDURES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Model configurations</head><p>Although DPRNN can be applied to any systems that require longterm sequential modeling, we investigate its application to the timedomain audio separation network (TasNet) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b23">22]</ref>, an adaptive front-end method that achieves high speech separation performance on a benchmarking dataset. TasNet contains three parts: (1) a linear 1-D convolutional encoder that encapsulates the input mixture waveform into an adaptive 2-D front-end representation, (2) a separator that estimates C masking matrices for C target sources, and (3) a linear 1-D transposed convolutional decoder that converts the masked 2-D representations back to waveforms. We use the same encoder and decoder design as in <ref type="bibr" target="#b3">[4]</ref> while the number of filters is set to be 64. As for the separator, we compare the proposed deep DPRNN with the optimally configured TCN described in <ref type="bibr" target="#b3">[4]</ref>. We use 6 DPRNN blocks using BLSTM <ref type="bibr" target="#b24">[23]</ref> as the intra-and interchunk RNNs with 128 hidden units in each direction. The chunk size K for DPRNN is defined empirically according to the length of the front-end representation such that K ≈ √ 2L in the training set as discussed in Section 2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Dataset</head><p>We evaluate our approach on two-speaker speech separation and recognition tasks. The separation-only experiment is conducted on the widely-used WSJ0-2mix dataset <ref type="bibr" target="#b25">[24]</ref>. WSJ0-2mix contains 30 hours of 8k Hz training data that are generated from the Wall Street Journal (WSJ0) si tr s set. It also has 10 hours of validation data and 5 hours of test data generated by using the si dt 05 and si et 05 sets, respectively. Each mixture is artificially generated by randomly selecting different speakers from the corresponding set and mixing them at a random relative signal-to-noise ratio (SNR) between -5 and 5 dB. For the speech separation and recognition experiment, we create 200 hours and 10 hours of artificially mixed noisy reverberant mixtures sampled from the Librispeech dataset <ref type="bibr" target="#b26">[25]</ref> for training and validation, respectively. The 16 kHz signals are convolved with room impulse responses generated by the image method <ref type="bibr" target="#b27">[26]</ref>. The length and width of the room are randomly sampled between 2 and 10 meters, and the height is randomly sampled between 2 and 5 meters. The reverberation time (T60) is randomly sampled between 0.1 and 0.5 seconds. The locations for the speakers as well as the single microphone are all randomly sampled. The two reverberated signals are rescaled to a random SNR between -5 and 5 dB, and further shifted such that the overlap ratio between the two speakers is 50% on average. The resultant mixture is further corrupted by random isotropic noise at a random SNR between 10 and 20 dB <ref type="bibr" target="#b28">[27]</ref>. For evaluation, we generate mixture in the same manner sampled from Microsoft's internal gender-balanced clean speech collection consisting of 44 speakers. The target for separation is the reverberant clean speech for both speakers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Experiment configurations</head><p>We train all models for 100 epochs on 4-second long segments. The learning rate is initialized to 1e −3 and decays by 0.98 for every two epochs. Early stopping is applied if no best model is found in the validation set for 10 consecutive epochs. Adam <ref type="bibr" target="#b29">[28]</ref> is used as the optimizer. Gradient clipping with maximum L2-norm of 5 is applied for all experiments. All models are trained with utterance-level permutation invariant training (uPIT) <ref type="bibr" target="#b30">[29]</ref> to maximize scale-invariant SNR (SI-SNR) <ref type="bibr" target="#b7">[8]</ref>.</p><p>The effectiveness of the systems is assessed both in terms of signal fidelity and speech recognition accuracy. The degree of improvement in the signal fidelity is measured by signal-to-distortion ratio improvement (SDRi) <ref type="bibr" target="#b31">[30]</ref> as well as SI-SNR improvement (SI-SNRi). The speech recognition accuracy is measured by the word error rate (WER) on both separated speakers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">RESULTS AND DISCUSSIONS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Results on WSJ0-2mix</head><p>We first report the results on the WSJ0-2mix dataset. <ref type="table">Table 1</ref> compares the TasNet-based systems with different separator networks. We can see that simply replacing TCN by DPRNN improves the separation performance by 4.6% with a 49% smaller model. This shows the superiority of the proposed local-global modeling to the previous CNN-based local-only modeling. Moreover, the performance can be consistently improved by further decreasing the filter length (and the hop size as a consequence) in the encoder and decoder. The best performance is obtained when the filter length is 2 samples with an encoder output of more than 30000 frames. This can be extremely hard or even impossible for standard RNNs or CNNs to model, while with the proposed DPRNN the use of such a short filter becomes possible and achieves the best performance. <ref type="table">Table 2</ref> compares the DPRNN-TasNet with other previous systems on WSJ0-2mix. We can see that DPRNN-TasNet achieves a new record on SI-SNRi with a 20 times smaller model than Fur-caNeXt <ref type="bibr" target="#b4">[5]</ref>, the previous state-of-the-art system. The small model size and the superior performance of DPRNN-TasNet indicate that speech separation on WSJ0-2mix dataset can be solved without using enormous or complex models, revealing the need for using more challenging and realistic datasets in future research. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Speech separation and recognition results</head><p>We use a conventional hybrid system for speech recognition. Our recognition system is trained on large-scale single-speaker noisy reverberant speech collected from various sources <ref type="bibr" target="#b36">[35]</ref>. <ref type="table" target="#tab_1">Table 3</ref> compares TCN-and DPRNN-based TasNet models with a 2-ms window (32 samples with 16 kHz sample rate). We can observe that DPRNN-TasNet significantly outperforms TCN-TasNet in SI-SNRi and WER, showing that the speriority of DPRNN even under challenging noisy and reverberant conditions. This further indicates that DPRNN can replace conventional sequential modeling modules across a range of tasks and scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>In this paper, we proposed dual-path recurrent neural network (DPRNN), a simple yet effective way of organizing any types of RNN layers for modeling an extremely long sequence. DPRNN splits the sequential input into overlapping chunks and performs intra-chunk (local) and inter-chunk (global) processing with two RNNs alternately and iteratively. This design allows the length of each RNN input to be proportional to the square root of the original input length, enabling sublinear processing and alleviating optimization challenges. We also described an application to single-channel time-domain speech separation using time-domain audio separation network (TasNet). By replacing 1-D CNN modules with deep DPRNN and performing sample-level separation in the TasNet framework, a new state-of-the-art performance was obtained on WSJ0-2mix with a 20 times smaller model than the previously reported best system. Experimental results of noisy reverberant speech separation and recognition were also reported, proving DPRNN's effectiveness in challenging acoustic conditions. These results demonstrate the superiority of the proposed approach in various scenarios and tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>System flowchart of dual-path RNN (DPRNN). (A)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>Comparison of different separator networks and configurations on WSJ0-2mix in TasNet-based speech separation. Prior work used TCN-TasNet. Comparison with other methods on WSJ0-2mix.</figDesc><table><row><cell>Separator</cell><cell>Model</cell><cell>Window</cell><cell cols="2">Chunk size</cell><cell cols="2">SI-SNRi</cell><cell>SDRi</cell></row><row><cell>network</cell><cell>size</cell><cell>(samples)</cell><cell>(frames)</cell><cell></cell><cell>(dB)</cell><cell>(dB)</cell></row><row><cell>TCN</cell><cell>5.1M</cell><cell>16</cell><cell>-</cell><cell></cell><cell>15.2</cell><cell>15.5</cell></row><row><cell></cell><cell></cell><cell>16</cell><cell>100</cell><cell></cell><cell>16.0</cell><cell>16.2</cell></row><row><cell cols="2">DPRNN 2.6M</cell><cell>8 4</cell><cell>150 200</cell><cell></cell><cell>17.0 17.9</cell><cell>17.3 18.1</cell></row><row><cell></cell><cell></cell><cell>2</cell><cell>250</cell><cell></cell><cell>18.8</cell><cell>19.0</cell></row><row><cell></cell><cell>Method</cell><cell></cell><cell>Model size</cell><cell cols="2">SI-SNRi (dB)</cell><cell>SDRi (dB)</cell></row><row><cell></cell><cell cols="2">DPCL++ [31]</cell><cell>13.6M</cell><cell></cell><cell>10.8</cell><cell>-</cell></row><row><cell cols="3">uPIT-BLSTM-ST [29]</cell><cell>92.7M</cell><cell></cell><cell>-</cell><cell>10.0</cell></row><row><cell></cell><cell cols="2">ADANet [32]</cell><cell>9.1M</cell><cell></cell><cell>10.4</cell><cell>10.8</cell></row><row><cell cols="3">WA-MISI-5 [33]</cell><cell>32.9M</cell><cell></cell><cell>12.6</cell><cell>13.1</cell></row><row><cell cols="3">Conv-TasNet-gLN [4]</cell><cell>5.1M</cell><cell></cell><cell>15.3</cell><cell>15.6</cell></row><row><cell cols="4">Sign Prediction Net [34] 55.2M</cell><cell></cell><cell>15.3</cell><cell>15.6</cell></row><row><cell cols="3">Deep CASA [20]</cell><cell>12.8M</cell><cell></cell><cell>17.7</cell><cell>18.0</cell></row><row><cell></cell><cell cols="2">FurcaNeXt [5]</cell><cell>51.4M</cell><cell></cell><cell>-</cell><cell>18.4</cell></row><row><cell cols="3">DPRNN-TasNet</cell><cell>2.6M</cell><cell></cell><cell>18.8</cell><cell>19.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>SI-SNRi and WER results for noisy reverberant separation and recognition task. Window size is set to 32 samples for both models, and the chunk size is set to 100 frames for DPRNN-TasNet. WER is calculated for both separated speakers.</figDesc><table><row><cell>Separator</cell><cell>Model</cell><cell>SI-SNRi</cell><cell>WER</cell></row><row><cell>network</cell><cell>size</cell><cell>(dB)</cell><cell>(%)</cell></row><row><cell>TCN</cell><cell>5.1M</cell><cell>7.6</cell><cell>28.7</cell></row><row><cell>DPRNN</cell><cell>2.6M</cell><cell>8.4</cell><cell>25.9</cell></row><row><cell>Noise-free reverberant speech</cell><cell>-</cell><cell>-</cell><cell>9.1</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">TasNet: time-domain audio separation network for real-time, single-channel speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Wave-U-Net: A multi-scale neural network for end-to-end audio source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Stoller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ewert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Dixon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Society for Music Information Retrieval (ISMIR) Conference</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="334" to="340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">End-to-end source separation with adaptive front-ends</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shrikant</forename><surname>Venkataramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonah</forename><surname>Casebeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paris</forename><surname>Smaragdis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 52nd Asilomar Conference on Signals, Systems, and Computers</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="684" to="688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Conv-TasNet: Surpassing ideal time-frequency magnitude masking for speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP)</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1256" to="1266" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">FurcaNeXt: End-to-end monaural speech separation with dynamic gated dilated temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqiang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiqing</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anyan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimedia Modeling</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="653" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A comprehensive study of speech separation: Spectrogram vs waveform separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahimeh</forename><surname>Bahmaninezhad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongzhi</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi-Xiong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4574" to="4578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">End-to-end waveform utterance enhancement for direct evaluation metrics optimization by fully convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Szu-Wei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao-Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Tsao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xugang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisashi</forename><surname>Kawai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP)</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1570" to="1584" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">SDR-half-baked or well done?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP), 2019 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="626" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">FaSNet: Low-latency adaptive beamforming for multi-microphone audio processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enea</forename><surname>Ceolini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Chii</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">End-to-end music source separation: Is it possible in the waveform domain?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesc</forename><surname>Lluís</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Serra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4619" to="4623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Universal sound separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Kavalerov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Patton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="175" to="179" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Independently recurrent neural network (indrnn): Building a longer and deeper rnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbo</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5457" to="5466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">An empirical evaluation of generic convolutional and recurrent networks for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Zico</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01271</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A hierarchical neural autoencoder for paragraphs and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1106" to="1115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Hierarchical multiscale recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.01704</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">SampleRNN: An unconditional end-to-end neural audio generation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soroush</forename><surname>Mehri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kundan</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rithesh</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubham</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Sotelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.07837</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Chunk-based decoder for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shonosuke</forename><surname>Ishiwatari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingtao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoki</forename><surname>Yoshinaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaru</forename><surname>Kitsuregawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijia</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1901" to="1912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Chunk-based bi-scale decoder for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="580" to="586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dilated recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Witbrock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="77" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Divide and conquer: A deep casa approach to talker-independent monaural speaker separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuzhou</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deliang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech and Language Processing (TASLP)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2092" to="2102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Layer normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Real-time single-channel dereverberation and separation with time-domain audio separation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Mesgarani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="342" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep clustering: Discriminative embeddings for segmentation and separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="31" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Librispeech: an asr corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassil</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoguo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Image method for efficiently simulating small-room acoustics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Berkley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="943" to="950" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Generating sensor signals in isotropic noise fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Emanuël</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Habets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gannot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">122</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3464" to="3470" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multitalker speech separation with utterance-level permutation invariant training of deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morten</forename><surname>Kolbaek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Hua</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesper</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP)</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1901" to="1913" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Performance measurement in blind audio source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Gribonval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cédric</forename><surname>Févotte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP)</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1462" to="1469" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Single-channel multi-speaker separation using deep clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuf</forename><surname>Isik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="545" to="549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Speakerindependent speech separation with deep attractor network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP)</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="787" to="796" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">End-to-end speech separation with unfolded iterative phase reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhong-Qiu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deliang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2718" to="2712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep learning based phase reconstruction for speaker separation: A trigonometric perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhong-Qiu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deliang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP), 2019 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="71" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Meeting transcription using virtual microphone arrays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuya</forename><surname>Yoshioka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Dimitriadis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Hinthorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuedong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zeng</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1905.02545" />
		<imprint>
			<date type="published" when="2019-07" />
			<pubPlace>Microsoft Research</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep. MSR-TR-2019-11</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
