<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient parametrization of multi-domain deep neural networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvestre-Alvise</forename><surname>Rebuffi</surname></persName>
							<email>srebuffi@robots.ox.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Visual Geometry Group</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
							<email>hbilen@ed.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
							<email>vedaldi@robots.ox.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Visual Geometry Group</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Efficient parametrization of multi-domain deep neural networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A practical limitation of deep neural networks is their high degree of specialization to a single task and visual domain. Recently, inspired by the successes of transfer learning, several authors have proposed to learn instead universal, fixed feature extractors that, used as the first stage of any deep network, work well for several tasks and domains simultaneously. Nevertheless, such universal features are still somewhat inferior to specialized networks.</p><p>To overcome this limitation, in this paper we propose to consider instead universal parametric families of neural networks, which still contain specialized problem-specific models, but differing only by a small number of parameters. We study different designs for such parametrizations, including series and parallel residual adapters, joint adapter compression, and parameter allocations, and empirically identify the ones that yield the highest compression. We show that, in order to maximize performance, it is necessary to adapt both shallow and deep layers of a deep network, but the required changes are very small. We also show that these universal parametrization are very effective for transfer learning, where they outperform traditional fine-tuning techniques.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>As deep neural networks continue to dramatically improve results in almost all traditional problems in computer vision, the interest of the community has started to shift towards more ambitious goals. One of them is to supersede the common paradigm of addressing different image understanding problems independently, using ad-hoc solutions and learning different and largely incompatible models for each of them. Just like the human brain is capable of addressing a very large number of different image analysis tasks, so it should be possible to develop models that address well and efficiently a variety of different computer vision problems, with better efficiency and generalization (a) universal parametric family (b) universal feature extractor <ref type="figure">Figure 1</ref>: Universal parametric network families. We develop compact parametric families of neural networks (a) that can target very different visual domains, from Ima-geNet to stop signs and characters, while sharing the vast majority of their parameters w. Domain-specific parameters α t are isolated in small modular adapters that can be attached to an existing network to steer it non-disruptively to different domains and enable efficient model storage, transfer, and exchange, as well as transfer learning. Parametric families are shown empirically to be much more powerful than sharing a fixed universal feature extractor as in (b). than individual networks. There are at least three aspects to this challenge. The first is to construct a multi-task model that can extract multiple types of information from an image, performing class/object detection and segmentation, boundary extraction, motion estimation, etc. <ref type="bibr" target="#b13">[14]</ref>. The second is to develop a multi-domain model that can work well for many different visual domains, such as Internet images, scene text, medical images, satellite images, driving images, etc <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b22">23]</ref>. The third is to develop an extensible model that can evolve over time, reusing previously acquired knowledge to learn to process new tasks and domains efficiently, while at the same time avoiding to forget previously-acquired abilities <ref type="bibr" target="#b17">[18]</ref>.</p><p>Concerned with the second and third problem, several authors before us have framed this as the problem of learning a single universal first-stage to be shared among different deep networks ( <ref type="figure">fig. 1.b)</ref>. The idea is that early layers should process low-level and hence widely-applicable visual information. However, such universal feature extractors do not work quite as well as learning problem-specific networks, either from scratch or using transfer learning.</p><p>In this paper, we propose an alternative perspective. Instead of seeking a single, fixed first stage, we want to develop compact parametrizations for multi-domain networks ( <ref type="figure">fig. 1.a)</ref>. Consider a deep network Φ(x; w, α) applied to an image x, for example for image classification. We partition the network parameters in a universal vector w, which is fixed and shared among all domains, and a parameter vector α, which is instead domain specific. We then seek architectures that: 1) can share the vast majority of their parameters, so that the size of α is a small fraction of the size of w, and 2) can learn a new α for a new domain from a very small number of training examples. In other words, we would like to compress a family of domain-specific neural networks so that they can be exchanged and learned more efficiently.</p><p>While our method does not result in a single, universal neural network, as the parameters α are still domainspecific, finding architectures that afford a great degree of parameter sharing is an important step in this direction. There are also concrete practical benefits. First, universal families work better than the standard transfer learning approach of fine-tuning off-the-shelf models; hence, they may replace the latter strategy in numerous applications. Second, there are applications such as mobile devices that require running several different neural networks, which may incur a significant computational and energy overhead due simply to the need of swapping their parameters on a dedicated integrated circuit. One may face similar overheads when transmitting model parameters over a network, or storing them locally. Our approach makes storing, exchanging, and updating models much more efficient.</p><p>Related to our work, a few papers <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b24">25]</ref> have proposed low-dimensional parametrizations of the filters in a neural network with good compression results. The paper of <ref type="bibr" target="#b22">[23]</ref>, in particular, proposed the idea of residual adapters as base building for networks with a high-degree of parameter sharing. In this work, we propose some important improvements over this basic module. First, we show that a simple change, where the topology of the adapter is parallel rather than series, results in major improvements across the board, in terms of overall accuracy, applicability to existing off-the-shelf network, and transfer learning. Second, we in-vestigate which parts of typical network require adaptation, and we show that often both early and late layers need to be adapted to obtain the best performance. Third, we experiment with different regularization strategies for the adapters such as dropout which proves highly beneficial when using a bigger pretrained network. Fourth, we introduce a crossdomain compression procedure for the adapters which allows to reduce significantly the numbers of adapters parameters. Most importantly, this compression contributes to multi-domain regularization resulting in improved overall performance thanks to information sharing among target datasets.</p><p>The rest of the paper is organized as follows. Section 2 discusses related work. Section 3 describes our neural network parametrization and how it applies to state-of-the-art neural network architectures. Section 4 demonstrates empirically the power of our approach on standard datasets, setting in particular the new state of the art on the Visual Decathlon benchmark, as well as demonstrating excellent transfer learning capabilities. Finally, section 5 summarizes our findings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Our work intersects with various lines of research in multi-task learning, learning without forgetting, domain adaptation, and other areas.</p><p>Multi-task learning (MTL) aims at learning multiple related tasks simultaneously by sharing information and computation among them. Early work <ref type="bibr" target="#b4">[5]</ref> in this area focuses on deep neural network (DNN) models which share weights in the earlier layers and use specialized ones in the later layers. It is shown in <ref type="bibr" target="#b4">[5]</ref> that sharing parameters during training helps exploiting regularities present across tasks and improving the performance by constraining the learned representation. However this setting requires to manually design the network and decide which layers should be shared across multiple tasks. This paradigm is applied to various learning problems from natural language processing <ref type="bibr" target="#b5">[6]</ref> and automated drug discovery <ref type="bibr" target="#b6">[7]</ref> to speech recognition <ref type="bibr" target="#b11">[12]</ref>. In computer vision, deep MTL models are applied to object tracking <ref type="bibr" target="#b33">[34]</ref>, facial-landmark detection <ref type="bibr" target="#b34">[35]</ref>, object and part detection <ref type="bibr" target="#b1">[2]</ref>, object detection and instance segmentation <ref type="bibr" target="#b9">[10]</ref>, a collection of low-level and high-level vision tasks <ref type="bibr" target="#b13">[14]</ref>. Differently from our work, this line of research focuses on learning a diverse set of tasks in the same visual domain.</p><p>Multi-domain learning. Our method is most related to recent works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25]</ref> which aim at learning a single network to perform image classification tasks in a diverse set of domains. The main focus is to learn a single network that can represent compactly all the domains with minimal number of task specific parameters. To do so, Bilen and Vedaldi <ref type="bibr" target="#b2">[3]</ref> propose to model different domains in a sin-gle neural network by sharing all core model parameters except parameters in batch and instance normalization layers. Rebuffi et al. <ref type="bibr" target="#b22">[23]</ref> extend <ref type="bibr" target="#b2">[3]</ref> and propose a new parameterization of the standard residual network architecture that enables a high degree of parameter sharing between domains with a small increase (&lt; 10%) in the model parameters. The authors of <ref type="bibr" target="#b24">[25]</ref> propose a parameter-efficient architecture that enables learning new domains sequentially without forgetting. We build our method on <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b24">25]</ref> and significantly improve over them in terms of accuracy and compression ratio by introducing a novel and more compact adapter module, and a better regularization strategy.</p><p>Parameterized MTL. Another MTL approach <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b19">20]</ref> focuses on dynamically generating DNN weights given the task identity. Bertinetto et al. <ref type="bibr" target="#b0">[1]</ref> propose a method to learn the parameters of a deep model from a single exemplar for one-shot classification. As a naive predicting of high dimensional weights is not feasible, the authors first obtain a low rank decomposition of filters and define the new network as a linear combination of the low-rank filters. Similarly, the authors of <ref type="bibr" target="#b31">[32]</ref> propose a tensor factorization method that can realize automatic learning of endto-end knowledge sharing in deep networks. <ref type="bibr">Meyerson and Miikkulainen [20]</ref> propose a soft ordering approach, which dynamically computes to what extent each filter contributes to each tasks and thus how much is shared across different tasks. As a matter of fact, we also use a similar low rank decomposition technique to the one in <ref type="bibr" target="#b0">[1]</ref>. However, the decomposition is used to design a more compact sharing across tasks.</p><p>Domain adaptation. There is a rich body of work in domain adaptation including the ones in deep learning such as <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b30">31]</ref> that minimizes the domain discrepancy. The authors of <ref type="bibr" target="#b18">[19]</ref> propose a deep network architecture that can jointly learn adaptive classifiers and transferable features from the source to target domain by modeling source classifier as sum of target classifier and a residual function. Bousmalis et al. <ref type="bibr" target="#b3">[4]</ref> consider an explicit parameterization of domain-generic and domain-specific that learns to extract image representations from the partitioned subspaces. Li et al. <ref type="bibr" target="#b16">[17]</ref> propose a meta-learning method that trains any given model to be more robust to domain shift. Our method differs to this group of work in two important aspects: First, in addition to domain change (e.g. DSLR vs. webcam), each domain contains a unique set of outputs (i.e. object categories) in our case. Second, domain adaptation typically aims to maximise performance on the target domain regardless of potential forgetting.</p><p>Life-long learning. Another important research direction in MTL is sequential learning of multiple tasks <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b29">30]</ref>. While the key idea is to exploit the knowledge from the previous tasks, learning sequentially typically suffers from forgetting the previous tasks, a phenomenon referred as "catas- trophic forgetting" in <ref type="bibr" target="#b7">[8]</ref>. Recent work <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b25">26]</ref> address this problem by freezing the network parameters for the old tasks and only updating the parameters of the new task which leads to a linear growth in the number of total parameters with the number of tasks. Another approach is to preserve the previous knowledge by retaining the response of the original network on the new task <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b23">24]</ref>. The problem is also addressed by keeping the network parameters <ref type="bibr" target="#b12">[13]</ref> and features <ref type="bibr" target="#b21">[22]</ref> of the new task close to the original ones. Our method can also be related to both <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b12">13]</ref>, as it retains the knowledge of previous tasks perfectly, while adding a small number of extra parameters for the new tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>This section describes different ways of constructing a parametric family of neural networks that can tackle multiple domains while sharing the vast majority of their parameters. Section 3.1 introduces a number of adapter modules. These modules attach to a standard deep neural network architecture such as ResNet <ref type="bibr" target="#b10">[11]</ref> to steer it to different problems by means of a small number of adaptation parameters. Section 3.2 discusses different ways in which residual adapters can be injected in a standard neural network, section 3.3 how they can be regularized, and section 3.4 how the parameters can be further compressed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Adapter modules</head><p>We begin by reviewing the recent adapter modules of <ref type="bibr" target="#b22">[23]</ref> (section 3.1.1). We then discuss a number of alternative designs that, as shown empirically in section 4, perform significantly better (3.1.2). These modules are illustrated in <ref type="figure" target="#fig_0">fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Series residual adapters</head><p>The residual adapter modules introduced by <ref type="bibr">[</ref> </p><formula xml:id="formula_0">y = ρ(x; α) = x + diag 1 (α) * x.</formula><p>If the input tensor has shape x ∈ R H×W ×C , then α ∈ R C×C has O(C 2 ) parameters. Here we use the operator diag L (A) ∈ R L×L×C×D to reshape a matrix A ∈ R C×D in a bank of "diagonal" filters:</p><formula xml:id="formula_1">[diag L (A)] vucd = A dc , v = u = (L − 1)/2 + 1, 0, otherwise.</formula><p>This operator transforms the matrix A into a 1×1 filter bank embedded as the central element of a larger L×L filter bank by appending zeros around it (L is assumed to be odd). An advantage of this relatively cumbersome notation is that we can rewrite the module as a single filter:</p><formula xml:id="formula_2">ρ(x; α) = diag 1 (I + α) * x</formula><p>The rationale for the additive parameterization is that the identity function is recovered if α = 0. This is the case when a strong regularizer is applied on α during learning, shrinking the weights towards zero. In turn, this allow to easily control the adaptation strength, and thus generalization.</p><p>Residual adapters are installed in series with standard filter banks f ∈ R L×L×C×C in the neural network. So for example a typical sequence is</p><formula xml:id="formula_3">z = ρ(f * x; α) = (diag 1 (I + α) * f ) * x.</formula><p>This can also be interpreted as a low-rank decomposition of a filter bank g, using f as a basis:</p><formula xml:id="formula_4">ρ(f * x; α) = g * x, [g] vucd = d (1 + α dc )[f ] vucd .</formula><p>This also means that the adapters can be "fused" with the convolutional layer f by computing g explicitly, with no added evaluation cost at test time. However, this operation is difficult to undo, preventing from retargeting the network to another problem, which may be inappropriate in certain applications.</p><p>Size of the adapters. In this configuration, the adapter parameters are a fraction C 2 /L 2 C 2 = 1/L 2 of the filter bank parameters. For example, for a 3 × 3 filter bank, L = 3 and the adapters are 9 times smaller. Relationship to batch normalization. For learning, it is customary to inject batch normalization (BN) layers in architectures, especially of the very deep variety such as ResNet. <ref type="figure" target="#fig_0">Figure 2.</ref>(a) illustrates a complete residual module, inclusive of BN, ReLU, convolution, and adapter layers for the series configuration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Parallel residual adapters</head><p>While in the previous section adapters are installed in series with existing filter banks f , we propose here an alternative configuration in which adapters are connected in parallel instead ( <ref type="figure" target="#fig_0">fig. 2.b)</ref>:</p><formula xml:id="formula_5">y = f * x + diag 1 (α) * x = (f + diag L (α)) * x.</formula><p>Parallel adapters can also be interpreted as a lowdimensional parametrization of a filter bank g:</p><formula xml:id="formula_6">ρ(f * x; α) = g * x, [g] vucd = [f ] vucd + α dc , v = u = (L − 1)/2 + 1, 0, otherwise.</formula><p>However, differently from series ones, in this case the decomposition is affine. The parameters f can be thought as a universal filter bank which is adjusted additively by modifying the "diagonal" elements of the filters based on α.</p><p>Like for the series residual adapters, at test time it is possible to "fuse" the adapters α and filters f by computing g explicitly. Differently from that case, however, this additive change can be easily undone to allow to retarget the network to a new task. Size of the adapters. If f ∈ R L×L×C×C , then α ∈ R C×C has the same dimensions as before, so parallel and series adapters have the same number of parameters. It also benefits from the same shrinking to identity property, as setting α = 0 recovers f . Relationship to batch normalization. Just as for series adapters, injection in a neural network such as ResNet <ref type="bibr" target="#b10">[11]</ref> requires to clarify the relationship between the adapters and other layers such as BN. This is illustrated in <ref type="figure" target="#fig_0">fig. 2.(b)</ref>. Note that the parallel configuration is significantly simpler. For example, compared to the parallel adapters, it saves one BN layer per application. Further discussion.</p><p>For both residual and parallel adapters, the filters g are points in a certain low-dimensional affine subspace parameterized by α. However, for residual adapters the affine subspace is linear (passes through the origin) and its orientation is variable. For parallel adapters the subspace is affine and the orientation is fixed (given by coordinate axis along the "diagonal").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Network architecture</head><p>Having chosen a type of adapter modules, the next question is how they can be best applied to a deep neural network. Adapters may be applied throughout its depth, or more adaptation may be required at the shallower, intermediate, or deeper layers.</p><p>To explore these design strategies, we consider as baseline model ResNet <ref type="bibr" target="#b10">[11]</ref> in the 26-layer configuration (suitable for medium-sized images). This network (section 3.2) is formed of 3 macro-blocks of convolutional layers, each outputting 64, 128 and 256 feature channels. Each macroblock contains 4 residual blocks each, each of which consists of two convolutional layers using 3 × 3 filters and a skip connection. The resolution of the data is halved from a macro-block to the next using average pooling. Note that, compared to other architecture such as AlexNet <ref type="bibr" target="#b15">[16]</ref> and VGG16 <ref type="bibr" target="#b26">[27]</ref>, ResNet has a minimal fully-connected layer, meaning that abstraction is likely to increase more uniformly throughout the convolutional part of the network.</p><p>In order to experiment with different placements for the adapters, ResNet is broken down into three trunks: early, mid, and late, corresponding to the three macro blocks. Empirically (section 4), we apply the adapters to each stage individually, or to the three stages together. We also experiment with distributing adapters throughout the depth of the model, but skipping one every two, by adapting only the second convolutional layer in each residual block.</p><p>Note that the adapter dimensionality is determined by the number of channels in different layers of the architectures. Adapters applied to deeper layers are therefore bigger because the number of feature channels increases with depth. In section 3.4 we show how adapters can be further compressed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Regularization: shrinkage vs dropout</head><p>One advantage of residual adapters is that they revert to the original neural network when α is zero. This is true for series adapter (as noted before) as well as for parallel adapters.</p><p>However, there are many alternative forms of regularization that apply to deep networks. For example, BN layers are noisy by construction, and are known to help regularize learning. Another well known method is dropout <ref type="bibr" target="#b27">[28]</ref>. In the experiments, shrinkage is compared empirically against dropout, and the latter is shown to be necessary when using a bigger pretrained network. Note that, due to the additive nature of the adapter, dropout in this case is akin to injecting additive noise to the output of the network filters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Cross-domain adapter compression</head><p>The size of a residual adapter is determined by the number of feature channels of the convolutional layer it is applied to. For deep layers in a neural network, where the number of channels C can be quite large, the number C 2 of adapter parameters can still be non-negligible.</p><p>In order to address this issue, we propose to further compress the adapters. A simple approach is to consider a low rank decomposition α = βγ of the adapter matrix α ∈ R C×C , where β, γ ∈ R C×K and K C. Such a decomposition can be obtained efficiently using the SVD to minimize the reconstruction residual α − βγ F . After replacing α with β, γ, the latter are fine-tuned again on the target task to improve performance further. This scheme uses a fraction 2KC/C 2 = 2K/C of the parameters.</p><p>Better compression can be obtained by decomposing the adapters jointly for all domains. In order to do so, let α 1 , . . . , α T ∈ R C×C be domain-specific adapters for T tasks. After stacking these matrices, computing the SVD decomposition of the result, and retaining only the top K singular values, one gets:</p><formula xml:id="formula_7">α 1 . . . α T = U ΣV = U . . . Σ . . . V 1 |V T . . . . . . where U,V t ∈ R C×K , U U = tV tVt = I ∈ R K×K andΣ ∈ K × K is diagonal. Setting β = UΣ and γ t =V t ,</formula><p>we obtain the approximation:</p><formula xml:id="formula_8">∀t = 1, . . . , T : α t ≈ βγ t<label>(1)</label></formula><p>where β, γ t ∈ R C×K . In this case, β is shared between domains acting as a common metric and only the factors γ t are fine-tuned to simplify optimization. The total number of parameters in (β, γ 1 , . . . , γ T ) over the parameters in (α 1 , . . . , α T ) for a large number of tasks T is given by</p><formula xml:id="formula_9">T CK + CK T C 2 → K C .</formula><p>In practice, we show that good results can be obtained by setting K = C/2, therefore with a 2× reduction in the adapter parameters. Joint compression also allows target tasks to communicate and further share parameters (β), in contrast with <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b24">25]</ref> where adapters are independent. We show that this results in a multi-task regularizer which allows each domain to further benefit from the knowledge of the others. Finally, note that the parallel adapters can be seen as a parametrization of filters spanning a fixed coordinate subspace. Equation (1) provides a more efficient parametrization of the same subspace, resulting in a higher degree of parameter sharing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>This section thoroughly assesses the proposed designs, including the topology and position of the residual adapters  <ref type="figure" target="#fig_0">fig. 2</ref>, adapters are added to each residual block (here given by a pair of convolutional blocks). We experiment with focusing adaptation on different segments of the network: early, mid, and late. adapters and the regularization and compression strategies introduced in section 3. We evaluate these decisions quantitatively in multi-domain learning (section 4.1) and transfer learning scenarios (section 4.2). We share our code and models in https://github.com/srebuffi/ residual_adapters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Learning multiple domains</head><p>We first investigate the problem of learning multiple, visually-diverse domains using a parameterized neural network family. To this end, we use the recently-introduced Visual Decathlon benchmark <ref type="bibr" target="#b22">[23]</ref>. This benchmark consists of 10 different well known datasets, from ImageNet, to OmniGlot (glyphs) and German Traffic Signs. In the benchmark, images are resized to a common resolution of roughly 72 pixels to accelerate evaluation. Furthermore, given the different nature and difficulty of the problems, results are reported both in terms of top-1 accuracy as well as using a "Decathlon score" that rebalances the different problems making them comparable <ref type="bibr" target="#b22">[23]</ref>  <ref type="table">(see table 1</ref>).</p><p>Following <ref type="bibr" target="#b22">[23]</ref>, we first train the universal parameters w of the model using the ImageNet data with a 26 layer ResNet <ref type="bibr" target="#b10">[11]</ref> via a stochastic gradient optimization with momentum and finally obtain 60.32% top-1 accuracy on 72 pixel resized validation set. As the first baseline, we finetune the pre-trained network for each dataset separately, denote it as "Finetuning" and report its performance in table 1. This standard procedure produces a strong baseline with competitive results, 76.9% mean accuracy and score 3096. However it requires ten times more parameter capacity than the base network, as it needs to train one network for each domain. Parallel vs Series. Next, we compare different topologies for the adapter modules, series and parallel (see section 3.1). For both settings, we first freeze the weights of the pre-trained ImageNet model and learn only the adaptation parameters α 1,··· ,K for each domain. Compared to fine-tuning, adding class-specific adapters lead to a modest increase (2× vs 10×) in total number of parameters. Despite their compactness, both approaches outperform the fine-tuning baseline, achieving similar or better accuracy over all datasets. This indicates that substantial parameter sharing is possible. We also see that the parallel configuration outperforms the series one (by 1 point in average accuracy and 250 decathlon points).</p><p>The parallel configuration has the key advantage of being plug-and-play whereas the series configuration of <ref type="bibr" target="#b22">[23]</ref> requires the adapters to be included when ResNet is pretrained on ImageNet. Indeed, adding them a-posteriori decreases performance substantially (-1.73 point in accuracy on average over 4 datasets). In contrast, parallel adapters can be appended to any pre-trained network, which allows them to be used with off-the-shelf models. Location of residual adapters. Here we study the optimal placement strategy for the residual adapters throughout the network. As shown in <ref type="figure" target="#fig_2">fig. 3</ref>, the network is composed of three macro blocks, early, mid and late. In the first experiment, we apply the parallel residual adapters to each macroblock, skip the other two and report the results in <ref type="table">table 1</ref> as "Parallel (early,mid,late)". We observe that it is crucial to use the adapters in all the macro blocks as these 3 partial models perform significantly worse than the full model. Still, the adapters are most beneficial in the last block which suggests that, as expected, filters become more specialized and domain specific towards the end of network. We also investigate how the adapters should be distributed within each residual block. In the default setting, the adapters are applied at each of the two convolutional layers (see <ref type="figure" target="#fig_0">fig. 2</ref>). We evaluate the performance when it is applied to only the second convolutional layer which reduces the number of domain specific parameters by half. We observe that this results in a consistent drop in classification accuracy, suggesting that adapting each convolutional layer is beneficial. Regularization. One of the challenges of training a single network for multiple tasks is to find an optimal training setting that can work when tasks differ in their difficulty level and number of training images. For instance, we observe in the preliminary experiments that training the adapter modules on the domains with fewer images per class such as Aircraft, DTD, Flowers datasets lead to overfitting on the training set after only a few iterations. To prevent this, we apply a stronger regularization by increasing the weight decay  <ref type="table">Table 1</ref>: Reports the (top-1) classification accuracy (%) and decathlon overall score (S) of different models on the decathlon tasks <ref type="bibr" target="#b22">[23]</ref>. The model size ("#par") is the number of parameters w.r.t. the vanilla network pretrained on ImageNet. Our best models use the parallel adapters and SVD, indicated as "Parallel SVD". during training time. In particular, we group the datasets in terms of size of their training set as in <ref type="bibr" target="#b22">[23]</ref> and assign a different weight decay value for each dataset i.e. higher weight decay for smaller datasets (0.002 for Aircraft, DTD, Flowers, 0.0005 for Omniglot, Pedestrian and UCF101 and 0.0001 for CIFAR100, GTSRB and SVHN). This forces a stronger regularization for smaller datasets such that the resulting network has to stay close to the pretrained network. In addition to shrinkage, we also evaluate the effect of another popular regularization strategy, dropout <ref type="bibr" target="#b27">[28]</ref>. In this experiment, we apply dropout just before the second parallel adapter in each residual block as done in the standard WideResNet <ref type="bibr" target="#b32">[33]</ref>. <ref type="figure" target="#fig_3">Figure 4a</ref> shows classification accuracies for parallel adapters (with and without dropout) used with pre-trained ResNet models with varying filter widths. We see that dropout needs a wider pretrained network (2.5×) to be effective and that the effect is significant no matter what the size of the training set with a state-of-the-art 85% accuracy using the full training set or an impressive 73% accuracy using only 50 images per class. Thus, dropout enables a better use of the adapters for high capacity pretrained networks even when few images per class are available.</p><p>Adapter compression. The size of each residual adapter is dictated by the number of filters in its corresponding convolutional layer. In most of the modern deep network architectures such as AlexNet <ref type="bibr" target="#b14">[15]</ref>, ResNet <ref type="bibr" target="#b10">[11]</ref>, the number of convolutional filters is designed to double after each block. This leads to significant increase in the adapter size at the later layers. While this is found to be beneficial for a generic network design, we speculate that the dimensionality of required residual modules can be reduced without any drop in classification performance, as some filter combinations can be useful for more than one domain. Thus, we assume that weights of adapter modules α for different domains are not linearly independent. To test our reason-ing, we first take the pre-trained ResNet model and freeze all the weights and only learn domain specific parameters α. As described in section 3.4, we stack these weights, apply the SVD and only retain half of the original dimensionality that yields a further 50% reduction in parameters. Finally, we freeze β weights and fine-tune γ for each domain. We show in table 1 that this approach preserves the performance of the default parallel residual modules while having lower number of parameters (twice as less adapters parameters). As expected, the cross-domain compression acts as a multi-task regularizer and thus prevents from overfitting on small datasets. For example, we can point out the significant effect on Aircraft, VGG-Flowers or UCF 101 with respectively an improvement of 1.8, 1.1 and 1.5 accuracy points with less trainable parameters. For bigger datasets, the performances are preserved while reducing the number of parameters. Comparison to the state-of-the-art. We also compare our method to the recent work <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b24">25]</ref> that report results on the Decathlon dataset and show in table 1 that our method significantly outperforms both. Our approach is directly comparable to <ref type="bibr" target="#b22">[23]</ref> as the same base network is used; in particular, "Series Res. adapt." in table 1 is our re-implementation of <ref type="bibr" target="#b22">[23]</ref>, which outperforms the original in terms of mean accuracy and Decathlon score (3159 vs 2643). Our final result (SVD) achieves a boost of 1.2% in classification accuracy and approximately 750 Decathlon points while employing only half of the additional parameters used in <ref type="bibr" target="#b22">[23]</ref>. Similarly, we obtain a remarkable improvement over <ref type="bibr" target="#b24">[25]</ref> (1.4% in classification accuracy and approximately 550 Decathlon points) with a significantly more compact architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Transfer learning results</head><p>A desired property for a multiple domain learning method is the ability to learn a previously unseen domain  especially when training data is limited. To assess this quantitatively, we take the pretrained ResNet model on the Ima-geNet and finetune by using the residual adapters on three datasets, UCF-101 ("small"), CIFAR-100 ("medium") and MIT Places 205 ("large") with more than 2 million images, all resized to 72 pixels. We train our method with varying the percent of training data and report the results in <ref type="figure" target="#fig_3">Figure 4</ref>. Both the parallel and series configurations clearly outperform finetuning, not only when there is fewer data available but also for the full size of CIFAR-100 and UCF101. Finetuning only outperforms our method when it is trained on the full training set of the MIT Places and obtains 51.13% compared to our 47.2% validation accuracy. As our method only updates the adapter parameters, finetuning can exploit the high capacity of the network. Hence, the parallel adapters compare very favorably to standard fine-tuning except for extremely large datasets. Series adapters are similar, but with the key difference that the parallel configuration can be applied to an off-the-shelf model a-posteriori. In short, parallel adapters are a simple strategy that can replace and outperform standard fine-tuning in almost every way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Influence of the pre-training network</head><p>We discussed previously that dropout allows an efficient use of wider networks with the parallel adapters and <ref type="figure" target="#fig_3">fig. 4a</ref> shows that increasing the pretrained network size (from 0.5× to 2.5×) helps even when amount of training data is limited. Here we also study how the pretrained network af-fects the performances of transfer learning when it is trained on a training set sampled from a smaller number of categories. We observe in <ref type="figure" target="#fig_3">fig. 4b</ref> that the classification accuracies on the target task decrease steadily if we pretrain the same network with less ImageNet classes. Thus a good network for transfer learning with adapters should be saturated with as many classes as possible during pretraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper we have shown that it is possible to build universal parametric families of networks that can share parameters very efficiently among multiple domains. We have proposed and evaluated several design strategies for the design of such architectures. The best results were obtained by using parallel residual adapter modules distributed throughout a neural network architecture and further jointly rank-compressed. The resulting network families are very compact, resulting in substantial savings in terms of model storage, exchange, update, and transmission. They also significantly outperform recent alternatives in benchmarks such as Visual Decathlon. We have also showed that parallel adapter can replace traditional fine-tuning techniques, achieving far superior performance that those in almost all cases with no additional constraint or limitation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Series vs parallel residual adapters. (a) typical module of a residual network inclusive of batch normalization layers and residual adapters (in blue). (b) the same configuration, but with parallel adapters instead, resulting in a simpler network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>23] consist of a 1×1 filter bank in parallel with a skip connection (fig. 2.a):</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Adapter injection in ResNet-26. Following the scheme of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>(a,b) analyze on CIFAR100 the influence of pretrained network settings when combined with parallel adapters. (c,d,e) compare the performances of the different methods on 3 datasets in the Transfer Learning setting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>10× 60.32 60.34 82.12 92.82 55.53 99.42 81.41 89.12 96.55 51.20 76.88 3096 Series Res. adapt. 2× 60.32 61.87 81.22 93.88 57.13 99.27 81.67 89.62 96.57 50.12 77.17 3159 Parallel Res. adapt. 2× 60.32 64.21 81.91 94.73 58.83 99.38 84.68 89.21 96.54 50.94 78.07 3412 2× 57.74 64.11 80.07 91.29 56.54 98.46 86.05 89.67 96.77 49.38 77.01 2851</figDesc><table><row><cell>Model</cell><cell cols="7">#par. ImNet Airc. C100 DPed DTD GTSR Flwr</cell><cell cols="3">OGlt SVHN UCF mean</cell><cell>S</cell></row><row><cell># images</cell><cell>1.3m</cell><cell>7k</cell><cell>50k</cell><cell>30k</cell><cell>4k</cell><cell>40k</cell><cell>2k</cell><cell>26k</cell><cell>70k</cell><cell>9k</cell></row><row><cell>Finetuning</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Parallel (early) Parallel (mid) Parallel (late) Parallel (half)</cell><cell cols="11">2× 60.32 50.47 78.58 93.26 58.46 99.00 82.27 87.68 95.39 47.77 75.32 2610 2× 60.32 57.88 79.25 94.24 56.65 98.85 83.43 88.47 95.96 48.98 76.40 2852 2× 60.32 61.06 80.58 94.02 57.87 99.19 84.68 89.06 96.30 50.94 77.40 3159 1.5× 60.32 61.15 81.24 94.36 58.40 98.85 84.76 88.69 96.19 49.99 77.40 3061</cell></row><row><cell>Parallel SVD</cell><cell cols="11">1.5× 60.32 66.04 81.86 94.23 57.82 99.24 85.74 89.25 96.62 52.50 78.36 3398</cell></row><row><cell>Rebuffi et al. [23] Rosenfeld &amp; Tsotsos [25]</cell><cell cols="11">2× 59.23 63.73 81.31 93.30 57.02 97.47 83.43 89.82 96.17 50.28 77.17 2643</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments: This work acknowledges the support of Mathworks/DTA DFR02620, EPSRC SeeBiByte and ERC 677195-IDIU.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning feed-forward one-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="523" to="531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Integrated perception with recurrent multi-task neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07275</idno>
		<title level="m">Universal representations: The missing link between faces, text, planktons, and cat breeds</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Domain separation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="343" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">icml</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1231</idno>
		<title level="m">Multitask neural networks for qsar predictions</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Catastrophic forgetting in connectionist networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>French</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in cognitive sciences</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="128" to="135" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.06870</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Mask r-cnn. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Crosslanguage knowledge transfer using multilingual deep neural network with shared hidden layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="7304" to="7308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Overcoming catastrophic forgetting in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grabska-Barwinska</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
		<respStmt>
			<orgName>National Academy of Sciences</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Ubernet: Training a universal convolutional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning to generalize: Meta-learning for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.03463</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="614" to="629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised Domain Adaptation with Residual Transfer Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="136" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Beyond shared hierarchies: Deep multitask learning through soft layer ordering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Meyerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Miikkulainen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00108</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Never-ending learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note type="report_type">DTIC Document</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Encoder based lifelong learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Aljundi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1320" to="1328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning multiple visual domains with residual adapters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rebuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">icarl: Incremental classifier and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Rebuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sperl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Tsotsos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.04228</idno>
		<title level="m">Incremental learning through deep adaptation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04671</idno>
		<title level="m">Progressive neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Knowledge transfer in deep block-modular neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Terekhov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Montone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Oregan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Biomimetic and Biohybrid Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="268" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Lifelong learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning to learn</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="181" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Simultaneous deep transfer across domains and tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4068" to="4076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep Multi-task Representation Learning: A Tensor Factorisation Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<title level="m">Wide residual networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Robust visual tracking via structured multi-task sparse learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="367" to="383" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Facial landmark detection by deep multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
