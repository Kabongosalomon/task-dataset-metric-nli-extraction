<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Reversible Architectures for Arbitrarily Deep Residual Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chang</surname></persName>
							<email>bchang@stat.ubc.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">University of British Columbia</orgName>
								<address>
									<settlement>Vancouver</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Xtract Technologies Inc</orgName>
								<address>
									<settlement>Vancouver</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Meng</surname></persName>
							<email>menglili@cs.ubc.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">University of British Columbia</orgName>
								<address>
									<settlement>Vancouver</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Xtract Technologies Inc</orgName>
								<address>
									<settlement>Vancouver</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldad</forename><surname>Haber</surname></persName>
							<email>haber@math.ubc.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">University of British Columbia</orgName>
								<address>
									<settlement>Vancouver</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Xtract Technologies Inc</orgName>
								<address>
									<settlement>Vancouver</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Ruthotto</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Xtract Technologies Inc</orgName>
								<address>
									<settlement>Vancouver</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Emory University</orgName>
								<address>
									<settlement>Atlanta</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Begert</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Xtract Technologies Inc</orgName>
								<address>
									<settlement>Vancouver</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elliot</forename><surname>Holtham</surname></persName>
							<email>elliot@xtract.ai</email>
							<affiliation key="aff1">
								<orgName type="institution">Xtract Technologies Inc</orgName>
								<address>
									<settlement>Vancouver</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Reversible Architectures for Arbitrarily Deep Residual Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>* Authors contributed equally.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, deep residual networks have been successfully applied in many computer vision and natural language processing tasks, pushing the state-of-the-art performance with deeper and wider architectures. In this work, we interpret deep residual networks as ordinary differential equations (ODEs), which have long been studied in mathematics and physics with rich theoretical and empirical success. From this interpretation, we develop a theoretical framework on stability and reversibility of deep neural networks, and derive three reversible neural network architectures that can go arbitrarily deep in theory. The reversibility property allows a memoryefficient implementation, which does not need to store the activations for most hidden layers. Together with the stability of our architectures, this enables training deeper networks using only modest computational resources. We provide both theoretical analyses and empirical results. Experimental results demonstrate the efficacy of our architectures against several strong baselines on CIFAR-10, CIFAR-100 and STL-10 with superior or on-par state-of-the-art performance. Furthermore, we show our architectures yield superior results when trained using fewer training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>We introduce three new reversible architectures for deep neural networks and discuss their stability. We capitalize on the link between ResNets and ODEs to guarantee stability of the forward propagation process and the well-posedness of the learning problem. Finally, we present regularization functionals that favor smooth time dynamics.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep learning powers many research areas and impacts various aspects of society <ref type="bibr" target="#b30">(LeCun, Bengio, and Hinton 2015)</ref> from computer vision <ref type="bibr" target="#b23">(He et al. 2016;</ref><ref type="bibr" target="#b28">Huang et al. 2017)</ref>, natural language processing <ref type="bibr" target="#b10">(Cho et al. 2014)</ref> to biology <ref type="bibr" target="#b17">(Esteva et al. 2017</ref>) and e-commerce. Recent progress in designing architectures for deep networks has further accelerated this trend <ref type="bibr" target="#b39">(Simonyan and Zisserman 2015;</ref><ref type="bibr" target="#b23">He et al. 2016;</ref><ref type="bibr" target="#b28">Huang et al. 2017</ref>). Among the most successful architectures are deep residual network (ResNet) and its variants, which are widely used in many computer vision applications <ref type="bibr" target="#b23">(He et al. 2016;</ref><ref type="bibr" target="#b38">Pohlen et al. 2017</ref>) and natural language processing tasks <ref type="bibr" target="#b36">(Oord et al. 2016;</ref><ref type="bibr" target="#b42">Xiong et al. 2017;</ref><ref type="bibr" target="#b40">Wu et al. 2016)</ref>. However, there still are few theoretical analyses and guidelines for designing and training ResNet.</p><p>In contrast to the recent interest in deep residual networks, system of Ordinary Differential Equations (ODEs), special kinds of dynamical systems, have long been studied in mathematics and physics with rich theoretical and empirical Copyright c 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. success <ref type="bibr" target="#b12">(Coddington and Levinson 1955;</ref><ref type="bibr">Simmons 2016;</ref><ref type="bibr" target="#b1">Arnold 2012)</ref>. The connection between nonlinear ODEs and deep ResNets has been established in the recent works of <ref type="bibr" target="#b22">Haber, Ruthotto, and Holtham 2017;</ref><ref type="bibr" target="#b32">Lu et al. 2017;</ref><ref type="bibr" target="#b31">Long et al. 2017;</ref><ref type="bibr" target="#b9">Chang et al. 2017)</ref>. The continuous interpretation of ResNets as dynamical systems allows the adaption of existing theory and numerical techniques for ODEs to deep learning. For example, the paper  introduces the concept of stable networks that can be arbitrarily long. However, only deep networks with simple single-layer convolution building blocks are proposed, and the architectures are not reversible (and thus the length of the network is limited by the amount of available memory), and only simple numerical examples are provided. Our work aims at overcoming these drawbacks and further investigates the efficacy and practicability of stable architectures derived from the dynamical systems perspective.</p><p>In this work, we connect deep ResNets and ODEs more closely and propose three stable and reversible architectures. We show that the three architectures are governed by stable and well-posed ODEs. In particular, our approach allows to train arbitrarily long networks using only minimal memory storage. We illustrate the intrinsic reversibility of these architectures with both theoretical analysis and empirical results. The reversibility property easily leads to a memoryefficient implementation, which does not need to store the activations at most hidden layers. Together with the stability, this allows one to train almost arbitrarily deep architectures using modest computational resources.</p><p>The remainder of our paper is organized as follows. We discuss related work in Sec. 2. In Sec. 3 we review the notion of reversibility and stability in ResNets, present three new architectures, and a regularization functional. In Sec. 4 we show the efficacy of our networks using three common classification benchmarks (CIFAR-10, CIFAR-100, STL-10). Our new architectures achieve comparable or even superior accuracy and, in particular, generalize better when a limited number of labeled training data is used. In Sec. 5 we conclude the paper.  <ref type="figure">Figure 1</ref>: The Hamiltonian Reversible Block. First, the input feature map is equally channel-wise split to Y j and Z j . Then the operations described in Eq. 10 are performed, resulting in Y j+1 and Z j+1 . Finally, Y j+1 and Z j+1 are concatenated as the output of the block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Residual Neural Networks and Extensions</head><p>ResNets are deep neural networks obtained by stacking simple residual blocks <ref type="bibr" target="#b23">(He et al. 2016)</ref>. A simple residual network block can be written as</p><formula xml:id="formula_0">Y j+1 = Y j + F(Y j , θ j ) for j = 0, ..., N − 1.</formula><p>(1) Here, Y j are the values of the features at the jth layer and θ j are the jth layer's network parameters. The goal of the training is to learn the network parameters θ. Eq. (1) represents a discrete dynamical system. An early review on neural networks as dynamical systems is presented in <ref type="bibr" target="#b8">(Cessac 2010)</ref>.</p><p>ResNets have been broadly applied in many domains including computer vision tasks such as image recognition <ref type="bibr" target="#b23">(He et al. 2016)</ref>, object detection , semantic segmentation <ref type="bibr" target="#b38">(Pohlen et al. 2017</ref>) and visual reasoning <ref type="bibr" target="#b37">(Perez et al. 2017)</ref>, natural language processing tasks such as speech synthesis <ref type="bibr" target="#b36">(Oord et al. 2016)</ref>, speech recognition <ref type="bibr" target="#b42">(Xiong et al. 2017</ref>) and machine translation <ref type="bibr" target="#b40">(Wu et al. 2016)</ref>.</p><p>Besides broadening the application domain, some ResNet successors focus on improving accuracy <ref type="bibr" target="#b41">(Xie et al. 2017;</ref><ref type="bibr" target="#b44">Zagoruyko and Komodakis 2016)</ref> and stability , saving GPU memory <ref type="bibr" target="#b19">(Gomez et al. 2017)</ref>, and accelerating the training process <ref type="bibr" target="#b27">(Huang et al. 2016)</ref>. For instance, ResNxt <ref type="bibr" target="#b41">(Xie et al. 2017</ref>) introduces a homogeneous, multi-branch architecture to increase the accuracy. Stochastic depth <ref type="bibr" target="#b27">(Huang et al. 2016</ref>) reduces the training time while increases accuracy by randomly dropping a subset of layers and bypassing them with identity function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Systems of Ordinary Differential Equations</head><p>To see the connection between ResNet and ODE systems we add a hyperparameter h &gt; 0 to Eq. (1) and rewrite the equation</p><formula xml:id="formula_1">as Y j+1 − Y j h = F(Y j , θ j ).<label>(2)</label></formula><p>For a sufficiently small h, Eq.</p><p>(2) is a forward Euler discretization of the initial value probleṁ</p><formula xml:id="formula_2">Y(t) = F(Y(t), θ(t)), Y(0) = Y 0 .<label>(3)</label></formula><p>Thus, the problem of learning the network parameters, θ, is equivalent to solving a parameter estimation problem or optimal control problem involving the ODE system Eq. (3). In some cases (e.g., in image classification), Eq.</p><p>(3) can be interpreted as a system of Partial Differential Equations (PDEs). Such problems have rich theoretical and computational framework, including techniques to guarantee stable networks by using appropriate functions F, the discretization of the forward propagation process <ref type="bibr" target="#b3">(Ascher and Petzold 1998;</ref><ref type="bibr" target="#b4">Ascher 2010;</ref><ref type="bibr" target="#b5">Bellman 1953)</ref>, theoretical frameworks for the optimization over the parameters θ <ref type="bibr" target="#b7">(Bock 1983;</ref><ref type="bibr">Ulbrich 2002;</ref><ref type="bibr" target="#b20">Gunzburger 2003)</ref>, and methods for computing the gradient of the solution with respect to θ <ref type="bibr" target="#b6">(Bliss 1919)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reversible Architectures</head><p>Reversible numerical methods for dynamical systems allow the simulation of the dynamic going from the final time to the initial time, and vice versa. Reversible numerical methods are commonly used in the context of hyperbolic PDEs, where various methods have been proposed and compared <ref type="bibr" target="#b35">(Nguyen and McMechan 2014)</ref>. The theoretical framework for reversible methods is strongly tied to issues of stability. In fact, as we show here, not every method that is algebraically reversible is numerically stable. This has a strong implication for the practical applicability of reversible methods to deep neural networks.</p><p>Recently, various reversible neural networks have been proposed for different purposes and based on different architectures. Recent work by <ref type="bibr" target="#b15">(Dosovitskiy and Brox 2016;</ref><ref type="bibr" target="#b34">Mahendran and Vedaldi 2015)</ref> inverts the feed-forward net and reproduces the input features from their values at the final layers. This suggests that some deep neural networks are reversible: the generative model is just the reverse of the feed-forward net <ref type="bibr" target="#b2">(Arora, Liang, and Ma 2016)</ref>. <ref type="bibr" target="#b18">(Gilbert et al. 2017</ref>) provide a theoretical connection between a model-based compressive sensing and CNNs. NICE <ref type="bibr" target="#b13">(Dinh, Krueger, and Bengio 2015;</ref><ref type="bibr" target="#b14">Dinh, Sohl-Dickstein, and Bengio 2016)</ref> uses an invertible non-linear transformation to map the data distribution into a latent space where the resulting distribution factorizes, yielding good generative models. Besides the implications that reversibility has on the deep generative models, the property can be used for developing memory-efficient algorithms. For instance, RevNet <ref type="bibr" target="#b19">(Gomez et al. 2017)</ref>, which is inspired by NICE, develops a variant of ResNet where each layer's activations can be reconstructed from next layer's. This allows one to avoid storing activations at all hidden layers, except at those layers with stride larger than one. We will show later that our physicallyinspired network architectures also have the reversible property and we derive memory-efficient implementations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ResNet as an ODE</head><p>Eq.</p><p>(3) interprets ResNet as a discretization of a differential equation, whose parameters θ are learned in the training process. The process of forward propagation can be viewed as simulating the nonlinear dynamics that take the initial data, Y 0 , which are hard to classify, and moves them to a final state Y N , which can be classified easily using, e.g., a linear classifier.</p><p>A fundamental question that needs to be addressed is, under what conditions is forward propagation well-posed? This question is important for two main reasons. First, instability of the forward propagation means that the solution is highly sensitive to data perturbation (e.g., image noise or adversarial attacks). Given that most computations are done in single precision, this may cause serious artifacts and instabilities in the final results. Second, training unstable networks may be very difficult in practice and, although impossible to prove, instability can add many local minima.</p><p>Let us first review the issue of stability. A dynamical system is stable if a small change in the input data leads to a small change in the final result. To better characterize this, assume a small perturbation, δY(0) to the initial data Y(0) in Eq. <ref type="formula" target="#formula_2">(3)</ref>. Assume that this change is propagated throughout the network. The question is, what would be the change after some time t, that is, what is δY(t)?</p><p>This change can be characterized by the Lyapunov exponent <ref type="bibr" target="#b33">(Lyapunov 1992)</ref>, which measures the difference in the trajectories of a nonlinear dynamical system given the initial conditions. The Lyapunov exponent, λ, is defined as the exponent that measures the difference:</p><formula xml:id="formula_3">δY(t) ≈ exp(λt) δY(0) .<label>(4)</label></formula><p>The forward propagation is well-posed when λ ≤ 0, and ill-posed if λ &gt; 0. A bound on the value of λ can be derived from the eigenvalues of the Jacobian matrix of F with respect to Y, which is given by</p><formula xml:id="formula_4">J(t) = ∇ Y(t) F(Y(t)).</formula><p>A sufficient condition for stability is</p><formula xml:id="formula_5">max i=1,2,...,n Re(λ i (J(t))) ≤ 0, ∀t ∈ [0, T ],<label>(5)</label></formula><p>where λ i (J) is the ith eigenvalue of J, and Re(·) denotes the real part. This observation allows us to generate networks that are guaranteed to be stable. It should be emphasized that the stability of the forward propagation is necessary to obtain stable networks that generalize well, but not sufficient. In fact, if the real parts of the eigenvalues in Eq. (5) are negative and large, λ 0, Eq. (4) shows that differences in the input features decay exponentially in time. This complicates the learning problem and therefore we consider architectures that lead to Jacobians with (approximately) purely imaginary eigenvalues. We now discuss three such networks that are inspired by different physical interpretations.</p><p>The two-layer Hamiltonian network  propose a neural network architecture inspired by Hamiltonian systemṡ</p><formula xml:id="formula_6">Y(t) = σ(K(t)Z(t) + b(t)), Z(t) = −σ(K(t) T Y(t) + b(t)),<label>(6)</label></formula><p>where Y(t) and Z(t) are partitions of the features, σ is an activation function, and the network parameters are θ = (K, b). For convolutional neural networks, K(t) and K(t) T are convolution operator and convolution transpose operator respectively. It can be shown that the Jacobian matrix of this ODE satisfies the condition in Eq. <ref type="formula" target="#formula_5">(5)</ref>, thus it is stable and well-posed. The authors also demonstrate the performance on a small dataset. However, in our numerical experiments we have found that the representability of this "one-layer" architecture is limited. According to the universal approximation theorem <ref type="bibr" target="#b26">(Hornik 1991</ref>), a two-layer neural network can approximate any monotonically-increasing continuous function on a compact set. Recent work  shows that simple two-layer neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points. Therefore, we propose to extend Eq. (6) to the following two-layer structure:</p><formula xml:id="formula_7">Y(t) = K T 1 (t)σ(K 1 (t)Z(t) + b 1 (t)), Z(t) = −K T 2 (t)σ(K 2 (t)Y(t) + b 2 (t)).<label>(7)</label></formula><p>In principle, any linear operator can be used within the Hamiltonian framework. However, since our numerical experiments consider images, we choose K i to be a convolution operator, K T i as its transpose. Rewriting Eq. <ref type="formula" target="#formula_7">(7)</ref> in matrix form gives Ẏ</p><formula xml:id="formula_8">Z = K T 1 0 0 −K T 2 σ 0 K 1 K 2 0 Y Z + b 1 b 2 .</formula><p>(8) There are different ways of partitioning the input features, including checkerboard partition and channel-wise partition <ref type="bibr" target="#b14">(Dinh, Sohl-Dickstein, and Bengio 2016)</ref>. In this work, we use equal channel-wise partition, that is, the first half of the channels of the input is Y and the second half is Z.</p><p>It can be shown that the Jacobian matrix of Eq. (8) satisfies the condition in Eq. (5), that is,</p><formula xml:id="formula_9">J = ∇ ( Y Z ) K T 1 0 0 −K T 2 σ 0 K 1 K 2 0 y z = K T 1 0 0 −K T 2 diag(σ ) 0 K 1 K 2 0 ,<label>(9)</label></formula><p>where diag(σ ) is the derivative of the activation function.</p><p>The eigenvalues of J are all imaginary (see the Appendix for a proof). Therefore Eq. (5) is satisfied and the forward propagation of our neural network is stable and well-posed. A commonly used discretization technique for Hamiltonian systems such as Eq. <ref type="formula" target="#formula_7">(7)</ref> is the Verlet method (Ascher and Petzold 1998) that reads</p><formula xml:id="formula_10">Y j+1 = Y j + hK T j1 σ(K j1 Z j + b j1 ), Z j+1 = Z j − hK T j2 σ(K j2 Y j+1 + b j2 ).<label>(10)</label></formula><p>We choose Eq. (10) to be our Hamiltonian blocks and illustrate it in <ref type="figure">Fig. 1</ref>. Similar to ResNet <ref type="bibr" target="#b23">(He et al. 2016</ref>), our Hamiltonian reversible network is built by first concatenating blocks to units, and then concatenating units to a network. An illustration of our architecture is provided in <ref type="figure" target="#fig_1">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The midpoint network</head><p>Another reversible numerical method for discretizing the first-order ODE in Eq. <ref type="formula" target="#formula_2">(3)</ref> is obtained by using central finite differences in time</p><formula xml:id="formula_11">Y j+1 − Y j−1 2h = F(Y j ).<label>(11)</label></formula><p>This gives the following forward propagation</p><formula xml:id="formula_12">Y j+1 = Y j−1 + 2hF(Y j ), for j = 1, . . . , N − 1,<label>(12)</label></formula><p>where Y 1 is obtained by one forward Euler step. To guarantee stability for a single layer we can use the function F to contain an anti-symmetric linear operator, that is,</p><formula xml:id="formula_13">F(Y) = σ((K − K T )Y + b).<label>(13)</label></formula><p>The Jacobian of this forward propagation is</p><formula xml:id="formula_14">J = diag(σ )(K − K T ),<label>(14)</label></formula><p>which has only imaginary eigenvalues. This yields the single layer midpoint network</p><formula xml:id="formula_15">Y j+1 = 2hσ((K j − K T j )Y j + b j ), j = 0, Y j−1 + 2hσ((K j − K T j )Y j + b j ), j &gt; 0.<label>(15)</label></formula><p>As we see next, it is straightforward to show that the midpoint method is reversible (at least algebraically). However, while it is possible to potentially use a double layer midpoint network, it is difficult to ensure the stability of such network. To this end, we explore the leapfrog network next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The leapfrog network</head><p>A stable leapfrog network can be seen as a special case of the Hamiltonian network in Eq. (7) when one of the kernels is the identity matrix and one of the activation is the identity function. The leapfrog network involves two derivatives in time and reads</p><formula xml:id="formula_16">Y(t) = −K(t) T σ(K(t)Y(t)+b(t)), Y(0) = Y 0 . (16)</formula><p>It can be discretized, for example, using the conservative leapfrog discretization, which uses the following symmetric approximation to the second derivative in timë</p><formula xml:id="formula_17">Y(t j ) ≈ h −2 (Y j+1 − 2Y j + Y j−1 ).</formula><p>Substituting the approximation in Eq. (16), we obtain:</p><formula xml:id="formula_18">Y j+1 = 2Y j − h 2 K T j σ(K j Y j + b j ), j = 0, 2Y j − Y j−1 − h 2 K T j σ(K j Y j + b j ), j &gt; 0.<label>(17)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reversible architectures and stability</head><p>An architecture is called reversible if it allows the reconstruction of the activations going from the end to the beginning. Reversible numerical methods for ODEs have been studied in the context of hyperbolic differential equations <ref type="bibr" target="#b35">(Nguyen and McMechan 2014)</ref>, and reversibility was discovered recently in the machine learning community <ref type="bibr" target="#b13">(Dinh, Krueger, and Bengio 2015;</ref><ref type="bibr" target="#b19">Gomez et al. 2017)</ref>. Reversible techniques enable memory-efficient implementations of the network that requires the storage of the last activations only. Let us first demonstrate the reversibility of the leapfrog network. Assume that we are given the last two states, Y N and Y N −1 . Then, using Eq. (17) it is straight-forward to compute Y N −2 :</p><formula xml:id="formula_19">Y N −2 = 2Y N −1 − Y N − h 2 K T N −1 σ(K N −1 Y N −1 + b N −1 )</formula><p>. (18) Given Y N −2 one can continue and re-compute the activations at each hidden layer during backpropagation. Similarly, it is straightforward to show that the midpoint network is reversible.</p><p>The Hamiltonian network is similar to the RevNet and can be described as</p><formula xml:id="formula_20">Y j+1 = Y j + F(Z j ), Z j+1 = Z j + G(Y j+1 ),<label>(19)</label></formula><p>where Y j and Z j are a partition of the units in block j; F and G are the residual functions. Eq. <ref type="formula" target="#formula_9">(19)</ref> is reversible as each layer's activations can be computed from the next layer's as follows:</p><formula xml:id="formula_21">Z j = Z j+1 − G(Y j+1 ), Y j = Y j+1 − F(Z j ).<label>(20)</label></formula><p>It is clear that Eq. (10) is a special case of Eq. <ref type="formula" target="#formula_9">(19)</ref>, which enables us to implement Hamiltonian network in a memory efficient way. While RevNet and MidPoint represent reversible networks algebraically, they may not be reversible in practice without restrictions on the residual functions. To illustrate, consider the simple linear case where G(Y) = αY and F(Z) = βZ. The RevNet in this simple case reads</p><formula xml:id="formula_22">Y j+1 = Y j + βZ j , Z j+1 = Z j + αY j+1 .</formula><p>One way to simplify the equations is to look at two time steps and subtract them:</p><formula xml:id="formula_23">Y j+1 − 2Y j + Y j−1 = β(Z j − Z j−1 ) = αβY j , which implies that Y j+1 − (2 + αβ)Y j + Y j−1 = 0.</formula><p>These type of equations have a solution of the form Y j = ξ j . The characteristic equation is</p><formula xml:id="formula_24">ξ 2 − (2 + αβ)ξ + 1 = 0.<label>(21)</label></formula><p>Define a = 1 2 (2 + αβ), the roots of the equation are ξ = a± √ a 2 − 1. If a 2 ≤ 1 then we have that ξ = a±i √ 1 − a 2 .  <ref type="figure">Fig. 1 and pooling layer.</ref> and |ξ| 2 = 1, which implies that the method is stable and no energy in the feature vectors is added or lost. It is obvious that Eq. <ref type="formula" target="#formula_1">(21)</ref> is not stable for every choice of α and β. Indeed, if, for example, α and β are positive then |ξ| &gt; 1 and the solution can grow at every layer exhibiting unstable behavior. It is possible to obtain stable solutions if 0 &lt; α and β &lt; 0 and both are sufficiently small. This is the role of h in our Hamiltonian network.</p><p>This analysis plays a key role in reversibility. For unstable networks, either the forward or the backward propagation consists of an exponentially growing mode. For computation in single precision (like most practical CNN), the gradient can be grossly inaccurate. Thus we see that not every choice of the functions F and G lead to a reasonable network in practice and that some control is needed if we are to have a network that does not grow exponentially neither forward nor backwards.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Arbitrarily deep residual neural networks</head><p>All three architectures we proposed can be used with arbitrary depth, since they do not have any dissipation. This implies that the signal that is input into the system does not decay even for arbitrarily long networks. Thus signals can propagate through this system to infinite network depth. We have also experimented with slightly dissipative networks, that is, networks that attenuate the signal at each layer, that yielded results that were comparable to the ones obtained by the networks proposed here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Regularization</head><p>Regularization plays a vital role serving as parameter tuning in the deep neural network training to help improve generalization performance . Besides commonly used weight decay, we also use weight smoothness decay. Since we interpret the forward propagation of our Hamiltonian network as a time-dependent nonlinear dynamic process, we prefer convolution weights K that are smooth in time by using the regularization functional</p><formula xml:id="formula_25">R(K) = T 0 K 1 (t) 2 F + K 2 (t) 2 F dt,</formula><p>where · F represents the Frobenius norm. Upon discretization, this gives the following weight smoothness decay as a regularization function</p><formula xml:id="formula_26">R h (K) = h T −1 j=1 2 k=1 K jk − K j+1,k h 2 F .<label>(22)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate our methods on three standard classification benchmarks (CIFAR-10, CIFAR100 and STL10) and compare against state-of-the-art results from the literature. Furthermore, we investigate the robustness of our method as the amount of training data decrease and train a deep network with 1,202 layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets and baselines</head><p>CIFAR-10 and CIFAR-100 The CIFAR-10 dataset (Krizhevsky and Hinton 2009) consists of 50,000 training images and 10,000 testing images in 10 classes with 32 × 32 image resolution. The CIFAR-100 dataset uses the same image data and train-test split as CIFAR-10, but has 100 classes. We use the common data augmentation techniques including padding four zeros around the image, random cropping, random horizontal flipping and image standardization. Two state-of-the-art methods ResNet <ref type="bibr" target="#b23">(He et al. 2016)</ref> and RevNet <ref type="bibr" target="#b19">(Gomez et al. 2017</ref>) are used as our baseline methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>STL-10</head><p>The STL-10 dataset <ref type="bibr" target="#b11">(Coates, Ng, and Lee 2011)</ref> is an image recognition dataset with 10 classes at image resolutions of 96 × 96. It contains 5,000 training images and 8,000 test images. Thus, compared with CIFAR-10, each class has fewer labeled training samples but higher image resolution. We used the same data augmentation as the CIFAR-10 except padding 12 zeros around the images. We use three state-of-the-art methods as baselines for the STL-10 dataset: Deep Representation Learning <ref type="bibr" target="#b43">(Yang et al. 2015)</ref>, Convolutional Clustering <ref type="bibr" target="#b16">(Dundar, Jin, and Culurciello 2015)</ref>, and Stacked what-where auto-encoders <ref type="bibr" target="#b46">(Zhao et al. 2016</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Neural network architecture specifications</head><p>We provide the neural network architecture specifications here. The implementation details are in the Appendix. All the networks contain 3 units, and each unit has n blocks. There is also a convolution layer at the beginning of the network and a fully connected layer in the end. For Hamiltonian networks, there are 4 convolution layers in each block, so the total number of layers is 12n + 2. For MidPoint and Leapfrog, there are 2 convolution layers in each block, so the total number of layers is 6n + 2. In the first block of each unit, the feature map size is halved and the number of filters is doubled. We perform downsampling by average pooling and increase the number of filters by padding zeros.  <ref type="bibr" target="#b23">(He et al. 2016)</ref> and</p><p>RevNet <ref type="bibr" target="#b19">(Gomez et al. 2017)</ref>. Please note RevNet and our three architectures are much more memory-efficient than ResNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Accuracy Baselines <ref type="bibr" target="#b43">(Yang et al. 2015)</ref> 73.15% <ref type="bibr" target="#b16">(Dundar et al. 2015)</ref> 74.1% <ref type="bibr" target="#b46">(Zhao et al. 2016)</ref> 74.3%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours</head><p>Hamiltonian 85.5% MidPoint 84.6% Leapfrog 83.7% Main Results and Analysis CIFAR-10 and CIFAR-100 We show the main results of different architectures on CIFAR-10/100 in <ref type="table">Table 1</ref>. Our three architectures achieve comparable performance with ResNet and RevNet in term of accuracy using similar number of model parameters. Compared with ResNet, our architectures are more memory efficient as they are reversible, thus we do not need to store activations for most layers. While compared with RevNet, our models are not only reversible, but also stable, which is theoretically proved in Sec. 3. We show later that the stable property makes our models more robust to small amount of training data and arbitrarily deep.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>STL-10</head><p>Main results on STL-10 are shown in <ref type="table" target="#tab_1">Table 2</ref>. Compared with the state-of-the-art results, all our architectures achieve better accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Robustness to training data subsampling</head><p>Sometimes labeled data are very expensive to obtain. Thus, it is desirable to design architectures that generalize well when trained with few examples. To verify our intuition that stable architectures generalize well, we conducted extensive numerical experiments using the CIFAR-10 and STL-10 datasets with decreasing number of training data. Our focus is on the behavior of our neural network architecture in face of this data subsampling, instead of improving the state-of-the-art results. Therefore we intentionally use simple architectures: 4 blocks, each has 4 units, and the number of filters are 16 − 64 − 128 − 256. For comparison, we use ResNet <ref type="bibr" target="#b23">(He et al. 2016</ref>) as our baseline. CIFAR-10 has much more training data than STL-10 (50,000 vs 5,000), so we randomly subsample the training data from 20% to 0.05% for CIFAR-10, and from 80% to 5% for STL-10. The test data set remains unchanged.</p><p>CIFAR-10 <ref type="figure" target="#fig_2">Fig. 3</ref> shows the result on CIFAR-10 when decreasing the number examples in the training data from 20% to 5%. Our Hamiltonian network performs consistently better in terms of accuracy than ResNet, achieving up to 13% higher accuracy when trained using just 3% and 4% of the original training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>STL-10</head><p>From the result as shown in <ref type="figure" target="#fig_3">Fig. 4</ref>, we see that Hamiltonian consistently achieves better accuracy than ResNet with the average improvement being around 3.4%. Especially when using just 40% of the training data, Hamiltonian has a 5.7% higher accuracy compared to ResNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training a 1202-layer Hamiltonian</head><p>To demonstrate the stability and memory-efficiency of the Hamiltonian network with arbitrary depth, we explore a 1202-layer architecture on CIFAR-10. An aggressively deep ResNet is also trained on CIFAR-10 in <ref type="bibr" target="#b23">(He et al. 2016</ref>) with 1202 layers, which has an accuracy of 92.07%. Our result is shown at the last row of <ref type="table">Table 1</ref>. Compared with the original ResNet, our architecture uses only a half of parameters and obtains better accuracy. Since the Hamiltonian network is intrinsically stable, it is guaranteed that there is no issue of exploding or vanishing gradient. We can easily train an arbitrarily deep Hamiltonian network without any difficulty of optimization. The implementation of our reversible architecture is memory efficient, which enables a 1202 layer Hamiltonian model running on a single GPU machine with 10GB GPU memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We present three stable and reversible architectures that connect the stable ODE with deep residual neural networks and yield well-posed learning problems. We exploit the intrinsic reversibility property to obtain a memory-efficient implementation, which does not need to store the activations at most of the hidden layers. Together with the stability of the forward propagation, this allows training deeper architectures with limited computational resources. We evaluate our methods on three publicly available datasets against several state-of-the-art methods. Our experimental results demonstrate the efficacy of our method with superior or on-par state-of-the-art performance. Moreover, with small amount of training data, our architectures achieve better accuracy compared with the widely used state-of-the-art ResNet. We attribute the robustness to small amount of training data to the intrinsic stability of our Hamiltonian neural network architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Appendix</head><p>Proof: All eigenvalues of J in Eq. (9) are imaginary.</p><p>The Jacobian matrix J is defined in Eq. (9). If A and B are two invertible matrices of the same size, then AB and BA have the same eigenvalues (Theorem 1.3.22 in <ref type="bibr" target="#b25">(Horn and Johnson 2012)</ref>). If we define</p><formula xml:id="formula_27">J = diag(σ ) 0 K 1 K 2 0 K T 1 0 0 −K T 2 = diag(σ ) 0 −K 1 K T 2 K 2 K T 1 0 := DM,<label>(23)</label></formula><p>then J and J have the same eigenvalues. D is a diagonal matrix with non-negative elements, and M is a real antisymmetric matrix such that M T = −M. Let λ and v be a pair of eigenvalue and eigenvector of J = DM, then</p><formula xml:id="formula_28">DMv = λv,<label>(24)</label></formula><formula xml:id="formula_29">Mv = λD −1 v,<label>(25)</label></formula><formula xml:id="formula_30">v * Mv = λ(v * D −1 v),<label>(26)</label></formula><p>where D −1 is the generalized inverse of D. On one hand, since D −1 is non-negative definite, v * D −1 v is real. On the other hand,</p><formula xml:id="formula_31">(v * Mv) * = v * M * v = v * M T v = −v * Mv,<label>(27)</label></formula><p>where * represents conjugate transpose. Eq. 27 implies that v * Mv is imaginary. Therefore, λ has to be imaginary. As a result, all eigenvalues of J are imaginary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation details</head><p>Our method is implemented using TensorFlow library <ref type="bibr" target="#b0">(Abadi et al. 2016</ref>). The CIFAR-10/100 and STL-10 experiments are evaluated on a desktop with an Intel Quad-Core i5 CPU and a single Nvidia 1080 Ti GPU. For CIFAR-10 and CIFAR-100 experiments, we use a fixed mini-batch size of 100 both for training and test data except Hamiltonian-1202, which uses a batch-size of 32. The learning rate is initialized to be 0.1 and decayed by a factor of 10 at 80, 120 and 160 training epochs. The total training step is 80K. The weight decay constant is set to 2 × 10 −4 , weight smoothness decay is 2 × 10 −4 and the momentum is set to 0.9.</p><p>For STL-10 experiments, the mini-batch size is 128. The learning rate is initialized to be 0.1 and decayed by a factor of 10 at 60, 80 and 100 training epochs. The total training step is 20K. The weight decay constant is set to 5 × 10 −4 , weight smoothness decay is 3 × 10 −4 and the momentum is set to 0.9.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The Hamiltonian Reversible Neural Network. It is the simple stacking of several Hamiltonian reversible blocks as shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Hamiltonian vs ResNet test accuracy on CI-FAR10 with a small subset of training data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Hamiltonian vs ResNet test accuracy for STL10 with a small subset of training data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Main results on STL-10. All our three architectures outperform the benchmark methods by about 10%.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<title level="m">Tensorflow: Large-scale machine learning on heterogeneous distributed systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Geometrical methods in the theory of ordinary differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Arnold</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Why are deep nets reversible: A simple theory, with implications for training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Ascher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Petzold</surname></persName>
		</author>
		<title level="m">Computer Methods for Ordinary Differential Equations and Differential-Algebraic Equations</title>
		<meeting><address><addrLine>Philadelphia: SIAM</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Numerical methods for Evolutionary Differential Equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Ascher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<pubPlace>Philadelphia: SIAM</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">An introduction to the theory of dynamic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bellman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1953" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The use of adjoint systems in the problem of differential corrections for trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Bliss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JUS Artillery</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="296" to="311" />
			<date type="published" when="1919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Recent advances in parameter identification techniques for ode</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Numerical treatment of inverse problems</title>
		<editor>Deuflhard, P., and Hairer, E.</editor>
		<meeting><address><addrLine>Boston</addrLine></address></meeting>
		<imprint>
			<publisher>Birkhauser</publisher>
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A view of neural networks as dynamical systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cessac</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Bifurcation and Chaos</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Begert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10348</idno>
		<title level="m">Multi-level residual networks from dynamical systems view</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An analysis of single-layer networks in unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Theory of ordinary differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Coddington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Levinson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1955" />
			<publisher>Tata McGraw-Hill Education</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Nice: Non-linear independent components estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR-Workshop</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Density estimation using real NVP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Inverting visual representations with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A proposal on machine learning via dynamical systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dundar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">; E</forename><surname>Culurciello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06241</idno>
	</analytic>
	<monogr>
		<title level="m">Convolutional clustering for unsupervised learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dermatologist-level classification of skin cancer with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Esteva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kuprel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Novoa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Swetter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Blau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.08664</idno>
		<title level="m">Towards understanding the invertibility of convolutional neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Grosse</surname></persName>
		</author>
		<title level="m">The reversible residual network: Backpropagation without storing activations. NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Perspectives in flow control and optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Gunzburger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>SIAM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ruthotto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.03341</idno>
		<title level="m">Stable architectures for deep neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning across scales-a multiscale method for convolution neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ruthotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Holtham</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.02009</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mask R-CNN. In ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johnson</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Matrix Analysis</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Approximation capabilities of multilayer feedforward networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hornik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<title level="m">Deep learning. Nature</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09668</idno>
		<title level="m">Pde-net: Learning pdes from data</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10121</idno>
		<title level="m">Beyond finite layer neural networks: Bridging deep architectures and numerical differential equations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The general problem of the stability of motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Lyapunov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Control</title>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Understanding deep image representations by inverting them</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Five ways to avoid storing source wavefield snapshots in 2d elastic prestack reverse time migration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Mcmechan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Geophysics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03499</idno>
		<title level="m">Wavenet: A generative model for raw audio</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.03017</idno>
		<title level="m">Learning visual reasoning without strong priors</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Full resolution image compression with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pohlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. Simmons, G. F. 2016. Differential equations with applications and historical notes</title>
		<imprint>
			<publisher>CRC Press</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition. ICLR. Ulbrich, S. 2002. A sensitivity and adjoint calculus for discontinuous solutions of hyperbolic conservation laws with source terms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Control and Optimization</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="740" to="797" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<title level="m">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The microsoft 2016 conversational speech recognition system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Droppo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Seide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Seltzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stolcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep representation learning with target coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Shum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<title level="m">Wide residual networks. BMVC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>ICLRworkshop</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
