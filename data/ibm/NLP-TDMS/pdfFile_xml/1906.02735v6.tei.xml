<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Residual Flows for Invertible Generative Modeling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricky</forename><forename type="middle">T Q</forename><surname>Chen</surname></persName>
							<email>rtqichen@cs.toronto.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Toronto 1</orgName>
								<orgName type="institution" key="instit2">University of Bremen 2</orgName>
								<orgName type="institution" key="instit3">Vector Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Behrmann</surname></persName>
							<email>jensb@uni-bremen.de</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Toronto 1</orgName>
								<orgName type="institution" key="instit2">University of Bremen 2</orgName>
								<orgName type="institution" key="instit3">Vector Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Duvenaud</surname></persName>
							<email>duvenaud@cs.toronto.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Toronto 1</orgName>
								<orgName type="institution" key="instit2">University of Bremen 2</orgName>
								<orgName type="institution" key="instit3">Vector Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörn-Henrik</forename><surname>Jacobsen</surname></persName>
							<email>j.jacobsen@vectorinstitute.ai</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Toronto 1</orgName>
								<orgName type="institution" key="instit2">University of Bremen 2</orgName>
								<orgName type="institution" key="instit3">Vector Institute</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Residual Flows for Invertible Generative Modeling</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Flow-based generative models parameterize probability distributions through an invertible transformation and can be trained by maximum likelihood. Invertible residual networks provide a flexible family of transformations where only Lipschitz conditions rather than strict architectural constraints are needed for enforcing invertibility. However, prior work trained invertible residual networks for density estimation by relying on biased log-density estimates whose bias increased with the network's expressiveness. We give a tractable unbiased estimate of the log density using a "Russian roulette" estimator, and reduce the memory required during training by using an alternative infinite series for the gradient. Furthermore, we improve invertible residual blocks by proposing the use of activation functions that avoid derivative saturation and generalizing the Lipschitz condition to induced mixed norms. The resulting approach, called Residual Flows, achieves state-of-theart performance on density estimation amongst flow-based models, and outperforms networks that use coupling blocks at joint generative and discriminative modeling.</p><p>Existing flow-based models <ref type="bibr" target="#b39">(Rezende and Mohamed, 2015;</ref><ref type="bibr" target="#b28">Kingma et al., 2016;</ref><ref type="bibr" target="#b10">Dinh et al., 2014;</ref><ref type="bibr" target="#b8">Chen et al., 2018</ref>) make use of restricted transformations with sparse or structured Jacobians <ref type="figure">(Fig-33rd</ref> </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>(Free-form) <ref type="figure">Figure 1</ref>: Pathways to designing scalable normalizing flows and their enforced Jacobian structure. Residual Flows fall under unbiased estimation with free-form Jacobian.</p><p>Maximum likelihood is a core machine learning paradigm that poses learning as a distribution alignment problem. However, it is often unclear what family of distributions should be used to fit high-dimensional continuous data. In this regard, the change of variables theorem offers an appealing way to construct flexible distributions that allow tractable exact sampling and efficient evaluation of its density. This class of models is generally referred to as invertible or flow-based generative models <ref type="bibr" target="#b9">(Deco and Brauer, 1995;</ref><ref type="bibr" target="#b39">Rezende and Mohamed, 2015)</ref>.</p><p>With invertibility as its core design principle, flow-based models (also referred to as normalizing flows) have shown to be capable of generating realistic images <ref type="bibr" target="#b27">(Kingma and Dhariwal, 2018)</ref> and can achieve density estimation performance on-par with competing state-of-the-art approaches <ref type="bibr" target="#b19">(Ho et al., 2019)</ref>. In applications, they have been applied to study adversarial robustness  and are used to train hybrid models with both generative and classification capabilities <ref type="bibr">(Nalisnick et al., 2019)</ref> using a weighted maximum likelihood objective. ure 1). These allow efficient computation of the log probability under the model but at the cost of architectural engineering. Transformations that scale to high-dimensional data rely on specialized architectures such as coupling blocks <ref type="bibr" target="#b10">(Dinh et al., 2014</ref><ref type="bibr" target="#b11">(Dinh et al., , 2017</ref> or solving an ordinary differential equation . Such approaches have a strong inductive bias that can hinder their application in other tasks, such as learning representations that are suitable for both generative and discriminative tasks.</p><p>Recent work by  showed that residual networks <ref type="bibr" target="#b17">(He et al., 2016)</ref> can be made invertible by simply enforcing a Lipschitz constraint, allowing to use a very successful discriminative deep network architecture for unsupervised flow-based modeling. Unfortunately, the density evaluation requires computing an infinite series. The choice of a fixed truncation estimator used by  leads to substantial bias that is tightly coupled with the expressiveness of the network, and cannot be said to be performing maximum likelihood as bias is introduced in the objective and gradients.</p><p>In this work, we introduce Residual Flows, a flow-based generative model that produces an unbiased estimate of the log density and has memory-efficient backpropagation through the log density computation. This allows us to use expressive architectures and train via maximum likelihood. Furthermore, we propose and experiment with the use of activations functions that avoid derivative saturation and induced mixed norms for Lipschitz-constrained neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Maximum likelihood estimation. To perform maximum likelihood with stochastic gradient descent, it is sufficient to have an unbiased estimator for the gradient as</p><formula xml:id="formula_0">∇ θ D KL (p data || p θ ) = ∇ θ E x∼pdata(x) [log p θ (x)] = E x∼pdata(x) [∇ θ log p θ (x)] ,<label>(1)</label></formula><p>where p data is the unknown data distribution which can be sampled from and p θ is the model distribution. An unbiased estimator of the gradient also immediately follows from an unbiased estimator of the log density function, log p θ (x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Change of variables theorem.</head><p>With an invertible transformation f , the change of variables</p><formula xml:id="formula_1">log p(x) = log p(f (x)) + log det df (x) dx<label>(2)</label></formula><p>captures the change in density of the transformed samples. A simple base distribution such as a standard normal is often used for log p(f (x)). Tractable evaluation of (2) allows flow-based models to be trained using the maximum likelihood objective (1). In contrast, variational autoencoders (Kingma and Welling, 2014) can only optimize a stochastic lower bound, and generative adversial networks <ref type="bibr" target="#b13">(Goodfellow et al., 2014)</ref> require an extra discriminator network for training.</p><p>Invertible residual networks (i-ResNets). Residual networks are composed of simple transformations y = f (x) = x + g(x).  noted that this transformation is invertible by the Banach fixed point theorem if g is contractive, i.e. with Lipschitz constant strictly less than unity, which was enforced using spectral normalization <ref type="bibr" target="#b32">(Miyato et al., 2018;</ref><ref type="bibr" target="#b14">Gouk et al., 2018)</ref>.</p><p>Applying i-ResNets to the change-of-variables (2), the identity</p><formula xml:id="formula_2">log p(x) = log p(f (x)) + tr ∞ k=1 (−1) k+1 k [J g (x)] k<label>(3)</label></formula><p>was shown, where J g (x) = dg(x) dx . Furthermore, the Skilling-Hutchinson estimator <ref type="bibr" target="#b42">(Skilling, 1989;</ref><ref type="bibr" target="#b21">Hutchinson, 1990</ref>) was used to estimate the trace in the power series.  used a fixed truncation to approximate the infinite series in (3). However, this naïve approach has a bias that grows with the number of dimensions of x and the Lipschitz constant of g, as both affect the convergence rate of this power series. As such, the fixed truncation estimator requires a careful balance between bias and expressiveness, and cannot scale to higher dimensional data. Without decoupling the objective and estimation bias, i-ResNets end up optimizing for the bias without improving the actual maximum likelihood objective (see <ref type="figure" target="#fig_1">Figure 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Residual Flows</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Unbiased Log Density Estimation for Maximum Likelihood Estimation</head><p>Evaluation of the exact log density function log p θ (·) in (3) requires infinite time due to the power series. Instead, we rely on randomization to derive an unbiased estimator that can be computed in finite time (with probability one) based on an existing concept <ref type="bibr" target="#b24">(Kahn, 1955)</ref>.</p><p>To illustrate the idea, let ∆ k denote the k-th term of an infinite series, and suppose we always evaluate the first term then flip a coin b ∼ Bernoulli(q) to determine whether we stop or continue evaluating the remaining terms. By reweighting the remaining terms by 1 1−q , we obtain an unbiased estimator</p><formula xml:id="formula_3">∆ 1 + E ∞ k=2 ∆ k 1 − q 1 b=0 + (0)1 b=1 = ∆ 1 + ∞ k=2 ∆ k 1 − q (1 − q) = ∞ k=1 ∆ k .<label>(4)</label></formula><p>Interestingly, whereas naïve computation would always use infinite compute, this unbiased estimator has probability q of being evaluated in finite time. We can obtain an estimator that is evaluated in finite time with probability one by applying this process infinitely many times to the remaining terms. Directly sampling the number of evaluated terms, we obtain the appropriately named "Russian roulette" estimator <ref type="bibr" target="#b24">(Kahn, 1955</ref>)</p><formula xml:id="formula_4">∞ k=1 ∆ k = E n∼p(N ) n k=1 ∆ k P(N ≥ k) .<label>(5)</label></formula><p>We note that the explanation above is only meant to be an intuitive guide and not a formal derivation. The peculiarities of dealing with infinite quantities dictate that we must make assumptions on ∆ k , p(N ), or both in order for the equality in (5) to hold. While many existing works have made different assumptions depending on specific applications of (5), we state our result as a theorem where the only condition is that p(N ) must have support over all of the indices. Theorem 1 (Unbiased log density estimator). Let f (x) = x + g(x) with Lip(g) &lt; 1 and N be a random variable with support over the positive integers. Then</p><formula xml:id="formula_5">log p(x) = log p(f (x)) + E n,v n k=1 (−1) k+1 k v T [J g (x) k ]v P(N ≥ k) ,<label>(6)</label></formula><p>where n ∼ p(N ) and v ∼ N (0, I).</p><p>Here we have used the Skilling-Hutchinson trace estimator <ref type="bibr" target="#b42">(Skilling, 1989;</ref><ref type="bibr" target="#b21">Hutchinson, 1990)</ref> to estimate the trace of the matrices J k g . A detailed proof is given in Appendix B.  Note that since J g is constrained to have a spectral radius less than unity, the power series converges exponentially. The variance of the Russian roulette estimator is small when the infinite series exhibits fast convergence (Rhee and Glynn, 2015; <ref type="bibr" target="#b3">Beatson and Adams, 2019)</ref>, and in practice, we did not have to tune p(N ) for variance reduction. Instead, in our experiments, we compute two terms exactly and then use the unbiased estimator on the remaining terms with a single sample from p(N ) = Geom(0.5). This results in an expected compute cost of 4 terms, which is less than the 5 to 10 terms that  used for their biased estimator.</p><p>Theorem 1 forms the core of Residual Flows, as we can now perform maximum likelihood training by backpropagating through (6) to obtain unbiased gradients. This allows us to train more expressive networks where a biased estimator would fail ( <ref type="figure" target="#fig_1">Figure 2</ref>). The price we pay for the unbiased estimator is variable compute and memory, as each sample of the log density uses a random number of terms in the power series.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Memory-Efficient Backpropagation</head><p>Memory can be a scarce resource, and running out of memory due to a large sample from the unbiased estimator can halt training unexpectedly. To this end, we propose two methods to reduce the memory consumption during training.</p><p>To see how naïve backpropagation can be problematic, the gradient w.r.t. parameters θ by directly differentiating through the power series (6) can be expressed as</p><formula xml:id="formula_6">∂ ∂θ log det I + J g (x, θ) = E n,v n k=1 (−1) k+1 k ∂v T (J g (x, θ) k )v ∂θ .<label>(7)</label></formula><p>Unfortunately, this estimator requires each term to be stored in memory because ∂ /∂θ needs to be applied to each term. The total memory cost is then O(n · m) where n is the number of computed terms and m is the number of residual blocks in the entire network. This is extremely memory-hungry during training, and a large random sample of n can occasionally result in running out of memory.</p><p>Neumann gradient series. Instead, we can specifically express the gradients as a power series derived from a Neumann series (see Appendix C). Applying the Russian roulette and trace estimators, we obtain the following theorem.</p><p>Theorem 2 (Unbiased log-determinant gradient estimator). Let Lip(g) &lt; 1 and N be a random variable with support over positive integers. Then</p><formula xml:id="formula_7">∂ ∂θ log det I + J g (x, θ) = E n,v n k=0 (−1) k P(N ≥ k) v T J(x, θ) k ∂(J g (x, θ)) ∂θ v ,<label>(8)</label></formula><p>where n ∼ p(N ) and v ∼ N (0, I).</p><p>As the power series in (8) does not need to be differentiated through, using this reduces the memory requirement by a factor of n. This is especially useful when using the unbiased estimator as the memory will be constant regardless of the number of terms we draw from p(N ).</p><p>Backward-in-forward: early computation of gradients. We can further reduce memory by partially performing backpropagation during the forward evaluation. By taking advantage of log det(I + J g (x, θ)) being a scalar quantity, the partial derivative from the objective L is  For every residual block, we compute ∂ log det(I+Jg(x,θ)) /∂θ along with the forward pass, release the memory for the computation graph, then simply multiply by ∂L /∂ log det(I+Jg(x,θ)) later during the main backprop. This reduces memory by another factor of m to O(1) with negligible overhead.</p><formula xml:id="formula_8">∂L ∂θ = ∂L ∂ log det(I + J g (x, θ)) scalar ∂ log det(I + J g (x, θ)) ∂θ vector .<label>(9</label></formula><p>Note that while these two tricks remove the memory cost from backpropagating through the log det terms, computing the path-wise derivatives from log p(f (x)) still requires the same amount of memory as a single evaluation of the residual network. <ref type="figure" target="#fig_2">Figure 3</ref> shows that the memory consumption can be enormous for naïve backpropagation, and using large networks would have been intractable. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Avoiding Derivative Saturation with the LipSwish Activation Function</head><p>As the log density depends on the first derivatives through the Jacobian J g , the gradients for training depend on second derivatives. Similar to the phenomenon of saturated activation functions, Lipschitzconstrained activation functions can have a derivative saturation problem. For instance, the ELU activation used by  achieves the highest Lipschitz constant when ELU (z) = 1, but this occurs when the second derivative is exactly zero in a very large region, implying there is a trade-off between a large Lipschitz constant and non-vanishing gradients.</p><p>We thus desire two properties from our activation functions φ(z):</p><p>1. The first derivatives must be bounded as |φ (z)| ≤ 1 for all z 2. The second derivatives should not asymptotically vanish when |φ (z)| is close to one.</p><p>While many activation functions satisfy condition 1, most do not satisfy condition 2. We argue that the ELU and softplus activations are suboptimal due to derivative saturation. <ref type="figure" target="#fig_3">Figure 4</ref> shows that when softplus and ELU saturate at regions of unit Lipschitz, the second derivative goes to zero, which can lead to vanishing gradients during training.</p><p>We find that good activation functions satisfying condition 2 are smooth and non-monotonic functions, such as Swish <ref type="bibr">(Ramachandran et al., 2017)</ref>. However, Swish by default does not satisfy condition 1</p><formula xml:id="formula_9">as max z | d dz Swish(z)| 1.1. But scaling via LipSwish(z) := Swish(z)/1.1 = z · σ(βz)/1.1,<label>(10)</label></formula><p>where σ is the sigmoid function, results in max z | d dz LipSwish(z)| ≤ 1 for all values of β. LipSwish is a simple modification to Swish that exhibits a less than unity Lipschitz property. In our experiments, we parameterize β to be strictly positive by passing it through softplus. <ref type="figure" target="#fig_3">Figure 4</ref> shows that in the region of maximal Lipschitz, LipSwish does not saturate due to its non-monotonicity property.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Estimation of Infinite Series. Our derivation of the unbiased estimator follows from the general approach of using a randomized truncation <ref type="bibr" target="#b24">(Kahn, 1955)</ref>. This paradigm of estimation has been repeatedly rediscovered and applied in many fields, including solving of stochastic differential equations <ref type="bibr" target="#b31">(McLeish, 2011;</ref><ref type="bibr">Glynn, 2012, 2015)</ref>, ray tracing for rendering paths of light <ref type="bibr" target="#b2">(Arvo and Kirk, 1990)</ref>, and estimating limiting behavior of optimization problems <ref type="bibr" target="#b43">(Tallec and Ollivier, 2017;</ref><ref type="bibr" target="#b3">Beatson and Adams, 2019)</ref>, among many other applications. Some recent works use Chebyshev polynomials to estimate the spectral functions of symmetric matrices <ref type="bibr" target="#b16">(Han et al., 2018;</ref><ref type="bibr" target="#b0">Adams et al., 2018;</ref><ref type="bibr" target="#b38">Ramesh and LeCun, 2018)</ref>. These works estimate quantities that are similar to those presented in this work, but a key difference is that the Jacobian in our power series is not symmetric. Works that proposed the random truncation approach typically made assumptions on p(N ) in order for it to be applicable to general infinite series <ref type="bibr" target="#b31">(McLeish, 2011;</ref><ref type="bibr" target="#b41">Rhee and Glynn, 2015;</ref><ref type="bibr" target="#b16">Han et al., 2018)</ref>. Fortunately, since the power series in Theorems 1 and 2 converge fast enough, we were able to make use of a different set of assumptions requiring only that p(N ) has sufficient support (details in Appendix B).</p><p>Memory-efficient Backpropagation. The issue of computing gradients in a memory-efficient manner was explored by <ref type="bibr" target="#b12">Gomez et al. (2017)</ref> and <ref type="bibr" target="#b7">Chang et al. (2018)</ref> for residual networks with a <ref type="table" target="#tab_4">Table 1</ref>: Results [bits/dim] on standard benchmark datasets for density estimation. In brackets are models that used "variational dequantization" <ref type="bibr" target="#b19">(Ho et al., 2019)</ref>, which we don't compare against.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>MNIST CIFAR-10 ImageNet 32 ImageNet 64 CelebA-HQ 256</p><p>Real NVP <ref type="bibr" target="#b11">(Dinh et al., 2017)</ref>    0.99 3.40 ---Flow++ <ref type="bibr" target="#b19">(Ho et al., 2019)</ref> -3.29 (3.09) -(3.86) -(3.69) i-ResNet    coupling-based architecture devised by <ref type="bibr" target="#b10">Dinh et al. (2014)</ref>, and explored by Chen et al. <ref type="formula" target="#formula_0">(2018)</ref> for a continuous analogue of residual networks. These works focus on the path-wise gradients from the output of the network, whereas we focus on the gradients from the log-determinant term in the change of variables equation specifically for generative modeling. On the other hand, our approach shares some similarities with Recurrent Backpropagation <ref type="bibr" target="#b1">(Almeida, 1987;</ref><ref type="bibr" target="#b36">Pineda, 1987;</ref><ref type="bibr" target="#b29">Liao et al., 2018)</ref>, since both approaches leverage convergent dynamics to modify the derivatives.</p><p>Invertible Deep Networks. Flow-based generative models are a density estimation approach which has invertibility as its core design principle <ref type="bibr" target="#b39">(Rezende and Mohamed, 2015;</ref><ref type="bibr" target="#b9">Deco and Brauer, 1995)</ref>. Most recent work on flows focuses on designing maximally expressive architectures while maintaining invertibility and tractable log determinant computation <ref type="bibr" target="#b10">(Dinh et al., 2014</ref><ref type="bibr" target="#b11">(Dinh et al., , 2017</ref><ref type="bibr" target="#b27">Kingma and Dhariwal, 2018</ref>). An alternative route has been taken by Continuous Normalizing Flows (Chen et al., 2018) which make use of Jacobian traces instead of Jacobian determinants, provided that the transformation is parameterized by an ordinary differential equation. Invertible architectures are also of interest for discriminative problems, as their information-preservation properties make them suitable candidates for analyzing and regularizing learned representations .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Density &amp; Generative Modeling</head><p>We use a similar architecture as , except without the immediate invertible downsampling <ref type="bibr" target="#b11">(Dinh et al., 2017)</ref> at the image pixel-level. Removing this substantially increases the amount of memory required (shown in <ref type="figure" target="#fig_2">Figure 3</ref>) as there are more spatial dimensions at every layer, but increases the overall performance. We also increase the bound on the Lipschitz constants of each weight matrix to 0.  Furthermore, it is possible to generalize the Lipschitz condition of Residual Flows to arbitrary p-norms and even mixed matrix norms. By learning the norm orders jointly with the model, we achieved a small gain of 0.003 bits/dim on CIFAR-10 compared to spectral normalization. In addition, we show that others norms like p = ∞ yielded constraints more suited for lower dimensional data. See Appendix D for a discussion on how to generalize the Lipschitz condition and an exploration of different norm-constraints for 2D problems and image data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Sample Quality</head><p>We are also competitive with state-of-the-art flow-based models in regards to sample quality. <ref type="figure" target="#fig_4">Figure 5</ref> shows random samples from the model trained on CelebA. Furthermore, samples from Residual Flow trained on CIFAR-10 are more globally coherent ( <ref type="figure" target="#fig_5">Figure 6</ref>) than PixelCNN and variational dequantized Flow++, even though our likelihood is worse. For quantitative comparison, we report FID scores <ref type="bibr" target="#b18">(Heusel et al., 2017)</ref> in <ref type="table" target="#tab_5">Table 2</ref>. We see that Residual Flows significantly improves on i-ResNets and PixelCNN, and achieves slightly better sample quality than an official Glow model that has double the number of layers. It is well-known that visual fidelity and loglikelihood are not necessarily indicative of each other <ref type="bibr" target="#b44">(Theis et al., 2015)</ref>, but we believe residual blocks may have a better inductive bias than coupling blocks or autoregressive architectures as generative models. More samples are in Appendix A.</p><p>To generate visually appealing images, Kingma and Dhariwal (2018) used temperature annealing (ie. sampling from [p(x)] T 2 with T &lt; 1) to sample closer to the mode of the distribution, which helped remove artifacts from the samples and resulted in smoother looking images. However, this is done by reducing the entropy of p(z) during sampling, which is only equivalent to temperature annealing if the change in log-density does not depend on the sample itself. Intuitively, this assumption implies that the mode of p(x) and p(z) are the same. As this assumption breaks for general flow-based models, including Residual Flows, we cannot use the same trick to sample efficiently from a temperature annealed model. <ref type="figure" target="#fig_6">Figure 7</ref> shows the results of reduced entropy sampling on CelebA-HQ 256, but the samples do not converge to the mode of the distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablation Experiments</head><p>We report ablation experiments for the unbiased estimator and the LipSwish activation function in <ref type="table" target="#tab_7">Table 3</ref>. Even in settings where the Lipschitz constant and bias are relatively low, we observe a significant improvement from using the unbiased estimator. Training the larger i-ResNet model   on CIFAR-10 results in the biased estimator completely ignoring the actual likelihood objective altogether. In this setting, the biased estimate was lower than 0.8 bits/dim by 50 epochs, but the actual bits/dim wildly oscillates above 3.66 bits/dim and seems to never converge. Using LipSwish not only converges much faster but also results in better performance compared to softplus or ELU, especially in the high Lipschitz settings <ref type="figure">(Figure 8</ref> and <ref type="table" target="#tab_7">Table 3</ref>).</p><formula xml:id="formula_10">T =0.7 T =0.8 T =0.9 T =1.0 T =0.7 T =0.8 T =0.9 T =1.0</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Hybrid Modeling</head><p>Next, we experiment on joint training of continuous and discrete data. Of particular interest is the ability to learn both a generative model and a classifier, referred to as a hybrid model which is useful for downstream applications such as semi-supervised learning and out-of-distribution detection <ref type="bibr">(Nalisnick et al., 2019)</ref>. Let x be the data and y be a categorical random variable. The maximum likelihood objective can be separated into log p(x, y) = log p(x) + log p(y|x), where log p(x) is modeled using a flow-based generative model and log p(y|x) is a classifier network that shares learned features from the generative model. However, it is often the case that accuracy is the metric of interest and log-likelihood is only used as a surrogate training objective. In this case, (Nalisnick et al., 2019) suggests a weighted maximum likelihood objective,</p><formula xml:id="formula_11">E (x,y)∼pdata [λ log p(x) + log p(y|x)],<label>(11)</label></formula><p>where λ is a scaling constant. As y is much lower dimensional than x, setting λ &lt; 1 emphasizes classification, and setting λ = 0 results in a classification-only model which can be compared against.  In general, we find that residual blocks perform much better than coupling blocks at learning representations for both generative and discriminative tasks. Coupling blocks have very high bits per dimension when λ = 1 /D while performing worse at classification when λ = 1, suggesting that they have restricted flexibility and can only perform one task well at a time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have shown that invertible residual networks can be turned into powerful generative models. The proposed unbiased flow-based generative model, coined Residual Flow, achieves competitive or better performance compared to alternative flow-based models in density estimation, sample quality, and hybrid modeling. More generally, we gave a recipe for introducing stochasticity in order to construct tractable flow-based models with a different set of constraints on layer architectures than competing approaches, which rely on exact log-determinant computations. This opens up a new design space of expressive but Lipschitz-constrained architectures that has yet to be explored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Random Samples</head><p>Real Data Residual Flow PixelCNN Flow++    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Proofs</head><p>We start by formulating a Lemma, which gives the condition when the randomized truncated series is an unbiased estimator in a fairly general setting. Afterwards, we study our specific estimator and prove that the assumption of the Lemma is satisfied.</p><p>Note, that similar conditions have been stated in previous works, e.g. in <ref type="bibr" target="#b31">McLeish (2011)</ref> and Rhee and Glynn (2012). However, we use the condition from Bouchard-Côté (2018), which only requires p(N ) to have sufficient support.</p><p>To make the derivations self-contained, we reformulate the conditions from Bouchard-Côté (2018) in the following way: Lemma 3 (Unbiased randomized truncated series). Let Y k be a real random variable with</p><formula xml:id="formula_12">lim k→∞ E[Y k ] = a for some a ∈ R. Further, let ∆ 0 = Y 0 and ∆ k = Y k − Y k−1 for k ≥ 1. Assume E ∞ k=0 |∆ k | &lt; ∞</formula><p>and let N be a random variable with support over the positive integers and n ∼ p(N ). Then for</p><formula xml:id="formula_13">Z = n k=0 ∆ k P(N ≥ k) , it holds a = lim k→∞ E[Y k ] = E n∼p(N ) [Z] = a.</formula><p>Proof. First, denote</p><formula xml:id="formula_14">Z M = M k=0 1[N ≥ k]∆ k P(N ≥ k) and B M = M k=0 1[N ≥ k]|∆ k | P(N ≥ k) ,</formula><p>where |Z M | ≤ B M by the triangle inequality. Since B M is non-decreasing, the monotone convergence theorem allows swapping the expectation and limit as</p><formula xml:id="formula_15">E[B] = E[lim M →∞ B M ] = ∆ k = lim M →∞ E [Y k ] = a,</formula><p>where we used the definition of Y M and lim k→∞ E[Y k ] = a.</p><p>Proof. (Theorem 1) To simplify notation, we denote J := J g (x). Furthermore, let</p><formula xml:id="formula_16">Y N = E v N k=1 (−1) k+1 k v T J k v</formula><p>denote the real random variable and let ∆ 0 = Y 0 and ∆ k = Y k − Y k−1 for k ≥ 1, as in Lemma 3. To prove the claim of the theorem, we can use Lemma 3 and we only need to prove that the assumption</p><formula xml:id="formula_17">E v [ ∞ k=1</formula><p>|∆ k |] &lt; ∞ holds for this specific case. In order to exchange summation and expectation via Fubini's theorem, we need to prove that</p><formula xml:id="formula_18">∞ k=1 |∆ k | &lt; ∞ for all v. Using the assumption Lip(g) &lt; 1, it is ∞ k=1 |∆ k | = ∞ k=1 (−1) k+1 k v T J k v = ∞ k=1 v T J k v 2 k ≤ ∞ k=1 v T 2 J k 2 v 2 k ≤ 2 v 2 ∞ k=1 J k 2 k ≤ 2 v 2 ∞ k=1 Lip(g) k 2 k = 2 v 2 log 1 − Lip(g) &lt; ∞,</formula><p>for an arbitrary v. Hence,</p><formula xml:id="formula_19">E v ∞ k=1 |∆ k | = ∞ k=1 E v [|∆ k |].<label>(12)</label></formula><p>Since</p><formula xml:id="formula_20">tr(A) = E v [v T Av]</formula><p>for v ∼ N (0, I) via the Skilling-Hutchinson trace estimator <ref type="bibr" target="#b21">(Hutchinson, 1990;</ref><ref type="bibr" target="#b42">Skilling, 1989)</ref>, it is</p><formula xml:id="formula_21">E v [|∆ k |] = tr(J k ) k .</formula><p>To show that (12) is bounded, we derive the bound</p><formula xml:id="formula_22">1 k | tr(J k )| ≤ 1 k d i=d λ i (J k ) ≤ 1 k d i=d |λ i (J k )| ≤ d k ρ(J k ) ≤ d k J k 2 ≤ d k Lip(g) k ,</formula><p>where λ(J k ) denote the eigenvalues and ρ(J k ) the spectral radius. Inserting this bound into (12) and using Lip(g) &lt; 1 yields</p><formula xml:id="formula_23">E v [|∆ k |] ≤ d ∞ k=1 Lip(g) k k = −d log 1 − Lip(g) &lt; ∞.</formula><p>Hence, the assumption of Lemma 3 is verified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof. (Theorem 2)</head><p>The result can be proven in an analogous fashion to the proof of Theorem 1, which is why we only present a short version without all steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>By obtaining the bound</head><formula xml:id="formula_24">∞ k=0 (−1) k v T J(x, θ) k ∂(J g (x, θ)) ∂θ v ≤ 2 v 2 ∂(J g (x, θ)) ∂θ ∞ k=0 Lip(g) k = 2 v 2 ∂(J g (x, θ)) ∂θ 1 1 − Lip(g) &lt; ∞,</formula><p>Fubini's theorem can be applied to swap the expection and summation. Furthermore, by using the trace estimation and similar bounds as in the proof of Theorem 1, the assumption E [ ∞ k=0 |∆ k |] &lt; ∞ from Lemma 3 can be proven.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Memory-Efficient Gradient Estimation of Log-Determinant</head><p>Derivation of gradient estimator via differentiating power series:</p><formula xml:id="formula_25">∂ ∂θ i log det I + J g (x, θ) = ∂ ∂θ i ∞ k=1 (−1) k+1 tr(J g (x, θ) k ) k = tr ∞ k=1 (−1) k+1 k ∂(J g (x, θ) k ) ∂θ i</formula><p>Derivation of memory-efficient gradient estimator:</p><formula xml:id="formula_26">∂ ∂θ i log det I + J g (x, θ) = 1 det(I + J g (x, θ)) ∂ ∂θ i det I + J g (x, θ)<label>(13)</label></formula><p>= 1 det(I + J g (x, θ)) det(I + J g (x, θ)) tr (I + J(x, θ)) −1 ∂(I + J g (x, θ)) ∂θ i (14)</p><formula xml:id="formula_27">= tr (I + J g (x, θ)) −1 ∂(I + J g (x, θ)) ∂θ i = tr (I + J g (x, θ)) −1 ∂(J g (x, θ)) ∂θ i = tr ∞ k=0 (−1) k J g (x, θ) k ∂(J g (x, θ)) ∂θ i .<label>(15)</label></formula><p>Note, that (13) follows from the chain rule of differentiation, for the derivative of the determinant in (14), see <ref type="bibr">(Petersen and Pedersen, 2012, eq. 46)</ref>. Furthermore, (15) follows from the properties of a Neumann-Series which converges due to J g (x, θ) &lt; 1.</p><p>Hence, if we are able to compute the trace exactly, both approaches will return the same values for a given truncation n. However, when estimating the trace via the Hutchinson trace estimator the estimation is not equal in general:</p><formula xml:id="formula_28">v T ∞ k=1 (−1) k+1 k ∂(J g (x, θ) k ) ∂θ i v = v T ∞ k=0 (−1) k J k g (x, θ) ∂(J g (x, θ)) ∂θ i v.</formula><p>This is due to the terms in these infinite series being different.</p><p>Another difference between both approaches is their memory consumption of the corresponding computational graph. The summation ∞ k=0 (−1) k J k g (x, θ) is not being tracked for the gradient, which allows to compute the gradient with constant memory with respect to the truncation n. Bits/dim p = 2 p = learned p = (b) CIFAR-10 <ref type="figure" target="#fig_4">Figure 15</ref>: Lipschitz constraints with different induced matrix norms. (a) On 2D density, both learned and ∞-norm improve upon spectral norm, allowing more expressive models with fewer blocks. Standard deviation across 3 random seeds. (b) On CIFAR-10, learning the norm orders give a small performance gain and the ∞-norm performs much worse than spectral norm (p = 2).</p><p>Comparisons are made using identical initialization.  used spectral normalization <ref type="bibr" target="#b32">(Miyato et al., 2018)</ref> (which relies on power iteration to approximate the spectral norm) to enforce the Lipschitz constraint on g. Specifically, this bounds the spectral norm of the Jacobian J g by the sub-multiplicativity property. If g(x) is a neural network with pre-activation defined recursively using z l = W l h l−1 + b l and h l = φ(z l ), with x = z 0 , g(x) = z L , then the data-independent upper bound</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Generalizing Lipschitz Constraints to Induced Mixed Norms</head><formula xml:id="formula_29">||J g || 2 = ||W L φ (z L−1 ) · · · W 2 φ (z 1 )W 1 φ (z 0 )|| 2 ≤ ||W 1 || 2 · · · ||W L || 2<label>(16)</label></formula><p>holds, where φ (z) are diagonal matrices containing the first derivatives of the activation functions. The inequality in (16) is a result of using a sub-multiplicative norm and assuming that the activation functions have Lipschitz less than unity. However, any induced matrix norm satisfies the submultiplicativity property, including mixed norms ||W || p→q := sup x =0 ||W x||q /||x||p, where the input and output spaces have different vector norms.</p><p>As long as g(x) maps back to the original normed (complete) vector space, the Banach fixed point theorem used in the proof of invertibility of residual blocks  still holds. As such, we can choose arbitrary p 0 , . . . , p L−2 ∈ [1, ∞] such that</p><formula xml:id="formula_30">||J g || p0 ≤ ||W 1 || p0→p1 ||W 2 || p1→p2 · · · ||W L || p L−2 →p0 .<label>(17)</label></formula><p>We use a more general form of power iteration <ref type="bibr" target="#b23">(Johnston, 2016)</ref> for estimating induced mixed norms, which becomes the standard power iteration for p = q = 2. Furthermore, the special cases where p l = 1 or p l = ∞ are of particular interest, as the matrix norms can be computed exactly <ref type="bibr" target="#b45">(Tropp, 2004)</ref>. Additionally, we can also optimize the norm orders during training by backpropagating through the modified power method. Lastly, we emphasize that the convergence of the infinite series (3) is guaranteed for any induced matrix norm, as they still upper bound the spectral radius <ref type="bibr" target="#b20">(Horn and Johnson, 2012)</ref>. <ref type="figure" target="#fig_4">Figure 15a</ref> shows that we obtain some performance gain by using either learned norms or the infinity norm on a difficult 2D dataset, where similar performance can be achieved by using fewer residual blocks. While the infinity norm works well with fully connected layers, we find that it does not perform as well as the spectral norm for convolutional networks. Instead, learned norms always perform slightly better and was able to obtain an improvement of 0.003 bits/dim on CIFAR-10. Ultimately, while the idea of generalizing spectral normalization via learnable norm orders is interesting in its own right to be communicated here, we found that the improvements are very marginal.</p><p>Data p = 2 (5.13 bits) p = ∞ (5.09 bits) <ref type="figure" target="#fig_5">Figure 16</ref>: Learned densities on Checkerboard 2D.</p><p>Using different induced p-norms on Checkerboard 2D. We experimented with the checkerboard 2D dataset, which is a rather difficult two-dimensional data to fit a flow-based model on due to the discontinuous nature of the true distribution. We used brute-force computation of the log-determinant for change of variables (which, in the 2D case, is faster than the unbiased estimator). In the 2D case, we found that ∞-norm always outperforms or at least matches the p = 2 norm (ie. spectral norm). <ref type="figure" target="#fig_5">Figure 16</ref> shows the learned densities with 200 residual blocks. The color represents the magnitude of p θ (x), with brighter values indicating larger values. The ∞-norm model produces density estimates that are more evenly spread out across the space, whereas the spectral norm model focused its density to model between-density regions. Learning norm orders on CIFAR-10. We used 1 + tanh(s)/2 where s is a learned weight. This bounds the norm orders to (1.5, 2.5). We tried two different setups. One where all norm orders are free to change (conditioned on them satisfying the constraints <ref type="formula" target="#formula_0">(17)</ref>), and another setting where each states within each residual block share the same order. <ref type="figure" target="#fig_6">Figure 17</ref> shows the improvement in bits from using learned norms. The gain in performance is marginal, and the final models only outperformed spectral norm by around 0.003 bits/dim. Interestingly, we found that the learned norms stayed around p = 2, shown in <ref type="figure" target="#fig_13">Figure 18</ref>, especially for the input and output spaces of g, ie. between blocks. This may suggest that spectral norm, or a norm with p = 2 is already optimal in this setting. The input and two hidden states for each block use different normed spaces. We observe multiple trends: (i) the norms for the first hidden states are consistently higher than the input, and lower for the second. (ii) The orders for the hidden states drift farther away from 2 as depth increases. (iii) The ending order of one block and the starting order of the next are generally consistent and close to 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Experiment Setup</head><p>We use the standard setup of passing the data through a "unsquashing" layer (we used the logit transform <ref type="bibr" target="#b11">(Dinh et al., 2017)</ref>), followed by alternating multiple blocks and squeeze layers <ref type="bibr" target="#b11">(Dinh et al., 2017)</ref>. We use activation normalization <ref type="bibr" target="#b27">(Kingma and Dhariwal, 2018)</ref>  For density modeling on MNIST and CIFAR-10, we added 4 fully connected residual blocks at the end of the network, with intermediate hidden dimensions of 128. These residual blocks were not used in the hybrid modeling experiments or on other datasets. For datasets with image size higher than 32×32, we factored out half the variables after every squeeze operation other than the first one.</p><p>For hybrid modeling on CIFAR-10, we replaced the logit transform with normalization by the standard preprocessing of subtracting the mean and dividing by the standard deviation across the training data. The MNIST and SVHN architectures for hybrid modeling were the same as those for density modeling.</p><p>For augmenting our flow-based model with a classifier in the hybrid modeling experiments, we added an additional branch after every squeeze layer and at the end of the network. Each branch consisted of 3×3 Conv → ActNorm → ReLU → AdaptiveAveragePooling <ref type="figure">((1, 1)</ref>)</p><p>where the adaptive average pooling averages across all spatial dimensions and resulted in a vector of dimension 256. The outputs at every scale were concatenated together and fed into a linear softmax classifier.</p><p>Adaptive number of power iterations. We used spectral normalization for convolutions <ref type="bibr" target="#b14">(Gouk et al., 2018)</ref>. To account for variable weight updates during training, we implemented an adaptive version of spectral normalization where we performed as many iterations as needed until the relative change in the estimated spectral norm was sufficiently small. As this acted as an amortization that reduces the number of iterations when weight updates are small, this did not result in higher time cost than a fixed number of power iterations, and at the same time, acts as a more reliable guarantee that the Lipschitz is bounded.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>i-ResNets suffer from substantial bias when using expressive networks, whereas Residual Flows principledly perform maximum likelihood with unbiased stochastic gradients.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Memory usage (GB) per minibatch of 64 samples when computing n=10 terms in the corresponding power series. CIFAR10-small uses immediate downsampling before any residual blocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>LipSwishFigure 4 :</head><label>4</label><figDesc>Common smooth Lipschitz activation functions φ usually have vanishing φ when φ is maximal. LipSwish has a non-vanishing φ in the region where φ is close to one.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Qualitative samples. Real (left) and random samples (right) from a model trained on 5bit 64×64 CelebA. The most visually appealing samples were picked out of 5 random batches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>98, whereas Behrmann et al. (2019) used 0.90 to reduce the error of the biased estimator. More detailed description of architectures is in Appendix E.Unlike prior works that use multiple GPUs, large batch sizes, and a few hundred epochs, Residual Flow models are trained with the standard batch size of 64 and converges in roughly 300-350 epochs for MNIST and CIFAR-10. Most network settings can fit on a single GPU (seeFigure 3), though we use 4 GPUs in our experiments to speed up training. On CelebA-HQ, Glow had to use a batchsize of 1 per GPU with a budget of 40 GPUs whereas we trained our model using a batchsize of 3 per GPU and a budget of 4 GPUs, owing to the smaller model and memory-efficient backpropagation. Random samples from Residual Flow are more globally coherent.PixelCNN (Oord et al.,  2016)  and Flow++ samples reprinted from<ref type="bibr" target="#b19">Ho et al. (2019)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Reduced entropy sampling does not equate with proper temperature annealing for general flow-based models. Naïvely reducing entropy results in samples that exhibit black hair and background, indicating that samples are not converging to the mode of the distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Random samples from CIFAR-10 models. PixelCNN (Oord et al., 2016) and Flow++ samples reprinted from Ho et al. (2019), with permission.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :Figure 11 :</head><label>1011</label><figDesc>Random samples from MNIST. Random samples from ImageNet 32×32.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 12 :</head><label>12</label><figDesc>Random samples from ImageNet 64×64. Real Data Residual Flow Figure 13: Random samples from 5bit CelebA-HQ 64×64.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 14 :</head><label>14</label><figDesc>Random samples from 5bit CelebA-HQ 256×256. Most visually appealing batch out of five was chosen.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 17 :</head><label>17</label><figDesc>Improvement from using generalized spectral norm on CIFAR-10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 18 :</head><label>18</label><figDesc>Learned norm orders on CIFAR-10. Each residual block is visualized as a single line.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1</head><label>1</label><figDesc>reports the bits per dimension (log 2 p(x)/d where x ∈ R d ) on standard benchmark datasets MNIST, CIFAR-10, downsampled ImageNet, and CelebA-HQ. We achieve competitive performance to state-of-the-art flow-based models on all datasets. For evaluation, we computed 20 terms of the power series (3) and use the unbiased estimator (6) to estimate the remaining terms. This reduces the standard deviation of the unbiased estimate of the test bits per dimension to a negligible level.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Lower FID implies better sample quality.</figDesc><table><row><cell>Model</cell><cell>CIFAR10 FID</cell></row><row><cell>PixelCNN  *</cell><cell>65.93</cell></row><row><cell>PixelIQN  *</cell><cell>49.46</cell></row><row><cell>i-ResNet</cell><cell>65.01</cell></row><row><cell>Glow</cell><cell>46.90</cell></row><row><cell>Residual Flow</cell><cell>46.37</cell></row><row><cell>DCGAN  *</cell><cell>37.11</cell></row><row><cell>WGAN-GP  *</cell><cell>36.40</cell></row></table><note>* Results taken from Ostrovski et al. (2018).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Ablation results.</figDesc><table /><note>† Uses immediate downsampling before any residual blocks.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Comparison of residual vs. coupling blocks for the hybrid modeling task.</figDesc><table><row><cell></cell><cell></cell><cell>MNIST</cell><cell></cell><cell></cell><cell></cell><cell>SVHN</cell><cell></cell></row><row><cell></cell><cell>λ = 0</cell><cell>λ = 1 /D</cell><cell>λ = 1</cell><cell></cell><cell>λ = 0</cell><cell>λ = 1 /D</cell><cell>λ = 1</cell></row><row><cell>Block Type</cell><cell>Acc↑</cell><cell>BPD↓ Acc↑</cell><cell cols="2">BPD↓ Acc↑</cell><cell>Acc↑</cell><cell>BPD↓ Acc↑</cell><cell cols="2">BPD↓ Acc↑</cell></row><row><cell cols="2">Nalisnick et al. (2019) 99.33%</cell><cell>1.26 97.78%</cell><cell>−</cell><cell>−</cell><cell>95.74%</cell><cell>2.40 94.77%</cell><cell>−</cell><cell>−</cell></row><row><cell>Coupling</cell><cell>99.50%</cell><cell>1.18 98.45%</cell><cell cols="2">1.04 95.42%</cell><cell>96.27%</cell><cell>2.73 95.15%</cell><cell cols="2">2.21 46.22%</cell></row><row><cell>+ 1 × 1 Conv</cell><cell>99.56%</cell><cell>1.15 98.93%</cell><cell cols="2">1.03 94.22%</cell><cell>96.72%</cell><cell>2.61 95.49%</cell><cell cols="2">2.17 46.58%</cell></row><row><cell>Residual</cell><cell>99.53%</cell><cell>1.01 99.46%</cell><cell cols="2">0.99 98.69%</cell><cell>96.72%</cell><cell>2.29 95.79%</cell><cell cols="2">2.06 58.52%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Hybrid modeling results on CIFAR-10.SinceNalisnick et al. (2019)  performs approximate Bayesian inference and uses a different architecture than us, we perform our own ablation experiments to compare residual blocks to coupling blocks<ref type="bibr" target="#b10">(Dinh et al., 2014)</ref> as well as 1×1 convolutions<ref type="bibr" target="#b27">(Kingma and Dhariwal, 2018)</ref>. We use the same architecture as the density estimation experiments and append a classification branch that takes features at the final output of multiple scales (see details in Appendix E). This allows us to also use features from intermediate blocks whereasNalisnick et al. (2019)  only used the final output of the entire network for classification. Our implementation of coupling blocks uses the same architecture for g(x) except we use ReLU activations and no longer constrain the Lipschitz constant.</figDesc><table><row><cell></cell><cell>λ = 0</cell><cell>λ = 1 /D</cell><cell>λ = 1</cell></row><row><cell>Block Type</cell><cell>Acc↑</cell><cell>BPD↓ Acc↑</cell><cell>BPD↓ Acc↑</cell></row><row><cell>Coupling</cell><cell>89.77%</cell><cell>4.30 87.58%</cell><cell>3.54 67.62%</cell></row><row><cell cols="2">+ 1 × 1 Conv 90.82%</cell><cell>4.09 87.96%</cell><cell>3.47 67.38%</cell></row><row><cell>Residual</cell><cell>91.78%</cell><cell>3.62 90.47%</cell><cell>3.39 70.32%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>before and after every residual block. Each residual connection consists of LipSwish → 3×3 Conv → LipSwish → 1×1 Conv → LipSwish → 3×3 Conv with hidden dimensions of 512. Below are the architectures for each dataset.MNIST. With α =1e-5. Image → LogitTransform(α) → 16×ResBlock → Squeeze → 16×ResBlock ×2 CIFAR-10. With α = 0.05. Image → LogitTransform(α) → 16×ResBlock → Squeeze → 16×ResBlock ×2 SVHN. With α = 0.05. Image → LogitTransform(α) → 16×ResBlock → Squeeze → 16×ResBlock ×2 ImageNet 32×32. With α = 0.05. Image → LogitTransform(α) → 32×ResBlock → Squeeze → 32×ResBlock ×2 ImageNet 64×64. With α = 0.05. Image → Squeeze → LogitTransform(α) → 32×ResBlock → Squeeze → 32×ResBlock ×2 CelebA 5bit 64×64. With α = 0.05. Image → Squeeze → LogitTransform(α) → 16×ResBlock → Squeeze → 16×ResBlock ×3CelebA 5bit 256×256. With α = 0.05.</figDesc><table /><note>Image → Squeeze → LogitTransform(α) → 16×ResBlock → Squeeze → 16×ResBlock ×5</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Real DataResidual Flow</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Jens Behrmann gratefully acknowledges the financial support from the German Science Foundation for RTG 2224 "π 3 : Parameter Identification -Analysis, Algorithms, Applications"</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Estimating the spectral density of large implicit matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaniv</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Ovadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Patton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saunderson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03451</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A learning rule for asynchronous perceptrons with feedback in a combinatorial environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Almeida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE First International Conference on Neural Networks</title>
		<meeting>the IEEE First International Conference on Neural Networks</meeting>
		<imprint>
			<date type="published" when="1987" />
			<biblScope unit="page" from="609" to="618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Particle transport and image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Arvo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Kirk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGGRAPH Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="63" to="66" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Efficient optimization of loops and limits with randomized telescoping sums</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Beatson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Behrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricky</forename><forename type="middle">T Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörn-Henrik</forename><surname>Jacobsen</surname></persName>
		</author>
		<title level="m">Invertible residual networks. International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Bouchard-Côté</surname></persName>
		</author>
		<ptr target="https://www.stat.ubc.ca/bouchard/courses/stat547-fa2018-19//files/assignment1-solution.pdf" />
		<title level="m">Topics in probability assignment 1</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2019" to="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised feature selection for principal components analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Boutsidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petros</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Drineas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 14th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="61" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Reversible architectures for arbitrarily deep residual neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldad</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Ruthotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Begert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elliot</forename><surname>Holtham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Neural ordinary differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Ricky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Nonlinear higher-order statistical decorrelation by volumeconserving neural architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Deco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilfried</forename><surname>Brauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="525" to="535" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">NICE: Non-linear independent components estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.8516</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Density estimation using real nvp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The reversible residual network: Backpropagation without storing activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Aidan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger B</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2214" to="2224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Gouk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eibe</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Cree</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.04368</idno>
		<title level="m">Regularisation of neural networks by enforcing lipschitz continuity</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Ffjord: Free-form continuous dynamics for scalable reversible generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricky</forename><forename type="middle">T Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Stochastic chebyshev gradient descent for spectral optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Insu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haim</forename><surname>Avron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">GANs trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6626" to="6637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Flow++: Improving flowbased generative models with variational dequantization and architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles R Johnson</forename><surname>Horn</surname></persName>
		</author>
		<title level="m">Matrix analysis</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A stochastic estimator of the trace of the influence matrix for Laplacian smoothing splines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hutchinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications in Statistics-Simulation and Computation</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="433" to="450" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörn-Henrik</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Behrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<title level="m">Excessive invariance causes adversarial vulnerability. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">QETLAB: A MATLAB toolbox for quantum entanglement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathaniel</forename><surname>Johnston</surname></persName>
		</author>
		<ptr target="http://qetlab.com" />
		<imprint>
			<date type="published" when="2016-01" />
		</imprint>
	</monogr>
	<note>version 0.9</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Use of different monte carlo sampling techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herman</forename><surname>Kahn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Glow: Generative flow with invertible 1x1 convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Durk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dhariwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="10215" to="10224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Improved variational inference with inverse autoregressive flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Durk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4743" to="4751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kijung</forename><surname>Yoon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.06396</idno>
		<title level="m">Xaq Pitkow, Raquel Urtasun, and Richard Zemel. Reviving and improving recurrent back-propagation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A general method for debiasing a monte carlo estimator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Don</forename><surname>Mcleish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Monte Carlo Methods and Applications</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="301" to="315" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Eric Nalisnick, Akihiro Matsukawa, Yee Whye Teh, Dilan Gorur, and Balaji Lakshminarayanan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Hybrid models with deep and invertible features. International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>International Conference on Learning Representations</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<title level="m">Pixel recurrent neural networks. International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Autoregressive quantile networks for generative modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Munos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.05575</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">The matrix cookbook</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">B</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Pedersen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Generalization of back-propagation to recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><forename type="middle">J</forename><surname>Pineda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review Letters</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="2229" to="2232" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Acceleration of stochastic approximation by averaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Boris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anatoli</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Juditsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.05941</idno>
	</analytic>
	<monogr>
		<title level="m">Prajit Ramachandran, Barret Zoph, and Quoc V Le. Searching for activation functions</title>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Backpropagation for implicit spectral densities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno>abs/1806.00499</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Variational inference with normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1530" to="1538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A new approach to unbiased estimation for sde&apos;s</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chang-Han Rhee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peter W Glynn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Winter Simulation Conference</title>
		<meeting>the Winter Simulation Conference</meeting>
		<imprint>
			<publisher>Winter Simulation Conference</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Unbiased estimation with square root convergence for sde models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chang-Han Rhee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peter W Glynn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations Research</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1026" to="1043" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The eigenvalues of mega-dimensional matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Skilling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Maximum Entropy and Bayesian Methods</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1989" />
			<biblScope unit="page" from="455" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Unbiasing truncated backpropagation through time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Ollivier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.08209</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.01844</idno>
		<title level="m">Aäron van den Oord, and Matthias Bethge. A note on the evaluation of generative models</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Three mechanisms of weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel Aaron Tropp ; Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Grosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
		<respStmt>
			<orgName>University of Texas</orgName>
		</respStmt>
	</monogr>
	<note>PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">For stochastic gradient descent, we used Adam (Kingma and Ba, 2015) with a learning rate of 0.001 and weight decay of 0.0005 applied outside the adaptive learning rate computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
			<publisher>Polyak and Juditsky</publisher>
			<biblScope unit="page">999</biblScope>
		</imprint>
	</monogr>
	<note>We used Polyak averaging</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">For density estimation experiments, we used random horizontal flipping for CIFAR-10, CelebA-HQ 64, and CelebA-HQ 256. For CelebA-HQ 64 and 256, we preprocessed the samples to be 5bit. For hybrid modeling and classification experiments, we used random cropping after reflection padding with 4 pixels for SVHN and CIFAR-10</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Preprocessing</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>CIFAR-10 also included random horizontal flipping</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
