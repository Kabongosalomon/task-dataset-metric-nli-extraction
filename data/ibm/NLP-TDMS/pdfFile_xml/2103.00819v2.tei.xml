<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SANDGLASSET: A LIGHT MULTI-GRANULARITY SELF-ATTENTIVE NETWORK FOR TIME-DOMAIN SPEECH SEPARATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><forename type="middle">W Y</forename><surname>Lam</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Su</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">AI Lab</orgName>
								<address>
									<settlement>Bellevue</settlement>
									<region>Tencent, WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Tencent AI Lab</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SANDGLASSET: A LIGHT MULTI-GRANULARITY SELF-ATTENTIVE NETWORK FOR TIME-DOMAIN SPEECH SEPARATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Speech separation</term>
					<term>multi-granularity</term>
					<term>self- attentive network</term>
					<term>single-channel</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>One of the leading single-channel speech separation (SS) models is based on a TasNet with a dual-path segmentation technique, where the size of each segment remains unchanged throughout all layers. In contrast, our key finding is that multi-granularity features are essential for enhancing contextual modeling and computational efficiency. We introduce a self-attentive network with a novel sandglass-shape, namely Sandglasset, which advances the state-of-the-art (SOTA) SS performance at significantly smaller model size and computational cost. Forward along each block inside Sandglasset, the temporal granularity of the features gradually becomes coarser until reaching half of the network blocks, and then successively turns finer towards the raw signal level. We also unfold that residual connections between features with the same granularity are critical for preserving information after passing through the bottleneck layer. Experiments show our Sandglasset with only 2.3M parameters has achieved the best results on two benchmark SS datasets -WSJ0-2mix and WSJ0-3mix, where the SI-SNRi scores have been improved by absolute 0.8 dB and 2.4 dB, respectively, comparing to the prior SOTA results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Separating a relatively clean speech signal in the presence of multiple speaking voices is a fundamental and crucial problem (a.k.a. "cocktail party problem" <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>) for many downstream speech processing tasks <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref>. In this paper, we focus on the single-channel speech separation (SS) task, which is considerably more challenging than in a multi-channel setting but at the same time applies to broader scenarios, e.g., telephone conversations, many VoIP usage cases, and numerous smartphone applications. The performance of singlechannel SS has been recently advanced by a variety of deep learning methods <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref>. The current leading methods are based on the timedomain audio separation network (TasNet) <ref type="bibr" target="#b5">[6]</ref>, which takes waveform inputs and directly reconstruct sources by computing timedomain loss with utterance-level permutation invariant training (u-PIT) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. In particular, there are many variants of TasNets: the long short-term memory (LSTM) based TasNet <ref type="bibr" target="#b5">[6]</ref>, the Conv-TasNet <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11]</ref>, the dual-path recurrent neural network (DPRNN) <ref type="bibr" target="#b7">[8]</ref>, the dual-path Transformer network (DPTNet) <ref type="bibr" target="#b11">[12]</ref>, the gated DPRNN <ref type="bibr" target="#b12">[13]</ref> and the Wavesplit <ref type="bibr" target="#b13">[14]</ref>.</p><p>Previous works of TasNets have shown that a smaller window for encoding improves the separation performance <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>, leading to much longer sequences, which poses special challenges for modeling long-term global dependencies. To handle the very long sequences, current SOTA methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13</ref>] employ a dual-path segmentation technique, which performs over a whole encoded sequence and divides it into intra-segment and inter-segment sequences, to which we simply refer as local and global sequences. A common strategy in prior works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref> is to use RNNs to model both the local and global sequences. Instead, we find that a self-attentive network (SAN) <ref type="bibr" target="#b14">[15]</ref> would be a better structure to model the global sequence. Given an n-length sequence, in SAN every element can connect to another element using a direct path (i.e., in O(1) time) rather than recursively processing, resetting, and updating memory (i.e., in O(n) time) as in RNNs. Although SAN is notorious for its inefficiency in processing very long sequences due to its inherent quadratic cost, the global sequence length in the dual-path setting becomes feasible for SAN to model.</p><p>Moreover, existing segmentation-based models generally use a fixed segment size unchanged throughout all layers of computation. Our finding is that the modeling capabilities of these networks could not be fully exploited if constantly modeling the global sequences with only one fixed granularity. Especially, time-domain signals essentially have different abstract contexts, e.g., phonemes, syllables, or words, at various granularity levels. Furthermore, SANs have been proven superior for modeling high-level contexts in a number of tasks <ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref>. These together inspire us to design a new neural network architecture, where features are modeled in multigranularity by SANs. Consequently, we propose a novel neural network architecture called Sandglasset, for its sandglass shape and its modest model size and complexity. Forward along each of its blocks, the granularity of the features gradually becomes coarser until reaching half of the network blocks, and then successively turns finer towards the raw signal level. We also unfold that residual connections between features with the same granularity are critical for preserving information after passing through the bottleneck layer.</p><p>Finally, the proposed Sandglasset, which is very light with only 2.3M model parameters, has achieved the SOTA results on two benchmark speech separation datasets -WSJ0-2mix and WSJ0-3mix, where the SI-SNRi scores have been pushed to 20.8 dB and 17.1 dB, surpassing the prior SOTA results by a large margin of absolute 0.8 dB and 2.4 dB. Moreover, compared to the smallest model in literature -DPRNN, our proposed Sandglasset is remarkably lighter with 58.4% less memory and 66.0% fewer floating-point operations. To the best of our knowledge, Sandglasset is the first work that models multi-granularity segments using SANs in signal processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">SANDGLASSET</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Overall Architecture</head><p>Our proposed Sandglasset is composed of N blocks, as presented on the right diagram of <ref type="figure">Fig. 1</ref>  <ref type="figure">Fig. 1</ref>: An illustration of the information flow inside Sandglasset. The left diagram shows the multi-granularity features with variable segment sizes that form a sandglass shape; on the right, it shows the Sandglasset blocks, each of which models a granularity depicted on the left. glasset, where the signal frames are successively down-sampled into shorter feature sequences of larger segments in coarser time scales, i.e., large-granularity, high-level abstract features. Then, the last N/2 blocks constitute a pyramid, where these high-level features are then inversely up-sampled back into longer feature sequences of smaller segments in finer time scales, i.e., fine-granularity, low-level features. To preserve information, the up-sampled features in the last N/2 blocks are aggregated with the earlier computed features with the same granularity using residual connections. This processing is useful for better signal reconstruction as well as avoiding gradient vanishing issues. This sandglass-shape processing strategy is capable of modeling multi-scale temporal granularity to process the input signal hierarchically and progressively, e.g., processing sounds, syllables, and words at different block levels successively. In the remainder of this section, we present the inner machinery of each module in a block as shown in the right diagram of <ref type="figure">Fig. 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Encoding and Segmentation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1.">TasNet Encoder</head><p>First of all, the input signal is a time-domain waveform mixture x ∈ R T . Similar to other TasNet systems <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref>, the input mixture signal is encoded into a sequence of 50%-overlapping frames, denoted bỹ X = [x1, ...,xL] ∈ R M ×L , where M is a hyperparameter that is generally referred to as the window length, and L = 2T /M . In TasNet, we use a ReLU-gated 1D convolutional layer to replace the traditional short-time Fourier transform (STFT) for signal encoding:</p><formula xml:id="formula_0">X = ReLU Conv1D X ; U ,<label>(1)</label></formula><p>where Conv1D(X; U) denotes the 1D convolution operation applied onX parameterized by a learnable weight U ∈ R E×M with 1 × 1 kernels, ReLU(·) is the element-wise rectified linear unit used in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12]</ref> to ensure non-negative outputs, and E is the dimensionality of each encoded frame. Instead of directly usingX for the subsequent computation, we linearly map the matrix into bottleneck features X = BX ∈ R D×L , where B ∈ R D×E and D &lt; E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.">Segmentation Module</head><p>Given a sequence of frames in matrix form X ∈ R D×L , we use a segmentation module to split X into S 50%-overlapping segments, each of length K. The first and last segments are padded with zeros to create S = 2L/K equal-size segments. These segments can be packed together to create a 3D tensor, denoted by X ∈ R D×K×S . Note that the segment size K is a hyperparameter that can be used to control the scale of the locality. The segments X are then passed to a stack of Sandglasset blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Sandglasset Blocks</head><p>For the b-th block, we are given a 3D tensor input X b ∈ R D×K×S , enclosing S segments each containing K frames of D dimensions.</p><p>To make the following recurrence relations mathematically sound, we define X1 = X . As shown in <ref type="figure">Fig. 1</ref>, each Sandglasset block consists of mainly two operations -firstly processing the intra-segment sequence using a recurrent neural network for modeling locality, as in <ref type="bibr" target="#b7">[8]</ref>, and secondly modeling the inter-segment sequence using a SAN to capture the global dependencies. Interleaving with these two modules, a downsampling and an upsampling operation alter the granularity of the global sequence to be processed by the SAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1.">Recurrent Neural Network for Local Sequence Processing</head><p>In our task, intra-segment sequences are the local sequences, each of length K, which contain subtle local details, e.g. temporal or spectral continuity, spectral structure, timbre, etc., which are rather irrelevant to the long-term context. In Sandglasset, we assign the local sequence processing task to a one-layer RNN. Specifically, in each Sandglasset block, the 3D tensor X LR b = X b obtained from the segmentation process is passed to a bi-directional LSTM of H hidden nodes. Here, for ease of reference, we use X LR and Y LR to respectively denote the inputs for the local RNN and the outputs from the local RNN. The superscript LR is used to differentiate from the corresponding input-output pairs in the global SAN model.</p><formula xml:id="formula_1">Y LR b = M b · BiLSTM b X LR b [:, s, :] + c b , s = 1, ..., S ,<label>(2)</label></formula><p>where · is used to denote matrix multiplication, X LR b [:, s, :] ∈ R D×K refers to the local sequence within the s-th chunk, M b ∈ R D×2H and c b ∈ R D are the parameters of a linear transformation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2.">Self-Attentive Network for Multi-Granularity Modeling</head><p>After processing the intra-segment sequences each of length K, we aim at modeling the inter-segment sequences, each of length S. Noted that inter-segment sequences are likely to encode the contextual information of the speech signal. In Sandglasset, we employ a variable-context-aware self-attentive network (SAN) to capture the global dependencies in different time scales.</p><p>Instead of directly taking Y LR b as the input to a SAN, we first apply a layer normalization operation LN(·) to the LR layer's output and add a residual connection to the block input:</p><formula xml:id="formula_2">X GA b = LN Y LR b + X b ,<label>(3)</label></formula><p>which is then re-sampled to modify the time scale for global processing across segments:</p><formula xml:id="formula_3">Y GA b = US b SAN b DS b X GA b<label>(4)</label></formula><p>where US b (·) and DS b (·) are the upsampling and downsampling operations, respectively, which are defined as the follows:</p><formula xml:id="formula_4">US b (X ) = ConvTrans1DK X ; 4 b if b ≤ N/2; ConvTrans1DK X ; 4 N −b−1 if b &gt; N/2,<label>(5)</label></formula><formula xml:id="formula_5">DS b (X ) = Conv1DK X ; 4 b if b ≤ N/2; Conv1DK X ; 4 N −b−1 if b &gt; N/2,<label>(6)</label></formula><p>where Conv1DA(·; B) and ConvTran1DA(·; B) respectively denote the 1D and 1D transposed convolution operations along the axis of length A with a kernel size of B and a stride length of B such that the resultant length becomes A/B (in DS) or AB (in US) long. We also employ the variable-context-aware self-attentive network SAN b (·), which is modified from the pioneering work <ref type="bibr" target="#b14">[15]</ref>. For simplicity, we generally define our SAN for any input X ∈ R D×S×K : SAN(X ) = [SelfAttn (LN (X [:, :, k]) + P) , k = 1, ..., K] , <ref type="bibr" target="#b6">(7)</ref> where P denotes the positional encoding matrix as introduced in <ref type="bibr" target="#b14">[15]</ref>, and X [:, :, k] ∈ R D×S refers to the inter-segment sequence.</p><p>Here, SelfAttn(·) is a typical multi-head self-attention function that linearly projects an input matrix X ∈ R D×S into three forms of matrices, commonly denoted as query Qj, key Kj, and value Vj matrices to compute the scaled dot-product attention for different heads j = 1, ..., J, which are finally combined by a concatenation plus a matrix multiplication:</p><formula xml:id="formula_6">[Qj Kj Aj] = W Q j W K j W V j X + b Q j b K j b V j (8) Aj = Softmax Q j Kj D/J Vj<label>(9)</label></formula><p>A = W · Concat (A1, ..., AJ ) (10) SelfAttn(X) = LN(X + DROP(A))</p><p>where DROP(·) denotes the dropout technique <ref type="bibr" target="#b19">[20]</ref>, and W ∈</p><formula xml:id="formula_8">R D×D , W Q j , W K j , W V j ∈ R D/J×D and b Q j , b K j , b V j ∈ R D/J are the parameters for SAN.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3.">Residual Connections to Prevent Information Loss</head><p>One of the highlights in Sandglasset is to add residual connections between pairs of Sandglasset blocks that are of the same granularity. This technique is used to prevent information loss after passing through the middle blocks, where the granularity is on the coarsest scale. Mathematically, we define</p><formula xml:id="formula_9">X LR b+1 = Y GA b if b ≤ N/2; Y GA b + Y GA b−N/2 if b &gt; N/2,<label>(12)</label></formula><p>which also defines the recurrence relation between the b-th and the (b + 1)-th Sandglasset block. Our experimental result indicates that in practice adding residual connections is critical to remedy raw signal level details for improving signal reconstruction and to avoid gradient vanishing issues for better parameter learning. A seminal work in signal processing -U-Net <ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref> seems a similar idea to ours for re-sampling and combining features at different time scales. Nonetheless, Sandglasset is very different in many aspects: <ref type="bibr" target="#b0">(1)</ref> we have downsampling and upsampling operations together performed in one block; (2) our multi-granularity features are only processed by the SANs within each block; and (3) the residual connections across Sandglasset blocks are purely based on addition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Merge Segments and Decoding</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1.">Mask Estimation</head><p>After passing through N Sandglass blocks, we obtain a 3D tensor output X LR N +1 ∈ R D×K×S , which can be used to estimate masks for C sources. To do so, we first transform the last block's output using a PReLU-gated 2D convolutional layer to obtain a 4D tensor of shape C × E × K × S:</p><formula xml:id="formula_10">Y = Conv2D PReLU X LR N +1 ; C ,<label>(13)</label></formula><p>where Conv2D(Y; C) denotes the 2D convolution operation applied on Y parameterized by a learnable weight C ∈ R CE×D with a 1 × 1 kernel, PReLU(·) is the element-wise parametric ReLU. We then merge the output segments Y using an OverlapAdd 1 approach [8] to match the shape of the mixture framesX ∈ R E×L for masking:</p><formula xml:id="formula_11">M = ReLU (OverlapAdd (Y)) ,<label>(14)</label></formula><p>where is the element-wise product operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2.">Decoder for Waveform Reconstruction</head><p>Finally, the c-th source signal is reconstructed by applying the c-th estimated mask to the initially computed mixture framesX and then using OverlapAdd to merge frames into waveform:</p><formula xml:id="formula_12">sc = OverlapAdd(X Mc).<label>(15)</label></formula><p>Last but not least, given C estimated sources, the scale-invariant source-to-noise ratio (SI-SNR) loss <ref type="bibr" target="#b5">[6]</ref> is used with u-PIT <ref type="bibr" target="#b8">[9]</ref> to learn the network parameters and to solve the permutation problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Data</head><p>To compare with the SOTA speech separation networks, we used two benchmark datasets for evaluation -WSJ0-2mix and WSJ0-3mix <ref type="bibr" target="#b23">[24]</ref>, which are generated from the Wall Street Journal (WSJ0) <ref type="bibr" target="#b24">[25]</ref> dataset by randomly mixing clean utterances from different speakers at a sampling rate of 8 kHz with SNRs between 0 dB and 5 dB. The separation datasets consist of 30 hours of training, 10 hours of validation, and 5 hours of test data from 16 unseen speakers. Both WSJ0-2mix and WSJ0-3mix have been widely used as the benchmark in single-channel speech separation <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Implementation Details</head><p>In our implementation, we used the setting of encoder-decoder modules in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref> and the segmentation module described in <ref type="bibr" target="#b7">[8]</ref>. In particular, we set M = 4, E = 256, and D = 128. For Sandglasset, we used 6 Sandglasset blocks, i.e., N = 6. In the first Sandglasset block, we used an initial segment size K = 256, which would be shortened/prolonged by a factor of 4 in the first/last three blocks, as described in Eq. <ref type="bibr" target="#b4">(5)</ref><ref type="bibr" target="#b5">(6)</ref>. Within each Sandglasset block, we used local Bi-LSTM with 128 hidden units, i.e., H = 128. The global SAN was set to be 8-head, i.e., J = 8 with a 0.1 dropout rate. For training, we used Adam <ref type="bibr" target="#b29">[30]</ref> optimizer with an initial learning rate of 0.001 and a decaying rate of 0.98. The optimization was stopped if no lower validation loss was obtained for 10 consecutive epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3.">Mixing Same-Speaker Utterances as Post Training</head><p>By inspecting the poor separation cases of Sandglasset in WSJ0-2mix, we found that those mixture inputs shared a common characteristic that both speakers have a similar voice timbre, so that the model may keep both voices in the two output signals. This reveals that our model's separation is highly dependent on the voice timbre of different speakers. We conceived that one of the main reasons is that mixture inputs with similar voice timbres are rare in the training set, which makes it hard for our model to learn to differentiate those similar voices. To alleviate this problem, we designed a simple, easy-to-implement post training method for Sandglasset. In particular, after the convergence of the normal training, we expanded the training set by adding dynamically mixed utterances from the same speaker in a 1:1 ratio in sample size relative to the original training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Performance Comparisons</head><p>The SISNRi and SDRi performances of Sandglasset in WSJ0-2mix are reported in <ref type="table" target="#tab_1">Table 1</ref>. First of all, for an ablation study on our proposed multi-granularity (MG) strategy, we trained an ablated baseline system -"Sandglasset (SG)", in which each Sandglasset block uses a single-granularity strategy with a fixed segment size (K = 256). Comparing "Sandglasset (SG)" to "Sandglasset (MG)", we can see a significant drop in SI-SNRi and SDRi scores if Sandglasset was deprived of the multi-granularity mechanism. This asserts our initial expectation that multi-granularity can better exploit SANs for modeling multi-level contexts. For another ablation study, we trained a Sandglasset without residual connections, denoted by "Sandglasset (w/o RES)", which produced a much-degraded performance. We also found that by using the simple post training strategy  Overall, the proposed Sandglasset has achieved the best separation performance with parameters as few as 2.3M, which is the lightest model size that is ever reported for the SS tasks. We would like to emphasize that, to focus on studying the advantage of the network architecture only, we purposely avoid using any speaker information to help further increase the scores of Sandglasset, unlike what has been done in the two most recent systems "Gated DPRNN + Spk ID" <ref type="bibr" target="#b12">[13]</ref> and "Wavesplit + Spk ID" <ref type="bibr" target="#b13">[14]</ref>. Comparing to the strongest reference model regardless of speaker information, Sandglasset has attained an absolute improvement of 0.8 dB SI-SNRi. The WSJ0-3mix result of Sandglasset, as shown in <ref type="table" target="#tab_2">Table 2</ref>, also consistently shows an absolute improvement of 2.4 dB SI-SNRi over the best reference model with no speaker information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Computational Cost Analysis</head><p>Moreover, thanks to some coarser-scale global processing, another merit of Sandglasset is a significant reduction in computational cost, relative to a model that is comparable in size -DPRNN. In <ref type="table" target="#tab_3">Table 3</ref>, we reported the runtime memory and the floating-point operations (FLOPs) 2 which indicates the model efficiency for processing each second of mixture input. Finally, compared to the best performing DPRNN (i.e., 2-sample window), Sandglasset consumed 58.4% less memory and 66.0% fewer FLOPs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSIONS</head><p>This paper proposes a novel sandglass-shape network for timedomain single-channel speech separation, namely Sandglasset. This advanced network architecture combines the advantages of the selfattention networks and the proposed multi-granularity mechanism to hierarchically and progressively model high-level, large-granularity contexts and low-level, fine-granularity details. In our experiment, Sandglasset achieved state-of-the-art results on two benchmark datasets, especially, with the lightest model size that has ever been reported for SS tasks. Comparing to the previous smallest and strongest model in literature, our proposed model is also very light in terms of memory (58.4% less) and computations (66% fewer), which suggests Sandglasset a more economical and practical model for industrial deployment.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Inter-Segment Granularity Sandglasset Residual Connections</head><label></label><figDesc></figDesc><table><row><cell></cell><cell>Mixture Signal Input</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Encoding &amp; Segmentation</cell></row><row><cell></cell><cell></cell><cell>Local RNN</cell><cell>Downsampling</cell><cell>Global SAN</cell><cell>Upsampling</cell></row><row><cell>arXiv:2103.00819v2 [eess.AS] 8 Mar 2021</cell><cell>Separated Signals</cell><cell>Local RNN</cell><cell cols="2">Merge Segments &amp; Decoding Global SAN Downsampling</cell><cell>Upsampling</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">. If information flows from top to</cell></row><row><cell></cell><cell></cell><cell cols="4">bottom, the first N/2 blocks constitute an inverted pyramid in Sand-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison of performances on the WSJ0-2mix test set. The models that exploit speaker IDs as additional information for training and testing are marked with "+ Spk ID". † denotes our estimated model size based on the authors' description.</figDesc><table><row><cell>Model</cell><cell cols="3">Params. SI-SNRi SDRi</cell></row><row><cell>BLSTM-TasNet [6]</cell><cell>23.6M</cell><cell>13.2</cell><cell>13.6</cell></row><row><cell>Conv-TasNet [7]</cell><cell>8.8M</cell><cell>15.3</cell><cell>15.6</cell></row><row><cell>Conv-TasNet + MBT [29]</cell><cell>8.8M</cell><cell>15.5</cell><cell>15.9</cell></row><row><cell>FurcaNeXt [28]</cell><cell>51.4M</cell><cell>18.4</cell><cell>-</cell></row><row><cell>DPRNN [8]</cell><cell>2.6M</cell><cell>18.8</cell><cell>19.1</cell></row><row><cell>DPTNet [12]</cell><cell>2.7M</cell><cell>20.2</cell><cell>20.6</cell></row><row><cell>Sandglasset (w/o RES)</cell><cell>2.3M</cell><cell>20.1</cell><cell>20.3</cell></row><row><cell>Sandglasset (SG)</cell><cell>2.3M</cell><cell>20.3</cell><cell>20.5</cell></row><row><cell>Sandglasset (MG)</cell><cell>2.3M</cell><cell>20.8</cell><cell>21.0</cell></row><row><cell>Sandglasset (MG) + PT</cell><cell>2.3M</cell><cell>21.0</cell><cell>21.2</cell></row><row><cell>Gated DPRNN + Spk ID [13]</cell><cell>7.5M</cell><cell>20.1</cell><cell>-</cell></row><row><cell>Wavesplit + Spk ID [14]</cell><cell>† 42.5M</cell><cell>21.0</cell><cell>21.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison of performances on the WSJ0-3mix test set.</figDesc><table><row><cell>Model</cell><cell cols="3">Params. SI-SNRi SDRi</cell></row><row><cell>Conv-TasNet [7]</cell><cell>8.8M</cell><cell>12.7</cell><cell>13.1</cell></row><row><cell>DPRNN [8]</cell><cell>2.6M</cell><cell>14.7</cell><cell>-</cell></row><row><cell>Sandglasset (MG)</cell><cell>2.3M</cell><cell>17.1</cell><cell>17.4</cell></row><row><cell>Gated DPRNN + Spk ID [13]</cell><cell>7.5M</cell><cell>16.7</cell><cell>-</cell></row><row><cell>Wavesplit + Spk ID [14]</cell><cell>† 42.5M</cell><cell>17.3</cell><cell>17.6</cell></row><row><cell cols="4">described in Section 3.1.3, the performance of Sandglasset can be</cell></row><row><cell>further improved.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison of computational costs.</figDesc><table><row><cell>Model</cell><cell cols="3">Params. Memory (GB) GFLOPs (10 9 )</cell></row><row><cell>DPRNN [8]</cell><cell>2.6M</cell><cell>1.97</cell><cell>84.7</cell></row><row><cell>Sandglasset</cell><cell>2.3M</cell><cell>0.82 (↓58.4%)</cell><cell>28.8 (↓66.0%)</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/tensorflow/tensorflow/blob/r1.12/tensorflow/contrib/ signal/python/ops/reconstruction ops.py</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/sovrasov/flops-counter.pytorch</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Some experiments on the recognition of speech, with one and with two ears</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cherry</forename><surname>E Colin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the acoustical society of America</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="975" to="979" />
			<date type="published" when="1953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The cocktail party problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Haykin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1875" to="1902" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Investigation of speech separation as a front-end for noise robust speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deliang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="826" to="835" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Extract, adapt and recognize: an end-to-end neural network for corrupted monaural speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Max</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xunying</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2778" to="2782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">End-to-end training of time domain audio separation and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Thilo Von Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Kinoshita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Drude</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Boeddeker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomohiro</forename><surname>Delcroix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhold</forename><surname>Nakatani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Haeb-Umbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7004" to="7008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Tasnet: time-domain audio separation network for real-time, single-channel speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
		<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="696" to="700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Conv-tasnet: Surpassing ideal time-frequency magnitude masking for speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">speech, and language processing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1256" to="1266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Dual-path rnn: efficient long sequence modeling for time-domain singlechannel speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuya</forename><surname>Yoshioka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.06379</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Permutation invariant training of deep models for speakerindependent multi-talker speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morten</forename><surname>Kolbaek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Hua</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesper</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
		<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="241" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multitalker speech separation with utterance-level permutation invariant training of deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morten</forename><surname>Kolbaek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Hua</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesper</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morten</forename><surname>Kolbaek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Hua</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesper</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TASLP</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1901" to="1913" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">An empirical evaluation of generic convolutional and recurrent networks for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zico</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01271</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Dual-path transformer network: Direct context-aware modeling for end-to-end monaural speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qirong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.13975</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Voice separation with an unknown number of multiple speakers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eliya</forename><surname>Nachmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Adi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.01531</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Wavesplit: End-toend speech separation by speaker clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Zeghidour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.08933v1</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Disan: Directional self-attention network for rnn/cnn-free language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.04696</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Transformer-xl: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.02860</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Self-attention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning. PMLR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7354" to="7363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Wave-unet: A multi-scale neural network for end-to-end audio source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Stoller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ewert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Dixon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">19th International Society for Music Information Retrieval Conference, ISMIR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Phase-aware single-stage speech denoising and dereverberation with u-net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeong-Seok</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoon</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyogu</forename><surname>Jie Hwan Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.00687</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep clustering: Discriminative embeddings for segmentation and separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuo</forename><surname>John R Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">Le</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
		<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="31" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Continuous speech recognition (csr-i) wall street journal (wsj0) news, complete. linguistic data consortium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Garofalo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Graff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pallett</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<pubPlace>philadelphia</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Divide and conquer: A deep casa approach to talker-independent monaural speaker separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuzhou</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deliang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2092" to="2102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">End-to-end speech separation with unfolded iterative phase reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhong-Qiu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deliang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.10204</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Furcanext: End-to-end monaural speech separation with dynamic gated dilated temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqiang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiqing</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anyan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimedia Modeling</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="653" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Mixupbreakdown: a consistency training method for improving generalization of speech separation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Max</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
