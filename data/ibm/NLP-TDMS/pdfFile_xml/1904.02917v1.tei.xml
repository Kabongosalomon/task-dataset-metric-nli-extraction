<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">3D LiDAR and Stereo Fusion using Stereo Matching Network with Conditional Cost Volume Normalization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsun-Hsuan</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Chieh</roleName><forename type="first">Hou-Ning</forename><surname>Hu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chen</forename><surname>Chiu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Sun</surname></persName>
						</author>
						<title level="a" type="main">3D LiDAR and Stereo Fusion using Stereo Matching Network with Conditional Cost Volume Normalization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The complementary characteristics of active and passive depth sensing techniques motivate the fusion of the Li-DAR sensor and stereo camera for improved depth perception. Instead of directly fusing estimated depths across LiDAR and stereo modalities, we take advantages of the stereo matching network with two enhanced techniques: Input Fusion and Conditional Cost Volume Normalization (CCVNorm) on the LiDAR information. The proposed framework is generic and closely integrated with the cost volume component that is commonly utilized in stereo matching neural networks. We experimentally verify the efficacy and robustness of our method on the KITTI Stereo and Depth Completion datasets, obtaining favorable performance against various fusion strategies. Moreover, we demonstrate that, with a hierarchical extension of CCVNorm, the proposed method brings only slight overhead to the stereo matching network in terms of computation time and model size.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>The accurate 3D perception has been desired since its vital role in numerous tasks of robotics and computer vision, such as autonomous driving, localization and mapping, path planning, and 3D reconstruction. Various techniques have been proposed to obtain depth estimation, ranging from active sensing sensors (e.g., RGB-D cameras and 3D LiDAR scanners) to passive sensing ones (e.g., stereo cameras). We observe that these sensors all have their own pros and cons, in which none of them perform well on all practical scenarios. For instance, RGB-D sensor is confined to its short-range depth acquisition and thereby 3D LiDAR is a common alternative in the challenging outdoor environment. However, 3D LiDARs are much more expensive and only provide sparse 3D depth estimates. In contrast, a stereo camera is able to obtain denser depth map based on stereo matching algorithms but is typically incapable of producing reliable matches in regions with repetitive patterns, homogeneous appearance, or large illumination change.</p><p>Thanks to the complementary characteristic across different sensors, several works <ref type="bibr" target="#b0">[1]</ref> <ref type="bibr" target="#b1">[2]</ref> have studied how to fuse multiple modalities in order to provide more accurate and denser depth estimation. In this paper, we consider the fusion of passive stereo camera and active 3D LiDAR sensor, which is a practical and popular choice. Existing works along this research direction mainly investigate the output-level combination of the dense depth from stereo matching with the sparse measurement from 3D LiDAR. However, rich information provided in stereo images is thus not well utilized in the procedure of fusion. In order to address this issue, we propose to study the design choices for more closely integrating the 3D LiDAR information into the process of stereo matching methods (illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>). The high-level concept of stereo matching pipeline involves 2D feature extraction from the stereo pair, obtaining pixel correspondence, and finally disparity computation. In this paper, we present (1) Input Fusion and (2) Conditional Cost Volume Normalization that are closely integrated with stereo matching networks. By leveraging the complementary nature of LiDAR and stereo modalities, our model produces high-precision disparity estimation.</p><p>The motivation that drives us toward this direction is an observation that typical stereo matching algorithms usually suffer from having ambiguous pixel correspondences across stereo pairs, and thereby 3D LiDAR depth points are able to help reduce the search space of matching and resolve ambiguities. As depth points from 3D LiDAR sensors are sparse, it is not straightforward to simply treat them as additional features connected to each pixel location of a stereo pair during performing stereo matching. Instead, we focus on facilitating sparse points to regularize higher-level feature representations in deep learning-based stereo matching. Recent stateof-the-arts on deep models of stereo matching are composed of two main components: matching cost computation <ref type="bibr" target="#b2">[3]</ref> <ref type="bibr" target="#b3">[4]</ref> and cost volume regularization <ref type="bibr" target="#b4">[5]</ref>[6][7] <ref type="bibr" target="#b7">[8]</ref>, where the former basically extracts the deep representation of image patches and the latter builds up the search space to aggregate all potential matches across stereo images with further regularization (e.g., 3D CNN) for predicting the final depth estimate.</p><p>Being aligned with these two components, we extend the stereo matching network by proposing two techniques: (1) Input Fusion to incorporate the geometric information from sparse LiDAR depth with the RGB images for learning joint feature representations, and (2) CCVNorm (Conditional Cost Volume Normalization) to adaptively regularize cost volume optimization in dependence on LiDAR measurements. It is worth noting that our proposed techniques have little dependency on particular network architectures but only relies on a commonly-utilized cost volume component, thus having more flexibility to be adapted into different models. Extensive experiments are conducted on the KITTI Stereo 2015 Dataset <ref type="bibr" target="#b8">[9]</ref> and the KITTI Depth Completion Dataset <ref type="bibr" target="#b9">[10]</ref> to evaluate the effectiveness of our proposed method. In addition, we perform ablation study on different variants of our approach in terms of performance, model size and computation time. Finally, we analyze how our method exploits the additional sparse sensory inputs and provide qualitative comparisons with other fusion schemes to further highlight the strengths and merits of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head><p>Stereo Matching. Stereo matching has been a fundamental problem in computer vision. In general, a typical stereo matching algorithm can be summarized into a four-stage pipeline <ref type="bibr" target="#b10">[11]</ref>, consisting of matching cost computation, cost support aggregation, cost volume regularization, and disparity refinement. Even when deep learning is introduced to stereo matching in recent years and brings a significant leap in performance of depth estimation, such design paradigm is still widely utilized. For instance, <ref type="bibr" target="#b2">[3]</ref> and <ref type="bibr" target="#b3">[4]</ref> propose to learn a feature representation for matching cost computation by using a deep Siamese network, and then adopt the classical semi-global matching (SGM) <ref type="bibr" target="#b11">[12]</ref> to refine the disparity map. <ref type="bibr" target="#b12">[13]</ref> and <ref type="bibr" target="#b4">[5]</ref> further formulate the entire stereo matching pipeline as an end-to-end network, where the cost volume aggregation and regularization are modelled jointly by 3D convolutions. Moreover, <ref type="bibr" target="#b5">[6]</ref> and <ref type="bibr" target="#b6">[7]</ref> propose several network designs to better exploit multi-scale and context information. Built upon the powerful learning capacity of deep models, this paper aims to integrate LiDAR information into the procedure of stereo matching networks for a more efficient scheme of fusion.</p><p>RGB Imagery and LiDAR Fusion. Sensor fusion of RGB imagery and LiDAR data obtain more attention in virtue of its practicability and performance for depth perception. Two different settings are explored by several prior works: LiDAR fused with a monocular image or stereo ones. As the depth estimation from a single image is typically based on a regression from pixels, which is inherently unreliable and ambiguous, most of the recent monocular-based works aim to achieve the completion on the sparse depth map obtained by LiDAR sensor with the help of rich information from RGB images <ref type="bibr" target="#b13">[14]</ref>[15] <ref type="bibr" target="#b15">[16]</ref> <ref type="bibr" target="#b16">[17]</ref>[18] <ref type="bibr" target="#b18">[19]</ref>, or refine the depth regression by having LiDAR data as a guidance <ref type="bibr" target="#b19">[20]</ref> <ref type="bibr" target="#b20">[21]</ref>.</p><p>On the other hand, since the stereo camera relies on the geometric correspondence across images of different viewing angles, its depth estimates are less ambiguous in terms of the absolute distance between objects in the scene and can be well aligned with the scale of 3D LiDAR measurements. This property of stereo camera makes it a practical choice to be fused with 3D LiDAR data in robotic applications, where the complementary characteristics of passive (stereo) and active (LiDAR) depth sensors are better utilized <ref type="bibr" target="#b21">[22]</ref>[23] <ref type="bibr" target="#b23">[24]</ref>.</p><p>For instance, Maddern et al. <ref type="bibr" target="#b24">[25]</ref> propose a probabilistic framework for fusing LiDAR data with stereo images to generate both the depth and uncertainty estimate. With the power of deep learning, Park et al. <ref type="bibr" target="#b0">[1]</ref> utilize convolutional neural network (CNN) to incorporate sparse LiDAR depth into the estimation from SGM <ref type="bibr" target="#b11">[12]</ref> of stereo matching. However, we argue that the sensor fusion directly applied to the depth outputs is not able to resolve the ambiguous correspondences existing in the procedure of stereo matching. Therefore, in this paper we advance to encode sparse LiDAR depth at earlier stages in stereo matching, i.e., matching cost computation and cost regularization, based on our proposed CCVNorm and Input Fusion techniques.</p><p>Conditional Batch Normalization. While the Batch Normalization layer improves network training via normalizing neural activations according to the statistics of each minibatch, the Conditional Batch Normalization (CBN) operation instead learns to predict the normalization parameters (i.e., feature-wise affine transformation) in dependence on some conditional input. CBN has shown its generability in various application for coordinating different sources of information into joint learning. For instance, <ref type="bibr" target="#b25">[26]</ref> and <ref type="bibr" target="#b26">[27]</ref> utilize CBN to modulate imaging features by a linguistic embedding and successfully prove its efficacy for visual question answering. Perez et al. <ref type="bibr" target="#b27">[28]</ref> further generalize the CBN idea and point out its connections to other conditioning mechanisms, such as concatenation <ref type="bibr" target="#b28">[29]</ref>, gating features <ref type="bibr" target="#b29">[30]</ref>, and hypernetworks <ref type="bibr" target="#b30">[31]</ref>. Lin et al. <ref type="bibr" target="#b31">[32]</ref> introduces CBN to a task of generating patches with spatial coordinates as conditions, which shares similar concept of modulating features by spatial-related information. In our proposed method for the fusion of stereo camera and LiDAR sensor, we adopt the mechanism of CBN to integrate LiDAR data into the cost volume regularization step of the stereo matching framework, not only because of its effectiveness but also the clear motivation on reducing the search space of matching for more reliable disparity estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head><p>As motivated above, we propose to fuse 3D LiDAR data into a stereo matching network by using two techniques: Input Fusion and CCVNorm. In the following, we will first describe the baseline stereo matching network, and then sequentially provide the details of our proposed techniques. Finally, we introduce a hierarchical extension of CCVNorm which is more efficient in terms of runtime and memory consumption. The overview of our proposed method is illustrated in <ref type="figure">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Preliminaries of Stereo Matching Network</head><p>The end-to-end differentiable stereo matching network used in our proposed method, as shown in the bottom part of <ref type="figure">Fig. 2</ref>, is based on the work of GC-Net <ref type="bibr" target="#b4">[5]</ref> and is composed of four primary components which are in line with the typical pipeline of stereo matching algorithms <ref type="bibr" target="#b10">[11]</ref>. First, the deep feature extracted from a rectified left-right stereo pair is learned to compute the cost of stereo matching. The representation with encoded context information acts as a similarity measurement that is more robust than simple photometric appearance, and thus it benefits the estimation of pixel matches across stereo images. A cost volume is then constructed by aggregating the deep features extracted from the left-image with their corresponding ones from the rightimage across each disparity level, where the size of cost volume is 4-dimensional C ×H ×W ×D (i.e., feature size × height × width × disparity). To be detailed, the cost volume actually includes all the potential matches across stereo images and hence serves as a search space of matching. Afterwards, a sequence of 3D convolutional operations (3D-CNN) is applied for cost volume regularization and the final disparity estimation is carried out by regression with respect to the output volume of 3D-CNN along the D dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3D LiDAR and Stereo Fusion Conditional Cost Volume Norm</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Input Fusion</head><p>In the cost computation stage of stereo matching network, both left and right images of a stereo pair are passed through layers of convolutions for extracting features. In order to enrich the representation by jointly reasoning on appearance and geometry information from RGB images and LiDAR data respectively, we propose Input Fusion that simply concatenates stereo images with their corresponding sparse LiDAR depth maps. Different from <ref type="bibr" target="#b13">[14]</ref> that has explored a similar idea, for the setting of stereo and LiDAR fusion, we form the two sparse LiDAR depth maps corresponding to stereo images by reprojecting the LiDAR sweep to both left and right image coordinates with triangulation for converting depth values into disparity ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Conditional Cost Volume Normalization (CCVNorm)</head><p>In addition to Input Fusion, we propose to incorporate information of sparse LiDAR depth points into the cost regularization step (i.e., 3D-CNN) of stereo matching network, learning to reduce the search space of matching and resolve ambiguities. As inspired by Conditional Batch Normalization (CBN) <ref type="bibr" target="#b26">[27]</ref>[26], we propose CCVNorm (Conditional Cost Volume Normalization) to encode the sparse LiDAR information L s into the features of 4D cost volume F of size</p><formula xml:id="formula_0">C × H × W × D. Given a mini-batch B = {F i,·,·,·,· } N i=1 composed of N examples, 3D Batch Normalization (BN)</formula><p>is defined at training time as follows:</p><formula xml:id="formula_1">F BN i,c,h,w,d = γ c F i,c,h,w,d − EB[F·,c,·,·,·] V ar B [F ·,c,·,·,· ] + + β c<label>(1)</label></formula><p>where is a small constant for numerical stability and {γ c , β c } are learnable BN parameters. When it comes to Conditional Batch Normalization, the new BN parameters {γ i,c , β i,c } are defined as functions of conditional information L s i , for modulating the feature maps of cost volume in dependence on the given LiDAR data:</p><formula xml:id="formula_2">γ i,c = g c (L s i ), β i,c = h c (L s i )<label>(2)</label></formula><p>However, directly applying typical CBN to 3D-CNN in stereo matching networks could be problematic due to few considerations: (1) Different from previous works <ref type="bibr" target="#b26">[27]</ref>[26], the conditional input in our setting is a sparse map L s with varying values across pixels, which implies that normalization parameters should be carried out pixel-wisely; (2) An alternative strategy is required to tackle the void information contained in the sparse map L s ; (3) A valid value in L s h,w should contribute differently to each disparity level of the cost volume. Therefore, we introduce CCVNorm (as shown in bottomleft of <ref type="figure">Fig. 3</ref>) which better coordinates the 3D LiDAR information with the nature of cost volume to tackle the aforementioned issues:</p><formula xml:id="formula_3">F CCV N orm i,c,h,w,d = γ i,c,h,w,d F i,c,h,w,d − EB[F·,c,·,·,·] V arB[F·,c,·,·,·] + + β i,c,h,w,d γ i,c,h,w,d = g c,d (L s i,h,w ), if L s i,h,w is valid g c,d , otherwise β i,c,h,w,d = h c,d (L s i,h,w ), if L s i,h,w is valid h c,d , otherwise<label>(3)</label></formula><p>Intuitively, given a LiDAR point L s h,w with a valid value, the representation (i.e., F c,h,w,d ) of its corresponding pixel in the cost volume under a certain disparity level d would be enhanced/suppressed via the conditional modulation when the depth value of L s h,w is consistent/inconsistent with d. In contrast, for those LiDAR points with invalid values, the regularization upon the cost volume degenerates back to a unconditional batch normalization version and the same modulation parameters {g c,d , h c,d } are applied to them. We experiment the following two different choices for modelling the functions g c,d and h c,d : Categorical CCVNorm: aD-entry lookup table with each element as a D × C vector is constructed to map LiDAR values into normalization parameters {γ, β} of different feature channels and disparity levels, where the LiDAR depth values are discretized here intoD levels as entry indexes. Continuous CCVNorm: a CNN is utilized to model the continuous mapping between the sparse LiDAR data L s and the normalization parameters of D × C-channels. In our implementation, we use the first block of ResNet34 <ref type="bibr" target="#b32">[33]</ref> to encode LiDAR data, followed by one 1 × 1 convolution for CCVNorm in different layers respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Hierarchical Extension</head><p>We observe that both Categorical and Continuous CCVNorm require a huge number of parameters. For each normalization layer, the Categorical version demands O(DDC) parameters to build up the lookup table while the CNN for Continuous one even needs more for desirable performance. In order to reduce the model size for practical usage, we advance to propose a hierarchical extension (denoted as HierCCVNorm, which is shown in the top-right of <ref type="figure">Fig. 3</ref>), serving as an approximation of the Categorical CCVNorm with much fewer model parameters. The normalization parameters of HierCCVNorm for valid LiDAR points are computed by:</p><formula xml:id="formula_4">γ i,c,h,w,d = φ g (d)g c (L s i,h,w ) + ψ g (d) β i,c,h,w,d = φ h (d)h c (L s i,h,w ) + ψ h (d)<label>(4)</label></formula><p>Basically, the procedure of mapping from LiDAR disparity to a D×C vector in Categorical CCVNorm is now decomposed into two sequential steps. Take γ for an example, g c is first used to compute the intermediate representation (i.e., a vector in size C) conditioned on L s i,h,w , and is then modulated by another pair of modulation parameters {φ g (d), ψ g (d)} to obtain the final normalization parameter γ. Note that φ g , ψ g , φ h , ψ h are basically the lookup table with the size of D × C. With this hierarchical approximation, each normalization layer only requires O(DC) parameters.</p><formula xml:id="formula_5">Invalid Disparity D+1 1 2 3 … ! D CCVNorm D+1 1 2 3 … ! D 1 2 3 … ! D HierCCVNorm # $ D D % $ &amp;</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS</head><p>We evaluate the proposed method on two KITTI datasets <ref type="bibr" target="#b8">[9]</ref>[10] and show that our framework is able to achieve favorable performance in comparison with several baselines. In addition, we extensively conduct a series of ablation study to sequentially demonstrate the effectiveness of our design choices in the proposed method. Moreover, we investigate the robustness of our approach with respect to the density of LiDAR data, as well as benchmark the runtime and memory consumption. The code and model will be made available for the public.  Evaluation Metric. We adopt standard metrics in stereo matching and depth estimation respectively for the two datasets: (1) On KITTI Stereo <ref type="bibr" target="#b8">[9]</ref>, we follow its development kit to compute the percentage of disparity error that is greater than 1, 2 and 3 pixel(s) away from the ground truth; (2) On KITTI Depth Completion <ref type="bibr" target="#b9">[10]</ref>, Root Mean Square Error (RMSE), Mean Absolute Error (MAE), and their inverse ones (i.e., iRMSE and iMAE) are used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental Settings</head><p>Implementation Details. Our implementation is based on PyTorch and follows the training setting of GC-Net <ref type="bibr" target="#b4">[5]</ref> to have L1 loss for disparity estimation. The optimizer is RMSProp <ref type="bibr" target="#b33">[34]</ref> with a constant learning rate 1 × 10 −3 . The model is trained with batch size of 1 using a randomlycropped 512 × 256 image for 170k iterations. The maximum disparity is set to 192. We apply CCVNorm to the 21, 24, 27, 30, 33, 34, 35th layers in GC-Net. We note that our full model refers to the setting of having both Input Fusion and HierCCVNorm, unless otherwise specified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation on the KITTI Datasets</head><p>For the KITTI Stereo 2015 dataset, we compare our proposed method to several baselines of stereo matching and LiDAR fusion in <ref type="table" target="#tab_2">Table I</ref>. We draw few observations here: 1) Without using any LiDAR data, deep learning-based stereo matching algorithms (i.e., MC-CNN <ref type="bibr" target="#b2">[3]</ref> and GC-Net <ref type="bibr" target="#b4">[5]</ref>) perform better than the conventional one (i.e., SGM <ref type="bibr" target="#b11">[12]</ref>) by a large margin; 2) GC-Net outperforms MC-CNN since its entire stereo matching process is formulated in an end-toend learning framework, and it even performs competitively compared to two other baselines having LiDAR data fused either in input or output spaces (i.e., Probabilistic Fusion <ref type="bibr" target="#b24">[25]</ref>  and Park et al. <ref type="bibr" target="#b0">[1]</ref> respectively). This observation shows the importance of using an end-to-end trainable stereo matching network as well as designing a proper fusion scheme; 3) Our full model learns to well leverage the LiDAR information into both the matching cost computation and cost regularization stages of the stereo matching network and obtains the best accuracy for disparity estimation against all the baselines. In addition to disparity estimation, we compare our model with both monocular depth completion approaches and fusion methods of stereo and LiDAR data on the KITTI Completion dataset in <ref type="table" target="#tab_2">Table II</ref>. From the results of Park et al., we observe that even with more information from stereo pairs, the performance is not guaranteed to be better than state-of-the-art method for monocular depth completion (i.e., NConv-CNN <ref type="bibr" target="#b17">[18]</ref>, Ma et al. <ref type="bibr" target="#b14">[15]</ref>, and FusionNet <ref type="bibr" target="#b18">[19]</ref>) if the stereo images and LiDAR data are not properly integrated. On the contrary, our method with careful designs of the proposed Input Fusion and HierCCVNorm is able to outperform baselines of both monocular or stereo fusion. It is also worth noting that, our model shows significant boost on the metrics related to inverse depth (i.e., iRMSE and iMAE) since our method is trained to predict disparity. Particularly, we emphasize here the importance of the inverse depth metrics, since they demand higher accuracy in the closer region, which are especially suitable for robotic tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation Study</head><p>In <ref type="table" target="#tab_2">Table III</ref>, we show the effectiveness of the proposed components step-by-step. Two additional baselines for fusion are introduced to have more throughout comparison: Feature Concat and Naive CBN. Feature Concat uses a ResNet34 <ref type="bibr" target="#b32">[33]</ref> to encode LiDAR data, as utilized in other depth completion methods <ref type="bibr" target="#b13">[14]</ref> <ref type="bibr" target="#b14">[15]</ref>, and concatenate the LiDAR feature to the cost volume feature. Naive CBN follows a straightforward design of CBN that modulates the cost volume feature conditioned on valid LiDAR depth values.</p><p>Overall Results. First, we find that Input Fusion significantly improves the performance comparing to GC-Net. This highlights the significance of incorporating geometry information in the early matching cost computation (MCC) stage, mentioned in Sec. III-B. Next, in the cost regularization (CR) stage, we compare Feature Concat, Naive CBN, and different variants of our methods. All our CCVNorm variants outperform other mechanisms in fusing the LiDAR information to the cost volume in stereo matching networks. This demonstrates the benefit of applying the proposed CCVNorm scheme which serves as a regularization step on III: Ablation study on the KITTI Depth Completion Dataset. "IF", "Cat", and "Cont" stand for Input Fusion, categorical and continuous variants of CCVNorm, respectively. For different stages, "MCC" stands for Matching Cost Computation and "CR" is Cost Regularization. The bold font indicates top-2 performance.  feature fusion for facilitating stereo matching (Sec. III-C). Finally, our full models with Input Fusion and categorical CCVNorm (with and without the hierarchical extension) produce the best results in the ablation.</p><p>Categorical v.s. Continuous. In addition, we empirically find that the categorical CCVNorm may serve as a better conditioning strategy than the continuous variant. Another interesting discovery is that the categorical variant performs competitively compared to the continuous one in most metrics (for disparity) except for the 1-px error. This is not surprising since the conditioning label for categorical CCVNorm is actually discretized LiDAR data, which may possibly lead to the propagation of quantization error. While the continuous variant performs better in 1-px error, they may not necessarily yield better results in sub-pixel errors (i.e., disparity RMSE and MAE), since cost volume is naturally a discretization in the disparity space, thus making the continuous variant harder to handle sub-pixel predictions <ref type="bibr" target="#b4">[5]</ref>.</p><p>Benefits of Hierarchical CCVNorm. In <ref type="table" target="#tab_2">Table III</ref>  accurate and efficient model. The figure shows that our hierarchical CCVNorm achieves good performance boost with only a small overhead in both computational time and model parameters compared to GC-Net. Note that, Feature Concat adopts a standard strategy to encode LiDAR data in depth completion methods <ref type="bibr" target="#b13">[14]</ref>[15], resulting in much more parameters introduced. Overall, our hierarchical extension can be viewed as an approximation of the original CCVNorm with a huge reduction of computational time and parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Robustness to LiDAR Density</head><p>In <ref type="figure" target="#fig_3">Fig. 5</ref>, we study the robustness of different fusion mechanisms to the change of density in LiDAR data. We use 1.0 on the horizontal axis of <ref type="figure" target="#fig_3">Fig. 5</ref> to indicate a full LiDAR sweep, and gradually sub-sample from it and observe how the performance of each fusion approach varies. The results highlight that both variants of CCVNorm (i.e., Categorical and Hierarchical CCVNorm) are consistently more robust to different density levels in comparison with other baselines (i.e., Feature Concat and Input Fusion only).</p><p>First, Input Fusion is highly sensitive to the density of sparse depth due to its property of treating both valid/invalid pixels equally and setting invalid values as a fixed constant,  <ref type="figure">Fig. 6</ref>: Sensitivity to LiDAR data. We manually modify the sparse disparity input (indicated by the white dashed box in "Modified Sparse Disparity") and observe the effect in disparity estimates. The results show that all our variants better reflect the modification of LiDAR data during the matching process. and hence introducing numerical instability during network training/inference. Second, by comparing our two variants with Feature Concat, we observe that both methods do not suffer from severe performance drop in high LiDAR density (0.7 ∼ 1.0). However, in low-level density (0.1 ∼ 0.6), Feature Concat drastically degrades the performance while ours remains robust to the sub-sampling. This robustness results from our CCVNorm that modulates the pixel-wise feature during the cost regularization stage and introduces additional modulation parameters for invalid pixels (shown in Eq. <ref type="formula" target="#formula_3">(3)</ref>). Overall, this experiment validates that our CCVNorm can function well under varying or non-stable LiDAR density.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Discussions</head><p>Sensitivity to Sensory Inputs. In <ref type="figure">Fig. 6</ref>, we present an example to investigate the sensitivity of different fusion mechanisms with respect to the conditional 3D LiDAR data: we manually modify a certain portion of sparse LiDAR disparity map (indicated by the white dashed box on the third image in the top row of <ref type="figure">Fig. 6</ref>), and visualize the changes in stereo matching outputs produced by this modification (referring to the bottom two rows of <ref type="figure">Fig. 6</ref>).</p><p>Interestingly, using "Input Fusion only" is unaware of the modification in the LiDAR data and produces almost identical output (before v.s. after in <ref type="figure">Fig. 6</ref>). The reason is that fusion solely on the input level is likely to lose the LiDAR information through the procedure of network inference. For "Feature Concat", where the fusion is performed in the later cost regularization stage, the change starts to be visible but not significant. On the contrary, all our variants based on CCVNorm (or having combination with Input Fusion) successfully reflect the modification of the sparse LiDAR data onto the disparity estimation output. Hence, this verifies again our contribution in proposing proper mechanisms for incorporating sparse LiDAR information with dense stereo matching.</p><p>Qualitative Results. <ref type="figure" target="#fig_4">Fig. 7</ref> provides an example to illustrate qualitative comparisons between several baselines and the variants of our proposed method. Our full model (i.e., Input Fusion + hierarchical CCVNorm) is able to handle scenarios with complex structure by taking advantage of the complementary nature of stereo and LiDAR sensors. For instance, as indicated by the white dashed bounding box in <ref type="figure" target="#fig_4">Fig. 7</ref>, GC-Net fails to estimate disparity accurately on the objects containing the deformed shape (e.g., bicycles) in low illumination. In contrast, our method is capable of capturing the details of bicycles in disparity estimation due to the help from the sparse LiDAR data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Computational Time</head><p>We provide an analysis of computational time in <ref type="table" target="#tab_2">Table IV</ref>. Except for Probabilistic Fusion <ref type="bibr" target="#b24">[25]</ref> which is tested on a Core i7 processor and an AMD Radeon R9 295x2 GPU as reported in the original paper, all the other methods run on a machine with a Core i7 processor and an NVIDIA 1080Ti GPU. In general, the models based on stereo matching networks (i.e., GC-Net and ours) take longer for computation but provide significant improvement in performance (see <ref type="table" target="#tab_2">Table I</ref>) in comparison with conventional algorithms. While improving the overall runtime performance via introducing more efficient stereo matching networks is out of the scope of this paper, we show that the overhead introduced by our Input Fusion and CCVNorm mechanisms upon the GC-Net method is only 0.049 seconds, validating the efficiency of our fusion scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>In this paper, built upon deep learning-based stereo matching, we present two techniques for incorporating LiDAR information with stereo matching networks: (1) Input Fusion that jointly reasons about geometry information extracted from LiDAR data in the matching cost computation stage and (2) CCVNorm that conditionally modulates cost volume feature in the cost regularization stage. Furthermore, with the hierarchical extension of CCVNorm, the proposed method only brings marginal overhead to stereo matching networks in runtime and memory consumption. We demonstrate the efficacy of our method on both the KITTI Stereo and Depth Completion datasets. In addition, a series of ablation studies validate our method over different fusion strategies in terms of performance and robustness. We believe that the detailed analysis and discussions provided in this paper could become an important reference for future exploration on the fusion of stereo and LiDAR data. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>+1Fig. 1 :</head><label>1</label><figDesc>Illustration of our method for 3D LiDAR and stereo fusion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>KITTI</head><label></label><figDesc>Stereo 2015 Dataset. KITTI Stereo dataset<ref type="bibr" target="#b8">[9]</ref> is commonly used for evaluating stereo matching algorithms. It contains 200 stereo pairs for each of training and testing set, where the images are in size of 1242 × 375. As the ground truth is only provided for the training set, we follow the identical setting as previous works<ref type="bibr" target="#b24">[25]</ref>[1] to evaluate our model on the training set with LiDAR data. For model training, since only 142 pairs among the training set are associated with LiDAR scans and they cover 29 scenes in the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Error v.s. computation time and model parameters. It demonstrates that our hierarchical CCVNorm achieves comparable performance to the original CCVNorm but with much less overhead in computational time and model parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Robustness to LiDAR density. The 1.0 value in the horizontal axis indicates a complete LiDAR sweep and the shadow indicates the standard deviation. The figure shows that our method is more robust to LiDAR sub-sampling comparing to other baselines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 :</head><label>7</label><figDesc>Qualitative Results. Comparing to other baselines and variants, our method captures details in complex structure area (the white dashed bounding box) by leveraging complementary characteristics of LiDAR and stereo modalities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I :</head><label>I</label><figDesc>Evaluation on the KITTI Stereo 2015 Dataset.</figDesc><table><row><cell>Method</cell><cell>Sparsity</cell><cell>&gt; 3 px</cell><cell>&gt; 2 px</cell><cell>&gt; 1 px</cell></row><row><cell>SGM [12]</cell><cell></cell><cell>20.7</cell><cell>-</cell><cell>-</cell></row><row><cell>MC-CNN [3]</cell><cell>None</cell><cell>6.34</cell><cell>-</cell><cell>-</cell></row><row><cell>GC-Net [5]</cell><cell></cell><cell>4.24</cell><cell>5.82</cell><cell>9.97</cell></row><row><cell>Prob. Fusion [25] Park et al. [1] Ours Full</cell><cell>LiDAR Data</cell><cell>5.91 4.84 3.35</cell><cell>--4.38</cell><cell>--6.79</cell></row><row><cell cols="5">KITTI Completion dataset [10], we hence train our network</cell></row><row><cell cols="5">on the subset of the Completion dataset with images of</cell></row><row><cell cols="5">non-overlapping scenes (i.e., 33k image pairs remained for</cell></row><row><cell>training).</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">KITTI Depth Completion Dataset. KITTI Depth Comple-</cell></row><row><cell cols="5">tion dataset [10] collects semi-dense ground truth of LiDAR</cell></row><row><cell cols="5">depth map by aggregating 11 consecutive LiDAR sweeps</cell></row><row><cell cols="5">together, with roughly 30% pixels annotated. The dataset</cell></row><row><cell cols="5">consists of 43k image pairs for training, 3k for validation, and</cell></row></table><note>1k for testing. Since no ground truth is available in the testing set, we split the validation set into 1k pairs for validation and another 1k pairs for testing that contain non-overlapped scenes with respect to the training set. We also note that the full-resolution images (in size of 1216 × 352) of this dataset are bottom-cropped to 1216×256 because there is no ground truth on the top.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II :</head><label>II</label><figDesc>Evaluation on the KITTI Depth Completion Dataset.</figDesc><table><row><cell>Data</cell><cell>Method</cell><cell>iRMSE</cell><cell>iMAE</cell><cell>RMSE</cell><cell>MAE</cell></row><row><cell></cell><cell>NConv-CNN [18]</cell><cell>2.60</cell><cell>1.03</cell><cell>0.8299</cell><cell>0.2333</cell></row><row><cell>Mono</cell><cell>Ma et al. [15]</cell><cell>2.80</cell><cell>1.21</cell><cell>0.8147</cell><cell>0.2499</cell></row><row><cell></cell><cell>FusionNet [19]</cell><cell>2.19</cell><cell>0.93</cell><cell>0.7728</cell><cell>0.2150</cell></row><row><cell>Stereo</cell><cell>Park et al. [1] Ours Full</cell><cell>3.39 1.40</cell><cell>1.38 0.81</cell><cell>2.0212 0.7493</cell><cell>0.5005 0.2525</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>InputFusion FeatureConcat CCVNorm HierCCVNorm InputFusion +CCVNorm InputFusion +HierCCVNorm Original Sparse Disparity Modified Sparse Disparity Ground Truth RGB Left Before After</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE IV :</head><label>IV</label><figDesc>Computational time (unit: second). Our method only brings small overhead (0.049 seconds) compared to the baseline GC-Net.Method SGM<ref type="bibr" target="#b11">[12]</ref> Prob.<ref type="bibr" target="#b24">[25]</ref> Park. [1] GC-Net<ref type="bibr" target="#b4">[5]</ref> </figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Ours</cell></row><row><cell>Time</cell><cell>0.040</cell><cell>0.024</cell><cell>0.043</cell><cell>0.962</cell><cell>1.011</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">High-precision depth estimation with the 3d lidar and stereo fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Real time dense depth estimation by fusing stereo with sparse depth measurements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Shivakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mohta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pfrommer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<idno>ArXiv:1809.07677</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Computing the stereo matching cost with a convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zbontar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Efficient deep learning for stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">End-to-end learning of geometry and context for deep stereo regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Martirosyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bachrach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cascade residual learning: A two-stage convolutional neural network for stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision Workshops (ICCV Workshops</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pyramid stereo matching network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-R</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deepmvs: Learning multi-view stereopsis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Matzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Object scene flow for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sparsity invariant cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A taxonomy and evaluation of dense two-frame stereo correspondence algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Stereo processing by semiglobal matching and mutual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hirschmuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Sparse-to-dense: Depth prediction from sparse depth samples and a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karaman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Self-supervised sparseto-dense: Self-supervised depth completion from lidar and monocular camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">V</forename><surname>Cavalheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karaman</surname></persName>
		</author>
		<idno>ArXiv:1807.00275</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Sparsity invariant cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Hms-net: Hierarchical multi-scale sparsity-invariant network for sparse depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno>ArXiv:1808.08685</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Confidence propagation through cnns for guided sparse depth regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eldesokey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<idno>ArXiv:1811.01791</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Sparse and noisy lidar completion with rgb guidance and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno>ArXiv:1902.05356</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Depth estimation via affinity learned with convolutional spatial propagation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Plug-and-play: Improve depth estimation via sparse data propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-E</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">AarXiv:1812.08350</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fusion of lidar and stereo range for mobile robots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nickels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Castano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cianci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Advanced Robotics (ICAR)</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Integrating lidar into stereo for fast and improved disparity computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Imaging, Modeling, Processing, Visualization and Transmission (3DIMPVT)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">High-resolution depth maps based on tof-stereo fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Čech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Horaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Real-time probabilistic fusion of sparse 3d lidar and dense stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Maddern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Learning visual reasoning without strong priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">De</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno>ArXiv:1707.03017</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Modulating early visual processing by language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">De</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Film: Visual reasoning with a general conditioning layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">De</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno>ArXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hypernetworks</surname></persName>
		</author>
		<idno>ArXiv:1609.09106</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">COCO-GAN: Conditional coordinate generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-C</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Neural networks for machine learning lecture 6a overview of mini-batch gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
