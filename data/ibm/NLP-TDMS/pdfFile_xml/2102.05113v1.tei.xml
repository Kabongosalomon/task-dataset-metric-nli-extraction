<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Published as a conference paper at ICLR 2021 NEGATIVE DATA AUGMENTATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Sinha</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kumar</forename><surname>Ayush</surname></persName>
							<email>kayush@stanford.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Song</surname></persName>
							<email>tsong@stanford.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burak</forename><surname>Uzkent</surname></persName>
							<email>buzkent@stanford.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxia</forename><surname>Jin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
							<email>ermon@stanford.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<address>
									<country>Samsung Research America</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Published as a conference paper at ICLR 2021 NEGATIVE DATA AUGMENTATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Data augmentation is often used to enlarge datasets with synthetic samples generated in accordance with the underlying data distribution. To enable a wider range of augmentations, we explore negative data augmentation strategies (NDA) that intentionally create out-of-distribution samples. We show that such negative out-of-distribution samples provide information on the support of the data distribution, and can be leveraged for generative modeling and representation learning. We introduce a new GAN training objective where we use NDA as an additional source of synthetic data for the discriminator. We prove that under suitable conditions, optimizing the resulting objective still recovers the true data distribution but can directly bias the generator towards avoiding samples that lack the desired structure. Empirically, models trained with our method achieve improved conditional/unconditional image generation along with improved anomaly detection capabilities. Further, we incorporate the same negative data augmentation strategy in a contrastive learning framework for self-supervised representation learning on images and videos, achieving improved performance on downstream image classification, object detection, and action recognition tasks. These results suggest that prior knowledge on what does not constitute valid data is an effective form of weak supervision across a range of unsupervised learning tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Data augmentation strategies for synthesizing new data in a way that is consistent with an underlying task are extremely effective in both supervised and unsupervised learning <ref type="bibr" target="#b28">(Oord et al., 2018;</ref><ref type="bibr" target="#b38">Zhang et al., 2016;</ref><ref type="bibr" target="#b26">Noroozi &amp; Favaro, 2016;</ref><ref type="bibr" target="#b1">Asano et al., 2019)</ref>. Because they operate at the level of samples, they can be combined with most learning algorithms. They allow for the incorporation of prior knowledge (inductive bias) about properties of typical samples from the underlying data distribution <ref type="bibr" target="#b21">(Jaiswal et al., 2018;</ref><ref type="bibr" target="#b0">Antoniou et al., 2017)</ref>, e.g., by leveraging invariances to produce additional "positive" examples of how a task should be solved.</p><p>To enable users to specify an even wider range of inductive biases, we propose to leverage an alternative and complementary source of prior knowledge that specifies how a task should not be solved. We formalize this intuition by assuming access to a way of generating samples that are guaranteed to be out-of-support for the data distribution, which we call a Negative Data Augmentation (NDA). Intuitively, negative out-of-distribution (OOD) samples can be leveraged as a useful inductive bias because they provide information about the support of the data distribution to be learned by the model. For example, in a density estimation problem we can bias the model to avoid putting any probability mass in regions which we know a-priori should have zero probability. This can be an effective prior if the negative samples cover a sufficiently large area. The best NDA candidates are ones that expose common pitfalls of existing models, such as prioritizing local structure over global Furthermore, we propose a way of leveraging NDA for unsupervised representation learning. We propose a new contrastive predictive coding <ref type="bibr" target="#b15">(He et al., 2019;</ref> (CPC) objective that encourages the distribution of representations corresponding to in-support data to become disjoint from that of NDA data. Empirically, we show that applying NDA with our proposed transformations (e.g., forcing the representation of normal and jigsaw images to be disjoint) improves performance in downstream tasks.</p><p>With appropriately chosen NDA strategies, we obtain superior empirical performance on a variety of tasks, with almost no cost in computation. For generative modeling, models trained with NDA achieve better image generation, image translation and anomaly detection performance compared with the same model trained without NDA. Similar gains are observed on representation learning for images and videos over downstream tasks such as image classification, object detection and action recognition. These results suggest that NDA has much potential to improve a variety of self-supervised learning techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">NEGATIVE DATA AUGMENTATION</head><p>The input to most learning algorithms is a dataset of samples from an underlying data distribution p data . While p data is unknown, learning algorithms always rely on prior knowledge about its properties (inductive biases <ref type="bibr" target="#b35">(Wolpert &amp; Macready, 1997)</ref>), e.g., by using specific functional forms such as neural networks. Similarly, data augmentation strategies exploit known invariances of p data , such as the conditional label distribution being invariant to semantic-preserving transformations.</p><p>While typical data augmentation strategies exploit prior knowledge about what is in support of p data , in this paper, we propose to exploit prior knowledge about what is not in the support of p data . This information is often available for common data modalities (e.g., natural images and videos) and is under-exploited by existing approaches. Specifically, we assume: (1) there exists an alternative distribution p such that its support is disjoint from that of p data ; and (2) access to a procedure to efficiently sample from p. We emphasize p need not be explicitly defined (e.g., through an explicit density) -it may be implicitly defined by a dataset or by a procedure that transforms samples from p data into ones from p by suitably altering their structure. Analogous to typical data augmentations, NDA strategies are by definition domain and task specific. In this paper, we focus on natural images and videos, and leave the application to other domains (such as natural language processing) as future work. How do we select a good NDA strategy? According to the manifold hypothesis <ref type="bibr" target="#b9">(Fefferman et al., 2016)</ref>, natural images lie on low-dimensional manifolds: p data is supported on a low-dimensional manifold of the ambient (pixel) space. This suggests that many negative data augmentation strategies exist. Indeed, sampling random noise is in most cases a valid NDA. However, while this prior is generic, it is not very informative, and this NDA will likely be ineffective for most learning problems. Intuitively, NDA is informative if its support is close (in a suitable metric) to that of p data , while being disjoint. These negative samples will provide information on the "boundary" of the support of p data , which we will show is helpful in several learning problems. In most of our tasks, the images are processed by convolutional neural networks (CNNs) that are good at processing local features but not necessarily global features <ref type="bibr" target="#b10">(Geirhos et al., 2018)</ref>. Therefore, we may consider NDA examples to be ones that preserve local features ("informative") and break global features, so that it forces the CNNs to learn global features (by realizing NDAs are different from real data).</p><p>Leveraging this intuition, we show several image transformations from the literature that can be viewed as generic NDAs over natural images in <ref type="figure" target="#fig_1">Figure 2</ref>, that we will use for generative modeling and representation learning in the following sections. Details about these transformations can be found in Appendix B. <ref type="figure">Figure 3</ref>: Schematic overview of our NDA framework. Left: In the absence of NDA, the support of a generative model P θ (blue oval) learned from samples (green dots) may "over-generalize" and include samples from P 1 or P 2 . Right: With NDA, the learned distribution P θ becomes disjoint from NDA distributions P 1 and P 2 , thus pushing P θ closer to the true data distribution p data (green oval). As long as the prior is consistent, i.e. the supports of P 1 and P 2 are truly disjoint from p data , the best fit distribution in the infinite data regime does not change.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">NDA FOR GENERATIVE ADVERSARIAL NETWORKS</head><p>In GANs, we are interested in learning a generative model G θ from samples drawn from some data distribution p data <ref type="bibr" target="#b11">(Goodfellow et al., 2014)</ref>. GANs use a binary classifier, the so-called discriminator D φ , to distinguish real data from generated (fake) samples. The generator G θ is trained via the following mini-max objective that performs variational Jensen-Shannon divergence minimization:</p><formula xml:id="formula_0">min G θ ∈P(X ) max D φ L JS (G θ , D φ ) where (1) L JS (G θ , D φ ) = E x∼p data [log(D φ (x))] + E x∼G θ [log(1 − D φ (x))]<label>(2)</label></formula><p>This is a special case to the more general variational f -divergence minimization objective <ref type="bibr" target="#b27">(Nowozin et al., 2016)</ref>. The optimal D φ for any G θ is (p data /G θ )/(1 + p data /G θ ), so the discriminator can serve as a density ratio estimator between p data and G θ .</p><p>With sufficiently expressive models and infinite capacity, G θ will match p data . In practice, however, we have access to finite datasets and limited model capacity. This means that the generator needs to generalize beyond the empirical distribution, which is challenging because the number of possible discrete distributions scale doubly exponentially w.r.t. to the data dimension. Hence, as studied in , the role of the inductive bias is critical. For example,  report that when trained on images containing 2 objects only, GANs and other generative models can sometimes "generalize" by generating images with 1 or 3 objects (which were never seen in the training set). The generalization behavior -which may or may not be desirable -is determined by factors such as network architectures, hyperparameters, etc., and is difficult to characterize analytically.</p><p>Here we propose to bias the learning process by directly specifying what the generator should not generate through NDA. We consider an adversarial game based on the following objective:</p><formula xml:id="formula_1">min G θ ∈P(X ) max D φ L JS (λG θ + (1 − λ)P , D φ )<label>(3)</label></formula><p>where the negative samples are generated from a mixture of G θ (the generator distribution) and P (the NDA distribution); the mixture weights are controlled by the hyperparameter λ. Intuitively, this can help addresses the above "over-generalization" issue, as we can directly provide supervision on what should not be generated and thus guide the support of G θ (see <ref type="figure">Figure 3</ref>) . For instance, in the object count example above, we can empirically prevent the model from generating images with an undesired number of objects (see Appendix Section A for experimental results on this task).</p><p>In addition, the introduction of NDA samples will not affect the solution of the original GAN objective in the limit. In the following theorem, we show that given infinite training data and infinite capacity discriminators and generators, using NDA will not affect the optimal solution to the generator, i.e. the generator will still recover the true data distribution. Theorem 1. Let P ∈ P(X ) be any distribution over X with disjoint support than p data , i.e., such that supp(p data ) ∩ supp(P ) = ∅. Let D φ : X → R be the set of all discriminators over X , f : R ≥0 → R be a convex, semi-continuous function such that f (1) = 0, f be the convex conjugate of f , f its derivative, and G θ be a distribution with sample space X . Then ∀λ ∈ (0, 1], we have:</p><formula xml:id="formula_2">arg min G θ ∈P(X ) max D φ :X →R L f (G θ , D φ ) = arg min G θ ∈P(X ) max D φ :X →R L f (λG θ + (1 − λ)P , D φ ) = p data (4) where L f (Q, D φ ) = E x∼p data [D φ (x)] − E x∼Q [f (D φ (x))]</formula><p>is the objective for f -GAN <ref type="bibr" target="#b27">(Nowozin et al., 2016)</ref>. However, the optimal discriminators are different for the two objectives:</p><p>arg max</p><formula xml:id="formula_3">D φ :X →R L f (G θ , D φ ) = f (p data /G θ ) (5) arg max D φ :X →R L f (λG θ + (1 − λ)P , D φ ) = f (p data /(λG θ + (1 − λ)P ))<label>(6)</label></formula><p>Proof. See Appendix C.</p><p>The above theorem shows that in the limit of infinite data and computation, adding NDA changes the optimal discriminator solution but not the optimal generator. In practice, when dealing with finite data, existing regularization techniques such as weight decay and spectral normalization <ref type="bibr" target="#b23">(Miyato et al., 2018)</ref> allow potentially many solutions that achieve the same objective value. The introduction of NDA samples allows us to filter out certain solutions by providing additional inductive bias through OOD samples. In fact, the optimal discriminator will reflect the density ratio between p data and λG θ + (1 − λ)P (see Eq. <ref type="formula" target="#formula_3">(6)</ref>), and its values will be higher for samples from p data compared to those from P . As we will show in Section 5, a discriminator trained with this objective and suitable NDA performs better than relevant baselines for other downstream tasks such as anomaly detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">NDA FOR CONSTRASTIVE REPRESENTATION LEARNING</head><p>Using a classifier to estimate a density ratio is useful not only for estimating f -divergences (as in the previous section) but also for estimating mutual information between two random variables. In representation learning, mutual information (MI) maximization is often employed to learn compact yet useful representations of the data, allowing one to perform downstream tasks efficiently <ref type="bibr" target="#b34">(Tishby &amp; Zaslavsky, 2015;</ref><ref type="bibr" target="#b25">Nguyen et al., 2008;</ref><ref type="bibr" target="#b30">Poole et al., 2019b;</ref><ref type="bibr" target="#b28">Oord et al., 2018)</ref>. Here, we show that NDA samples are also beneficial for representation learning.</p><p>In contrastive representation learning (such as CPC (Oord et al., 2018)), the goal is to learn a mapping h θ (x) : X → P(Z) that maps a datapoint x to some distribution over the representation space Z; once the network h θ is learned, representations are obtained by sampling from z ∼ h θ (x). CPC maximizes the following objective:</p><formula xml:id="formula_4">I CPC (h θ , g φ ) := E x∼p data (x),z∼h θ (x), zi∼p θ (z) log ng φ (x, z) g φ (x, z) + n−1 j=1 g φ (x, z j )<label>(7)</label></formula><p>where p θ (z) = h θ (z|x)p data (x)dx is the marginal distribution of the representations associated with p data . Intuitively, the CPC objective involves an n-class classification problem where g φ attempts to identify a matching pair (i.e. (x, z)) sampled from the joint distribution from the (n − 1) non-matching pairs (i.e. (x, z j )) sampled from the product of marginals distribution. Note that g φ plays the role of a discriminator/critic, and is implicitly estimating a density ratio. As n → ∞, the optimal g φ corresponds to an un-normalized density ratio between the joint distribution and the product of marginals, and the CPC objective matches its upper bound which is the mutual information between X and Z <ref type="bibr" target="#b29">(Poole et al., 2019a;</ref><ref type="bibr" target="#b32">Song &amp; Ermon, 2019)</ref>. However, this objective is no longer able to control the representations for data that are out of support of p data , so there is a risk that the representations are similar between p data samples and out-of-distribution ones.</p><p>To mitigate this issue, we propose to use NDA in the CPC objective, where we additionally introduce a batch of NDA samples, for each positive sample:</p><formula xml:id="formula_5">I CPC (h θ , g φ ) := E log (n + m)g φ (x, z) g φ (x, z) + n−1 j=1 g φ (x, z j ) + m k=1 g φ (x, z k )<label>(8)</label></formula><p>where the expectation is taken over</p><formula xml:id="formula_6">x ∼ p data (x), z ∼ h θ (x), z i ∼ p θ (z), x k ∼ p (NDA dis- tribution), z k ∼ h θ (x k ) for all k ∈ [m].</formula><p>Here, the behavior of h θ (x) when x is NDA is optimized explicitly, allowing us to impose additional constraints to the NDA representations. This corresponds to a more challenging classification problem (compared to basic CPC) that encourages learning more informative representations. In the following theorem, we show that the proposed objective encourages the representations for NDA samples to become disjoint from the representations for p data samples, i.e. NDA samples and p data samples do not map to the same representation. Theorem 2. (Informal) The optimal solution to h θ in the NDA-CPC objective maps the representations of data samples and NDA samples to disjoint regions.</p><p>Proof. See Appendix D for a detailed statement and proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">NDA-GAN EXPERIMENTS</head><p>In this section we report experiments with different types of NDA for image generation. Additional details about the network architectures and hyperparameters can be found in Appendix J. Unconditional Image Generation. We conduct experiments on various datasets using the BigGAN architecture <ref type="bibr" target="#b3">(Brock et al., 2018)</ref> for unconditional image generation 1 . We first explore various image transformations from the literature to evaluate which ones are effective as NDA. For each transformation, we evaluate its performance as NDA (training as in Eq. 3) and as a traditional data augmentation strategy, where we enlarge the training set by applying the transformation to real images (denoted PDA for positive data augmentation). <ref type="table" target="#tab_0">Table 1</ref> shows the FID scores for different types of transformations as PDA/NDA. The results suggest that transformations that spatially corrupt the image are strong NDA candidates. It can be seen that Random Horizontal Flip is not effective as an NDA; this is because flipping does not spatially corrupt the image but is rather a semantic preserving transformation, hence the NDA distribution P is not disjoint from p data . On the contrary, it is reasonable to assume that if an image is likely under p data , its flipped variant should also be likely. This is confirmed by the effectiveness of this strategy as PDA.  We believe spatially corrupted negatives perform well as NDA in that they push the discriminator to focus on global features instead of local ones (e.g., texture). We confirm this by plotting the histogram of differences in the discriminator output for a real image and it's Jigsaw version as shown in <ref type="figure" target="#fig_2">Fig. 4</ref>. We show that the difference is (a) centered close to zero for normal BigGAN (so without NDA training, the discriminator cannot distinguish real and Jigsaw samples well), and (b) centered at a positive number (logit 10) for our method (NDA-BigGAN). Following our findings, in our remaining experiments we use Jigsaw, Cutout, Stitch, Mixup and Cutmix as they achieve significant improvements when used as NDA for unconditional image generation on CIFAR-10. <ref type="table" target="#tab_1">Table 2</ref> shows the FID scores for BigGAN when trained with five types of negative data augmentation on four different benchmarks. Almost all the NDA augmentations improve the baseline across datasets. For all the datasets except CIFAR-100, λ = 0.25, whereas for CIFAR-100 it is 0.5. We show the effect of λ on CIFAR-10 performance in Appendix G. We additionally performed an experiment using a mixture of augmentation policy. The results <ref type="bibr">(FID 16.24)</ref> were better than the baseline method (18.64) but not as good as using a single strategy.</p><p>Conditional Image Generation. We also investigate the benefits of NDA in conditional image generation using BigGAN. The results are shown in <ref type="table" target="#tab_2">Table 3</ref>. In this setting as well, NDA gives a significant boost over the baseline model. We again use λ = 0.25 for CIFAR-10 and λ = 0.5 for CIFAR-100. For both unconditional and conditional setups we find the Jigsaw and Stitching augmentations to achieve a better FID score than the other augmentations.  <ref type="table" target="#tab_3">Table 4</ref> shows the quantitative gains obtained by using Jigsaw NDA 3 while <ref type="figure" target="#fig_5">Figure 7</ref> in Appendix E highlights the qualitative improvements. The NDA-Pix2Pix model avoids noisy segmentation on objects including buildings and trees.  Anomaly Detection. As another added benefit of NDA for GANs, we utilize the output scores of the BigGAN discriminator for anomaly detection. We experiment with 2 different types of OOD datasets. The first set consists of SVHN <ref type="bibr" target="#b24">(Netzer et al., 2011)</ref>, DTD <ref type="bibr" target="#b4">(Cimpoi et al., 2014)</ref>, <ref type="bibr">Places-365 (Zhou et al., 2017)</ref>, TinyImageNet, and CIFAR-100 as the OOD datapoints following the protocol in <ref type="bibr" target="#b8">(Du &amp; Mordatch, 2019;</ref>. We train BigGAN w/ and w/o Jigsaw NDA on the train set of CIFAR-10 and then use the output value of discriminator to classify the test set of CIFAR-10 (not anomalous) and different OOD datapoints (anomalous) as anomalous or not.</p><p>We use the AUROC metric as proposed in <ref type="bibr" target="#b17">(Hendrycks &amp; Gimpel, 2016)</ref> to evaluate the anomaly detection performance. <ref type="table" target="#tab_4">Table 5</ref> compares the performance of NDA with a likelihood based model (Energy Based Models (EBM <ref type="bibr" target="#b8">(Du &amp; Mordatch, 2019)</ref>). Results show that Jigsaw NDA performs much better than baseline BigGAN and other generative models. We did not include other NDAs as Jigsaw achieved the best results.</p><p>We consider the extreme corruptions in CIFAR-10-C  as the second set of OOD datasets. It consists of 19 different corruptions, each having 5 different levels of severity. We only consider the corruption of highest severity for our experiment, as these constitute a significant shift from the true data distribution. Averaged over all the 19 different corruptions, the AUROC score for the normal BigGAN is 0.56, whereas the BigGAN trained with Jigsaw NDA achieves 0.63. The histogram of difference in discriminator's output for clean and OOD samples are shown in <ref type="figure">Figure 8</ref> in the appendix. High difference values imply that the Jigsaw NDA is better at distinguishing OOD samples than the normal BigGAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">REPRESENTATION LEARNING USING CONTRASTIVE LOSS AND NDA</head><p>Unsupervised Learning on Images. In this section, we perform experiments on three benchmarks: (a) CIFAR10 (C10), (b) CIFAR100 (C100), and (c) ImageNet-100 <ref type="bibr" target="#b6">(Deng et al., 2009)</ref> to show the benefits of NDA on representation learning with the contrastive loss function. In our experiments, we use the momentum contrast method <ref type="bibr" target="#b15">(He et al., 2019)</ref>, MoCo-V2, as it is currently the state-of-theart model on unsupervised learning on ImageNet. For C10 and C100, we train the MoCo-V2 model for unsupervised learning (w/ and w/o NDA) for 1000 epochs. On the other hand, for ImageNet-100, we train the MoCo-V2 model (w/ and w/o NDA) for 200 epochs. Additional hyperparameter details can be found in the appendix. To evaluate the representations, we train a linear classifier on the representations on the same dataset with labels. <ref type="table" target="#tab_5">Table 6</ref> shows the top-1 accuracy of the classifier. We find that across all the three datasets, different NDA approaches outperform MoCo-V2. While Cutout NDA performs the best for C10, the best performing NDA for C100 and ImageNet-100 are Jigsaw and Mixup respectively. <ref type="figure" target="#fig_6">Figure 9</ref> compares the cosine distance of the representations learned w/ and w/o NDA (jigsaw) and shows that jigsaw and normal images are projected far apart from each other when trained using NDA whereas with original MoCo-v2 they are projected close to each other.</p><p>Transfer Learning for Object Detection. We transfer the network pre-trained over ImageNet-100 for the task of Pascal-VOC object detection using a Faster R-CNN detector (C4 backbone) Ren et al. <ref type="bibr">(2015)</ref>. We fine-tune the network on Pascal VOC 2007+2012 trainval set and test it on the 2007 test Unsupervised Learning on Videos. In this section, we investigate the benefits of NDA in selfsupervised learning of spatio-temporal embeddings from video, suitable for human action recognition. We apply NDA to Dense Predictive Coding , which is a single stream (RGB only) method for self-supervised representation learning on videos. For videos, we create NDA samples by performing the same transformation on all frames of the video (e.g. the same jigsaw permutation is applied to all the frames of a video). We evaluate the approach by first training the DPC model with NDA on a large-scale dataset (UCF101), and then evaluate the representations by training a supervised action classifier on UCF101 and HMDB51 datasets. As shown in <ref type="table" target="#tab_6">Table 7</ref>, Jigsaw and Cutmix NDA improve downstream task accuracy on UCF-101 and HMDB-51, achieving new state-of-the-art performance among single stream (RGB only) methods for self-supervised representation learning (when pre-trained using UCF-101). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">RELATED WORK</head><p>In several machine learning settings, negative samples are produced from a statistical generative model. <ref type="bibr" target="#b33">Sung et al. (2019)</ref> aim to generate negative data using GANs for semi-supervised learning and novelty detection while we are concerned with efficiently creating negative data to improve generative models and self-supervised representation learning. <ref type="bibr" target="#b14">Hanneke et al. (2018)</ref> also propose an alternative theoretical framework that relies on access to an oracle which classifies a sample as valid or not, but do not provide any practical implementation. <ref type="bibr" target="#b2">Bose et al. (2018)</ref> use adversarial training to generate hard negatives that fool the discriminator for NLP tasks whereas we obtain NDA data from positive data to improve image generation and representation learning. <ref type="bibr" target="#b19">Hou et al. (2018)</ref> use a GAN to learn the negative data distribution with the aim of classifying positive-unlabeled (PU) data whereas we do not have access to a mixture data but rather generate negatives by transforming the positive data.</p><p>In contrastive unsupervised learning, common negative examples are ones that are assumed to be further than the positive samples semantically. Word2Vec <ref type="bibr" target="#b22">(Mikolov et al., 2013)</ref> considers negative samples to be ones from a different context and CPC-based methods (Oord et al., 2018) such as momentum contrast <ref type="bibr" target="#b15">(He et al., 2019)</ref>, the negative samples are data augmentations from a different image. Our work considers a new aspect of "negative samples" that are neither generated from some model, nor samples from the data distribution. Instead, by applying negative data augmentation (NDA) to existing samples, we are able to incorporate useful inductive biases that might be difficult to capture otherwise .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>We proposed negative data augmentation as a method to incorporate prior knowledge through out-ofdistribution (OOD) samples. NDAs are complementary to traditional data augmentation strategies, which are typically focused on in-distribution samples. Using the NDA framework, we interpret existing image transformations (e.g., jigsaw) as producing OOD samples and develop new learning algorithms to leverage them. Owing to rigorous mathematical characterization of the NDA assumption, we are able to theoretically analyze their properties. As an example, we bias the generator of a GAN to avoid the support of negative samples, improving results on conditional/unconditional image generation tasks. Finally, we leverage NDA for unsupervised representation learning in images and videos. By integrating NDA into MoCo-v2 and DPC, we improve results on image and action recognition on CIFAR10, CIFAR100, ImageNet-100, UCF-101, and HMDB-51 datasets. Future work include exploring other augmentation strategies as well as NDAs for other modalities.</p><p>Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10 million image database for scene recognition. IEEE transactions on pattern analysis and machine intelligence, 40(6):1452-1464, 2017.</p><p>A NUMEROSITY CONTAINMENT  systematically investigate generalization in deep generative models using two different datasets: (a) a toy dataset where there are k non-overlapping dots (with random color and location) in the image (see <ref type="figure">Figure 5a</ref>), and (b) the CLEVR dataset where ther are k objects (with random shape, color, location, and size) in the images (see <ref type="figure">Figure 5b</ref>). They train a GAN model (WGAN-GP <ref type="bibr" target="#b12">Gulrajani et al. (2017)</ref>) with (either) dataset and observe that the learned distribution does not produce the same number of objects as in the dataset it was trained on. The distribution of the numerosity in the generated images is centered at the numerosity from the dataset, with a slightbias towards over-estimation. For, example when trained on images with six dots, the generated images contain anywhere from two to eight dots (see <ref type="figure" target="#fig_3">Figure 6a</ref>). The observation is similar when trained on images with two CLEVR objects. The generated images contain anywhere from one to three dots (see <ref type="figure" target="#fig_3">Figure 6b</ref>).</p><p>In order to remove samples with numerosity different from the train dataset, we use such samples as negative data during training. For example, while training on images with six dots we use images with four, five and seven dots as negative data for the GAN. The resulting distribution of the numerosity in the generated images is constrained to six. We observe similar behaviour when training a GAN with images containing two CLEVR objects as positive data and images with one or three objects as negative data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B IMAGE TRANSFORMATIONS</head><p>Given an image of size H × W , the different image transformations that we used are described below.</p><p>Jigsaw-K <ref type="bibr" target="#b26">(Noroozi &amp; Favaro, 2016)</ref> We partition the image into a grid of K × K patches of size (H/K) × (W/K), indexed by [1, . . . , K × K]. Then we shuffle the image patches according to a random permutation (different from the original order) to produce the NDA image. Empirically, we find K = 2 to work the best for Jigsaw-K NDA. Stitching We stitch two equal-sized patches of two different images, either horizontally (H/2×W ) or vertically (H × W/2), chosen uniformly at random, to produce the NDA image. Generating CLEVR is harder so we explore only one, but the behaviour with NDA is similar to dots.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cutout / Cutmix</head><p>We select a random patch in the image with its height and width lying between one-third and one-half of the image height and width respectively. To construct NDA images, this patch is replaced with the mean pixel value of the patch (like cutout <ref type="bibr" target="#b7">(DeVries &amp; Taylor, 2017)</ref> with the only difference that they use zero-masking), or the pixel values of another image at the same location (cutmix <ref type="bibr" target="#b36">(Yun et al., 2019)</ref>).</p><p>Mixup-α NDA image is constructed from a linear interpolation between two images x and y <ref type="bibr" target="#b37">(Zhang et al., 2017)</ref>, γx + (1 − γ)y; γ ∼ Beta(α, α). α is chosen so that the distribution has high density at 0.5.</p><p>Other classes NDA images are sampled from other classes in the same dataset. See Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C NDA FOR GANS</head><p>Theorem 1. Let P ∈ P(X ) be any distribution over X with disjoint support than p data , i.e., such that supp(p data ) ∩ supp(P ) = ∅. Let D φ : X → R be the set of all discriminators over X , f : R ≥0 → R be a convex, semi-continuous function such that f (1) = 0, f be the convex conjugate of f , f its derivative, and G θ be a distribution with sample space X . Then ∀λ ∈ (0, 1], we have:</p><formula xml:id="formula_7">arg min G θ ∈P(X ) max D φ :X →R L f (G θ , D φ ) = arg min G θ ∈P(X ) max D φ :X →R L f (λG θ + (1 − λ)P , D φ ) = p data (4) where L f (Q, D φ ) = E x∼p data [D φ (x)] − E x∼Q [f (D φ (x))]</formula><p>is the objective for f -GAN <ref type="bibr" target="#b27">(Nowozin et al., 2016)</ref>. However, the optimal discriminators are different for the two objectives:</p><p>arg max</p><formula xml:id="formula_8">D φ :X →R L f (G θ , D φ ) = f (p data /G θ ) (5) arg max D φ :X →R L f (λG θ + (1 − λ)P , D φ ) = f (p data /(λG θ + (1 − λ)P ))<label>(6)</label></formula><p>Proof. Let us use p(x), p(x), q(x) to denote the density functions of p data , P and G θ respectively (and P , P , Q for the respective distributions). First, from Lemma 1 in <ref type="bibr" target="#b25">Nguyen et al. (2008)</ref>, we have that</p><formula xml:id="formula_9">max D φ :X →R L f (G θ , D φ ) = D f (P G θ ) (9) max D φ :X →R L f (λG θ + (1 − λ)P , D φ ) = D f (P λQ + (1 − λ)P )<label>(10)</label></formula><p>where D f refers to the f -divergence. Then, we have</p><formula xml:id="formula_10">D f (P ||λQ + (1 − λ)P ) = X (λq(x) + (1 − λ)p(x)) f p(x) λq(x) + (1 − λ)p(x) = X λq(x)f p(x) λq(x) + (1 − λ)p(x) + (1 − λ)f (0) ≥λf X q(x) p(x) λq(x) + (1 − λ)p(x) + (1 − λ)f (0) (11) =λf 1 λ X λq(x) p(x) λq(x) + (1 − λ)p(x) + (1 − λ)f (0) =λf 1 λ X (λq(x) + (1 − λ)p(x) − (1 − λ)p(x)) p(x) λq(x) + (1 − λ)p(x) + (1 − λ)f (0) =λf 1 λ − X ((1 − λ)p(x)) p(x) λq(x) + (1 − λ)p(x) + (1 − λ)f (0) =λf 1 λ + (1 − λ)f (0) (12)</formula><p>where we use the fact that f is convex with Jensen's inequality in Eq.(11) and the fact that p(x)p(x) = 0, ∀x ∈ X in Eq.(12) since P and P has disjoint support.</p><p>We also have</p><formula xml:id="formula_11">D f (P ||λP + (1 − λ)P ) = X (λp(x) + (1 − λ)p(x)) f p(x) λp(x) + (1 − λ)p(x) = X (λp(x)) f p(x) λp(x) + (1 − λ)p(x) + (1 − λ)f (0) = X (λp(x)) f p(x) λp(x) + 0 + (1 − λ)f (0) = λf 1 λ + (1 − λ)f (0)</formula><p>Therefore, in order for the inequality in Equation 11 to be an equality, we must have that q(x) = p(x) for all x ∈ X . Therefore, the generator distribution recovers the data distribution at the equlibrium posed by the NDA-GAN objective, which is also the case for the original GAN objective.</p><p>Moreover, from Lemma 1 in <ref type="bibr" target="#b25">Nguyen et al. (2008)</ref>, we have that:</p><formula xml:id="formula_12">arg max D φ L f (Q, D φ ) = f (p data /Q)<label>(13)</label></formula><p>Therefore, by replacing Q with G θ and (λG θ + (1 − λ)P ), we have:</p><formula xml:id="formula_13">arg max D φ :X →R L f (G θ , D φ ) = f (p data /G θ )<label>(14)</label></formula><p>arg max</p><formula xml:id="formula_14">D φ :X →R L f (λG θ + (1 − λ)P , D φ ) = f (p data /(λG θ + (1 − λ)P ))<label>(15)</label></formula><p>which shows that the optimal discriminators are indeed different for the two objectives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D NDA FOR CONTRASTIVE REPRESENTATION LEARNING</head><p>We describe the detailed statement of Theorem 2 and proof as follows.</p><p>Theorem 3. For some distribution p over X such that supp(p) ∩ supp(p data ) = ∅, and for any maximizer of the NDA-CPC objectivê</p><formula xml:id="formula_15">h ∈ arg max h θ max g φ I CPC (h θ , g φ )</formula><p>the representations of negative samples are disjoint from that of positive samples forĥ; i.e., ∀x ∈ supp(p data ),x ∈ supp(p), supp(ĥ(x)) ∩ supp(ĥ(x)) = ∅ Proof. We use a contradiction argument to establish the proof. For any representation mapping that maximizes the NDA-CPC objective,</p><formula xml:id="formula_16">h ∈ arg max h θ max g φ I CPC (h θ , g φ )</formula><p>suppose that the positive and NDA samples share some support, i.e., ∃x ∈ supp(p data ),x ∈ supp(p), supp(ĥ(x)) ∩ supp(ĥ(x)) = ∅</p><p>We can always constructĥ that shares the same representation withĥ for p data but have disjoint representations for NDA samples; i.e., ∀x ∈ supp(p data ),x ∈ supp(p), the following two statements are true:</p><formula xml:id="formula_17">1.ĥ(x) =ĥ (x); 2. supp(ĥ (x)) ∩ supp(ĥ (x)) = ∅.</formula><p>Our goal is to prove that:</p><formula xml:id="formula_18">max g φ I CPC (ĥ , g φ ) &gt; max g φ I CPC (ĥ, g φ )<label>(16)</label></formula><p>which shows a contradiction.</p><p>For ease of exposition, let us allow zero values for the output of g, and define 0/0 = 0 (in this case, if g assigns zero to positive values, then the CPC objective becomes −∞, so it cannot be a maximizer to the objective).</p><p>Letĝ ∈ arg max I CPC (ĥ, g φ ) be an optimal critic to the representation modelĥ θ . We then define a following critic function:</p><formula xml:id="formula_19">g (x, z) = ĝ(x, z) if ∃x ∈ supp(p data ) s.t. z ∈ supp(ĥ (x)) 0 otherwise<label>(17)</label></formula><p>In other words, the critic assigns the same value for data-representation pairs over the support of p data and zero otherwise. From the assumption overĥ, ∃x ∈ supp(p data ),x ∈ supp(p), and z ∈ supp(ĥ(x)), z ∈ supp(ĥ(x)) so (x, z) can be sampled as a positive pair andĝ(x, z) &gt; 0.</p><p>Therefore, E PIX2PIX <ref type="figure" target="#fig_5">Figure 7</ref> highlights the qualitative improvements when we apply the NDA method to Pix2Pix model <ref type="bibr" target="#b20">(Isola et al., 2017)</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F ANOMALY DETECTION</head><p>Here, we show the histogram of difference in discriminator's output for clean and OOD samples in <ref type="figure">Figure 8</ref>. High difference values imply that the Jigsaw NDA is better at distinguishing OOD samples than the normal BigGAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G EFFECT OF HYPERPARAMETER ON UNCONDITIONAL IMAGE GENERATION</head><p>Here, we show the effect of λ for unconditional image generation on CIFAR-10 dataset.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I DATASET PREPARATION FOR FID EVALUATION</head><p>For dataset preparation, we follow the the following procedures: (a) CIFAR-10 contains 60K 32×32 images with 10 labels, out of which 50K are used for training and 10K are used for testing, (b) CIFAR-100 contains 60K 32 × 32 images with 100 labels, out of which 50K are used for training and 10K are used for testing, (c) CelebA contains 162,770 train images and 19,962 test images (we resize the images to 64×64px), (d) STL-10 contains 100K (unlabeled) train images and 8K (labeled) test images (we resize the images to 32×32px). In our experiments the FID is calculated on the test dataset. In particular, we use 10K generated images vs. 10K test images for CIFAR-10, 10K vs. 10K for CIFAR-100, 19,962 vs. 19,962 for CelebA, and 8K vs 8K for STL-10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J HYPERPARAMETERS AND NETWORK ARCHITECTURE</head><p>Generative Modeling. We use the same network architecture in <ref type="bibr">BigGAN Brock et al. (2018)</ref> for our experiments. The code used for our experiments is based over the author's PyTorch code. For CIFAR-10, CIFAR-100, and CelebA we train for 500 epochs whereas for STL-10 we train for 300 epochs. For all the datasets we use the following hyperparameters: batch-size = 64, generator learning rate = 2e-4, discriminator learning rate = 2e-4, discriminator update steps per generator update step = 4. The best model was selected on the basis of FID scores on the test set (as explained above).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Negative Data Augmentation for GANs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Negative augmentations produce out-of-distribution samples lacking the typical structure of natural images; these negative samples can be used to inform a model on what it should not learn.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Histogram of difference in the discriminator output for a real image and it's Jigsaw version.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Left: Distribution over number of dots. The arrows are the number of dots the learning algorithm is trained on, and the solid line is the distribution over the number of dots the model generates. Right: Distribution over number of CLEVR objects the model generates.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>II</head><label></label><figDesc>CPC (ĥ , g φ ) ≥ I CPC (ĥ ,ĝ ) CPC (ĥ, g φ ) (Assumption thatĝ is optimal critic) which proves the theorem via contradiction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Qualitative results on Cityscapes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>Comparing the cosine distance of the representations learned with Jigsaw NDA and Moco-V2 (shaded blue), and original Moco-V2 (white). With NDA, we project normal and its jigsaw image representations further away from each other than the one without NDA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>FID scores over CIFAR-10 using different transformations as PDA and NDA in BigGAN.The results indicate that some transformations yield better results when used as NDA. The common feature of such transformations is they all spatially corrupt the images.12.61 79.72 14.69 108.69 13.97 70.64 17.29 90.81 15.01 20.02 15.05  16.65 124.32 44.41 18.72    </figDesc><table><row><cell>w/o Aug.</cell><cell>Jigsaw</cell><cell>Cutout</cell><cell cols="2">Stitch</cell><cell>Mixup</cell><cell>Cutmix</cell><cell cols="2">Random Crop Random Flip</cell><cell>Gaussian</cell></row><row><cell></cell><cell cols="3">PDA NDA PDA NDA PDA</cell><cell cols="4">NDA PDA NDA PDA NDA PDA NDA</cell><cell>PDA NDA</cell><cell>PDA NDA</cell></row><row><cell>18.64</cell><cell>98.09</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison of FID scores of different types of NDA for unconditional image generation on various datasets. The numbers in bracket represent the corresponding image resolution in pixels. Jigsaw consistently achieves the best or second best result.</figDesc><table><row><cell></cell><cell cols="7">BigGAN Jigsaw Stitching Mixup Cutout Cutmix CR-BigGAN</cell></row><row><cell>CIFAR-10 (32)</cell><cell>18.64</cell><cell>12.61</cell><cell>13.97</cell><cell>17.29</cell><cell>14.69</cell><cell>15.01</cell><cell>14.56</cell></row><row><cell cols="2">CIFAR-100 (32) 22.19</cell><cell>19.72</cell><cell>20.99</cell><cell>22.21</cell><cell>22.08</cell><cell>20.78</cell><cell>-</cell></row><row><cell>CelebA (64)</cell><cell>38.14</cell><cell>37.24</cell><cell>37.17</cell><cell>37.51</cell><cell>37.39</cell><cell>37.46</cell><cell>-</cell></row><row><cell>STL10 (32)</cell><cell>26.80</cell><cell>23.94</cell><cell>26.08</cell><cell>24.45</cell><cell>24.91</cell><cell>25.34</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>FID scores for conditional image generation using different NDAs. 2 Image Translation. Next, we apply the NDA method to image translation. In particular, we use the Pix2Pix model<ref type="bibr" target="#b20">(Isola et al., 2017)</ref> that can perform image-to-image translation using GANs provided paired training data. Here, the generator is conditioned on an image I, and the discriminator takes as input the concatenation of generated/real image and I. We use Pix2Pix for semantic segmentation on Cityscapes dataset<ref type="bibr" target="#b5">(Cordts et al., 2016)</ref> (i.e. photos → labels).</figDesc><table><row><cell></cell><cell cols="7">BigGAN Jigsaw Stitching Mixup Cutout Cutmix CR-BigGAN</cell></row><row><cell>C-10</cell><cell>11.51</cell><cell>9.42</cell><cell>9.47</cell><cell>13.87</cell><cell>10.52</cell><cell>10.3</cell><cell>11.48</cell></row><row><cell cols="2">C-100 15.04</cell><cell>14.12</cell><cell>13.90</cell><cell>15.27</cell><cell>14.21</cell><cell>13.99</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Results on CityScapes, using per pixel accuracy (Pp.), per class accuracy (Pc.) and mean Intersection over Union (mIOU). We compare Pix2Pix and its NDA version.</figDesc><table><row><cell>Metric</cell><cell>Pp.</cell><cell>Pc.</cell><cell>mIOU</cell></row><row><cell>Pix2Pix (cGAN)</cell><cell cols="3">0.80 0.24 0.27</cell></row><row><cell>NDA (cGAN)</cell><cell cols="3">0.84 0.34 0.28</cell></row><row><cell>Pix2Pix (L1+cGAN)</cell><cell cols="3">0.72 0.23 0.18</cell></row><row><cell>NDA (L1+cGAN)</cell><cell cols="3">0.75 0.28 0.22</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>AUROC scores for different OOD datasets. OOD-1 contains different datasets, while OOD-2 contains the set of 19 different corruptions in CIFAR-10-C (the average score is reported).</figDesc><table><row><cell></cell><cell></cell><cell cols="3">BigGAN Jigsaw EBM</cell></row><row><cell></cell><cell>DTD</cell><cell>0.70</cell><cell>0.69</cell><cell>0.48</cell></row><row><cell></cell><cell>SVHN</cell><cell>0.75</cell><cell>0.61</cell><cell>0.63</cell></row><row><cell>OOD-1</cell><cell cols="2">Places-365 TinyImageNet 0.40 0.35</cell><cell>0.58 0.62</cell><cell>0.68 0.67</cell></row><row><cell></cell><cell>CIFAR-100</cell><cell>0.63</cell><cell>0.64</cell><cell>0.50</cell></row><row><cell></cell><cell>Average</cell><cell>0.57</cell><cell>0.63</cell><cell>0.59</cell></row><row><cell cols="2">OOD-2 CIFAR-10-C</cell><cell>0.56</cell><cell>0.63</cell><cell>0.60</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Top-1 accuracy results on image recognition w/ and w/o NDA on MoCo-V2. The baseline MoCo achieves 38.47 AP, 65.99 AP50, 38.81 AP75 whereas the MoCo trained with mixup NDA gets 38.72 AP, 66.23 AP50, 39.16 AP75 (an improvement of ≈ 0.3).</figDesc><table><row><cell></cell><cell cols="6">MoCo-V2 Jigsaw Stitching Cutout Cutmix Mixup</cell></row><row><cell>CIFAR-10</cell><cell>91.20</cell><cell>91.66</cell><cell>91.59</cell><cell>92.26</cell><cell>91.51</cell><cell>91.36</cell></row><row><cell>CIFAR-100</cell><cell>69.63</cell><cell>70.17</cell><cell>69.21</cell><cell>69.81</cell><cell>69.83</cell><cell>69.99</cell></row><row><cell cols="2">ImageNet-100 69.41</cell><cell>69.95</cell><cell>69.54</cell><cell>69.77</cell><cell>69.61</cell><cell>70.01</cell></row><row><cell>set.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Top-1 accuracy results on action recognition in videos w/ and w/o NDA in DPC.</figDesc><table><row><cell>DPC</cell><cell cols="5">Jigsaw Stitching Cutout Cutmix Mixup</cell></row><row><cell cols="2">UCF-101 (Pre-trained on UCF-101) 61.35 64.54</cell><cell>66.07</cell><cell>64.52</cell><cell>63.52</cell><cell>63.65</cell></row><row><cell cols="2">HMDB51 (Pre-trained on UCF-101) 45.31 46.88</cell><cell>45.31</cell><cell>45.31</cell><cell>48.43</cell><cell>43.75</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Effect of λ on the FID score for unconditional image generation on CIFAR-10 using Jigsaw as NDA.</figDesc><table><row><cell>λ</cell><cell>1.0</cell><cell>0.75</cell><cell>0.5</cell><cell>0.25</cell><cell>0.15</cell></row><row><cell cols="6">FID 18.64 16.61 14.95 12.61 13.01</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We feed a single label to all images to make the architecture suitable for unconditional generation.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We use a PyTorch code for BigGAN. The number reported in<ref type="bibr" target="#b3">Brock et al. (2018)</ref> for C-10 is 14.73.3  We use the official PyTorch implementation and show the best results.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K CODE</head><p>The code to reproduce our experiments is given here.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antreas</forename><surname>Antoniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><surname>Storkey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harrison</forename><surname>Edwards</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.04340</idno>
		<title level="m">Data augmentation generative adversarial networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A critical analysis of self-supervision, or what we can learn from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.13132</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adversarial contrastive estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avishek Joey</forename><surname>Bose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanshuai</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1021" to="1032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Large scale gan training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.11096</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Describing textures in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<title level="m">Improved regularization of convolutional neural networks with cutout</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Implicit generation and generalization in energy-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Mordatch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.08689</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Testing the manifold hypothesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Fefferman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjoy</forename><surname>Mitter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hariharan</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Mathematical Society</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="983" to="1049" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patricia</forename><surname>Rubisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wieland</forename><surname>Felix A Wichmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brendel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.12231</idno>
		<title level="m">Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5767" to="5777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Video representation learning by dense predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengda</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Actively avoiding nonsense in generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Hanneke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><forename type="middle">Tauman</forename><surname>Kalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautam</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Tzamos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference On Learning Theory</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="209" to="227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05722</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Benchmarking neural network robustness to common corruptions and surface variations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dietterich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.01697</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A baseline for detecting misclassified and out-of-distribution examples in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02136</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Deep anomaly detection with outlier exposure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Dietterich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.04606</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Generative adversarial positive-unlabeled learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brahim</forename><surname>Chaib-Draa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 27th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2255" to="2261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Wael Abd-Almageed, and Prem Natarajan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayush</forename><surname>Jaiswal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><forename type="middle">Yue</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5092" to="5102" />
		</imprint>
	</monogr>
	<note>Unsupervised adversarial invariance</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05957</idno>
		<title level="m">Spectral normalization for generative adversarial networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Estimating divergence functionals and the likelihood ratio by convex risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanlong</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Wainwright</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIT.2010.2068870</idno>
		<idno type="arXiv">arXiv:0809.0853</idno>
		<imprint>
			<date type="published" when="2008-09" />
			<biblScope unit="page" from="5847" to="5861" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="69" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">f-gan: Training generative neural samplers using variational divergence minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Botond</forename><surname>Cseke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryota</forename><surname>Tomioka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="271" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">On variational bounds of mutual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tucker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.06922</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">On variational bounds of mutual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tucker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.06922</idno>
		<imprint>
			<date type="published" when="2019-05" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Understanding the limitations of variational mutual information estimators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.06222</idno>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Difference-seeking generative adversarial network-unseen sample generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Lin Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung-Hsien</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soo-Chang</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Shien</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naftali</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noga</forename><surname>Zaslavsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02406</idno>
		<title level="m">Deep learning and the information bottleneck principle</title>
		<imprint>
			<date type="published" when="2015-03" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">No free lunch theorems for optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">G</forename><surname>Wolpert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Macready</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on evolutionary computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="67" to="82" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="649" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Bias and generalization in deep generative models: An empirical study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjia</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arianna</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="10792" to="10801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">We use the official PyTorch implementation for our experiments. For CIFAR-10 and CIFAR-100, we perform unsupervised pre-training for 1000 epochs and supervised training (linear classifier) for 100 epochs. For Imagenet-100, we perform unsupervised pre-training for 200 epochs and supervised training (linear classifier) for 100 epochs. For CIFAR-10 and CIFAR-100, we use the following hyperparameters during pre-training: batch-size = 256, learning-date = 0.3, temperature = 0.07, feature dimensionality = 2048. For ImageNet-100 pretraining we have the following: batch-size = 128, learning-date = 0.015, temperature = 0.2, feature dimensionality = 128. During linear classification we use a batch size of 256 for all the datasets and learning rate of 10 for CIFAR-10</title>
	</analytic>
	<monogr>
		<title level="m">Momentum Contrastive Learning</title>
		<imprint/>
	</monogr>
	<note>CIFAR-100, whereas for ImageNet-100 we use learning rate of 30</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">We use the same network architecture and hyper-parameters in DPC Han et al. (2019) for our experiments and use the official PyTorch implementation. We perform self-supervised training on UCF-101 for 200 epochs and supervised training (action classifier) for 200 epochs on both UCF-101 and HMDB51 datasets</title>
		<imprint/>
	</monogr>
	<note>Dense Predictive Coding</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
