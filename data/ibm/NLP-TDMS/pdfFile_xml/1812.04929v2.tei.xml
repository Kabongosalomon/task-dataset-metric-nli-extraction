<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semi-Supervised Learning for Face Sketch Synthesis in the Wild</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaofeng</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Tan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Baidu Research</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwan-Yee</forename><forename type="middle">K</forename><surname>Wong</surname></persName>
							<email>kykwong@cs.hku.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Semi-Supervised Learning for Face Sketch Synthesis in the Wild</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Face sketch synthesis has made great progress in the past few years. Recent methods based on deep neural networks are able to generate high quality sketches from face photos. However, due to the lack of training data (photo-sketch pairs), none of such deep learning based methods can be applied successfully to face photos in the wild. In this paper, we propose a semi-supervised deep learning architecture which extends face sketch synthesis to handle face photos in the wild by exploiting additional face photos in training. Instead of supervising the network with ground truth sketches, we first perform patch matching in feature space between the input photo and photos in a small reference set of photo-sketch pairs. We then compose a pseudo sketch feature representation using the corresponding sketch feature patches to supervise our network. With the proposed approach, we can train our networks using a small reference set of photo-sketch pairs together with a large face photo dataset without ground truth sketches. Experiments show that our method achieves state-of-the-art performance both on public benchmarks and face photos in the wild. Codes are available at https://github.com/chaofengc/Face-Sketch-Wild.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Face sketch synthesis targets at generating a sketch from an input face photo. It has many useful applications. For instance, police officers often have to rely on face sketches to identify suspects, and face sketch synthesis makes it feasible for matching sketches against photos in a mugshot database automatically. Artists can also employ face sketch synthesis to simplify the animation production process <ref type="bibr" target="#b0">[1]</ref>. Many people prefer using sketches as their profile pictures in social media networks <ref type="bibr" target="#b1">[2]</ref>, and face sketch synthesis allows them to produce sketches without the help of a professional artist.</p><p>Much effort has been devoted to face sketch synthesis. In particular, exemplar based methods dominated in the past two decades. These methods can achieve good performance without explicitly modeling the highly nonlinear mapping between face photos and sketches. They commonly subdivide a test photo into overlapping patches, and match these test patches with the photo patches in a reference set of photo-sketch pairs. They then compose an output sketch using the corresponding sketch patches in the reference set. Although promising results arXiv:1812.04929v2 [cs.CV] <ref type="bibr" target="#b4">5</ref> May 2019 have been reported <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>, these methods have several drawbacks. For example, sketches in <ref type="figure">Fig. 4</ref>(c)(d)(e)(f) are over-smoothed and fail to preserve subtle contents such as strands of hair on the forehead. Moreover, the patch matching and optimization processes are often very time-consuming. Recent methods exploited Convolutional Neural Networks (CNNs) to learn a direct mapping between photos and sketches, which is, however, a non-trivial task. The straight forward CNN based method produces blurry sketches (see <ref type="figure">Fig. 4(g)</ref>), and methods based on Generative Adversary Networks (GAN) <ref type="bibr" target="#b5">[6]</ref> introduce undesirable artifacts (see <ref type="figure">Fig. 4</ref>(h),(i)). Besides, all these CNN based methods do not generalize well to face photos in the wild due to the lack of large training datasets of photo-sketch pairs. Although unpaired GAN based methods such as Cycle-GAN <ref type="bibr" target="#b6">[7]</ref> can use unpaired data to transfer images between different domains, they fail to well preserve the facial content because of the weak content constraint (see <ref type="figure" target="#fig_5">fig. 8</ref>).</p><p>In this paper, we propose a semi-supervised learning framework for face sketch synthesis that takes advantages of the exemplar based approach, the perceptual loss and GAN. We design a residual net <ref type="bibr" target="#b7">[8]</ref> with skip connections as our generator network. Suppose we have a small reference set of photo-sketch pairs and a large face photo dataset without ground truth sketches. Similar to the exemplar based approach, we subdivide the VGG-19 <ref type="bibr" target="#b8">[9]</ref> feature maps of the input photo into overlapping patches, and match them with the photo patches (in feature space) in the reference set. We then compose a pseudo sketch feature representation using the corresponding sketch patches (in feature space) in the reference set. We can then supervise our generator network using a perceptual loss based on the mean squared error (MSE) between the feature maps of the generated sketch and the corresponding pseudo sketch feature of the input photo. An adversary loss is also utilized to make the generated sketches more realistic.</p><p>In summary, our main contributions are three folds: (1) A semi-supervised learning framework for face sketch synthesis. Our framework allows us to train our networks using a small reference set of photo-sketch pairs together with a large face photo dataset without ground truth sketches. This enables our networks to generalize well to face photos in the wild. (2) A perceptual loss based on pseudo sketch feature. We show that the proposed loss is critical in preserving both facial content and texture details in the generated sketches. Extensive experiments are conducted to verify the effectiveness of our model. Both qualitative and quantitative results illustrate the superiority of our method. (3) To the best of our knowledge, our method is the first work that can generate visually pleasant sketches for face photos in the wild.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Exemplar Based Methods</head><p>Tang and Wang <ref type="bibr" target="#b9">[10]</ref> first introduced the exemplar based method based on eigentransformation. They projected an input photo onto the eignspace of the training photos, and then reconstruct a sketch from the eignspace of the training sketches using the same projection. Liu et al. <ref type="bibr" target="#b10">[11]</ref> observed that the linear model holds better locally, and therefore proposed a nonlinear model based on local linear embedding (LLE). They first subdivided an input photo into overlapping patches and reconstructed each photo patch as a linear combination of the training photo patches. They then obtained the sketch patches by applying the same linear combinations to the corresponding training sketch patches. Wang and Tang <ref type="bibr" target="#b4">[5]</ref> employed a multi-scale markov random fields (MRF) model to improve the consistency between neighboring patches. By introducing shape priors and SIFT features, Zhang et al. <ref type="bibr" target="#b11">[12]</ref> proposed an extended version of MRF which can handle face photos under different illuminations and poses. However, these MRF based methods are not capable of synthesizing new sketch patches since they only select the best candidate sketch patch for each photo patch. To tackle this problem, Zhou et al. <ref type="bibr" target="#b2">[3]</ref> presented the markov weight fields (MWF) model which produces a target sketch patch as a linear combination of K best candidate sketch patches. Considering that patch matching based on traditional image features (e.g., PCA and SIFT) is not robust, a recent method <ref type="bibr" target="#b3">[4]</ref> used CNN feature to represent the training patches and computed more accurate combination coefficients. To accelerate the synthesis procedure, Song et al. <ref type="bibr" target="#b0">[1]</ref> formulated face sketch synthesis as a spatial sketch denoising (SSD) problem, and Wang et al. <ref type="bibr" target="#b12">[13]</ref> presented an offline random sampling strategy for nearest neighbor selection of patches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Learning Based Methods</head><p>Recent works applied CNN to synthesize sketches and produced promising results. Zhang et al. <ref type="bibr" target="#b13">[14]</ref> proposed a 7-layer fully convolutional network (FCN) to directly transfer an input photo to a sketch. Although their model can roughly estimate the outline of a face, it fails to capture texture details with the use of intensity based mean squared error (MSE) loss. Zhang et al. <ref type="bibr" target="#b14">[15]</ref> utilized a branched fully convolutional network (BFCN) consisting of a content branch and a texture branch. Because the face content and texture are predicted separately with different loss metrics, the final sketch looks disunited. Chen et al. <ref type="bibr" target="#b15">[16]</ref> proposed the pyramid column feature and used it to compose a reference style for a test photo from the training sketches. They utilized a CNN to create a content image from the photo, and then transferred the reference style to introduce shadings and textures in the output sketch. Wang et al. <ref type="bibr" target="#b16">[17]</ref> presented the multiscale generative adversarial networks (GANs) to generate sketches from photos and vice versa. Multiple discriminators at different hidden layers are used to supervise the synthesis process. Gao et al. <ref type="bibr" target="#b17">[18]</ref> took advantage of the facial parsing map and proposed a composition-aided stack GAN. All these deep learning based methods require ground truth photo-sketch pairs for training, and they do not generalize well to face photos in the wild due to the lack of training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Semi-Supervised Face Sketch Synthesis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>Our framework is composed of three main parts, namely a generator network G, a pseudo sketch feature generator and a discriminator network D (see <ref type="figure">Fig. 1</ref>). The generator network is a deep residual network with skip connections. It is used to generate a synthesized sketchŷ for each input photo x. The pseudo sketch feature generator is the key to our semi-supervised learning approach. Instead of training the generator network directly with ground truth sketches, we construct a pseudo sketch feature for each input photo to supervise the synthesis ofŷ. In this way, we can train our network on any face photo datasets, and generalize our model to face photos in the wild. We further adopt a discriminator network D to minimize the gap between generated sketches and real sketches drawn by artists.  <ref type="figure">Fig. 1</ref>: Framework of the proposed method. The generator network is a deep residual network with skip connections. It generates a synthesized sketch from an input photo. The pseudo sketch feature generator utilizes patch matching in the deep feature space to generate a pseudo sketch feature for an input photo in training. The discriminator network tries to distinguish between generated sketches and sketches drawn by artists.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Pseudo Sketch Feature Generator</head><p>Given a reference set</p><formula xml:id="formula_0">R = {(x R i , y R i )} N i=1</formula><p>, the pseudo sketch feature generator targets at constructing a pseudo sketch feature Φ (x) for a test photo x which is used to supervise the synthesis of the sketchŷ. We follow MRF-CNN <ref type="bibr" target="#b18">[19]</ref> to extract a local patch representation of an image. We first feed x into a pretrained VGG-19 network and extract the feature map Φ l (x) at the l-th layer. Similarly,</p><formula xml:id="formula_1">we obtain {Φ l (x R i )} N i=1 and {Φ l (y R i )} N i=1 . Let us denote a k ×k patch centered at a point j of Φ l (x) as Ψ j Φ l (x) , and the same definition applies to Ψ j Φ l (x R i ) and Ψ j (Φ l y R i ) . Now for each patch Ψ j Φ l (x) , where j = 1, 2, . . . , m and m = (H l − 2 × k 2 ) × (W l − 2 × k 2 )</formula><p>with H l and W l being the height and width of Φ l (x), we find its best match Ψ j Φ l (x R i ) in the reference set based on cosine distance, i.e.,</p><formula xml:id="formula_2">(i , j ) = arg max i * =1∼N j * =1∼m Ψ j Φ l (x) · Ψ j * Φ l (x R i * ) Ψ j (Φ l (x)) 2 Ψ j * Φ l (x R i * ) 2 .<label>(1)</label></formula><p>Since the photos and the corresponding sketches in R are well aligned, we directly apply (i , j ) to index the corresponding sketch feature patch <ref type="figure" target="#fig_0">Fig. 2</ref> visualizes an example of the pseudo sketch feature. It can be seen that the pseudo sketch feature provides a good approximation of the real sketch feature (see <ref type="figure" target="#fig_0">Fig. 2(a)</ref>). We also show a naïve reconstruction in <ref type="figure" target="#fig_0">Fig. 2</ref>(b) obtained by directly using the matching index to index the pixel values in the training sketches. We can see such a naïve reconstruction does roughly resemble the real sketch, which also justifies the effectiveness of the pseudo sketch feature. Note that we only need alignment between photos and sketches in R. Since we perform a dense patch matching between the input photo and the reference photos, we can also generate reasonable pseudo sketch features for input faces under different poses (see <ref type="figure" target="#fig_0">Fig. 2</ref> </p><formula xml:id="formula_3">Ψ j Φ l (y R i ) for Ψ j Φ l (x R i ) , and use it as the pseudo sketch feature patch Ψ j Φ l (x) for Ψ j Φ l (x) . Finally, a pseudo sketch feature representation (at layer l) for x is given by {Ψ j Φ l (x) } m j=1 .</formula><formula xml:id="formula_4">(c)). (a) (b) (c)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Loss Functions</head><p>Pseudo Sketch Feature Loss We define our pseudo sketch feature loss as</p><formula xml:id="formula_5">L p (x,ŷ) = 5 l=3 m j=1 Ψ j Φ l (ŷ) − Ψ j Φ l (x) 2 2 ,<label>(2)</label></formula><p>where l = 3, 4, 5 refer to layers relu3 1, relu4 1, and relu5 1 respectively. High level features after relu3 1 are better representations of textures and more robust to appearance changes and geometric transforms <ref type="bibr" target="#b18">[19]</ref>. <ref type="figure">Fig. 3</ref> shows the results of using different layers in L p . As expected, low level features (e.g., relu1 1 and relu2 1) fail to generate sketch textures. While high level features (e.g., relu5 1) can better preserve textures, they produce artifacts in terms of details (see the eyes of sketches in <ref type="figure">Fig. 3</ref>). To get better performance and reduce the computation cost of patch matching, we set l = 3, 4, 5.</p><p>Photo &amp; Sketch relu1 1 relu2 1 relu3 1 relu4 1 relu5 1 <ref type="figure">Fig. 3</ref>: Results of using different layers in pseudo sketch feature loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GAN Loss</head><p>For easier convergence, we use the least square loss when training the GAN, known as LSGAN <ref type="bibr" target="#b19">[20]</ref>. The objective functions of LSGAN are given by</p><formula xml:id="formula_6">L GAN D = 1 2 E y∼p sketch (y) [(D(y) − 1) 2 ] + 1 2 E x∼p photo (x) [(D(G(x))) 2 ] (3) L GAN G = E x∼p photo (x) [(D(G(x)) − 1) 2 ]<label>(4)</label></formula><p>Total Variation Loss Sketches generated by CNN may be unnatural and noisy. Following previous works <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b21">22]</ref>, we adopt the total variation loss as a natural image prior to further improve the sketch quality,</p><formula xml:id="formula_7">L tv (ŷ) = u,v (ŷ u+1,v −ŷ u,v ) 2 + (ŷ u,v+1 −ŷ u,v ) 2 ,<label>(5)</label></formula><p>whereŷ u,v denotes the intensity value at (u, v) of the synthesized sketchŷ. Based on the above loss terms, we can train our generator network G and discriminator network D using the following two loss functions respectively:</p><formula xml:id="formula_8">L G = λ p L p + λ adv L GAN G + λ tv L tv ,<label>(6)</label></formula><formula xml:id="formula_9">L D = L GAN D<label>(7)</label></formula><p>where L G and L D are minimized alternatively until convergence. λ p , λ adv and λ tv are trade-off weights for each loss term respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>Photo-Sketch Pairs We use two public datasets: the CUFS dataset(consisting of the CUHK student dataset <ref type="bibr" target="#b9">[10]</ref>, the AR dataset <ref type="bibr" target="#b22">[23]</ref>, and the XM2VTS dataset <ref type="bibr" target="#b23">[24]</ref>) and the CUFSF dataset <ref type="bibr" target="#b24">[25]</ref>, to evaluate our model 1 . The CUFSF dataset is more challenging than the CUFS dataset because (1) the photos were captured under different lighting conditions and (2) the sketches exhibit strong deformation in shape and cannot be aligned with the photos well. Details of these datasets are summarized in <ref type="table" target="#tab_1">Table 1</ref>. Face Photos We use the VGG-Face dataset <ref type="bibr" target="#b25">[26]</ref> to evaluate our model on photos in the wild. There are 2,622 persons in this dataset and each person has 1,000 photos. We randomly select 2,000 persons for training and the rest for testing. For each person in the training split, we randomly select N photos and named the resulting dataset VGG-FaceN 2 , where N = 01, 02, . . . , 10. We also randomly select 2 photos for each person in the testing split to construct a VGG test set of 1,244 photos.</p><p>Preprocessing For photos/sketches which have already been aligned and have a size of 250 × 200, we leave them unchanged. For the rest, we first detect 68 face landmarks on the image using dlib 3 , and calculate a similarity transform to warp the image into one with the two eyes located at (75, 125) and (125, 125) respectively. We then crop the resulting image to a size of 250 × 200. We simply drop those photos/sketches from which we fail to detect face landmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Patch Matching</head><p>As in exemplar based methods, patch matching is a time-consuming process. We accelerate this process in three ways. First, we precompute and store the feature patches for the photos and sketches in the reference set (i.e.,</p><formula xml:id="formula_10">{Ψ j Φ l (x R i ) } and {Ψ j (Φ l y R i ) })</formula><p>. Second, instead of searching the whole reference feature set, we first identify k best matched reference photos for each input photo based on the cosine distance of their relu5 1 feature maps. Patch matching is then restricted within these k reference photos (we set k = 5 in the whole training process). Third, Equ. 1 is implemented as a convolution operator which can be computed efficiently on GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Training Details</head><p>We updated the generator and discriminator alternatively at every iteration. The trade-off weights λ p , λ adv were set to 1 and 10 3 , and λ tv was set to 10 −5 when using CUFS as reference set and 10 −2 when using CUFSF. We implemented our model using PyTorch 4 , and trained it on a Nvidia Titan X GPU. We used Adam <ref type="bibr" target="#b26">[27]</ref> with learning rates from 10 −3 to 10 −5 , decreasing with a factor of 10 −1 . Data augmentation was done online in the color space (brightness, contrast, saturation and sharpness). Each iteration took about 2s with a batch size of 6, and the model converged after about 5 hours of training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation on Public Benchmarks</head><p>In this section, we evaluate our model using two public benchmarks, namely CUFS and CUFSF, which were captured under laboratory conditions. We use the training photos from CUFS∪CUFSF to train our networks. When evaluating on CUFS, the reference photo-sketch pairs only comes from CUFS, and the same applies to CUFSF. To demonstrate the effectiveness of our model, we compare our results both qualitatively and quantitatively with seven other methods, namely MWF <ref type="bibr" target="#b2">[3]</ref>, SSD <ref type="bibr" target="#b0">[1]</ref>, RSLCR <ref type="bibr" target="#b12">[13]</ref>, DGFL <ref type="bibr" target="#b3">[4]</ref>, FCN <ref type="bibr" target="#b13">[14]</ref>, Pix2Pix-GAN <ref type="bibr" target="#b27">[28]</ref>, and Cycle-GAN <ref type="bibr" target="#b6">[7]</ref>. We also compare our results quantitatively with the latest GAN based sketch synthesis methods, i.e., PS 2 -MAN <ref type="bibr" target="#b28">[29]</ref> and stack-CA-GAN <ref type="bibr" target="#b17">[18]</ref>. Since the models of their work are not available, we can only compare with the results that are directly taken from their published papers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Qualitative Comparison</head><p>As we can observe in <ref type="figure">Fig. 4</ref>, exemplar based methods (see <ref type="figure">Fig. 4(c),(d)</ref>,(e) in general perform worse than learning based methods (see <ref type="figure">Fig. 4(g)</ref>,(h),(i),(j)), especially in preserving contents of the input photos. Using deep features in exemplar based methods helps to alleviate the problem, but the results are oversmoothed (see <ref type="figure">Fig. 4(f)</ref>). Due to the lack of training data, FCN produces bad</p><formula xml:id="formula_11">(a) (b) (c) (d) (e) (f) (g) (h) (i) (j) Photo Artist MWF[3] SSD[1] RS- LCR[13]</formula><p>DGFL <ref type="bibr" target="#b3">[4]</ref> FCN <ref type="bibr" target="#b13">[14]</ref>Pix2Pix-GAN <ref type="bibr" target="#b27">[28]</ref> Cycle-GAN <ref type="bibr" target="#b29">[30]</ref> Ours <ref type="figure">Fig. 4</ref>: Sketches generated using different methods. First 3 rows: test photos from CUFS. Last row: test photo from CUFSF.</p><p>results when the photos are taken under very different lighting conditions (see last two rows of <ref type="figure">Fig. 4(g)</ref>). Although the two GANs can produce much better results than FCN, they also introduce many artifacts and noise. Thanks to the pseudo sketch feature loss, our method does not suffer from the above problems. In particular, our semi-supervised strategy allows us to incorporate more training photos without ground truth in training, which helps to improve the generalization ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Quantitative Comparison</head><p>Image Quality Assessment For datasets with ground truth sketches (e.g., CUFS and CUFSF), previous work <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b3">4]</ref> typically used structural similarity (SSIM) <ref type="bibr" target="#b30">[31]</ref> as an image quality assessment metric to measure the similarity between a generated sketch and the ground truth sketch. However, many researchers (e.g., in super resolution <ref type="bibr" target="#b31">[32]</ref> and face sketch synthesis <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b28">29]</ref>) pointed out that SSIM is not always consistent with the perceptual quality. One main reason is that SSIM favors slightly blurry images when the images contain rich textures. To demonstrate this, we show some sketches generated using different methods together with their SSIM scores in <ref type="figure" target="#fig_2">Fig. 5</ref>. It can be seen that sketch generated by RSLCR is smoother than those by Pix2Pix-GAN and our model, but have higher SSIM scores. We applied a bilateral filter to smooth all the sketches. It can be observed that the SSIM scores of the sketch generated by RSLCR remain roughly the same after smoothing, whereas those of the sketches generated by Pix2Pix-GAN and our model improve by more than 1.5%.</p><p>In <ref type="figure" target="#fig_3">Fig. 6(a)</ref>, we show the average SSIM scores of the sketches generated by differ-  ent methods on CUFS, together with the average SSIM scores of their smoothed counterparts. As expected, the average SSIM scores of most of the methods improve after smoothing, same for a few exemplar based methods which produce over-smoothed sketches. The average SSIM score of our smoothed results is comparable to that of the state-of-the-art method. In <ref type="figure" target="#fig_3">Fig. 6(b)</ref>, we show the corresponding results on CUFSF. Similar conclusions can be drawn.</p><p>Due to the drawback of SSIM, we use feature similarity (FSIM) <ref type="bibr" target="#b32">[33]</ref> as our image quality assessment metric. FSIM is better at evaluating detailed textures compared with SSIM. It can be observed from <ref type="figure" target="#fig_2">Fig. 5</ref> that the FSIM scores of the sketches decrease after smoothing. The average FSIM scores of the sketches generated by different methods on CUFS and CUFSF are shown in <ref type="figure" target="#fig_3">Fig. 6(c)</ref> and <ref type="figure" target="#fig_3">Fig. 6(d)</ref> respectively. It can be seen that our method achieves the state-of-theart in terms of FSIM score on both CUFS and CUFSF.</p><p>Face Sketch Recognition Sketch recognition is an important application of face sketch synthesis. We follow the same practice of Wang et al. <ref type="bibr" target="#b12">[13]</ref> and employ the null-space linear discriminant analysis (NLDA) <ref type="bibr" target="#b33">[34]</ref> to perform the recognition experiments. <ref type="figure">Fig. 7</ref> shows the recognition accuracy of different methods on the two datasets. Our method achieves the best result when the dimension of the reduced eigenspace is less than 100, and achieves a competitive result to the state-of-the-art method <ref type="bibr" target="#b3">[4]</ref> when the dimension is above 100.</p><p>Comparison with PS 2 -MAN and stack-CA-GAN To further demonstrate the effectiveness of the proposed method, we compare it with two latest GAN methods, namely PS 2 -MAN <ref type="bibr" target="#b28">[29]</ref> and stack-CA-GAN <ref type="bibr" target="#b17">[18]</ref>, which are specially designed for sketch synthesis. As shown in <ref type="table" target="#tab_2">Table 2</ref>, our method achieves the best performance on almost all datasets, except for the SSIM score in CUFSF. However, we obtain a better performance on NLDA which indicates that our model can better preserve the identify information. Note that both of these GAN methods use extra information to train their network, i.e., multi-scale supervision (PS 2 -MAN) and parsing map (stack-CA-GAN). Compared with them, our perceptual loss can not only avoid producing artifacts but also help to improve the generalization of the network. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Qualitative Comparison</head><p>As photos in the wild are captured under uncontrolled environments, their appearance may vary largely. <ref type="figure" target="#fig_5">Fig. 8</ref> shows some photos sampled from our VGG-Face test dataset and the sketches generated by different methods. It can be observed that these photos may show very different lightings, poses, image resolutions, and hair styles. Besides, some photos may be incomplete and people may also use a cartoon as their photos for entertainment (see the last row of <ref type="figure" target="#fig_5">Fig. 8</ref>). It is therefore very difficult, if not impossible, for a method which only learns from a small set of photo-sketch pairs to generate sketches for photos in the wild. Among the results of other methods, exemplar based methods (see <ref type="figure" target="#fig_5">Fig. 8</ref>(b)(c)) fail to deal with pose changes and different hair styles. FCN produces sketches (see <ref type="figure" target="#fig_5">Fig. 8(d)</ref>) that can roughly preserve the contour of the face but lose important facial components (e.g., nose and eyes). Although GANs can generate some sketch like textures, none of them can well preserve the contents.</p><p>The face shapes are distorted and the key facial parts are lost. It can be seen from <ref type="figure" target="#fig_5">Fig. 8</ref>(g) that our model can handle photos in the wild well and generate pleasant results.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Effectiveness of Additional Training Photos</head><p>Introducing more training photos from VGG-Face dataset is the key to improve the generalization ability of our model. As demonstrated in <ref type="figure" target="#fig_6">Fig. 9</ref>, the model trained without additional photos from VGG-Face has difficulty in handling uncontrolled lightings and different hair colors (see <ref type="figure" target="#fig_6">Fig. 9(b)</ref>). As we add more photos to the training set, the results improve significantly (see the eyes and hair in <ref type="figure" target="#fig_6">Fig. 9</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Mean Opinion Score Test</head><p>Since there are no ground truth sketches for the photos in the wild, we performed a MOS test to assess the perceptual quality of the sketches generated by different  methods. Specifically, we randomly selected 30 photos from the VGG test set, and then generated the sketches for these photos using SSD, FCN, Fast-RSLCR, Pix2Pix-GAN and our method respectively. Given the example photo-sketch pairs from public benchmarks as reference, 108 raters were asked to rank 10 groups of randomly selected sketches synthesized by the five different methods. We assigned a score of 1-5 to the sketches based on their rankings (5 being the best). The results are presented in <ref type="figure" target="#fig_7">Fig. 10</ref>. It can be observed that the MOS of our results significantly outperforms that of the other methods. This demonstrates the superiority of our method on photos in the wild.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we propose a semi-supervised learning framework for face sketch synthesis in the wild. We design a residual network with skip connections to transfer photos to sketches. Instead of supervising our network using ground truth sketches, we construct a novel pseudo sketch feature representation for each input photo based on feature space patch matching with a small reference set of photo-sketch pairs. This allows us to train our model using a large face photo dataset (without ground truth sketches) with the help of a small reference set of photo-sketch pairs. Training with a large face photo dataset enables our model to generalize better to photos in the wild. Experiments show that our method can produce sketches comparable to those produced by other state-ofthe-art methods on four public benchmarks (in terms of SSIM and FSIM), and outperforms them on photos in the wild.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Acknowledgment</head><p>We thank Nannan Wang, Hao Zhou and Yibing Song for providing their codes and data. We also gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan X Pascal GPU used for this research.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>(a) Ground truth sketch feature (middle) and pseudo sketch feature of the relu3 1 layer (right). (b) Ground truth sketch (left) and pixel level projection of the patch matching result (right). (c) Photos in the wild without ground truth sketches. (Note that the pixel level results are only for visualization, and they are not used in training.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 :</head><label>5</label><figDesc>SSIM and FSIM scores of some generated sketches (left) and their smoothed counterparts (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 :</head><label>6</label><figDesc>(a) SSIM Score on CUFS (b) SSIM Score on CUFSF (c) FSIM Score on CUFS (d) FSIM Score on CUFSF Average SSIM and FSIM scores of the sketches generated by different methods on CUFS and CUFSF. The proposed method achieves state-of-the-art FSIM score on both datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 :</head><label>8</label><figDesc>Qualitative comparison of different methods for images in the wild. Benefit from the additional training photos, the proposed method can deal with various photos.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9 :</head><label>9</label><figDesc>Effectiveness of additional training photos. The results improve a lot when more and more photos are added to the training set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 10 :</head><label>10</label><figDesc>Results of MOS test on the quality of sketches generated by SSD, FCN, Fast-RSLCR, Pix2Pix-GAN and our model on photos in the wild.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell cols="2">Dataset</cell><cell cols="2">Total Pairs Train Test Align Var</cell></row><row><cell></cell><cell>CUHK</cell><cell>188</cell><cell>88 100</cell></row><row><cell>CUFS</cell><cell>AR</cell><cell>123</cell><cell>80 43</cell></row><row><cell></cell><cell>XM2VTS</cell><cell>295</cell><cell>100 195</cell></row><row><cell cols="2">CUFSF</cell><cell>1194</cell><cell>250 944</cell></row></table><note>Details of benchmark datasets. Align: whether the sketches are well aligned with photos. Var: whether the photos have lighting variations.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Quantitative comparison with PS 2 -MAN and stack-CA-GAN. Results are taken from their original papers.There are two challenges for sketch synthesis in the wild. The first challenge is how to deal with real photos captured under uncontrolled environments with varying pose and lighting, and cluttered backgrounds. The second is the computation time. Our method tackles the first challenge by introducing more training GPU for a 250 × 200 photo. We compared our method with five other methods , including SSD 5 , FCN, Pix2Pix-GAN 6 , Cycle-GAN 7 , and Fast-RSLCR 8 . In this experiment, we trained our model using CUFS as the reference set and all the training photos from the CUFS, CUFSF and VGG-Face10 as the training set. Since there are no ground truth sketches for the test photos, we carried out a mean opinion score (MOS) test to quantitatively evaluate the results.</figDesc><table><row><cell></cell><cell>CUHK</cell><cell>CUFS</cell><cell>CUFSF</cell></row><row><cell></cell><cell cols="3">SSIM FSIM SSIM NLDA SSIM NLDA</cell></row><row><cell>PS 2 -MAN</cell><cell>0.6156 0.7361</cell><cell>-</cell><cell>-</cell></row><row><cell>stack-CA-GAN</cell><cell>-</cell><cell cols="2">0.5266 96.04 0.4106 77.31</cell></row><row><cell>Ours</cell><cell cols="3">0.6328 0.7423 0.5463 98.22 0.4085 78.04</cell></row><row><cell cols="3">6 Sketch Synthesis in the Wild</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Data comes from http://www.ihitworld.com/RSLCR.html 2 The dataset will be made available. 3 http://dlib.net/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">http://pytorch.org/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">http://www.cs.cityu.edu.hk/~yibisong/eccv14/index.html 6 https://github.com/phillipi/pix2pix 7 https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix 8 http://www.ihitworld.com/RSLCR.html</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Real-time exemplar-based face sketch synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="800" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Style and abstraction in portrait sketching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mahler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hodgins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">55</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Markov weight fields for face sketch synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Y K</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1091" to="1097" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep graphical feature learning for face sketch synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Sixth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3574" to="3580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Face photo-sketch synthesis and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1955" to="1967" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networkss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Computer Vision (ICCV</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<title level="m">Deep residual learning for image recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Face sketch synthesis and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="687" to="694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A nonlinear approach for face sketch synthesis and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern recognition</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1005" to="1010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Lighting and pose robust face sketch synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="420" to="433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.01911</idno>
		<title level="m">Random sampling for fast face sketch synthesis</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">End-to-end photo-sketch generation via fully convolutional representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th ACM on International Conference on Multimedia Retrieval (ICMR</title>
		<meeting>the 5th ACM on International Conference on Multimedia Retrieval (ICMR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="627" to="634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Content-adaptive sketch portrait generation by decompositional representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Izquierdo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing (TIP)</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="328" to="339" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Face sketch synthesis with style transfer using pyramid column feature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Y K</forename><surname>Wong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>IEEE Winter Conference on Applications of Computer Vision</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Data-driven vs. model-driven: Fast face sketch synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.00899</idno>
		<title level="m">Composition-aided sketch-realistic portrait generation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Combining markov random fields and convolutional neural networks for image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2479" to="2486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Least squares generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Y</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Smolley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2813" to="2821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kaur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Dana</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.04306</idno>
		<title level="m">Photo-realistic facial texture transfer</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The AR face database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVC Tech. Report</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Xm2vtsdb: The extended m2vts database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Messer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jonsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Second International Conference on Audio and Video-based Biometric Person Authentication</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="72" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Coupled information-theoretic encoding for face photo-sketch recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10182</idno>
		<title level="m">High-quality facial photo-sketch synthesis using multi-adversarial networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Back projection: An effective postprocessing method for gan-based face sketch synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Structure-preserving image smoothing via region covariances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Karacan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Erdem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Erdem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">176</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.04802</idno>
		<title level="m">Photo-realistic single image superresolution using a generative adversarial network</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fsim: A feature similarity index for image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="2378" to="2386" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A new lda-based face recognition system which can solve the small sample size problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Y M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1713" to="1726" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
