<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Segmental Spatiotemporal CNNs for Fine-grained Action Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Lea</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Reiter</surname></persName>
							<email>areiter@cs.</email>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">René</forename><surname>Vidal</surname></persName>
							<email>rvidal@cis.</email>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
							<email>hager@cs.jhu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Segmental Spatiotemporal CNNs for Fine-grained Action Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Joint segmentation and classification of fine-grained actions is important for applications of human-robot interaction, video surveillance, and human skill evaluation. However, despite substantial recent progress in large-scale action classification, the performance of stateof-the-art fine-grained action recognition approaches remains low. We propose a model for action segmentation which combines low-level spatiotemporal features with a high-level segmental classifier. Our spatiotemporal CNN is comprised of a spatial component that uses convolutional filters to capture information about objects and their relationships, and a temporal component that uses large 1D convolutional filters to capture information about how object relationships change across time. These features are used in tandem with a semi-Markov model that models transitions from one action to another. We introduce an efficient constrained segmental inference algorithm for this model that is orders of magnitude faster than the current approach. We highlight the effectiveness of our Segmental Spatiotemporal CNN on cooking and surgical action datasets for which we observe substantially improved performance relative to recent baseline methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>New spatiotemporal feature representations <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> and massive datasets like Ac-tivityNet <ref type="bibr" target="#b2">[3]</ref> have catalyzed progress towards large-scale action recognition in recent years. In large-scale action recognition, the goal is to classify diverse actions like skiing and basketball, so it is often advantageous to capture contextual cues like the background appearance. In sharp contrast, in fine-grained action recognition, background appearance cues are insufficient to capture the nuances of a complex action, such as subtle changes in object location or state. As a consequence, progress on fine-grained action recognition has been comparatively modest despite active recent developments (e.g. <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>).</p><p>In this paper we propose a new approach to fine-grained action recognition that aims to capture information about object states, their relationships, and how they change over time. Our goal is to temporally segment a video and to classify each of its constituent actions. We target goal-driven activities performed in a situated environment, like a kitchen, where a static camera captures a user who performs dozens of actions. For concreteness, refer to the sub-sequence arXiv:1602.02995v4 [cs.CV] 30 Sep 2016 depicted in <ref type="figure" target="#fig_0">Figure 1</ref>: A user places a tomato onto a cutting board, cuts it with a knife, and places it into a salad bowl. This is part of a much longer salad preparation sequence. There are many applications of this task including in industrial manufacturing <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>, surgical training <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>, and general human activity analysis (e.g. cooking, sports) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref>.</p><p>To address this problem, we introduce a Spatiotemporal Convolutional Neural Network (ST-CNN), which encodes low-and mid-level visual information, and a semi-Markov model that models high-level temporal information. The spatial component of the ST-CNN is a variation on VGG <ref type="bibr" target="#b17">[18]</ref> designed for finegrained tasks which we empirically find captures information about object locations, their states (e.g. whole tomato vs. diced tomato), and inter-object relationships. Our network is smaller than models like VGG <ref type="bibr" target="#b17">[18]</ref> and AlexNet <ref type="bibr" target="#b18">[19]</ref> and induces more spatial invariance. This model diverges from recent fine-grained models, which typically use holistic approaches to model the scene.</p><p>The temporal component of the ST-CNN captures how object relationships change over the course of an action. In the tomato cutting example the cut action changes the tomato's state from whole to diced and the place action requires moving the tomato from location cutting board to bowl. The ST-CNN applies a set of shared temporal 1D convolutional filters to the output of the spatial component. The temporal filters are on the order of 10 seconds long and explicitly capture mid-range motion patterns. The output of these filters is then fed to a linear classifier that computes an action activation score. The probability of an action at any given time is then estimated by applying a softmax to the action activation scores.</p><p>The segmental component jointly segments and classifies actions using a semi-Markov model that models pairwise transitions between action segments. This model offers two benefits over traditional time series models like linear chain Conditional Random Fields (CRFs) and Recurrent Neural Networks (RNNs). Features are computed segment-wise, as opposed to per-frame, and the action at each segment is conditioned on the previous segment instead of the previous frame. Typically, inference in semi-Markov models is of much higher computa-tional complexity than their frame-wise alternatives. In this work we introduce a new constrained inference algorithm that is one to three orders of magnitude faster than the common semi-Markov inference technique.</p><p>Despite a large number of action recognition datasets in the computer vision community, few are sufficient for modeling fine-grained segmentation and classification. We apply our approach to two datasets: University of Dundee 50 Salads <ref type="bibr" target="#b20">[21]</ref>, which is in the cooking domain, and the JHU-ISI Surgical Assessment Working Set (JIGSAWS) <ref type="bibr" target="#b10">[11]</ref>, which is in the surgical robotics domain. Both of these datasets have reasonable amounts of data, interesting task granularity, and realistic task variability. On these datasets, our model substantially outperforms popular methods such as Dense Trajectories, spatial CNNs, and RNNs with Long Short Term Memory (LSTM).</p><p>In summary, our contributions are:</p><p>-We develop a Spatiotemporal CNN which we empirically find captures information about object relationships and how relationships change over time, -We introduce an efficient algorithm for segmental inference that is one to three orders of magnitude faster than the common approach, -We substantially outperform recent methods for fine-grained recognition on two challenging datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Holistic Features: Holistic methods using spatiotemporal features with a bag of words representation are standard for large-scale <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref> and fine-grained <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25]</ref> action analysis. The typical baseline represents a given clip using Improved Dense Trajectories (IDT) <ref type="bibr" target="#b0">[1]</ref> with a histogram of dictionary elements <ref type="bibr" target="#b3">[4]</ref> or a Fisher Vector encoding <ref type="bibr" target="#b0">[1]</ref>. Dense Trajectories concatenate HOG, HOF, and MBH texture descriptors extracted along optical flow trajectories to characterize small spatiotemporal patches. Empirically they perform well on large-scale tasks, in part because of their ability to capture background detail (e.g. sport arena versus mountaintop). However, for fine-grained tasks the image background is often constant so it is more important to model objects and their relationships. These are typically not modeled in holistic approaches. Furthermore, the typical image patch size for IDT (neighborhood=32px, cell size=2px) is too small to extract high-level object information.</p><p>Large-scale Action Classification: Recent efforts to extend CNN models to video <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref> improve over holistic methods by encoding spatial and temporal information. However, results from these models are often only superior when CNN features are concatenated with IDT features <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref>. Furthermore, performance using some of these spatiotemporal CNNs (e.g. <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b29">30]</ref>) is only marginally better than their spatial-only counterparts or the IDT baselines. Our approach is similar in that we propose a spatiotemporal CNN, but our temporal filters are applied in 1D and are much longer in duration.</p><p>From Large-scale Detection to Fine-grained Segmentation: Despite success in classification, large-scale approaches are inadequate for tasks like action localization and detection which are more similar to fine-grained segmentation.</p><p>In the 2015 THUMOS large-scale action recognition challenge 1 , the top team fused IDT and CNN approaches to achieve 70% mAP on classification. However, the top method only achieved 18% (overlap ≥ 0.5) for localization. Heilbron et al. <ref type="bibr" target="#b2">[3]</ref> found similar results on ActivityNet with 11.9% (overlap ≥ 0.2). This suggests that important methodological changes are necessary for identifying and localizing actions regardless of fine-grained or large-scale.</p><p>Moving to fine-grained recognition, recent work has combined holistic methods with human pose or object detection. On MPII Cooking, Rohrbach et al. <ref type="bibr" target="#b3">[4]</ref> combine IDT with pose features to get a detection score of 34.5% compared to 29.5% without pose features. Cheron et al. <ref type="bibr" target="#b6">[7]</ref> show that if temporal segmentation on MPII is known then CNN-based pose features achieve 71.4% mAP. While this performance is comparatively high, classification is a much easier problem than detection. Object-centric methods (e.g. <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">8]</ref>), first detect the identity and location of objects in an image. Ni et al. <ref type="bibr" target="#b7">[8]</ref> achieve 54.3% mAP on MPII Cooking and 79% on the ICPR 2012 Kitchen Scene Context-based Gesture Recognition dataset. While performance is state of the art, their method requires learning object models from a large number of manual annotations. In our work we learn a latent object representation without object annotations. Lastly, on Georgia Tech Egocentric Activities, Li et al. <ref type="bibr" target="#b5">[6]</ref> use object, egocentric, and hand features to achieve 66.8% accuracy for action classification versus an IDT baseline of 39.8%. Their features are similar to IDT but they use a recent hand-detection method to find the regions of most importance in each image.</p><p>Temporal Models: Several papers have used Conditional Random Fields for action segmentation and classification (e.g. <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref>). CRFs offer a principled approach for combining multiple energy terms like segment-wise unaries and pairwise action transitions. Most of these approaches have been applied to simpler activities like recognizing walking versus bending versus drawing <ref type="bibr" target="#b30">[31]</ref>. In each of the listed cases, segments are modeled with histograms of holistic features. In our work segments are modeled using spatiotemporal CNN activations.</p><p>Recently, there has been significant interest in RNNs, specifically those using LSTM (e.g. <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref>). RNNs with LSTM use gating mechanisms to implicitly learn how latent states transition within and between actions. While their performance is often impressive, they are black box models that are hard to interpret. In contrast, the temporal component of our CNN explicitly learns how latent states transition and is easy to interpret and visualize. It is more similar to models in speech recognition (e.g. <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref>), which learn phonemes using 1D convolutional filters, or in robotics, which learn sensor-based action primitives <ref type="bibr" target="#b36">[37]</ref>. For completeness we compare our model with LSTM-based RNNs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Spatiotemporal CNN Model</head><p>In this section we introduce the spatial and temporal components of our ST-CNN. The input is a video including a color image and a motion image for each frame. The output is a vector of action probabilities at every frame. <ref type="figure" target="#fig_1">Figure 2</ref> (left) depicts the full Segmental Spatiotemporal model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Spatial Component</head><p>In this section, we introduce a CNN topology inspired by VGG <ref type="bibr" target="#b17">[18]</ref> that uses hierarchical convolutional filters to capture object texture and spatial location. First we introduce the mathematical framework, as depicted in <ref type="figure" target="#fig_1">Figure 2</ref> (right), and then highlight differences between our approach and other CNNs. For a recent introduction to CNNs see <ref type="bibr" target="#b37">[38]</ref>.</p><p>For each time t there is an image pair I t = {I c t , I m t }, where I c t is a color image and I m t is a Motion History Image <ref type="bibr" target="#b38">[39]</ref>. The motion image captures when an object has moved into or out of a region and is computed by taking the difference between frames across a 2 second window. Other work (e.g. <ref type="bibr" target="#b26">[27]</ref>) has shown success using optical flow as a motion image. We found optical flow to be insufficient for capturing small hand motions and noisy due to the video compression.</p><p>The image pair I t is fed into a CNN with L spatial units, each of which is composed of a convolutional layer with F l filters of size 3 × 3, a Rectified Linear Unit (ReLU), and 3 × 3 max pooling. The output of each unit is r</p><formula xml:id="formula_0">l = {r l i } R l i=1 , where r l i ∈ R F l ,</formula><p>is the activation vector for a specific region in an image. For an N l × N l grid there are R l = N 2 l regions as depicted by the colored blocks in <ref type="figure" target="#fig_1">Figure 2</ref> (right).</p><p>The output of the L spatial units is fed into a fully connected layer which has F f c states that capture relationships between regions and their corresponding latent object representations. For example, a state may produce a high score for tomato in the region with the cutting board and knife in the region next to  </p><formula xml:id="formula_1">it. The state h ∈ R F f c is a function of weights W (0) ∈ R F f c ×F L R L and biases b (0) ∈ R F f c where r L ∈ R F L R L is</formula><formula xml:id="formula_2">h = ReLU(W (0) r L + b (0) ).<label>(1)</label></formula><p>The above spatial component, when applied to frame t, produces state vector h t . Ideally, the spatial and temporal components of our CNN should be trained jointly, but this requires an exorbitant amount of GPU memory. Therefore, we first train the spatial model and then train the temporal model. As such, we train the spatial component with auxiliary labels, z. Let z t ∈ {0, 1} C , where C is the number of classes, be the ground truth action label for each time step. We predict the probability,ẑ t ∈ [0, 1] C , of each action class at that frame using the softmax function:ẑ</p><formula xml:id="formula_3">t = softmax(W (1) h t + b (1) ).<label>(2)</label></formula><p>where W (1) ∈ R C×F fc and b (1) ∈ R C . Note,ẑ t is computed solely for the purpose of training the spatial component. The input to the temporal component is h t . <ref type="figure" target="#fig_2">Figure 3</ref> shows example CNN activations after each spatial unit. The top row shows the sum of all filter activations after that layer and the bottom row shows the color corresponding to the best scoring filter at that location. We find that these filters are similar to mid-level object detectors. Notice the relevant objects in the image and the regions corresponding to the action all have high activations and different best-scoring filters. Relationships to other CNNs: Our network is inspired by models like VGG and AlexNet but differs in important ways. Like VGG, we employ a sequence of spatial units with common parameters like filter size. However, we found that using two consecutive convolution layers in each spatial unit has negligible impact on performance. Normalization layers, like in AlexNet, did not improve performance either. Overall our network is shallower, has fewer spatial regions, and contains only one fully connected layer. In addition, common data-augmentation techniques, including image rotation and translation, introduce unwanted spatial and rotational invariances, which have a negative impact on our performance.</p><p>We performed cross validation using one to seven spatial units and grid sizes from 1 × 1 to 9 × 9 and found three spatial units with a 3 × 3 grid achieved the best results. By contrast, for image classification, deep networks tend to use at least four spatial units and have larger grid counts. VGG uses a 7 × 7 grid and AlexNet uses a 12 × 12 grid. A low spatial resolution naturally induces more spatial invariance, which is useful when there is limited amounts of training data. To contrast, if the grid resolution is larger, more training data is necessary to capture all object configurations. We compare the performance of our model with a pre-trained VGG network in the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Temporal Component</head><p>Temporal convolutional filters capture how the scene changes over the course of an action. These filters capture properties like the scene configuration at the beginning or end of an action and different ways users perform the same action.</p><p>For video duration T , let h = {h t } T t=1 be the set of spatial features and y t ∈ {1, ..., C} be an action label at time t. We learn F e temporal filters W (2) = {W</p><formula xml:id="formula_4">(2) 1 , . . . , W (2) Fe } with biases b (2) = {b (2) 1 , . . . , b<label>(2)</label></formula><p>Fe } shared across actions. Each filter is of duration d such that W (2) i ∈ R d×F f c . The activation for the i-th filter at time t is given by a 1D convolution between the spatial features h and the temporal filters using a ReLU non-linearity:</p><formula xml:id="formula_5">a t,i = ReLU( d t =1 W (2) i,t h t+d−t + b (2) i ).<label>(3)</label></formula><p>A score vector s t ∈ R C is a function of weight vectors W (3) ∈ R C×Fe and biases b (3) ∈ R C with the softmax function:</p><formula xml:id="formula_6">s t = softmax(W (3) a t + b (3) ).<label>(4)</label></formula><p>We choose filter lengths spanning 10 seconds of video. This is much larger than in related work (e.g. <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b29">30]</ref>). Qualitatively, we found these filters capture states, transitions between states, and attributes like action duration. In principle, we could create a deep temporal model. Multiple layers did not improve performance in preliminary experiments, however, it is worth further exploration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Learning</head><p>We learn parameters W = {W 0 , W 1 , W 2 , W 3 }, b = {b 0 , b 1 , b 2 , b 3 }, and the convolutional filters with the cross entropy loss function. We minimize the spatial and temporal network losses independently using ADAM <ref type="bibr" target="#b39">[40]</ref>. Dropout regularization is used on fully connected layers. Parameters such as grid size, number of filters, and non-linearity functions are determined from cross validation using one split each from the datasets described later. We use F = {64, 96, 128} filters in the three corresponding spatial units and F f c = 256 fully connected states. We used Keras <ref type="bibr" target="#b40">[41]</ref>, a library of deep learning tools, to implement our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Segmental Model</head><p>We jointly segment and classify actions with a variation on a semi-Markov model that uses activations from the ST-CNN and a pairwise term to capture segmentwise action-to-action transitions. Similar models have been proposed by Sarawagi and Cohen <ref type="bibr" target="#b19">[20]</ref> for Semi-Markov CRFs, by Shi et al. <ref type="bibr" target="#b30">[31]</ref> for discriminative semi-Markov models and by Pirsiavash and Ramanan <ref type="bibr" target="#b22">[23]</ref> for Segmental Regular Grammars. We show a more efficient inference algorithm made possible by reformulating the traditional segmental inference problem.</p><p>We start with notation equivalent to Sarawagi and Cohen <ref type="bibr" target="#b19">[20]</ref>. Let tuple P j = (y j , t j , d j ) be the jth action segment, where y j is the action label, t j is the start time, and d j is the segment duration. There is a sequence of M segments P = {P 1 , ..., P M }, 0 &lt; M ≤ T , such that the start of segment j coincides with the end of the previous segment, i.e. t j = t j−1 + d j−1 , and the durations sum to the total time M i=1 d i = T . Given scores S = {s 1 , . . . , s T }, we infer segments P that maximize total score E(S, P ) for the video using segment function f (·):</p><formula xml:id="formula_7">E(S, P ) = M j=1 f (S, y j−1 , y j , t j , d j ).<label>(5)</label></formula><p>Our segment function is the sum of ST-CNN scores plus the transition score for going from action y j−1 at segment j − 1 to action y j at segment j:</p><formula xml:id="formula_8">f (S, y j−1 , y j , t j , d j ) = A yj−1,yj + tj +dj −1 t=tj s t,yj<label>(6)</label></formula><p>The entries of A ∈ R C×C are estimated from the training data as the log of the probabilities of transitioning from one action to another.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Segmental Inference</head><p>The traditional semi-Markov inference method solves the following discrete optimization problem</p><formula xml:id="formula_9">max P ∈P E(S, P ) s.t. t j = t j−1 + d j−1 ∀j and M i=j d j = T.<label>(7)</label></formula><p>where P is the set of all segmentations. The optimal labeling is typically found using an extension of the Viterbi algorithm to the semi-Markov case, which we refer to as Segmental Viterbi <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b22">23]</ref>. The algorithm recursively computes the score V t,c for the best labeling whose last segment ends at time t and is assigned class c:</p><formula xml:id="formula_10">V t,c = max d∈{1...D} c ∈Y\c V t−d,c + f (S, c , c, t − d, d).<label>(8)</label></formula><p>The optimal labels are recovered by backtracking through the matrix V using the predicted segment durations. This approach is inherently frame-wise: for each frame, compute scores for all possible segment durations, current labels, and previous labels. This results in an algorithm of complexity O(T 2 C 2 ), in the naive case, because the duration of each segment ranges from 1 to T . If the segment duration is bounded then complexity is reduced to O(T DC 2 ), where D is the maximum segment duration <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>To further accelerate the computation of the optimal labels, we introduce an alternative approach in which we constrain the number of segments, M , by an upper bound, K, such that 0 &lt; M ≤ K. If K = T , this is equivalent to that of the previous problem. Furthermore, we remove the duration variables d j , which are redundant given all times t j , and simplify the segment notation to beP j = (y j , t j ). Now, instead of adding constraints on the durations of each segment, we only require that the start of the jth segment comes after segment j − 1. We solve the problem max M ∈{1,...,K}</p><formula xml:id="formula_11">P ∈P M E(S,P ) s.t. t j−1 &lt; t j ∀j ∈ {1, . . . , M }<label>(9)</label></formula><p>whereP M is the set of all segmentations with M segments. The d found in function f (·) is now simply t j − t j−1 . As a byproduct, this formulation prevents gross over-segmentations of the data and thus improves performance. We assume the score for each segment is the sum of scores over each frame, so the total score for the segmentation containing k segments can be recursively computed using the scores for a segmentation containing (k−1) segments. Specifically, we first compute the best segmentation assuming M = 1 segments, then compute the best segmentation for M = 2 segments, up to M = K segments. LetV k t,c be the score for the best labeling with k segments ending in class c at time t:V k t,c = max max</p><formula xml:id="formula_12">c∈Y\c (V k−1 t−1,c + A c ,c ),V k t−1,c + s t,c .<label>(10)</label></formula><p>This recursion contains two cases: (1) if transitioning into a new segment (c = c), use the best incoming score from the previous segment k − 1 at t − 1 and <ref type="formula" target="#formula_3">(2)</ref> if staying in the same segment (c = c), use the score from the current segment at t − 1. Our forward pass, in which we compute each scoreV k t,c , is shown in Algorithm 1. The optimal labeling is found by backtracking throughV .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Our Semi-Markov Forward Pass</head><formula xml:id="formula_13">for k = 1 : K do for t = k : T do for c = 1 : C do vcur =V k t−1,c vprev = max c ∈Y\cV k−1 t−1,c + A c ,c V k t,c = max(vcur, vprev) + st,c</formula><p>The complexity of our algorithm, O(KT C 2 ), is D K times more efficient than Segmental Viterbi assuming K &lt; D. In most practical applications, K is much smaller than D. In the evaluated datasets there is a speedup of one to three orders of magnitude. Note, however, our method requires K times more memory than Segmental Viterbi. Ours has space complexity O(KT C), whereas Segmental Viterbi has complexity of O(T C). Typically K T so the increase in memory is easily manageable on any modern computer. In all cases, we set K based on the maximum number of segments in the training split.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Setup</head><p>Historically, most action recognition datasets were developed for classifying individual actions using pre-trimmed clips. Recent datasets for fine-grained recognition have been developed to classify many actions, however they often contain too few users or an insufficient amount of data to learn complex models. MPII Cooking <ref type="bibr" target="#b3">[4]</ref> has a larger number of videos but some actions are rarely performed. Specifically, seven actions are performed fewer than ten times each. Furthermore there is gratuitous use of a background class because it was labeled for (sparse) action detection instead of (dense) action segmentation. Georgia Tech Egocentric Activities <ref type="bibr" target="#b41">[42]</ref> has 28 videos across seven tasks. Unfortunately, the actions in each task are independent thus there are only three videos to train on and one for evaluation. Furthermore the complexities of egocentric video are beyond the scope of this work. We use datasets from the ubiquitous computing and surgical robotics communities which contain many instances of each action. University of Dundee 50 Salads: Stein and McKenna introduced 50 Salads <ref type="bibr" target="#b20">[21]</ref> for evaluating fine-grained action recognition in the cooking domain. We believe this dataset provides great value to the computer vision community due to the large number of action instances per class, the high quality labels, plethora of data, and multi-modal sensors (RGB, depth, and accelerometers).</p><p>This dataset includes 50 instances of salad preparation, where each of the 25 users makes a salad in two different trials. Videos are annotated at four levels of granularity. The coarsest level ("high") consists of labels cut and mix ingredients, prepare dressing, and serve salad. At the second tier ("mid") there are 17 fine-grained actions like add vinegar, cut tomato, mix dressing, peel cucumber, place cheese into bowl, and serve salad. At the finest level ("low") there are 51 actions indicating the start, middle, and end of the previous 17 actions. For each granularity there is also a background class.</p><p>A fourth granularity ("eval"), suggested by <ref type="bibr" target="#b20">[21]</ref>, consolidates some objectspecific actions like cutting a tomato and cutting a cucumber into object-agnostic actions like cutting. Actions include add dressing, add oil, add pepper, cut, mix dressing, mix ingredients, peel, place, serve salad on plate, and background. These labels coincide with the tools instrumented with accelerometers.</p><p>JHU-ISI Gesture and Skill Assessment Working Set (JIGSAWS): JIGSAWS <ref type="bibr" target="#b10">[11]</ref> was developed for recognizing actions in robotic surgery training tasks like suturing, needle passing, and knot tying. In this work we evaluate using the suturing task, which includes 39 trials of synchronized video and robot kinematics data collected from a daVinci medical robot. The video is captured from an overhead endoscopic camera and depicts two tools and the training task apparatus. The suturing task consists of 10 fine-grained actions such as insert needle into skin, tie a knot, transfer needle between tools, and drop needle at finish. Videos last about two minutes and contain 15 to 37 action instances per video. Users perform low-level actions in significantly different orders. We evaluate using Leave One User Out as described in <ref type="bibr" target="#b10">[11]</ref>. Most prior work on this dataset focuses on the kinematics data which consists of positions, velocities, and robot joint information. We compare against the video-based results of Tao et al. <ref type="bibr" target="#b12">[13]</ref>, which uses holistic features with a Markov Semi-Markov CRF.</p><p>Metrics: We evaluate on segmental and frame-wise metrics as suggested by <ref type="bibr" target="#b36">[37]</ref> for the 50 Salads dataset. The first measures segment cohesion and the latter captures overall coverage.</p><p>The segmental metric evaluates the ordering of actions but not the specific timings. The motiviation is that in many applications there is high uncertainty in the location of temporal boundaries. For example, different annotators may have different interpretations of when an action starts or ends. As such, the precise location of temporal boundaries may be inconsequential. This score, A edit (P, P * ), is computed using the Levenshtein distance, which is a function of segment insertions, deletions, and substitutions <ref type="bibr" target="#b42">[43]</ref>. Let the ground truth segments be P = {P 1 , . . . , P M } and predicted segments be P * = {P * 1 , . . . , P * N }. The number of edits is normalized by the maximum of M and N . For clarity we show the score (1 − A edit (P, P * )) × 100, which ranges from 0 to 100.</p><p>Frame-wise accuracy measures the percentage of correct frames in a sequence. Let y = {y 1 , . . . , y T } be the true labels and y * = {y * 1 , . . . , y * T } be the predicted labels. The score is a function of each frame: A acc (y, y * ) = 1 T T t=1 1(y t = y * t ). We also include action classification results which assume temporal segmentation is known and compare with the video-based results from Zappella et al. <ref type="bibr" target="#b11">[12]</ref>. These use the accuracy metric applied to segments instead of individual frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baselines:</head><p>We evaluate two spatial baselines on both datasets using IDT and a pre-trained VGG network, and one temporal baseline using an RNN with LSTM. For the classification results, the (known) start and end times are fed into the segmental model to predict each class.    The IDT baseline is comparable to Rohrbach et al. <ref type="bibr" target="#b3">[4]</ref> on the MPII dataset. We extract IDT, create a KMeans dictionary (k = 2000), and aggregate the dictionary elements into a locally normalized histogram with a sliding window of 30 frames. We only use one feature type, HOG, because it outperformed all other feature types or their combination. This may be due to the large dimensionality of IDT and relatively low number of samples from our training sets. Note that it took 18 hours to compute IDT features on 50 Salads compared to less than 5 hours for the CNN features using an Nvidia Titan X graphics card.</p><p>For our spatial-only results, we classify the action at each time step with a linear Support Vector Machine using the features from IDT, VGG, or our spatial CNN. These results highlight how effective each model is at representing the scene and are not meant to be state of the art. The CNN baseline uses the VGG network <ref type="bibr" target="#b17">[18]</ref> pretrained on Imagenet. We use the activations from FC6, the first of VGG's three fully connected layers, as the features at each frame.</p><p>In addition we compare our temporal model to an RNN with LSTM using our spatial CNN as input. The LSTM baseline was implemented in Keras and uses one LSTM layer with 64 latent states. <ref type="table" target="#tab_1">Tables 1 and 2</ref> show performance using IDT, VGG, LSTM, and our models and <ref type="figure" target="#fig_4">Figure 4</ref> shows example predictions on each dataset. S-CNN, ST-CNN, and ST-CNN + Seg refer to the spatial, spatiotemporal, and segmental components of Spatial Model: Our results are consistent with the claim that holistic methods like IDT are insufficient for fine-grained action segmentation. Interestingly, we also see that the VGG results are also relatively poor, which could be due to the data augmentation to train the model. While our results are still insufficient for many practical applications the accuracy of our spatial model is at least 12% better than IDT and 21% better than VGG on both datasets. Note that the edit score is very low for all of these models. This is not surprising because each model only uses local temporal information, which results in many oscillations in predictions, as shown in <ref type="figure" target="#fig_4">Figure 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results &amp; Discussion</head><p>Many actions in 50 Salads, like cutting, require capturing small hand motions. We visualized IDT 3 and found it does not detect many tracklets for these actions. In contrast, when the user places ingredients in the bowl IDT generates thousands of tracklets. We found this to be problematic despite the fact that the IDT features are normalized. Qualitatively found our model is better at capturing details necessary for finer motions, as shown in <ref type="figure" target="#fig_2">Figure 3</ref>.</p><p>Temporal Model: The spatiotemporal model (ST-CNN) outperforms the spatial model (S-CNN) on both datasets. The effect on edit score is substantial and likely due to the large temporal filters. Aside from modeling temporal evolution these have a byproduct of smoothing out predictions. By visualizing these features we see they tend to capture different phases of an action like the start or finish. In contrast, while LSTM substantially improves edit score over the spatial model it has a negligible impact on accuracy. LSTM is capable of learning how actions transition across time, however, it does not appear that it sufficiently captures this information. Due to the complex nature of this method, we were not able to visualize the internal parameters in a meaningful way.</p><p>Segmental Model: The segmental model provides a notable improvement on JIGSAWS but only a modest improvement on 50 Salads. By visualizing the results we see that the segmental model helps in some cases and hurts in others. For example, when the predictions oscillate (like in <ref type="figure" target="#fig_4">Figure 4 (right)</ref>) the segmental model provides a large improvement. However, sometimes the model smooths over actions that are short in duration. Future work should look at incorporating additional cues such as duration to better model each action class. Action Granularity: <ref type="table" target="#tab_2">Table 3</ref> shows performance on all four action granularities from 50 Salads using our full model. Columns 3 and 4 show scores for segmental and frame-wise metrics on the action segmentation task and the last shows action classification accuracies assuming known temporal segmentation. While performance decreases as the number of classes increases, results degrade sublinearly with each additional class. Some errors at the finer levels are likely due to temporal shifts in the predictions. Given the high accuracy at the coarser levels, future work should look at hierarchical modeling of finer granularities. Other Results: Lea et al. <ref type="bibr" target="#b36">[37]</ref> achieved an edit score of 58.46% and accuracy of 81.75% using the instrumented kitchen tools on 50 Salads. They also achieved state of the art performance on JIGSAWS with 78.91% edit and 83.45% accuracy. These results used domain-specific sensors which are well suited to each application but may not be practical for real-world deployment. To contrast, video is much more practical for deployment but is more complicated to model. Therefore, we should not expect to achieve as high performance from video.</p><p>Our classification accuracy on JIGSAWS is 90.47%. This is notably higher than the state of the art <ref type="bibr" target="#b11">[12]</ref>, which achieved 81.17% using a video-based linear dynamical system model and also better than their hybrid approach using video and kinematics, which achieved 86.56%. For joint segmentation and classification the improvement over the state of the art <ref type="bibr" target="#b12">[13]</ref> is modest. These surgical actions can be recognized well using position and velocity information <ref type="bibr" target="#b36">[37]</ref>, thus our ability to capture object relationships may be less important on this dataset. Speedup: <ref type="table" target="#tab_3">Table 4</ref> shows the speedup of our inference algorithm compared to Segmental Viterbi on all 50 Salads and JIGSAWS label sets. One practical implication is that our algorithm scales readily to full-length videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper we introduced a segmental spatiotemporal CNN that substantially outperforms popular methods like Dense Trajectories, pre-trained spatial CNNs, and temporal models like RNNs with LSTM. Furthermore, our approach takes less time to compute features than IDT, less time to train than LSTM, and performs inference more efficiently than traditional Segmental methods. Acknowledgments: This work was funded in part by grants NIH R01HD87133-01, ONR N000141310116, and an Intuitive Surgical Research Grant.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Our model captures object relationships and how these relationships change temporally. (top) Latent hand and tomato regions are highlighted in different colors on images from the 50 Salads dataset. (bottom) We evaluate on multiple label granularities that model fine-grained or coarse-grained actions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>(left) Our model contains three components. The spatial, temporal, and segmental units encode object relationships, how those relationships change, and how actions transition from one to another. (right) The spatial component of our model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>The user is chopping vegetables. The top images show the best filter activations after each convolutional unit from the CNN. The activations around the cutting board and bowl are high (yellow) whereas in unimportant regions are low (black/red). The bottom images indicate which filter gave the highest activation for each region. Each color corresponds to a different filter index.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>the concatenation of activations in all regions after the Lth spatial unit: 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>The plots on top depict the ground truth action predictions for a given video. Each color corresponds to a different class label. Subsequent rows show predictions using VGG, S-CNN, ST-CNN, and ST-CNN + Seg. our model. Our full model has 27.8% better accuracy on 50 Salads and 37.6% better accuracy on JIGSAWS relative to the IDT baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>JIGSAWS</figDesc><table><row><cell cols="3">Labels Classes Edit Acc. Classif.</cell></row><row><cell>Low</cell><cell>52</cell><cell>29.30 44.13 39.67</cell></row><row><cell>Mid</cell><cell>18</cell><cell>48.94 58.06 63.49</cell></row><row><cell>Eval</cell><cell>10</cell><cell>62.06 72.00 86.63</cell></row><row><cell>High</cell><cell>4</cell><cell>83.2 92.43 95.14</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>50 Salads Granularity Analysis</figDesc><table><row><cell>Labels</cell><cell cols="3">Dur #Segs Speedup</cell></row><row><cell>Low</cell><cell>2289</cell><cell>65</cell><cell>35x</cell></row><row><cell>Mid</cell><cell>3100</cell><cell>25</cell><cell>124x</cell></row><row><cell>Eval</cell><cell>3100</cell><cell>24</cell><cell>129x</cell></row><row><cell>High</cell><cell>11423</cell><cell>6</cell><cell>1902x</cell></row><row><cell cols="2">JIGSAWS 1107</cell><cell>37</cell><cell>30x</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Speedup Analysis</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">THUMOS Challenge: http://www.thumos.info/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">For notational clarity we denote all weight matrices as W (·) and bias vectors b (·) to reduce the number of variables.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Visualization was performed using the public software from Wang et al.<ref type="bibr" target="#b0">[1]</ref>.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Human action recognition using factorized spatio-temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">ActivityNet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carlos Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Recognizing fine-grained and composite activities using hand-centric features and script data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Regneri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pinkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Modeling actions through state changes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Delving into egocentric actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">P-cnn: Pose-based cnn features for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multiple granularity analysis for finegrained action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>Paramathayalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Moulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">From stochastic grammar to bayes network: Probabilistic parsing of complex activity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">N</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Bobick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Anticipating human actions for collaboration in the presence of task and sensor uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">N</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Bobick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Vedula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Reiley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahmidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Varadarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zappella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Béjar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Yuh</surname></persName>
		</author>
		<title level="m">JHU-ISI Gesture and Skill Assessment Working Set (jigsaws): A surgical activity dataset for human motion modeling. MICCAI Workshop: Modeling and Monitoring of Computer Assisted Interventions</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Surgical gesture classification from video and kinematic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zappella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">B</forename><surname>Haro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Surgical gesture segmentation and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zappella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Computing and Computer Assisted Intervention (MICCAI)</title>
		<imprint>
			<biblScope unit="page" from="339" to="346" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fine-grained kitchen activity recognition using RGB-D</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conference on Ubiquitous Computing (UbiComp</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi-agent event recognition in structured scenarios</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning human activities and object affordances from rgb-d videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<date type="published" when="2013" />
			<publisher>IJRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Activity recognition using semimarkov models on real world smart home datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Van Kasteren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Englebienne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Kröse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Ambient Intelligence and Smart Environments</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<editor>Pereira, F., Burges, C., Bottou, L., Weinberger, K.</editor>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semi-markov conditional random fields for information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sarawagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Combining embedded accelerometers with computer vision for recognizing food preparation activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Mckenna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conference on Ubiquitous Computing (UbiComp)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Action Recognition by Dense Trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kläser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Parsing videos of actions with segmental grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">What do 15,000 object categories tell us about classifying and localizing actions?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fine-grained activity recognition with holistic and pose based features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition (GCPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Largescale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Encoding feature maps of cnns for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, THUMOS Challenge</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Human action segmentation and recognition using discriminative semi-markov models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning latent temporal structure for complex event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Deep speech: Scaling up end-to-end speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<idno>abs/1412.5567</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Exploring convolutional neural network structures and optimization techniques for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Abdel-Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">INTERSPEECH International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning convolutional action primitives for finegrained action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Deep learning. Book in preparation for</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">B</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The representation and recognition of action using temporal templates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bobick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Keras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://github.com/fchollet/keras" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Understanding egocentric activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A guided tour to approximate string matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Navarro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
