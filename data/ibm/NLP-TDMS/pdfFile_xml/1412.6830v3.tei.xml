<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LEARNING ACTIVATION FUNCTIONS TO IMPROVE DEEP NEURAL NETWORKS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Forest</forename><surname>Agostinelli</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of California -Irvine Irvine</orgName>
								<address>
									<postCode>92697</postCode>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Hoffman</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Adobe Research San Francisco</orgName>
								<address>
									<postCode>94103</postCode>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Sadowski</surname></persName>
							<email>peter.j.sadowski@uci.edu</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of California -Irvine Irvine</orgName>
								<address>
									<postCode>92697</postCode>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Baldi</surname></persName>
							<email>pfbaldi@uci.edu</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of California -Irvine Irvine</orgName>
								<address>
									<postCode>92697</postCode>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">LEARNING ACTIVATION FUNCTIONS TO IMPROVE DEEP NEURAL NETWORKS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Accepted as a workshop contribution at ICLR 2015</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Artificial neural networks typically have a fixed, non-linear activation function at each neuron. We have designed a novel form of piecewise linear activation function that is learned independently for each neuron using gradient descent. With this adaptive activation function, we are able to improve upon deep neural network architectures composed of static rectified linear units, achieving state-of-theart performance on CIFAR-10 (7.51%), CIFAR-100 (30.83%), and a benchmark from high-energy physics involving Higgs boson decay modes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Deep learning with artificial neural networks has enabled rapid progress on applications in engineering (e.g., <ref type="bibr" target="#b15">Krizhevsky et al., 2012;</ref><ref type="bibr" target="#b9">Hannun et al., 2014)</ref> and basic science (e.g., Di <ref type="bibr" target="#b5">Lena et al., 2012;</ref><ref type="bibr" target="#b18">Lusci et al., 2013;</ref><ref type="bibr" target="#b0">Baldi et al., 2014)</ref>. Usually, the parameters in the linear components are learned to fit the data, while the nonlinearities are pre-specified to be a logistic, tanh, rectified linear, or max-pooling function. A sufficiently large neural network using any of these common nonlinear functions can approximate arbitrarily complex functions <ref type="bibr" target="#b11">(Hornik et al., 1989;</ref><ref type="bibr">Cho &amp; Saul, 2010)</ref>, but in finite networks the choice of nonlinearity affects both the learning dynamics (especially in deep networks) and the network's expressive power.</p><p>Designing activation functions that enable fast training of accurate deep neural networks is an active area of research. The rectified linear activation function <ref type="bibr" target="#b12">(Jarrett et al., 2009;</ref><ref type="bibr" target="#b6">Glorot et al., 2011)</ref>, which does not saturate like sigmoidal functions, has made it easier to quickly train deep neural networks by alleviating the difficulties of weight-initialization and vanishing gradients. Another recent innovation is the "maxout" activation function, which has achieved state-of-the-art performance on multiple machine learning benchmarks <ref type="bibr" target="#b7">(Goodfellow et al., 2013)</ref>. The maxout activation function computes the maximum of a set of linear functions, and has the property that it can approximate any convex function of the input. <ref type="bibr" target="#b21">Springenberg &amp; Riedmiller (2013)</ref> replaced the max function with a probabilistic max function and <ref type="bibr" target="#b8">Gulcehre et al. (2014)</ref> explored an activation function that replaces the max function with an L P norm. However, while the type of activation function can have a significant impact on learning, the space of possible functions has hardly been explored. One way to explore this space is to learn the activation function during training. Previous efforts to do this have largely focused on genetic and evolutionary algorithms <ref type="bibr" target="#b26">(Yao, 1999)</ref>, which attempt to select an activation function for each neuron from a pre-defined set. Recently, <ref type="bibr" target="#b24">Turner &amp; Miller (2014)</ref> combined this strategy with a single scaling parameter that is learned during training.</p><p>In this paper, we propose a more powerful adaptive activation function. This parametrized, piecewise linear activation function is learned independently for each neuron using gradient descent, and can represent both convex and non-convex functions of the input. Experiments demonstrate that like other piecewise linear activation functions, this works well for training deep neural networks, and we obtain state-of-the-art performance on multiple benchmark deep learning tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">ADAPTIVE PIECEWISE LINEAR UNITS</head><p>Here we define the adaptive piecewise linear (APL) activation unit. Our method formulates the activation function h i (x) of an APL unit i as a sum of hinge-shaped functions,</p><formula xml:id="formula_0">h i (x) = max(0, x) + S s=1 a s i max(0, −x + b s i )<label>(1)</label></formula><p>The result is a piecewise linear activation function. The number of hinges, S, is a hyperparameter set in advance, while the variables a s i , b s i for i ∈ 1, ..., S are learned using standard gradient descent during training. The a s i variables control the slopes of the linear segments, while the b s i variables determine the locations of the hinges.</p><p>The number of additional parameters that must be learned when using these APL units is 2SM , where M is the total number of hidden units in the network. This number is small compared to the total number of weights in typical networks. shows that the activation function can also be non-convex. Asymptotically, the activation functions tend to g(x) = x as x → ∞ and g(x) = αx − c as x ← −∞ for some α and c. S = 1 for all plots. <ref type="figure" target="#fig_0">Figure 1</ref> shows example APL functions for S = 1. Note that unlike maxout, the class of functions that can be learned by a single unit includes non-convex functions. In fact, for large enough S, h i (x) can approximate arbitrarily complex continuous functions, subject to two conditions:</p><p>Theorem 1 Any continuous piecewise-linear function g(x) can be expressed by Equation 1 for some S, and a i , b i , i ∈ 1, ..., S, assuming that:</p><p>1. There is a scalar u such that g(x) = x for all x ≥ u.</p><p>2. There are two scalars v and α such that ∇ x g(x) = α for all x &lt; v.</p><p>This theorem implies that we can reconstruct any piecewise-linear function g(x) over any subset of the real line, and the two conditions on g(x) constrain the behavior of g(x) to be linear as x gets very large or small. The first condition is less restrictive than it may seem. In neural networks, g(x) is generally only of interest as an input to a linear function wg(x) + z; this linear function effectively restores the two degrees of freedom that are eliminated by constraining the rightmost segment of g(x) to have unit slope and bias 0.</p><p>Proof Let g(x) be piecewise linear with K + 2 linear regions separated by ordered boundary points b 0 , b 1 , ...,b K , and let a k be the slope of the k-th region. Assume also that g(x) = x for all x ≥ b K .</p><p>We show that g(x) can be expressed by the following special case of Equation 1:</p><formula xml:id="formula_1">h(x) ≡ −a 0 max(0, −x + b 0 ) + K k=1 a k (max(0, −x + b k−1 ) − max(0, −x + b k )) − max(0, −x) + max(0, x) + max(0, −x + b K ),<label>(2)</label></formula><p>The first term has slope a 0 in the range (−∞, b 0 ) and 0 elsewhere. Each element in the summation term of Equation 2 has slope a k over the range (b k−1 , b k ) and 0 elsewhere. The last three terms together have slope 1 when x ∈ (b K , ∞) and 0 elsewhere. Now, g(x) and h(x) are continuous, their slopes match almost everywhere, and it is easily verified that h(x) = g(x) = x for x ≥ b K . Thus, we conclude that h(x) = g(x) for all x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">COMPARISON WITH OTHER ACTIVATION FUNCTIONS</head><p>In this section we compare the proposed approach to learning activation functions with two other nonlinear activation functions: maxout <ref type="bibr" target="#b7">(Goodfellow et al., 2013)</ref>, and network-in-network <ref type="bibr" target="#b17">(Lin et al., 2013)</ref>.</p><p>We observe that both maxout units and network-in-network can learn any nonlinear activation function that APL units can, but require many more parameters to do so. This difference allows APL units to be applied in very different ways from maxout and network-in-network nonlinearities: the small number of parameters needed to tune an APL unit makes it practical to train convolutional networks that apply different nonlinearities at each point in each feature map, which would be completely impractical in either maxout networks or network-in-network approaches.</p><p>Maxout. Maxout units differ from traditional neural network nonlinearities in that they take as input the output of multiple linear functions, and return the largest:</p><formula xml:id="formula_2">h maxout (x) = max k∈{1,...,K} w k · x + b k .<label>(3)</label></formula><p>Incorporating multiple linear functions increases the expressive power of maxout units, allowing them to approximate arbitrary convex functions, and allowing the difference of a pair of maxout units to approximate arbitrary functions.</p><p>Networks of maxout units with a particular weight-tying scheme can reproduce the output of an APL unit. The sum of terms in Equation 1 with positive coefficients (including the initial max(0, x) term) is a convex function, and the sum of terms with negative coefficients is a concave function. One could approximate the convex part with one maxout unit, and the concave part with another maxout unit:</p><formula xml:id="formula_3">h convex (x) = max k c convex k w · x + d convex k ; h concave (x) = max k c concave k w · x + d concave k ,<label>(4)</label></formula><p>where c and d are chosen so that</p><formula xml:id="formula_4">h convex (x) − h concave (x) = max(0, w · x + u) + s a s max(0, w · x + u).<label>(5)</label></formula><p>In a standard maxout network, however, the w vectors are not tied. So implementing APL units (Equation 1) using a maxout network would require learning O(SK) times as many parameters, where K is the size of the maxout layer's input vector. Whenever the expressive power of an APL unit is sufficient, using the more complex maxout units is therefore a waste of computational and modeling power.</p><p>Network-in-Network. <ref type="bibr" target="#b17">Lin et al. (2013)</ref> proposed replacing the simple rectified linear activation in convolutional networks with a fully connected network whose parameters are learned from data. This "MLPConv" layer couples the outputs of all filters applied to a patch, and permits arbitrarily complex transformations of the inputs. A depth-M MLPConv layer produces an output vector f M ij from an input patch x ij via the series of transformations</p><formula xml:id="formula_5">f 1 ijk = max(0, w 1 k · x ij + b 1 k ), . . . , f M ijk = max(0, w M k · f M −1 ij + b M k ).<label>(6)</label></formula><p>As with maxout networks, there is a weight-tying scheme that allows an MLPConv layer to reproduce the behavior of an APL unit:</p><formula xml:id="formula_6">f 1 ijk = max(0, c k w κ(k) · x ij + b 1 k ), f 2 ijk = |κ( )=k a k f 1 ij ,<label>(7)</label></formula><p>where the function κ(k) maps from hinge output indices k to filter indices κ, and the coefficient</p><formula xml:id="formula_7">c k ∈ {−1, 1}.</formula><p>This is a very aggressive weight-tying scheme that dramatically reduces the number of parameters used by the MLPConv layer. Again we see that it is a waste of computational and modeling power to use network-in-network wherever an APL unit would suffice.</p><p>However, network-in-network can do things that APL units cannot-in particular, it efficiently couples and summarizes the outputs of multiple filters. One can get the benefits of both architectures by replacing the rectified linear units in the MLPconv layer with APL units.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTS</head><p>Experiments were performed using the software package CAFFE <ref type="bibr" target="#b13">(Jia et al., 2014)</ref>. The hyperparameter, S, that controls the complexity of the activation function was determined using a validation set for each dataset. The a s i and b s i parameters were regularized with an L2 penalty, scaled by 0.001. Without this penalty, the optimizer is free to choose very large values of a s i balanced by very small weights, which would lead to numerical instability. We found that adding this penalty improved results. The model files and solver files are available at https://github.com/ForestAgostinelli/Learned-Activation-Functions-Source/tree/master.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">CIFAR</head><p>The CIFAR-10 and CIFAR-100 datasets <ref type="bibr" target="#b14">(Krizhevsky &amp; Hinton, 2009</ref>) are 32x32 color images that have 10 and 100 classes, respectively. They both have 50,000 training images and 10,000 test images. The images were preprocessed by subtracting the mean values of each pixel of the training set from each image. Our network for CIFAR-10 was loosely based on the network used in <ref type="bibr" target="#b22">(Srivastava et al., 2014)</ref>. It had 3 convolutional layers with 96, 128, and 256 filters, respectively. Each kernel size was 5x5 and was padded by 2 pixels on each side. The convolutional layers were followed by a max-pooling, average-pooling, and average-pooling layer, respectively; all with a kernel size of 3 and a stride of 2. The two fully connected layers had 2048 units each. We applied dropout  to the network as well. We found that applying dropout both before and after a pooling layer increased classification accuracy. The probability of a unit being dropped before a pooling layer was 0.25 for all pooling layers. The probability for them being dropped after each pooling layers was 0.25, 0.25, and 0.5, respectively. The probability of a unit being dropped for the fully connected layers was 0.5 for both layers. The final layer was a softmax classification layer. For CIFAR-100, the only difference was the second pooling layer was max-pooling instead of average-pooling. The baseline used rectified linear activation functions.</p><p>When using the APL units, for CIFAR-10, we set S = 5. For CIFAR-100 we set S = 2. <ref type="table">Table 1</ref> shows that adding the APL units improved the baseline by over 1% in the case of CIFAR-10 and by almost 3% in the case of CIFAR-100. In terms of relative difference, this is a 9.4% and a 7.5% decrease in error rate, respectively. We also try the network-in-network architecture for CIFAR-10 ( <ref type="bibr" target="#b17">Lin et al., 2013)</ref>. We have S = 2 for CIFAR-10 and S = 1 for CIFAR-100. We see that it improves performance for both datasets.</p><p>We also try our method with the augmented version of CIFAR-10 and CIFAR-100. We pad the image all around with a four pixel border of zeros. For training, we take random 32 x 32 crops of the image and randomly do horizontal flips. For testing we just take the center 32 x 32 image. To the best of our knowledge, the results we report for data augmentation using the network-in-network architecture are the best results reported for CIFAR-10 and CIFAR-100 for any method.</p><p>In section 3.4, one can observe that the learned activations can look similar to leaky rectified linear units (Leaky ReLU) <ref type="bibr" target="#b19">(Maas et al., 2013)</ref>. This activation function is slightly different than the ReLU because it has a small slope k when the input x &lt; 0.</p><formula xml:id="formula_8">h(x) = x, if x &gt; 0 kx, otherwise</formula><p>In <ref type="bibr" target="#b19">(Maas et al., 2013)</ref>, k is equal to 0.01. To compare Leaky ReLUs to our method, we try different values for k and pick the best value one. The possible values are positive and negative 0.01, 0.05, 0.1, and 0.2. For the standard convolutional neural network architecture k = 0.05 for CIFAR-10 and k = −0.05 for CIFAR-100. For the network-in-network architecture k = 0.05 for CIFAR-10 and k = 0.2 for CIFAR-100. APL units consistently outperform leaky ReLU units, showing the value of tuning the nonlinearity (see also section 3.3). <ref type="table">Table 1</ref>: Error rates on CIFAR-10 and CIFAR-100 with and without data augmentation. This includes standard convolutional neural networks (CNNs) and the network-in-network (NIN) architecture <ref type="bibr" target="#b17">(Lin et al., 2013)</ref>. The networks were trained 5 times using different random initializationswe report the mean followed by the standard deviation in parenthesis. The best results are in bold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>CIFAR-10 CIFAR-100 Without Data Augmentation CNN + ReLU <ref type="bibr" target="#b22">(Srivastava et al., 2014)</ref> 12.61% 37.20% CNN + Channel-Out <ref type="bibr" target="#b25">(Wang &amp; JaJa, 2013)</ref> 13.2% 36.59% CNN + Maxout <ref type="bibr" target="#b7">(Goodfellow et al., 2013)</ref> 11.68% 38.57% CNN + Probout <ref type="bibr" target="#b21">(Springenberg &amp; Riedmiller, 2013)</ref> 11  <ref type="bibr" target="#b7">(Goodfellow et al., 2013)</ref> 9.38% -CNN + Probout <ref type="bibr" target="#b21">(Springenberg &amp; Riedmiller, 2013)</ref> 9.39% -CNN + Maxout <ref type="bibr" target="#b23">(Stollenga et al., 2014)</ref> 9.61% 34.54% CNN + Maxout + Selective Attention <ref type="bibr" target="#b23">(Stollenga et al., 2014)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">HIGGS BOSON DECAY</head><p>The Higgs-to-τ + τ − decay dataset comes from the field of high-energy physics and the analysis of data generated by the Large Hadron Collider <ref type="bibr" target="#b1">(Baldi et al., 2015)</ref>. The dataset contains 80 million collision events, characterized by 25 real-valued features describing the 3D momenta and energies of the collision products. The supervised learning task is to distinguish between two types of physical processes: one in which a Higgs boson decays into τ + τ − leptons and a background process that produces a similar measurement distribution. Performance is measured in terms of the area under the receiver operating characteristic curve (AUC) on a test set of 10 million examples, and in terms of discovery significance <ref type="bibr" target="#b4">(Cowan et al., 2011)</ref> in units of Gaussian σ, using 100 signal events and 5000 background events with a 5% relative uncertainty.</p><p>Our baseline for this experiment is the 8 layer neural network architecture from <ref type="bibr" target="#b1">(Baldi et al., 2015)</ref> whose architecture and training hyperparameters were optimized using the Spearmint algorithm <ref type="bibr" target="#b20">(Snoek et al., 2012)</ref>. We used the same architecture and training parameters except that dropout was used in the top two hidden layers to reduce overfitting. For the APL units we used S = 2. <ref type="table">Table 2</ref> shows that a single network with APL units achieves state-of-the-art performance, increasing performance over the dropout-trained baseline and the ensemble of 5 neural networks from <ref type="bibr" target="#b1">(Baldi et al., 2015)</ref>. <ref type="table">Table 2</ref>: Performance on the Higgs boson decay dataset in terms of both AUC and expected discovery significance. The networks were trained 4 times using different random initializations -we report the mean followed by the standard deviation in parenthesis. The best results are in bold.</p><p>Method AUC Discovery Significance DNN + ReLU <ref type="bibr" target="#b1">(Baldi et al., 2015)</ref> 0.802 3.37σ DNN + ReLU + Ensemble <ref type="bibr" target="#b1">(Baldi et al., 2015)</ref>   <ref type="table" target="#tab_4">Table 3</ref> shows the effect of varying S on the CIFAR-10 benchmark. We also tested whether learning the activation function was important (as opposed to having complicated, fixed activation functions). For S = 1, we tried freezing the activation functions at their random initialized positions, and not allowing them to learn. The results show that learning activations, as opposed to keeping them fixed, results in better performance. In <ref type="figure">figure 4</ref>, for each layer, 1000 activation functions (or the maximum number of activation functions for that layer, whichever is smaller) are plotted. One can see that there is greater variance in the learned activations for CIFAR-100 than there is for CIFAR-10. There is greater variance in the learned activations for Higgs→ τ + τ − than there is for CIFAR-100. For the case of Higgs→ τ + τ − , a trend that can be seen is that the variance decreases in the higher layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CONCLUSION</head><p>We have introduced a novel neural network activation function in which each neuron computes an independent, piecewise linear function. The parameters of each neuron-specific activation function are learned via gradient descent along with the network's weight parameters. Our experiments demonstrate that learning the activation functions in this way can lead to significant performance improvements in deep neural networks without significantly increasing the number of parameters. Furthermore, the networks learn a diverse set of activation functions, suggesting that the standard one-activation-function-fits-all approach may be suboptimal.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>F. Agostinelli was supported by the GEM Fellowship. This work was done during an internship at Adobe. We also wish to acknowledge the support of NVIDIA Corporation with the donation of the Tesla K40 GPU used for this research, NSF grant IIS-0513376, and a Google Faculty Research award to P. Baldi, and thanks to Yuzo Kanomata for computing support.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Sample activation functions obtained from changing the parameters. Notice that figure b</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>CIFAR-100 Sample Activation Functions. Initialization (dashed line) and the final learned function (solid line).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Higgs→ τ + τ − Sample Activation Functions. Initialization (dashed line) and the final learned function (solid line).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Classification accuracy on CIFAR-10 for varying values of S. Shown are the mean and standard deviation over 5 trials. VISUALIZATION AND ANALYSIS OF ADAPTIVE PIECEWISE LINEAR FUNCTIONS The diversity of adaptive piecewise linear functions was visualized by plotting h i (x) for sample neurons. Figures 2 and 3 show adaptive piecewise linear functions for the CIFAR-100 and Higgs→ τ + τ − experiments, along with the random initialization of that function.</figDesc><table><row><cell>Values of S</cell><cell>Error Rate</cell></row><row><cell>baseline</cell><cell>12.56 (0.26)%</cell></row><row><cell cols="2">S = 1 (activation not learned) 12.55 (0.11)%</cell></row><row><cell>S = 1</cell><cell>11.59 (0.16)%</cell></row><row><cell>S = 2</cell><cell>11.73 (0.23)%</cell></row><row><cell>S = 5</cell><cell>11.38 (0.09)%</cell></row><row><cell>S = 10</cell><cell>11.60 (0.16)%</cell></row><row><cell>3.4</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Searching for exotic particles in high-energy physics with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Baldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sadowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Whiteson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Enhanced higgs to τ + τ − searches with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Baldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Sadowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Whiteson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physics Review Letters</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>In press</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Large margin classification in infinite neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngmin</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010. (a) CIFAR-10 Activation Functions. (b) CIFAR-100 Activation Functions</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">Visualization of the range of the values for the learned activation functions for the deep neural network for the CIFAR datasets and Higgs→ τ + τ − dataset</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Asymptotic formulae for likelihoodbased tests of new physics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glen</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cranmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kyle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eilam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofer</forename><surname>Vitells</surname></persName>
		</author>
		<idno type="DOI">10.1140/epjc/s10052-011-1554-0</idno>
	</analytic>
	<monogr>
		<title level="j">Eur.Phys.J</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page">1554</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep architectures for protein contact map prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Di Lena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nagata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Baldi</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/bts475</idno>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2449" to="2457" />
			<date type="published" when="2012-07-30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Conference on Artificial Intelligence and Statistics. JMLR W&amp;CP Volume</title>
		<meeting>the 14th International Conference on Artificial Intelligence and Statistics. JMLR W&amp;CP Volume</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Warde</forename><forename type="middle">-</forename><surname>Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bengio</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1302.4389</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">Yoshua. Maxout networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learned-norm pooling for deep feedforward and recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kyunghyun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="530" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deep speech: Scaling up end-to-end speech recognition. CoRR, abs/1412</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Awni</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Case</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Greg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Erich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Prenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sanjeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shubho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.5567" />
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5567</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nitish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ruslan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multilayer feedforward networks are universal approximators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Hornik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><surname>Stinchcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Halbert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="359" to="366" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">What is the best multi-stage architecture for object recognition? In Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Jarrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2146" to="2153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Evan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sergey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrell</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Caffe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>Computer Science Department, University of Toronto, Tech. Rep</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Yu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saining</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyou</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.5185</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Deeplysupervised nets. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4400</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep architectures and deep learning in chemoinformatics: the prediction of aqueous solubility for drug-like molecules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Lusci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gianluca</forename><surname>Pollastri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Baldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1563" to="1575" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Awni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Practical bayesian optimization of machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adams</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2951" to="2959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Improving deep neural networks with probabilistic maxout units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tobias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6116</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep networks with internal selective attention through feedback connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marijn</forename><forename type="middle">F</forename><surname>Stollenga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faustino</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3545" to="3553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Neuroevolution: Evolving heterogeneous artificial neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Francis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Evolutionary Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">From maxout to channel-out: Encoding information on sparse pathways</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Jaja</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.1909</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Evolving artificial neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1423" to="1447" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
