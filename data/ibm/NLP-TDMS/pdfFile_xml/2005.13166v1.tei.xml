<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SAFEML: SAFETY MONITORING OF MACHINE LEARNING CLASSIFIERS THROUGH STATISTICAL DIFFERENCE MEASURE A PREPRINT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="202027-05-28">May 28, 2020 27 May 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koorosh</forename><surname>Aslansefat</surname></persName>
							<email>k.aslansefat-2018@hull.ac.uk</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Declan</forename><surname>Whiting</surname></persName>
							<email>d.whiting-2018@hull.ac.uk</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramin</forename><forename type="middle">Tavakoli</forename><surname>Kolagari</surname></persName>
							<email>ramin.tavakolikolagari@th-nuernberg.de</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiannis</forename><surname>Papadopoulos</surname></persName>
							<email>y.i.papadopoulos@hull.ac.uk</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Hull Kingston upon Hull</orgName>
								<address>
									<postCode>HU6 7RX</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Ioannis Sorokos Fraunhofer Institute for Experimental Software Engineering, IESE Fraunhofer-Gesellschaft Fraunhofer-Platz 1</orgName>
								<address>
									<postCode>67663</postCode>
									<settlement>Kaiserslautern</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Hull</orgName>
								<address>
									<addrLine>APD Communications Kingston upon Hull</addrLine>
									<postCode>HU6 7RX, HU1 1RR</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Faculty of Computer Science</orgName>
								<orgName type="institution">Nuremberg Tech</orgName>
								<address>
									<addrLine>Keßlerplatz 12</addrLine>
									<postCode>90489</postCode>
									<settlement>Nürnberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Hull</orgName>
								<address>
									<addrLine>Kingston upon Hull</addrLine>
									<postCode>HU6 7RX</postCode>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SAFEML: SAFETY MONITORING OF MACHINE LEARNING CLASSIFIERS THROUGH STATISTICAL DIFFERENCE MEASURE A PREPRINT</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="202027-05-28">May 28, 2020 27 May 2020</date>
						</imprint>
					</monogr>
					<note>1 Our preliminary code and results are available at https://github.com/ISorokos/SafeML A PREPRINT -MAY 28, 2020</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Safety · SafeML · Machine Learning · Deep Learning · Artificial Intelligence · Statistical Difference · Domain Adaptation</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Ensuring safety and explainability of machine learning (ML) is a topic of increasing relevance as data-driven applications venture into safety-critical application domains, traditionally committed to high safety standards that are not satisfied with an exclusive testing approach of otherwise inaccessible black-box systems. Especially the interaction between safety and security is a central challenge, as security violations can lead to compromised safety. The contribution of this paper to addressing both safety and security within a single concept of protection applicable during the operation of ML systems is active monitoring of the behavior and the operational context of the data-driven system based on distance measures of the Empirical Cumulative Distribution Function (ECDF). We investigate abstract datasets (XOR, Spiral, Circle) and current security-specific datasets for intrusion detection (CICIDS2017) of simulated network traffic, using statistical distance measures including the Kolmogorov-Smirnov, Kuiper, Anderson-Darling, Wasserstein and mixed Wasserstein-Anderson-Darling measures. Our preliminary findings indicate that there is a meaningful correlation between ML decisions and the ECDF-based distances measures of the input features. Thus, they can provide a confidence level that can be used for a) analyzing the applicability of the ML system in a given field (safety/security) and b) analyzing if the field data was maliciously manipulated 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Machine Learning (ML) is expanding rapidly in numerous applications. In parallel with this rapid growth, the expansion of ML towards dependability-critical applications raises societal concern regarding the reliability and safety assurance of ML. For instance, ML in medicine by <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>, in autonomous systems e.g. self-driving cars by <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, in military <ref type="bibr" target="#b5">[6]</ref>, and in economic applications by <ref type="bibr" target="#b6">[7]</ref>. In addition, different organizations and governmental institutes are trying to establish new rules, regulations and standards for ML, such as in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>.</p><p>While ML is a powerful tool for enabling data-driven applications, its unfettered use can pose risks to financial stability, privacy, the environment and in some domains even life. Poor application of ML is typically characterized by poor design, misspecification of the objective functions, implementation errors, choosing the wrong learning process, or using poor or non-comprehensive datasets for training. Thus, safety for ML can be defined as a set of actions to prevent any harm to humanity by ML failures or misuse. However, there are many perspectives and directions to be defined for ML Safety. In fact, <ref type="bibr" target="#b10">[11]</ref> have addressed different research problems of certifying ML systems operating in the field. They have categorized safety issues into five categories: a) safe exploration, b) robustness to distributional shift, c) avoiding negative side effects, d) avoiding "reward hacking" and "wire heading", e) scalable oversight. This categorization is helpful for an adequate assessment of the applicability a concept for a given (safety) problem. In the work presented here, we will be focusing on addressing distributional shift, however using a non-standard interpretation. Distributional shift is usually interpreted as the gradual deviation of the initial state of learning of an ML component and its ongoing state as it performs online learning. As will be shown later, distributional shift will instead be used by our approach to evaluate the distance between the training and observed data of an ML component.</p><p>Statistical distance measures can be considered as a common method to measure distributional shift. Furthermore, in modern ML algorithms like Generative Adversarial Nets (GANs), statistical distance or divergence measures are applied as a loss function, such as the Jensen-Shannon divergence <ref type="bibr" target="#b11">[12]</ref>, the Wasserstein distance <ref type="bibr" target="#b12">[13]</ref>, and the Cramer distance <ref type="bibr" target="#b13">[14]</ref>. For dimension reduction, the t-SNE (t-distributed stochastic neighbour embedding) algorithm uses the Kullback-Leibler divergence as a loss function <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Contributions</head><p>This paper studies the applicability of safety-security monitoring based on statistical distance measures on the robustness of ML systems in the field.The basis of this work is a modified version of the statistical distance concept to allow the comparison of the data set during the ML training procedure and the observed data set during the use of the ML classifier in the field. The calculation of the distance is carried out in a novel controller-in-the-loop procedure to estimate the accuracy of the classifier in different scenarios. By exploiting the accuracy estimation, applications can actively identify situations where the ML component may be operating far beyond its trained cases, thereby risking low accuracy, and adjust accordingly. The main advantage of this approach is its flexibility in potentially handling a large range of ML techniques, as it is not dependent on the ML approach. Instead, the approach focuses on the quality of the training data and its deviation from the field data. In a comprehensive case study we have analyzed the possibilities and limitations of the proposed approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Overview of the Paper</head><p>The rest of the paper is organised as follows: In Section 1.3, previous work related to this publication is discussed. In Section 2, the problem definition is provided. The proposed method is addressed in Section 3. Numerical results are demonstrated in Section 4 with a brief discussion. Explainable AI is introduced and discussed as a highly relevant topic in Section 5. The capabilities and limitations of the proposed method are summarised in Section 6 and the paper terminates with a conclusion in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Related Work</head><p>Our analysis of the research literature did not reveal any reference to existing publications dealing with the safety, security and accuracy of ML-based classification using statistical measures of difference. Nevertheless, there are publications that provide a basis for comparison with the current study. A Resampling Uncertainty Estimation (RUE)based algorithm has been proposed by <ref type="bibr" target="#b15">[16]</ref> to ensure the point-wise reliability of the regression when the test or field data set is different from the training dataset. The algorithm has created prediction ensembles through the modified gradient and Hessian functions for ML-based regression problems. An uncertainty wrapper for black-box models based on statistical measures has been proposed by <ref type="bibr" target="#b16">[17]</ref>. Hobbhahn. M. et al. <ref type="bibr" target="#b17">[18]</ref> have proposed a method to evaluate the uncertainty of Bayesian Deep Networks classifiers using Dirichlet distributions. The results were promising but to a limited class of classifiers (Bayesian Network-based classifiers). A new Key Performance Index (KPI), the Expected Odds Ratio (EOR) has been introduced in <ref type="bibr" target="#b22">[23]</ref>. This KPI was designed to estimate empirical uncertainty in deep neural networks based on entropy theories. However, this KPI has not yet been applied to other types of machine learning algorithms. A comprehensive study on dataset shift has been provided by <ref type="bibr" target="#b18">[19]</ref> and the dataset issues such as projection and projectability, simple and prior probability shift are discussed there. However, the mentioned study does not address the use of statistical distance and error bound to evaluate the dataset shift, in contrast to the work presented here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem Definition</head><p>Classification ML algorithms are typically employed to categorize input samples into predetermined categories. For instance, abnormality detection can be performed by detecting whether a given sample falls within known ranges i.e. categories. A simple example of a classifier for 1-dimensional input can be a line or threshold. Consider a hypothetical measurement t (e.g. time, temperature etc.) and a classifier D based on it, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>-(a) and defined as <ref type="bibr" target="#b0">(1)</ref>. Note that <ref type="figure" target="#fig_0">Figure 1</ref> shows the true classes of the input.</p><formula xml:id="formula_0">D(t) = Class1, if 0 &lt; n ≤ 100 Class2, if 100 &lt; n ≤ 200<label>(1)</label></formula><p>The classifier D (t) can predict two classes which represent, in this example, the normal and abnormal state of a system. From measurement input 0 to 100, the sample is considered to fall under class 1 and from above 100 to 200 under class 2. The Probability Density Functions (PDFs) of the (true) classes can be estimated as shown in <ref type="figure" target="#fig_0">Figure 1</ref>-(b). In this figure, the threshold of the classifier has been represented with a red vertical dash-line and value of four. The area with an overlap in this figure can cause false detection to occur, as the classifier misclassifies the input belonging to the opposite class. These type of misclassifications are also known as false positive/type I errors (e.g. when misclassifying input as being class 1) and false-negative/type II errors (e.g. when misclassifying input as not being class 1). Considering <ref type="figure" target="#fig_0">Figure  1</ref>-(b) of probability density functions, we notice that in the area where the two probability density functions merge, the misclassifications and thus the errors can occur. The probability of the error or misclassification can be calculated with (2) <ref type="bibr" target="#b19">[20]</ref>. Note that the error probability is also related to the threshold value (x considered as the threshold value), (for more details see <ref type="bibr" target="#b20">[21]</ref>).</p><formula xml:id="formula_1">P (error) = +∞ −∞ P (error|x) P (x) dx<label>(2)</label></formula><p>In listing (2), the P (error|x) can be calculated as the minimum of both PDFs as <ref type="bibr" target="#b2">(3)</ref>. The minimization is subject to variation of threshold value from −∞ to +∞.</p><formula xml:id="formula_2">P (error|x) = min [P (Class 1|x) , P (Class 2|x)]<label>(3)</label></formula><p>By dividing the space into two regions as R 1 and R 2 , the probability of error can be written in two parts.</p><formula xml:id="formula_3">P (error) = P (x ∈ R 1 , Class 1) + P (x ∈ R 2 , Class 2) = R1 P (x|Class 1) P (Class 1) dx + R2 P (x|Class 2) P (Class 2) dx (4)</formula><p>To ease the minimization problem, consider the following inequality rule <ref type="bibr" target="#b21">[22]</ref>.</p><formula xml:id="formula_4">min [a, b] ≤ a λ b 1−λ where a, b ≥ 0 and 0 ≤ α ≤ 1<label>(5)</label></formula><p>Equation <ref type="formula" target="#formula_2">(3)</ref> can be rewritten as <ref type="bibr" target="#b5">(6)</ref>. Note that in (5) the " ≤ " can considered as " = " when we consider the worst-case scenario or upper bound error.</p><formula xml:id="formula_5">P (error|x) = min [P (Class 1|x) , P (Class 2|x)] = min P (x|Class 1) P (Class 1) P (x) , P (x|Class 2) P (Class 2) P (x)<label>(6)</label></formula><p>Using the inequality rule and equation <ref type="formula" target="#formula_5">(6)</ref>, the conditional probability of error can be derived as <ref type="bibr" target="#b6">(7)</ref>.</p><formula xml:id="formula_6">P (error|x) ≤ P (x|Class 1) P (Class 1) P (x) λ P (x|Class 2) P (Class 2) P (x) 1−λ<label>(7)</label></formula><p>The equation <ref type="formula" target="#formula_7">(8)</ref> can be obtained using equations (2) and <ref type="bibr" target="#b6">(7)</ref>.</p><formula xml:id="formula_7">P (error) ≤ (P (Class 1)) λ (P (Class 2)) 1−λ +∞ −∞ (P (x|Class 1)) λ (P (x|Class 2)) 1−λ dx<label>(8)</label></formula><p>In safety assurance, it is important to consider the worst-case scenario which can lead us to <ref type="bibr" target="#b8">(9)</ref>, known as the Chernoff upper bound of error <ref type="bibr" target="#b21">[22]</ref>.</p><formula xml:id="formula_8">P (error) = P (Class 1) λ P (Class 2) 1−λ +∞ −∞ P (x|Class 1) λ P (x|Class 2) 1−λ dx<label>(9)</label></formula><p>If the probability distributions of the features obey normal or exponential distribution families, the integral part of (9) can be solved through (10) <ref type="bibr" target="#b21">[22]</ref>.</p><formula xml:id="formula_9">+∞ −∞ P (x|Class 1) λ P (x|Class 2) 1−λ dx = e −θ(λ)<label>(10)</label></formula><p>The θ (λ) can be calculated using <ref type="bibr" target="#b10">(11)</ref> where µ and Σ are mean vector and variance matrix of each class respectively.</p><formula xml:id="formula_10">θ (λ) = λ (1 − λ) 2 [µ 2 − µ 1 ] T [λΣ 1 + (1 − λ) Σ 2 ] −1 [µ 2 − µ 1 ] +0.5 log |λΣ 1 + (1 − λ) Σ 2 | |Σ 1 | λ |Σ 2 | (1−λ)<label>(11)</label></formula><p>Considering α = 0.5 the equation <ref type="formula" target="#formula_0">(11)</ref> effectively becomes the Bhattacharyya distance. It can be proven that this value is the optimal value when Σ 1 = Σ 2 <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b24">25]</ref>. In this study, for simplicity, the Bhattacharyya distance will be used to demonstrate the approach. It should be noted that there may be cases where the calculated error bound is higher than the real value. However, this is acceptable as an overestimation of the classifier error would not introduce safety concerns (although it may impact performance). As the P (error) and P (correct) are complementary, the probability of having a correct classification can be calculated using <ref type="bibr" target="#b11">(12)</ref>.</p><formula xml:id="formula_11">P (correct) = 1 − P (Class 1) P (Class 2) e −θ(λ)<label>(12)</label></formula><p>The Chernoff upper bound of error is usually used as a measure of separability of two classes of data, but in the above context, equation <ref type="bibr" target="#b11">(12)</ref> measures the similarity between two classes. In other words, in an ideal situation, by comparing the P (error) of a class, with itself, the response should be equal to one while P (correct) should be zero. The intuitive explanation is to determine whether the distribution of the data during training is the same as the distribution observed in the field (or not). Assuming P (Class 1) = P (Class 2), the integral part of P (error) can be converted to the cumulative distribution function as <ref type="bibr" target="#b12">(13)</ref>.</p><formula xml:id="formula_12">P (error) = T −∞ P Class 1 (x)dx + ∞ T P Class 2 (x)dx = T −∞ P Class 1 (x)dx + +∞ T P Class 2 (x)dx = (F Class 1 (T ) + (1 − F Class 2 (T ))) = 1 − (F Class 2 (T ) − F Class 1 (T ))<label>(13)</label></formula><p>Equation <ref type="formula" target="#formula_0">(13)</ref> shows that there is relation between probability of error (and also accuracy) and statistical difference between two Cumulative Distribution Functions (CDF) of two classes. Using this fact and considering that the Empirical CDFs of each class is available, ECDF-based statistical measures such as the Kolmogorov-Smirnov distance (equation 14) and similar distance measures can be used <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref>.</p><formula xml:id="formula_13">P (error) ≈ sup x (F Class 2 (x) − F Class 1 (x))<label>(14)</label></formula><p>It should be mentioned that such ECDF-based distances are not bounded between zero and one and, in some cases, need a coefficient to be adjusted as a measure for accuracy estimation. In section 4.3, the correlation between ECDF-based distance and accuracy will be discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SafeML Method</head><p>To begin with, we should note that while this study focuses on ML classifiers, the proposed approach does not prohibit application on ML components for regression tasks either. <ref type="figure">Figure 2</ref> illustrates how we envision the approach to be applied practically. In this flowchart, there are two main sections; the training phase and the application phase. A) The 'training' phase is an offline procedure in which a trusted dataset is used to train the ML algorithm. Once training is complete, the classifier's performance is measured with user-defined KPIs. Meanwhile, the PDF and statistical parameters of each class are also computed and stored for future comparison in the second phase. B) The second or 'application' phase is an online procedure in which real-time and unlabelled data is provided to the system. For example, consider an autonomous vehicle's machine vision system. Such a system has been trained to detect obstacles (among other tasks), so that the vehicle can avoid collisions with them. A critical issue to note in the application phase is that the incoming data is unlabeled. So, it cannot be assured that the classifier will perform as accurately as it had in during the training phase. As input samples are collected, the PDF and statistical parameters of each class can be estimated. The system requires enough samples to reliably determine the statistical difference, so a buffer of samples may have to be accumulated before proceeding. Using the modified Chernoff error bound in 12, the statistical difference of each class in the training phase and application phase is compared. If the statistical difference is very low, the classifier results and accuracy can be trusted. In the example mentioned above, the autonomous vehicle would continue its operation in this case. Instead, if the statistical difference is greater, the classifier results and accuracy are no longer considered valid (as the difference between the training and observed data is too large). In this case, the system should use an alternative approach or notify a human operator. In the above example, the system could ask the driver to takeover control of the vehicle. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Case Studies</head><p>In this section, the proposed method described in Section 3 is applied on typical synthetic benchmarks for ML classification. The proposed method has been implemented in three different programming languages including R, Python and MATLAB. Regarding R programming, three well-known benchmarks have been selected: a) the XOR dataset, b) the Spiral dataset and c) the Circle dataset. Each dataset has two features (i.e. input variables) and two classes. <ref type="figure" target="#fig_1">Figure 3</ref> illustrates the scatter plots of the selected benchmarks. More examples and benchmarks are available at SafeML Github Repository.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Methodology for Evaluation against Benchmark Datasets</head><p>To XOR Dataset: The XOR dataset has two features and two classes in which features have the same mean and variance characteristics. <ref type="table" target="#tab_0">Table 1</ref> compares the estimated accuracy based on the ECDF measures with the Minimum True Accuracy (MTA) and the Average True Accuracy (ATA) over 10 folds. For instance, the second column of this table provides the estimated accuracy based on the KSD measure. As a matter of safety, MTA is more important because it represents the worst-case scenarios, where the lowest accuracy may be experienced and impact safety. We observe that the KSD measure reports low accuracy for the LDA classifier ( .77). Instead, the ADD and WAD measures significantly overestimate the accuracy of the LDA. Based on <ref type="table" target="#tab_0">Table 1</ref>, <ref type="table" target="#tab_1">Table 2</ref> represents the (absolute) difference between accuracy estimations of each measure and the MTA of each classifier. The ADD, WD and WAD measures have the best accuracy estimations overall. In particular, when a LDA classifier is used, the WD measure provides an estimated accuracy with comparatively less error.  Circle dataset: The circle dataset has similar statistical characteristics with the spiral dataset. <ref type="table" target="#tab_3">Table 4</ref> provides the difference between ECDF-based distance measures and MTA for this dataset. As can be seen, the worst accuracy estimation is related to the accuracy estimation of the LDA classifier. For the LDA, the Kuiper distance estimates with less error, with the KSD and WD being in second and third place respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Security dataset</head><p>This case-study applies the proposed method towards the CICIDS2017 dataset, which was originally produced by <ref type="bibr" target="#b29">[30]</ref> at the Canadian Institute for Cyber Security (CICS) as an aide to the development and research of anomaly-based intrusion detection techniques for use in Intrusion Detection Systems (IDSs) and Intrusion Prevention Systems (IPSs) <ref type="bibr" target="#b30">[31]</ref>. The labelled dataset includes both benign (Monday) and malicious (Tuesday, Wednesday, Thursday, Friday) activity. The benign network traffic is simulated by abstraction of typical user activity using a number of common protocols such as HTTP, HTTPS, FTP and SHH. Benign and malicious network activity is included as packet payloads in packet capture format (PCAPS). Wednesday Attack: This attack occurred on Wednesday, July 5, 2017, and different types of attacks on the availability of the victim's system have been recorded, such as DoS / DDoS, DoS slowloris (9:47 -10:10 a.m.), DoS Slowhttptest (10:14 -10:35 a.m.), DoS Hulk (10:43 -11 a.m.), and DoS GoldenEye (11:10 -11:23 a.m.). Regarding the crossvalidation, a hold-out approach has been used, in which 70 percent of data has been randomly extracted for testing and training and the rest has been used for accuracy estimation. Additionally, traditional classifiers including 'Naive Bayes','Discriminant Analysis','Classification Tree', and 'Nearest Neighbor' have been used. <ref type="figure" target="#fig_3">Figure 4</ref> shows the confusion matrix when Naive Bayes classifier is used. <ref type="figure" target="#fig_5">Figure 5</ref> has been generated over 100 iterations. For each iteration, 70 percent of the data has been randomly extracted for testing and training and the rest has been used for accuracy estimation. <ref type="figure" target="#fig_5">Figure 5</ref> shows the box plot of the statistical distance measurements vs. the evaluated accuracy over 100 iterations. By observing the average values (red lines) of each box plot, the relationship between each measure and the average change in accuracy can be understood. In  <ref type="figure" target="#fig_6">Figure 6</ref> shows the confusion matrix for Thursday morning's security intrusion in the CICIDS2017 dataset when the Naive Bayes classifier is applied. Similar to Wednesday, 70 percent hold-out cross validation is used for this dataset. As can be seen, this dataset has four classes and the classifier has problem to detect the last class or last type of intrusion. Similar to the previous example, <ref type="figure" target="#fig_4">Figure 7</ref> has been generated over 100 times and the box plot of <ref type="figure">Figure 8</ref> can be seen. In this figure, the Kolmogorov-Smirnov, Kuiper and Wassertein distance measures have a better performance, however, their decision variance is a bit high.</p><p>The rest of results for Security Intrusion Detection in CICIDS2017 dataset are available in the SafeML Github Repository. <ref type="figure">Figure 9</ref> shows Pearson's correlation between the classes of Wednesday's data and the statistical ECDF-based distances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Correlation Analysis</head><p>As can be seen, the WD and WAD distances have more correlation with the classes. This figure also shows the correlation between the measures themselves. The KSD and KD appear to be correlated. The WD and WAS also seem to be correlated. These correlations can be explained due to the similarity in their formulation. P-values for the above correlations were evaluated to be zero, thereby validating the correlation hypotheses above.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Towards Explainable AI</head><p>In this section, we discuss a relevant topic to our proposed approach, to explain how the proposed approach could be applied for this purpose as well. Explainable AI (XAI) can be defined as a tool or framework that increases interpretability of ML algorithms and their outputs <ref type="bibr" target="#b23">[24]</ref>. Our proposed approach can also be used to improve the interpretability of ML classifiers using the statistical ECDF-based distance measures seen previously. We shall discuss a small example here and intend to delve further on this topic in our future works. For the example, the Wednesday data from the security dataset mentioned previously is chosen and its class labels vs. the sample time has been plotted in <ref type="figure" target="#fig_0">Figure 10</ref>. This dataset has six different classes with variable number of occurrence. In this figure a sliding window with the size of d = 1500 is used. In the beginning, 1500 samples of class one are considered as reference and then compared with the rest of the samples for each window using the statistical ECDF-based distance measures. It should be mentioned that the smoothness of the output is related to the sliding window's size. As can be see in the figure, the change in the average distance vs. the class shows the existing high correlation. In addition, it seems that class number five is slightly robust to statistical change and class number six has a low number of samples, that cannot produce meaningful statistical difference. The problem of detecting class six can be solved by decreasing the size of the sliding window.This figure can be generated for different classifiers and show how their decisions are correlated to the ECDF-based distance measures. As an future work, we aim to investigate ECDF-based distance inside different algorithms to better understand their actions. This section is just a hint for future works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>Overall, our preliminary investigation indicates that statistical distance measures offer the potential for providing a suitable indicator for ML performance, specifically for accuracy estimation. In particular, we further denote the following capabilities and limitations for the proposed approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Capabilities of SafeML</head><p>• By modifying the existing statistical distance and error bound measures, the proposed method enables estimation of the accuracy bound of the trained ML algorithm in the field with no label on the incoming data.  • A novel human-in-loop monitoring procedure is proposed to certify the ML algorithm during operation. The procedure has three levels of operation: I) nominal operation allowed with assured ML-accuracy based on the distance estimation, II) buffering data samples to generate estimation, and III) low estimated accuracy estimated, leading to external intervention by automated/human controller being needed.</p><p>• The proposed approach is easy to implement, and can support a variety of distributions (i.e. exponential and normal distribution families).</p><p>• The outcome of the proposed approach can be used as an input for runtime safety analysis in adaptive systems <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref> 6.2 Limitations of the proposed method</p><p>• The proposed algorithm is currently only tackling the safety evaluation problem of the machine-learning-based classification. However, we believe it can be easily expanded for clustering, dimension reduction or any problem that can be evaluated through statistical difference.</p><p>• Some of the machine learning algorithms can be robust to a certain distributional shift or variation in the dataset distribution. This may limit the effectiveness of the discussed distance measures. That being said, the proposed measures can then be used as additional confirmation of the robustness, contributing to certification arguments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>The expansion of ML applications to safety-critical domains is a major research question. We investigate the problem of context applicability of an ML classifier, specifically the distributional shift between its training and observed data. We have identified and evaluated sets of statistical distance measures that can provide estimated upper error bounds in classification tasks based on the training and observed data distance. Further, we have proposed how this approach can be used as part of safety and security-critical systems to provide active monitoring and thus improve their robustness. The overall most effective distance measure was identified to be the Kolmogorov-Smirnov. The proposed human-in-the-loop procedure uses this statistical distance measure to monitor the estimated accuracy of the ML component and notify its AI or human controller when the deviation exceeds specific boundaries. The study is still in its early stages, but we believe the results to offer a promising starting point. The strengths and weaknesses of the proposed approach are discussed in the previous section. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(a) A hypothetical measurement (i.e. from 0 to 100 is Class 1 and from 101 to 200 is Class 2) (b) The estimated probability density function for both Class 1 and Class 2 with a classifier threshold equal to four</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Scatter plot of XOR, Spiral and Circle Benchmarks</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>start the ML-based classification, 80 percent of each dataset was used for training and testing and 20 percent of the dataset has been used for validation, with 10-fold cross-validation. Both linear and nonlinear classifiers have been selected for classification. The Linear discriminant analysis (LDA) and the Classification And Regression Tree (CART) are used as linear methods. Moreover, The Random Forest (RF), K-Nearest Neighbours (KNN) and Support Vector Machine (SVM) are applied as nonlinear methods. As KPIs, the accuracy and Kappa measure are used to measure the performance of each classifier. Finally, as Empirical Cumulative Distribution Function (ECDF)-based statistical distance measures, the Kolmogorov-Smirnov Distance (KSD), Kuiper Distance, Anderson-Darling Distance (ADD), Wasserstein Distance (WD), and a combination of ADD and Wasserstein-Anderson-Darling Distance (WAD) have been selected for evaluation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Confusion matrix for Wednesday Security Intrusion Detection in CICIDS2017 Dataset addition, this plot shows which method has less variation. For instance, the Kuiper distance and WD have the best performance while Chernoff has the least performance. Thursday Attack: This attack occurred on Thursday, July 6, 2017, and various attacks, such as the Web Attack -Brute Force (9:20 -10 a.m.), Web Attack -XSS (10:15 -10:35 a.m.), and Web Attack -Sql Injection (10:40 -10:42 a.m.) have been recorded.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7</head><label>7</label><figDesc>shows a sample result of six statistical measures (Chern-off and five ECDF-based measures) vs. accuracy of the classifier. In this sample, the Kolmogorov-Smirnov and Kuiper measures have better performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Box plot of statistical distance measures vs. accuracy over 100 iterations</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Confusion matrix for Thursday Security Intrusion Detection in CICIDS2017 dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Sample statistical distance measures vs. accuracy for Thursday Security Intrusion Detection in CICIDS2017 dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :Figure 9 :</head><label>89</label><figDesc>Box plot of statistical distance measures vs. accuracy over 100 iterations for Thursday Security Intrusion Detection in CICIDS2017 dataset Correlation between class label numbers and statistical ECDF-based Distance Measures</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Plot of class label and statistical ECDF-based Distances vs. time (Security dataset: Wednesday)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of estimated accuracies vs minimum true accuracy for XOR dataset</figDesc><table><row><cell>Method</cell><cell>KSD</cell><cell>Kuiper</cell><cell>ADD</cell><cell>WD</cell><cell>WAD</cell><cell>BD</cell><cell>MTA</cell><cell>ATA</cell></row><row><cell>LDA</cell><cell>0.772217</cell><cell>0.770600</cell><cell>0.902818</cell><cell>0.755064</cell><cell>0.985666</cell><cell>0.154506</cell><cell>0.50833</cell><cell>0.59121</cell></row><row><cell>CART</cell><cell>0.928179</cell><cell>0.921982</cell><cell>0.987722</cell><cell>0.92545</cell><cell>0.995211</cell><cell>0.497243</cell><cell>0.98744</cell><cell>0.99415</cell></row><row><cell>KNN</cell><cell>0.93057</cell><cell>0.913063</cell><cell>0.993151</cell><cell>0.958768</cell><cell>0.997076</cell><cell>0.497102</cell><cell>0.97489</cell><cell>0.98666</cell></row><row><cell>SVM</cell><cell>0.931045</cell><cell>0.917586</cell><cell>0.993489</cell><cell>0.95819</cell><cell>0.997064</cell><cell>0.496731</cell><cell>0.97916</cell><cell>0.98791</cell></row><row><cell>RF</cell><cell>0.92962</cell><cell>0.910749</cell><cell>0.992742</cell><cell>0.957821</cell><cell>0.997018</cell><cell>0.496856</cell><cell>0.99583</cell><cell>0.99833</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Difference between Distance Measures and MTA for XOR dataset Similar to the XOR dataset, the proposed method can be applied for the spiral dataset.Table 3presents difference between ECDF-based distance measures and minimum true accuracy for this dataset. For brevity, for this dataset and the next one, only the difference table is provided. Based on this table, the KSD and Kuiper distance have better estimation for accuracy of the classifiers for the spiral dataset.</figDesc><table><row><cell>Method</cell><cell>KSD</cell><cell>Kuiper</cell><cell>ADD</cell><cell>WD</cell><cell>WAD</cell><cell>BD</cell></row><row><cell>LDA</cell><cell>0.263883</cell><cell>0.262267</cell><cell>0.394484</cell><cell>0.246731</cell><cell>0.477333</cell><cell>0.353828</cell></row><row><cell>CART</cell><cell>0.059269</cell><cell>0.065466</cell><cell>0.000274</cell><cell>0.06199</cell><cell>0.007763</cell><cell>0.490205</cell></row><row><cell>KNN</cell><cell>0.044320</cell><cell>0.061833</cell><cell>0.018256</cell><cell>0.016127</cell><cell>0.02218</cell><cell>0.477793</cell></row><row><cell>SVM</cell><cell>0.048122</cell><cell>0.061580</cell><cell>0.014322</cell><cell>0.020976</cell><cell>0.017897</cell><cell>0.482310</cell></row><row><cell>RF</cell><cell>0.066207</cell><cell>0.085084</cell><cell>0.003092</cell><cell>0.038012</cell><cell>0.001184</cell><cell>0.499102</cell></row><row><cell>Spiral Dataset:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Difference between Distance Measures and MTA for Spiral dataset</figDesc><table><row><cell>Method</cell><cell>KSD</cell><cell>Kuiper</cell><cell>ADD</cell><cell>WD</cell><cell>WAD</cell><cell>BD</cell></row><row><cell>LDA</cell><cell>0.099447</cell><cell>0.088252</cell><cell>0.269975</cell><cell>0.248396</cell><cell>0.528852</cell><cell>0.043445</cell></row><row><cell>CART</cell><cell>0.056131</cell><cell>0.031092</cell><cell>0.149191</cell><cell>0.09477</cell><cell>0.158529</cell><cell>0.355675</cell></row><row><cell>KNN</cell><cell>0.047526</cell><cell>0.075598</cell><cell>0.001468</cell><cell>0.014756</cell><cell>0.002734</cell><cell>0.496559</cell></row><row><cell>SVM</cell><cell>0.047526</cell><cell>0.075598</cell><cell>0.001468</cell><cell>0.014756</cell><cell>0.002734</cell><cell>0.496608</cell></row><row><cell>RF</cell><cell>0.024471</cell><cell>0.050261</cell><cell>0.018778</cell><cell>0.003885</cell><cell>0.019643</cell><cell>0.479893</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Difference between Distance Measures and MTA for Circle dataset</figDesc><table><row><cell>Method</cell><cell>KSD</cell><cell>Kuiper</cell><cell>ADD</cell><cell>WD</cell><cell>WAD</cell><cell>BD</cell></row><row><cell>LDA</cell><cell>0.329391</cell><cell>0.250345</cell><cell>0.412382</cell><cell>0.347450</cell><cell>0.49826</cell><cell>0.236670</cell></row><row><cell>CART</cell><cell>0.114312</cell><cell>0.019111</cell><cell>0.168596</cell><cell>0.099549</cell><cell>0.24322</cell><cell>0.455675</cell></row><row><cell>KNN</cell><cell>0.004833</cell><cell>0.037554</cell><cell>0.027649</cell><cell>0.010871</cell><cell>0.02775</cell><cell>0.498459</cell></row><row><cell>SVM</cell><cell>0.016133</cell><cell>0.043604</cell><cell>0.019147</cell><cell>0.001695</cell><cell>0.01935</cell><cell>0.498808</cell></row><row><cell>RF</cell><cell>0.004663</cell><cell>0.034529</cell><cell>0.027776</cell><cell>0.012814</cell><cell>0.02782</cell><cell>0.468893</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported by the DEIS H2020 Project under Grant 732242. We would like to thank EDF Energy R&amp;D UK Centre, AURA Innovation Centre and the University of Hull for their support.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The need for uncertainty quantification in machine-assisted medical decision making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Begoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kusnezov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="20" to="23" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Do no harm: a roadmap for responsible machine learning for health care</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wiens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sendak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghassemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Doshi-Velez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Heller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saeed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature medicine</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1337" to="1340" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Secure and robust machine learning for healthcare: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Qayyum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qadir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bilal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Al-Fuqaha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08103</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mind the gaps: Assuring the safety of autonomous systems from an engineering, ethical, and legal perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Burton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Habli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lawton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mcdermid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">279</biblScope>
			<biblScope unit="page">103201</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">What is ai? applications of artificial intelligence to dermatology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Du-Harpur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Watt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Luscombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lynch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">British Journal of Dermatology</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Autonomous weapons systems, killer robots and human dignity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharkey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ethics and Information Technology</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="75" to="87" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E M A J H</forename><surname>Davenport</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wilson</surname></persName>
		</author>
		<title level="m">Artificial Intelligence: The Insights You Need from Harvard Business Review</title>
		<imprint>
			<publisher>Harvard Business Press</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Iso/iec jtc 1/sc 42: Artificial intelligence</title>
		<ptr target="https://www.iso.org/committee/6794475.html" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Artificial intelligence and public standards</title>
		<ptr target="https://www.gov.uk/government/publications/artificial-intelligence-and-public-standards-report" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
		<respStmt>
			<orgName>U. C. on Standards in Public Life</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Safety assurance objectives for autonomous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Asgari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ashmore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Banks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bongirwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bradshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bragg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clegg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fenn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Harper</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Concrete problems in ai safety</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Christiano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mané</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.06565</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5767" to="5777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">The cramer distance as a solution to biased wasserstein gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.10743</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Accelerating t-sne using tree-based algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3221" to="3245" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Can you trust this prediction? auditing pointwise reliability after learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schulam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saria</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.00403</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Uncertainty wrappers for data-driven models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kläs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sembach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Safety, Reliability, and Security</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="358" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Fast predictive uncertainty for classification with bayesian deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hobbhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kristiadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hennig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.01227</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Dataset shift in machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quionero-Candela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schwaighofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Theodoridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Koutroumbas</surname></persName>
		</author>
		<title level="m">Pattern Recognition</title>
		<imprint>
			<publisher>Elsevier Inc</publisher>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Performance evaluation and design for variable threshold alarm systems through semi-markov process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aslansefat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Gogani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kabir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Shoorehdeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISA transactions</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="282" to="295" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Introduction to statistical pattern recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fukunaga</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Empirical confidence estimates for classification by deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finlay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Oberman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.09215</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Machine intelligence in healthcare-perspectives on trustworthiness, explainability, usability, and transparency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Cutillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Foschini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mackintosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">D</forename><surname>Mandl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NPJ Digital Medicine</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The chord gap divergence and a generalization of the bhattacharyya distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nielsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2276" to="2280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Distances in probability theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Deza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Deza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Encyclopedia of Distances</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="257" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Empirical behaviour of tests for the beta distribution and their application in environmental research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raschke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stochastic Environmental Research and Risk Assessment</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="79" to="89" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A runtime safety analysis concept for open adaptive systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kabir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sorokos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aslansefat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gheraibia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Reich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saimler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Model-Based Safety and Assessment</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="332" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aslansefat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Katsaros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bozzano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Model-Based Safety and Assessment: 6th International Symposium, IMBSA 2019</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">11842</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Toward generating a new intrusion detection dataset and intrusion traffic characterization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sharafaldin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Lashkari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Ghorbani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Information Systems Security and Privacy</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="108" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A detailed analysis of cicids2017 dataset for designing intrusion detection systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Panigrahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Borah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Engineering &amp; Technology</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="479" to="482" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
