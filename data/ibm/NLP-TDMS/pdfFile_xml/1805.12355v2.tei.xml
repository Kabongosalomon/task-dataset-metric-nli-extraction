<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep-Energy: Unsupervised Training of Deep Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="20181">OCTOBER 2018 1</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alona</forename><surname>Golts</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Freedman</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow</roleName><forename type="first">Michael</forename><surname>Elad</surname></persName>
						</author>
						<title level="a" type="main">Deep-Energy: Unsupervised Training of Deep Neural Networks</title>
					</analytic>
					<monogr>
						<title level="j" type="main">IEEE TRANSACTIONS ON IMAGE PROCESSING</title>
						<imprint>
							<biblScope unit="issue">Y</biblScope>
							<date type="published" when="20181">OCTOBER 2018 1</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Energy functions</term>
					<term>deep neural networks</term>
					<term>unsu- pervised learning</term>
					<term>weakly-supervised learning</term>
					<term>seeded segmenta- tion</term>
					<term>image matting</term>
					<term>single image dehazing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The success of deep learning has been due, in no small part, to the availability of large annotated datasets. Thus, a major bottleneck in current learning pipelines is the timeconsuming human annotation of data. In scenarios where such input-output pairs cannot be collected, simulation is often used instead, leading to a domain-shift between synthesized and realworld data. This work offers an unsupervised alternative that relies on the availability of task-specific energy functions, replacing the generic supervised loss. Such energy functions are assumed to lead to the desired label as their minimizer given the input. The proposed approach, termed Deep-Energy, trains a Deep Neural Network (DNN) to approximate this minimization for any chosen input. Once trained, a simple and fast feed-forward computation provides the inferred label. This approach allows us to perform unsupervised training of DNNs with real-world inputs only, and without the need for manually-annotated labels, nor synthetically created data. Deep-Energy is demonstrated in this paper on three different tasks -seeded segmentation, image matting and single image dehazing -exposing its generality and wide applicability. Our experiments show that the solution provided by the network is often much better in quality than the one obtained by a direct minimization of the energy function, suggesting an added regularization property in our scheme.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>of work for gathering the training data. This bottleneck is even worse in medical imaging, where data is often of higher dimensions, and the annotators must be experienced radiologists <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. In other applications, e.g., single image dehazing, the collection of clear and hazy images of the exact same scene is generally impossible. The common practice in such cases is simulation of input-output pairs <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>. Apart from requiring non-trivial domain knowledge, such a simulation can potentially inject errors into the learning process, creating a domain-shift when treating real-world data.</p><p>All this brings us to the main question this paper addresses: can we train DNNs while avoiding the need for labels, bypassing the above-described bottleneck? In order to answer this question positively, we recall a classic and successful strategy in handling many computer vision and image processing tasks energy minimization. In this pre-deep-learning era approach, a cost function over the unknown output is formulated, reflecting its desired relationship with the input, along with other penalties characterizing the desired result. The "inference", i.e., getting the desired output for a given input, is obtained by minimizing this energy function. For example, in image denoising one such a cost function could force the output to resemble the input, while also being piece-wise smooth. Inference in this case will result in a smoothed, cartoon-like version of the input. The idea we promote in this work is to rely on such energy functions in order to avoid having explicit labels. However, where could we find appropriate energy functions? Fortunately, the computer vision and image processing literature is replete with examples of such functions for solving a wide variety of problems, such as depth from stereo <ref type="bibr" target="#b13">[14]</ref>, super-resolution <ref type="bibr" target="#b14">[15]</ref>, single-image dehazing <ref type="bibr" target="#b15">[16]</ref>, optical flow estimation <ref type="bibr" target="#b16">[17]</ref>, and many more.</p><p>Note that while in data-driven methods training is usually slow while inference is quite fast, in energy minimization the opposite is correct. There is no training involved, but inference requires a tedious (and possibly iterative) minimization of the energy function for each input signal, posing a difficulty in realtime applications. Additionally, any newly formulated energy function must be accompanied with a tailored optimization scheme. Indeed, in order to ease the optimization and use standard tools, often times the energy function is over-simplified while sacrificing output quality.</p><p>Our proposed approach, termed Deep Energy, offers an unsupervised training of DNNs by a direct minimization of a well-chosen energy function, suited for the task at hand. Specifically, we enforce the output of a DNN to minimize a task-specific energy function, when fed with the corresponding input. We do so by optimizing the parameters of the network using SGD (Stochastic Gradient Descent) and back-propagation, arXiv:1805.12355v2 <ref type="bibr">[cs.</ref>LG] <ref type="bibr" target="#b19">20</ref> Aug 2019 such that, averaged over all examples in the training set, the energy function is minimized. Since these energy functions are unsupervised (i.e. do not assume the knowledge of the output), this removes entirely the dependency on manually-annotated or synthetic input-output pairs.</p><p>As this work relies on the classic energy minimization approach and bridges it to the more recent data-driven methods, a natural question to ask is: Why bother learning at all? What is the benefit in the proposed scheme when compared to a direct optimization of the energy at inference time? The answer has several facets to it. The proposed approach replaces the minimization of the energy function by an approximation obtained using DNN, and as a consequence (i) we are free from devising a tailored optimization algorithm for the minimization task; (ii) we can handle more challenging energy functions, which are considered as hard to solve using existing tools; (iii) the inference itself, once the network has finished learning, is computed by an almost instant forward-pass operation, typically orders of magnitude faster than an explicit optimization; and most importantly, (iv) we show that the incorporated DNN induces an effective regularization over the analytic solver, improving its performance. Indeed, the last point is closely related to the main idea appearing in <ref type="bibr" target="#b17">[18]</ref>. To summarize, this work provides the following contributions: 1) We suggest incorporating domain-knowledge into loss functions by a new methodology of unsupervised training of DNNs through task-specific energy functions. 2) We demonstrate this concept by training a single network architecture to preform three different tasks -seeded segmentation, image matting and single image dehazing -each relying on its appropriate task-specific energy function. 3) We provide a clear comparison between the optimizationbased analytical solver and the network's approximated solution, showing that our method achieves an additional effective regularization, which improves the eventual solution in terms of both quality and speed. The remainder of this paper is structured as follows: Section II reviews previous related work; Section III introduces our Deep-Energy approach and provides three different energy functions for treating seeded segmentation (III-B), image matting (III-C) and single image dehazing (III-D). Section IV discusses our experimental environment and reports quantitative and qualitative results. Section V concludes this paper, and Appendix A offers additional technical details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Our General Approach: Our approach is inspired by recent work on style transfer <ref type="bibr" target="#b18">[19]</ref>, which creates a stylized image by optimizing a two-term energy function. These two terms ensure perceptual loss proximity between the input image and its stylized output version, and a similar proximity of the second order statistics between the style and output images. Instead of repeated optimization of the energy function per each input image, the works in <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref> train a feed-forward DNN to produce the stylized image by a near-instant forward-pass operation. They do so in an unsupervised manner by training the network to minimize the energy function directly.</p><p>There is a growing body of work suggesting training DNNs using energy functions <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>. In <ref type="bibr" target="#b21">[22]</ref>, a DNN is trained by minimizing the large-displacement photometric consistency for optical flow estimation. The work reported in <ref type="bibr" target="#b22">[23]</ref> refines coarse 3D face models by training with an SfS (Shape-from-Shading) energy function. In <ref type="bibr" target="#b23">[24]</ref>, a DNN is taught to perform image smoothing using an unsupervised energybased loss function. Finally, the work most resembling ours, <ref type="bibr" target="#b24">[25]</ref>, performs semantic segmentation by combining a weaklysupervised cross-entropy term with an energy-based normalizedcuts regularization. While these works focus on achieving SOTA (State-Of-The-Art) in specific applications, we inspect energyminimization training from a broader and holistic point of view. We formulate energy-based learning for three different applications and show the extra regularization obtained, along with the other benefits, arising from this approach.</p><p>Speeding-up image-processing operators, as reported in <ref type="bibr" target="#b2">[3]</ref>, bears some similarity to our approach as well. In <ref type="bibr" target="#b2">[3]</ref>, an analytical or numerical solver of an energy function is invoked thousands of times to create input and output pairs for supervised learning of a DNN. Once training of the DNN is over, evaluation can be made via an almost instant forward-pass operation, instead of an explicit optimization for each image. As opposed to <ref type="bibr" target="#b2">[3]</ref>, we skip the prolonged optimization for creating input-output pairs, and directly minimize the energy function during training. Doing so achieves effective regularization which actually improves the results of the analytical solver. Past Work on Seeded Segmentation: Seeded segmentation has enjoyed a fair amount of attention with the introduction of the popular Graph-Cuts <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref> and Random-Walker <ref type="bibr" target="#b27">[28]</ref> algorithms. With the emergence of deep learning and fully annotated datasets, such as Pascal VOC 2012 <ref type="bibr" target="#b28">[29]</ref> and Imagenet <ref type="bibr" target="#b29">[30]</ref>, the focus has shifted to automatic semantic segmentation <ref type="bibr" target="#b30">[31]</ref>. Recent attempts to alleviate the burden of pixel-wise annotation have incorporated "weak-supervision" in the form of image-level labels <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, bounding-boxes <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, scribbles <ref type="bibr" target="#b36">[37]</ref> and points <ref type="bibr" target="#b5">[6]</ref>.</p><p>These approaches differ from ours in two important aspects: (1) They perform weakly-supervised semantic segmentation, i.e., the additional supervision is given only at training time, whereas our energy-based seeded segmentation receives seeds both at training and at test time; and (2) Most of these works use an external analytical solver for either pre-processing of input-output pairs, or post-processing to refine the result of the network. We circumvent the use of an external analytical solver and train end-to-end on the energy function directly, reducing time and effort in both training and evaluation. Past Work on Image Matting: The ill-posed problem of image matting commonly requires additional user assistance in the form of seeds or trimaps. A trimap is a rough segmentation, dividing the input image to three sections: constant foreground and background pixels, and unknown pixels whose opacity is to be determined. The two leading approaches of tackling image matting are prior-based and learning-based. Prior-based methods <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref> mitigate the ambiguity in image matting by formulating energy functions whose solution entails a non-trivial optimization. Conversely, learningbased methods <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref> compose input-output pairs for supervised training of DNNs. However, obtaining the ground-truth pixel-wise opacity of a furry/semi-opaque object in the wild is generally infeasible. A common alternative is training on simulated images, either created with oversimplified backgrounds <ref type="bibr" target="#b46">[47]</ref>, or as compositions of non-related foregrounds and backgrounds <ref type="bibr" target="#b43">[44]</ref>. This approach may create a domain-shift in treating challenging real-world images. Our method avoids utilizing the ground truth during training altogether, circumventing this issue entirely. Past Work on Single Image Dehazing:</p><p>Prior to the emergence of deep learning solutions, single image dehazing has been handled as an energy minimization task <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b50">[51]</ref>. This implied choosing a clever prior and incorporating it into an energy function, followed by an execution of a carefully selected optimization scheme. However, in addition to performing repeated optimization for each image, the returned results were often characterized as having non-physical colors and increased saturation and contrast. In recent years, there has been a general shift towards supervised data-driven and deep-learning-based solutions <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>. Generally, it is impossible to capture clear and hazy images sharing the exact same time, scene and lighting conditions. Thus, supervised methods resort to simulating input and output pairs, while relying on the haze formation model <ref type="bibr" target="#b51">[52]</ref>. Hazy images can be reliably synthesized using clearday images and their corresponding depth maps. However, outdoor depth maps are costly and highly inaccurate <ref type="bibr" target="#b52">[53]</ref>; hence, indoor depth images are frequently used instead. This leads to an inherent domain shift when treating outdoor hazy images which are the final target of dehazing algorithms. Our proposed scheme evades this issue by training on real-world outdoor hazy images only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED APPROACH</head><p>In this section we present the Deep Energy scheme, first describing it in general terms, and then discussing three different image processing tasks that harness it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Deep Energy: General Scheme</head><p>Our interpretation of an "Energy Function" is simply a cost function E(x, y) for the given input x and the unknown output y, describing a desired behaviour of the output. For example, the output should be required to be close to the input, while being piece-wise-constant. These two conditions can be explicitly enforced by formulating a specific energy function of the form</p><formula xml:id="formula_0">E(x, y) = y T Ly + (y − x) T (y − x),<label>(1)</label></formula><p>where L is a smoothness-enforcing Laplacian matrix. Considering a general energy function E(x, y), the optimal output given an input x can be derived bŷ</p><formula xml:id="formula_1">y = arg min y E(x, y).<label>(2)</label></formula><p>The process of minimizing the energy function, called "inference", might be computationally exhaustive, depending on the specific energy function, the dimensions involved, and the chosen optimization method. In its core, the reliance on an energy function poses an unsupervised approach, as it does not require any learning, and the optimization is conducted for each signal x individually. The above line of reasoning has been quite popular over the past three decades, and numerous energy functions have been proposed and leveraged for tackling different tasks in the fields of computer vision and signal and image processing. Instead of repeatedly solving the energy function for each input instance x m , we suggest training a DNN to solve the problem posed in Equation <ref type="bibr" target="#b1">(2)</ref>. We do so by directly minimizing an unsupervised energy loss during training. Given an unlabelled dataset X , {x n } M m=1 , we propose representing the output of the energy function as the prediction of a DNN y p (x, θ) where 'p' stands for "prediction", θ are the network parameters, and the network's input is x. To tune the network parameters, we minimize the following energy-based equivalent of the empirical loss over the training set:</p><formula xml:id="formula_2">min θ 1 M M m=1 E(x m , y p (x m ; θ)).<label>(3)</label></formula><p>Thus we learn the network such that, averaged over all examples, the energy function is minimized. This promotes a desired behavior of the output without enforcing closeness to ground-truth labels. We call this strategy Deep Energyunsupervised learning by minimization of a task-specific energy function. The proposed scheme can serve any application, as long as it can be posed in terms of an informative energy function, and for which there is difficulty in collecting largescale data for supervised learning. Examples for such tasks are optical flow estimation <ref type="bibr" target="#b53">[54]</ref>, single image depth estimation <ref type="bibr" target="#b54">[55]</ref>, image retargetting <ref type="bibr" target="#b55">[56]</ref>, Retinex <ref type="bibr" target="#b56">[57]</ref>, and more. In the remaining part of this section we describe three applications in which we have incorporated Deep Energy: seeded segmentation, image matting and single image dehazing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Application 1: Seeded Segmentation</head><p>In seeded segmentation, the input consists of an image to be segmented, and a sparse set of user-provided labelled seeds, indicating the initial location of each object in the image; the output is a pixelwise segmentation map. Seeded segmentation has enjoyed popularity within the medical imaging community <ref type="bibr" target="#b57">[58]</ref>, allowing the user a degree of control over the final segmentation result. In recent years, with the emergence of deep learning, seeded segmentation has been treated from a "weaklysupervised" perspective. In this regime, the user-provided seeds are only given at training, whereas the segmentation is performed automatically at test time, without user assistance.</p><p>The majority of "weakly-supervised" methods <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b5">[6]</ref> use external analytic solvers to extract input-output pairs for supervised training. We, on the other hand, propose to use the Deep Energy scheme, thereby avoiding the need for labeling altogether. For this purpose, we use the energy function of the well-known Random Walker algorithm by Grady et al. <ref type="bibr" target="#b27">[28]</ref>, in which the image is represented as a weighted undirected graph G = (V, E, W ). The vertices V are the pixels, and the edges E correspond to their 4-neighborhoods. The weight of an edge e ij is given by w ij = exp{−β I i − I j 2 }, where I i , I j are the RGB values of the pixels i and j, and β is a global scaling parameter.</p><p>Random walker models each pixel as conducting a random walk over the above described graph. The probability of a given pixel reaching each and every seed/class in the image is also the final score of that class. Formally, let y l ∈ R N be the pixelwise probability of belonging to class l, where N denotes the number of pixels in the image. Denote x l ∈ R N as the "seed image" of class l whose elements are 0 or 1, indicating absence or presence of a seed respectively. The vectorized versions of the initial seed and the final probabilities over L classes are denoted by Y, X ∈ R N ×L , where each column corresponds to a separate class. The random walker probabilities, y l , l ∈ [1, L], can be computed by minimizing the following energy function 2 <ref type="bibr" target="#b27">[28]</ref>:</p><formula xml:id="formula_3">E(X, Y) = l (y l ) T Ly l + λ l (y l − x l ) T Q(y l − x l ) = tr Y T LY + λ(Y − X) T Q(Y − X) ,<label>(4)</label></formula><p>where Q = diag( l x l ) ∈ R N ×N is a matrix whose diagonal elements indicate the presence of a seed of any class at a given pixel, and L is the Laplacian of the graph given as</p><formula xml:id="formula_4">L ij =      d i = j w ij if i = j, −w ij</formula><p>if v i and v j are adjacent nodes, 0</p><p>otherwise.</p><p>The first term in Equation <ref type="formula" target="#formula_3">(4)</ref> is a "smoothness term", ensuring that adjacent pixels with similar colors have similar output probabilities. Conversely, the weights of adjacent pixels in discontinuous regions should be close to zero, allowing for different probabilities in the output. The second is a "data term", encouraging fidelity to the input seeds. The final loss function for training our DNN is a tensor-friendly version of Equation <ref type="formula" target="#formula_3">(4)</ref>, explained in details in Appendix A.</p><p>Returning to our Deep Energy paradigm, our aim is to use it in order to put forward an alternative and faster way for handling this segmentation task. Given a set of input images and their seeds, </p><formula xml:id="formula_6">{I m , X m } M m=1 ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Application 2: Image Matting</head><p>In image matting, an object is extracted from its background by determining the opacity and color of each pixel in the foreground. The input is an image, assumed to be a composite of foreground and background images, and the output is an alpha matte, indicating the opacity of the foreground versus the background in each pixel. The following is known as the "matting equation",</p><formula xml:id="formula_7">I i = α i F i + (1 − α i )B i , ∀i ∈ I,<label>(6)</label></formula><p>where I ∈ R N ×3 is the input RGB image, F, B ∈ R N ×3 are the unknown foreground and background images, and α ∈ R N ∈ [0...1] is the alpha matte. Note that recovering the alpha matte from a single input image is extremely ill-posed, since seven quantities per each pixel must be deduced (the RGB values of F i , B i and the alpha matte α i ). The energy function we use for this task is the closedform matting by Levin et al. <ref type="bibr" target="#b37">[38]</ref>. By incorporating minimal user interaction in the form of seeds and assuming that the background and foreground are locally smooth, <ref type="bibr" target="#b37">[38]</ref> manages to eliminate the dependency on B, F and obtain a closed-form solution for α. Computing the solution of α is obtained by minimizing</p><formula xml:id="formula_8">E(x, α) = α T Lα + λ(α − x) T Q(α − x),<label>(7)</label></formula><p>where Q ∈ R N ×N is a diagonal matrix whose nonzero elements indicate the presence of a foreground or background seed. The vector x ∈ R N is the seed image with 1 s in respective foreground seed locations and 0 s elsewhere. The matrix L is a special Laplacian-like matrix <ref type="bibr" target="#b37">[38]</ref> given by</p><formula xml:id="formula_9">L ij = n|(i,j)∈pn (δ ij − w n ij ),<label>(8)</label></formula><p>where</p><formula xml:id="formula_10">w n ij = 1 |p n | (1 + (I i − µ n ) T (Σ n + ε |p n | I) −1 (I j − µ n )).</formula><p>In the above, i, j are pixels within the patch p n , centered at the pixel n; µ n ∈ R 3×1 , Σ n ∈ R 3×3 are the mean and covariance of the patch of size |p n |; I ∈ R 3×3 is the identity matrix, and ε provides an additional control over the smoothness. The reformulation of Equation <ref type="formula" target="#formula_8">(7)</ref> as a loss function for training is detailed in Appendix A. We should note the clear similarity between this formulation and the random walker used in the seeded segmentation. Still, there are two main differences between the two: (i) The matting problem recovers a single image layer, whereas the segmentation produces L layers; and (ii) The Laplacian L matrices are formed very differently.</p><p>As </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Application 3: Single Image Dehazing</head><p>While seeded segmentation and image matting contain some user intervention, the third application we consider, single image dehazing, is completely unsupervised. Given a hazy input image, our goal is to lift the haze and reveal the hidden details in the image. This is a vital preliminary stage in many computer vision pipelines, including autonomous car navigation and object detection.</p><p>The common haze formation model <ref type="bibr" target="#b51">[52]</ref> postulates a hazy image I ∈ R N ×3 as a linear combination of a clear-day scene radiance image J ∈ R N ×3 and a constant atmospheric component A ∈ R 3×1 , called the "airlight". The relation between these two is governed by a transmission map t ∈ R N , dependent on the pixelwise depth d ∈ R N from the camera:</p><formula xml:id="formula_11">I i = t i J i + (1 − t i )A, ∀i ∈ I t i = exp −βdi ,<label>(9)</label></formula><p>where β is a scattering coefficient, controlling haze thickness. This model induces an under-determined set of 3N equations for the known pixels of I and 4N + 3 unknowns for J, t, A.</p><p>We adopt the energy function of the well-known "Dark Channel Prior" (DCP) <ref type="bibr" target="#b15">[16]</ref> by He et al., who resolved the ambiguity of the haze formation model by assuming a specific prior. DCP is based on a statistical property of natural outdoor images (excluding sky regions), stating that in small patches, the darkest pixel of the patch across all RGB channels tends to zero. This is due to shades and naturally dark and monochromatic objects. The "Dark Image" is defined as a minimum filter over small patches across RGB channels. In case of natural outdoor images, this dark image is mostly zero,</p><formula xml:id="formula_12">J dark i = min c∈{r,g,b} ( min k∈Ω(i) (J c k )) → 0, ∀i ∈ I<label>(10)</label></formula><p>where Ω(i) is a patch around pixel i, and c are the RGB channels. Envoking the DCP on both I and J in the haze formation model and assuming a constant transmission within a small patch, results in the following coarse transmission map:</p><formula xml:id="formula_13">t i = 1 − ω · min c∈{r,g,b} min k∈Ω(i) I c k A c , ∀i ∈ I<label>(11)</label></formula><p>where the constant ω injects a small amount of haze for better visual perception. In <ref type="bibr" target="#b15">[16]</ref>, the resulting block-artifact-filled transmission mapt is smoothed with the same image matting technique in Section III-C by Levin et al. <ref type="bibr" target="#b37">[38]</ref>. The energy function that provides the refined version of the transmission map t is given by </p><formula xml:id="formula_14">E(t,t) = t T Lt + λ(t −t) T (t −t),<label>(12)</label></formula><formula xml:id="formula_15">J = I − A max(t θ , t 0 ) + A,<label>(13)</label></formula><p>where t 0 is a small constant threshold, used to avoid division by zero. The only missing quantity is the airlight A. We follow the heuristic in <ref type="bibr" target="#b15">[16]</ref> of inspecting the 0.1% brightest pixels in the dark channel image of I. Out of these locations, the RGB value of the brightest pixel in I is chosen as the airlight A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. RESULTS</head><p>We now present the results of our Deep Energy approach on the three applications described above: the two weaklysupervised tasks of seeded segmentation and image matting and the fully unsupervised implementation of single image dehazing. Our results clearly show that through energy-based training, the network is able to mimic the results of the original classical solver, and even outperform it in terms of applicationspecific criteria.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Architecture</head><p>We use the same network structure for all three applications, as it was found to provide favorable results for various image-toimage applications. Our network architecture, shown in <ref type="figure" target="#fig_0">Figure  1</ref>, is fully convolutional and based on the Context Aggregation Network (CAN), introduced in <ref type="bibr" target="#b58">[59]</ref>. The spatial dimension of the input, output and all intermediate layers remains the same, without using pooling or strided convolutions, preserving fine details in the image. To capture larger-scale semantic structures, we use dilated convolutions that also aggregate the sparse seed information in both seeded segmentation and image matting. These dilated convolutions have exponentially increasing dilation rates, this way enlarging the network receptive field. We incorporate Resnet-style <ref type="bibr" target="#b59">[60]</ref> skip connections to allow for additional gradient flow through the network layers and to facilitate the direct propagation of finer details in the image.</p><p>Specifically, our network is a cascade of "Dilated Residual Blocks". Each block contains two regular convolution layers, followed by a single dilated-convolution layer. The dilation factor is 2 d , where d is the block number, starting from dilation of 1, then 2, 4, and so on, until 2 d . All convolution layers, apart from the final output, are followed by batch normalization and ReLU nonlinearity, and have 3 × 3 filters with an output width of 32. The final output layer is a linear 1 × 1 convolution of width equal to the size of the desired output. In seeded segmentation, there is an additional softmax layer to output the probabilities of Random Walker. Finally, we add Resnet-style addition skip connections between the input and output of each block, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Datasets</head><p>Seeded Segmentation and Image Matting: We utilize the Pascal VOC 2012 dataset <ref type="bibr" target="#b28">[29]</ref>, augmented with extra annotations from <ref type="bibr" target="#b60">[61]</ref>. We randomly split the original 10, 582 training images to 500 for validation and parameter tuning, and the rest for training. We use the original 1, 449 images in Pascal 'val' as test. We adopt the seed annotations supplied in <ref type="bibr" target="#b36">[37]</ref>. Since seeded segmentation is originally intended as user-assisted segmentation, we use the seeds during training and test time.</p><p>To determine the quality of our solution during validation and test, we evaluate the mean-Intersection-over-Union (mIoU), averaged over all 21 classes. Image matting is originally a two-class task of separating a foreground object from its background. Thus, to construct two-class images out of the multi-class Pascal VOC, we create L copies of the same L-class natural image, with different seed locations from <ref type="bibr" target="#b36">[37]</ref> for each individual class. We repeat the same train-validation split and obtain two-class images, resulting in 14, 772 training and 735 validation images. Note that Pascal VOC was not originally intended for image matting. Two possible alternatives are (i) Smaller datasets <ref type="bibr" target="#b46">[47]</ref> that are less suitable for deep learning applications, and (ii) Carefully controlled simulated datasets <ref type="bibr" target="#b43">[44]</ref> that may not fully capture the behavior in the wild. Instead, we train on easy-to-obtain natural images and use the minimal seeds described above, rather than complex trimaps commonly used in image matting <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b43">[44]</ref>. We evaluate the performance of the trained network on the 27 test images in alphamatting.com, accompanied with corresponding trimaps. We treat the constant background and foreground pixels as seeds; the missing gray pixels are completed by the algorithm. Single Image Dehazing: We use the RESIDE (REalistic Single Image DEhazing) dataset <ref type="bibr" target="#b52">[53]</ref>, containing both real-world and simulated hazy images, accompanied with corresponding clearday ground-truths. In general, collecting real-world pairs of clear and hazy images is impossible. Thus, learning-based methods resort to simulating hazy images with the haze formation model in Equation <ref type="bibr" target="#b8">(9)</ref>. Given a clear-day image and its corresponding depth map, one can generate multiple hazy images with varying amounts of haze thickness β and airlight components A. Since outdoor depth maps are less accurate and much harder to acquire, the common practice is using indoor depth data and creating an indoor dehazing dataset. We, on the other hand, do not need the ground-truth outputs and can directly train on the RTTS -RESIDE's collection of 4, 322 real-world outdoor hazy images. To evaluate the PSNR (Peak Signal-to-Noise Ratio) and SSIM (Structured-Similarity) values of our trained model during validation, we use a random subset of 500 images from RESIDE's OTS dataset. Our test set is RESIDE's collection of 500 outdoor images, called "SOTSoutdoor", and the smaller HSTS, consisting of 10 outdoor images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Implementation Details</head><p>Data Augmentation: In image matting no data augmentation is performed and the single-class images of Pascal VOC are resized using bilinear interpolation to 128 × 128. In seeded segmentation we enlarge the train set by a factor of four to 40, 328 images. The first augmentation is a simple resize of the Pascal VOC images to 128 × 128. The second, third and fourth augmentations are obtained by a horizontal flipping, and random crop to a random-sized square (between 200 pixels and the minimum dimension of the image), and random rotation by 0, 90, 180, 270 degrees. All resulting augmentations are resized using bilinear interpolation to 128 × 128. In Single image dehazing, we again enlarge the training set by a factor of four. The first set are simply the train images resized to 128×128. The other augmentations are horizontal flips, random crops to 256 × 256 or 512 × 512, and random rotations by 0, 45, 90, 135 degrees. Finally, the images are resized to 128 × 128. This creates a total of 17, 288 training images. Experimental Setup: We implement our scheme in Tensor-Flow on a GTX Titan-X Nvidia GPU. In all applications we use the Adam <ref type="bibr" target="#b61">[62]</ref> optimizer for training, and draw the initial network weights from a Gaussian distribution N ∼ (0, 0.1). In seeded segmentation, the network consists of 7 dilated residual blocks with zero padding, and a maximum dilation factor of ×64. Although our network is fully-convolutional, naïve forward-pass of a large image results in an insufficient spread of the seeds. Instead, to evaluate a new larger-sized image, we follow <ref type="bibr" target="#b24">[25]</ref> and resize it (and its seed) to 128 × 128, pass it through the network, and resize the resulting segmentation or probability maps back to original size. We found that simple nearest neighbor interpolation of the segmentation result is accurate while being much faster than interpolating 21 probability maps.</p><p>Our training schedule operates as follows: we first initialize the learning rate to 0.01 and decrease it by a factor of √ 2 every two sign changes of the derivative of the validation loss. We allow a cool-off period of two epochs, during which sign changes are ignored. Finally, our batch size is 13. As to the parameters of the energy function, we found the effect of the parameter λ to be negligible in comparison to β. Thus, we set λ = 1 and tune only the β parameter using the validation set. The best performance of the analytic solution is obtained for β = 100 and the network's best solution corresponds to β = 1000. For our best configuration of β = 1000, we stop training after 25 epochs, corresponding to 15 hours.</p><p>In image matting, the network consists of 6 dilated residual blocks with reflective padding. The learning rate schedule is similar to seeded segmentation and starts from an initial rate of 0.01, decreases by √ 2 every 4 sign changes of the derivative of the validation loss, and features a 2 epoch cooloff period. The batch size is 10 and the hyper-parameters of the energy function are as recommended in <ref type="bibr" target="#b37">[38]</ref>: the patch size is set to 3 × 3, and λ = 1, ε = 10 −5 . Training lasts for 59 epochs, corresponding to roughly 35 hours. During validation, we perform a fully-convolutional forward-pass through the network, with the original input dimensions.</p><p>Finally, in single image dehazing the network consists of 6 dilated residual blocks with zero padding. The learning rate starts from 3 × 10 −4 , exponentially decreased by a factor of 0.96 every 3 epochs. The batch size is 24, and the energy function hyper-parameters are adopted exactly from <ref type="bibr" target="#b15">[16]</ref>: λ = 10 −4 , ω = 0.95, t 0 = 0.1, = 10 −6 , the DCP patch size is 15 × 15, and the soft matting patch size is 3 × 3. Training is stopped after 30 epochs, which are roughly 8 hours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Quantitative Results</head><p>First, we wish to quantify the proximity of the trained network solution to the "analytic solution", obtained by directly minimizing the energy functions in Equations (4), <ref type="bibr" target="#b6">(7)</ref>, and <ref type="bibr" target="#b11">(12)</ref>. By plugging-in the solution of the network or the analytic solver back to the loss function, one can obtain a loss value. We measure this average loss value over the test set for both the analytic and the network solutions and report the results in <ref type="table" target="#tab_3">Table I</ref>. Additionally, we compare the overall quality of the solutions by task-specific criteria. In seeded segmentation, we provide the mIOU score over the Pascal VOC 'val' dataset <ref type="bibr" target="#b2">3</ref> . In image matting, we report the MSE (Mean Square Error) and SAD (Sum Absolute Differences) scores of both solutions on the training set of alphamatting.com. Finally, in single image dehazing we measure the PSNR and SSIM metrics on SOTS-outdoor and HSTS. These results are given in <ref type="table" target="#tab_3">Table I</ref> as well.</p><p>In seeded segmentation we show the performance for β = 100 and β = 1000, the best hyper-parameter values for the analytic and network solutions respectively. In both cases, the average loss value of the analytic solution is lower than that of the network. Clearly, Deep Energy cannot reach the absolute <ref type="bibr" target="#b2">3</ref> We report the loss of the 128 × 128 images and the mIOU score of the fully-resized segmentation results of both the analytic and network solutions. minimum of the loss function with the chosen architecture 4 . Nonetheless, in terms of the actual quality of the obtained segmentation, represented by the mIOU metric, the network's solution is better. For β = 100 it slightly outperforms the analytic solution, and for β = 1000 it improves it considerably. In addition, while the analytic solution is sensitive to the choice of β (5% difference in mIOU between the two configurations), our network is more robust.</p><p>In image matting, the analytic solver outperforms the network solution. This result is expected since we made a compromise of training on Pascal VOC images, originally intended for the coarser task of semantic segmentation. The test set, however, consists of challenging furry and hairy objects, relevant to image matting. The ideal solution would be constructing a large-scale weakly-annotated image matting dataset, but it is out of the scope of this paper. That said, the network's solution is competitive due to its much improved run-time.</p><p>Finally, in single image dehazing, we encounter an even stronger regularization ability as compared to the analytic solution, reflected in a 7 dB increase in PSNR and a substantial increase in SSIM. We believe that this large gap is attributed to the relative weakness of the DCP energy in dealing with the sky regions in outdoor images. Our network is able to mitigate this difficulty by facing thousands of real-world hazy images and characterizing the general appearance of the sky.</p><p>Recall that our initial goal was teaching the network to approximate the minimizer of the energy function. However, by early-stopping the learning process, before reaching the absolute minimum, we often obtain effective regularization. This regularization may stem either from the network architecture, as shown in <ref type="bibr" target="#b17">[18]</ref>, or from the learning process, as shown in <ref type="bibr" target="#b62">[63]</ref>. Further extensive exploration of this regularization is left for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Runtime Comparison</head><p>The analytic solver, as efficient as it may be, requires separate optimization for each input image. Our method, on the other hand, allows for a fast prediction via a simple forwardpass operation over the trained network. To demonstrate the efficiency of our approach, we provide a runtime comparison with the analytic solver for each application. We implement the analytic solver in fully-vectorized Numpy-Scipy code, and compare to the network solution, implemented in TensorFlow. Note that in seeded segmentation we compare the overall runtime of calculating the solution of the 128 × 128 images, along with the interpolation of the segmentation result back to original size. In other applications we compute the solutions for the original sizes of the test images. The last row of <ref type="table" target="#tab_3">Table  I</ref> shows the average evaluation time for each test set using the analytic and network solutions. One can clearly see the benefit in terms of speed in favor of Deep Energy, reflected in a speedup factor of 22 − 56 across all applications. We should note that our approach enables incorporating more complex energy functions into real-time applications, while avoiding the computationally-heavy direct minimization algorithms.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Qualitative Results</head><p>We turn to present the results of seeded segmentation on images taken from Pascal VOC 2012 'val' dataset, accompanied with seeds collected by <ref type="bibr" target="#b36">[37]</ref>. The upper and lower parts of <ref type="figure" target="#fig_3">Figure 5</ref> 2 feature results with β = 1000 and β = 100 respectively. For the analytic solution, higher values of β lead to better localized, but often unstable, results (e.g., holes within objects and over-segmentation), thus a lower value of β = 100 is optimal. The network solution, however, returns blurrier probability maps, thus an initially higher value of β = 1000 creates a better balance. Overall, the analytic solution, by its construction, is highly affected by local grayscale changes in the image. For example, it is strongly affected by the white <ref type="bibr" target="#b4">5</ref> Here and elsewhere, visual results are best viewed with zoom of 300%. straps around the horse's head in the first row, or the train window in the sixth row. The network solution, on the other hand, seems to better perform in these cases, due to the extra regularization induced by the chosen network. While the initial energy function used for the analytic and network solutions are the same, the solutions may differ and depend on the hyper-parameters and the network's architecture.</p><p>In image matting, we show the results of images from the training set of alphamatting.com <ref type="bibr" target="#b46">[47]</ref>, which feature heavier user assistance in the form of trimaps. Although we trained on Pascal VOC 2012, originally intended for image segmentation, and used minimal seeds, our network solution, shown in <ref type="figure">Figure  3</ref>, exhibits good generalization ability. In the top four rows, the analytic and network solutions appear similar, although the network returns slightly blurrier results. In the fifth row, the holes of the decorative object are completely filled by the analytic solution, whereas the network solution is closer to the ground truth. The last row showcases the failure of the network in capturing the very fine hair on the trolls' heads. This blurriness is a result of the architecture that features a long chain of repeated convolutions. It can possibly be remedied by an additional "refinement network" as suggested in <ref type="bibr" target="#b43">[44]</ref> and adapted to the Deep Energy loss. We leave this important direction for future work. We present the qualitative performance for the single image dehazing problem on RESIDE's HSTS benchmark images. The analytic solution amplifies the contrast and colors in the image, especially in the sky regions, where the dark channel prior assumption is not valid. Our solution, on the other hand, returns a more realistic result, much similar to the ground truth. While we train our network using the DCP energy function, our aim is not reaching the absolute global minimum of the energy function. By an early-stopping during training, we achieve an additional effective regularization and provide even better results than DCP. <ref type="figure" target="#fig_3">Figure 5</ref> shows real-world image results of our network, trained for 27 and 30 epochs. As can be seen, as training progresses the network reaches a lower minimum of the energy function, providing similar results as classical DCP. Early stopping after 27 epochs provides a more subtle dehazing effect without an enhanced contrast, but with residual haze. By changing the training stopping point, we can control the proximity of the network to the analytic solution of DCP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>This work has introduced a new and principled approach for unsupervised training of DNNs, through direct minimization of energy functions that describe the desired inference. The proposed Deep Energy paradigm has been demonstrated on three applications: seeded segmentation, image matting and single image dehazing. Our approach incorporates task-specific domain knowledge into the loss function and allows for reduced dependency on annotated and synthetic datasets. We have demonstrated that even though we train our network to approximate the minimization of a certain energy function, our solution is faster and often of much better quality, compared to that of the analytic alternative. This implies an effective regularization, which may stem either from the architecture itself or from the learning process. Future directions of research include combining supervised and energy-based losses, investigating the source of added regularization, and a derivation of a theoretical comparison between supervised and energy-based training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX TENSORIZATION OF ENERGY FUNCTIONS</head><p>Seeded Segmentation One can directly use the energy expression in Equation (4) as a loss function during training. Instead, we convert this function to a more "tensor-friendly" form using a common expansion of Laplacian matrices:</p><formula xml:id="formula_16">E(X, Y) = 1 2 l (i,j)∈E w ij (y l i − y l j ) 2 + λ i l x l i l y l i − x l i 2 .<label>(14)</label></formula><p>Then, we concatenate C identical copies of the output Y ∈ R N ×L , along the last dimension, to createỸ ∈ R N ×L×C , where C is the number of neighbors (C = 4 in our case). Further, we formỸ η ∈ R N ×L×C as a concatenation of the C "neighbor images" of the output y l ; in the case of 4neighborhoods, the neighbor images are simply the image y l shifted left, right, up and down. Finally, the weights can be represented as an N × C matrix; we then take L copies of this matrix and put them in the 3-tensor W ∈ R N ×L×C . The energy function can now be written as follows:</p><formula xml:id="formula_17">E(X, Y) = 1 2 B b=1 N n=1 L l=1 C c=1 W (Ỹ −Ỹ η ) 2 + λ B b=1 N n=1 L l=1 X L l=1 (Y − X) 2 ,<label>(15)</label></formula><p>where we have summed over the batch dimension, b = 1...B; and the powers are taken elementwise.</p><p>Image Matting We again rephrase the first term in the energy function in Equation <ref type="formula" target="#formula_8">(7)</ref> in terms of weights:</p><formula xml:id="formula_18">α T Lα = N n=1 9 i=1 9 j=1 w n ij (α i − α j ) 2 ,<label>(16)</label></formula><p>where we sum over all overlapping patches around N pixels in the resulting alpha matte, as well as over all possible combinations of pixel pairs i, j in a given 3 × 3 patch, where the total number of combinations is (3 2 ) * (3 2 ) = 81. The weights w n i,j are given in 8. We can then add the tensorized data term to get the final loss function:</p><formula xml:id="formula_19">E(X, α) = B b=1 N n=1 K k=1 W (α I −α J ) 2 + λ B b=1 N n=1 [X F + X B ] [α − X F ] 2 ,<label>(17)</label></formula><p>where k ∈ [1...K = 81] indexes the pixel pairs i, j in a 3 × 3 patch, and W ∈ R B×N ×81 is the matrix of weights.α I ,α J ∈ R B×N ×81 are repetitions of the alpha matte; the first represents the alpha mattes in index i → (1, .., 1, 2, ..., 2, ..., 9, ..., 9) ∈ R 81 , and the second represents the alpha mattes in index j → (1, 2, ..., 9, 1, 2, ..., 9, ..., 1, 2, ...9) ∈ R 81 . The data term is exactly the same as in seeded segmentation, only there is no need for summation over the classes l; X F ∈ R B×N and X B ∈ R B×N are the foreground and background seed images.</p><p>Single Image Dehazing Note that the energy for this task, given in <ref type="bibr">Equation 12</ref>, is exactly identical to the image matting energy in 7; α, X are replaced with t,t correspondingly, and the seed matrix Q is now identity. Following the same steps as in matting tensorization, we get the following loss function:</p><formula xml:id="formula_20">E(t,t) = B b=1 N n=1 K k=1 W (T I − T J ) 2 + λ B b=1 N n=1 (t −t) 2 ,<label>(18)</label></formula><p>where T I , T J are constructed asα I ,α J in image matting. The coarse transmission mapt is calculated using 11.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>System architecture. At input, the network receives either regular images or images concatenated with seeds, and outputs a prediction. Our network is a series of "Dilated Residual Blocks" in which the spatial dimensions remain intact, but the receptive field increases due to dilated convolutions with increasing dilation factor. Additional Resnet-style skip-connections between the input and output of each block are added for improved gradient flow. The Deep Energy loss receives only the input and prediction of the network, without relying on fully-annotated ground-truth labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Seeded segmentation results with different β values. From left to right: original image, analytic and network probabilities of 'background' class, ground truth segmentation with overlayed seeds, analytic and network segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Single image dehazing results. From left to right: hazy image, analytic solution, network solution and ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Single image dehazing results on real-world images. From left to right: hazy image, analytic solution, network solution after 27 epochs and after 30 epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>our loss module computes the corresponding matrices {L m , Q m } M m=1 , and optimizes for the parameters θ of the network Y p (I m , X m ; θ), such that its prediction over all training examples {Y m } M m=1 minimizes the expected energy function described above. After training has finalized, the network has learned to approximate the output probability Y test , corresponding to a new set of image and seeds {I test , X test }, done via a simple forward-pass operation.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>for the deployment of Deep Energy, during training, pairs of input images and their seeds {I m , x m } M m=1 are fed to the network, and the Deep Energy module computes the intermediate {L m , Q m } M m=1 matrices for the corresponding matting and data terms. The SGD optimization tunes the network parameters such that the predictions {α m } M m=1 minimize the average loss function in Equation (7). At inference, a new α test is computed for a fresh set {I test , x test } by a simple forward pass computation.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>where L is the matting Laplacian from Equation.<ref type="bibr" target="#b7">(8)</ref>, encapsulating the inter-pixel relations in the input hazy image I. As opposed to image matting, there are no user-provided seeds and the data term enforces closeness to all pixels in the coarse transmittance mapt. The detailed implementation of Equation (12) as a loss function for training is given in Appendix A.During training, the network is given the hazy images{I m } M m=1 and Deep Energy computes the intermediate {t m , A m } M m=1 and the final smoothing and data terms. At test time, a new hazy image enters the learned network that computes the predicted transmission map t θ . Given this predicted transmission, the scene radiance J can be computed using the haze formation model</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE I :</head><label>I</label><figDesc>Quantitative results of the Deep Energy approach. Each cell describes (analytic/network) performance values, and the better is shown in Bold. In mIOU, SAD, PSNR and SSIM, higher is better, and in MSE lower is better.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Seeded Segmentation</cell><cell cols="2">Image Matting</cell><cell cols="2">Single Image Dehazing</cell></row><row><cell></cell><cell></cell><cell>β = 100</cell><cell>β = 1000</cell><cell>fine trimap</cell><cell>coarse trimap</cell><cell>HSTS</cell><cell>SOTS-outdoor</cell></row><row><cell></cell><cell>Loss</cell><cell>5.153/8.918</cell><cell>0.669/3.425</cell><cell>1.732/4.141</cell><cell>-</cell><cell>0.138/0.910</cell><cell>0.158/1.059</cell></row><row><cell></cell><cell>mIOU [%]</cell><cell>71.99/72.12</cell><cell>66.97/73.76</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>MSE</cell><cell>-</cell><cell>-</cell><cell>0.029/0.038</cell><cell>0.033/0.047</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>SAD</cell><cell>-</cell><cell>-</cell><cell>6,810/9, 990</cell><cell>10,512/17, 709</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>PSNR [dB]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>15.96/24.41</cell><cell>16.96/24.07</cell></row><row><cell></cell><cell>SSIM</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.877/0.934</cell><cell>0.886/0.933</cell></row><row><cell></cell><cell>Time [sec]</cell><cell>0.657/0.029(×22)</cell><cell>1.599/0.029(×56)</cell><cell cols="2">21.635/0.532(×41) 24.789/0.524(×47)</cell><cell>28.018/0.506(×55)</cell><cell>28.887/0.582(×50)</cell></row><row><cell>input</cell><cell>trimap</cell><cell>GT</cell><cell>analytic</cell><cell>network</cell><cell></cell><cell></cell></row><row><cell cols="5">Fig. 3: Image matting results. From left to right: original</cell><cell></cell><cell></cell></row><row><cell cols="5">image, input trimap, matting ground truth, analytic solution</cell><cell></cell><cell></cell></row><row><cell cols="2">and network solution.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We use a soft version of random walker<ref type="bibr" target="#b27">[28]</ref>, instead of the original formulation, which uses a hard constraint on the seed locations.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">In fact, this is true for all three applications.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page">436</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep speech 2: End-to-end speech recognition in english and mandarin</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ananthanarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Anubhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast image processing with fullyconvolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Multimedia</title>
		<meeting>the 22nd ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">What&apos;s the point: Semantic segmentation with point supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bearman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="549" to="565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">V-net: Fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-A</forename><surname>Ahmadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 Fourth International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="565" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">3d deeply supervised network for automated segmentation of volumetric medical images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="40" to="54" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A fast single image haze removal algorithm using color attenuation prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3522" to="3533" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Single image dehazing via multi-scale convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dehazenet: An end-to-end system for single image haze removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5187" to="5198" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Aod-net: All-in-one dehazing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Gated fusion network for single image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An experimental comparison of mincut/max-flow algorithms for energy minimization in vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1124" to="1137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Restoration of a single superresolution image from several blurred, noisy, and undersampled measured images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Feuer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1646" to="1658" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Single image haze removal using dark channel prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="2341" to="2353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Structure-and motionadaptive regularization for high accuracy optic flow,&quot; in Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1663" to="1668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Deep image prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.10925</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">A neural algorithm of artistic style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.06576</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Texture networks: Feed-forward synthesis of textures and stylized images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lebedev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1349" to="1357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unsupervised deep learning for optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1495" to="1501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning detailed face reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Or-El</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kimmel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5553" to="5562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Image smoothing via unsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH Asia 2018 Technical Papers</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page">259</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Normalized cut loss for weakly-supervised cnn segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Djelouah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schroers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Salt Lake City</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Grabcut: Interactive foreground extraction using iterated graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM transactions on graphics (TOG)</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="309" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Interactive graph cuts for optimal boundary &amp; region segmentation of objects in nd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-P</forename><surname>Jolly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, 2001. ICCV 2001. Proceedings. Eighth IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="105" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Random walks for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Grady</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1768" to="1783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Constrained convolutional neural networks for weakly supervised segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1796" to="1804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Seed, expand and constrain: Three principles for weakly-supervised image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="695" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Stc: A simple to complex framework for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2314" to="2320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Weakly-and semi-supervised learning of a deep convolutional network for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1742" to="1750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Boxsup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1635" to="1643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Scribblesup: Scribble-supervised convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3159" to="3167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A closed-form solution to natural image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="228" to="242" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A bayesian approach to digital matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Y</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Salesin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2001 IEEE Computer Society Conference on</title>
		<meeting>the 2001 IEEE Computer Society Conference on</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>Computer Vision and Pattern Recognition. II-II</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Shared sampling for real-time alpha matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S</forename><surname>Gastal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Oliveira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="575" to="584" />
			<date type="published" when="2010" />
			<publisher>Wiley Online Library</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A global sampling method for alpha matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2049" to="2056" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Random walks for interactive alpha-matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Grady</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schiwietz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Aharon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Westermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of VIIP</title>
		<meeting>VIIP</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="423" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Optimized color sampling for robust matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition, 2007. CVPR&apos;07. IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep automatic portrait matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="92" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Natural image matting using deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="626" to="643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A perceptually motivated online benchmark for image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gelautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1826" to="1833" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Visibility in bad weather from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Efficient image dehazing with boundary constraint and contextual regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Dehazing using color-lines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Non-local image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Vision through the atmosphere</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">K</forename><surname>Middleton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Geophysik II/Geophysics II</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1957" />
			<biblScope unit="page" from="254" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Benchmarking single-image dehazing and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="492" to="505" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of mathematical models in computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="237" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Image and depth from a conventional camera with a coded aperture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM transactions on graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">70</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A comparative study of image retargeting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gutierrez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sorkine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shamir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM transactions on graphics (TOG)</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">160</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A multiscale retinex for bridging the gap between color images and the human observation of scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Jobson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Woodell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image processing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="965" to="976" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Seeded segmentation methods for medical image analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Talbot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="27" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="991" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Implicit bias of gradient descent on linear convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gunasekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Soudry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.00468</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
