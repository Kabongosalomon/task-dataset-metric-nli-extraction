<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Rethinking of Pedestrian Attribute Recognition: Realistic Datasets and A Strong Baseline</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Jia</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">CRISE</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houjing</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">CRISE</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">CRISE</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaotang</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">CRISE</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
							<email>kaiqi.huang@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">CRISE</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">CAS Center for Excellence in Brain Science and Intelligence Technology</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Rethinking of Pedestrian Attribute Recognition: Realistic Datasets and A Strong Baseline</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Pedestrian Attribute Recognition</term>
					<term>Realistic Datasets</term>
					<term>Multi-label Clas- sification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite various methods are proposed to make progress in pedestrian attribute recognition, a crucial problem on existing datasets is often neglected, namely, a large number of identical pedestrian identities in train and test set, which is not consistent with practical application. Thus, images of the same pedestrian identity in train set and test set are extremely similar, leading to overestimated performance of state-of-the-art methods on existing datasets. To address this problem, we propose two realistic datasets PETAzs and RAPv2zs following zero-shot setting of pedestrian identities based on PETA and RAPv2 datasets. Furthermore, compared to our strong baseline method, we have observed that recent state-of-the-art methods can not make performance improvement on PETA, RAPv2, PETAzs and RAPv2zs. Experiments on existing and proposed datasets verify the superiority of our method by achieving state-of-the-art performance. Code is available at https://github.com/valencebond/ Strong_Baseline_of_Pedestrian_Attribute_Recognition</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Pedestrian attribute recognition <ref type="bibr" target="#b33">[34]</ref> is to predict multiple attributes of pedestrian images as semantic descriptions in video surveillance, such as age, gender and clothing.</p><p>Recently, pedestrian attribute recognition has drawn increasing attention due to its great potential in real world application such as person retrieval <ref type="bibr" target="#b17">[18]</ref>, person search <ref type="bibr" target="#b8">[9]</ref> and person re-identification <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b21">22]</ref>. Similar as many vision tasks, progress on pedestrian attribute recognition is significantly advanced by deep learning. From the pioneer work DeepMAR <ref type="bibr" target="#b15">[16]</ref> based on CaffeNet <ref type="bibr" target="#b7">[8]</ref> to most recent work VAC <ref type="bibr" target="#b9">[10]</ref> based on ResNet50 <ref type="bibr" target="#b10">[11]</ref>, mA performance is increased from 73.79 to 78.47 in RAPv1 <ref type="bibr" target="#b18">[19]</ref> dataset <ref type="bibr" target="#b3">4</ref> .</p><p>However, when various networks are proposed to improve the performance by extracting more discriminative features, a crucial problem in existing popular datasets is  often neglected, i.e. there are a large number of identical pedestrian identities in train and test set. This results in plenty of extremely similar images of the same pedestrian identity in the train and test set. For example, in PETA <ref type="bibr" target="#b6">[7]</ref> and RAPv2 <ref type="bibr" target="#b17">[18]</ref>, the first large-scale pedestrian attribute dataset and the updated version of the most popular dataset RAPv1 <ref type="bibr" target="#b18">[19]</ref>, the images of the same pedestrian identity in train set and test set are almost the same except for negligible background and pose variation, as shown in <ref type="figure" target="#fig_0">Fig. 1a</ref> and <ref type="figure" target="#fig_0">Fig. 1b</ref>. The proportion of common-identity <ref type="bibr" target="#b4">5</ref> images in test set are calculated as shown in <ref type="figure" target="#fig_0">Fig. 1c</ref>. It is worth noting that 57.5% and 31.5% of test set images have similar counterparts of the same pedestrian in the train set of PETA and RAPv2 respectively. Although there are also plenty of similar images in train set and test set of RAPv1, we can not get the accurate proportion of common-identity images in test set, due to pedestrian identity label is not provided. Thus, existing datasets are unreasonable in practical application in which test set identities barely have overlap with identities of train set.</p><p>More importantly, the performance of state-of-the-art(SOTA) methods is overestimated on existing datasets, which misleads the progress of pedestrian attribute recognition. To verify our hypothesis, we reimplement the recent SOTA methods MsVAA <ref type="bibr" target="#b25">[26]</ref>, VAC <ref type="bibr" target="#b9">[10]</ref>, ALM <ref type="bibr" target="#b27">[28]</ref> and experiments are conducted to evaluate the performance of common-identity images and unique-identity images separately on PETA. As illustrated in <ref type="figure">Fig. 2(a</ref> F1, which proves that the performance of existing methods is overestimated. Similar performance degradation is also observed for VAC and ALM methods on RAPv2 as well.</p><p>To address the problem in existing datasets, we propose a realistic setting for pedestrian attribute datasets: pedestrian identities of test set have no overlap with identities of train set, i.e. pedestrian identities follow the zero-shot setting. Based on PETA <ref type="bibr" target="#b6">[7]</ref> and RAPv2 <ref type="bibr" target="#b17">[18]</ref> datasets provided with pedestrian identity label, we construct two realistic datasets PETA zs and RAPv2 zs to make them in line with zero-shot setting of pedestrian identities illustrated in <ref type="figure">Fig. 3</ref>. Consistent performance drop of different SOTA methods in proposed datasets highlights the rationality of our proposed datasets as detailed in <ref type="table" target="#tab_4">Table 2</ref> .</p><p>Furthermore, we find experimentally that SOTA methods can not make performance improvement based on our strong baseline method. There are two reasons. First, performance improvement gained by recent SOTA methods is based on underutilized baseline. Second, SOTA methods resort to localize the attribute-specific region to achieve better performance by attention module <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b9">10]</ref> or Spatial Transformer Network(STN) <ref type="bibr" target="#b27">[28]</ref>. However, the strong baseline itself can achieve a good localization of the attribute area, so further strengthening the localization capability cannot bring more performance improvements.</p><p>The contributions of this paper are as follows:</p><p>-We observe the crucial problem in existing pedestrian attribute datasets, i.e. a large number of identical pedestrian identities in train and test set, which is impractical and misleads the model evaluation. -To solve the dataset problem, we propose two datasets PETA zs and RAPv2 zs following zero-shot setting of pedestrian identities. -Based on our strong baseline, we experimentally find that enhancing localization of attribute-specific area adopted by SOTA methods is not beneficial for performance improvement.</p><p>The rest of this paper is organized as follows. Section 2 reviews the related work of pedestrian attribute recognition. Section 3 rethinks existing pedestrian attribute setting and proposes two realistic datasets. Section 4 proposes our method and experiments are conducts on Section 5. Finally, section 6 concludes this work and discusses future directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Pedestrian Attribute Recognition</head><p>Most recent efforts resort to learning discriminate attribute-specific feature representation by enhancing attribute localization. Li et al. <ref type="bibr" target="#b15">[16]</ref> firstly considered pedestrian attribute recognition as a multi-label classification task and proposed the weighted sigmoid cross entropy loss. Considering better attribute localization can reduce the impact of irrelevant areas, Liu et al. <ref type="bibr" target="#b24">[25]</ref> proposed HydraPlus-Net with multi-directional attention modules to locate fine-grained attributes. Liu et al. <ref type="bibr" target="#b22">[23]</ref> proposed a Localization Guided Network based on Class Activation Map(CAM) <ref type="bibr" target="#b32">[33]</ref> and EdgeBox <ref type="bibr" target="#b34">[35]</ref> to extract attribute-specific local features. Considering corresponding area scales of attributes are various, multi-scale feature maps were reused from different backbone layers <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b25">26]</ref>, i.e. pyramidal feature hierarchy, instead of single feature map of last layer. Guo et al. <ref type="bibr" target="#b9">[10]</ref> utilized the assumption that visual attention regions are consistent between different spatial transforms of same image and proposed an attention consistency loss to get a robust attribute localization. Inspired by Feature Pyramid Network(FPN) <ref type="bibr" target="#b19">[20]</ref>, Tang et al. <ref type="bibr" target="#b27">[28]</ref> constructed Attribute Localization Module with Squeeze-and-Excitation(SE) block <ref type="bibr" target="#b12">[13]</ref> and Spatial Transformer Network (STN) <ref type="bibr" target="#b14">[15]</ref> to enhance attribute localization. From the viewpoint of capturing attribute semantic dependencies, some methods focused on modeling attribute relationship. Wang et al. <ref type="bibr" target="#b29">[30]</ref> transformed pedestrian attribute recognition to sequence prediction by Long-Shot-Term-Memory (LSTM) <ref type="bibr" target="#b11">[12]</ref> to explore attribute context and correlation.</p><p>Compared to previous work, our work rethinks recent progress made on pedestrian attribute recognition from the perspective of datasets and methods. First, the dataset problem we first noticed leads to overestimated performance and misleads the evaluation of recent methods. Thus, we propose two reasonable and realistic datasets. Second, based on a fully utilized baseline network, we find localizing specific-attribute area adopted by recent SOTA methods <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b27">28]</ref> is not beneficial for performance improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Realistic Datasets</head><p>In this section, we first answer two questions: what's wrong with the existing pedestrian attribute datasets and Why is it important for academic research and industry applications. Then we introduce a new realistic setting and propose two reasonable and practical datasets PETA zs , RAPv2 zs .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problems of existing Datasets</head><p>As far as we know, APiS <ref type="bibr" target="#b33">[34]</ref> is the first pedestrian attribute recognition dataset followed by PETA <ref type="bibr" target="#b6">[7]</ref>, RAPv1 <ref type="bibr" target="#b18">[19]</ref>, PA100k <ref type="bibr" target="#b24">[25]</ref>, RAPv2 <ref type="bibr" target="#b17">[18]</ref> which promote the de-velopment of pedestrian attribute recognition. However, no clear, concrete and unified setting is proposed for pedestrian attribute dataset construction.</p><p>For PETA, RAPv1, RAPv2 datasets, randomly splitting is adopted as default setting to construct train and test set and the protocol are used by almost all methods <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b27">28]</ref>. This results in a large number of identical pedestrian identities in train and test set. Considering images of identical pedestrian identities are often collected by the same surveillance camera in very short frame intervals, the appearance of these images is extremely similar with negligible background and pose variation as detailed in <ref type="figure" target="#fig_0">Fig. 1(a)</ref> and <ref type="figure" target="#fig_0">Fig. 1(b)</ref>. As a result, there are plenty of extremely similar images in train set and test set as shown in <ref type="figure" target="#fig_0">Fig. 1(c</ref> To further illuminate the difference between existing datasets and our proposed datasets, the pedestrian identity distribution of PETA is given in <ref type="figure">Fig. 3</ref>. In existing dataset PETA, there are 1106 identical identities in train and test set, accounting for 26.91% identities and 57.70% images of the test set. It means that more than half of test set images have their similar counterparts in train set, which is similar to 'data leakage'. In our proposed PETA zs , there is no overlap between pedestrian identities of train and test set. As for RAPv1 (or RAPv2), all images (or part of images) have no pedestrian identity annotation, so we can not get the accurate identity distribution.</p><p>Given the problem of existing datasets, the reasons why it is crucial for industry application and academic research are given as follows. Whether used as primary task in video surveillance or auxiliary task in person retrieval, pedestrian identities of test set barely overlap with identities of train set. So, existing datasets setting is inconsistent with real world application. More importantly, the performance of SOTA methods is overestimated on existing datasets and the model evaluation is misled. We reimplement recent methods MsVAA <ref type="bibr" target="#b25">[26]</ref>, VAC <ref type="bibr" target="#b9">[10]</ref>, ALM <ref type="bibr" target="#b27">[28]</ref> and report their performance in <ref type="figure">Fig. 2</ref>. Compared to performance on the whole test set, consistent performance degra-dation of three methods on unique-identity images of test set confirms the overestimated model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Construction of Proposed Datasets</head><p>To solve the problem of existing pedestrian attribute datasets, we propose a realistic setting: there is no identical pedestrian identities in train and test set, i.e. pedestrian identities follow the zero-shot setting. Concretely, we propose the following criteria for dataset construction, to serve as reference for future dataset.</p><p>Criteria of proposed datasets:</p><formula xml:id="formula_0">1. I all = I train ∪ I valid ∪ I test , |I train | : |I valid | : |I test | ≈ 3 : 1 : 1 . 2. I train ∩ I valid = ∅, I train ∩ I test = ∅, I valid ∩ I test = ∅ . 3. ||I valid | − |I test || &lt; |I all | × T id . 4. |N valid − N test | &lt; T img . 5. |R valid − R test | &lt; T attr .</formula><p>where I denote the pedestrian identity set, N , R, T denote the number of images, the ratio of positive samples of attributes and pre-defined threshold separately. T id = 0.01, T img = 300, T attr = 0.03 are used in our experiments by default. Subscript train, valid, test denotes train set, validation set and test set separately. | · | is the set cardinality.</p><p>To solve the problem of identical identities between train set and test set, Criterion 1,2,3 are proposed to repartition train set and test set of PETA and RAPv2 datasets based on pedestrian identities to make sure that pedestrian identities follow zero-shot setting. To better evaluate the model performance and control the attribute distribution difference between train set and test set, Criterion 4, 5 are proposed.</p><p>Based on criteria mentioned above, considering pedestrian identity label is only provided in PETA and RAPv2 datasets, two realistic datasets PETA zs and RAPv2 zs are proposed, where subscript zs denotes pedestrian identities of dataset following zeroshot setting. Details of the two proposed dataset are given in <ref type="table" target="#tab_3">Table 1</ref> and supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The Strong Baseline</head><p>Given a dataset D = {(X i , y i )}, y i ∈ {0, 1} M , i = 1, 2, ..., N , where y i indicates ground truth vector and N , M denotes the number of train images and attributes respectively. X i denotes i-th pedestrian image. Pedestrian attribute recognition is a multilabel task to learn to predict attributesŷ i ∈ {0, 1} M , given the pedestrian image X i . The element values of zeros and ones in the label vector y andŷ denote the absence and presence of the corresponding attributes in the pedestrian image.</p><p>Pedestrian attribute model generally adopts multiple binary classifiers with sigmoid function <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b27">28]</ref> instead of a multi-class classifier with softmax function <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b5">6]</ref>. So the loss can be computed as</p><formula xml:id="formula_1">Loss = 1 N N i=1 M j=1 ω j (y ij log(σ(logits ij )) + (1 − y ij ) log(1 − σ(logits ij ))) (1)</formula><p>where σ(z) = 1/(1 + e −z ) and logits ij is the output of classifier layer and ω j adopted here is proposed in <ref type="bibr" target="#b16">[17]</ref> to alleviate the distribution imbalance between attributes.</p><formula xml:id="formula_2">ω j = e 1−rj , y ij = 1 e rj , y ij = 0 (2)</formula><p>where r j is the positive sample ratio of j-th attribute in train set. Denote the feature representation of the i-th example as x i ∈ R d , then the conditional probability output by a deep neural network can be predicted by a linear classifier followed by a sigmoid function in Eq. (3)</p><formula xml:id="formula_3">p ij = P r(Y = y ij |x i ) = 1 1 + e −w T j x i (3)</formula><p>where [w 1 , w 2 , ..., w M ] ∈ R d×M is weight of linear classifier and p ij is the j-th attribute probability of the i-th image. We denote this method as baseline in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets and Evaluation Metrics</head><p>We conduct experiments in four existing pedestrian attribute datasets, PETA <ref type="bibr" target="#b6">[7]</ref>, RAPv1 <ref type="bibr" target="#b18">[19]</ref>, RAPv2 <ref type="bibr" target="#b17">[18]</ref>, PA100k <ref type="bibr" target="#b24">[25]</ref> and two proposed realistic datasets PETA zs , RAPv2 zs . Details of each dataset are given in <ref type="table" target="#tab_3">Table 1</ref>. Two types of metrics, i.e. a label based metric and four instance based metrics, are adopted to evaluate attribute recognition performance <ref type="bibr" target="#b17">[18]</ref>. For label based metric, we compute the mean value of classification accuracy of positive samples and negative samples as the metric of specific attribute, then we take an average over all attributes as mean Accuracy (mA). For instance based metrics, Accuracy, Precision, Recall and F1-score are used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Implementation Details</head><p>The proposed method is implemented with PyTorch and trained end-to-end. ResNet50 <ref type="bibr" target="#b10">[11]</ref> is adopted as backbone to extract pedestrian image feature. Pedestrian images are resized to 256 × 192 with random horizontal mirroring as inputs. SGD is employed for training, with momentum of 0.9, and weight decay of 0.0005. The initial learning rate equals to 0.01 and batch size is set to 64. Plateau learning rate scheduler is used with reduction factor 0.1. Total epoch number of training is 30 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Comparison with State-of-the-art Methods</head><p>Results on proposed datasets To fully validate the rationality of proposed datasets, we reimplement the recent methods MsVAA <ref type="bibr" target="#b25">[26]</ref>, VAC <ref type="bibr" target="#b9">[10]</ref> and ALM <ref type="bibr" target="#b27">[28]</ref> as MsVAA 1 , VAC 1 and ALM 1 respectively based on backbone ResNet50 <ref type="bibr" target="#b10">[11]</ref> . Quantitative experiments are conducted in PETA zs , RAPv2 zs datasets and results are reported in <ref type="table" target="#tab_4">Table 2</ref>. It is worth noting that, compared to existing datasets, remarkable performance drop of all methods exists on proposed datasets even if PETA zs have more train images compared to PETA as shown in <ref type="table" target="#tab_3">Table 1</ref>. The result further validates our insight that performance on existing datasets is overestimated and existing datasets mislead the model evaluation. We find experimentally that there is a trade-off between Precision and Recall, so mA and F1 score are more reliable and convinced.The proposed method achieves state-of-the-art performance, with significantly less parameters and computation. Results are reimplemented with the same setting of our baseline for a fair comparison. Results on existing datasets Experiments are also conducted on existing PETA, RAPv1, PA100k datasets to make a comparison with recent methods and results are reported in <ref type="table" target="#tab_5">Table 3</ref>. We compare with state-of-the-art methods, including MsVAA <ref type="bibr" target="#b25">[26]</ref>, VAC <ref type="bibr" target="#b9">[10]</ref> and ALM <ref type="bibr" target="#b27">[28]</ref>. According to experiments, we have following observations: 1)  Results are reimplemented with the same setting of our baseline for a fair comparison..</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Ablation Study of Baseline</head><p>To make a fair comparison with previous SOTA methods, we reimplement MsVAA, VAC, ALM methods and report their performance on PETA, PA100k, RAPv1 as well as their corresponding baseline performance, as shown in <ref type="table">Table.</ref> 4. It is worth noting that our baseline achieve a much better performance than baseline of previous methods, even if they adopt a powerful backbone ResNet101 <ref type="bibr" target="#b10">[11]</ref>. And compared to previous SOTA methods reimplemented with same backbone, our baseline achieve a comparable even better performance. We argue that the effectiveness of a method can not be fully verified when compared with a worse baseline. The reason why our baseline can achieve a comparable even better performance than previous methods is that a strong baseline itself can implicitly learn the location of attribute-specific area. We utilize GradCAM <ref type="bibr" target="#b26">[27]</ref> to locate discriminative visual cues of our baseline model. As show in <ref type="figure">Fig. 4</ref>, even if without explicitly modeling the localization of attribute-specific area, our baseline can localize attribute-specific area to learn discriminative representation. The important thing is not to locate the area of specific attribute, but to distinguish the fine-grained attributes in the same are, such as distinguish sandals from sneakers. Compared to original performance of SOTA methods, there is little difference in performance of reimplemented methods except for ALM. The reason is that attention area of ALM is hard bounding-box, which is coarse-grained and introduces environmental noise.  <ref type="figure">Fig. 4</ref>: Specific-attribute attention area of our baseline method on PETA, RAP, PA100k datasets from top to down.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose two realistic datasets PETA zs and RAPv2 zs to solve the unreasonable and impractical setting of existing datasets, which misleads model evaluation. Meanwhile, we find SOTA methods can not make any performance improvement on our strong baseline.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Extremely similar images of the same pedestrian identity in train set and test set. (a) Images in PETA dataset. (b) Images in RAPv2 dataset. (c) The proportions of common-identities images in the test sets of PETA and RAPv2. The proportion of common-identity images in RAPv2 test set is at least 31.5%, due to some images are not labeled with pedestrian identity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>), the performance gaps of MsVAA between common-identity images and unique-identity images are 20.51%, 24.70%, 15.12%, 16.87%, 16.32% in mA, Acc, Precision, Recall and F1 respectively. Significant performance gaps validates the existing datasets are impractical under real scenario. And compared to the performance of all the images in the test set, the performance of unique-identity images is reduced by 12.7%, 12.14%, 6.95%, 8.4%, 7.86% respectively in mA, Acc, Precision, Recall and</figDesc><table><row><cell>20 40 60 80 100 Performance(%)</cell><cell>90.37</cell><cell cols="2">89.32 common-identity images 71.65 66.55 84.35 78.69</cell><cell>93.85</cell><cell cols="2">92.58 unique-identity images 80.32 77.11 87.27 85.51</cell><cell>93.05 all images 78.23 86.09</cell><cell>20 40 60 80 100 Performance(%)</cell><cell>89.21</cell><cell cols="2">88.31 common-identity images 66.15 70.58 83.63 78.94</cell><cell>93.22</cell><cell>91.85 unique-identity images 79.99 76.73 87.63 85.45</cell><cell>92.35 all images 77.88 86.23</cell></row><row><cell>0</cell><cell></cell><cell>mA</cell><cell>Acc</cell><cell cols="2">Prec Metrics</cell><cell>Recall</cell><cell>F1</cell><cell>0</cell><cell></cell><cell>mA</cell><cell>Acc</cell><cell>Prec Metrics</cell><cell>Recall</cell><cell>F1</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">(a) MsVAA</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(b) VAC</cell></row><row><cell cols="13">Fig. 2: Performance of MsVAA [26] and VAC [10] methods on common-identity im-</cell></row><row><cell cols="13">ages, unique-identity images and all images of PETA test set. There is a significant</cell></row><row><cell cols="13">performance gap between common-identity images and unique-identity images as well</cell></row><row><cell cols="13">as unique-identity images and all images of test set. The remarkable performance gap</cell></row><row><cell cols="13">shows the irrationality of existing datasets. Similar phenomenon can be observed on</cell></row><row><cell cols="9">ALM [28] method and RAPv2 dataset as well.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>). Pedestrian identity distribution on PETA and PETA zs . X-axis indicates the pedestrian identity number and Y-axis indicates the proportion of corresponding images of the pedestrian identity. There are 1106 common identities in train and test set, accounting for 19.42% identities (55.27% images) of train set and 26.91% identities (57.70% images) of test set. Our proposed dataset PETA zs solves the problem by completely separating the identities of test set from the identities of train set.</figDesc><table><row><cell>1 2 3 4 5 6 7 8 Probability</cell><cell>1e 4</cell><cell>Current Dataset train set test set</cell><cell></cell><cell>0.2 0.4 0.6 0.8 1.0 1.2 1.4 Probability</cell><cell>1e 3</cell><cell>Proposed Dataset train set test set</cell><cell></cell></row><row><cell>0</cell><cell>0</cell><cell>2000 Pedestrian Identity 4000 6000</cell><cell>8000</cell><cell>0.0</cell><cell>0</cell><cell>2000 Pedestrian Identity 4000 6000</cell><cell>8000</cell></row><row><cell>Fig. 3:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Details of existing datasets and proposed datasets. Zero-shot setting of pedestrian identities is considered in proposed PETA zs and RAPv2 zs datasets. I train , I valid , I test indicates the number of identities in train set, validation set and test set respectively. Pedestrian identities are not provided in RAPv1, PA100k and are partly provided in RAPv2, so the exact quantity cannot be counted, which is denoted by -. Due to the overlapped identities between I train , I valid and I test of PETA, the sum of I train , I valid and I test is not equal to that in PETA zs . Attribute here denotes the number of attributes used for evaluation.Dataset Setting Itrain I valid Itest Attribute Images Ntrain N val Ntest</figDesc><table><row><cell cols="4">PETA existing 4,886 1,264 4,110</cell><cell>35</cell><cell cols="3">19,000 9,500 1,900 7,600</cell></row><row><cell cols="4">PETAzs zero-shot 5,211 1,703 1,785</cell><cell>35</cell><cell cols="3">19,000 11,051 3,980 3,969</cell></row><row><cell>RAPv2 existing</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>54</cell><cell cols="3">84,928 50,957 16,986 16,985</cell></row><row><cell cols="4">RAPv2zs zero-shot 1,508 546 535</cell><cell>54</cell><cell cols="3">26,632 14,729 5,961 5,948</cell></row><row><cell>RAPv1 existing</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>51</cell><cell>41,585 33,268</cell><cell>-</cell><cell>8,317</cell></row><row><cell>PA100k existing</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>26</cell><cell cols="3">100,000 80,000 10,000 10,000</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison of four methods on the PETA, RAPv2 datasets. We use zero-shot setting to denote our proposed PETA zs or RAPv2 zs dataset. existing 84.35 78.69 87.27 85.51 86.09 77.87 67.19 79.03 79.79 79.04 141.27 6.28 zero-shot 71.03 59.38 74.75 70.10 72.37 71.32 63.59 77.22 76.62 76.44 VAC[10] 1 existing 83.63 78.94 87.63 85.45 86.23 76.74 67.52 80.42 78.78 79.24 23.61 14.335 zero-shot 71.05 58.90 74.98 70.48 72.13 70.20 65.45 79.87 76.65 77.07 ALM[28] 1 existing 84.24 77.84 85.79 85.60 85.41 78.21 66.98 78.25 80.43 78.93 30.86 4.32 zero-shot 70.67 58.56 72.97 71.31 71.65 71.97 64.52 77.28 77.74 77.06 Baseline existing 85.11 79.14 86.99 86.33 86.39 77.34 66.12 81.99 75.62 78.21 23.61 4.05 zero-shot 71.84 58.77 77.06 68.24 71.72 70.83 63.63 82.28 72.22 76.34</figDesc><table><row><cell>Five metrics,</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Performance comparison with state-of-the-art methods on the PETA, RAPv1, PA100k datasets. Five metrics, mA, Accuracy, Precision, Recall, F1 are evaluated. Accu Prec Recall F1 mA Accu Prec Recall F1 mA Accu Prec Recall F1 DeepMAR [16] (ACPR15) CaffeNet 82.89 75.07 83.68 83.14 83.41 72.70 70.39 82.24 80.42 81.32 73.79 62.02 74.92 76.21 75.56 HPNet[25] (ICCV17) InceptionNet 81.77 76.13 84.92 83.24 84.07 74.21 72.19 82.97 82.09 82.53 76.12 65.39 77.33 78.79 78.05 Inception 86.30 79.52 85.65 88.09 86.85 80.68 77.08 84.21 88.84 86.46 81.87 68.17 74.71 86.48 80.16 FocalLoss ResNet50 83.00 76.18 84.85 84.44 84.31 78.49 77.87 86.96 85.00 85.58 77.32 65.91 80.74 75.13 76.47 Baseline ResNet50 85.11 79.14 86.99 86.33 86.39 79.38, 78.56 89.41 84.78 86.55 78.48 67.17 82.84 76.25 78.94</figDesc><table><row><cell cols="7">Method mA JRL [30] (ICCV17) Backbone AlexNet 82.13 -82.55 82.12 82.02 -PETA</cell><cell>-</cell><cell>PA100k -</cell><cell>-</cell><cell cols="2">RAPv1 -74.74 -75.08 74.96 74.62</cell></row><row><cell cols="3">LGNet [23] (BMVC18) Inception-V2 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="6">-76.96 75.55 86.99 83.17 85.04 78.68 68.00 80.36 79.82 80.09</cell></row><row><cell>PGDM [17] (ICME18)</cell><cell cols="11">CaffeNet 82.97 78.08 86.86 84.68 85.76 74.95 73.08 84.36 82.24 83.29 74.31 64.57 78.86 75.90 77.35</cell></row><row><cell>MsVAA[26](ECCV18)</cell><cell cols="6">ResNet101 84.59 78.56 86.79 86.12 86.46 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>VAC [10] (CVPR19)</cell><cell>ResNet50</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="6">-79.16 79.44 88.97 86.26 87.59 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ALM[28] (ICCV19)</cell><cell>BN-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Considering the overlapped identities in train and test set on existing PETA and RAPv1 datasets, performance in PA100k are more convinced. 3) Our proposed method with backbone ResNet50 achieves a better performance with only 16.71% parameters and 64.49% computation than MsVAA method which adopts ResNet101 as backbone. 4) Compared to VAC<ref type="bibr" target="#b9">[10]</ref> using extra training augmentation and two-branch network, our baseline method achieves a comparable performance with only 28.25% computation in PA100k. Replacing linear classifier with cosine-distance based classifier, our method obtains 86.02%, 80.71 80.07% mA in PETA, RAPv1, PA100K, which outperforms baseline by 0.91%, 2.23%, 0.69% respectively. Consistent improvements achieved by proposed method compared to baseline demonstrate our strategy effectiveness, i.e. normalizing the classifier weight of attributes to make it independent on positive samples of attributes.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Performance comparison with reimplement methods and their baseline on PETA, RAPv1, PA100k. Five metrics, mA, Accuracy, Precision, Recall, F1 are evaluated. Parameters(Params) and multiply-accumulate operations(MACs) of various methods are also reported. Inception 82.66 77.73 86.68 84.20 85.57 77.47 75.05 86.61 85.34 85.97 75.76 65.57 78.92 77.49 78.20 --Baseline(ours) ResNet50 85.11 79.14 86.99 86.33 86.09 79.38, 78.56 89.41 84.78 86.25 78.48 67.17 82.84 76.25 78.94 78.69 87.27 85.51 86.09 80.10 76.98 86.26 85.62 85.50 79.75 65.74 77.69 78.99 77.93 141.27 4.93 VAC [10] (CVPR19) ResNet50 83.63 78.94 87.63 85.45 86.23 79.04 78.25 88.01 86.07 86.83 78.47 68.55 81.05 79.79 80.02 23.61 14.335 ALM[28] (ICCV19) ResNet50 84.24 77.84 85.79 85.60 85.41 77.47 75.05 86.61 85.34 85.97 75.76 65.57 78.92 77.49 78.20 30.86 4.32</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="12">PETA mA Accu Prec Recall F1 mA Accu Prec Recall F1 mA Accu Prec Recall F1 Params(M) MACs(G) PA100k RAPv1</cell></row><row><cell cols="7">Baseline(MsVAA[26]) ResNet101 82.67 76.63 85.13 84.46 84.79 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Baseline(VAC[10])</cell><cell>ResNet50</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="6">-78.12 75.23 88.47 83.41 85.86 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="14">Baseline(ALM[28]) BN--</cell><cell>-</cell></row><row><cell>MsVAA[26](ECCV18)</cell><cell cols="2">ResNet50 84.35</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We use RAPv1, RAPv2 to represent dataset published by Li et al.<ref type="bibr" target="#b18">[19]</ref> and Li et al.<ref type="bibr" target="#b17">[18]</ref> respectively.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Common-identity indicates pedestrian identity exists both in train set and test set. Uniqueidentity indicates the identity only exists in train set or test set.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Learning imbalanced datasets with label-distribution-aware margin loss</title>
		<imprint>
			<date type="published" when="2019" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">BBN: Bilateral-branch network with cumulative learning for long-tailed visual recognition</title>
		<idno type="arXiv">arXiv:1912.02413</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A systematic study of the class imbalance problem in convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Buda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Mazurowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="249" to="259" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.03372</idno>
		<title level="m">What is the effect of importance weighting in deep learning? arXiv preprint</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Class-balanced loss based on effective number of samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9268" to="9277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Arcface: Additive angular margin loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4690" to="4699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pedestrian attribute recognition at far distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Multimedia</title>
		<meeting>the 22nd ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="789" to="792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="647" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Attribute-based people search: Lessons learnt from a practical surveillance system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bobbitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pankanti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Multimedia Retrieval</title>
		<meeting>International Conference on Multimedia Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page">153</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Visual attention consistency under image transforms for multi-label image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="729" to="739" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning deep representation for imbalanced classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5375" to="5384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">Spatial transformer networks. In: Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Multi-attribute learning for pedestrian attribute recognition in surveillance scenarios</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ACPR</publisher>
			<biblScope unit="page" from="111" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pose guided deep model for pedestrian attribute recognition in surveillance scenarios</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A richly annotated pedestrian dataset for person retrieval in real surveillance scenarios</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1575" to="1590" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">A richly annotated dataset for pedestrian attribute recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.07054</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Improving person reidentification by attribute and identity learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Localization guided learning for pedestrian attribute recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sphereface: Deep hypersphere embedding for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="212" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hydraplus-net: Attentive deep features for pedestrian analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="350" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep imbalanced attribute classification using visual attention aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sarafianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Kakadiaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="680" to="697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Improving pedestrian attribute recognition with weakly-supervised multi-scale attribute-specic localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Normface: l 2 hypersphere embedding for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM international conference on Multimedia</title>
		<meeting>the 25th ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1041" to="1049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Attribute recognition by joint recurrent learning of context and correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="531" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Towards rich feature discovery with class activation maps augmentation for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1389" to="1398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Weakly-supervised learning of mid-level features for pedestrian attribute recognition and localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pedestrian attribute classification in surveillance: Database and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision workshops</title>
		<meeting>the IEEE international conference on computer vision workshops</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="331" to="338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="391" to="405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for semantic segmentation via class-balanced self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Vijaya Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="289" to="305" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
