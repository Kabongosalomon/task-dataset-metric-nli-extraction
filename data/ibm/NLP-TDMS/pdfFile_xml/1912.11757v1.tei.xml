<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Label Graph Convolutional Network Representation Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer &amp; Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">Florida Atlantic University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufei</forename><surname>Tang</surname></persName>
							<email>tangy@fau.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer &amp; Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">Florida Atlantic University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingquan</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer &amp; Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">Florida Atlantic University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxun</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Hunan University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Label Graph Convolutional Network Representation Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Multi-label learning</term>
					<term>network embedding</term>
					<term>deep neural networks</term>
					<term>label correlation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Knowledge representation of graph-based systems is fundamental across many disciplines. To date, most existing methods for representation learning primarily focus on networks with simplex labels, yet real-world objects (nodes) are inherently complex in nature and often contain rich semantics or labels, e.g., a user may belong to diverse interest groups of a social network, resulting in multi-label networks for many applications. The multi-label network nodes not only have multiple labels for each node, such labels are often highly correlated making existing methods ineffective or fail to handle such correlation for node representation learning. In this paper, we propose a novel multi-label graph convolutional network (ML-GCN) for learning node representation for multi-label networks. To fully explore label-label correlation and network topology structures, we propose to model a multi-label network as two Siamese GCNs: a node-node-label graph and a label-label-node graph. The two GCNs each handle one aspect of representation learning for nodes and labels, respectively, and they are seamlessly integrated under one objective function. The learned label representations can effectively preserve the inner-label interaction and node label properties, and are then aggregated to enhance the node representation learning under a unified training framework. Experiments and comparisons on multi-label node classification validate the effectiveness of our proposed approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Graphs have become increasingly common structures for organizing data in many complex systems such as sensor networks, citation networks, social networks and many more <ref type="bibr" target="#b0">[1]</ref>. Such a development raised new requirement of efficient network representation or embedding learning algorithms for various real-world applications, which seeks to learn lowdimensional vector representations of all nodes with preserved graph topology structures, such as edge links, degrees, and communities etc. The graph edges inherently reflect semantic relevance between nodes, where nodes with similar neighborhood structures usually tend to share identical labeling information, i.e., clustering together characterized by a single grouping label. For examples, in a scientific collaboration network, two connected authors often belong to a common area of science <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, and in a protein-protein interaction network, proteins co-appeared in many identical protein complexes are likely to serve with the similar biological functions.</p><p>To date, a large body of work has been focused on the representation learning of graphs with simplex labels <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, where each node only has a single label which is used to model node relationships, i.e., two nodes in a neighborhood are forced to have an identical unique label in the learning process. However, graph nodes associated with multiple labels are ubiquitous in many real-world applications. For example, in an image network, a photograph can belong to more than one semantic class, such as sunsets and beaches. In a patient social network, a patient may be suffering from diabetes and kidney cancer at the same time. Similarly, in many social networks, such as BlogCatalog and Flickr, users are allowed to join various groups that respectively represent their multiple interests. For all these networks, each node not only has content (or features), it is also associated with multiple class labels.</p><p>In general, multi-label graphs primarily differ from simplexlabel graphs in twofold. First, every node in a multi-label graph could be associated with a set of labels, thus graph structures usually encode much more complicated relationships between nodes with shared labels, i.e., an edge could either reflect a simple relationship of some single label or interpret a very complex relationship of multiple combined labels. Second, it has been widely accepted that label correlations and dependencies are widespread between multiple labels <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, i.e., the sunsets are frequently correlated with the beaches, and diabetes could finally lead to kidney cancer. Therefore, the correlation and interaction between labels could provide implicit and supplemental factors to enhance and differentiate node relationships that cannot be explicitly captured by the discrete and independent labels in a simplex-label graph.</p><p>Indeed, multi-label learning is a fundamental problem in the machine learning community <ref type="bibr" target="#b7">[8]</ref>, with significant attentions in many research domains such as computer vision <ref type="bibr" target="#b8">[9]</ref>, text classification <ref type="bibr" target="#b9">[10]</ref> and tag recommendation <ref type="bibr" target="#b10">[11]</ref>. However, research on multi-label graph learning is still in its infancy. Existing methods either consider graphs with simplex labels <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b11">[12]</ref> or treat multiple labels as plain attribute information to enhance the graph learning process <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>. Such learning paradigms, however, neglect the fact that the information of one label may be helpful for the learning of another related label <ref type="bibr" target="#b5">[6]</ref>-the label correlations may provide helpful extra information especially when some labels have insufficient training examples. To address this constrain and meanwhile move forward the graph learning theory a litter, we propose multi-label graph representation learning in this paper, where each node has a collection of features as well as a set of labels. <ref type="figure">Fig. 1</ref>: Illustration of the difference between simplex-label graph learning vs. multi-label graph learning, where the labels of each node are highlighted. In a simplex-label graph, each node is associated with only one label and performs singlelabel classification based on learned representations. In a multi-label graph, each node may be associated with multiple labels and these node labels are often highly correlated to represent node semantics. <ref type="figure">Figure 1</ref> illustrates the difference between our studied problem and the traditional simplex-label graph learning. We argue the key for multi-label graph learning is to efficiently combine network structures, node features and label correlations for enriched node relationships modeling in a mutually reinforced manner.</p><p>Incorporating labels and their correlations with graph structures for graph representation learning is a nontrivial task. First, in a multi-label graph, two linked nodes may share one or multiple identical labels, thus their affinity cannot be simply determined by one observed edge that is indistinguishable from others. Second, while each label can be seen as an abstraction of nodes sharing similar network structures and features, the label-label correlations would bring about dramatic impact on the node-node interactions, thus it is hard to constrain and balance the two aspects of relations modeling for an optimal graph representation learning as a whole. Recently, a general class of neural network called Graph Convolutional Networks (GCN) <ref type="bibr" target="#b14">[15]</ref> shows good performance for learning node representations from graph structures and features by performing the supervised single-label node classification training. GCN operates directly on a graph and induces embedding vectors of nodes based on the spectral convolutional filter that enforces each node to aggregate features from all neighbors to form its representation.</p><p>In this paper, we advance this emerging tool to multilabel node classification and propose a novel model called Multi-Label GCN (ML-GCN) to specifically handle the multilabel graph learning problem. ML-GCN contains two Siamese GCNs to learn label and node representations from a highlayer label-label-node graph and a low-layer node-node-label graph, respectively. The high-layer graph learning serves to model label correlations, which only updates the label representations with preserved labels, label correlations and node community information by performing a single-label classification. The derived label representations are subsequently aggregated to enhance the low-layer graph learning, which carries out node representation learning from graph structures and features by performing a multi-label classification. Learning in these two layers can enhance each other in an alternative training manner to optimize a collective classification objective.</p><p>Our main contributions are summarized as follows:</p><p>1) We advance the traditional simplex-label graph learning to a multi-label graph learning setting, which is more general and common in many real-world graph-based systems. 2) Instead of treating multiple labels as flat attributes, like many existing methods do, We propose to leverage label correlations to strengthen and differentiate edge relationships between nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3)</head><p>We propose a novel model ML-GCN to handle multilabel graphs. It can simultaneously integrate graph structures, features, and label correlations for enhanced node representation learning and classification.</p><p>The rest of this paper is organized as follows. Section II surveys the related work. Section III reviews some preliminaries, including definition of the multi-label graph learning problem and the graph convolutional networks used in our approach. The proposed model for multi-label graph embedding is introduced in Section IV. Section V reports experiments and comparisons to validate the proposed approach. Finally, Section VI concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head><p>This section presents existing works related to our studied problem in this paper, including multi-label learning and graph representation learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Multi-label Learning</head><p>Multi-label learning is a classical research problem in the machine learning community with applications ranging from document classification and gene function prediction to automatic image annotation <ref type="bibr" target="#b15">[16]</ref>. In a multi-label learning task, each instance is associated with multiple labels represented by a sparse label vector. The objective is to learn a classifier that can automatically assign an instance with the most relevant subset of labels <ref type="bibr" target="#b7">[8]</ref>. Techniques for multi-label classification learning can be broadly divided into two categories <ref type="bibr" target="#b16">[17]</ref>: the problem transformation-based and the algorithm adaptionbased. The former class of methods generally transforms the multi-label classification task into a series of binary classification problems <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b17">[18]</ref> while adaption-based methods try to generalize some popular learning algorithms to enable a multi-label learning setting <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>.</p><p>Multi-label learning methods for graph-based data did not attract much attention in the past. DeepWalk <ref type="bibr" target="#b20">[21]</ref> was proposed to learn graph representations that are then used for training a multi-label classifier. However, DeepWalk only exploits graph structures, with valuable label and label correlation information not preserved in learned node embeddings. Wang et al. <ref type="bibr" target="#b3">[4]</ref> and Huang et al. <ref type="bibr" target="#b13">[14]</ref> proposed to leverage labeling information along with graph structures for enriched representation learning. However, these methods either consider simplex-label graphs or treat multiple labels as plain attribute genes to support graph structure modeling. Such paradigms still neglect frequent label correlations and dependencies which are demonstrably helpful properties in multi-label learning problems <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Graph Representation Learning</head><p>Graph representation learning <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b21">[22]</ref> seeks to learn lowdimensional vector representations of a given network, such that various downstream analytic tasks like link prediction and node classification can be benefited. Traditional methods in this area are generally developed based on shallow neural models, such as DeepWalk <ref type="bibr" target="#b20">[21]</ref>, Node2vec <ref type="bibr" target="#b22">[23]</ref> and LINE <ref type="bibr" target="#b23">[24]</ref>. To preserve the node neighborhood relationships, they typically perform truncated random walk over the whole graph to generate a collection of fixed-length node sequences, where nodes within the same sequences are assumed to have semantic connections and will be mapped to be close in the learned embedding space. However, above methods only consider modeling the edge links to constrain node relations, which may be insufficient especially when the network structures are very sparse. To mitigate this issue, many methods <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b24">[25]</ref> are proposed to additionally embed the rich network contents or features associated such as the user profiles in a social network and the publication descriptions in a citation network. For example, TriDNR <ref type="bibr" target="#b25">[26]</ref> was proposed to learn simultaneously from the network structures and textual contents, where structures and texts are mutually boosted to collectively constrain the similarities between learned node representations. In general, most real-word graphs are sparse in connectivity (e.g., each node only connects several others in the huge node space), while node contents or features can be leveraged to either enhance node relevance or repair the missing links in the original network structures <ref type="bibr" target="#b26">[27]</ref>.</p><p>However, above representation learning methods belong to the class of shallow neural models, which may have limitations in learning complex relational patterns between graph nodes. Recently, there is growing interest in adapting deep neural networks to handle the non-Euclidean graph data <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b27">[28]</ref>. Several works seek to apply the concepts of convolutional neural networks to process arbitrary graph structures <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b23">[24]</ref>, with GCN <ref type="bibr" target="#b14">[15]</ref> achieving state-of-the-art representation learning and node classification performance on a number of benchmark graph datasets. Following this success, Yao el al. <ref type="bibr" target="#b28">[29]</ref> proposed a Text GCN for document embedding and text classification based on a constructed heterogeneous worddocument graph. Graph Attention Networks (GAN) <ref type="bibr" target="#b11">[12]</ref> are another recently proposed end-to-end neural network structure similar to GCNs, which introduce attention mechanisms that assign larger weights to the more important nodes, walks, or models. Inspired by these deep neural models targeted at mostly the simplex-label graphs, we generalize GCN and propose a novel training framework, ML-GCN, to address the multi-label graph learning problem in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROBLEM DEFINITION &amp; PRELIMINARIES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Problem Definition</head><p>A multi-label graph is represented as G = (v, e, l, A, B, X),</p><formula xml:id="formula_0">where v = {v i } i=1, ··· ,</formula><p>n is a set of unique nodes, e = {e i, j } i, j=1, ··· ,n; i j is a set of edges and l = {c r } r=1, ··· ,m is a set of unique labels, respectively. n is the total number of nodes in the graph and m is the total number of labels in the labeling space. A is a n × n adjacency matrix with</p><formula xml:id="formula_1">A i, j = w i, j &gt; 0 if e i, j ∈ e and A i, j = 0 if e i, j e. B is a n × m affiliation matrix of labels with B i,r = 1 if v i has label c r ∈ l or otherwise B i,r = 0.</formula><p>Finally, X ∈ R n×d i is a matrix containing all n nodes with their features, i.e., X i ∈ R d i represents the feature vector of node v i , where d i is the feature vector's dimension.</p><p>In this paper, the multi-label graph learning aims to represent nodes of graph G in a new m-dimensional feature space H m , embedding information of graph structures, features, labels and label correlations preserved, i.e., learning a mapping f : G → {h i } i=1, ··· ,n such that h i ∈ H m can be used to accurately infer labels associated with node v i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Graph Convolutional Networks</head><p>GCN <ref type="bibr" target="#b21">[22]</ref> is a general class of convolutional neural networks that operate directly on graphs for node representation learning and classification by encoding both the graph structures and node features. In this paper, we focus on the spectralbased GCN <ref type="bibr" target="#b15">[16]</ref>, which assumes that neighborhood nodes tend to have identical labels guaranteed by each node gathers features from all neighbors to form its representation. Given a network G = (v, e, X), which has n nodes and each node has a set of d i -dimensional features (X ∈ R n×d i denotes the feature vector matrix of all nodes), GCN takes this graph as input and obtains the new low-dimensional vector representations of all nodes though a convolutional learning process. More specifically, with one convolutional layer, GCN is able to preserve the 1-hop neighborhood relationships between nodes, where each node will be represented as a m−dimension vector. The output feature matrix for all nodes X <ref type="bibr" target="#b0">(1)</ref> ∈ R n×m can be computed by:</p><formula xml:id="formula_2">X (1) = ρ(ÃX (0) W 0 ) (1) whereÃ = D − 1 2 (I + A)D − 1 2</formula><p>is the normalized symmetric adjacency matrix. D is the degree matrix of (I + A) and I is an identity matrix with corresponding shape. X (0) ∈ R n×d i is the input feature matrix (e.g., X (0) = X ) for GCN and W 0 ∈ R d i ×m is a weight matrix for the first convolutional layer. ρ is an activation function such as the ReLU represented by ρ(x) = max(0, x). If it is necessary to encode k-hop neighborhood relationships, one can easily stack multiple GCN layers, where the output node features of the jth (1 ≤ j ≤ k) layer is calculated by: where W j ∈ R d h ×d h is the weight matrix for the jth layer and d h is the feature vector dimension output in the hidden convolutional layer.</p><formula xml:id="formula_3">X (j+1) = ρ(ÃX (j) W j )<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. THE PROPOSED APPROACH</head><p>In this section, we first present the proposed Multi-Label Graph Convolutional Networks (ML-GCN) model, where the node representations are learned and trained through the supervised classification. Then, we provide the training and optimization details which incorporate node and label representation learning by a collective objective, followed by the computation complexity analysis of the ML-GCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Multi-Label Graph Convolutional Networks</head><p>As discussed in previous sections, the key and challenge for multi-label graph learning are to simultaneously learn from the graph structures, features, labels and label correlations, where different aspects of learning could enhance each other to achieve a global good network representation. To support the incorporation of labels, we can simply build a heterogeneous node-label graph similar to the text GCN <ref type="bibr" target="#b28">[29]</ref>, where common nodes and label nodes are directly connected by their labeling relationships. However, such a diagram makes it hard to model the higher-order label correlations since labels must reach each other through common nodes, i.e., one cannot directly encode k-hop neighborhood node relations and label correlations by a GCN with k convolutional layers. To enable immediate and flexible label interactions, we consider a stratified graph structure shown in <ref type="figure" target="#fig_0">Fig. 2(a)</ref>, which is defined as follows: Label-label-node graph: In this graph, labels connect each other by their co-occurrence relations, i.e., two labels have an edge if they appear at the same time in the label set of some common node. Meanwhile, common nodes are seen as attributes that link with label nodes based on their corresponding labeling relationships.</p><p>Node-node-label graph: In this graph, common nodes link together to form the original graph structure. Meanwhile, associated label nodes are seen as the attributes of the common node-node graph.</p><p>Such a construction of the layered multi-label graph in <ref type="figure" target="#fig_0">Fig. 2</ref>(a) could bring three main favorable properties. First, the label-label connectivity allows direct and efficient higherorder label interactions by simply adjusting the number of convolutional layers in GCN. In addition, common nodes as attributes of the label nodes enable to encode graph community information in learned label representations, as nodes with identical labels tend to form a cluster or community. Lastly, the learned node representations can naturally preserve labels, label correlations and graph community information by taking label nodes as attributes of the node-node graph. <ref type="figure" target="#fig_0">Fig. 2(b)</ref> shows the proposed ML-GCN model that contains two Siamese GCNs to simultaneously learn the label and node representations from the given multi-label graph, where input feature vectors of both the attributed label and common nodes will be regularly updated during the training. First, the highlayer GCN learns label representations from the label-labelnode graph through supervised single-label classification. Let Y ∈ R m×d i be the input feature matrix of all m label nodes, C be the m × m adjacency matrix recording the co-occurrence relations between label nodes, and F be the (n + m) × (n + m) adjacency matrix of the input label-label-node graph. The first convolutional layer aggregates information from both the neighborhood label nodes and the associated common nodes (e.g., label node L1 in <ref type="figure" target="#fig_0">Fig. 2(a)</ref>), where the new m-dimensional label node feature matrix L (1) ∈ R n×m is computed by:</p><formula xml:id="formula_4">L (1) = ρ(F * Y * W l 0 )<label>(3)</label></formula><p>where ρ is an activation function, such as the ReLU represented by ρ(x) = max(0, x), W l 0 ∈ R d i ×m is a weight matrix for the first label-label-node GCN layer and Y * = [Y; X] T is a vertically stacked (m + n) × d i feature matrix.F * is a truncated normalized symmetric adjacency matrix obtained by:</p><formula xml:id="formula_5">F * = F + I m+n ;F = D − 1 2 f F * D − 1 2 f ;F * =F[: m]<label>(4)</label></formula><p>where I m+n is the identity matrix, D f is the degree matrix with D f ,ii = j F * i j . One layer GCN only incorporates immediate label node neighbors. When higher order label correlations need to be preserved, we can easily stack multiple GCN layers (e.g., the layer number k ≥ 2) by:</p><formula xml:id="formula_6">L (k) = ρ(CL (k−1) W l k )<label>(5)</label></formula><formula xml:id="formula_7">whereC = D − 1 2 c (C + I m )D − 1 2 c</formula><p>is the normalized symmetric adjacency matrix and D c,ii = j (C + I m ) i j . The last layer output label embeddings have the same size as the total number of labels, m, and are through a softmax classifier to perform the single-label classification (e.g., assume we consider a two-layer GCN) by:</p><formula xml:id="formula_8">O l =CReLU(F * Y * W l 0 )W l 1 (6) Z l = so f tmax(O l ) = exp(o l ) i exp(o l i )<label>(7)</label></formula><p>where W l 0 ∈ R d i ×d h and W l 1 ∈ R d h ×m are the weight matrices for the first and second label-label-node GCN layers, respectively. Let Y l be the one-hot label indicator matrix of all label nodes, the classification loss can be defined as the cross-entropy error computed by:</p><formula xml:id="formula_9">L 1 = − m d=1 Y l d ln Z l d<label>(8)</label></formula><p>Then, the low-layer GCN learns node representations from the node-node-label graph. Similarly, in the first layer each convolution node aggregates information from both the neighborhood common nodes and the associated attributed label nodes (e.g., take node V2 in <ref type="figure" target="#fig_0">Fig. 2(a)</ref> as an example). Let E be the (n + m) × (n + m) adjacency matrix of the input nodenode-label graph, the d o -dimensional node embeddings output by the first GCN layer are computed as:</p><formula xml:id="formula_10">N (1) = ρ(Ẽ * X * W v 0 )<label>(9)</label></formula><p>where W v 0 ∈ R d i ×m is a weight matrix for the first nodenode-label GCN layer, X * = [X; Y] T andẼ * is a truncated normalized symmetric adjacency matrix obtained by:</p><formula xml:id="formula_11">E * = E + I m ; E = D − 1 2 e E * D − 1 2 e ;Ẽ * =Ẽ[: n]<label>(10)</label></formula><p>where D e is the degree matrix with D e,ii = j E * i j . As in the label-label-node graph, we can also incorporate k-hop neighborhood information by stacking multiple GCN layers:</p><formula xml:id="formula_12">N (k) = ρ(ÃN (k−1) W v k )<label>(11)</label></formula><p>whereÃ = D </p><formula xml:id="formula_13">O v =ÃReLU(Ẽ * X * W v 0 )W v 1<label>(12)</label></formula><formula xml:id="formula_14">L 2 = − i ∈y L * 2<label>(13)</label></formula><p>where W v 0 ∈ R d i ×d h and W v 1 ∈ R d h ×m are the weight matrices for the first and second node-node-label GCN layers, respectively, y is the set of node indices that have labels. Let Y v be the one-hot label indicator matrix of all common nodes, then L * 2 is calculated:</p><formula xml:id="formula_15">L * 2 = Y v i log(σ(O v i )) + (1 − Y v i ) log(1 − σ(O v i )) = Y v i log 1 1 + exp(−O v i ) + 1 − Y v i log exp(−O v i ) 1 + exp(−O v i ) = −Y v i log 1 + exp(−O v i ) − 1 − Y v i log O v i + log 1 + exp(−O v i ) = − 1 − Y v i O v i − log(1 + exp(−O v i )).<label>(14)</label></formula><p>The above two aspects of representation learning for labels and nodes are trained together and impact one another by sharing the common classification labeling space of l from the target graph G, and in the meantime a subset of input features, i.e., through the attributed label nodes in the low-level nodenode graph and the attributed common nodes in the high-level label-label graph. Let the total training epoch for ML-GCN be I, after N-epoch training of common node representations, the input feature matrix for the label-label-node graph will be updated:</p><formula xml:id="formula_16">X new = ρ(O v W v ); Y * = [Y; X new ] T ,<label>(15)</label></formula><p>and in the meantime, after M-epoch training of label representations, the input feature matrix for the node-node-label graph will be updated:</p><formula xml:id="formula_17">Y new = ρ(O l W l ); X * = [X; Y new ] T<label>(16)</label></formula><p>where W v ∈ R m×d i and W l ∈ R m×d i are weight matrices. The collective training procedure for the ML-GCN model has been summarized in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Algorithm Optimization and Complexity Analysis</head><p>As can be seen from Algorithm 1, the node representations and label representations are not learned independently, but depend on each other through shared embedding features learned from two reciprocally enhanced GCNs. In addition, the two level GCNs conduct two supervised classification tasks within the same labelling space: the top label-label-node GCN is doing a single-label classification and the bottom node-nodelabel GCN is doing a multi-label node classification. Finally, The global learning objective is to minimize the following collective classification loss:</p><formula xml:id="formula_18">L = L 1 + L 2<label>(17)</label></formula><p>In this paper, all weight parameters are optimized using gradient descent as in <ref type="bibr" target="#b14">[15]</ref> and <ref type="bibr" target="#b28">[29]</ref>.</p><p>The training of ML-GCN is efficient in terms of the computational complexity. In this paper, we adopt a two-layer GCN and one-layer GCN for learning the node and feature representations, respectively. Since multiplication of the adjacency matrix (e.g., A for the node-node graph and F for the labellabel graph) and feature matrix (e.g., Y * and X * in Eqs. <ref type="bibr" target="#b5">(6)</ref> and <ref type="bibr" target="#b11">(12)</ref>, respectively) can be implemented as a product of a sparse matrix with a dense matrix, the algorithm complexity of ML-GCN can be represented as O((E d i d h m + nd i )+(Ld i m + md i )), where n and m are the number of nodes and labels, E and L are the number of edges in node-node-label graph and labellabel-node graph, respectively. d i is the dimension (for both nodes and labels) of input feature vectors. d h is dimension of the hidden feature vectors produced in the first node-nodelabel GCN layer of all common nodes. In addition, because for most networks m, L and n are generally far more less than E (e.g., as we will see in section V, for the Filckr dataset, E is 4,332,620, compared with m, L and n are merely 194, 3,716 and 8,052, respectively), therefore the complexity of ML-GCN is approximately equivalent to O(E d i d h m), which is the same as GCN. Meanwhile, since Eqs. <ref type="bibr" target="#b14">(15)</ref> and <ref type="bibr" target="#b15">(16)</ref> are not computed in each epoch (e.g., every 50 epoch), the complexity for our model is still O(E d i d h m), the same theoretical asymptotic complexity as the GCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS &amp; RESULTS</head><p>In this section, we compare the proposed approach against a set of strong baselines on three real-world datasets by conducting supervised node classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Benchmark Datasets</head><p>We collect three multi-label networks <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b20">[21]</ref>, Blog-Catalog, Flickr, and YouTube, as the benchmark. They are described as follows.</p><p>BlogCatalog is a network of social relationships among 10,312 blogger authors (nodes), where the node labels represent bloggers' interests such as Education, Food and Health. There are 39 unique labels in total and each node may be associated with one or multiple labels. It is easy to find that users' labels of interest often interact and correlate with each other to enhance the affinities between blogger authors. For example, food is highly related with Health in real life, where two users have both labels food and life should be much closer compared with those whom only share either label food or label life. There are 615 co-occurrence relationships (e.g., correlations) among all 39 labels in this dataset.</p><p>Flickr is a photo-sharing network between users, where node labels represent user interests, such as Landscapes and Travel. There are 8,052 users and 4,332,620 interactions (e.g., edges) among them. Each user could have one or multiple labels of interest from the same labeling space of 194 labels in total.</p><p>YouTube is a social network formed by video-sharing behaviors, where labels represent the interest groups of users who enjoy common video genres such as anime and wrestling. <ref type="table" target="#tab_0">Table I</ref> summaries their detailed statistic information. There are 22,693 users and 192,722 links between them. Each pair of linked users may share multiple identical labels out of the total 47 labels. The number of correlations between these labels is 1,079.</p><p>The detailed statistic information of the above three multilabel networks are summarized in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comparative Methods</head><p>We compare the performance of the proposed method with the following state-of-the-art methods for multi-label node classification:</p><p>• DeepWalk [21] is a shallow network embedding model that only preserves the topology structures. It captures the node neighborhood relations based on random walks and then derives node representations based on SkipGram model. • LINE <ref type="bibr" target="#b29">[30]</ref> is also a structure preserving method. It optimizes a carefully designed objective function that preserves both the local and global network structures, compared with the DeepWalk that encodes only the local structures. • Node2vec [23] adopts a more flexible neighborhood sampling process than DeepWalk to capture the node relations. The biased random walk of Node2vec can capture second-order and high-order node proximity for representation learning. • GENE [13] is a network embedding method that simultaneously preserves the topology structures and label information. Different from the proposed approach in this paper, GENE simply models labels as plain attributes to enhance structure-based representation learning process, whereas our model considers multi-label correlation and network structure for representation learning. <ref type="figure" target="#fig_3">• GCN [15]</ref> is a state-of-the-art method that can naturally learn node relations from network structures and features, where each node forms its representation by adopting a spectral-based convolutional filter to recursively aggregate features from all its neighborhood nodes. • Text GCN <ref type="bibr" target="#b28">[29]</ref> is built on GCN that aims to embed heterogeneous information network. In this paper, we construct heterogeneous node-label graph, where common nodes and label nodes are directly connected by their labeling relationships. • ML-GCN node is a variant of the proposed ML-GCN model that removes attributes of common nodes from the label-label-node graph. Therefore, the community information is not preserved in this method. • ML-GCN 1n is a variant of the proposed ML-GCN model.</p><p>The only difference with ML-GCN is that ML-GCN 1n takes only one convolutional layer while learning from the node-node-label graph. • ML-GCN 2l is a variant of the proposed ML-GCN model, which adopts a two consecutive convolutional layers to learn from the label-label-node graph, compared with one layer in GCN. • ML-GCN is our proposed multi-label learning approach in this paper. It considers a two-layer graph structure-a high-level label-label-node graph which allows the preservation of label correlations and meanwhile a low-level node-node-label graph that enables the label correlationenhanced node representation learning.</p><p>The above baselines can be roughly separated into three categories based on the types of information (e.g., network structures and labels) and how it is incorporated in the graph embedding models. The first class belongs to methods that only preserve graph structures, including DeepWalk, Node2vec, LINE, GCN (e.g., we use the structure-based identity matrix as the original features of all node). The second class includes GENE and Text GCN that preserve both the graph structures and label information, where the labels are modeled as plain attribute information to enhance structurebased representation learning. The proposed method MG-GCN and its variants (ML-GCN node , ML-GCN 1n , and ML-GCN 2l ) represent the thrid class, which not only preserve structural and label information, but also the correlations between labels.</p><p>It is worth noting that we designed three variants of MG-GCN (including ML-GCN node , ML-GCN 1n , and ML-GCN 2l ) to validate its performance under different settings. This allows us to fully observe ML-GCN's performance and conclude which part is playing major roles for multiple-label GCN learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experiment Setup</head><p>There are many hyper-parameters involved. Some are empirically set <ref type="bibr" target="#b28">[29]</ref> while others are selected through sensitivity experiments. For ML-GCN, we use two-layer and one-layer GCNs to learn from the node-node-label graph and the labellabel-node graph respectively. We test hidden embedding size, d h , between 50 to 500, training ratios, α, of supervised labeled instances between 0.025 and 0.2, and updating frequencies N and M from 10 to 100, respectively. We also compare the performance of ML-GCN through differing numbers of GCN convolutional layers (e.g., ML-GCN 1n and ML-GCN 2l ). For comparison, we set the learning rate η for gradient decent as 0.02, training epoch as 300, dropout probability as 0.5, L 2 norm regularization weight decay as 0, and the default values of d h , α, N and M as 400, 0.2, 50 and 50, respectively. After selecting the labeled training instances, the rest is split into two parts: 10% as validation set and 90% for testing set.</p><p>It is necessary to mention that all baselines are set to conduct the multi-label node classification (e.g., each node can belong to multiple labels) within the same environmental settings. As metrics used in <ref type="bibr" target="#b20">[21]</ref> and <ref type="bibr" target="#b29">[30]</ref>, we adopt Micro-F1 and Macro-F1 to evaluate the node classification performance, which are defined as follows:</p><formula xml:id="formula_19">Micro − F1 = l i=1 2T P i l i=1 (2T P i + FP i + F N i ) (18) Macro − F1 = 1 l l i=1 2T P i (2T P i + FP i + F N i )<label>(19)</label></formula><p>where l is the set of labels from the target graph G. T P i , F N i and FP i denote the number of true positives, false negatives and false positives w.r.t the ith label category, respectively. All experiments are repeated 10 times with the average results and their standard deviations reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Experimental Results</head><p>Table II presents the comparative results of all methods with respect to the multi-label classification performance within the same environment settings, where the top three best results have been highlighted. From the table, we have the following four main observations.  nodes <ref type="bibr" target="#b0">[1]</ref>. For example, although Node2vec relies on a carefully designed random walk process to capture the node neighborhood relationships, it cannot differentiate the affinities between a node and others within the same walk sequence. In comparison, GCN uses a more efficient way to constrain the neighborhood relations between nodes, where each node only interact with its neighbors in each convolution layer. Such a learning paradigm is more accurate to maintain the actual node relevance reflected by the edge links without introducing noise neighborhood relationships as in Node2vec. • In terms of the performance of methods (GENE and Text GCN) that have incorporated the labels to enhance the structures modeling. GENE is built on DeepWalk to additionally preserve the label information. We can observe that GENE performs slightly better than DeepWalk on both Flickr and YouTube datasets. The reason is that each label is considered as the higher-level summation of a group of similar nodes, thus can be used to supervise and distinguish the neighborhood affinities between nodes within the same random walk sequence. Nevertheless, LINE and Node2vec can perform better than GENE in most cases over three datasets, i.e., the Micro-F1 performance of LINE and Node2vec on BlogCatalog increased 2.0% and 4.6%, respectively. The reason is probably that they have adopted more efficient random walk process to capture node neighborhood relationships. In addition, as we can see from <ref type="table" target="#tab_0">Table II</ref>, the label-preserved model, Text GCN, has no advantages compared with the basic GCN model. The reason is probably that the labels are considered as attributes that have not been leveraged in a meaningful manner. In other words, only the labeled nodes have the attribute of labels in the supervised node representation leaning and training, the scattered labels could have become the noisy information to confuse the neighborhood relationships modeling between nodes. • For deep models that both preserved graph structures, labels and label correlations, we can observe that ML-GCN is consistently superior to the Text GCN in leaning multi-label graphs over all three datasets. The reasons are mainly in three-fold. First, ML-GCN allows immediate and efficient label correlation modeling without depending on common nodes, i.e., labels directly interact with each other over the label-label graph in the proposed model. Second, it is common to use different numbers of convolutional layer (e.g., based on the results observed by comparing ML-GCN and ML-GCN 2l , where the former performs far more better) to learn from the label and node graphs respectively, since the node-node graph is much more complicated than the label-label graph that <ref type="figure">Fig. 4</ref>: Impact of the information updating frequencies. involves simple label interaction patterns. To obtain a model that best fits the given label and node graphs of different scales, one can easily change the number of layers used by the two-layer graph modelings in ML-GCN independently. But this is hard for Text GCN to coordinate the layer settings that are most suitable to model node relations and label relations simultaneously in the node-label graph. Finally, in the structure design of ML-GCN, each label has preserved the community information by taking all related common nodes as attributes, which make the node relations modeling and the label correlations modeling more dependent on each other to optimize the global network representation learning, i.e., the node representations of one label could refine the node representation learning of another correlated label, as we will demonstrate in the case study later. • In terms of the performance of different variants for the proposed ML-GCN, we can condlude that Ml-GCN is superior to ML-GCN node , ML-GCN 1n and ML-GCN 2l , where the possible reasons are given as follows: 1) The comparison between ML-GCN and ML-GCN node demonstrates that taking the common nodes as attributes of the label-label network is beneficial, where labels could learn more enriched representations (with encoding label correlations and the node communities) to refine the neighborhood feature aggregation for node representation learning in the low-level node-node graph; 2) ML-GCN performs better than ML-GCN 2l , which illustrates a single-layer GCN is appropriate to model the label correlations, since compared with the node-node interactions, the label-label interactions are generally simple and explicit; 3) ML-GCN 2l is poorly performed than ML-GCN, which demonstrates that exploring the high-order neighborhood relationships between nodes is important.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Parameter Sensitivity</head><p>We designed extensive experiments to test the sensitivities of various parameters between a wider range of values, such as the training ratio α of labeled nodes, the feature updating frequencies N and M, and the embedding size of the first hidden convolution layer while modeling the node-node-label graph. <ref type="figure" target="#fig_2">Fig. 3</ref> shows the impacts of different portions of labeled training instances. In general, for all test models, we can observe that both the Micro-F1 (e.g., <ref type="figure" target="#fig_2">Fig. 3(a)</ref>) and Macro-F1 (e.g., <ref type="figure" target="#fig_2">Fig. 3(b)</ref>) performances increase with more labeled training nodes. This is reasonable since all these models adopt a supervised node representation learning and training manner, where the model parameters can be fully trained with larger labeled data <ref type="bibr" target="#b28">[29]</ref>. <ref type="figure">Fig. 4</ref> shows the influence of the input feature-updating frequencies controlled by N and M (e.g., used in Eqs. <ref type="bibr" target="#b14">(15)</ref> and <ref type="formula" target="#formula_17">(16)</ref>). We can see from <ref type="figure">Fig. 4(a)</ref> where the performance changes with N but no clear patterns can be observed in Micro-F1, while <ref type="figure">Fig. 4(b)</ref> shows an deceasing trend with larger values of N in the Macro-F1 scores. In comparison, from <ref type="figure">Fig. 4(c)</ref> and <ref type="figure">Fig. 4(d)</ref> the accuracy first increases then decreases with larger values of M w.r.t. both Micro-F1 and Macro-F1 scores. We also test the impact of node embedding size generated by the first convolutional layer with the trend shown in <ref type="figure" target="#fig_3">Fig. 5(a)</ref>. The accuracy fluctuates before peaking at 400 and 450 w.r.t. Micro-F1 and Macro-F1 results, followed by a sharp decline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Case Study</head><p>To illustrate how label correlations affect the multi-label graph learning performance, we present the classification results through four related label categories shown in <ref type="figure" target="#fig_4">Fig. 6</ref>. <ref type="figure" target="#fig_5">Fig. 7</ref> presents their correlation matrix, where deeper colors imply higher correlation between two corresponding labels, i.e., L1 and L2 are highly correlated. We can see in <ref type="figure" target="#fig_4">Fig. 6</ref> that ML-GCN and GCN perform similarly with respect to the node classification of category L1. However, the accuracy of MC-GCN improved over GCN with respect to categories L2, L3 and L4. It is interesting to note how much these categories improved (e.g., L2 &gt; L3 &gt; L4) is related to how frequently they correlate with label L1 respectively. This phenomenon might be caused by the fact that the L1 has an impact on its  correlated labels during training. This also verifies that label interaction is critical for multi-label graph learning, and our proposed ML-GCN model can effectively capture and utilize this property.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS</head><p>In this paper, we formulated a new multi-label network representation learning problem, where each node of the network may have multiple labels. To simultaneously explore label-label correlation and the network topology, we proposed a multi-label graph convolution network (ML-GCN) to build two siamese GCNs, a node-node-label graph and a labellabel-node graph from the multi-label network, and carry out learning of node representation and label representation from the two GCNs simultaneously. Because the two GCNs are unified to achieve one optimization goal, the learning of node representation and label representation can mutually benefit each other for maximum performance gain. Experiments on three real-word datasets verified the effectiveness of ML-GCN in combining labels, label correlations, and graph structures for enhanced node representation learning and classification.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>The proposed ML-GCN model for multi-label graph learning. (a) shows a multi-label graph organized in two layers. (b) shows the proposed architecture that contains two Siamese GCNs to learn from the label-label-node graph and node-node-label graph, respectively. The upper panel uses label-label-node graph to learn label representation (from right to left), and the lower panel uses node-node-label graph to learn node representation (from left to right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>− 1 2 a 1 2aAlgorithm 1 :</head><label>211</label><figDesc>(A+I m )D − and D a,ii = j (A+I n ) i j . The node embeddings output by the last layer have size m and are passed through a sigmoid transformation to perform supervised multilabel classification with the collective cross-entropy loss (e.g., Training ML-GCNs Input : A multi-label graph G = (v, e, l, A, B, X) Output: The node representations O n = {h i } i=1, ··· ,n Initialization: i = 0, the training epoch I, the information updating frequencies M and N while i ≤ I do Feed the label-label-node graph to train label representations; Feed the node-node-label graph to train node representations; if i%M = 0 then Update the feature matrix by Eq. (15); end if i%N = 0 then Update the feature matrix by Eq. (16); end Optimize L 1 and L 2 by the collective classification objective of Eq. (17); i = i +1. end the two-layer GCN are used in this paper) over all labeled nodes computed by:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Algorithm performance comparisons with respect to different percentage of training sample ratios (the x−axis denotes the ratio of training samples comparing to the whole network).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Impact of the hidden node embedding size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>Classification performance (Macro-F1) with respect to different label categories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 :</head><label>7</label><figDesc>Pair-wise label correlation matrix. A higher gray intensity value (excluding main diagonal values) indicates a stronger correlation between two labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Dataset characteristics.</figDesc><table><row><cell>Items</cell><cell>BlogCatalog</cell><cell>Flickr</cell><cell>YouTube</cell></row><row><cell># Nodes</cell><cell>10, 312</cell><cell>8, 052</cell><cell>22, 693</cell></row><row><cell># Edges</cell><cell>333, 983</cell><cell>4, 332, 620</cell><cell>192, 722</cell></row><row><cell># Labels</cell><cell>39</cell><cell>194</cell><cell>47</cell></row><row><cell># Co-occur.</cell><cell>615</cell><cell>3, 716</cell><cell>1, 079</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II :</head><label>II</label><figDesc>Multi-label classification performance comparison. The 1 st , 2 nd , and 3 rd best results are bold-faced, italic-formatted and underscored respectively.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">• Among all methods that encode only the graph topology structures, the shallow neural networks-based methods (e.g., DeepWalk, LINE and Node2vec) perform poorly with a wide gap compared with deep model GCN over all three datasets, i.e., on BlogCatalog network, the classification performance of GCN improved 30.6% and 70.9% over Node2vec w.r.t Micro-F1 and Macro-F1, respectively. This is because shallow models have limitations in learning complex relational patterns among</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Network representation learning: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Big Data</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Defining and evaluating network communities based on ground-truth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge and Information Systems</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="181" to="213" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Attributed social network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2257" to="2270" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Community preserving network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Variational graph auto-encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07308</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-label learning by exploiting label correlations locally</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-sixth AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multilabel classification with label correlations and missing labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Eighth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-label classification using hierarchical embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Pujari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Padmanabhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Sahu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>Kagita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="page" from="263" to="269" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multilabel image classification with regional latent semantic dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2801" to="2813" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Online multi-label dependency topic models for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Burkhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kramer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page" from="859" to="886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A topic-sensitive method for mashup tag recommendation utilizing multi-relational service data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Services Computing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<title level="m">Graph attention networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Incorporate group information to enhance network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM International on Conference on Information and Knowledge Management</title>
		<meeting>the 25th ACM International on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1901" to="1904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Label informed attributed network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Tenth ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="731" to="739" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Large margin metric learning for multi-label prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Ninth AAAI Conf. on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A review on multi-label learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1819" to="1837" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Binary relevance for multi-label learning: an overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers of Computer Science</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="191" to="202" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ml-knn: A lazy learning approach to multi-label learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2038" to="2048" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Largescale multi-label text classification-revisiting neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Mencía</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fürnkranz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint european conference on machine learning and knowledge discovery in databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="437" to="452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.00596</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Network representation learning with rich text information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Fourth International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Tri-party deep network representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Network</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Probabilistic latent document network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Lauw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Conference on Data Mining</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="270" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Graph convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.05679</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on world wide web. International World Wide Web Conferences Steering Committee</title>
		<meeting>the 24th international conference on world wide web. International World Wide Web Conferences Steering Committee</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
