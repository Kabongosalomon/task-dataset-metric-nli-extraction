<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Joint entity recognition and relation extraction as a multi-head selection problem</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-12-17">17 Dec 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giannis</forename><surname>Bekoulis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Technology</orgName>
								<orgName type="institution">Ghent University -imec</orgName>
								<address>
									<addrLine>Technologiepark Zwijnaarde 15</addrLine>
									<postCode>9052</postCode>
									<settlement>IDLab, Ghent</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Deleu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Technology</orgName>
								<orgName type="institution">Ghent University -imec</orgName>
								<address>
									<addrLine>Technologiepark Zwijnaarde 15</addrLine>
									<postCode>9052</postCode>
									<settlement>IDLab, Ghent</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Demeester</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Technology</orgName>
								<orgName type="institution">Ghent University -imec</orgName>
								<address>
									<addrLine>Technologiepark Zwijnaarde 15</addrLine>
									<postCode>9052</postCode>
									<settlement>IDLab, Ghent</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Develder</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Technology</orgName>
								<orgName type="institution">Ghent University -imec</orgName>
								<address>
									<addrLine>Technologiepark Zwijnaarde 15</addrLine>
									<postCode>9052</postCode>
									<settlement>IDLab, Ghent</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Joint entity recognition and relation extraction as a multi-head selection problem</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-12-17">17 Dec 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>entity recognition</term>
					<term>relation extraction</term>
					<term>multi-head selection</term>
					<term>joint model</term>
					<term>sequence labeling * Corresponding author Email addresses: giannisbekoulis@ugentbe (Giannis Bekoulis)</term>
					<term>johannesdeleu@ugentbe (Johannes Deleu)</term>
					<term>thomasdemeester@ugentbe (Thomas Demeester)</term>
					<term>chrisdevelder@ugentbe (Chris Develder)</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>State-of-the-art models for joint entity recognition and relation extraction strongly rely on external natural language processing (NLP) tools such as POS (part-of-speech) taggers and dependency parsers. Thus, the performance of such joint models depends on the quality of the features obtained from these NLP tools. However, these features are not always accurate for various languages and contexts. In this paper, we propose a joint neural model which performs entity recognition and relation extraction simultaneously, without the need of any manually extracted features or the use of any external tool. Specifically, we model the entity recognition task using a CRF (Conditional Random Fields) layer and the relation extraction task as a multi-head selection problem (i.e., potentially identify multiple relations for each entity). We present an extensive experimental setup, to demonstrate the effectiveness of our method using datasets from various contexts (i.e., news, biomedical, real estate) and languages (i.e., English, Dutch). Our model outperforms the previous neural models that use automatically extracted features, while it performs within a reasonable margin of feature-based neural models, or even beats them.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The goal of the entity recognition and relation extraction is to discover relational structures of entity mentions from unstructured texts. It is a central problem in information extraction since it is critical for tasks such as knowledge base population and question answering.</p><p>The problem is traditionally approached as two separate subtasks, namely (i) named entity recognition (NER) <ref type="bibr" target="#b24">(Nadeau &amp; Sekine, 2007)</ref> and (ii) relation extraction (RE) <ref type="bibr" target="#b2">(Bach &amp; Badaskar, 2007)</ref>, in a pipeline setting. The main limitations of the pipeline models are: (i) error propagation between the components (i.e., NER and RE) and (ii) possible useful information from the one task is not exploited by the other (e.g., identifying a Works for relation might be helpful for the NER module in detecting the type of the two entities, i.e., PER, ORG and vice versa). On the other hand, more recent studies propose to use joint models to detect entities and their relations overcoming the aforementioned issues and achieving state-of-the-art performance <ref type="bibr" target="#b17">(Li &amp; Ji, 2014;</ref><ref type="bibr" target="#b22">Miwa &amp; Sasaki, 2014)</ref>.</p><p>The previous joint models heavily rely on hand-crafted features. Recent advances in neural networks alleviate the issue of manual feature engineering, but some of them still depend on NLP tools (e.g., POS taggers, dependency parsers). <ref type="bibr" target="#b20">Miwa &amp; Bansal (2016)</ref> propose a Recurrent Neural Network (RNN)-based joint model that uses a bidirectional sequential LSTM (Long Short Term Memory) to model the entities and a tree-LSTM that takes into account dependency tree information to model the relations between the entities. The dependency information is extracted using an external dependency parser. Similarly, in the work of <ref type="bibr" target="#b15">Li et al. (2017)</ref> for entity and relation extraction from biomedical text, a model which also uses tree-LSTMs is applied to extract dependency information. <ref type="bibr">Gupta et al. (2016)</ref> propose a method that relies on RNNs but uses a lot of hand-crafted features and additional NLP tools to extract features such as POS-tags, etc. <ref type="bibr" target="#b1">Adel &amp; Sch√ºtze (2017)</ref> replicate the context around the entities with Convolutional Neural Networks (CNNs). Note that the aforementioned works examine pairs of entities for relation extraction, rather than modeling the whole sentence directly. This means that relations of other pairs of entities in the same sentence -which could be helpful in deciding on the relation type for a particular pair -are not taken into account. <ref type="bibr">Katiyar &amp; Cardie (2017)</ref> propose a neural joint model based on LSTMs where they model the whole sentence at once, but still they do not have a principled way to deal with multiple relations. <ref type="bibr" target="#b4">Bekoulis et al. (2018)</ref> introduce a quadratic scoring layer to model the two tasks simultaneously. The limitation of this approach is that only a single relation can be assigned to a token, while the time complexity for the entity recognition task is increased compared to the standard approaches with linear complexity.</p><p>In this work, we focus on a new general purpose joint model that performs the two tasks of entity recognition and relation extraction simultaneously, and that can handle multiple relations together. Our model achieves state-of-the-art performance in a number of different contexts (i.e., news, biomedical, real estate) and languages (i.e., English, Dutch) without relying on any manually engineered features nor additional NLP tools. In summary, our proposed model (which will be detailed next in Section 3) solves several shortcomings that we identified in related works (Section 2) for joint entity recognition and relation extraction: (i) our model does not rely on external NLP tools nor hand-crafted features, (ii) entities and relations within the same text fragment (typically a sentence) are extracted simultaneously, where (iii) an entity can be involved in multiple relations at once. Specifically, the model of <ref type="bibr" target="#b20">Miwa &amp; Bansal (2016)</ref> depends on dependency parsers, which perform particularly well on specific languages (i.e., English) and contexts (i.e., news). Yet, our ambition is to develop a model that generalizes well in various setups, therefore using only automatically extracted features that are learned during training.</p><p>For instance, <ref type="bibr" target="#b20">Miwa &amp; Bansal (2016)</ref> and <ref type="bibr" target="#b15">Li et al. (2017)</ref> use exactly the same model in different contexts, i.e., news (ACE04) and biomedical data (ADE), respectively. Comparing our results to the ADE dataset, we obtain a 1.8% improvement on the NER task and ‚àº3% on the RE task. On the other hand, our model performs within a reasonable margin (‚àº0.6% in the NER task and ‚àº1% on the RE task) on the ACE04 dataset without the use of pre-calculated features. This shows that the model of <ref type="bibr" target="#b20">Miwa &amp; Bansal (2016)</ref> strongly relies on the features extracted by the dependency parsers and cannot generalize well into different contexts where dependency parser features are weak.</p><p>Comparing to <ref type="bibr" target="#b1">Adel &amp; Sch√ºtze (2017)</ref>, we train our model by modeling all the entities and the relations of the sentence at once. This type of inference is beneficial in obtaining information about neighboring entities and relations instead of just examining a pair of entities each time. Finally, we solve the underlying problem of the models proposed by <ref type="bibr">Katiyar &amp; Cardie (2017)</ref> and <ref type="bibr" target="#b3">Bekoulis et al. (2017)</ref>, who essentially assume classes (i.e., relations) to be mutually exclusive: we solve this by phrasing the relation extraction component as a multi-label prediction problem. 1</p><p>To demonstrate the effectiveness of the proposed method, we conduct the largest experimental evaluation to date (to the best of our knowledge) in jointly performing both entity recognition and relation extraction (see Section 4 and Section 5), using different datasets from various domains (i.e., news, biomedical, real estate) and languages (i.e., English, Dutch). Specifically, we apply our method to four datasets, namely ACE04 (news), Adverse Drug Events (ADE), Dutch Real Estate Classifieds <ref type="bibr">(DREC)</ref> and CoNLL'04 (news). Our method outperforms all state-of-the-art methods that do not rely on any additional features or tools, while performance is very close (or even better in the biomedical dataset) compared to methods that do exploit hand-engineered features or NLP tools.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>The tasks of entity recognition and relation extraction can be applied either one by one in a pipeline setting <ref type="bibr" target="#b11">(Fundel et al., 2007;</ref><ref type="bibr">Gurulingappa et al., 2012a;</ref><ref type="bibr" target="#b3">Bekoulis et al., 2017)</ref> or in a joint model <ref type="bibr" target="#b22">(Miwa &amp; Sasaki, 2014;</ref><ref type="bibr" target="#b20">Miwa &amp; Bansal, 2016;</ref><ref type="bibr" target="#b4">Bekoulis et al., 2018)</ref>. In this section, we present related work for each task (i.e., named entity recognition and relation extraction) as well as prior work into joint entity and relation extraction. 1 Note that another difference is that we use a CRF layer for the NER part, while Katiyar &amp; Cardie (2017) uses a softmax and <ref type="bibr" target="#b3">Bekoulis et al. (2017)</ref> uses a quadratic scoring layer; see further, when we discuss performance comparison results in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Named entity recognition</head><p>In our work, NER is the first task which we solve in order to address the end-to-end relation extraction problem. A number of different methods for the NER task that are based on hand-crafted features have been proposed, such as CRFs <ref type="bibr" target="#b12">(Lafferty et al., 2001</ref><ref type="bibr">), Maximum Margin Markov Networks (Taskar et al., 2003</ref> and support vector machines (SVMs) for structured output <ref type="bibr">(Tsochantaridis et al., 2004)</ref>, to name just a few. Recently, deep learning methods such as CNN-and RNN-based models have been combined with CRF loss functions <ref type="bibr" target="#b7">(Collobert et al., 2011;</ref><ref type="bibr">Huang et al., 2015;</ref><ref type="bibr" target="#b13">Lample et al., 2016;</ref><ref type="bibr" target="#b18">Ma &amp; Hovy, 2016)</ref> for NER. These methods achieve state-of-theart performance on publicly available NER datasets without relying on hand-crafted features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Relation extraction</head><p>We consider relation extraction as the second task of our joint model. The main approaches for relation extraction rely either on hand-crafted features <ref type="bibr" target="#b32">(Zelenko et al., 2003;</ref><ref type="bibr">Kambhatla, 2004)</ref> or neural networks <ref type="bibr">(Socher et al., 2012;</ref><ref type="bibr" target="#b33">Zeng et al., 2014)</ref>.</p><p>Feature-based methods focus on obtaining effective hand-crafted features, for instance defining kernel functions <ref type="bibr" target="#b32">(Zelenko et al., 2003;</ref><ref type="bibr" target="#b8">Culotta &amp; Sorensen, 2004)</ref> and designing lexical, syntactic, semantic features, etc. <ref type="bibr">(Kambhatla, 2004;</ref><ref type="bibr" target="#b26">Rink &amp; Harabagiu, 2010)</ref>. Neural network models have been proposed to overcome the issue of manually designing hand-crafted features leading to improved performance. CNN- <ref type="bibr" target="#b33">(Zeng et al., 2014;</ref><ref type="bibr" target="#b29">Xu et al., 2015a;</ref><ref type="bibr" target="#b28">dos Santos et al., 2015)</ref> and <ref type="bibr">RNN-based (Socher et al., 2013;</ref><ref type="bibr" target="#b34">Zhang &amp; Wang, 2015;</ref><ref type="bibr" target="#b30">Xu et al., 2015b)</ref> models have been introduced to automatically extract lexical and sentence level features leading to a deeper language understanding. <ref type="bibr">Vu et al. (2016)</ref> combine CNNs and RNNs using an ensemble scheme to achieve state-of-the-art results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Joint entity and relation extraction</head><p>Entity and relation extraction includes the task of (i) identifying the entities (described in Section 2.1) and (ii) extracting the relations among them (described in Section 2.2).</p><p>Feature-based joint models <ref type="bibr">(Kate &amp; Mooney, 2010;</ref><ref type="bibr" target="#b31">Yang &amp; Cardie, 2013;</ref><ref type="bibr" target="#b17">Li &amp; Ji, 2014;</ref><ref type="bibr" target="#b22">Miwa &amp; Sasaki, 2014)</ref> have been proposed to simultaneously solve the entity recognition and relation extraction (RE) subtasks. These methods rely on the availability of NLP tools (e.g., POS taggers) or manually designed features and thus (i) require additional effort for the data preprocessing, (ii) perform poorly in different application and language settings where the NLP tools are not reliable, and (iii) increase the computational complexity. In this paper, we introduce a joint neural network model to overcome the aforementioned issues and to automatically perform end-to-end relation extraction without the need of any manual feature engineering or the use of additional NLP components.</p><p>Neural network approaches have been considered to address the problem in a joint setting (end-to-end relation extraction) and typically include the use of RNNs and CNNs <ref type="bibr" target="#b20">(Miwa &amp; Bansal, 2016;</ref><ref type="bibr" target="#b36">Zheng et al., 2017;</ref><ref type="bibr" target="#b15">Li et al., 2017)</ref>. <ref type="bibr">Specifically, Miwa &amp; Bansal (2016)</ref> propose the use of bidirectional tree-structured RNNs to capture dependency tree information (where parse trees are extracted using state-of-the-art dependency parsers) which has been proven beneficial for relation extraction <ref type="bibr">(Xu et al., 2015a,b)</ref>. <ref type="bibr" target="#b15">Li et al. (2017)</ref> apply the work of <ref type="bibr" target="#b20">Miwa &amp; Bansal (2016)</ref> to biomedical text, reporting state-of-the-art performance for two biomedical datasets. <ref type="bibr">Gupta et al. (2016)</ref> propose the use of a lot of hand-crafted features along with RNNs. <ref type="bibr" target="#b1">Adel &amp; Sch√ºtze (2017)</ref> solve the entity classification task (which is different from NER since in entity classification the boundaries of the entities are known and only the type of the entity should be predicted) and relation extraction problems using an approximation of a global normalization objective (i.e., CRF): they replicate the context of the sentence (left and right part of the entities) to feed one entity pair at a time to a CNN for relation extraction. Thus, they do not simultaneously infer other potential entities and relations within the same sentence. Katiyar &amp; Cardie (2017) and <ref type="bibr" target="#b4">Bekoulis et al. (2018)</ref> investigate RNNs with attention for extracting relations between entity mentions without using any dependency parse tree features. Different from <ref type="bibr">Katiyar &amp; Cardie (2017)</ref>, in this work, we frame the problem as a multi-head selection problem by using a sigmoid loss to obtain multiple relations and a CRF loss for the NER component. This way, we are able to independently predict classes that are not mutually exclusive, instead of assigning equal probability values among the tokens. We overcome the issue of addi-   <ref type="figure" target="#fig_5">Figure 1</ref>: The multi-head selection model for joint entity and relation extraction. The input of our model is the words of the sentence which are then represented as word vectors (i.e., embeddings). The BiLSTM layer extracts a more complex representation for each word. Then the CRF and the sigmoid layers are able to produce the outputs for the two tasks. The outputs for each token (e.g., Smith) are: (i) an entity recognition label (e.g., I-PER) and (ii) a set of tuples comprising the head tokens of the entity and the types of relations between them (e.g., {(Center, Works for), (Atlanta, Lives in)}).</p><p>tional complexity described by <ref type="bibr" target="#b4">Bekoulis et al. (2018)</ref>, by dividing the loss functions into a NER and a relation extraction component. Moreover, we are able to handle multiple relations instead of just predicting single ones, as was described for the application of structured real estate advertisements of <ref type="bibr" target="#b4">Bekoulis et al. (2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Joint model</head><p>In this section, we present our multi-head joint model illustrated in <ref type="figure" target="#fig_5">Fig. 1</ref>. The model is able to simultaneously identify the entities (i.e., types and boundaries) and all the possible relations between them at once. We formulate the problem as a multi-head selection problem extending previous work <ref type="bibr" target="#b4">Bekoulis et al., 2018)</ref> as described in Section 2.3. By multi-head, we mean that any particular entity may be the CoNLL04 dataset is presented. The input of our model is a sequence of tokens (i.e., words of the sentence) which are then represented as word vectors (i.e., word embeddings). The BiLSTM layer is able to extract a more complex representation for each word that incorporates the context via the RNN structure. Then the CRF and the sigmoid layers are able to produce the outputs for the two tasks. The outputs for each token (e.g., Smith) are twofold: (i) an entity recognition label (e.g., I-PER, denoting the token is inside a named entity of type PER) and (ii) a set of tuples comprising the head tokens of the entity and the types of relations between them (e.g., {(Center, Works for), (Atlanta, Lives in)}). Since we assume token-based encoding, we consider only the last token of the entity as head of another token, eliminating redundant relations.</p><p>For instance, there is a Works for relation between entities "John Smith" and "Disease Control Center". Instead of connecting all tokens of the entities, we connect only "Smith" with "Center". Also, for the case of no relation, we introduce the "N" label and we predict the token itself as the head.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Embedding layer</head><p>Given a sentence w = w 1 , ..., w n as a sequence of tokens, the word embedding layer is responsible to map each token to a word vector (w word2vec ). We use pre-trained word embeddings using the Skip-Gram word2vec model .</p><p>In this work, we also use character embeddings since they are commonly applied to neural NER <ref type="bibr" target="#b18">(Ma &amp; Hovy, 2016;</ref><ref type="bibr" target="#b13">Lample et al., 2016)</ref>. This type of embeddings is able to capture morphological features such as prefixes and suffixes. For instance, in the Adverse Drug Events (ADE) dataset, the suffix "toxicity" can specify an adverse drug event entity such as "neurotoxicity" or "hepatotoxicity" and thus it is very informative. Another example might be the Dutch suffix "kamer" ("room" in English) in the Dutch Real Estate Classifieds (DREC) dataset which is used to specify the space entities "badkamer" ("bathroom" in English) and "slaapkamer" ("bedroom" in English).</p><p>Character-level embeddings are learned during training, similar to <ref type="bibr" target="#b18">Ma &amp; Hovy (2016)</ref> and <ref type="bibr" target="#b13">Lample et al. (2016)</ref>. In the work of <ref type="bibr" target="#b13">Lample et al. (2016)</ref>, character embeddings lead to a performance improvement of up to 3% in terms of NER F 1 score. In our work, by incorporating character embeddings, we report in <ref type="table" target="#tab_1">Table 2</ref> an increase of ‚àº2% overall F 1 scoring points. For more details, see Section 5.2. representation w word2vec to obtain the complete word embedding vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Bidirectional LSTM encoding layer</head><p>RNNs are commonly used in modeling sequential data and have been successfully applied in various NLP tasks <ref type="bibr">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b13">Lample et al., 2016;</ref><ref type="bibr" target="#b20">Miwa &amp; Bansal, 2016)</ref>. In this work, we use multi-layer LSTMs, a specific kind of RNNs which are able to capture long term dependencies well <ref type="bibr" target="#b5">(Bengio et al., 1994;</ref><ref type="bibr" target="#b25">Pascanu et al., 2013)</ref>. We employ a BiLSTM which is able to encode information from left to right (past to future) and right to left (future to past). This way, we can combine bidirectional information for each word by concatenating the forward ( h i ) and the backward ( h i ) output at timestep i. The BiLSTM output at timestep i can be written as:</p><formula xml:id="formula_0">h i = [ h i ; h i ], i = 0, ..., n<label>(1)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Named entity recognition</head><p>We formulate the entity identification task as a sequence labeling problem, similar to previous work on joint learning models <ref type="bibr" target="#b20">(Miwa &amp; Bansal, 2016;</ref><ref type="bibr" target="#b15">Li et al., 2017;</ref><ref type="bibr">Katiyar &amp; Cardie, 2017)</ref> and named entity recognition <ref type="bibr" target="#b13">(Lample et al., 2016;</ref><ref type="bibr" target="#b18">Ma &amp; Hovy, 2016)</ref> using the BIO (Beginning, Inside, Outside) encoding scheme. Each entity consists of multiple sequential tokens within the sentence and we should assign a tag for every token in the sentence. That way we are able to identify the entity arguments (start and end position) and its type (e.g., ORG). To do so, we assign the B-type (beginning) to the first token of the entity, the I-type (inside) to every other token within the entity and the O tag (outside) if a token is not part of an entity. <ref type="figure" target="#fig_5">Fig. 1</ref> shows an example of the BIO encoding tags assigned to the tokens of the sentence. In the CRF layer, one can observe that we assign the B-ORG and I-ORG tags to indicate the beginning and the inside tokens of the entity "Disease Control Center", respectively. On top of the BiLSTM layer, we employ either a softmax or a CRF layer to calculate the most probable entity tag for each token. We calculate the score of each token w i for each entity tag:</p><formula xml:id="formula_1">s (e) (h i ) = V (e) f (U (e) h i + b (e) )<label>(2)</label></formula><p>where the superscript (e) is used for the notation of the NER task, f (¬∑) is an elementwise activation function (i.e., relu, tanh),</p><formula xml:id="formula_2">V (e) ‚àà R p√ól , U (e) ‚àà R l√ó2d , b (e) ‚àà R l ,</formula><p>with d as the hidden size of the LSTM, p the number of NER tags (e.g., B-ORG) and</p><p>l the layer width. We calculate the probabilities of all the candidate tags for a given</p><formula xml:id="formula_3">token w i as Pr(tag | w i ) = softmax(s(h i )) where Pr(tag | w i ) ‚àà R p .</formula><p>In this work, we employ the softmax approach only for the entity classification (EC) task (which is similar to NER) where we need to predict only the entity types (e.g., PER) for each token assuming boundaries are given. The CRF approach is used for the NER task which includes both entity type and boundaries recognition.</p><p>In the softmax approach, we assign entity types to tokens in a greedy way at prediction time (i.e., the selected tag is just the highest scoring tag over all possible set of tags). Although assuming an independent tag distribution is beneficial for entity classification tasks (e.g., POS tagging), this is not the case when there are strong de-pendencies between the tags. Specifically, in NER, the BIO tagging scheme forces several restrictions (e.g., B-LOC cannot be followed by I-PER). The softmax method allows local decisions (i.e., for the tag of each token w i ) even though the BiLSTM captures information about the neighboring words. Still, the neighboring tags are not taken into account for the tag decision of a specific token. For example, in the entity "John Smith", tagging "Smith" as PER is useful for deciding that "John" is B-PER. To this end, for NER, we use a linear-chain CRF, similar to <ref type="bibr" target="#b13">Lample et al. (2016)</ref> where an improvement of ‚àº1% F 1 NER points is reported when using CRF. In our case, with the use of CRF we also report a ‚àº1% overall performance improvement as observed in </p><formula xml:id="formula_4">S y (e) 1 , . . . , y (e) n = n i=0 s (e) i,y (e) i + n‚àí1 i=1 T y (e) i ,y (e) i+1 (3) where S ‚àà R, s (e) i,y (e) i</formula><p>is the score of the predicted tag for token w i , T is a square transition matrix in which each entry represents transition scores from one tag to another.</p><p>T ‚àà R (p+2)√ó(p+2) because y </p><p>We apply Viterbi to obtain the tag sequence≈∑ (e) with the highest score. We train both the softmax (for the EC task) and the CRF layer (for NER) by minimizing the cross-entropy loss L NER . We also use the entity tags as input to our relation extraction layer by learning label embeddings, motivated by <ref type="bibr" target="#b20">Miwa &amp; Bansal (2016)</ref> where an improvement of 2% F 1 is reported (with the use of label embeddings). In our case, label embeddings lead to an increase of 1% F 1 score as reported in <ref type="table" target="#tab_1">Table 2</ref> (see Section 5.2).</p><p>The input to the next layer is twofold: the output states of the LSTM and the learned label embedding representation, encoding the intuition that knowledge of named enti-ties can be useful for relation extraction. During training, we use the gold entity tags, while at prediction time we use the predicted entity tags as input to the next layer. The input to the next layer is the concatenation of the hidden LSTM state h i with the label embedding g i for token w i :</p><formula xml:id="formula_6">z i = [h i ; g i ], i = 0, ..., n<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Relation extraction as multi-head selection</head><p>In this subsection, we describe the relation extraction task, formulated as a multi-head selection problem <ref type="bibr" target="#b4">Bekoulis et al., 2018)</ref>. In the general formulation of our method, each token w i can have multiple heads (i.e., multiple relations with other tokens). We predict the tuple (≈∑ i ,ƒâ i ) where≈∑ i is the vector of heads andƒâ i is the vector of the corresponding relations for each token w i . This is different for the previous standard head selection for dependency parsing method  since (i) it is extended to predict multiple heads and (ii) the decisions for the heads and the relations are jointly taken (i.e., instead of first predicting the heads and then in a next step the relations by using an additional classifier). Given as input a token sequence w and a set of relation labels R, our goal is to identify for each token w i , i ‚àà {0, ..., n} the vector of the most probable heads≈∑ i ‚äÜ w and the vector of the most probable corresponding relation labelsr i ‚äÜ R. We calculate the score between tokens w i and w j given a label r k as follows:</p><formula xml:id="formula_7">s (r) (z j , z i , r k ) = V (r) f (U (r) z j + W (r) z i + b (r) )<label>(6)</label></formula><p>where the superscript (r) is used for the notation of the relation task, f (¬∑) is an elementwise activation function (i.e., relu, tanh),</p><formula xml:id="formula_8">V (r) ‚àà R l , U (r) ‚àà R l√ó(2d+b) , W (r) ‚àà R l√ó(2d+b) , b (r) ‚àà R l , d</formula><p>is the hidden size of the LSTM, b is the size of the label embeddings and l the layer width. We define</p><formula xml:id="formula_9">Pr(head = w j , label = r k | w i ) = œÉ(s (r) (z j , z i , r k ))<label>(7)</label></formula><p>to be the probability of token w j to be selected as the head of token w i with the relation label r k between them, where œÉ(.) stands for the sigmoid function. We minimize the cross-entropy loss L rel during training:</p><formula xml:id="formula_10">L rel = n i=0 m j=0 ‚àí log Pr(head = y i,j , relation = r i,j | w i )<label>(8)</label></formula><p>where y i ‚äÜ w and r i ‚äÜ R are the ground truth vectors of heads and associated relation labels of w i and m is the number of relations (heads) for w i . After training, we keep the combination of heads≈∑ i and relation labelsr i exceeding a threshold based on the estimated joint probability as defined in Eq. <ref type="formula" target="#formula_9">(7)</ref>. Unlike previous work on joint models (Katiyar &amp; Cardie, 2017), we are able to predict multiple relations considering the classes as independent and not mutually exclusive (the probabilities do not necessarily sum to 1 for different classes). For the joint entity and relation extraction task, we calculate the final objective as L NER + L rel .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Edmonds' algorithm</head><p>Our model is able to simultaneously extract entity mentions and the relations between them. To demonstrate the effectiveness and the general purpose nature of our model, we also test it on the recently introduced Dutch real estate classifieds (DREC) dataset <ref type="bibr" target="#b3">(Bekoulis et al., 2017)</ref> where the entities need to form a tree structure. By using thresholded inference, a tree structure of relations is not guaranteed. Thus we should enforce tree structure constraints to our model. To this end, we post-process the output of our system with Edmonds' maximum spanning tree algorithm for directed graphs <ref type="bibr" target="#b6">(Chu &amp; Liu, 1965;</ref><ref type="bibr" target="#b10">Edmonds, 1967</ref> dataset with entity and relation recognition corpora <ref type="bibr" target="#b27">(Roth &amp; Yih, 2004)</ref>. Our code is available in our github codebase. We follow the cross-validation setting of <ref type="bibr" target="#b17">Li &amp; Ji (2014)</ref> and <ref type="bibr" target="#b20">Miwa &amp; Bansal (2016)</ref>.</p><p>We removed DISC and did 5-fold cross-validation on the bnews and nwire subsets (348 documents). We obtained the preprocessing script from Miwa's github codebase. <ref type="bibr">3</ref> We measure the performance of our system using micro F 1 scores, Precision and Recall on both entities and relations. We treat an entity as correct when the entity type and the region of its head are correct. We treat a relation as correct when its type and argument entities are correct, similar to <ref type="bibr" target="#b20">Miwa &amp; Bansal (2016)</ref> and <ref type="bibr">Katiyar &amp; Cardie (2017)</ref>. We refer to this type of evaluation as strict. <ref type="bibr">4</ref> We select the best hyperparameter values on a randomly selected validation set for each fold, selected from the training set (15% of the data) since there are no official train and validation splits in the work of <ref type="bibr" target="#b20">Miwa &amp; Bansal (2016)</ref>.</p><p>CoNLL04: There are four entity types in the dataset (Location, Organization, Person,</p><p>and Other) and five relation types (Kill, Live in, Located in, OrgBased in and Work for). We use the splits defined by Gupta et al. <ref type="formula" target="#formula_0">(2016)</ref> and <ref type="bibr" target="#b1">Adel &amp; Sch√ºtze (2017)</ref>. The dataset consists of 910 training instances, 243 for validation and 288 for testing. <ref type="bibr">5</ref> We measure the performance by computing the F 1 score on the test set. We adopt two evaluation settings to compare to previous work. Specifically, we perform an EC task assuming the entity boundaries are given similar to <ref type="bibr">Gupta et al. (2016)</ref> and <ref type="bibr" target="#b1">Adel &amp; Sch√ºtze (2017)</ref>. To obtain comparable results, we omit the entity class "Other" when computing the EC score. We score a multi-token entity as correct if at least one of its comprising token types is correct assuming that the boundaries are given; a relation is correct when the type of the relation and the argument entities are both correct. We report macro-average F 1 scores for EC and RE to obtain comparable results to previous studies. Moreover, we perform actual NER evaluation instead of just EC, reporting results using the strict evaluation metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DREC:</head><p>The dataset consists of 2,318 classifieds as described in the work of Bekoulis et al. <ref type="bibr">(2018)</ref>. There are 9 entity types: Neighborhood, Floor, Extra building, Subspace, Invalid, Field, Other, Space and Property. Also, there are two relation classes Part-of and Equivalent. The goal is to identify important entities of a property (e.g., floors, spaces) from classifieds and structuring them into a tree format to get the structured description of the property. For the evaluation, we use 70% for training, 15% for validation and 15% as test set in the same splits as defined in <ref type="bibr" target="#b4">Bekoulis et al. (2018)</ref>.</p><p>We measure the performance by computing the F 1 score on the test set. To compare our results with previous work <ref type="bibr" target="#b4">(Bekoulis et al., 2018)</ref>, we use the boundaries evaluation setting. In this setting, we count an entity as correct if the boundaries of the entity are correct. A relation is correct when the relation is correct and the argument entities are both correct. Also, we report results using the strict evaluation for future reference.</p><p>ADE: There are two types of entities (drugs and diseases) in this dataset and the aim of the task is to identify the types of entities and relate each drug with a disease (adverse drug events). There are 6,821 sentences in total and similar to previous work <ref type="bibr" target="#b16">(Li et al., 2016</ref><ref type="bibr" target="#b15">(Li et al., , 2017</ref>, we remove ‚àº130 relations with overlapping entities (e.g., "lithium" is a drug which is related to "lithium intoxication"). Since there are no official sets, we evaluate our model using 10-fold cross-validation where 10% of the data was used as validation and 10% for test set similar to <ref type="bibr" target="#b15">Li et al. (2017)</ref>. The final results are displayed in F 1 metric as a macro-average across the folds. The dataset consists of 10,652 entities and 6,682 relations. We report results similar to previous work on this dataset using the strict evaluation metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Word embeddings</head><p>We use pre-trained word2vec embeddings used in previous work, so as to retain the same inputs for our model and to obtain comparable results that are not affected by the input embeddings. Specifically, we use the 200-dimensional word embeddings used in the work of <ref type="bibr" target="#b20">Miwa &amp; Bansal (2016)</ref> for the ACE04 dataset 6 trained on Wikipedia. We obtained the 50-dimensional word embeddings used by <ref type="bibr" target="#b1">Adel &amp; Sch√ºtze (2017)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Hyperparameters and implementation details</head><p>We have developed our joint model by using Python with the TensorFlow machine learning library <ref type="bibr" target="#b0">(Abadi et al., 2016)</ref>. Training is performed using the Adam optimizer (Kingma &amp; Ba, 2015) with a learning rate of 10 ‚àí3 . We fix the size of the LSTM to d = 64 and the layer width of the neural network to l = 64 (both for the entity and the relation scoring layers). We use dropout (Srivastava et al., 2014) to regularize our network. Dropout is applied in the input embeddings and in between the hidden layers for both tasks. Different dropout rates have been applied but the best dropout values (0.2 to 0.4) for each dataset have been used. The hidden dimension for the characterbased LSTMs is 25 (for each direction). We also fixed our label embeddings to be of size b = 25 for all the datasets except for CoNLL04 where the label embeddings were not beneficial and thus were not used. We experimented with tanh and relu activation functions (recall that this is the function f (¬∑) from the model description). We use the  relu activation only in the ACE04 and tanh in all other datasets. We employ the technique of early stopping based on the validation set. In all the datasets examined in this study, we obtain the best hyperparameters after 60 to 200 epochs depending on the size of the dataset. We select the best epoch according to the results in the validation set.</p><p>For more details about the effect of each hyperparameter to the model performance see the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results and discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Results</head><p>In <ref type="table" target="#tab_5">Table 1</ref>, we present the results of our analysis. The first column indicates the considered dataset. In the second column, we denote the model which is applied (i.e., previous work and the proposed models). The proposed models are the following:</p><p>(i) multi-head is the proposed model with the CRF layer for NER and the sigmoid loss for multiple head prediction, (ii) multi-head+E is the proposed model with addition of Edmonds' algorithm to guarantee a tree-structured output for the DREC dataset, (iii) single-head is the proposed method but it predicts only one head per token using a softmax loss instead of a sigmoid, and (iv) multi-head EC is the proposed method with a softmax to predict the entity classes assuming that the boundaries are given, and the sigmoid loss for multiple head selection. <ref type="table" target="#tab_5">Table 1</ref> also indicates whether the different settings include hand-crafted features or features derived from NLP tools (e.g., POS taggers, dependency parsers). We use the symbol to denote that the model includes this kind of additional features and the symbol to denote that the model is only based on automatically extracted features. Note that all the variations of our model do not rely on any additional features. In the next column, we declare the type of evaluation conducted for each experiment. We include here different evaluation types to be able to compare our results against previous studies. Specifically, we use three evaluation types, namely: (iii) Relaxed: we score a multi-token entity as correct if at least one of its comprising token types is correct assuming that the boundaries are given; a relation is correct when the type of the relation and the argument entities are both correct.</p><p>In the next three columns, we present the results for the entity identification task (Precision, Recall, F 1 ) and then (in the subsequent three columns) the results of the relation extraction task (Precision, Recall, F 1 ). Finally, in the last column, we report an additional F 1 measure which is the average F 1 performance of the two subtasks. We mark with bold font in <ref type="table" target="#tab_5">Table 1</ref>  <ref type="formula" target="#formula_0">(2017)</ref>, the class probabilities do not necessarily sum up to one since the classes are considered independent. Moreover, we use a CRF-layer to model the NER task to capture dependencies between sequential tokens. Finally, we obtain more effective word representations by using character-level embeddings. On the other hand, our model performs within a reasonable margin (‚àº0.5% for the NER task and ‚àº1%</p><p>for the RE task) compared to <ref type="bibr" target="#b20">Miwa &amp; Bansal (2016)</ref>. This difference is explained by the fact that the model of <ref type="bibr" target="#b20">Miwa &amp; Bansal (2016)</ref> relies on POS tagging and syntactic features derived by dependency parsing. However, this kind of features relies on NLP tools that are not always accurate for various languages and contexts. For instance, the same model is adopted by the work of <ref type="bibr" target="#b15">Li et al. (2017)</ref> for the ADE biomedical dataset and in this dataset our model reports more than 3% improvement in the RE task. This shows that our model is able to produce automatically extracted features which perform reasonably well in all contexts (e.g., news, biomedical).</p><p>For the CoNLL04 dataset, there are two different evaluation settings, namely relaxed and strict. In the relaxed setting, we perform an EC task instead of NER assuming that the boundaries of the entities are given. We adopt this setting to produce comparable results with previous studies <ref type="bibr">(Gupta et al., 2016;</ref><ref type="bibr" target="#b1">Adel &amp; Sch√ºtze, 2017)</ref>.</p><p>Similar to <ref type="bibr" target="#b1">Adel &amp; Sch√ºtze (2017)</ref>, we present results of single models and no ensembles. We observe that our model outperforms all previous models that do not rely on complex hand-crafted features by a large margin (&gt;4% for both tasks). Unlike these previous studies that consider pairs of entities to obtain the entity types and the corresponding relations, we model the whole sentence at once. That way, our method is able to directly infer all entities and relations of a sentence and benefit from their possible interactions that cannot be modeled when training is performed for each entity pair individually, one at a time. In the same setting, we also report the results of Gupta et al. <ref type="bibr">(2016)</ref> in which they use multiple complicated hand-crafted features coming from NLP tools. Our model performs slightly better for the EC task and within a margin of 1% in terms of overall F 1 score. The difference in the overall performance is due to the fact that our model uses only automatically generated features. We also report re-sults on the same dataset conducting NER (i.e., predicting entity types and boundaries) and evaluating using the strict evaluation measure, similar to <ref type="bibr" target="#b22">Miwa &amp; Sasaki (2014)</ref>.</p><p>Our results are not directly comparable to the work of <ref type="bibr" target="#b22">Miwa &amp; Sasaki (2014)</ref> because we use the splits provided by <ref type="bibr">Gupta et al. (2016)</ref>. However, in this setting we present the results from <ref type="bibr" target="#b22">Miwa &amp; Sasaki (2014)</ref> as reference. We report an improvement of ‚àº2% overall F 1 score, which suggests that our neural model is able to extract more informative representations compared to feature-based approaches.</p><p>We also report results for the DREC dataset, with two different evaluation settings.</p><p>Specifically, we use the boundaries and the strict settings. We transform the previous results from <ref type="bibr" target="#b4">Bekoulis et al. (2018)</ref> to the boundaries setting to make them comparable to our model since in their work, they report token-based F 1 score, which is not a common evaluation metric in relation extraction problems. Also, in their work, they focus on identifying only the boundaries of the entities and not the types (e.g., Floor, Space). In the boundaries evaluation, we achieve ‚àº3% improvement for both tasks.</p><p>This is due to the fact that their quadratic scoring layer is beneficial for the RE task, yet complicates NER, which is usually modeled as a sequence labeling task. Moreover, we report results using the strict evaluation which is used in most related works. Using the prior knowledge that each entity has only one head, we can simplify our model and predict only one head each time (i.e., using a softmax loss). The difference between the single and the multi-head models is marginal (&lt;0.1% for both tasks). This shows that our model (multi-head) can adapt to various environments, even if the setting is single head (in terms of the application, and thus also in both training and test data).</p><p>Finally, we compare our model with previous work <ref type="bibr" target="#b16">(Li et al., 2016</ref><ref type="bibr" target="#b15">(Li et al., , 2017</ref> on the ADE dataset. The previous models <ref type="bibr" target="#b16">(Li et al., 2016</ref><ref type="bibr" target="#b15">(Li et al., , 2017</ref> both use hand-crafted features or features derived from NLP tools. However, our model is able to outperform both models using the strict evaluation metric. We report an improvement of ‚àº2% in the NER and ‚àº3% in the RE tasks, respectively. The work of <ref type="bibr" target="#b15">Li et al. (2017)</ref> is similar to <ref type="bibr" target="#b20">Miwa &amp; Bansal (2016)</ref> and strongly relies on dependency parsers to extract syntactic information. A possible explanation for the better result obtained from our model is that the pre-calculated syntactic information obtained using external tools either is not so accurate or important for biomedical data.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Analysis of feature contribution</head><p>We conduct ablation tests on the ACE04 dataset reported in <ref type="table" target="#tab_1">Table 2</ref> to analyze the effectiveness of the various parts of our joint model. The performance of the RE task decreases (‚àº1% in terms of F 1 score) when we remove the label embeddings layer and only use the LSTM hidden states as inputs for the RE task. This shows that the NER labels, as expected, provide meaningful information for the RE component.</p><p>Removing character embeddings also degrades the performance of both NER (‚àº1%) and RE (‚àº2%) tasks by a relatively large margin. This illustrates that composing words by the representation of characters is effective, and our method benefits from additional information such as capital letters, suffixes and prefixes within the token (i.e., its character sequences).</p><p>Finally, we conduct experiments for the NER task by removing the CRF loss layer and substituting it with a softmax. Assuming independent distribution of labels (i.e., softmax) leads to a slight decrease in the F 1 performance of the NER module and a ‚àº2% decrease in the performance of the RE task. This happens because the CRF loss is able to capture the strong tag dependencies (e.g., I-LOC cannot follow B-PER) that are present in the dataset instead of just assuming that the tag decision for each token is independent from tag decisions of neighboring tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this work, we present a joint neural model to simultaneously extract entities and relations from textual data. Our model comprises a CRF layer for the entity recognition task and a sigmoid layer for the relation extraction task. Specifically, we model the relation extraction task as a multi-head selection problem since one entity can have multiple relations. Previous models on this task rely heavily on external NLP tools (i.e., POS taggers, dependency parsers). Thus, the performance of these models is affected by the accuracy of the extracted features. Unlike previous studies, our model produces automatically generated features rather than relying on hand-crafted ones, or existing NLP tools. Given its independence from such NLP or other feature generating tools, our approach can be easily adopted for any language and context. We demonstrate the effectiveness of our approach by conducting a large scale experimental study. Our model is able to outperform neural methods that automatically generate features while the results are marginally similar (or sometimes better) compared to feature-based neural network approaches.</p><p>As future work, we aim to explore the effectiveness of entity pre-training for the entity recognition module. This approach has been proven beneficial in the work of Miwa &amp; Bansal (2016) for both the entity and the relation extraction modules. In addition, we are planning to explore a way to reduce the calculations in the quadratic relation scoring layer. For instance, a straightforward way to do so is to use in the sigmoid layer only the tokens that have been identified as entities.</p><p>Gupta, P., <ref type="bibr">Sch√ºtze, H., &amp; Andrassy, B. (2016)</ref>. In this section, we report additional results for our multi-head selection framework.</p><p>Specifically, we (i) compare our model with the model of <ref type="bibr">Lample et al. (2016) (i.e.,</ref> optimize only over the NER task), (ii) explore several hyperparameters of the network (e.g., dropout, LSTM size, character embeddings size), and (iii) report F 1 score using different word embeddings compared to the embeddings used in previous works.</p><p>In <ref type="table" target="#tab_5">Table 1</ref> of the main paper, we focused on comparing our model against other joint models that are able to solve the two tasks (i.e., NER and relation extraction) simultaneously, mainly demonstrating superiority of phrasing the relation extraction as a multi-head selection problem (enabling the extraction of multiple relations at once).</p><p>Here, in <ref type="table" target="#tab_5">Table A1,</ref>  (2016) on the CoNLL-2003 test set is 0.01% and 0.17% F 1 points, respectively). This slight improvement suggests that the interaction of the two components by sharing the underlying LSTM layer is indeed beneficial (e.g., identifying a Works for relation might be helpful for the NER module in detecting the type of the two entities, i.e., PER, ORG and vice versa). Note that improving NER in isolation was not the objective of our multi-head model, but we rather aimed to compare our model against other joint models that solve the task of entity recognition and relation identification simultaneously.</p><p>We thus did not envision to claim or achieve state-of-the-art performance in each of the individual building blocks of our joint model. <ref type="table" target="#tab_1">Tables A2, A3</ref> and A4 show the performance of our model on the test set for different values of the embedding dropout, LSTM layer dropout and the LSTM output dropout hyperparameters, respectively. Note that the hyperparameter values used for the results in Section 5 were obtained by tuning over the development set, and these are indicated in bold face in the tables below. We vary one hyperparameter at a time in order to assess the effect of a particular hyperparameter. The main outcomes from these tables are twofold: (i) low dropout values (e.g., 0, 0.1) lead to a performance decrease in the overall F 1 score (see <ref type="table" target="#tab_14">Table A3</ref> where a ‚àº3% F 1 decrease is reported on the ACE04 dataset) and (ii) average dropout values (i.e., 0.2-0.4) lead to consistently similar results.</p><p>In <ref type="table" target="#tab_16">Tables A5, A6</ref>  In the main results (see Section 5), to guarantee a fair comparison to previous work and to obtain comparable results that are not affected by the input embeddings, we use embeddings used also in prior studies. To assess the performance of our system to input       variations, we also report results using different word embeddings (see <ref type="table" target="#tab_21">Table A9</ref>) (i.e.,</p><p>Adel &amp; Sch√ºtze (2017); Li et al. <ref type="formula" target="#formula_0">(2017)</ref>) on the ACE04 dataset. Our results showcase that our model, even when using different word embeddings, is still performing better compared to other works that, like ours, do not rely on additional NLP tools.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>involved in multiple relations with other entities. The basic layers of the model, shown in Fig. 1, are: (i) embedding layer, (ii) bidirectional sequential LSTM (BiLSTM) layer,(iii) CRF layer and the (iv) sigmoid scoring layer. InFig. 1, an example sentence from Embedding layer in detail. The characters of the word "Man" are represented by character vectors (i.e., embeddings) that are learned during training. The character embeddings are fed to a BiLSTM and the two final states (forward and backward) are concatenated. The vector w chars is the character-level representation of the word. This vector is then further concatenated to the word-level representation w word2vec to obtain the complete word embedding vector.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2</head><label>2</label><figDesc>illustrates the neural architecture for word embedding generation based on its characters. The characters of each word are represented by character vectors (i.e., embeddings). The character embeddings are fed to a BiLSTM and the two final states (forward and backward) are concatenated. The vector w chars is the character-level representation of the word. This vector is then further concatenated to the word-level</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>n</head><label></label><figDesc>are two auxiliary tags that represent the starting and the ending tags of the sentence, respectively. Then, the probability of a given sequence of tags over all possible tag sequences for the input sentence w is defined as:Pr y (e) 1 , . . . , y (e) n w = e S(y (e) 1 ,...,y (e) n ) ·ªπ1 (e) ,...,·ªπn (e) e S(·ªπ1 (e) ,...,·ªπn (e) )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>2 ACE04:</head><label>2</label><figDesc>There are seven main entity types namely Person (PER), Organization (ORG), Geographical Entities (GPE), Location (LOC), Facility (FAC), Weapon (WEA) and Vehicle (VEH). Also, the dataset defines seven relation types: Physical (PHYS), Person-Social (PER-SOC), Employment-Membership-Subsidiary (EMP-ORG), Agent-Artifact (ART), PER-ORG affiliation (Other-AFF), GPE affiliation (GPE-AFF), and Discourse (DISC).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>( i )</head><label>i</label><figDesc>Strict: an entity is considered correct if the boundaries and the type of the entity are both correct; a relation is correct when the type of the relation and the argument entities are both correct, (ii) Boundaries: an entity is considered correct if only the boundaries of the entity are correct (entity type is not considered); a relation is correct when the type of the relation and the argument entities are both correct and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>we evaluate the performance of just the first module of our joint multi-head model: we compare the performance of the NER component of our model against the state-of-the-art NER model of Lample et al. (2016). The results indicate a marginal performance improvement of our model over Lample's NER baseline in 3 out of 4 datasets. The improvement of our model's NER part is not substantial, since (i) our NER part is almost identical to Lample's, and (ii) recent advances in NER performance among neural systems are relatively small (improvements in the order of few 0.1 F 1 points -for instance, the contribution of Ma &amp; Hovy (2016) and Lample et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Embedding Layer BiLSTM Layer Sigmoid Layer Center, Atlanta Works for, Lives in</head><label></label><figDesc></figDesc><table><row><cell>h 1</cell><cell>h 2</cell><cell>h 3</cell><cell>h 4</cell><cell>h 5</cell><cell>h 6</cell></row><row><cell>Smith</cell><cell>headed</cell><cell>the</cell><cell>Disease</cell><cell>Control</cell><cell>Center</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>(e) 1 , ..., s</cell><cell>(e) n and a vector of tag predictions y 1 , ..., y (e)</cell><cell>(e)</cell></row></table><note>(see Section 5.2). Assuming the word vector w, a sequence of score vectors sn , the linear-chain CRF score is defined as:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>). A fully connected directed graph G = (V, E) is constructed, where the vertices V represent the last tokens of the identified entities (as predicted by NER) and the edges E represent the highest scoring relations with their scores as weights. Edmonds' algorithm is applied in cases a tree is not already formed by thresh-Dutch Real Estate Classifieds, DREC<ref type="bibr" target="#b3">(Bekoulis et al., 2017)</ref> and (iv) the CoNLL'04</figDesc><table><row><cell>(iii)</cell></row><row><cell>olded inference.</cell></row><row><cell>4. Experimental setup</cell></row><row><cell>4.1. Datasets and evaluation metrics</cell></row><row><cell>We conduct experiments on four datasets: (i) Automatic Content Extraction, ACE04 (Dod-</cell></row><row><cell>dington et al., 2004), (ii) Adverse Drug Events, ADE (Gurulingappa et al., 2012b),</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Comparison of our method (multi-head) with the state-of-the-art on the ACE04, CoNLL04, DREC and ADE datasets. The models: (i) multi-head+E (the model + the Edmond algorithm to produce a tree- structured output), (ii) single-head (the model predicts only one head per token) and (iii) multi-head EC (the model predicts only the entity classes assuming that the boundaries are given) are slight variations of the multi-head model adapted for each dataset and evaluation. The and symbols indicate whether or not the models rely on any hand-crafted features or additional tools. Note that all the variations of our models do not rely on any additional features. We include here different evaluation types (strict, relaxed and boundaries) to be able to compare our results against previous studies. Finally, we report results in terms of Precision, Recall, F 1 for the two subtasks as well as overall F 1 , averaging over both subtasks. Bold entries indicate the best result among models that only consider automatically learned features.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>, the best result for each dataset among those models that use only automatically extracted features.Considering the results in the ACE04, we observe that our model outperforms the model ofKatiyar &amp; Cardie (2017)  by ‚àº2% in both tasks. This improvement can be explained by the use of the multi-head selection method which can naturally capture multiple relations and model them as a multi-label problem. Unlike the work of Kati-</figDesc><table /><note>yar &amp; Cardie</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 2 :</head><label>2</label><figDesc>Ablation tests on the ACE04 test dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table filling</head><label>filling</label><figDesc></figDesc><table><row><cell>multi-task recurrent neural Socher, R., Chen, D., Manning, C. D., &amp; Ng, A. (2013). Reasoning with neural relation extraction based on a hybrid neural network. Neurocomputing, 257, 59 -</cell></row><row><cell>network for joint entity and relation extraction. In Proceedings of COLING 2016, tensor networks for knowledge base completion. In Proceedings of the 26th In-66. doi:10.1016/j.neucom.2016.12.075.</cell></row><row><cell>the 26th International Conference on Computational Linguistics: Technical Papers ternational Conference on Neural Information Processing Systems (pp. 926-934).</cell></row><row><cell>(pp. 2537-2547). Nevada, United States: Curran Associates, Inc.</cell></row><row><cell>Gurulingappa, H., MateenRajpu, A., &amp; Toldo, L. (2012a). Extraction of potential Socher, R., Huval, B., Manning, C. D., &amp; Ng, A. Y. (2012). Semantic compositionality</cell></row><row><cell>adverse drug events from medical case reports. Journal of Biomedical Semantics, through recursive matrix-vector spaces. In Proceedings of the 2012 Joint Conference</cell></row><row><cell>3(1), 1-15. doi:10.1186/2041-1480-3-15. on Empirical Methods in Natural Language Processing and Computational Natural</cell></row><row><cell>Gurulingappa, H., Rajput, A. M., Roberts, A., Fluck, J., Hofmann-Apitius, M., &amp; Language Learning (pp. 1201-1211). Jeju Island, Korea: Association for Computa-</cell></row><row><cell>Toldo, L. (2012b). Development of a benchmark corpus to support the auto-tional Linguistics.</cell></row><row><cell>matic extraction of drug-related adverse effects from medical case reports. Jour-Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., &amp; Salakhutdinov, R. (2014).</cell></row><row><cell>nal of Biomedical Informatics, 45(5), 885 -892. doi:https://doi.org/10. Dropout: A simple way to prevent neural networks from overfitting. Journal of</cell></row><row><cell>1016/j.jbi.2012.04.008. Machine Learning Research, 15(1), 1929-1958.</cell></row><row><cell>Huang, Z., Xu, W., &amp; Yu, K. (2015). Bidirectional LSTM-CRF models for sequence Sutskever, I., Vinyals, O., &amp; Le, Q. V. (2014). Sequence to sequence learning with</cell></row><row><cell>tagging. arXiv preprint arXiv:1508.01991, . neural networks. In Proceedings of the 27th International Conference on Neural</cell></row><row><cell>Kambhatla, N. (2004). Combining lexical, syntactic, and semantic features with maxi-Information Processing Systems (pp. 3104-3112). Montreal, Canada: MIT Press.</cell></row><row><cell>mum entropy models for extracting relations. In Proceedings of the Annual Meeting Taskar, B., Guestrin, C., &amp; Koller, D. (2003). Max-margin markov networks. In</cell></row><row><cell>of the Association for Computational Linguistics on Interactive poster and demon-Proceedings of the 16th International Conference on Neural Information Processing</cell></row><row><cell>stration sessions. Barcelona, Spain. doi:10.3115/1219044.1219066. Systems (pp. 25-32). Bangkok, Thailand: MIT Press.</cell></row><row><cell>Kate, R. J., &amp; Mooney, R. (2010). Joint entity and relation extraction using card-Tsochantaridis, I., Hofmann, T., Joachims, T., &amp; Altun, Y. (2004). Support vector</cell></row><row><cell>pyramid parsing. In Proceedings of the 14th Conference on Computational Natural machine learning for interdependent and structured output spaces. In Proceedings</cell></row><row><cell>Language Learning (pp. 203-212). Uppsala, Sweden: Association for Computa-of the 21st International Conference on Machine Learning (pp. 104-112). Helsinki,</cell></row><row><cell>tional Linguistics. Finland: ACM. doi:10.1145/1015330.1015341.</cell></row><row><cell>Katiyar, A., &amp; Cardie, C. (2017). Going out on a limb: Joint extraction of entity Vu, N. T., Adel, H., Gupta, P., &amp; Sch√ºtze, H. (2016). Combining recurrent and con-</cell></row><row><cell>mentions and relations without dependency trees. In Proceedings of the 55st Annual volutional neural networks for relation classification. In Proceedings of the 2016</cell></row><row><cell>Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Conference of the North American Chapter of the Association for Computational</cell></row><row><cell>Vancouver, Canada. Linguistics: Human Language Technologies (pp. 534-539). San Diego, California.</cell></row><row><cell>Kingma, D., &amp; Ba, J. (2015). Adam: A method for stochastic optimization. In Inter-URL: http://www.aclweb.org/anthology/N16-1065.</cell></row><row><cell>national Conference on Learning Representations. San Diego, USA.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>, A7 and A8, we report results for different values of the LSTM size, the size of the character embeddings, the size of the label embeddings and the layer width of the neural network l (both for the entity and the relation scoring layers), respectively. The reported results show that different hyperparameters settings do lead to noticeable performance differences, but we do not observe any clear trend. Moreover, we have not observed any significant performance improvement that affects the overall ranking of the models as reported inTable 1. On the other hand, the results indicate that increasing (character and label) embedding size and layer dimensions leads to a slight decrease in performance for the CoNLL04 dataset. This can be explained by the fact that the CoNLL04 dataset is relatively small and using more trainable model parameters (i.e., larger hyperparameter values) can make our multi-head selection method to overfit quickly on the training set. In almost any other case, variation of the hyperparameters does not affect the ranking of the models reported inTable 1.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Entity</cell><cell></cell></row><row><cell></cell><cell>Model</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell>ACE</cell><cell>NER baseline</cell><cell>81.06</cell><cell>81.13</cell><cell>81.10</cell></row><row><cell>04</cell><cell>multi-head</cell><cell>81.01</cell><cell>81.31</cell><cell>81.16</cell></row><row><cell>CoNLL</cell><cell>NER baseline</cell><cell>84.38</cell><cell>83.13</cell><cell>83.75</cell></row><row><cell>04</cell><cell>multi-head</cell><cell>83.75</cell><cell>84.06</cell><cell>83.90</cell></row><row><cell>DREC</cell><cell>NER baseline multi-head</cell><cell>78.22 78.97</cell><cell>84.89 83.98</cell><cell>81.42 81.39</cell></row><row><cell>ADE</cell><cell>NER baseline multi-head</cell><cell>83.97 84.72</cell><cell>88.59 88.16</cell><cell>86.22 86.40</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table A1 :</head><label>A1</label><figDesc>Comparison of the multi-head selection model (only the NER component) against the NER baseline of<ref type="bibr" target="#b13">Lample et al. (2016)</ref>. Bold font indicates the best results for each dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table A2 :</head><label>A2</label><figDesc>Model performance for different embedding dropout values. Bold entries indicate the result reported in Section 5.</figDesc><table><row><cell></cell><cell></cell><cell>LSTM</cell><cell></cell><cell>Entity</cell><cell></cell><cell></cell><cell>Relation</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Dropout</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>Overall F1</cell></row><row><cell></cell><cell></cell><cell>0.5</cell><cell>80.27</cell><cell>80.08</cell><cell>80.18</cell><cell>48.25</cell><cell>38.86</cell><cell>43.05</cell><cell>61.61</cell></row><row><cell></cell><cell></cell><cell>0.4</cell><cell>81.18</cell><cell>81.36</cell><cell>81.27</cell><cell>50.54</cell><cell>42.06</cell><cell>45.91</cell><cell>63.59</cell></row><row><cell>ACE</cell><cell>04</cell><cell>0.3 0.2</cell><cell>81.19 81.01</cell><cell>81.63 81.31</cell><cell>81.41 81.16</cell><cell>50.31 50.14</cell><cell>44.12 44.48</cell><cell>47.01 47.14</cell><cell>64.21 64.15</cell></row><row><cell></cell><cell></cell><cell>0.1</cell><cell>81.27</cell><cell>81.32</cell><cell>81.29</cell><cell>48.20</cell><cell>41.52</cell><cell>44.61</cell><cell>62.95</cell></row><row><cell></cell><cell></cell><cell>0</cell><cell>80.54</cell><cell>79.94</cell><cell>80.24</cell><cell>46.73</cell><cell>39.32</cell><cell>42.71</cell><cell>61.47</cell></row><row><cell></cell><cell></cell><cell>0.5</cell><cell>84.18</cell><cell>86.28</cell><cell>85.22</cell><cell>59.35</cell><cell>60.19</cell><cell>59.76</cell><cell>72.49</cell></row><row><cell>CoNLL</cell><cell>04</cell><cell>0.4 0.3 0.2</cell><cell>84.43 86.44 84.73</cell><cell>85.45 85.73 85.91</cell><cell>84.94 86.09 85.32</cell><cell>63.77 65.14 68.02</cell><cell>62.56 60.66 59.48</cell><cell>63.16 62.82 63.46</cell><cell>74.05 74.45 74.39</cell></row><row><cell></cell><cell></cell><cell>0.1</cell><cell>83.75</cell><cell>84.06</cell><cell>83.90</cell><cell>63.75</cell><cell>60.43</cell><cell>62.04</cell><cell>72.97</cell></row><row><cell></cell><cell></cell><cell>0</cell><cell>84.16</cell><cell>82.76</cell><cell>83.45</cell><cell>65.09</cell><cell>52.13</cell><cell>57.89</cell><cell>70.67</cell></row><row><cell></cell><cell></cell><cell>0.5</cell><cell>77.76</cell><cell>84.83</cell><cell>81.15</cell><cell>49.43</cell><cell>53.61</cell><cell>51.44</cell><cell>66.30</cell></row><row><cell>DREC</cell><cell></cell><cell>0.4 0.3 0.2</cell><cell>78.66 78.97 77.85</cell><cell>83.98 83.98 83.68</cell><cell>81.23 81.39 80.66</cell><cell>50.63 50.00 49.21</cell><cell>54.64 54.73 53.79</cell><cell>52.56 52.26 51.39</cell><cell>66.89 66.83 66.03</cell></row><row><cell></cell><cell></cell><cell>0.1</cell><cell>78.94</cell><cell>83.62</cell><cell>81.21</cell><cell>51.37</cell><cell>53.10</cell><cell>52.22</cell><cell>66.71</cell></row><row><cell></cell><cell></cell><cell>0</cell><cell>78.59</cell><cell>80.18</cell><cell>79.38</cell><cell>50.39</cell><cell>49.96</cell><cell>50.18</cell><cell>64.78</cell></row><row><cell></cell><cell></cell><cell>0.5</cell><cell>85.01</cell><cell>88.29</cell><cell>86.62</cell><cell>72.72</cell><cell>78.15</cell><cell>75.34</cell><cell>80.98</cell></row><row><cell></cell><cell></cell><cell>0.4</cell><cell>84.66</cell><cell>88.37</cell><cell>86.47</cell><cell>72.20</cell><cell>78.00</cell><cell>74.99</cell><cell>80.73</cell></row><row><cell>ADE</cell><cell></cell><cell>0.3 0.2</cell><cell>84.60 84.72</cell><cell>88.66 88.16</cell><cell>86.58 86.40</cell><cell>72.21 72.10</cell><cell>78.86 77.24</cell><cell>75.39 74.58</cell><cell>80.98 80.49</cell></row><row><cell></cell><cell></cell><cell>0.1</cell><cell>84.36</cell><cell>87.98</cell><cell>86.13</cell><cell>72.03</cell><cell>77.51</cell><cell>74.66</cell><cell>80.40</cell></row><row><cell></cell><cell></cell><cell>0</cell><cell>83.80</cell><cell>87.64</cell><cell>85.68</cell><cell>70.50</cell><cell>76.99</cell><cell>73.61</cell><cell>79.64</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table A3 :</head><label>A3</label><figDesc>Model performance for different LSTM layer dropout values. Bold entries indicate the result reported in Section 5.</figDesc><table><row><cell></cell><cell></cell><cell>LSTM output</cell><cell></cell><cell>Entity</cell><cell></cell><cell></cell><cell>Relation</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Dropout</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>Overall F1</cell></row><row><cell></cell><cell></cell><cell>0.5</cell><cell>81.25</cell><cell>81.79</cell><cell>81.52</cell><cell>51.16</cell><cell>41.94</cell><cell>46.09</cell><cell>63.81</cell></row><row><cell></cell><cell></cell><cell>0.4</cell><cell>81.23</cell><cell>81.70</cell><cell>81.47</cell><cell>51.44</cell><cell>42.77</cell><cell>46.71</cell><cell>64.09</cell></row><row><cell>ACE</cell><cell>04</cell><cell>0.3 0.2</cell><cell>81.31 81.01</cell><cell>81.72 81.31</cell><cell>81.51 81.16</cell><cell>48.69 50.14</cell><cell>44.21 44.48</cell><cell>46.35 47.14</cell><cell>63.93 64.15</cell></row><row><cell></cell><cell></cell><cell>0.1</cell><cell>81.01</cell><cell>81.12</cell><cell>81.07</cell><cell>47.55</cell><cell>42.82</cell><cell>45.06</cell><cell>63.07</cell></row><row><cell></cell><cell></cell><cell>0</cell><cell>80.10</cell><cell>80.69</cell><cell>80.39</cell><cell>47.20</cell><cell>40.54</cell><cell>43.61</cell><cell>62.00</cell></row><row><cell></cell><cell></cell><cell>0.5</cell><cell>85.81</cell><cell>86.84</cell><cell>86.32</cell><cell>64.18</cell><cell>59.01</cell><cell>61.48</cell><cell>73.90</cell></row><row><cell>CoNLL</cell><cell>04</cell><cell>0.4 0.3 0.2</cell><cell>83.27 85.13 84.13</cell><cell>84.89 84.89 84.52</cell><cell>84.08 85.01 84.32</cell><cell>66.07 64.82 66.03</cell><cell>61.37 55.45 57.58</cell><cell>63.63 59.77 61.52</cell><cell>73.85 72.39 72.92</cell></row><row><cell></cell><cell></cell><cell>0.1</cell><cell>83.75</cell><cell>84.06</cell><cell>83.90</cell><cell>63.75</cell><cell>60.43</cell><cell>62.04</cell><cell>72.97</cell></row><row><cell></cell><cell></cell><cell>0</cell><cell>83.65</cell><cell>84.89</cell><cell>84.27</cell><cell>65.23</cell><cell>53.79</cell><cell>58.96</cell><cell>71.61</cell></row><row><cell></cell><cell></cell><cell>0.5</cell><cell>78.74</cell><cell>84.22</cell><cell>81.39</cell><cell>51.24</cell><cell>52.69</cell><cell>51.96</cell><cell>66.68</cell></row><row><cell>DREC</cell><cell></cell><cell>0.4 0.3 0.2</cell><cell>78.45 78.97 77.82</cell><cell>85.20 83.98 84.68</cell><cell>81.69 81.39 81.11</cell><cell>50.34 50.00 51.05</cell><cell>55.45 54.73 54.19</cell><cell>52.77 52.26 52.57</cell><cell>67.23 66.83 66.84</cell></row><row><cell></cell><cell></cell><cell>0.1</cell><cell>78.84</cell><cell>83.75</cell><cell>81.22</cell><cell>51.74</cell><cell>54.75</cell><cell>53.20</cell><cell>67.21</cell></row><row><cell></cell><cell></cell><cell>0</cell><cell>77.63</cell><cell>83.85</cell><cell>80.62</cell><cell>51.16</cell><cell>51.39</cell><cell>51.28</cell><cell>65.95</cell></row><row><cell></cell><cell></cell><cell>0.5</cell><cell>84.33</cell><cell>87.95</cell><cell>86.10</cell><cell>71.54</cell><cell>77.27</cell><cell>74.29</cell><cell>80.20</cell></row><row><cell></cell><cell></cell><cell>0.4</cell><cell>85.16</cell><cell>88.16</cell><cell>86.63</cell><cell>72.87</cell><cell>77.81</cell><cell>75.26</cell><cell>80.95</cell></row><row><cell>ADE</cell><cell></cell><cell>0.3 0.2</cell><cell>84.27 84.72</cell><cell>88.00 88.16</cell><cell>86.10 86.40</cell><cell>71.83 72.10</cell><cell>77.42 77.24</cell><cell>74.52 74.58</cell><cell>80.31 80.49</cell></row><row><cell></cell><cell></cell><cell>0.1</cell><cell>84.65</cell><cell>88.04</cell><cell>86.31</cell><cell>72.38</cell><cell>77.49</cell><cell>74.85</cell><cell>80.58</cell></row><row><cell></cell><cell></cell><cell>0</cell><cell>84.44</cell><cell>88.14</cell><cell>86.25</cell><cell>71.64</cell><cell>77.82</cell><cell>74.61</cell><cell>80.43</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table A4 :</head><label>A4</label><figDesc>Model performance for different LSTM output dropout values. Bold entries indicate the best result reported in Section 5.</figDesc><table><row><cell></cell><cell></cell><cell>LSTM</cell><cell></cell><cell>Entity</cell><cell></cell><cell></cell><cell>Relation</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Size</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>Overall F1</cell></row><row><cell>ACE</cell><cell>04</cell><cell>32 64 128</cell><cell>80.99 81.01 80.31</cell><cell>81.25 81.31 80.87</cell><cell>81.12 81.16 80.59</cell><cell>50.33 50.14 47.30</cell><cell>42.60 44.48 41.77</cell><cell>46.14 47.14 44.36</cell><cell>63.63 64.15 62.47</cell></row><row><cell>CoNLL</cell><cell>04</cell><cell>32 64 128</cell><cell>82.83 83.75 82.43</cell><cell>83.13 84.06 83.04</cell><cell>82.98 83.90 82.73</cell><cell>65.78 63.75 64.86</cell><cell>58.29 60.43 53.79</cell><cell>61.81 62.04 58.81</cell><cell>72.39 72.97 70.77</cell></row><row><cell>DREC</cell><cell></cell><cell>32 64 128</cell><cell>77.74 78.97 79.04</cell><cell>85.43 83.98 83.49</cell><cell>81.40 81.39 81.20</cell><cell>50.92 50.00 51.27</cell><cell>52.31 54.73 53.64</cell><cell>51.60 52.26 52.42</cell><cell>66.50 66.83 66.81</cell></row><row><cell>ADE</cell><cell></cell><cell>32 64 128</cell><cell>83.89 84.72 84.27</cell><cell>87.78 88.16 87.87</cell><cell>85.79 86.40 86.04</cell><cell>70.46 72.10 71.36</cell><cell>76.89 77.24 76.77</cell><cell>73.54 74.58 73.97</cell><cell>79.66 80.49 80.00</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table A5 :</head><label>A5</label><figDesc>Model performance for different LSTM size values. Bold entries indicate the result reported in Section 5.</figDesc><table><row><cell></cell><cell></cell><cell>Character</cell><cell></cell><cell>Entity</cell><cell></cell><cell></cell><cell>Relation</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Embeddings</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>Overall F1</cell></row><row><cell>ACE</cell><cell>04</cell><cell>15 25 50</cell><cell>81.02 81.01 81.32</cell><cell>81.57 81.31 81.54</cell><cell>81.29 81.16 81.43</cell><cell>47.87 50.14 49.77</cell><cell>44.78 44.48 44.02</cell><cell>46.27 47.14 46.72</cell><cell>63.78 64.15 64.07</cell></row><row><cell>CoNLL</cell><cell>04</cell><cell>15 25 50</cell><cell>83.33 83.75 85.15</cell><cell>84.34 84.06 82.95</cell><cell>83.83 83.90 84.04</cell><cell>66.03 63.75 59.84</cell><cell>57.11 60.43 52.61</cell><cell>61.25 62.04 55.99</cell><cell>72.54 72.97 70.01</cell></row><row><cell>DREC</cell><cell></cell><cell>15 25 50</cell><cell>79.73 78.97 78.08</cell><cell>84.17 83.98 84.80</cell><cell>81.89 81.39 81.30</cell><cell>52.52 50.00 51.03</cell><cell>55.30 54.73 54.28</cell><cell>53.88 52.26 52.60</cell><cell>67.89 66.83 66.95</cell></row><row><cell>ADE</cell><cell></cell><cell>15 25 50</cell><cell>84.80 84.72 84.65</cell><cell>88.00 88.16 88.08</cell><cell>86.37 86.40 86.33</cell><cell>72.74 72.10 72.17</cell><cell>77.51 77.24 77.45</cell><cell>75.05 74.58 74.72</cell><cell>80.71 80.49 80.52</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table A6 :</head><label>A6</label><figDesc>Model performance for different character embeddings size values. Bold entries indicate the result reported in Section 5.</figDesc><table><row><cell></cell><cell></cell><cell>Label</cell><cell></cell><cell>Entity</cell><cell></cell><cell></cell><cell>Relation</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Embeddings</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>Overall F1</cell></row><row><cell>ACE</cell><cell>04</cell><cell>15 25 50</cell><cell>80.95 81.01 81.17</cell><cell>81.27 81.31 81.61</cell><cell>81.11 81.16 81.39</cell><cell>49.27 50.14 48.01</cell><cell>43.80 44.48 44.48</cell><cell>46.37 47.14 46.18</cell><cell>63.74 64.15 63.78</cell></row><row><cell>CoNLL</cell><cell>04</cell><cell>15 0 50</cell><cell>84.68 83.75 82.32</cell><cell>83.50 84.06 84.15</cell><cell>84.08 83.90 83.23</cell><cell>62.21 63.75 59.30</cell><cell>56.16 60.43 55.92</cell><cell>59.03 62.04 57.56</cell><cell>71.56 72.97 70.39</cell></row><row><cell>DREC</cell><cell></cell><cell>15 25 50</cell><cell>78.48 78.97 78.92</cell><cell>84.81 83.98 84.88</cell><cell>81.53 81.39 81.79</cell><cell>51.83 50.00 51.35</cell><cell>53.21 54.73 53.23</cell><cell>52.51 52.26 52.27</cell><cell>67.02 66.83 67.03</cell></row><row><cell>ADE</cell><cell></cell><cell>15 25 50</cell><cell>84.47 84.72 84.81</cell><cell>88.18 88.16 88.65</cell><cell>86.29 86.40 86.69</cell><cell>71.93 72.10 72.46</cell><cell>77.49 77.24 78.68</cell><cell>74.61 74.58 75.44</cell><cell>80.45 80.49 81.06</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table A7 :</head><label>A7</label><figDesc>Model performance for different label embeddings size values. Bold entries indicate the result reported in Section 5.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table A8 :</head><label>A8</label><figDesc>Model performance for different layer widths l of the neural network (both for the entity and the relation scoring layers). Bold entries indicate the result reported in Section 5.</figDesc><table><row><cell>Embeddings</cell><cell>Size</cell><cell></cell><cell>Entity</cell><cell></cell><cell></cell><cell>Relation</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>Overall F1</cell></row><row><cell>Miwa &amp; Bansal (2016)</cell><cell>200</cell><cell>81.01</cell><cell>81.31</cell><cell>81.16</cell><cell>50.14</cell><cell>44.48</cell><cell>47.14</cell><cell>64.15</cell></row><row><cell>Adel &amp; Sch√ºtze (2017)</cell><cell>50</cell><cell>82.18</cell><cell>79.83</cell><cell>80.99</cell><cell>49.10</cell><cell>41.40</cell><cell>44.92</cell><cell>62.96</cell></row><row><cell>Li et al. (2017)</cell><cell>200</cell><cell>81.51</cell><cell>81.35</cell><cell>81.43</cell><cell>46.59</cell><cell>44.43</cell><cell>45.49</cell><cell>63.46</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table A9 :</head><label>A9</label><figDesc>Model performance for different embeddings on the ACE04 dataset. Bold entries indicate the result reported in Section 5.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/bekou/multihead_joint_entity_relation_extraction 3 https://github.com/tticoin/LSTM-ER/tree/master/data/ace2004 4 For the CoNLL04, DREC and ADE datasets, the head region covers the whole entity (start and end boundaries). The ACE04 already defines the head region of an entity. 5 http://cistern.cis.lmu.de/globalNormalization/globalNormalization_ all.zip</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">http://tti-coin.jp/data/wikipedia200.bin 7 https://drive.google.com/uc?id=1Dvibr-Ps4G_GI6eDx9bMXnJphGhH_M1z&amp; export=download 8 http://evexdb.org/pmresources/vec-space-models/wikipedia-pubmed-and-PMC-w2v. bin</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation</title>
		<meeting>the 12th USENIX Conference on Operating Systems Design and Implementation<address><addrLine>Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Global normalization of convolutional neural networks for joint entity and relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sch√ºtze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A review of relation extraction. Literature review for Language and Statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Badaskar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">II</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Reconstructing the house from the ad: Structured prediction on real estate classifieds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bekoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deleu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Demeester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Develder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="274" to="279" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An attentive neural architecture for joint segmentation and parsing and its application to real estate ads</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bekoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deleu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Demeester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Develder</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.eswa.2018.02.031</idno>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page" from="100" to="112" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
		<idno type="DOI">10.1109/72.279181</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions on neural networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On shortest arborescence of a directed graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-J</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientia Sinica</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1396" to="1400" />
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dependency tree kernels for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Culotta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sorensen</surname></persName>
		</author>
		<idno type="DOI">10.3115/1218955.1219009</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 42nd Annual Meeting on Association for Computational Linguistics<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="423" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The automatic content extraction (ace) program-tasks, data, and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Doddington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Przybocki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Strassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Fourth International Conference on Language Resources and Evaluation</title>
		<meeting>Fourth International Conference on Language Resources and Evaluation<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Optimum branchings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Edmonds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of research of the National Bureau of Standards</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="233" to="240" />
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Relex-relation extraction using dependency parse trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fundel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kffner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zimmer</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/btl616</idno>
		<idno>doi:10</idno>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="365" to="371" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Conference on Machine Learning</title>
		<meeting>the 18th International Conference on Machine Learning<address><addrLine>San Francisco, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neural architectures for named entity recognition</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="260" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A neural joint model for entity and relation extraction from biomedical text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ji</surname></persName>
		</author>
		<idno type="DOI">10.1186/s12859-017-1609-9</idno>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Joint models for extracting adverse drug events from biomedical text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Fifth International Joint Conference on Artificial Intelligence<address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IJ-CAI/AAAI Press</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2838" to="2844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Incremental joint extraction of entity mentions and relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="402" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">End-to-end sequence labeling via bi-directional LSTM-CNNs-CRF</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1064" to="1074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Neural Information Processing Systems</title>
		<meeting>the 26th International Conference on Neural Information Processing Systems<address><addrLine>Nevada, United States</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">End-to-end relation extraction using LSTMs on sequences and tree structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1105" to="1116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germany</forename><surname>Berlin</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Modeling joint entity and relation extraction with table representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sasaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1858" to="1869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Distributional semantics resources for biomedical text processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S S</forename><surname>Ananiadou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Symposium on Languages in Biology and Medicine</title>
		<meeting>the 5th International Symposium on Languages in Biology and Medicine<address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="39" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A survey of named entity recognition and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nadeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sekine</surname></persName>
		</author>
		<idno type="DOI">10.1075/li.30.1.03nad</idno>
	</analytic>
	<monogr>
		<title level="m">Lingvisticae Investigationes</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="3" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on International Conference on Machine Learning</title>
		<meeting>the 30th International Conference on International Conference on Machine Learning<address><addrLine>Atlanta, USA</addrLine></address></meeting>
		<imprint>
			<publisher>JMLR.org</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Utd: Classifying semantic relations by combining lexical and semantic resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harabagiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Workshop on Semantic Evaluation</title>
		<meeting>the 5th International Workshop on Semantic Evaluation<address><addrLine>Los Angeles, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="256" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A linear programming formulation for global inference in natural language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W04-2401" />
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL 2004 Workshop: Eighth Conference on Computational Natural Language Learning</title>
		<meeting><address><addrLine>Boston, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Classifying relations by ranking with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="626" to="634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Semantic relation classification via convolutional neural networks with simple negative sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D15-1062" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="536" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Classifying relations via long short term memory networks along shortest dependency paths</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1785" to="1794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Joint inference for fine-grained opinion extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cardie</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P13-1161" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1640" to="1649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Kernel methods for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zelenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Aone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richardella</surname></persName>
		</author>
		<idno type="DOI">10.3115/1118693.1118703</idno>
		<idno>doi:10. 3115/1118693.1118703</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1083" to="1106" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Relation classification via convolutional deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01006</idno>
		<title level="m">Relation classification via recurrent neural network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dependency parsing as head selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="665" to="676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Joint entity and</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
