<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Win-Fail Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paritosh</forename><surname>Parmar</surname></persName>
							<email>parmap1@unlv.nevada.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Nevada</orgName>
								<address>
									<settlement>Las Vegas</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Morris</surname></persName>
							<email>brendan.morris@unlv.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Nevada</orgName>
								<address>
									<settlement>Las Vegas</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Win-Fail Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Current video/action understanding systems have demonstrated impressive performance on large recognition tasks. However, they might be limiting themselves to learning to recognize spatiotemporal patterns, rather than attempting to thoroughly understand the actions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Action recognition, which can be defined as the task of identifying various action classes in videos, has thus far been used as a representative task for video understanding. Video action recognition involves the processing of spatiotemporal data, and extracting low-dimensional spatiotemporal signatures from video volumes. Based on these signatures, probabilities of action classes are determined.</p><p>We make two observations regarding the task of action recognition. Firstly, while current action recognition datasets, like UCF101, HMDB51, Kinetics, Sports1M, etc., have focused on increasing their dataset sizes and covering a larger number of classes, samples in those datasets exhibit low intra-action-class variance in their spatiotemporal sig-  <ref type="figure">Figure 1</ref>: Illustration of intra-class variance (along columns, not rows) in a typical action recognition dataset vs. in our action understanding dataset. (Left) Samples from two randomly chosen classes, Basketball (BB) and TennisSwing (TS), from a typical action recognition dataset. (Right) Samples from ours newly compiled dataset, which has two action classes: wins and fails. As can be seen, the typical action recognition dataset does not have much intra-class variance, because of which action recognition is reduced to a pattern recognition problem. Please view in an Adobe Reader to play videos.</p><p>natures. For example, all of the samples from action class BasketBall contain identical spatiotemporal signatures, like people holding a basketball with their hands, and trying to throw it into the basket (refer to <ref type="figure">Fig. 1</ref>); or as another example, all the samples from action class TennisSwing contain people holding a tennis racquet in their hand, and moving their arm. As a result, action recognition has, so far, been limited to cases where spatiotemporal signatures within individual action classes remain identical. Secondly, action sequences are not complex, although this trend is starting to increase with the introduction of datasets like Something-Something. However, overall, and as a consequence of the first shortcoming, the datasets do not require video understanding models to reason at a deeper level: e.g., trying to draw logical inferences about the actors' goals (what they are trying to do, which is beyond current fixed action set classification) by piecing together contextual (including human-object interactions) and human-movement cues as the video progresses, to determine whether the actor/s were able to accomplish what they set out to do.</p><p>This raises a question as to, if the current video/action understanding systems make an attempt to really understand the action, or if they limit themselves to identifying spatiotemporal patterns. We believe that video understanding comes down to a pattern recognition problem. We are not conveying that action recognition is not needed or is unimportant, rather we view action recognition as a very important initial task. However, solely focusing on developing in the direction of action recognition might be limiting in nature. Therefore, in order to encourage action understanding systems to gain a deeper level understanding of human actions, we slightly redefine the task of action recognition as it currently stands. Instead of differentiating among action classes, we propose to repurpose the task of (action) recognition to differentiating between the concepts of winning and failing. Winning can be defined as completing a task that the human set out to do, while failing can be defined when human is not able to complete the task. For example, successfully flipping a cup, putting a basketball in the basket, or being able to walk on one's hands, etc. are considered as wins, while not successfully flipping a cup, throwing a basketball that does not go into the basket, or trying to walk on one's hands but instead falling over, are considered as fails. As one can imagine, &amp; see in <ref type="figure">Fig. 1</ref>, that the spatiotemporal signatures within the samples are very different, yet represent the same concepts.</p><p>Humans, even as young as a year old, are able to infer and/or reason about the goals of others' actions from experience, contextual cues, kinematic cues, etc. <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b47">48]</ref>. They are able to perceive the difficulty of a task/action <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b15">16]</ref>, and tend to put more value on actions that are more difficult or require perceivably more effort <ref type="bibr" target="#b20">[21]</ref>. In fact, in Olympic events like diving and gymnastic vaulting, the scores are directly proportional to the degree of difficulty of the actions. It has been shown that the degree of difficulty can be measured from videos <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b36">37]</ref>. Therefore, when observing competitive scenarios (where participants are trying to gain the maximum score), even when the game rules, or a task descriptions are not explicitly intimated to humans, they may still be able to figure them out by reason-ing about what action sequence might be more difficult to execute, and consequently, decide if an action instance was a win or fail. We aim to give our models this kind of deeper understanding through learning general notions of win and fail actions in videos in simplified setting.</p><p>To facilitate our novel task, we introduce a novel win-fail action understanding dataset. Our newly introduced dataset includes samples from the following categories: general stunts, win/fail internet videos, trick-shots, &amp; party games. It could be argued that it might not be possible for our networks to identify standalone wins or standalone fails. Therefore, we choose a comparative approach and, instead of compiling individual, unpaired samples from win and fail classes, we compile paired win-fail samples for each action instance. Currently, our dataset consists of 817 pairs. We provide further details regarding the dataset in Sec. 3. We analyze our dataset in detail and provide baselines for future works in Sec. 4. Additionally, a novel video retrieval task is explored to characterize the general action understanding capabilities learned through the win-fail task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Action recognition datasets: Datasets can be divided into the following categories: 1) short-term temporal dynamics (UCF101 <ref type="bibr" target="#b40">[41]</ref>, HMDB51 <ref type="bibr" target="#b21">[22]</ref>, Kinetics <ref type="bibr" target="#b2">[3]</ref>), where actions can be classified from a single or very few frames, or even the background; 2) long-term temporal dynamics (Something-Something <ref type="bibr" target="#b14">[15]</ref>, Diving48 <ref type="bibr" target="#b26">[27]</ref>, MTLAQA <ref type="bibr" target="#b34">[35]</ref>, Epic-Kitchens <ref type="bibr" target="#b4">[5]</ref>, Jester <ref type="bibr" target="#b30">[31]</ref>, etc.); 3) coarse-grained (UCF101, Kinetics, etc.); and 4) fine-grained (Diving48, MTLAQA, etc.). Unlike in coarse-grained action classification, actions in fine-grained classification category have very subtle differences between signature action patterns.</p><p>Regardless of how we group current action datasets, the task ultimately remains same -to learn the spatiotemporal signatures pertaining to each action class. Note that, longer temporal dynamics (for example, counting somersaults in <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b34">35]</ref> or differentiating between pulling and pushing in <ref type="bibr" target="#b14">[15]</ref>) do not necessarily require deeper understanding of actions. From earlier action recognition days to the present, the focus, thus far, has been to increase the dataset size and increase the number of action classes. We believe this limits the models from gaining deeper understanding of what is happening in front of a camera. Therefore, instead, we focus on increasing the intra-class variance.</p><p>Our work can also be considered closer in spirit to <ref type="bibr" target="#b13">[14]</ref>, which builds a video dataset with fully observable and controllable object and scene bias, and which truly requires spatiotemporal understanding in order to be solved. Our task and dataset are different in several ways, such as: ours is real world dataset; have humans performing actions/avtivities; objects in our dataset have purposes/meaning, etc.</p><p>Action recognition models: Unlike in image recognition, where a decision is made based on a single image, in a video understanding task, modeling temporal relationships is crucial. Some of the earlier deep learning based action recognition works, which considered multiple frames to do recognition include works like <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b51">52]</ref>.</p><p>TSN <ref type="bibr" target="#b44">[45]</ref> proposes a very simple, yet effective approach of sampling a few frames from the entire video and processing these frames individually to extract frame-wise features. Frame-level features are then combined using an aggregation scheme to get video-level representation. The authors found averaging to work the best. Averaging is actually temporal order agnostic, which indicates that action recognition tasks on datasets like UCF101, HMDB51, and Kinetics do not really demand temporal modeling.</p><p>The introduction of datasets like Something-Something, in which temporal order of frames matter (e.g. recognizing pushing vs. pulling something), motivated works like TRN <ref type="bibr" target="#b54">[55]</ref>, which proposed a temporal reasoning module. While TRN worked by modeling/discovering temporal relations from extracted features, TSM <ref type="bibr" target="#b28">[29]</ref> aimed extracting temporal relations in the CNN backbone at a lower computational cost. Some works like <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b48">49]</ref> propose approaches to combine short-term and long-term features.</p><p>Other works propose approaches that improve focus on the human actor, either by jointly estimating the pose of the actor <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b53">54]</ref>, or by modeling the relationship between the actor and objects <ref type="bibr" target="#b41">[42]</ref>.</p><p>We employ developments in designing our models, and then compare them to see what works and what does not work on our dataset.</p><p>AQA/skills assessment: AQA <ref type="bibr">[24, 25, 32-35, 37, 39, 43, 44, 46,50,51,53</ref>] is another action analysis task, which involves quantifying how well an action was performed. Similar in nature is the task of skills assessment <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b35">36]</ref>. Like action recognition, the task still comes to learning and/or recognizing spatiotemporal patterns, although it is more finegrained than action recognition. In addition to recognizing patterns, AQA/SA also involves valuation of those patterns.</p><p>In AQA, examples of these patterns include keeping legs straight in pike position, stable landing, tight form in tuck position, etc.; in SA, examples of these patterns include, not stretching tissues too much, handling them carefully, etc. We do note that there is an association between AQA/SA and win-fail recognition, in that higher skills-levels are generally associated with wins, and lower skills-levels are associated with fails. However, our work is different from these works, in that while these works propose action-specific approaches, our core idea is to increase intraclass variance among samples -our dataset contains four different domains -in order to encourage </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Win-Fail Action Recognition Dataset</head><p>To address the previously mentioned limitations of current action recognition datasets and to facilitate our new task of win-fail action recognition task, we introduce a novel Win-Fail action recognition dataset. The Win-Fail dataset has the following characteristics: 1) a large variance in the structure of the task (both in action and context) and in the semantic definitions of wins and fails across samples; and 2) action sequences that are complex but at the same time win/fail recognition task is feasible. It is possible to identify winning/failing through reasoning on human movements and context (including actor-object interactions, etc.), without requiring external knowledge. For example, we do not include games of chess in our dataset since it requires knowledge of game mechanics. Since identifying wins and fails in standalone fashion may be overly difficult, we collect paired win and fail samples: i.e. for every win action instance, we have provided a fail version of that action instance. We collected data samples from following domains:</p><p>1. General Stunts (GS): Actions from this domain resemble stunts similar to those seen in movies or arbitrarily choreographed stunts. To collect data sam-  ples from this domain, we made use of paired compilations released by the stunt artists themselves. In these paired compilations, they include and specifically indicate their failed and successful attempts at various stunt routines. Failures can be attributed to factors like: miscalculation in placement of limbs, imbalance, erroneous landing, not able to securely grip handles, etc. In the samples from this domain, people can be seen working/interacting with large objects like truck tires, foam plyo boxes, chutes, ladders, etc. Action sequences are mainly comprised of a single actor. Examples presented in <ref type="figure" target="#fig_1">Fig. 2a</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Internet Wins-Fails (IWF):</head><p>This is a popular category of videos on YouTube, where people attempt to do all sorts of things like walking on their hands, pole dancing, cycling at high speed through forests, skateboarding, hulahooping, etc. We collected pairs of wins and fails of people trying to these things. Note that these types of compilations many times include cases where mishaps happen because of some other person's mistake or some objects' failure (breaking off, falling, etc.). We did not include those kinds of samples; we only include samples where the wins and fails are outcomes of the efforts of the person under consideration.</p><p>We also did not included cases where the person was affected due to factors outside of their control. Examples of samples omitted are: a fan unexpectedly falls on a person working at their desk; or a pole becomes loose and comes off, while a pole dancer is using it. These kinds of videos may not require the actual understanding of actions, and may simply be classified by detecting the sudden increase in the video speed/motion magnitude.</p><p>Reasons for failure include errors in planning, aiming, perception/judgement, or execution; lack of skills/ability/strength (unlike in general stunts, actors are not always trained), etc. In this domain, actors can be seen interacting with large to medium sized objects such as skateboards, bicycles, hulahoops, skis, ropes, poles, exercise balls, etc. Action sequences mainly involves a single person. Examples presented in <ref type="figure" target="#fig_1">Fig. 2b.</ref> 3. Trick-shots (TS): This is another popular category of videos on YouTube, where people try to do things that are extremely difficult to perform. Examples include throwing compact disc into a very slim opening from a distance; generally, this requires many attempts before one succeeds. We compiled samples from failure footage and corresponding successful attempts. Objects used are medium to small sized such as, basketballs, spoons, bags, bottles, food items, cellphones, cups, etc. Unlike previous domains, the choreography of the action sequences in this domain is not limited to a single actor, and may involve the coordinated performance of two actors. Examples presented in <ref type="figure" target="#fig_1">Fig.  2c</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Party Games (PG):</head><p>Parties/social gatherings/gettogethers generally have a series of games. We collected pairs of failed and successful attempts at numerous indoor party games. We only selected games that are short (∼ 2.5 secs), and where win/fail can be recognized. Actors can be seen interacting with small sized objects like cups, pencils, ping-pong balls, etc. Action sequences mainly involve a single actor, although unlike other domains, human spectators can be seen in the background standing steadily or moving. Examples shown in <ref type="figure" target="#fig_1">Fig. 2d</ref>.</p><p>Excluding telltale signs: Sometimes, actors might be behaving joyously (after winning), or acting disappointed (after failing), which might give enough clue to the models to correctly predict win or fail without actually needing to understand the whole action sequence. There could be other signs as well. Therefore, during data collection, we made sure to not include any such signs in our action sequences.</p><p>All of the videos are of high resolution: 1280 × 720 pixels. Further specifications about our dataset are provided in <ref type="table" target="#tab_1">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we systematically determine the characteristics of our dataset, then provide baselines and suggestions for future efforts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><p>We used a CNN (G) to compute spatial features, followed by a temporal modeling module 1 (TMM) (F ) to compute temporal relationships from spatial features from G. In particular, we considered temporal order agnostic (averaging activations from G, and further processed through fc layers) and temporal order respecting (LSTM <ref type="bibr" target="#b17">[18]</ref> and TRN <ref type="bibr" target="#b54">[55]</ref>) as our temporal models.</p><p>We used cross-entropy loss, L as the objective function to train the networks. Let x i and y i be the predicted and ground-truth labels, then,</p><formula xml:id="formula_0">L = − 1 N N i=1 y i logx i<label>(1)</label></formula><p>We experimented with the following two approaches for the win-fail action recognition task:</p><p>1. Individual/standalone analysis: this is identical to any typical image/action classification, where we process the images/video-clip through a network, and it predicts the class (win or fail in our case). The model is illustrated in <ref type="figure" target="#fig_2">Fig. 3 (a)</ref>.</p><formula xml:id="formula_1">x i = H(F (G(V ))), where,</formula><p>V is input video frames in the case G is a 2D-CNN, or it is video clips if G is a 3D-CNN; and H represents a linear layer. This is a binary classification problem:</p><formula xml:id="formula_2">x i , y i ∈ {0, 1}.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Pairwise analysis:</head><p>In this approach, we are able to leverage the pairwise nature of our dataset using a siamese setup, as shown in <ref type="figure" target="#fig_2">Fig. 3 (b)</ref>. Let V a and V b represent two input videos, then,</p><formula xml:id="formula_3">x i = H(C(F (G(V a )), F (G(V b ))))</formula><p>, where, C is the concatenation operation. We allow V a = V b to incorporate the individual/standalone analysis of samples. We see pairwise loss as an aid to the learning process. In all, pairwise analysis is a four-way classification problem:</p><p>x i , y i ∈ {00, 01, 10, 11}.</p><p>Implementation details: We used PyTorch <ref type="bibr" target="#b37">[38]</ref> to implement all of our models. We used 2D ResNet-18 <ref type="bibr" target="#b16">[17]</ref> pretrained on ImageNet <ref type="bibr" target="#b5">[6]</ref> as our CNN, unless mentioned otherwise. We trained all of our models for 100 epochs using ADAM <ref type="bibr" target="#b19">[20]</ref> as our optimizer, with a learning rate of 1e-4, and a batchsize of 5. This also helped in keeping hyperparameter tuning to a minimum. We used a LSTM module with a hidden state of size 256. For a fair comparison with LSTM, for the averaging case, we further add fully-connected layers after the averaging operation. Unless specified otherwise, we uniformly sampled 16 frames from entire video sample sequences and employed our pairwise approach. We resized all of the videos to a resolution of 320 × 240 pixels, and applied center cropping (224 × 224 pixels); during the training phase, we also applied horizontal flipping. Center-cropping also removes branding/watermarking, which may give out the win/fail class, and may allow the network to take shortcuts. We will make our codebase publicly available.</p><p>Metric: Unless specified otherwise, we report overall accuracy in percentage.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Task feasibility, split ratios and aggregation schemes</head><p>First of all, we wanted to determine if our task is feasible. Secondly, video action recognition by definition is a task of spatiotemporal nature, which implies that, ideally, temporal order of frames/clips, and hence temporal modeling, plays a very important part. On current action datasets, averaging (which ignores temporal order) as the consensus scheme yields the best results <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b44">45]</ref>. Although some works incorporate local, short-term motion cues using 3D-CNN, optical flow, etc., the demand for actual long-term temporal modeling from current datasets is still limited. In this experiment, we wanted to determine which temporal modeling scheme is better suited for our dataset: temporal-order agnostic (averaging) or temporal-order sensitive (LSTM).</p><p>In order to focus only on temporal modeling, we pretrained our CNN backbone on ImageNet and then froze it, which acts as a general spatial feature extractor. Then, we learned only the parameters of the temporal model, which takes in features from the CNN backbone. We have decoupled the spatial learning aspect from temporal modelingboth temporal models are fed the same spatial features.</p><p>Thirdly, we wanted to determine a good train:test split ratio for our dataset. For that, we considered various train:test ratios. We compared Averaging (AVG) vs. LSTM for various train:test split ratios.</p><p>For this experiment, we employed pairwise comparative approach. Results are shown in <ref type="table">Table.</ref> 2a. Random guessing would have an accuracy of 25%, since a pairwise comparative approach is a four-way classification problem. Both models performed significantly better than random chance across all split ratios, which suggests that our task is feasible. We observed that LSTM outperforms AVG for all split ratios. We also note that the LSTM's gain over AVG increases as the training pool increases. LSTM performing better than AVG clearly indicates that our dataset demands actual temporal modeling from models. This is Method Accuracy Chance 50.00 Individual 58.65 Pairwise 76.05 <ref type="table">Table 3</ref>: Individual vs. Pairwise Assessment.</p><p>because, even with a comparative approach, various contextual and human-movement cues from start to finish need to be strung together in a sequential manner to infer about the actor's goal and determine whether they achieved it.</p><p>Noting the trade-off between split ratios and performance, we chose 30:70 as our optimal split, which was used for the rest of the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Individual vs. Pairwise</head><p>In this experiment, we aimed to determine the correct approach to a win-fail action understanding problem: individual/standalone analysis or a pairwise comparative approach.</p><p>We used the LSTM aggregation and the same settings as Experiment 4.1. We compare the performances of individual and pairwise approaches in <ref type="table">Table 3</ref>. Note that individual assessment is actually built into our pairwise approach as well. In <ref type="table">Table 3</ref>, for the pairwise approach, we show the average accuracy of 00 and 11 (individual assessment), which is directly comparable to that of actual individual assessment. Since individual assessment is a two-way (win or fail) classification problem, a random guess would have an accuracy of 50%. We observe that the individual assessment model has quite a poor performance. On other hand, by learning through pairwise comparison, the model was able learn in a much better way, and was able to perform significantly better, with an accuracy of 76.05%. These results indicate that a pairwise comparative approach is much more suitable, at least for the model that we used. We suggest a comparative approach, but we also want to encourage future works to develop better standalone approaches.</p><p>We could have altered the order of Experiments 4.1 and 4.2, but that would have resulted in performing an unnecessarily larger number of experiments. For the rest of the experiments, we use the LSTM based pairwise approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Rate of sampling input frames</head><p>In this experiment, we studied the effect of the rate of sampling input frames. In particular, we considered sampling uniformly spaced 4, 8, 16, and 32 frames as input. We also conducted the same experiment on a typical action recognition dataset. The results are in <ref type="table" target="#tab_5">Table 4a</ref>. We observed that on UCF101 dataset, the performance saturated with just 4 frames, while on ours action understanding dataset, it saturates at 16 frames. This indicates that   intermediate frames and the cues/details in those are important. We also show the effect of varying the number of input frames across individual domains in <ref type="table" target="#tab_5">Table 4b</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Typical 3DCNN as feature extractor</head><p>3DCNNs are known to extract much richer features than a 2DCNNs and as a result, obtain state-of-the results on action recognition tasks. In this experiment, we used a 3D counterpart (ResNet18-3D) of our 2DCNN. Both extract 512-dimensional features. ResNet18-3D extracts features from 16-frame clips. With 3DCNN as the backbone, we used 16 16-frame clips, in place of 16 frames, as input. Clips used with 3DCNN have a lower resolution (112 × 112 pixels), as compared to that of frames used with 2DCNN (224 × 224 pixels).</p><p>We compare the results in <ref type="table" target="#tab_6">Table 5</ref>. Interestingly, we found that 3DCNN performed worse than 2DCNN. Only domain where 3D-CNN performed better is General Stunts, probably because Kinetics has similar action classes like Gymnastics. Potential reasons for poorer performance of 3D-CNN could be: 1) smaller resolution input might be hurting in our case because of the smaller sized objects and interactions involved with those; 2) actions patterns are different; 3) multiple humans present in the scene; or 4) ImageNet contains classes for many objects found in our dataset, while Kinetics does not. We believe explicitly modeling/finetuning 3D-CNN for human-object interactions would be beneficial.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">End-to-End learning</head><p>So far, we have used spatial features extracted using an off-the-shelf CNN. In this experiment, we sought to determine if there is a utility in jointly learning spatial and temporal representations. We unfroze the CNN backbone and optimized the network end-to-end. Then, we again finetuned only the temporal modeling module. The results are presented in <ref type="table" target="#tab_8">Table 6a</ref>. We also evaluated a multiscale TRN (16f) baseline. We observed a significant boost in performance, indicating that the dataset requires spatial representation learning as well. We observe highest improvement in General Stunts, probably because ImageNet does not have people in unusual, convoluted poses, and hence, benefits a lot from finetuning. Furthermore,class-wise accuracies after end-to-end optimization are much more balanced w.r.t. 00 and 11 (compare <ref type="table" target="#tab_3">Tables 6c and 2c)</ref>.</p><p>These performances also serve as baselines for future works. Since the end-to-end optimized model worked best, we use that in the rest of the experiments. For simplicity, we continue to use LSTM as our temporal modeling module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Importance of temporal order</head><p>In this experiment, we wanted to confirm if the temporal order from various parts of sequences matter. For that, in the testphase, we perturbed the temporal order of only a part of the sequence, while keeping the temporal order of the other parts of the sequence intact. In particular, we shuffled: 1) one-third of the sequence from the start (first 5 of the 16 sampled frames); 2) the middle one-third of the sequence (middle 5 frames); and 3) the last one-third of the sequence. Additionally, we considered shuffling the entire sequence.</p><p>The results are shown in <ref type="table" target="#tab_9">Table 7</ref>. We observed that perturbing the temporal order in all the parts affected the performance negatively. Furthermore, we observed that impact of perturbation increased as we moved the focus of the shuffling towards the end of the sequence. Shuffling the entire sequence had the most negative impact. These observations support the hypothesis that our dataset demands/requires the algorithms/models to temporally model from the beginning to the end of the sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Where are the necessary cues?</head><p>In this experiment, we wanted to probe if "seeing" the entire sequence is needed, and/or if the model is able to make prediction just from a subsequence. During the testphase, we asked the model to classify based on partial sequences. Results are presented in <ref type="table" target="#tab_10">Table 8</ref>. We found that predictions based only on the first or last one-fourth sequence are equal to random guessing (25% accuracy). We noticed that the accuracy of the model increased the further it observed the sequence, indicating that the necessary cues are present along the entire sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8.">Video Retrieval</head><p>To evaluate if our task yields deeper understanding, we devised a novel video retrieval experiment. We collected an additional, separate set of win and fail samples (from Internet Wins-Fails domain), which served as our queries. We also collect a set of baby and animal win-fails. Actors in our original win-fail dataset and the additional samples (queries) are adolescent and adult human beings. We use queries to retrieve videos from databases, where we changed the situations and actors. Particularly, we considered three different databases:  etc. Note that animals have different structure and movements than humans. We consider animal win and fail as relevant to human win and fail queries, resp.</p><p>Note that these queries and databases were not seen dur-ing training. We used cosine similarity as a similarity measure when retrieving, and recall at rank 1 and 5 (R@1, R@5) as metrics. We also noted average similarity difference from query to relevant and irrelevant samples in the databases (Sim ∆), which would show sensitivity of features towards wins/fails in unseen domains. We considered model trained UCF101 for action recognition as our baseline. Results are summarized in Tab. 9. We found that win-fail recognition model outperformed action recognition model across all the databases. Moreover, the gap in performances increased when retrieving from animal database. We also observed that win-fail recognition model was more sensitive to win-fail aspect of the query, retrieved samples. These results also suggest application of WFR in areas like elderly and children safety monitoring. Qualitative results are presented in <ref type="figure">Fig. 4</ref>. For brevity, in the following we refer to individual samples using respective row, and column numbers in <ref type="figure">Fig. 4</ref>. We observe that AR model retrieves considerably on the basis of color (e.g.: (R1,C6), (R1,C7), (R5,C5), (R5,C7)); low-level motion patterns (e.g. extended hand in (R3,C1)→(R3,C6); sliding pattern (R4,C1)→(R4,C5) -skater smoothly gliding down the road is good, while the baby is smoothly falling down the slide is bad, jumping pattern (R6,C5),(R6,C6), (R6,C7)). Compared to that WFR model retrieves while maintaining meaning (e.g. (R2,C2), (R2,C4) both exhibit compact body form of the diver in (R2,C1) necessary to sit on the chair and pass through swim rings; landing safely in (R6,C4) and (R6,C1); stunt involving multiple parties (R6,C1), (R6,C3); falling while reaching out (R5,C1),(R5,C2), (R5,C3)). Sometimes, like AR, WFR also puts more emphasis on motion patterns (e.g., (R6,C2)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>As a step towards true video/action understanding, we proposed the task of differentiating between the concepts of wins and fails. To facilitate our task, we also introduced a new dataset, which contains 817 pairs of successful and failed attempts at various activities from 'General Stunts,' 'Internet Wins-Fails,' 'Trick Shots,' and 'Party Games' domains. The action sequences in our dataset are not overly long, yet are complex. We systematically analyzed our dataset and found that: 1) our dataset requires true temporal modeling; 2) pairwise approach worked better than individual/standalone assessment; 3) details/cues important for understanding video/action are present in intermediate frames along the entire sequence; 4) better performance (as compared to a 2DCNN) of an off-the-shelf 3DCNN did not translate well to dataset/task; and 5) spatial modeling is equally important. While current action recognition methods worked well on our task/dataset, they still leaves a large gap to cover, indicating that there is significant opportunity to improve on this task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Examples of pairwise samples from various domains in our dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Models: Individual (a) and Pairwise (b). 00 and 11 in pairwise are equivalent to 0 and 1 in individual.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Action domainNo. of pairs Avg. len. (# fr.; s) Dataset details.</figDesc><table><row><cell>General stunts</cell><cell>122</cell><cell>96; 3.83</cell></row><row><cell>Internet wins-fails</cell><cell>258</cell><cell>112; 4.46</cell></row><row><cell>Trickshots</cell><cell>135</cell><cell>66; 2.62</cell></row><row><cell>Party games</cell><cell>302</cell><cell>62; 2.48</cell></row><row><cell>Overall</cell><cell>817</cell><cell>84; 3.33</cell></row></table><note>models to understand actions at a deeper level beyond surface-level pattern recognition; action sequences in our dataset are much more complex. Parmar et al. [33] have proposed learning a single AQA model across multiple actions, resulting in more intra-class variance, but the goal of their work was to learn shared action quality elements more efficiently, while our goal in considering multiple domains is to gain an actual understanding of the actions. Visual concept learning: We find that works by Binder et al. [1], Zhou et al. [56], and Chesneau et al. [4] are closest to ours. While [1, 56] focus on recognizing more complex visual concepts, beyond objects in image domain, we in- troduce win-fail recognition in the video domain for deeper human action understanding. Chesneau et al. [4] address recognizing concepts like 'Birthday Party,' 'Grooming an Animal,' and 'Unstuck a Vehicle' in web videos. However, these concepts do not have large intra-class variance like ours, and are less complex and challenging than ours.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>: (a) Split ratios and aggregation methods; (b)</cell></row><row><cell>domain-wise gains of LSTM over AVG for 30:70 split; (c)</cell></row><row><cell>class-wise accuracy of LSTM for 30:70 split.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>(a) Effect of rate of sampling input frames. (b) Effect on individual domains.</figDesc><table><row><cell>CNN</cell><cell>Trained on Accu.</cell><cell>GS IWF TS</cell><cell>PG</cell></row><row><cell cols="2">R18-2D ImageNet 68.88 R18-3D Kinetics 65.65</cell><cell cols="2">+5.75 -1.25 -2.00 -8.75</cell></row><row><cell></cell><cell>(a)</cell><cell>(b)</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>: (a) Backbone Choice: 2DCNN vs. 3DCNN; (b)</cell></row><row><cell>effect of using 3DCNN compared to 2DCNN across all do-</cell></row><row><cell>mains.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell cols="2">: (a) End-to-End learning. (b) Domain-wise im-</cell></row><row><cell cols="2">provements. (c, d) Class-wise accuracy of LSTM and TRN.</cell></row><row><cell cols="2">Shuffle type Accu.</cell></row><row><cell>None</cell><cell>74.78</cell></row><row><cell>First 1/3</cell><cell>74.16</cell></row><row><cell>Middle 1/3</cell><cell>74.26</cell></row><row><cell>Last 1/3</cell><cell>72.42</cell></row><row><cell>Full</cell><cell>61.19</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Effect of shuffling.</figDesc><table><row><cell cols="2">Observed seq. Accu.</cell></row><row><cell>Full</cell><cell>74.78</cell></row><row><cell>First 1/4</cell><cell>25.13</cell></row><row><cell>First 1/2</cell><cell>35.62</cell></row><row><cell>First 3/4</cell><cell>48.60</cell></row><row><cell>Last 1/4</cell><cell>23.47</cell></row><row><cell>Last 1/2</cell><cell>28.93</cell></row><row><cell>Last 3/4</cell><cell>52.58</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Partial observations.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>1 .</head><label>1</label><figDesc>Activities of Daily Living (ADL)-Fall: We used the dataset released by<ref type="bibr" target="#b22">[23]</ref> as our database. ADL include activities such as sitting down, standing up, getting things from floor, etc. Falls include person walking and falling down. This dataset was built to be used in monitoring elderly people. We consider ADL, and Fall as relevant to win and fail queries, respectively.2. Win-Fail with Babies as actors: Fails in babies in-clude babies trying to get up, crawl, walk but falling over since they have not yet acquired balance, falling while sitting due to lack of balance and control, etc.Movements of babies are a lot more jittery compared to adults. Wins include climbing into their cradles successfully, throwing balls into baskets, passing through</figDesc><table><row><cell>Model</cell><cell></cell><cell>Win-Fail → Fall-ADL</cell><cell></cell><cell cols="3">Win-Fail → Baby Win-Fail</cell><cell></cell><cell cols="2">Win Fail → Animal Win-Fail</cell></row><row><cell></cell><cell>R@1</cell><cell>R@5</cell><cell cols="2">Sim ∆ R@1</cell><cell cols="3">R@5 Sim ∆ R@1</cell><cell>R@5</cell><cell>Sim ∆</cell></row><row><cell>AR</cell><cell>0.015</cell><cell>0.073</cell><cell>0.11</cell><cell>0.016</cell><cell>0.094</cell><cell>0.06</cell><cell>0.042</cell><cell>0.193</cell><cell>0.05</cell></row><row><cell>WFR</cell><cell cols="2">0.017 (↑13%) 0.088 (↑20%)</cell><cell>0.29</cell><cell cols="2">0.018 (↑13%) 0.094</cell><cell>0.13</cell><cell cols="2">0.057 (↑36%) 0.267 (↑38%)</cell><cell>0.44</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 :</head><label>9</label><figDesc>Video retrieval results. Higher is better. AR -action recognition model; WFR -win-fail recognition model. Qualitative results. Odd and even rows show 'Fails' and 'Wins' as queries, respectively. First, second, and third two rows: ADL-Fall, Baby Win-Fail, Animal Win-Fail as databases. Red and green indicate relevant and irrelevant retrievals w.r.t. Win/Fail aspect.narrow spaces, etc. We consider baby wins anf fails as relevant to adult win and fail queries, respectively.</figDesc><table><row><cell>Query</cell><cell>WFR Model</cell><cell>AR Model</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In this paper, we alternately use terms: temporal modeling module, aggregation scheme, and consensus scheme.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Machine learning for visual concept recognition and ranking for images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Motoaki</forename><surname>Kawanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Towards the Internet of Services: The THESEUS Research Program</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="211" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Infants&apos; perception of goaldirected actions: development through cue-based bootstrapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Szilvia</forename><surname>Biro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">M</forename><surname>Leslie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Developmental science</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="379" to="398" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning from web videos for event classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Chesneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karteek</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="3019" to="3029" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The epic-kitchens dataset: Collection, challenges and baselines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hazel</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><forename type="middle">Maria</forename><surname>Farinella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonino</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Moltisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Munro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><surname>Perrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Who&apos;s better? who&apos;s best? pairwise deep ranking for skill determination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hazel</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walterio</forename><surname>Mayol-Cuevas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6057" to="6066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The pros and cons: Rank-aware temporal attention for skill determination in long videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hazel</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walterio</forename><surname>Mayol-Cuevas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7862" to="7871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The role of motor simulation in action perception: a neuropsychological case study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Eskenazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Grosjean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glyn</forename><forename type="middle">W</forename><surname>Humphreys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guenther</forename><surname>Knoblich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Research PRPF</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="477" to="485" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Infants predict other people&apos;s action goals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terje</forename><surname>Falck-Ytter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustaf</forename><surname>Gredebäck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claes</forename><surname>Von Hofsten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature neuroscience</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="878" to="879" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6202" to="6211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The information capacity of the human motor system in controlling the amplitude of movement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fitts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of experimental psychology</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">381</biblScope>
			<date type="published" when="1954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Cater: A diagnostic dataset for compositional actions and temporal reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.04744</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">The&quot; something something&quot; video database for learning and evaluating visual common sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanne</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heuna</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingo</forename><surname>Fruend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Mueller-Freitag</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fitts&apos;s law holds for action perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Grosjean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maggie</forename><surname>Shiffrar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Günther</forename><surname>Knoblich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="95" to="99" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanketh</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The effort heuristic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Kruger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derrick</forename><surname>Wirtz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leaf</forename><surname>Van Boven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>William Altermatt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Social Psychology</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="91" to="98" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hmdb: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hildegard</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hueihan</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Estíbaliz</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2556" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Human fall detection on embedded platform using depth maps and wireless accelerometer. Computer methods and programs in biomedicine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Kwolek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Kepski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="page" from="489" to="501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning effective skeletal representations on rgb video for fine-grained human action quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji-Xiang</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Chih</forename><surname>Hsiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Cheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">568</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">End-to-end learning for action quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujuan</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific Rim Conference on Multimedia</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="125" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Scoringnet: Learning key fragment for action quality assessment with ranking loss in skilled sports</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujuan</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="149" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Resound: Towards action recognition without representation bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="513" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Manipulation-skill assessment from videos with spatial attention network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjie</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoichi</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Tsm: Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7083" to="7093" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">2d/3d pose estimation and action recognition using multitask deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Diogo C Luvizon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedi</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tabia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5137" to="5146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The jester dataset: A large-scale video dataset of human gestures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingo</forename><surname>Bax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Action assessment by joint relation graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jibin</forename><surname>Jia-Hui Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6331" to="6340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Action quality assessment across multiple actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paritosh</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Morris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1468" to="1476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Measuring the quality of exercises</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paritosh</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><forename type="middle">Tran</forename><surname>Morris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2241" to="2244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">What and how well you performed? a multitask learning approach to action quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paritosh</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><forename type="middle">Tran</forename><surname>Morris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="304" to="313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paritosh</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaiden</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Morris</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.04884</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">Piano skills assessment. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning to score olympic events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paritosh</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><forename type="middle">Tran</forename><surname>Morris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="20" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala</editor>
		<imprint>
			<date type="published" when="2019-03" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">View-invariant pose analysis for human movement assessment from rgb data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faegheh</forename><surname>Sardari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adeline</forename><surname>Paiement</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Majid</forename><surname>Mirmehdi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Analysis and Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="237" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Pulling out the intentional structure of action: the relation between action processing and action production in infancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jessica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><forename type="middle">L</forename><surname>Sommerville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Woodward</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page" from="1" to="30" />
		</imprint>
	</monogr>
	<note type="report_type">Cognition</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Actor-centric relation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Uncertainty-aware score distribution learning for action quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zanlin</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9839" to="9848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Assessing action quality via attentive spatio-temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyin</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Chinese Conference on Pattern Recognition and Computer Vision (PRCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Towards a data-driven method for rgb video-based hand action quality assessment in real time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minhao</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingying</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mian</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th Annual ACM Symposium on Applied Computing</title>
		<meeting>the 35th Annual ACM Symposium on Applied Computing</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2117" to="2120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Mirroring and the development of action understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Amanda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><forename type="middle">A</forename><surname>Woodward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophical Transactions of the Royal Society B: Biological Sciences</title>
		<imprint>
			<biblScope unit="volume">369</biblScope>
			<biblScope unit="page">20130181</biblScope>
			<date type="published" when="1644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Twelvemonth-old infants interpret action in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Amanda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><forename type="middle">A</forename><surname>Woodward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sommerville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="73" to="77" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Long-term feature banks for detailed video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="284" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">S3d: Stacking segmental p3d for action quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gregory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trac D</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">25th IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="928" to="932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning to score figure skating sport videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengming</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zitian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe Yue-Hei</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4694" to="4702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Hybrid dynamic-static context-aware attention network for action assessment in long videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling-An</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fa-Ting</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi-Zhi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao-Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Huang</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2526" to="2534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">From actemes to action: A strongly-supervised representation for detailed action understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos G</forename><surname>Derpanis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2248" to="2255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Temporal relational reasoning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="803" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning deep features for scene recognition using places database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="487" to="495" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
