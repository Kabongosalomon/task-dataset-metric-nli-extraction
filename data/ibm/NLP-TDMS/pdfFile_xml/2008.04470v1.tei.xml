<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PoCoNet: Better Speech Enhancement with Frequency-Positional Embeddings, Semi-Supervised Conversational Data, and Biased Loss</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umut</forename><surname>Isik</surname></persName>
							<email>umutisik@amazon.com</email>
							<affiliation key="aff0">
								<orgName type="department">Amazon Web Services</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ritwik</forename><surname>Giri</surname></persName>
							<email>ritwikg@amazon.com</email>
							<affiliation key="aff0">
								<orgName type="department">Amazon Web Services</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neerad</forename><surname>Phansalkar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Amazon Web Services</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Marc</forename><surname>Valin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Amazon Web Services</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karim</forename><surname>Helwani</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Amazon Web Services</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvindh</forename><surname>Krishnaswamy</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Amazon Web Services</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">PoCoNet: Better Speech Enhancement with Frequency-Positional Embeddings, Semi-Supervised Conversational Data, and Biased Loss</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Neural network applications generally benefit from larger-sized models, but for current speech enhancement models, larger scale networks often suffer from decreased robustness to the variety of real-world use cases beyond what is encountered in training data. We introduce several innovations that lead to better large neural networks for speech enhancement. The novel PoCoNet architecture is a convolutional neural network that, with the use of frequency-positional embeddings, is able to more efficiently build frequency-dependent features in the early layers. A semi-supervised method helps increase the amount of conversational training data by pre-enhancing noisy datasets, improving performance on real recordings. A new loss function biased towards preserving speech quality helps the optimization better match human perceptual opinions on speech quality. Ablation experiments and objective and human opinion metrics show the benefits of the proposed improvements.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Neural network based approaches have greatly improved the output quality of speech enhancement systems <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">3,</ref><ref type="bibr" target="#b4">4,</ref><ref type="bibr" target="#b5">5]</ref>. These networks are trained, typically, in a supervised setting, with synthetic mixtures of clean speech and known noise clips, sometimes with synthetic reverberation added. Usually, the model is used to estimate a magnitude gain for each bin in the time-frequency domain representation of the noisy and/or reverberant mixture signal. Recent phase-aware models use a complex ratio mask instead of magnitude gain <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b7">7]</ref>, while other approaches work directly in the waveform domain <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b10">10]</ref>.</p><p>The speech enhancement problem has multiple challenges associated with it. First, the model needs to be robust to the multitude of different speech, recording, and noise conditions present in real-world usage. Second, clean speech data for training is limited in the public domain, with the biggest datasets coming from read material. Third, the task becomes increasingly difficult in low signal-to-noise ratio (SNR) cases, which can be helped by training larger models, which in turn makes the model more prone to fitting to the biases of the available dataset, decreasing robustness to other real-world conditions, making both of the first two challenges more pronounced. And fourth, the mismatch between human perception of sound quality and standard loss functions and metrics <ref type="bibr" target="#b11">[11]</ref> can make welloptimized models perform worse in human evaluation.</p><p>We propose several architectural, data preparation, augmentation and loss-function innovations that help meet the above stated challenges for large neural networks for speech enhancement.</p><p>On convolutional architectures, standard implementations in the time-frequency domain rely on 1D or 2D convnets. In the typical 1D architecture (e.g. ConvTASNet <ref type="bibr" target="#b12">[12]</ref>), the kernels move in the time-direction, and are fully connected in the frequency direction. These tend to have very large weight matrices in the early layers, where the architecture could benefit from a more hierarchical development of features. On the other hand, in standard 2D U-Net models where kernels move in both the time and frequency directions <ref type="bibr" target="#b14">[13]</ref>, early layer activations are blind to what frequency they operate in -even in the case when padding is used, these early features' receptive fields have not yet reached the edges of the time-frequency image. Our proposed architecture has the advantages of both options, it is a 2D U-Net (with DenseNet blocks and self-attention) with small kernels, and can therefore develop features hierarchically, but can also take into account frequency information in early layers with the inclusion of frequency-positional embeddings.</p><p>On the data front, we scale up the amount of clean conversational data available for training by using a semi-supervised approach. The clean portion of the LibriSpeech dataset, our starting point, contains data only from audio books, which is not conversational. The larger VoxCeleb dataset <ref type="bibr" target="#b15">[14]</ref>, on the other hand, is from television broadcasts, and contains background music and effects, some of the data is also highly reverberant. We use LibriSpeech-trained speech enhancement models to isolate the clean speech in VoxCeleb2 and eliminate reverberant clips, and show that adding this processed clean speech dataset to the training data improves the robustness of the model to conditions not well-represented in LibriSpeech. To make the most effective use of the data, we also use an extensive data augmentation stack that also helps address specific failure modes.</p><p>We also apply synthetic reverberation in the dataset using a library of recorded and synthetically generated room impulse responses. We train separate models to target the task with and without partial dereverberation. For non-dereverberating models, reverberation is added during training to the clean speech data as an augmentation before mixing. For training partially dereverberating models, we add, to the clean speech labels, a faster decaying version of the reverberation as was done in <ref type="bibr" target="#b16">[15]</ref>.</p><p>We use L1 losses across the board to help deal with dataset noise. We use a linear combination of two losses. The first is a new L1-loss on magnitudes which is biased to penalize underestimation of speech time-frequency bin magnitudes, as well as weighted towards high-frequencies, which makes the output of the trained model better preserve speech quality and avoid muffling. The second, is an L1 loss in the audio waveform domain, which is backpropagated though the STFT layer and complex multiplication to the estimated complex ratio mask values in the time-frequency domain.</p><p>To measure the performance of our model, we rely on Mean Opinion Score (MOS) subjective testing crowd-sourced on Amazon Mechanical Turk, using model outputs on the Deep Noise Suppression (DNS) challenge <ref type="bibr" target="#b17">[16]</ref> pre-competition test set, as well as on standard numerical metrics on the synthetic portion of the same test set. An ablation study shows the added improvement to human MOS and numerical metrics from each proposed component discussed above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Method</head><p>Let s be the clean speech audio signal and x = s * h + n be the same signal with added noise n and reverberated version s * h, which is convolved with a room impulse response h, and let y be the denoised and/or dereverberated target signal. The neural model N takes as input the STFT of the reverberant and noisy example s * h + n and estimates the complex ratio mask that would give the target signal estimate as:</p><formula xml:id="formula_0">y = ISTFT(N (STFT(x)) · ST F T (x)).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Architecture</head><p>For the neural model N , we start with a fully-convolutional 2D U-Net architecture with self-attention layers and 4-layer DenseNet blocks at each level, similar to <ref type="bibr" target="#b18">[17]</ref>. We take the convolutions to be causal in the time direction, but not in the frequency direction, meaning that padding is applied symmetrically in the frequency direction as is usual in 2D convnets, but applied asymmetrically in the time direction in the sense that it is only used at the edge of each layer corresponding to the early part in time. This helps preserve the output quality at the lateportion of the output which is used in low-latency application as padding tends to hurt quality near edges and borders. Note that look-ahead is provided by the average-pooling layers, which are used instead of max-pooling. <ref type="figure">Figure 1</ref> shows the overall architecture, while <ref type="figure">Figure 2</ref> shows details of the DenseNet and attention blocks.</p><p>The self-attention blocks we use are the same as the ones used in <ref type="bibr" target="#b19">[18,</ref><ref type="bibr" target="#b20">19]</ref> with the exception that the mechanism aggregates information only in the time direction to increase efficiency during training and inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1.">Frequency-Positional Embeddings</head><p>For early convolutional layers to be able to do frequency-aware processing, we concatenate a vector of positional embeddings to each time-frequency bin at the input layer of the model. The frequency-positional embedding vector for time-frequency bin centered at (t, f ) depends only on f and is defined as:</p><formula xml:id="formula_1">ρ(t, f ) = (cos(π f F ), cos(2π f F ), . . . , cos(2 k−1 π f F )),</formula><p>where F is the frequency bandwidth and k = 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Datasets</head><p>For the clean signal s, we combine data from two sources. First, we take two subsets of the publicly available LibriVox dataset, totaling approximately 600 hours of speech data: the LibriSpeech-clean 1 , <ref type="bibr" target="#b21">[20]</ref>, as well as the subset of the LibriVox dataset filtered based on Mean Opinion Scores to form the DNS Challenge dataset <ref type="bibr" target="#b22">[21]</ref>. The second source is VoxCeleb2 from which we use approximately 800 hours of data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1.">Semi-Supervised Learning with the VoxCeleb2 Dataset</head><p>To be able to use this large and varied dataset, we first train two models on the LibriSpeech dataset described above. The first model is a speech enhancement model that also does full dereverberation that is trained to estimate the reverb-only portion h * s − s, along with the clean signal s and noise n. This model uses the same architecture as our proposed network, but we use fewer filters, and early stopping to avoid overfitting. We use this model to estimate the direct-to-reverberant ratio (DRR) of each clip in VoxCeleb2 and filter out clips with DRR less than 30 dB. While this model is better at estimating DRR compared to more traditional methods, its clean signal estimates contain artifacts and are not suitable for training. Instead, we use a second model with the same architecture, trained to estimate h * s and n only. We use this denoise-only model to filter out all clips with signal-to-noise ratio (SNR) less than 10 dB, and use its clean speech estimates as training data for subsequent experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.">Noise data and over-sampling nonstationary noise</head><p>For noise data, we filter the AudioSet dataset, selecting clips with tags from the AudioSet ontology that are sounds that a speech enhancement system would be expected to remove, while excluding any clips with tags related to sounds that humans make.</p><p>We found that, even though most AudioSet tags correspond to non-stationary noise categories, a random 1-second chunk we use in training will more often than not have no non-stationary noise. We compute, for each chunk, the energy levels in 50 ms windows, and upsample, during training, chunks that have a standard-deviation of windowed energy of at least 3 dB. This increase the prevalence of non-stationary noise during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Augmentations</head><p>We use the following augmentation stack. Unless specified otherwise, distributions are uniform in the given number ranges.</p><p>• EQ. Random high and low-shelf EQ filters. With center frequency chosen uniformly in logarithmic domain between 40 and 8000 Hz, gain between ±10 dB. Two random EQ bell-curves per datapoint, symmetric in log domain, with Q-value between 0.5 and 1.5; frequency chosen from the same interval as shelf EQ. Randomized and applied to both speech and noise separately.</p><p>• Pitch shifts. Random resampling with ±10% of the original sample rate.</p><p>• Clipping. Random clipping between 0.5 and 1 of the peak value of the signal, applied 10% of the time.</p><p>• Empty buffer simulation. Random deletion of the first 0.5 to 1 of the input signal to simulate partially filled buffer in low-latency evaluation.</p><p>• Level and Silence. We skip datapoints with foreground RMS less than -38 dBFS (dB relative to full-scale of 1.0) and normalize each signal to have RMS value of -20 dBFS. We then apply a random volume down between -30 and 0 dB to the background, normalize the mix to -20 dBFS RMS, then apply a random amplification between -25 to 5 dB to everything. We additionally use silence as the foreground 3% of the time.</p><p>• Band-limiting. To make the model robust to cases where the input signal is band-limited, we apply a low pass filter at a frequency between 4 and 7 kHz, 2.5% of  the time to background only, 2.5% of the time to foreground, and 5% of the time to both.</p><p>• Reverberation. Used both as an augmentation and for datapoint creation as described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1.">Reverberation</head><p>When adding reverberation, we first identify, in each Room Impulse Response (RIR), the portion corresponding to the direct path, i.e. the 'first tap', and scale and shift the RIR so that the first tap is at t = 0 and it has height 1. So we have x = s * (h0 + h&gt;0) + n where h0 is a single tap at time zero. We then apply a gain to all taps except the first tap by a value between -25 and 0 dB. Also, 60% of the time, we add reverberation via the same impulse response to the noise signal as well, except that there is a separate downward scaling of the non-first tap. Hence, the model input becomes x = s * (h0 + αh&gt;0) + (n * (h0 + βh&gt;0)).</p><p>We use both real-recorded and synthetic room impulse responses (RIRs). For real impulse responses, we use the Aachen Impulse Response dataset <ref type="bibr" target="#b23">[22]</ref> consisting of of 214 RIR recordings. For synthetic RIRs, we generate a library of 10,000 RIRs, using the image method <ref type="bibr" target="#b24">[23]</ref>, with random rectangular rooms with sizes from 2 to 10 meters with random reflection coefficients between 0.5 and 1.5.</p><p>We restrict to using impulse responses with RT60 &lt; 0.8 s. We further augment all impulse responses with random resampling, which simulates changing room sizes with the same materials, and random exponential decays, which approximate changing uniform absorption levels of the room material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2.">Partial or No Dereverberation</head><p>We experiment with no-dereverberation models, where, during training, reverberation is used simply as an augmentation, and the foreground speech label is y = s * h; and with partialdereverberation, where the label's room impulse response has the first 20ms unaltered, and then made to decay quickly, to make RT60 &lt; 0.2 s, by multiplying with an exponential decay function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Loss functions</head><p>We train the neural model by optimizing, for each target y, the loss function</p><formula xml:id="formula_2">L(y, y) = λaudioLaudio(y, y) + λspectralLspectral(Y, Y ),</formula><p>where the audio loss is the L1 loss, Laudio(y, y) = |y − y|.</p><p>For the spectral loss function Lspectral, let Y t,f = |STFT(y) t,f | and Y t,f = |STFT( y) t,f |, be the STFT bin magnitudes; we set</p><formula xml:id="formula_3">Lspectral = t,f w(f ) λover1 Y ≥Y,t,f + λunder1 Y &lt;Y,t,f |Y t,f − Y t,f |.</formula><p>Here, w is a frequency-weighting function, and 1 Y ≥Y,t,f is the characteristic function with value 1 if Y t,f ≥ Y t,f and value 0 otherwise. The variables λover and λunder bias the model for overestimation or underestimation of the speech magnitude.     For low-latency evaluation, we use 40 ms-sized input frames (i.e. 640 samples at 16kHz) with one-frame look-ahead. For each input chunk of samples, we run the model on the last 16384 samples in the input buffer. We use cross-over to eliminate artifacts from the frames. <ref type="figure" target="#fig_2">Figure 3</ref> illustrates the evaluation mechanism for two input chunks. Inference takes 0.65 seconds per second of audio on a V100 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Inference-Time</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments and Results</head><p>For each model, we use 6 down-blocks in the U-Net with the number of per-layer filters in each being 32, 64, 128, 256, 256, 256, and up-blocks symmetric to the down-blocks. There are a total of ∼50M parameters. We multiply the foreground and background losses with weighting coefficients λfg = 2.0, and λbg = 0.4 for the foreground and background estimation respectively. We take λaudio = 1, and λspectral = 1.5, with (only for the foreground signal) λover = 2.6 and λunder = 13.3. We have not used any hyper-parameter tuning techniques, with most parameters, especially those used for augmentations, set as sensible defaults and unmodified. We train each model for 700,000 iterations with total minibatch size of 112, using the ADAM optimizer, with learning rate of 1e-4, halved every 100,000 iterations. Training each model takes about 4 days on 8 V100 GPUs. Our implementation uses the MXNet <ref type="bibr" target="#b26">[25]</ref> framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Evaluation: Subjective and Objective Metrics</head><p>For human opinion tests, we use the methodology of ITU-T P.808 Subjective Evaluation of Speech Quality with a Crowdsourcing Approach <ref type="bibr" target="#b27">[26]</ref>, on Amazon Mechanical Turk.The MOS scores are based on 10 listenings each of the model's outputs on the 600 real and synthetic inputs on the INTERSPEECH 2020 DNS Challenge test set <ref type="bibr" target="#b17">[16]</ref>, which covers varied cases.</p><p>For objective metrics, we evaluate wide-band Perceptual Evaluation of Speech Quality (PESQ) -ITU-T P.862.2 - <ref type="bibr" target="#b28">[27]</ref>, and the composite CSIG, CBAK, and COVL scores proposed in <ref type="bibr" target="#b29">[28]</ref> on the 300 synthetic examples in the same test set. <ref type="table" target="#tab_1">Tables 1 and 2</ref> show, respectively, objective and subjective metric evaluation results. Note that, removing the semi-supervised conversational data has a strong effect on performance on real recordings which tend to have more varied speech styles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Results</head><p>The proposed model took 1st place in the 2020 Deep Noise Suppression Challenge's Non-Real-Time Track <ref type="bibr" target="#b17">[16]</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>We described new techniques that result in improvements to speech enhancement with large neural networks. The resulting PoCoNet speech enhancer is a large U-Net with DenseNet and self-attention blocks with frequency-positional embeddings, and is trained with a semi-supervised technique partly on conversational data, with an extensive augmentation stack including reverberation, and with a loss function that is biased to preserve speech. Evaluation results show the quality improvement on the overall system due to each component and demonstrate the effectiveness of the introduced techniques for training large neural speech enhancers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>ISTFT L audio (·, y) L spectral (·, |Y |)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :Figure 2 :</head><label>12</label><figDesc>Top two levels of the U-Net architecture shown with frequency-positional embeddings and STFT real and imaginary parts inputs; and real and imaginary parts of complex ratio mask outputs. We use a 6-level U-Net architecture. Details of the DenseNet and attention block. Straight arrows are convolutions with batch normalization and ReLU non-linearity, curved arrows are concatenations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The Inference-time mechanism. The convnet is evaluated always on the last second of the input buffer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Full</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Mean Opinion Score evaluation of different Algorithms over the DNS Challenge non-blind test sets.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Synthetic no Reverb Synthetic with Reverb Real Recordings</cell></row><row><cell>Proposed without dereverberation without positional embeddings without background reverbs without biased loss without semi-supervised data without reverb augmentations RNNoise [24] DNS Challenge Baseline [21] Noisy 95% confidence interval</cell><cell>3.833 3.807 3.789 3.768 3.766 3.755 3.467 3.464 3.439 3.432 ±0.04</cell><cell>4.160 4.128 4.092 4.119 4.094 4.131 4.133 3.660 3.703 3.568 ±0.06</cell><cell>3.613 3.497 3.548 3.573 3.519 3.620 2.358 3.162 3.120 3.183 ±0.08</cell><cell>3.779 3.802 3.758 3.690 3.726 3.634 3.688 3.517 3.466 3.489 ±0.05</cell></row><row><cell></cell><cell cols="2">Synthetic without Reverb</cell><cell cols="2">Synthetic with Reverb</cell></row><row><cell>Methods RNNoise [24] DNS Challenge Baseline [21] Noisy Proposed without reverb augmentation without semi-supervised data without positional embeddings without partial dereverberation without background reverbs without biased loss</cell><cell cols="2">PESQ CBAK COVL CSIG 1.973 3.463 2.789 2.692 1.811 2.003 2.235 2.781 1.582 2.533 2.350 3.186 2.745 3.040 3.422 4.080 2.748 3.043 3.366 3.965 2.722 3.022 3.343 3.94 2.721 3.012 3.358 3.982 2.707 3.015 3.286 3.851 2.667 2.997 3.296 3.909 2.457 2.904 3.106 3.749</cell><cell cols="2">PESQ CSIG CBAK COVL 1.777 3.407 2.709 2.569 1.515 1.937 1.949 2.515 1.821 2.801 2.635 3.499 1.609 2.303 2.223 2.906 1.293 1.935 1.582 2.017 1.612 2.302 2.214 2.887 1.638 2.312 2.188 2.807 2.832 3.209 3.349 3.834 1.613 2.301 2.223 2.906 1.545 2.258 2.070 2.671</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Objective evaluation of different Algorithms over the DNS Challenge synthetic non-blind test sets. Note that the Synthetic with Reverb test reference clean labels contain reverberation, results the model that is trained to keep all reverberation has the best performance on this set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Mean Opinion Score evaluation provided by the DNS Challenge based on the blind test set. Full results are available in<ref type="bibr" target="#b17">[16]</ref> (team #9).</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">'clean' clips in LibriSpeech were selected based on WER in ASR experiments, not directly by audio quality</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Speech enhancement with weighted denoising auto-encoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingyin</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changchun</forename><surname>Bao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3444" to="3448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A regression approach to speech enhancement based on deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Rong</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Hui</forename><surname>Lee</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="19" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Speech enhancement with lstm recurrent neural networks and its application to noise-robust asr</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Weninger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Björn</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Latent Variable Analysis and Signal Separation</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning spectral mapping for speech dereverberation and denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deliang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Woods</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Merks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="982" to="992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Attention wave-u-net for speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Giri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Isik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krishnaswamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)</title>
		<imprint>
			<biblScope unit="page" from="249" to="253" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Complex ratio masking for monaural speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM transactions on audio, speech, and language processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="483" to="492" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Phasebook and friends: Leveraging discrete representations for source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gordon</forename><surname>Wichern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Sarroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="370" to="382" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A wavenet for speech denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Rethage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Serra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5069" to="5073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Improved speech enhancement with the wave</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Macartney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tillman</forename><surname>Weyde</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.11307</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">u-net. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Francois G Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.10522</idno>
		<title level="m">Speech denoising with deep feature losses</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Evaluation of objective quality measures for speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Philipos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Loizou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on audio, speech, and language processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="229" to="238" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Conv-tasnet: Surpassing ideal time-frequency magnitude masking for speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Mesgarani</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">speech, and language processing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1256" to="1266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Se Rim Park and Jin Won Lee. A fully convolutional neural network for speech enhancement</title>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Voxceleb: a large-scale speaker identification dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon</forename><forename type="middle">Son</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.08612</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Late reverberation suppression using recurrent neural networks with long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5434" to="5438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Chandan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishak</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Gopal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ebrahim</forename><surname>Cutler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Beyrami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harishchandra</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergiy</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Matusevych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashkan</forename><surname>Aichner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Aazami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Braun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.13981</idno>
		<title level="m">The interspeech 2020 deep noise suppression challenge: Datasets, subjective testing framework, and challenge results</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Channel-attention dense u-net for multichannel speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tolooshams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Giri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Isik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krishnaswamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="836" to="840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08318</idno>
		<title level="m">Dimitris Metaxas, and Augustus Odena. Self-attention generative adversarial networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Librispeech: an asr corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassil</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoguo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">The interspeech 2020 deep noise suppression challenge: Datasets, subjective speech quality and testing framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Reddy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08662</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A binaural room impulse response database for the evaluation of dereverberation algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Jeub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Magnus</forename><surname>Schafer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Vary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th International Conference on Digital Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Image method for efficiently simulating small-room acoustics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David A</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Berkley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="943" to="950" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A hybrid DSP/deep learning approach to real-time full-band speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Valin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Multimedia Signal Processing (MMSP) Workshop</title>
		<meeting>IEEE Multimedia Signal Processing (MMSP) Workshop</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianjun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.01274</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Subjective evaluation of speech quality with a crowdsourcing approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Itu-T. Recommendation</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">808</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Perceptual evaluation of speech quality (PESQ): An objective method for end-to-end speech quality assessment of narrowband telephone networks and speech codecs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Itu-T</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Speech enhancement: theory and practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Philipos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Loizou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>CRC press</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
