<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pattern-Affinitive Propagation across Depth, Surface Normal and Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">PCA Lab</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Cui</surname></persName>
							<email>zhen.cui@njust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">PCA Lab</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">PCA Lab</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
							<email>yyan@njust.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
							<email>niculae.sebe@unitn.it</email>
							<affiliation key="aff0">
								<orgName type="laboratory">PCA Lab</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Multimedia and Human Understanding Group</orgName>
								<orgName type="institution">University of Trento</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">PCA Lab</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Pattern-Affinitive Propagation across Depth, Surface Normal and Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose a novel Pattern-Affinitive Propagation (PAP) framework to jointly predict depth, surface normal and semantic segmentation. The motivation behind it comes from the statistic observation that pattern-affinitive pairs recur much frequently across different tasks as well as within a task. Thus, we can conduct two types of propagations, cross-task propagation and task-specific propagation, to adaptively diffuse those similar patterns. The former integrates cross-task affinity patterns to adapt to each task therein through the calculation on non-local relationships. Next the latter performs an iterative diffusion in the feature space so that the cross-task affinity patterns can be widelyspread within the task. Accordingly, the learning of each task can be regularized and boosted by the complementary task-level affinities. Extensive experiments demonstrate the effectiveness and the superiority of our method on the joint three tasks. Meanwhile, we achieve the state-of-the-art or competitive results on the three related datasets, NYUD-v2, SUN-RGBD and KITTI.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The predictions of depth, surface normal and semantic segmentation are important and challenging for scene <ref type="bibr">*</ref>   The point pairs colored white are the matched affinity pixels across three tasks at the same positions, while the pairs of black points correspond to dissimilar pixels across three maps. For the similarity metrics, REL/RMSE/Label consistency are taken respectively for the three maps. (b) Statistical results. We compute the success ratio of pairs matching across different maps on NYUD-v2 and SUN-RGBD datasets, and observe that the success ratios of pairs matching cross tasks are rather high.</p><p>understanding. Also, they have many potential industrial applications such as autonomous driving system <ref type="bibr" target="#b3">[4]</ref>, simultaneous localization and mapping (SLAM) <ref type="bibr" target="#b51">[52]</ref> and socially interactive robotics <ref type="bibr" target="#b11">[12]</ref>. Currently, most methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b42">43]</ref> focused on one of the three tasks, and they also achieved the state-of-the-art performance through the technique of deep learning. In contrast to the single-task methods, recently, several joint-task learning methods <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b31">32]</ref> on these tasks have shown a promising direction to improve the predictions by utilizing task-correlative information to boost for each other. In a broad sense, the problem of joint-task learning has been widely studied in the past few decades <ref type="bibr" target="#b2">[3]</ref>. But more recently most approaches took the technique line of deep learning for possible different tasks <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref>. However, most methods aimed to perform feature fu-sion or parameter sharing for task interaction. The fusion or sharing ways may utilize the correlative information between tasks, but there exist some drawbacks. For examples, the integration of different features might result into the ambiguity of information; the fusion does not explicitly model the task-level interaction where we do not know what information are transmitted. Conversely, could we find some explicitly common patterns across different tasks for the jointtask learning?</p><p>We take the three relative tasks: depth estimation, surface normal prediction and semantic segmentation, and then conduct a statistical analysis on those second-order patterns across different tasks on NYUD-v2 <ref type="bibr" target="#b48">[49]</ref> and SUN-RGBD <ref type="bibr" target="#b50">[51]</ref> dataset. First, we define the metric of any two pixels in the predicted images. The average relative error (REL) is used for depth images, the root mean square error (RMSE) is used for surface normal images, and the label consistency is for segmentation images. A pair of pixels have an affinity (or similar) relationship when their error is less than a specified threshold, otherwise they have a dissimilar relationship. Next, we accumulate the matching number of those similar pairs (or dissimilar pairs) with the same space positions across the three types of corresponding images. As shown in <ref type="figure" target="#fig_0">Fig. 1(a)</ref>, the affinity pairs (colored white points) at the common positions may exist in different tasks. Meantime, there exist some common dissimilar pairs (colored black points) across tasks. The statistical results are shown in <ref type="figure" target="#fig_0">Fig. 1(b)</ref>, where REL threshold of depth is set to 20%, and RMSE threshold of surface normal is set to 26% according to the performances of some state-of-theart works <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b28">29]</ref>. We can observe that the success ratios of matching pairs across two tasks are rather high, and around 50% -60% similar pairs are matched. Moreover, we have the same observation on the matching dissimilar pairs, where REL threshold of depth is set to 20%, and RMSR threshold of surface normal is set to 40%. Anyhow, this observation of the second-order affinities is great important to bridge two tasks.</p><p>Just motivated by the statistical observation, in this paper we propose a Pattern-Affinitive Propagation (PAP) framework to utilize the cross-task affinity patterns to jointly estimate depth, surface normal and semantic segmentation. In order to encode long-distance correlations, the PAP utilizes non-local similarities within each task, different from the literatures <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b4">5]</ref> only considering local neighbor relationships. These pair-wise similarities are formulated as an affinity matrix to encode the pattern relationships of the task. To spread the affinity relationships, we take two propagation stages, cross-task propagation and task-specific propagation. The affinity relationships across tasks are first aggregated and optimized to adapt to each specific task by calculating on three affinity matrices. We then conduct an iterative task-specific diffusion on each task by leveraging the optimized affinity information from the corresponding other two tasks. The diffusion process is performed in the feature space so that the affinity information of other tasks can be widely spread into the current task. Finally, the learning of affinitive patterns and the two-stage propagations are encapsuled into an end-to-end network to boost the prediction process of each task.</p><p>In summary, our contributions are in three aspects: i) Motivated by an observation that pattern-affinitive pairs recur much frequently across different tasks, we propose a novel Pattern-affinitive Propagation (PAP) method to utilize the matched non-local affinity information across tasks. ii) Two-stage affinity propagations are designed to perform cross-task and task-specific learning. An adaptive ensemble network module is designed for the former while the strategy of graph diffusion is used for the latter. iii) We make extensive experiments to validate the effectiveness of PAP method and its modules therein, and achieve the competitive or superior performances on depth estimation, surface normal prediction and semantic segmentation on NYUD-v2 <ref type="bibr" target="#b48">[49]</ref>, SUN-RGBD <ref type="bibr" target="#b50">[51]</ref>, and KITTI <ref type="bibr" target="#b52">[53]</ref> datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Depth Estimation: Many works have been proposed for monocular depth estimation <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b61">62]</ref>. Recently, Xu et al. <ref type="bibr" target="#b58">[59]</ref> employed multi-scale continuous CRFs as a deep sequential network for depth prediction. Fu et al. <ref type="bibr" target="#b14">[15]</ref> tried to consider the ordinal information in depth maps and designed a ordinal regression loss function.</p><p>RGBD Semantic Segmentation: As the large RGBD dataset was released, some approaches <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b33">34]</ref> attempted to fuse depth information for better segmentation. Recently, Qi et al. <ref type="bibr" target="#b44">[45]</ref> designed a 3D graph neural network to fuse the depth information for segmentation. Cheng et al. <ref type="bibr" target="#b5">[6]</ref> computed the important locations from RGB images and depth maps for upsampling and pooling.</p><p>Surface Normal Estimation: Recent methods designed for surface normal estimation are mainly based on deep neural networks <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b54">55]</ref>. Wang et al. <ref type="bibr" target="#b55">[56]</ref> designed a network to incorporate local, global and vanishing point information for surface normal prediction. In work of <ref type="bibr" target="#b0">[1]</ref>, a skip-connected architecture was proposed to fuse features from different layers for surface normal estimation. 3D geometric information was also utilized in <ref type="bibr" target="#b45">[46]</ref> to predict depth and normal maps.</p><p>Affinity Learning: Many affinity learning methods were designed based on physical nature of the problems <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30]</ref>. Liu et al. <ref type="bibr" target="#b37">[38]</ref> improve the modeling of pairwise relationships by incorporating many priors into diffusion process. Recently, work of <ref type="bibr" target="#b1">[2]</ref> proposed an convolutional random walk approach to learn the image affinity by supervision. Wang et al. <ref type="bibr" target="#b56">[57]</ref> proposed a non-local neu- </p><formula xml:id="formula_0">----------------------Shared Encoder----------------------||---Task-specific Net---||-Initial Results-||-Cross-task propagation--| Task-specific propagation |------Reconstruction Net------||------Final Results------| Depth Surface Normal</formula><p>Semantic segmentation |-RGB Image-||  <ref type="figure">Figure 2</ref>. The overview of our Pattern-Affinitive Propagation network for jointly predicting depth, surface normal and semantic segmentation. The initial predictions are produced from each task-specific network. During cross-task propagation, the network firstly learns an affinity matrix by affinity learning layer to represent the pair-wise relationships of each task, then adaptively combines these matrices to propagate the cross-task affinitive patterns. Note that, the combined affinity matrices is different for each task. Then we use the combined matrix to conduct task-specific propagation by a diffusion layer, propagating the affinitive patterns back to the features for each task. Finally the diffused features are applied to three reconstruction networks to produce the final results with higher resolution.</p><p>ral network to mine the relationships with long distances. Some other works <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b22">23]</ref> tried to learn local pixelwise affinity for semantic segmentation or depth completion. Our method is different from these approaches in the following aspects: needs no prior knowledge and is datadriven; needs no task-specific supervisons; learns the nonlocal affinity rather than limited local pair-wise relationships; learns the cross-task affinity information rather than learning the single-task affinity for task-level interaction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Non-Local Affinities</head><p>Our aim is to model the affinitive patterns among tasks, and utilize such complementary information to boost and regularize the prediction process of each task. According to our analysis aforementioned, we want to learn the pair-wise similarities and then propagate the affinity information into each task. Instead of learning local affinities as literature <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b4">5]</ref>, we attempt to utilize non-local affinities, which also recur frequently as illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>. Formally, suppose x i , x j are the feature vectors of the i-th and j-th positions, we can define their similarity s(x i , x j ) through some functions such as L1 distance x i − x j , inner product x T i x j , and so on. We employ the exponential function (e s(·,·) or e −s(·,·) ) to make the similarities non-negative and larger for those similar pairs than dissimilar pairs. To reduce the influence of scale, we normalize the similarity matrix M into M ij / k M ik , where M is the matrix of pair-wise similarities across all pixel positions. In these ways, the matrix M is symmetric, has non-negative elements and finite Frobenius norm. Accordingly, for the three tasks, we can compute their similarity matrices M depth , M seg , M normal respectively. According to the above statistic analysis, we can propagate the affinities by integrating the three similarity matrices for one specific task, which will be introduced in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Pattern-Affinitive Propagation</head><p>In this section, we introduce the proposed Pattern-Affinitive Propagation (PAP) method. We efficiently implement the PAP method into a deep neural network through designing a series of network modules. The details are introduced in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">The Network Architecture</head><p>We implement the proposed method into a deep network as shown in <ref type="figure">Fig. 2</ref>, which depicts the network architecture. The RGB image is firstly fed into a shared encoder (e.g., ResNet <ref type="bibr" target="#b19">[20]</ref>) to generate hierarchical features. Then we upsample the features of the last convolutional layer and feed them to three task-specific networks. Note that we also integrate multi-scale features derived from different layers of encoder with each task-specific network, as shown by the gray dots. Each task-specific network has two residual blocks, and produces the initial prediction after a convolutional layer. Then we conduct cross-task propagations to learn the task-level affinitive patterns. Each task-specific network firstly learns an affinity matrix by the affinity learning layer to capture the pair-wise similarities for each task, and secondly adaptively combine the matrix with other two affinity matrices to integrate the task-correlative information. Note that, the adaptively combined matrix is different for each task. After that, we conduct task-specific propagation via a diffusion layer to spread the learned affinitive patterns back to the feature space. In each diffusion process, we diffuse both initial prediction and the last features from each task-specific network by the combined affinity matrix.  Finally, the diffused features of each task are fed into a reconstruction network to produce final prediction with higher resolution. We firstly use a shared and a task-specific upsampling block to upscale the feature maps. Each upsampling block is built as a up-projection block <ref type="bibr" target="#b28">[29]</ref>, and parameters in the shared upsampling block are shared for every task to capture correlative local details. After the upsampling with the two blocks, the features are concatenated and fed into a residual block to produce final predictions. The scale factor of each upsampling block is set to 2, and the final predictions are half of the input scale. This means that the number of upsampling blocks depends on the scale on which we want to learn affinity matrix. In experiments, we learn affinity matrices on 1/16, 1/8 and 1/4 input scale, which means there are 3, 2 and 1 upsampling stages in the reconstruction network respectively. The whole network can be trained in an end-to-end manner, and the details of the cross-task and task-specific propagations will be introduced in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Cross-Task Propagation</head><p>In this section we elaborate how to conduct cross-task propagation. Firstly, we learn an affinity matrix by affinity learning layer to represent the pair-wise similarities for each task. The detailed architecture of the affinity learning layer can be observed in <ref type="figure" target="#fig_3">Fig. 3(a)</ref>. Assuming the feature generated by the last layer of each task-specific network is F ∈ R H×W ×2C , we firstly shrink it using a 1 × 1 convolutional layer to get the featureF ∈ R H×W ×C . ThenF is reshaped to X ∈ R HW ×C . We utilize matrix multiplication to compute pair-wise similarities of inner product, and obtain the affinity matrix M = XX ∈ R HW ×HW .</p><p>Other pair-wise functions such as e − Xi−Xj can also be used, just not shown in the figure. Note that, different from non-local blocks <ref type="bibr" target="#b56">[57]</ref>, our affinity matrix must satisfy the symmetric and nonnegative properties to represent the pairwise similarities. Finally, as each row of the matrix M represents the pair-wise relationships between one position and all other positions, we conduct normalization along each row of M to reduce the influence of scale. In this way, the task-level patterns can be represented in each M. Note that we add no supervision to learn M as literature <ref type="bibr" target="#b1">[2]</ref>, because such supervision will cost extra memories and be not easy to define for some tasks. After that, we want to integrate the cross-task information for each task. Denote these three tasks as T 1 , T 2 , T 3 , and the corresponding affinity matrices as M T1 M T2 M T3 , then we can learn weights α Ti k (k = 1, 2, 3, n k=1 α Ti k = 1) to adaptively combine the matrices as:</p><formula xml:id="formula_1">M Ti = α Ti 1 · M T1 + α Ti 2 · M T2 + α Ti 3 · M T3 . (1)</formula><p>In this way, the cross-task affinitive patterns can be propagated intoM Ti . In practice, we implement affinity learning layers at decoding process on 1/16, 1/8 and 1/4 input scale respectively, hence it actually learns non-local patch-level relationships.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Task-Specific Propagation</head><p>After obtaining the combined affinity matrices, we spread such affinitive patterns into the feature space of each task by the task-spacific propagation. Different from nonlocal block <ref type="bibr" target="#b56">[57]</ref> and local spatial propagation <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b4">5]</ref>, we perform an iterative non-local diffusion process in each diffusion layer to capture long-distance similarities, as illustrated in <ref type="figure" target="#fig_3">Fig. 3(b)</ref>. The diffusion process is performed on initial prediction as well as features from task-specific network. Without loss of generality, assuming feature or initial prediction P ∈ R H×W ×C is from task-specific network, we firstly reshape it to h ∈ R HW ×C , and perform one step diffusion by using matrix multiplication withM. In this way, the feature vector of each position is obtained by weighted accumulating feature vectors of all positions using the learned affinity. Note that such one-step diffusion may not deeply and effectively propagate the affinity information to the feature space, we perform the multi-step iterative diffusion as:</p><formula xml:id="formula_2">h t+1 =Mh t , t ≥ 0,<label>(2)</label></formula><p>where h t means the diffused feature (or prediction) at step t. Such diffusion process can be also expressed with a partial differential equation (PDE):</p><formula xml:id="formula_3">h t+1 =Mh t = (I − L)h t , h t+1 − h t = − Lh t , ∂ t h t+1 = − Lh t ,<label>(3)</label></formula><p>where L is the Laplacian matrix. AsM is normalized and has finite Frobenius norm, the stability of such PDE can be guaranteed <ref type="bibr" target="#b38">[39]</ref>. Assuming we totally perform t * steps in each diffusion layer, in order to prevent the feature deviating too much from the initial one, we use the weighted accumulation on the initial feature (or prediction) h 0 as:</p><formula xml:id="formula_4">h out = βh t * + (1 − β)h 0 , 0 ≤ β ≤ 1,<label>(4)</label></formula><p>where h out means the final output from a diffusion layer. In this way, the learned affinitive patterns in eachM Ti can be effectively propagated into each task T i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">The Loss Function</head><p>In this section we introduce a pair-wise affinity loss for our PAP network. As PAP method is designed to learn task-correlative pair-wise similarities, we also hope our loss function can enhance the pair-wise constraints. Firstly we define the prediction at position i isẑ i , and the corresponding ground truth is z i . Then we define the pair-wise distance in prediction and corresponding ground truth aŝ d ij = |ẑ i −ẑ j | and d ij = |z i − z j |. We hope the distance in prediction to be similar to ground truth, so the pair-wise loss can be defined as L pair-wise = ∀i,j |d ij − d ij |. As the calculation of the pair-wise loss in each task will have a high memory burden, so we randomly select S pairs from each task and then compute the pair-wise loss L pair-wise = S |d ij − d ij |. As the pairs are randomly selected, such pair-wise loss can capture similarities of various-distance pairs, not only the adjacent pixels in <ref type="bibr" target="#b9">[10]</ref>. Meanwhile, we also use berHu loss <ref type="bibr" target="#b28">[29]</ref>, L1 loss and cross-entropy loss for depth estimation, surface normal prediction and semantic segmentation respectively, which are denoted as L Ti (T i means the i-th task). Finally the total loss of the joint task learning problem can be defined as:</p><formula xml:id="formula_5">L = Ti λ Ti (L Ti + ξ Ti L Ti pair-wise ),<label>(5)</label></formula><p>where L Ti pair-wise is the pair-wise loss for the corresponding i-th task, and λ Ti and ξ Ti are two weights for the i-th task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NYUD-v2:</head><p>The NYUD v2 dataset <ref type="bibr" target="#b48">[49]</ref> consists of RGB-D images of 464 indoor scenes. There are 1449 images with semantic labels, 795 of them are used for training and the remaining 654 images for testing. We randomly select more images (12k, same as <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b61">62]</ref> ) from the raw data of official training scenes. These images have the corresponding depth maps but no semantic labels or surface normals. We follow the procedure in <ref type="bibr" target="#b12">[13]</ref> and <ref type="bibr" target="#b45">[46]</ref> to generate surface normal ground truth. In this way, we can use more data to train our model for jointly depth and surface normal prediction.</p><p>SUN RGBD: The SUN RGBD dataset <ref type="bibr" target="#b50">[51]</ref> contains 10355 RGBD images with semantic labels of which 5285 for training and 5050 for testing. We use the official training set with depth and semantic labels to train our network, and the official testing set for evaluation. There is no surface normal ground truth on this dataset, so we perform experiments on jointly predicting depth and segmentation on this dataset.</p><p>KITTI: KITTI online benchmark <ref type="bibr" target="#b52">[53]</ref> is a widely-used outdoor dataset for depth estimation. There are 4k images for training, 1k images for validating and 500 images for testing on the online benchmark. As it has no semantic labels or surface normal ground truth, we mainly transform such information using our PAP method to demonstrate that PAP can distilling knowledge to improve the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Implementation Details and Metrics</head><p>We implement the proposed model using Pytorch <ref type="bibr" target="#b43">[44]</ref> on a single Nvidia P40 GPU. We build our network based on ResNet-18 and ResNet-50, and each model is pre-trained on the ImageNet classification task <ref type="bibr" target="#b6">[7]</ref>. In diffusion process, we use a same subsampling strategy as <ref type="bibr" target="#b56">[57]</ref> to downsample h in Eqn. <ref type="bibr" target="#b1">(2)</ref>, which can reduce the amount of pairwise computation by 1/4. We set the trade-off parameter β to 0.05. 300 pairs are randomly selected to compute the pair-wise loss in each task. We simply set λ Ti = 1 3 and ξ Ti = 0.2 to balance the loss functions. Initial learning rate is set to 10 −4 for the pre-trained convolutional layers and 0.01 for the other layers. For NYUD-v2, we train the model of 795 training images for 200 epochs and fine-tune 100 epochs, and train the model of 12k training images for jointly depth/normal predicting for 30 epochs and fine-tune for 10 epochs. For SUN-RGBD dataset, we train the model for 30 epochs and fine-tune it for 30 epochs using a learning rate of 0.001. For KITTI, we first train the model on NYUD-v2 for surface normal estimation, and then freeze the surface normal branch to train depth branch on KITTI for 15 epochs, finally we freeze the normal branch and finetune the model on KITTI for 20 epochs.</p><p>Similar to the previous works <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b58">59]</ref>, we evaluate our depth prediction results with the root mean square error (rmse), average relative error (rel), root mean square error in log space (rmse-log), and accuracy with threshold (δ): % of x i s.t. max( xi xi , xi xi )=δ, δ = 1.25, 1.25 2 , 1.25 <ref type="bibr" target="#b2">3</ref> , where x i is the predicted depth value at the pixel i, n is the number of valid pixels and x i is the ground truth. The evaluation metrics for surface normal prediction <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b9">10]</ref> are mean of angle error (mean), medians of the angle error (median), root mean square error for normal (rmse-n %), and pixel accuracy as percentage of pixels with angle error below threshold η where η ∈ [11.25 • , 22.50 • , 30 • ]. For the evaluation of  <ref type="bibr" target="#b34">[35]</ref> and use the common metrics including pixel accuracy (pixel-acc), mean accuracy (mean-acc) and mean intersection over union (IoU).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Ablation Study</head><p>In this section we perform many experiments to analyse the influence of different settings in our method.</p><p>Effectiveness of joint task learning: We first analyse the benefit of joint predicting depth, surface normal and semantic segmentation using our PAP method. The networks are trained on NYUD v2 dataset, and we select ResNet-18 as our shared network backbone and only learn the affinity matrix on 1/8 input scale in each experiment. As illustrated in <ref type="table" target="#tab_2">Table 1</ref>, we can see that joint-task models gets superior performances than the single task model, and further jointly learning three tasks obtains best results. It can be revealed that our PAP method does boost each task in the jointly learning procedures.</p><p>Analysis on network settings: We perform many experiments to analyse the effectiveness of each network modules. In each experiment we use ResNet-18 as our network backbone for equally comparing, and each model is trained on NYUD v2 dataset for the three tasks. The result can be seen in <ref type="table">Table 2</ref>. Note that the results of first five rows are computed from the model with affinity matrix learned on 1/16 input scale. We can observe that PAP, reconstruction net and pair-wise loss can all contribute to improve the performance. We also compare two approaches  <ref type="table" target="#tab_2">Iteration   42   43   44   45   46   47   4  8  12  16  20   IoU   Iteration   26   27   28   29   30   4  8  12  16  20   norma l  rms e %   Iteration   100   120   140   160   180   4  8  12  16  20</ref> time ms Iteration <ref type="figure">Figure 4</ref>. The influence of the iterations in diffusion process. The performance and time burden changes can be seen as a trade-off.</p><p>in the same settings, i.e., cross-stich units <ref type="bibr" target="#b40">[41]</ref> and convolutional spatial propagation layers <ref type="bibr" target="#b4">[5]</ref> which can also fuse and interact cross-task information. We find that they obtain weaker performances. It may be attributed to that: a) cross-stich layer only combines features, but cannot represent the affinitive patterns between tasks; b) they only use limited local information. The middle three rows of the <ref type="table">Table 2</ref> show the influence on which scale the affinity matrix is learned. We can find that learning affinity matrix on a larger scale may be beneficial, as the larger affinity matrices can describe the similarities between more patches. Note that the improvements of learning matrix on 1/4 input scale are comparatively smaller, and the reason may be that learning good non-local pair-wise similarities becomes more difficult with scale increasing. Finally we show the results using different functions to calculate the similarities. We find that these two functions does produce different performances, but with little difference. Hence, we mainly use dot product as our weight function in the following experiments for convenience.</p><p>Influence of the iteration: Here we make experiments to analyse the influence of the iterative steps in Eqn. <ref type="bibr" target="#b1">(2)</ref>. The models are based on ResNet-18 and trained on NYUD v2 dataset, and the affinity matrices are learned on 1/8 input scale. While testing, the input size is 480×640. As illustrated in <ref type="figure">Fig. 4</ref>, we can see that the performances of all tasks are improved with more iterations, at least in such a range. These results demonstrate that the pair-wise constraints and regularization may be enhanced with more iterations in diffusion. But the testing time will also increase with more steps, which can be seen as a trade-off.</p><p>Visualization of the affinity matrices: We show several examples of the learned affinity maps in <ref type="figure">Fig. 5</ref>. Note that the affinity maps belong to the white point in each image. We can see that the single-task affinity maps often show improper pair-wise relationships, while the cross-task affinity maps in our PAP method have closer relationships with the points which have similar depth, normal direction and semantic label. As the affinity matrices is non-local and actually a dense graph, it can well represent the long-distance similarities. Such observations demonstrate that the crosstask complementary affinity information can be learned to refine the single-task similarities in PAP method. Though without supervision as <ref type="bibr" target="#b1">[2]</ref>, our PAP method can still learn good affinity matrices in such task-regularized unsupervised approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground Truth</head><p>Single-task Affinity</p><p>Cross-task Affinity in PAP <ref type="figure">Figure 5</ref>. Visualization of the single-task and our cross-task affinity maps at the white point for each task. We can see that the pairwise similarities at the white point can be improved and corrected in our PAP method.  <ref type="figure">Figure 6</ref>. Visualization of our predicted depth maps. (a) image; (b) predictions of <ref type="bibr" target="#b59">[60]</ref>; (c) our results; (d) ground truth. We can find that our predictions have obviously finer details and closer to ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Comparisons with state-of-the-art methods</head><p>Depth Estimation: We mainly perform experiments on NYUD-v2 dataset to evaluate our depth predictions. The models are based on ResNet-50. As illustrated in <ref type="table" target="#tab_3">Table 3</ref>, our model trained for three tasks (ours d+s+n) obtains competitive results, though only 795 images are used for training. Such results demonstrate that our PAP method can well  boost each task and benefit joint task learning with limited training data. For the model trained for depth&amp;normal prediction (ours d+n), with more training data can be used, our PAP method gets significantly best performances in most of the metrics with more training data, which well proves the effectiveness of our approach. Qualitative results can be observed in <ref type="figure">Fig. 6</ref>, compared with the recent work <ref type="bibr" target="#b59">[60]</ref>, our predictions are more fine-detailed and closer to ground truth. Surface Normal Estimation: We mainly evaluate our surface normal predictions on NYUD-v2 dataset. As previous methods mainly build their network based on VGG-16 <ref type="bibr" target="#b49">[50]</ref>, we also utilize the same setting in our experiments. As illustrated in <ref type="table" target="#tab_4">Table 4</ref>, our PAP method obtains obviously superior performances than the previous approaches in all metrics. Such results well demonstrate that our joint task learning method can boost and benefit the surface normal estimation. Qualitative results can be observed in <ref type="figure">Fig. 7</ref>, we can find that our method can produce better or competitive results.</p><p>RGBD Semantic Segmentation: We evaluate our segmentation results on widely-used NYUD-v2 and SUN-   RGBD datasets. The model in each experiment is build based on ResNet-50 and trained for the three tasks on NYUD-v2, and jointly depth prediction and semantic segmentation on SUN-RGBD. The performance on NYUD-v2 dataset is shown in <ref type="table" target="#tab_5">Table 5</ref>. We can observe that the performances of our PAP method are superior or competitive, though using only RGB images as input. Such results can demonstrate that although depth ground truth is not directly use, our method can benefit the segmentation from jointly learning depth information. The performances on SUN-RGBD dataset are illustrated in <ref type="table" target="#tab_6">Table 6</ref>, we can see that though slightly weaker than RDF-152 <ref type="bibr" target="#b47">[48]</ref> in mean-acc metric, our method can obtain best results in other metrics. Such results reveal that our predictions are superior or at least competitive with state-of-the-art methods. Visualized results can be observed in <ref type="figure">Fig. 8</ref>, we can see that our predictions are with high quality and close to ground truth.</p><p>Image Our Depth Our Normal <ref type="figure">Figure 9</ref>. Qualitative results of our method on KITTI dataset. We can find that our model obtains good depth predictions and normal estimations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Effectiveness On Distilling</head><p>Sometimes the ground truth data cannot be always available for each task, e.g., some widely-used outdoor depth datasets, such as KITTI <ref type="bibr" target="#b52">[53]</ref>, has no or very limited surface normal and segmentation ground truth. However, we can use PAP method to distill the knowledge from other dataset to boost the target task. We train our model on NYUD-v2 for depth and normal estimation, and then freeze the normal branch to train the model on KITTI. We evaluate our predictions on the KITTI online evaluation server, and the results are shown in <ref type="table" target="#tab_7">Table 7</ref> ( * means anonymous method). Our PAP method outperforms our single-task and cross-stich based model. Compared with the state-of-theart methods, though slightly weaker than DORN <ref type="bibr" target="#b14">[15]</ref>, our method obtains superior performances than all other published or unpublished approaches. Note that our method runs faster than DORN, which can be seen as a trade-off. These results demonstrate the effectiveness and potential of PAP method on task distilling and transferring. Qualitative results can be seen on <ref type="figure">Fig. 9</ref>, and our predictions on depth and normal are both with high quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we propose a novel Pattern-affinitive Propagation method for jointly predicting depth, surface normal and semantic segmentation. Statistic results have shown that the affinitive patterns among tasks can be modeled in pair-wise similarities to some extent. The PAP can effectively learn the pair-wise relationships from each task, and further utilize such cross-task complementary affinity to boost and regularize the joint task learning procedure via the cross-task and task-specific propagation. Extensive experiments demonstrate our PAP method obtained state-ofthe-art or competitive results on these three tasks.In the future, we may generalize and improve the efficiency of the method on more vision tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Statistics of matched affinity (or dissimilar) pairs across depth, surface normal and segmentation maps. (a) Visual exhibition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>The detailed information of affinity learning layer and diffusion process, and each block describes the feature and its shape. ⊗ represents the matrix multiplication. (a) affinity learning layer. The dashed box is corresponding to the function for computing similarities, and we only illustrate the dot-product as an example. (b) diffusion process. ⊕ represents the weighted sum with a parameter β. The dashed arrows are only performed when the iteration is not finished.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .Figure 8 .</head><label>78</label><figDesc>Visualization of our predicted surface normal. (a) image; (b) predictions of [10]; (c) predictions of [1] ; (d) predictions of [46]; (e) our results; (f) ground truth. Qualitative semantic segmentation results of our method on NYUD-v2 and SUNRGBD datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Corresponding authors † Zhenyu Zhang, Zhen Cui, Chunyan Xu, Yan Yan and Jian Yang are with PCA Lab, Key Lab of Intelligent Perception and Systems for High-Dimensional Information of Ministry of Education, and Jiangsu Key Lab of Image and Video Understanding for Social Security, School of Computer Science and Engineering, Nanjing University of Science and Technology. Zhenyu Zhang is also a visiting student in University of Trento. Nicu Sebe is the head of Dept. of Information Engineering and Computer Science Leader of Multimedia and Human Understanding Group (MHUG) University of Trento.</figDesc><table><row><cell></cell><cell></cell><cell>60%</cell><cell cols="2">Rate of Matched Similar Pairs</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">rate% in NYUDv2 rate% in SUNRGBD</cell></row><row><cell></cell><cell></cell><cell>40%</cell><cell></cell></row><row><cell>Image</cell><cell>Depth</cell><cell>20%</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Segmentation &amp;</cell><cell>Normal &amp; Depth Segmentation &amp;</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Depth</cell><cell>Normal</cell></row><row><cell></cell><cell></cell><cell>60%</cell><cell cols="2">Rate of Matched Dissimilar Pairs</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>rate% in NYUDv2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>rate% in SUNRGBD</cell></row><row><cell>Segmentation</cell><cell>Surface Normal</cell><cell>50%</cell><cell></cell></row><row><cell></cell><cell>Similar Pairs</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Dissimilar Pairs</cell><cell>40%</cell><cell>Segmentation &amp;</cell><cell>Normal &amp; Depth Segmentation &amp;</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Depth</cell><cell>Normal</cell></row><row><cell>(a)</cell><cell></cell><cell></cell><cell></cell><cell>(b)</cell></row></table><note>‡</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Analyses on Joint task learning on NYU Depth V2.</figDesc><table><row><cell>Metric</cell><cell>rmse</cell><cell>iou</cell><cell cols="2">rmse-n</cell></row><row><cell>Depth only</cell><cell>0.570</cell><cell>-</cell><cell></cell></row><row><cell>Segmentation only</cell><cell>-</cell><cell>42.8</cell><cell>-</cell></row><row><cell>Normal only</cell><cell>-</cell><cell>-</cell><cell>28.7</cell></row><row><cell>Depth&amp;Seg jointly</cell><cell cols="2">0.556 44.3</cell><cell>-</cell></row><row><cell>Depth&amp;Normal jointly</cell><cell>0.550</cell><cell>-</cell><cell>28.1</cell></row><row><cell>Segmentation&amp;Normal jointly</cell><cell>-</cell><cell>44.5</cell><cell>28.3</cell></row><row><cell>Three task jointly</cell><cell cols="2">0.533 46.2</cell><cell>26.9</cell></row><row><cell cols="5">Table 2. Comparisons of different network settings and baselines</cell></row><row><cell>on NYU Depth v2 dataset.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell></cell><cell>rmse</cell><cell>IoU</cell><cell>rmse-n</cell></row><row><cell>initial prediction</cell><cell></cell><cell cols="2">0.582 41.3</cell><cell>29.6</cell></row><row><cell>+ PAP w/o cross-t prop.</cell><cell></cell><cell cols="2">0.574 41.8</cell><cell>29.1</cell></row><row><cell>+ PAP cross-t prop.</cell><cell></cell><cell cols="2">0.558 43.1</cell><cell>28.5</cell></row><row><cell>+ PAP cross-t prop. + recon-net</cell><cell></cell><cell cols="2">0.550 43.8</cell><cell>28.2</cell></row><row><cell cols="4">+ PAP cross-t prop + recon-net + pair-loss 0.543 44.2</cell><cell>27.8</cell></row><row><cell>+ cross-stich [41]</cell><cell></cell><cell cols="2">0.550 43.5</cell><cell>28.2</cell></row><row><cell>+ CSPN [5]</cell><cell></cell><cell cols="2">0.548 43.8</cell><cell>28.0</cell></row><row><cell>aff-matrix on 1/16 input scale</cell><cell></cell><cell cols="2">0.543 44.2</cell><cell>27.8</cell></row><row><cell>aff-matrix on 1/8 input scale</cell><cell></cell><cell cols="2">0.533 46.2</cell><cell>26.9</cell></row><row><cell>aff-matrix on 1/4 input scale</cell><cell></cell><cell>0.530</cell><cell>46.5</cell><cell>26.7</cell></row><row><cell>Inner product</cell><cell></cell><cell cols="2">0.543 44.2</cell><cell>27.8</cell></row><row><cell>L1 distance</cell><cell></cell><cell cols="2">0.540 44.0</cell><cell>27.9</cell></row><row><cell cols="5">semantic segmentation results, we follow the recent works</cell></row><row><cell>[6] [24]</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Comparisons with the state-of-the-art depth estimation approaches on NYU Depth V2 Dataset.</figDesc><table><row><cell>Method</cell><cell>data</cell><cell>rmse</cell><cell>rel</cell><cell>log</cell><cell>δ1</cell><cell>δ2</cell><cell>δ3</cell></row><row><cell>HCRF [32]</cell><cell>795</cell><cell>0.821</cell><cell>0.232</cell><cell>-</cell><cell>0.621</cell><cell>0.886</cell><cell>0.968</cell></row><row><cell>DCNF [37]</cell><cell>795</cell><cell>0.824</cell><cell>0.230</cell><cell>-</cell><cell>0.614</cell><cell>0.883</cell><cell>0.971</cell></row><row><cell>Wang [54]</cell><cell>795</cell><cell>0.745</cell><cell>0.220</cell><cell>0.262</cell><cell>0.605</cell><cell>0.890</cell><cell>0.970</cell></row><row><cell>NR forest [47]</cell><cell>795</cell><cell>0.744</cell><cell>0.187</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Xu [60]</cell><cell>795</cell><cell>0.593</cell><cell>0.125</cell><cell>-</cell><cell>0.806</cell><cell>0.952</cell><cell>0.986</cell></row><row><cell>PAD-Net [58]</cell><cell>795</cell><cell>0.582</cell><cell>0.120</cell><cell>-</cell><cell>0.817</cell><cell>0.954</cell><cell>0.987</cell></row><row><cell>Eigen [11]</cell><cell>120k</cell><cell>0.877</cell><cell>0.214</cell><cell>0.285</cell><cell>0.611</cell><cell>0.887</cell><cell>0.971</cell></row><row><cell>MS-CNN [10]</cell><cell>120k</cell><cell>0.641</cell><cell>0.158</cell><cell>0.214</cell><cell>0.769</cell><cell>0.950</cell><cell>0.988</cell></row><row><cell>MS-CRF [59]</cell><cell>95k</cell><cell>0.586</cell><cell>0.121</cell><cell>-</cell><cell>0.811</cell><cell>0.954</cell><cell>0.987</cell></row><row><cell>FCRN [29]</cell><cell>12k</cell><cell>0.573</cell><cell>0.127</cell><cell>0.194</cell><cell>0.811</cell><cell>0.953</cell><cell>0.988</cell></row><row><cell>GeoNet [46]</cell><cell>16k</cell><cell>0.569</cell><cell>0.128</cell><cell>-</cell><cell>0.834</cell><cell>0.960</cell><cell>0.990</cell></row><row><cell>AdaD-S [42]</cell><cell>100k</cell><cell>0.506</cell><cell>0.114</cell><cell>-</cell><cell>0.856</cell><cell>0.966</cell><cell>0.991</cell></row><row><cell>DORN [15]</cell><cell>120k</cell><cell>0.509</cell><cell>0.115</cell><cell>-</cell><cell>0.828</cell><cell>0.965</cell><cell>0.992</cell></row><row><cell>TRL [62]</cell><cell>12k</cell><cell>0.501</cell><cell>0.144</cell><cell>0.181</cell><cell>0.815</cell><cell>0.962</cell><cell>0.992</cell></row><row><cell>Ours d+s+n</cell><cell>795</cell><cell>0.530</cell><cell>0.142</cell><cell>0.190</cell><cell>0.818</cell><cell>0.957</cell><cell>0.988</cell></row><row><cell>Ours d+n</cell><cell>12k</cell><cell>0.497</cell><cell>0.121</cell><cell>0.175</cell><cell>0.846</cell><cell>0.968</cell><cell>0.994</cell></row><row><cell>(a)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(b)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(c)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(d)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Comparisons with the state-of-the-art surface normal estimation approaches on NYU Depth V2 Dataset.</figDesc><table><row><cell>Method</cell><cell>mean</cell><cell>median</cell><cell>rmse-n</cell><cell>11.25 •</cell><cell>22.50 •</cell><cell>30 •</cell></row><row><cell>3DP [13]</cell><cell>36.3</cell><cell>19.2</cell><cell>-</cell><cell>16.4</cell><cell>36.6</cell><cell>48.2</cell></row><row><cell>UNFOLD [14]</cell><cell>35.2</cell><cell>17.9</cell><cell>-</cell><cell>40.5</cell><cell>54.1</cell><cell>58.9</cell></row><row><cell>Discr. [61]</cell><cell>33.5</cell><cell>23.1</cell><cell>-</cell><cell>27.7</cell><cell>49.0</cell><cell>58.7</cell></row><row><cell>MS-CNN [10]</cell><cell>23.7</cell><cell>15.5</cell><cell>-</cell><cell>39.2</cell><cell>62.0</cell><cell>71.1</cell></row><row><cell>Deep3D [56]</cell><cell>26.9</cell><cell>14.8</cell><cell>-</cell><cell>42.0</cell><cell>61.2</cell><cell>68.2</cell></row><row><cell>SkipNet [1]</cell><cell>19.8</cell><cell>12.0</cell><cell>28.2</cell><cell>47.9</cell><cell>70.0</cell><cell>77.8</cell></row><row><cell>SURGE [55]</cell><cell>20.6</cell><cell>12.2</cell><cell>-</cell><cell>47.3</cell><cell>68.9</cell><cell>76.6</cell></row><row><cell>GeoNet [46]</cell><cell>19.0</cell><cell>11.8</cell><cell>26.9</cell><cell>48.4</cell><cell>71.5</cell><cell>79.5</cell></row><row><cell>Ours-VGG16</cell><cell>18.6</cell><cell>11.7</cell><cell>25.5</cell><cell>48.8</cell><cell>72.2</cell><cell>79.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Comparisons the state-of-the-art semantic segmentation methods on NYU Depth v2 dataset.</figDesc><table><row><cell>Method</cell><cell>data</cell><cell>pixel-acc</cell><cell>mean-acc</cell><cell>IoU</cell></row><row><cell>FCN [40]</cell><cell>RGB</cell><cell>60.0</cell><cell>49.2</cell><cell>29.2</cell></row><row><cell>Context [36]</cell><cell>RGB</cell><cell>70.0</cell><cell>53.6</cell><cell>40.6</cell></row><row><cell>Eigen et al. [10]</cell><cell>RGB</cell><cell>65.6</cell><cell>45.1</cell><cell>34.1</cell></row><row><cell>B-SegNet [24]</cell><cell>RGB</cell><cell>68.0</cell><cell>45.8</cell><cell>32.4</cell></row><row><cell>RefineNet-101 [35]</cell><cell>RGB</cell><cell>72.8</cell><cell>57.8</cell><cell>44.9</cell></row><row><cell>PAD-Net [58]</cell><cell>RGB</cell><cell>75.2</cell><cell>62.3</cell><cell>50.2</cell></row><row><cell>TRL-ResNet50 [62]</cell><cell>RGB</cell><cell>76.2</cell><cell>56.3</cell><cell>46.4</cell></row><row><cell>Deng et al. [8]</cell><cell>RGBD</cell><cell>63.8</cell><cell>-</cell><cell>31.5</cell></row><row><cell>He et al. [22]</cell><cell>RGBD</cell><cell>70.1</cell><cell>53.8</cell><cell>40.1</cell></row><row><cell>LSTM [34]</cell><cell>RGBD</cell><cell>-</cell><cell>49.4</cell><cell>-</cell></row><row><cell>Cheng et al. [6]</cell><cell>RGBD</cell><cell>71.9</cell><cell>60.7</cell><cell>45.9</cell></row><row><cell>3D-GNN [45]</cell><cell>RGBD</cell><cell>-</cell><cell>55.7</cell><cell>43.1</cell></row><row><cell>RDF-50 [48]</cell><cell>RGBD</cell><cell>74.8</cell><cell>60.4</cell><cell>47.7</cell></row><row><cell>Ours-ResNet50</cell><cell>RGB</cell><cell>76.2</cell><cell>62.5</cell><cell>50.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>Comparison with the state-of-the-art semantic segmentation methods on SUN-RGBD dataset.</figDesc><table><row><cell>Method</cell><cell>data</cell><cell>pixel-acc</cell><cell>mean-acc</cell><cell>IoU</cell></row><row><cell>Context [36]</cell><cell>RGB</cell><cell>78.4</cell><cell>53.4</cell><cell>42.3</cell></row><row><cell>B-SegNet [24]</cell><cell>RGB</cell><cell>71.2</cell><cell>45.9</cell><cell>30.7</cell></row><row><cell>RefineNet-101 [35]</cell><cell>RGB</cell><cell>80.4</cell><cell>57.8</cell><cell>45.7</cell></row><row><cell>TRL-ResNet50 [62]</cell><cell>RGB</cell><cell>83.6</cell><cell>58.9</cell><cell>50.3</cell></row><row><cell>LSTM [34]</cell><cell>RGBD</cell><cell>-</cell><cell>48.1</cell><cell>-</cell></row><row><cell>Cheng et al. [6]</cell><cell>RGBD</cell><cell>-</cell><cell>58.0</cell><cell>-</cell></row><row><cell>CFN [9]</cell><cell>RGBD</cell><cell>-</cell><cell>-</cell><cell>48.1</cell></row><row><cell>3D-GNN [45]</cell><cell>RGBD</cell><cell>-</cell><cell>57.0</cell><cell>45.9</cell></row><row><cell>RDF-152 [48]</cell><cell>RGBD</cell><cell>81.5</cell><cell>60.1</cell><cell>47.7</cell></row><row><cell>Ours-ResNet50</cell><cell>RGB</cell><cell>83.8</cell><cell>58.4</cell><cell>50.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 .</head><label>7</label><figDesc>Comparison with the state-of-the-art methods on KITTI online benchmark (lower is better).</figDesc><table><row><cell>Method</cell><cell>SILog</cell><cell>sqErrRel</cell><cell>absErrRel</cell><cell>iRMSE</cell><cell>time</cell></row><row><cell>DORN [15]</cell><cell>11.77</cell><cell>2.23</cell><cell>8.78</cell><cell>12.98</cell><cell>0.5s</cell></row><row><cell>VGG16-Unet  *</cell><cell>13.41</cell><cell>2.86</cell><cell>10.60</cell><cell>15.06</cell><cell>0.16s</cell></row><row><cell>FUSION-ROB  *</cell><cell>13.90</cell><cell>3.14</cell><cell>11.04</cell><cell>15.69</cell><cell>2s</cell></row><row><cell>BMMNet  *</cell><cell>14.37</cell><cell>5.10</cell><cell>10.92</cell><cell>15.51</cell><cell>0.1s</cell></row><row><cell>DABC [33]</cell><cell>14.49</cell><cell>4.08</cell><cell>12.72</cell><cell>15.53</cell><cell>0.7s</cell></row><row><cell>APMoE [27]</cell><cell>14.74</cell><cell>3.88</cell><cell>11.74</cell><cell>15.63</cell><cell>0.2s</cell></row><row><cell>CSWS [31]</cell><cell>14.85</cell><cell>3.48</cell><cell>11.84</cell><cell>16.38</cell><cell>0.2s</cell></row><row><cell>Ours single</cell><cell>14.58</cell><cell>3.96</cell><cell>11.50</cell><cell>15.24</cell><cell>0.1s</cell></row><row><cell>Ours cross-stich [41]</cell><cell>14.33</cell><cell>3.85</cell><cell>11.23</cell><cell>15.14</cell><cell>0.1s</cell></row><row><cell>Ours</cell><cell>13.08</cell><cell>2.72</cell><cell>10.27</cell><cell>13.95</cell><cell>0.2s</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgement</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Marr revisited: 2d-3d alignment via surface normal prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aayush</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5965" to="5974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Convolutional random walk networks for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6137" to="6145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="41" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deepdriving: Learning affordance for direct perception in autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alain</forename><surname>Kornhauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2722" to="2730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Depth estimation via affinity learned with convolutional spatial propagation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinjing</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="108" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Locality-sensitive deconvolution networks with gated fusion for rgb-d indoor semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanhua</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1475" to="1483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semantic segmentation of rgbd images with mutex constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuo</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinisa</forename><surname>Todorovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longin Jan</forename><surname>Latecki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1733" to="1741" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cascaded feature network for semantic segmentation of rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Guangyong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cohen-Or</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Pheng-Ann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huang</forename><surname>Hui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1320" to="1328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2650" to="2658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2366" to="2374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A survey of socially interactive robots. Robotics and autonomous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrence</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illah</forename><surname>Nourbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kerstin</forename><surname>Dautenhahn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="143" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Datadriven 3d primitives for single image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>David F Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3392" to="3399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unfolding an indoor origami world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ford Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="687" to="702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep ordinal regression network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaohui</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2002" to="2011" />
		</imprint>
	</monogr>
	<note>Kayhan Batmanghelich, and Dacheng Tao</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning rich features from rgb-d images for object detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arbelez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">8695</biblScope>
			<biblScope unit="page" from="345" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<editor>Piotr Dollr, and Ross Girshick. Mask R-CNN. ICCV</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Guided image filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1397" to="1409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Std2p: Rgbd semantic segmentation using spatio-temporal data-driven pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chen</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margret</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">I</forename><surname>Campus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7158" to="7167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Std2p: Rgbd semantic segmentation using spatio-temporal data-driven pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chen</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margret</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">I</forename><surname>Campus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7158" to="7167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adaptive affinity fields for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jyh-Jing</forename><surname>Tsung-Wei Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Bayesian segnet: Model uncertainty in deep convolutional encoder-decoder architectures for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.02680</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unified depth prediction and intrinsic image decomposition from a single image via joint convolutional neural fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungryong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihong</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwanghoon</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="143" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Ubernet: Training a &apos;universal&apos; convolutional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5454" to="5463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Pixel-wise attentional gating for parsimonious pixel labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless</forename><surname>Fowlkes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.01556</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deeper depth prediction with fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vison</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="239" to="248" />
		</imprint>
	</monogr>
	<note>Vasileios Belagiannis, Federico Tombari, and Nassir Navab</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A closed-form solution to natural image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anat</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="228" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Monocular depth estimation with hierarchical fusion of dilated cnns and softweighted-sum inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyi</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Depth and surface normal estimation from monocular images using regression on deep features and hierarchical crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1119" to="1127" />
		</imprint>
	</monogr>
	<note>Anton van den Hengel, and Mingyi He</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Deep attention-based classification network for robust depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruibo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxiao</forename><surname>Hang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03959</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Lstm-cf: Unifying context modeling and fusion with lstms for rgb-d scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="541" to="557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Refinenet: Multi-path refinement networks for highresolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5168" to="5177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Efficient piecewise training of deep structured models for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3194" to="3203" />
		</imprint>
	</monogr>
	<note>Anton Van Den Hengel, and Ian Reid</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning depth from single monocular images using deep convolutional neural fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fayao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="2024" to="2039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning to diffuse: A new perspective to design pdes for visual analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Risheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyu</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongxuan</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="2457" to="2471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning affinity via spatial propagation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sifei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwei</forename><surname>Shalini De Mello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyu</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1520" to="1530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="640" to="651" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Cross-stitch networks for multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3994" to="4003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Adadepth: Unsupervised content congruent adaptation for depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jogendra</forename><surname>Nath Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phani</forename><forename type="middle">Krishna</forename><surname>Uppala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anuj</forename><surname>Pahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonwoo</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Sanja Fidler, and Raquel Urtasun. 3d graph neural networks for rgbd semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5199" to="5208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Geonet: Geometric neural network for joint depth and surface normal estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengzhe</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="283" to="291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Monocular depth estimation using neural regression forest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirban</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinisa</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5506" to="5514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Rdfnet: Rgb-d multi-level residual feature fusion for indoor semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Park</forename><surname>Seong-Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Ki-Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee</forename><surname>Seungyong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4990" to="4999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from RGBD images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="746" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Sun RGB-D: A RGB-D scene understanding benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="567" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Cnn-slam: Real-time dense monocular slam with learned depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Tateno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Sparsity invariant cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Towards unified depth and semantic prediction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2800" to="2809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Surge: Surface regularized geometry estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="172" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Designing deep networks for surface normal estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="539" to="547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Abhinav Gupta, and Kaiming He. Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Pad-net: Multi-tasks guided prediction-and-distillation network for simultaneous depth estimation and scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Multi-scale continuous CRFs as sequential deep networks for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="161" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Structured attention guided convolutional neural fields for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3917" to="3925" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Discriminatively trained dense surface normal estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Zeisl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="468" to="484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Joint task-recursive learning for semantic segmentation and depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="235" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
