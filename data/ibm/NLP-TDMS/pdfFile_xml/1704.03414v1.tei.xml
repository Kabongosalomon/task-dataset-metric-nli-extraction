<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A-Fast-RCNN: Hard Positive Generation via Adversary for Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">The Robotics Institute</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">The Robotics Institute</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">The Robotics Institute</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A-Fast-RCNN: Hard Positive Generation via Adversary for Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T15:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>How do we learn an object detector that is invariant to occlusions and deformations? Our current solution is to use a data-driven strategy -collect large-scale datasets which have object instances under different conditions. The hope is that the final classifier can use these examples to learn invariances. But is it really possible to see all the occlusions in a dataset? We argue that like categories, occlusions and object deformations also follow a long-tail. Some occlusions and deformations are so rare that they hardly happen; yet we want to learn a model invariant to such occurrences. In this paper, we propose an alternative solution. We propose to learn an adversarial network that generates examples with occlusions and deformations. The goal of the adversary is to generate examples that are difficult for the object detector to classify. In our framework both the original detector and adversary are learned in a joint manner. Our experimental results indicate a 2.3% mAP boost on VOC07 and a 2.6% mAP boost on VOC2012 object detection challenge compared to the Fast-RCNN pipeline. We also release the code 1 for this paper.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The goal of object detection is to learn a visual model for concepts such as cars and use this model to localize these concepts in an image. This requires the ability to robustly model invariances to illuminations, deformations, occlusions and other intra-class variations. The standard paradigm to handle these invariances is to collect large-scale datasets which have object instances under different conditions. For example, the COCO dataset <ref type="bibr" target="#b17">[18]</ref> has more than 10K examples of cars under different occlusions and deformations. The hope is that these examples capture all possible variations of a visual concept and the classifier can then effectively model invariances to them. We believe this has been one of the prime reasons why ConvNets have been so successful at the task of object detection: they are able to use all this data to learn invariances.</p><p>However, like object categories, we believe even occlusions and deformations follow a long-tail distribution. That  <ref type="figure">Figure 1</ref>: We argue that both occlusions and deformations follow a long-tail distribution. Some occlusions and deformations are rare. In this paper, we propose to use an adversarial network to generate examples with occlusions and deformations that will be hard for an object detector to classify. Our adversarial network adapts as the object detector becomes better and better. We show boost in detection accuracy with this adversarial learning strategy empirically.</p><p>is, some of the occlusions and deformations are so rare that there is a low chance that they will occur in large-scale datasets. For example, consider the occlusions shown in <ref type="figure">Figure 1</ref>. We notice that some occlusions occur more often than others (e.g., occlusion from other cars in a parking garage is more frequent than from an air conditioner). Similarly, some deformations in animals are common (e.g., sitting/standing poses) while other deformations are very rare. So, how can we learn invariances to such rare/uncommon occlusions and deformations? While collecting even larger datasets is one possible solution, it is not likely to scale due to the long-tail statistics.</p><p>Recently, there has been a lot of work on generating images (or pixels) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b25">26]</ref>. One possible way to learn about these rare occurrences is to generate realistic images by sampling from the tail distribution. However, this is not a feasible solution since image generation would require training examples of these rare occurrences to begin with. Another solution is to generate all possible occlusions and deformations and train object detectors from them. However, since the space of deformations and occlusions is huge, this is not a scalable solution. It has been shown that using all examples is often not the optimal solution <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b38">39]</ref> and selecting hard examples is better. Is there a way we can generate "hard" positive examples with different occlusions and deformations and without generating the pixels themselves?</p><p>How about training another network: an adversary that creates hard examples by blocking some feature maps spatially or creates spatial deformations by manipulating feature responses. This adversary will predict what it thinks will be hard for a detector like Fast-RCNN <ref type="bibr" target="#b6">[7]</ref> and in turn the Fast-RCNN will adapt itself to learn to classify these adversarial examples. The key idea here is to create adversarial examples in convolutional feature space and not generate the pixels directly since the latter is a much harder problem.</p><p>In our experiments, we show substantial improvements in performance of the adversarial Fast-RCNN (A-Fast-RCNN) compared to the standard Fast-RCNN pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In recent years, significant gains have been made in the field of object detection. These recent successes build upon the powerful deep features <ref type="bibr" target="#b15">[16]</ref> learned from the task of Im-ageNet classification <ref type="bibr" target="#b2">[3]</ref>. R-CNN <ref type="bibr" target="#b7">[8]</ref> and OverFeat <ref type="bibr" target="#b29">[30]</ref> object detection systems led this wave with impressive results on PASCAL VOC <ref type="bibr" target="#b4">[5]</ref>; and in recent years, more computationally efficient versions have emerged that can efficiently train on larger datasets such as COCO <ref type="bibr" target="#b17">[18]</ref>. For example, Fast-RCNN <ref type="bibr" target="#b6">[7]</ref> shares the convolutions across different region proposals to provide speed-up, Faster-RCNN <ref type="bibr" target="#b27">[28]</ref> and R-FCN <ref type="bibr" target="#b1">[2]</ref> incorporate region proposal generation in the framework leading to a completely end-to-end version. Building on the sliding-window paradigm of the Overfeat detector, other computationally-efficient approaches have emerged such as YOLO <ref type="bibr" target="#b26">[27]</ref>, SSD <ref type="bibr" target="#b18">[19]</ref> and DenseBox <ref type="bibr" target="#b12">[13]</ref>. Thorough comparisons among these methods are discussed in <ref type="bibr" target="#b11">[12]</ref>.</p><p>Recent research has focused on three principal directions on developing better object detection systems. The first direction relies on changing the base architecture of these networks. The central idea is that using deeper networks should not only lead to improvements in classification <ref type="bibr" target="#b2">[3]</ref> but also object detection <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b17">18]</ref>. Some recent work in this direction include ResNet <ref type="bibr" target="#b9">[10]</ref>, Inception-ResNet <ref type="bibr" target="#b37">[38]</ref> and ResNetXt <ref type="bibr" target="#b42">[43]</ref> for object detection.</p><p>The second area of research has been to use contextual reasoning, proxy tasks for reasoning and other top-down mechanisms for improving representations for object detection <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b44">45]</ref>. For example, <ref type="bibr" target="#b31">[32]</ref> use segmentation as a way to contextually prime object detectors and provide feedback to initial layers. <ref type="bibr" target="#b0">[1]</ref> uses skipnetwork architecture and uses features from multiple layers of representation in conjunction with contextual reasoning. Other approaches include using a top-down features for incorporating context and finer details <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b33">34]</ref> which leads to improved detections.</p><p>The third direction to improve a detection systems is to better exploit the data itself. It is often argued that the recent success of object detectors is a product of better visual representation and the availability of large-scale data for learning. Therefore, this third class of approaches try to explore how to better utilize data for improving the performance. One example is to incorporate hard example mining in an effective and efficient setup for training region-based Con-vNets <ref type="bibr" target="#b32">[33]</ref>. Other examples of findind hard examples for training include <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b40">41]</ref>.</p><p>Our work follows this third direction of research where the focus is on leveraging data in a better manner. However, instead of trying to sift through the data to find hard examples, we try to generate examples which will be hard for Fast-RCNN to detect/classify. We restrict the space of new positive generation to adding occlusions and deformations to the current existing examples from the dataset. Specifically, we learn adversarial networks which try to predict occlusions and deformations that would lead to misclassification by Fast-RCNN. Our work is therefore related to lot of recent work in adversarial learning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b36">37]</ref>. For example, techniques have been proposed to improve adversarial learning for image generation <ref type="bibr" target="#b25">[26]</ref> as well as for training better image generative model <ref type="bibr" target="#b28">[29]</ref>. <ref type="bibr" target="#b28">[29]</ref> also highlights that the adversarial learning can improve image classification in a semi-supervised setting. However, the experiments in these works are conducted on data which has less complexity than object detection datasets, where image generation results are significantly inferior. Our work is also related to a recent work on adversarial training in robotics <ref type="bibr" target="#b24">[25]</ref>. However, instead of using an adversary for better supervision, we use the adversary to generate the hard examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Adversarial Learning for Object Detection</head><p>Our goal is to learn an object detector that is robust to different conditions such as occlusion, deformation and illumination. We hypothesize that even in large-scale datasets, it is impossible to cover all potential occlusions and deformations. Instead of relying heavily on the dataset or sifting through data to find hard examples, we take an alternative approach. We actively generate examples which are hard for the object detector to recognize. However, instead of generating the data in the pixel space, we focus on a restricted space for generation: occlusion and deformation.  Mathematically, let us assume the original object detector network is represented as F(X) where X is one of the object proposals. A detector gives two outputs F c which represents class output and F l represent predicted bounding box location. Let us assume that the ground-truth class for X is C with spatial location being L. Our original detector loss can be written down as,</p><formula xml:id="formula_0">LF = Lsoftmax(Fc(X), C) + [C / ∈ bg] Lbbox(F l (X), L),</formula><p>where the first term is the SoftMax loss and the second term is the loss based on predicted bounding box location and ground truth box location (foreground classes only). Let's assume the adversarial network is represented as A(X) which given a feature X computed on image I, generates a new adversarial example. The loss function for the detector remains the same just that the mini-batch now includes fewer original and some adversarial examples.</p><p>However, the adversarial network has to learn to predict the feature on which the detector would fail. We train this adversarial network via the following loss function,</p><formula xml:id="formula_1">LA = −Lsoftmax(Fc(A(X)), C).</formula><p>Therefore, if the feature generated by the adversarial network is easy for the detector to classify, we get a high loss for the adversarial network. On the other hand, if after adversarial feature generation it is difficult for the detector, we get a high loss for the detector and a low loss for the adversarial network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">A-Fast-RCNN: Approach Details</head><p>We now describe the details of our framework. We first give a brief overview of our base detector Fast-RCNN. This is followed by describing the space of adversarial generation. In particular, we focus on generating different types of occlusions and deformations in this paper. Finally, in Section 5, we describe our experimental setup and show the results which indicate significant improvements over baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Overview of Fast-RCNN</head><p>We build upon the Fast-RCNN framework for object detection <ref type="bibr" target="#b6">[7]</ref>. Fast-RCNN is composed of two parts: (i) a convolutional network for feature extraction; (ii) an RoI network with an RoI-pooling layer and a few fully connected layers that output object classes and bounding boxes.</p><p>Given an input image, the convolutional network of the Fast-RCNN takes the whole image as an input and produces convolutional feature maps as the output. Since the operations are mainly convolutions and max-pooling, the spatial dimensions of the output feature map will change according to the input image size. Given the feature map, the RoIpooling layer is used to project the object proposals <ref type="bibr" target="#b39">[40]</ref> onto the feature space. The RoI-pooling layer crops and resizes to generate a fixed size feature vector for each object proposal. These feature vectors are then passed through fully connected layers. The outputs of the fully connected layers are: (i) probabilities for each object class including the background class; and (ii) bounding box coordinates.</p><p>For training, the SoftMax loss and regression loss are applied on these two outputs respectively, and the gradients are back propagated though all the layers to perform endto-end learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Adversarial Networks Design</head><p>We consider two types of feature generations by adversarial networks competing against the Fast-RCNN (FRCN) detector. The first type of generation is occlusion. Here, we propose Adversarial Spatial Dropout Network (ASDN) … (b) Generated Masks (a) Pre-training via Searching which learns how to occlude a given object such that it becomes hard for FRCN to classify. The second type of generation we consider in this paper is deformation. In this case, we propose Adversarial Spatial Transformer Network (ASTN) which learns how to rotate "parts" of the objects and make them hard to recognize by the detector. By competing against these networks and overcoming the obstacles, the FRCN learns to handle object occlusions and deformations in a robust manner. Note that both the proposed networks ASDN and ASTN are learned simultaneously in conjunction with the FRCN during training. Joint training prevents the detector from overfitting to the obstacles created by the fixed policies of generation.</p><p>Instead of creating occlusions and deformations on the input images, we find that operating on the feature space is more efficient and effective. Thus, we design our adversarial networks to modify the features to make the object harder to recognize. Note that these two networks are only applied during training to improve the detector. We will first introduce the ASDN and ASTN individually and then combine them together in an unified framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Adversarial Spatial Dropout for Occlusion</head><p>We propose an Adversarial Spatial Dropout Network (ASDN) to create occlusions on the deep features for foreground objects. Recall that in the standard Fast-RCNN pipeline, we can obtain the convolutional features for each foreground object proposal after the RoI-pooling layer. We use these region-based features as the inputs for our adversarial network. Given the feature of an object, the ASDN will try to generate a mask indicating which parts of the feature to dropout (assigning zeros) so that the detector cannot recognize the object.</p><p>More specifically, given an object we extract the feature X with size d × d × c, where d is the spatial dimension and c represents the number of channels (e.g., c = 256, d = 6 in AlexNet). Given this feature, our ASDN will predict a mask M with d × d values which are either 0 or 1 after thresholding. We visualize some of the masks before thresholding in <ref type="figure" target="#fig_3">Fig. 3(b)</ref>. We denote M ij as the value for the ith row and jth column of the mask. Similarly, X ijk represents the value in channel k at location i, j of the feature. If M ij = 1, we drop out the values of all the channels in the corresponding spatial location of the feature map X, i.e., X ijk = 0, ∀k.</p><p>Network Architecture. We use the standard Fast-RCNN (FRCN) architecture. We initialize the network using pre-training from ImageNet <ref type="bibr" target="#b2">[3]</ref>. The adversarial network shares the convolutional layers and RoI-pooling layer with FRCN and then uses its own separate fully connected layers. Note that we do not share the parameters in our ASDN with Fast-RCNN since we are optimizing two networks to do the exact opposite tasks.</p><p>Model Pre-training. In our experiment, we find it important to pre-train the ASDN for creating occlusions before using it to improve Fast-RCNN. Motivated by the Faster RCNN detector <ref type="bibr" target="#b27">[28]</ref>, we apply stage-wise training here. We first train our Fast-RCNN detector without ASDN for 10K iterations. As the detector now has a sense of the objects in the dataset, we train the ASDN model for creating the occlusions by fixing all the layers in the detector.</p><p>Initializing ASDN Network. To initialize the ASDN network, given a feature map X with spatial layout d × d, we apply a sliding window with size d 3 × d 3 on it. We represent the sliding window process by projecting the window back to the image as 3(a). For each sliding window, we drop out the values in all channels whose spatial locations are covered by the window and generate a new feature vector for the region proposal. This feature vector is then passed through classification layers to compute the loss. Based on the loss of all the d 3 × d 3 windows, we select the one with the highest loss. This window is then used to create a single d × d mask (with 1 for the window location and 0 for the other pixels). We generate these spatial masks for n positive region proposals and obtain n pairs of training examples {(X 1 ,M 1 ), ..., (X n ,M n )} for our adversarial dropout network. The idea is that the ASDN should learn to generate the masks which can give the detector network high losses. We apply the binary cross entropy loss in training the ASDN and it can be formulated as,  where A ij (X p ) represents the outputs of the ASDN in location (i, j) given input feature map X p . We train the ASDN with this loss for 10K iterations. We show that the network starts to recognize which part of the objects are significant for classification as shown in <ref type="figure" target="#fig_3">Fig. 3(b)</ref>. Also note that our output masks are different from the Attention Mask proposed in <ref type="bibr" target="#b30">[31]</ref>, where they use the attention mechanism to facilitate classification. In our case, we use the masks to occlude parts to make the classification harder.</p><formula xml:id="formula_2">L = − 1 n n p d i,j [M p ij Aij(X p ) + (1 −M p ij )(1 − Aij(X p ))],<label>(1)</label></formula><p>Thresholding by Sampling. The output generated by ASDN network is not a binary mask but rather a continuous heatmap. Instead of using direct thresholding, we use importance sampling to select the top <ref type="bibr" target="#b0">1</ref> 3 pixels to mask out. Note that the sampling procedure incorporates stochasticity and diversity in samples during training. More specifically, given a heatmap, we first select the top 1 2 pixels with top probabilities and randomly select 1 3 pixels out of them to assign the value 1 and the rest of <ref type="bibr">2 3</ref> pixels are set to 0. Joint Learning. Given the pre-trained ASDN and Fast-RCNN model, we jointly optimize these two networks in each iteration of training. For training the Fast-RCNN detector, we first use the ASDN to generate the masks on the features after the RoI-pooling during forward propagation. We perform sampling to generate binary masks and use them to drop out the values in the features after the RoIpooling layer. We then forward the modified features to calculate the loss and train the detector end-to-end. Note that although our features are modified, the labels remain the same. In this way, we create "harder" and more diverse examples for training the detector.</p><p>For training the ASDN, since we apply the sampling strategy to convert the heatmap into a binary mask, which is not differentiable, we cannot directly back-prop the gradients from the classification loss. Alternatively, we take the inspirations from the REINFORCE <ref type="bibr" target="#b41">[42]</ref> approach. We compute which binary masks lead to significant drops in Fast-RCNN classification scores. We use only those hard example masks as ground-truth to train the adversarial network directly using the same loss as described in Eq. 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Adversarial Spatial Transformer Network</head><p>We now introduce the Adversarial Spatial Transformer Network (ASTN). The key idea is to create deformations on the object features and make object recognition by the detector difficult. Our network is built upon the Spatial Transformer Network (STN) proposed in <ref type="bibr" target="#b14">[15]</ref>. In their work, the STN is proposed to deform the features to make classification easier. Our network, on the other hand, is doing the exact opposite task. By competing against our ASTN, we can train a better detector which is robust to deformations. STN Overview. The Spatial Transformer Network <ref type="bibr" target="#b14">[15]</ref> has three components: localisation network, grid generator and sampler. Given the feature map as input, the localisation network will estimate the variables for deformations (e.g., rotation degree, translation distance and scaling factor). These variables will be used as inputs for the grid generator and sampler to operate on the feature map. The output is a deformed feature map. Note that we only need to learn the parameters in the localisation network. One of the key contribution of STN is making the whole process differentiable, so that the localisation network can be directly optimized for the classification objective via back propagation. Please refer to <ref type="bibr" target="#b14">[15]</ref> for more technical details.</p><p>Adversarial STN. In our Adversarial Spatial Transformer Network, we focus on feature map rotations. That is, given a feature map after the RoI-pooling layer as input, our ASTN will learn to rotate the feature map to make it harder to recognize. Our localisation network is composed with 3 fully connected layers where the first two layers are initialized with fc6 and fc7 layers from ImageNet pre-trained network as in our Adversarial Spatial Dropout Network.</p><p>We train the ASTN and the Fast-RCNN detector jointly. For training the detector, similar to the process in the ASDN, the features after RoI-pooling are first transformed by our ASTN and forwarded to the higher layers to compute the SoftMax loss. For training the ASTN, we optimize it so that the detector will classify the foreground objects as the background class. Different from training ASDN, since the spatial transformation is differentiable, we can directly use the classification loss to back-prop and finetune the parameters in the localisation network of ASTN.</p><p>Implementation Details. In our experiments, we find it very important to limit the rotation degrees produced by the ASTN. Otherwise it is very easy to rotate the object upside down which is the hardest to recognize in most cases. We constrain the rotation degree within 10 • clockwise and anti-clockwise. Instead of rotating all the feature map in the same direction, we divide the feature maps on the channel dimension into 4 blocks and estimate 4 different rotation angles for different blocks. Since each of the channel corresponds to activations of one type of feature, rotating channels separately corresponds to rotating parts of the object in different directions which leads to deformations. We also find that if we use one rotation angle for all feature maps, the ASTN will often predict the largest angle. By using 4 different angles instead of one, we increase the complexity of the task which prevents the network from predicting trivial deformations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Adversarial Fusion</head><p>The two adversarial networks ASDN and ASTN can also be combined and trained together in the same detection framework. Since these two networks offer different types of information. By competing against these two networks simultaneously, our detector become more robust.</p><p>We combine these two networks into the Fast-RCNN framework in a sequential manner. As shown in <ref type="figure" target="#fig_4">Fig. 4</ref>, the feature maps extracted after the RoI-pooling are first forwarded to our ASDN which drop out some activations. The modified features are further deformed by the ASTN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We conduct our experiments on PASCAL VOC 2007, PASCAL VOC 2012 <ref type="bibr" target="#b4">[5]</ref> and MS COCO <ref type="bibr" target="#b17">[18]</ref> datasets. As is standard practice, we perform most of the ablative studies on the PASCAL VOC 2007 dataset. We also report our numbers on the PASCAL VOC 2012 and COCO dataset. Finally, we perform a comparison between our method and the Online Hard Example Mining (OHEM) <ref type="bibr" target="#b32">[33]</ref> approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experimental settings</head><p>PASCAL VOC. For the VOC datasets, we use the 'trainval' set for training and 'test' set for testing. We follow most of the setup in standard Fast-RCNN <ref type="bibr" target="#b6">[7]</ref> for training.</p><p>We apply SGD for 80K to train our models. The learning rate starts with 0.001 and decreases to 0.0001 after 60K iterations. We use the selective search proposals <ref type="bibr" target="#b39">[40]</ref> during training.</p><p>MS COCO. For the COCO dataset, we use the 'train-val35k' set for training and the 'minival' set for testing. During training the Fast-RCNN <ref type="bibr" target="#b6">[7]</ref>, we apply SGD with 320K iterations. The learning rate starts with 0.001 and decreases to 0.0001 after 280K iterations. For object proposals, we use the DeepMask proposals <ref type="bibr" target="#b23">[24]</ref>.</p><p>In all the experiments, our minibatch size for training is 256 proposals with 2 images. We follow the Torch implementation <ref type="bibr" target="#b43">[44]</ref> of Fast-RCNN. With these settings, our baseline numbers for are slightly better than the reported number in <ref type="bibr" target="#b6">[7]</ref>. To prevent the Fast-RCNN from overfitting to the modified data, we provide one image in the batch without any adversarial occlusions/deformations and apply our approach on another image in the batch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">PASCAL VOC 2007 Results</head><p>We report our results for using ASTN and ASDN during training Fast-RCNN in <ref type="table" target="#tab_1">Table 1</ref>. For the AlexNet architecture <ref type="bibr" target="#b15">[16]</ref>, our implemented baseline is 57.0% mAP. Based on this setting, joint learning with our ASTN model reaches 58.1% and joint learning with the ASDN model gives higher performance of 58.5%. As both methods are complementary to each other, combining ASDN and ASTN into our full model gives another boost to 58.9% mAP.</p><p>For the VGG16 architecture <ref type="bibr" target="#b35">[36]</ref>, we conduct the same set of experiments. Firstly, our baseline model reaches 69.1% mAP, much higher than the reported number 66.9% in <ref type="bibr" target="#b6">[7]</ref>. Based on this implementation, joint learning with our ASTN model gives an improvement to 69.9% mAP and the ASDN model reaches 71.0% mAP. Our full model with both ASTN and ASDN improves the performance to 71.4%. Our final result gives 2.3% boost upon the baseline.</p><p>To show that our method also works with very deep CNNs, we apply the ResNet-101 <ref type="bibr" target="#b9">[10]</ref> architecture in training Fast-RCNN. As the last two lines in <ref type="table">Table.</ref>1 illustrate, the performance of Fast-RCNN with ResNet-101 is 71.8% mAP. By applying the adversarial training, the result is 73.6% mAP. We can see that our approach consistently improves performances on different types of architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Ablative Analysis</head><p>ASDN Analysis. We compare our Advesarial Spatial Dropout Network with various dropout/occlusion strategy in training using the AlexNet architecture. The first simple baseline we try is random spatial dropout on the feature after RoI-Pooling. For a fair comparison, we mask the activations of the same number of neurons as we do in the ASDN network. As <ref type="table" target="#tab_2">Table 2</ref> shows, the performance of random dropout is 57.3% mAP which is slightly better than the baseline. Another dropout strategy we compare to is a  similar strategy we apply in pre-training the ASDN <ref type="figure" target="#fig_3">(Fig. 3)</ref>. We exhaustively enumerate different kinds of occlusion and select the best ones for training in each iteration. The performance is 57.7% mAP (Ours (hard dropout)), which is slightly better than random dropout. As we find the exhaustive strategy can only explore very limited space of occlusion policies, we use the pre-trained ASDN network to replace it. However, when we fix the parameters of the ASDN, we find the performance is 57.5% mAP (Ours (fixed ASDN) ) , which is not as good as the exhaustive strategy. The reason is the fixed ASDN has not received any feedback from the updating Fast-RCNN while the exhaustive search has. If we jointly learn the ASDN and the Fast-RCNN together, we can get 58.5% mAP, 1.5% improvement compared to the baseline without dropout. This evidence shows that joint learning of ASDN and Fast-RCNN is where it makes a difference.</p><p>ASTN Analysis. We compared our Adversarial Spatial Transformer Network with random jittering on the object proposals. The augmentations include random changes of scale, aspect ratio and rotation on the proposals during training the Fast-RCNN. With AlexNet, the performance of using random jittering is 57.3% mAP while our ASTN results is 58.1%. With VGG16, we have 68.6% for random jittering and 69.9% for the ASTN. For both architectures, the model with ASTN works better than random jittering. terestingly the categories that seemed to be helped by both ASTN and ASDN seem to be quire similar. It seems that both plant and bottle performance improves with adversarial training. However, combining the two transformations together seems to improve performance on some categories which were hurt by using occlusion or deformations alone. Specifically, categories like car and aeroplane are helped by combining the two adversarial processes.   <ref type="figure" target="#fig_6">Figure 6</ref> shows some of the false positives of our approach with the diagnosing code <ref type="bibr" target="#b10">[11]</ref>. These examples are handpicked such that they only appeared in the list of false positives for adversarial learning but not the original Fast-RCNN. These results indicate some of the shortcomings of adversarial learning. In some cases, the adversary creates deformations or occlusions which are similar to other object categories and leading to over-generalization. For example, our approach hides the wheels of the bicycle which leads to a wheel chair being classified as a bike.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Category-based Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Qualitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Results on PASCAL VOC 2012 and MS COCO</head><p>We show our results with VGG16 on the PASCAL VOC 2012 dataset in <ref type="table" target="#tab_3">Table 3</ref>, where our baseline performance is 66.4% .Our full approach with joint learning of ASDN and ASTN gives 2.6% boost to 69.0% mAP. This again shows that the performance improvement using VGG on VOC2012 is significant. We also observe that our method improves performance of all the categories except sofa in VOC 2012. We believe this is probably because of larger diversity in VOC 2012.</p><p>We finally report the results in MS COCO dataset. The baseline method with VGG16 architecture is 42.7% AP 50 on the VOC metric and 25.7% AP on the standard COCO metric. By applying our method, we achieve 46.2% AP 50 and 27.1% AP on the VOC and COCO metric respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Comparisons with OHEM</head><p>Our method is also related to the Online Hard Example Mining (OHEM) approach <ref type="bibr" target="#b32">[33]</ref>. Our method allows us to sample data-points which might not exist in the dataset, whereas OHEM is bound by the dataset. However, OHEM has more realistic features since they are extracted from real images. For comparisons, our approach (71.4%) is better than OHEM (69.9%) in VOC2007. However, our result (69.0%) is not as good as OHEM (69.8%) in VOC2012. Since these two approaches are generating or selecting different types of features in training, we believe they should be complementary. To demonstrate this, we use an ensemble of these two approaches and compare it with separate ensembles of OHEM and Ours alone on VOC 2012. As a result, the ensemble of two methods achieves 71.7% mAP, while the ensemble of two OHEM models (71.2%) or two of our models (70.2%) are not as good, indicating the complementary nature of two approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>One of the long-term goals of object detection is to learn object models that are invariant to occlusions and deformations. Current approaches focus on learning these invariances by using large-scale datasets. In this paper, we argue that like categories, occlusions and deformations also follow a long-tail distribution: some of them are so rare that they might be hard to sample even in a large-scale dataset. We propose to learn these invariances using adversarial learning strategy. The key idea is to learn an adversary in conjunction with original object detector. This adversary creates examples on the fly with different occlusions and deformations, such that these occlusions/deformations make it difficult for original object detector to classify. Instead of generating examples in pixel space, our adversarial network modifies the features to mimic occlusion and deformations. We show in our experiments that such an adversarial learning strategy provides significant boost in detection performance on VOC and COCO dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1</head><label></label><figDesc>https://github.com/xiaolonw/adversarial-frcnn</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Our network architecture of ASDN and how it combines with Fast RCNN approach. Our ASDN network takes as input image patches with features extracted using RoI pooling layer. ASDN network than predicts an occlusion/dropout mask which is then used to drop the feature values and passed onto classification tower of Fast-RCNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>(a) Model pre-training: Examples of occlusions that are sifted to select the hard occlusions and used as ground-truth to train the ASDN network (b) Examples of occlusion masks generated by ASDN network. The black regions are occluded when passed on to FRCN pipeline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Network architecture for combining ASDN and ASTN network. First occlusion masks are created and then the channels are rotated to generate hard examples for training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 Figure 5 :</head><label>55</label><figDesc>shows the graph of how performance of each category changes with the occlusions and deformations. In-(a) Ours (ASTN) (b) Ours (ASDN) (c) Ours (Full) Changes of APs compared to baseline FRCN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Some of the false positives for our approach. These are top false positives for adversarial training but not the original Fast-RCNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>VOC 2007 test detection average precision (%). FRCN refers to FRCN [7] with our training schedule. method arch mAP aero bike bird boat bottle bus car cat chair cow table dog horse mbike persn plant sheep sofa train tv FRCN [7] AlexNet 55.4 67.2 71.8 51.2 38.7 20.8 65.8 67.7 71.0 28.2 61.2 61.6 62.6 72.0 66.0 54.2 21.8 52.0 53.8 66.4 53.9 FRCN AlexNet 57.0 67.3 72.1 54.0 38.3 24.4 65.7 70.7 66.9 32.4 60.2 63.2 62.5 72.4 67.6 59.2 24.1 53.0 60.6 64.0 61.5 Ours (ASTN) AlexNet 58.1 68.7 73.4 53.9 36.9 26.5 69.4 71.8 68.7 33.0 60.6 64.0 60.9 76.5 70.6 60.9 25.2 55.2 56.9 68.3 59.9 Ours (ASDN) AlexNet 58.5 67.1 72.0 53.4 36.4 25.3 68.5 71.8 70.0 34.7 63.1 64.5 64.3 75.5 70.0 61.5 26.8 55.3 58.2 70.5 60.5</figDesc><table><row><cell>Ours (full)</cell><cell cols="2">AlexNet 58.9 67.6 74.8 53.8 38.2 25.2 69.1 72.4 68.8 34.5 63.0 66.2 63.6 75.0 70.8 61.6 26.9 55.7 57.8 71.7 60.6</cell></row><row><cell>FRCN [7]</cell><cell>VGG</cell><cell>66.9 74.5 78.3 69.2 53.2 36.6 77.3 78.2 82.0 40.7 72.7 67.9 79.6 79.2 73.0 69.0 30.1 65.4 70.2 75.8 65.8</cell></row><row><cell>FRCN</cell><cell>VGG</cell><cell>69.1 75.4 80.8 67.3 59.9 37.6 81.9 80.0 84.5 50.0 77.1 68.2 81.0 82.5 74.3 69.9 28.4 71.1 70.2 75.8 66.6</cell></row><row><cell cols="2">Ours (ASTN) VGG</cell><cell>69.9 73.7 81.5 66.0 53.1 45.2 82.2 79.3 82.7 53.1 75.8 72.3 81.8 81.6 75.6 72.6 36.6 66.3 69.2 76.6 72.7</cell></row><row><cell cols="2">Ours (ASDN) VGG</cell><cell>71.0 74.4 81.3 67.6 57.0 46.6 81.0 79.3 86.0 52.9 75.9 73.7 82.6 83.2 77.7 72.7 37.4 66.3 71.2 78.2 74.3</cell></row><row><cell>Ours (full)</cell><cell>VGG</cell><cell>71.4 75.7 83.6 68.4 58.0 44.7 81.9 80.4 86.3 53.7 76.1 72.5 82.6 83.9 77.1 73.1 38.1 70.0 69.7 78.8 73.1</cell></row><row><cell>FRCN</cell><cell cols="2">ResNet 71.8 78.7 82.2 71.8 55.1 41.7 79.5 80.8 88.5 53.4 81.8 72.1 87.6 85.2 80.0 72.0 35.5 71.6 75.8 78.3 64.3</cell></row><row><cell>Ours (full)</cell><cell cols="2">ResNet 73.6 75.4 83.8 75.1 61.3 44.8 81.9 81.1 87.9 57.9 81.2 72.5 87.6 85.2 80.3 74.7 44.3 72.2 76.7 76.9 71.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>VOC 2007 test detection average precision (%). Ablative analysis on the Adversarial Spatial Dropout Network.FRCN refers to</figDesc><table><row><cell cols="3">FRCN [7] with our training schedule.</cell></row><row><cell>method</cell><cell>arch</cell><cell>mAP aero bike bird boat bottle bus car cat chair cow table dog horse mbike persn plant sheep sofa train tv</cell></row><row><cell>FRCN</cell><cell cols="2">AlexNet 57.0 67.3 72.1 54.0 38.3 24.4 65.7 70.7 66.9 32.4 60.2 63.2 62.5 72.4 67.6 59.2 24.1 53.0 60.6 64.0 61.5</cell></row><row><cell cols="3">Ours (random dropout) AlexNet 57.3 68.6 72.6 52.0 34.7 26.9 64.1 71.3 67.1 33.8 60.3 62.0 62.7 73.5 70.4 59.8 25.7 53.0 58.8 68.6 60.9</cell></row><row><cell>Ours (hard dropout)</cell><cell cols="2">AlexNet 57.7 66.3 72.1 52.8 32.8 24.3 66.8 71.7 69.4 33.4 61.5 62.0 63.4 76.5 69.6 60.6 24.4 56.5 59.1 68.5 62.0</cell></row><row><cell>Ours (fixed ASDN)</cell><cell cols="2">AlexNet 57.5 66.3 72.7 50.4 36.6 24.5 66.4 71.1 68.8 34.7 61.2 64.1 61.9 74.4 69.4 60.4 26.8 55.1 57.2 68.6 60.1</cell></row><row><cell>Ours (joint learning)</cell><cell cols="2">AlexNet 58.5 67.1 72.0 53.4 36.4 25.3 68.5 71.8 70.0 34.7 63.1 64.5 64.3 75.5 70.0 61.5 26.8 55.3 58.2 70.5 60.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>VOC 2012 test detection average precision (%). FRCN refers to FRCN [7] with our training schedule. method arch mAP aero bike bird boat bottle bus car cat chair cow table dog horse mbike persn plant sheep sofa train tv FRCN [7] VGG 65.7 80.3 74.7 66.9 46.9 37.7 73.9 68.6 87.7 41.7 71.1 51.1 86.0 77.8 79.8 69.8 32.1 65.5 63.8 76.4 61.7 FRCN VGG 66.4 81.8 74.4 66.5 47.8 39.3 75.9 69.1 87.4 44.3 73.2 54.0 84.9 79.0 78.0 72.2 33.1 68.0 62.4 76.7 60.8 Ours (full) VGG 69.0 82.2 75.6 69.2 52.0 47.2 76.3 71.2 88.5 46.8 74.0 58.1 85.6 80.3 80.5 74.7 41.5 70.4 62.2 77.4 67.0</figDesc><table><row><cell>bicycle (oth): ov=0.00 1−r=0.30</cell><cell>bird (sim): ov=0.00 1−r=0.54</cell></row><row><cell cols="2">boat (bg): ov=0.00 1−r=0.65 sheep (sim): ov=0.00 1−r=0.46</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement:This work is supported by the Intelligence Advanced</head><p>Research Projects Activity (IARPA) via Department of Interior/ Interior Business Center (DoI/IBC) contract number D16PC00007. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, DoI/IBC, or the U.S. Government. AG was also supported by Sloan Fellowship.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.04143</idno>
		<title level="m">Insideoutside net: Detecting objects in context with skip pooling and recurrent neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>R-Fcn</surname></persName>
		</author>
		<title level="m">Object detection via region-based fully convolutional networks. NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep generative image models using a laplacian pyramid of adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">The pascal visual object classes (voc) challenge. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Object detection via a multiregion and semantic segmentation-aware cnn model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Diagnosing error in object detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chodpathumwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Speed/accuracy trade-offs for modern convolutional object detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Korattikara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<editor>CoRR</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Densebox: Unifying landmark localization with end to end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<editor>CoRR</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<editor>CoRR</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Microsoft COCO: common objects in context. CoRR, abs/1405.0312</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Online batch selection for faster training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06343</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Deep multiscale video prediction beyond mean square error</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno>abs/1511.05440</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Conditional generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<idno>abs/1411.1784</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning to segment object candidates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Supervision via competition: Robot adversaries for learning tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno>abs/1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno>abs/1312.6229</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Action recognition using visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<editor>CoRR</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Contextual priming and feedback for faster r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Training regionbased object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Beyond skip connections: Top-down modulation for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<editor>CoRR</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ferraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6537</idno>
		<title level="m">Fracking deep convolutional image descriptors</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Unsupervised and semi-supervised learning with categorical generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<editor>CoRR</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alemi</surname></persName>
		</author>
		<editor>CoRR</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Takáč</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bijral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Richtárik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1303.2314</idno>
		<title level="m">Minibatch primal and dual methods for svms</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Selective search for object recognition. International Journal of Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smeulders</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations using videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine learning</title>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05431</idno>
		<title level="m">Aggregated residual transformations for deep neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A multipath network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Gated bi-directional cnn for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
