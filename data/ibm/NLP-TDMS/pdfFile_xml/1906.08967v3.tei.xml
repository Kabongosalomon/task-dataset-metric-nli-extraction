<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep RGB-D Canonical Correlation Analysis For Sparse Depth Completion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqi</forename><surname>Zhong</surname></persName>
							<email>yiqizhon@usc.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Ying</forename><surname>Wu</surname></persName>
							<email>choyingw@usc.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suya</forename><surname>You</surname></persName>
							<email>suya.you.civ@mail.mil</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Neumann</surname></persName>
							<email>uneumann@usc.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Southern California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">Army Research Laboratory Playa Vista</orgName>
								<orgName type="institution">US</orgName>
								<address>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">University of Southern California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep RGB-D Canonical Correlation Analysis For Sparse Depth Completion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose our Correlation For Completion Network (CFCNet), an end-to-end deep learning model that uses the correlation between two data sources to perform sparse depth completion. CFCNet learns to capture, to the largest extent, the semantically correlated features between RGB and depth information. Through pairs of image pixels and the visible measurements in a sparse depth map, CFCNet facilitates feature-level mutual transformation of different data sources. Such a transformation enables CFCNet to predict features and reconstruct data of missing depth measurements according to their corresponding, transformed RGB features. We extend canonical correlation analysis to a 2D domain and formulate it as one of our training objectives (i.e. 2d deep canonical correlation, or "2D 2 CCA loss"). Extensive experiments validate the ability and flexibility of our CFCNet compared to the state-of-the-art methods on both indoor and outdoor scenes with different reallife sparse patterns. Codes are available at: https://github.com/choyingw/CFCNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: Sample results of CFCNet on different sparse patterns. We show in the order of RGB images, sparse depth maps with different sparse patterns, and dense depth maps completed by CFCNet. For the stereo pattern and the ORB pattern, we show the depth groundtruth in the last column.</p><p>To accomplish the depth completion task from a novel perspective, we propose an end-to-end deep learning based framework, Correlation For Completion Network (CFCNet). We view a completed dense depth map as composed of two parts. One is the sparse depth which is observable and used as the input, another is non-observable and recovered by the task. Also, the corresponding full RGB image of the depth map can be decomposed into two parts, one is called the sparse RGB, which holds the corresponding RGB values at the observable locations in the sparse depth. The other part is complementary RGB, which is the subtraction of the sparse RGB from the full RGB images. See <ref type="figure" target="#fig_0">Figure 2</ref> for examples. During the training phase, CFCNet learns the relationship between sparse depth and sparse RGB and uses the learned knowledge to recover non-observable depth from complementary RGB.</p><p>To learn the relationship between two modalities, we propose a 2D deep canonical correlation analysis (2D 2 CCA). In the proposed method, our 2D 2 CCA tries to learn non-linear projections where the projected features from RGB and depth domain are maximally correlated. Using 2D 2 CCA as an objective function, we could capture the semantically correlated features from the RGB and depth domain. In this fashion, we utilize the relationship of observable depth and its corresponding nonobservable locations of the RGB input. We then use the joint information learned from the input data pairs to output a dense depth map. The pipeline of our CFCNet is shown in <ref type="figure" target="#fig_0">Figure 2</ref>. Details of our method are described in Section 3. The main contributions of CFCNet can be summarized as follows.</p><p>• Constructing a framework for the sparse depth completion task which leverages the relationship between sparse depth and its corresponding RGB image, using the complementary RGB information to complement the missing sparse depth information. • Proposing the 2D 2 CCA which forces feature encoders to extract the most similar semantics from multiple modalities. Our CFCNet is the first to apply the two-dimensional approach in CCA with deep learning studies. It overcomes the small sample size problem in other CCA based deep learning frameworks on modern computer vision tasks. • Achieving state-of-the-art of the depth completion on several datasets with a variety of sparse patterns that serve real-world settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Sparse Depth Completion is a task that targets at dense depth completion from sparse depth measurements and a corresponding RGB image. The nature of sparse depth measurements varies across scenarios and sensors. Sparse depth generated by the stereo method contains more information on object contours and less information on non-textured areas <ref type="bibr" target="#b10">[11]</ref>. LiDAR sensors produce structured sparsity due to the scanning behavior <ref type="bibr" target="#b11">[12]</ref>. Feature based SLAM systems (such as ORB SLAM <ref type="bibr" target="#b12">[13]</ref>) only capture depth information at the positions of corresponding feature points. Besides these most popular three patterns, some other patterns have also been studied. For instance, <ref type="bibr" target="#b13">[14]</ref> uses a line pattern to simulate partial observations from laser systems; <ref type="bibr" target="#b14">[15]</ref> culls the depth data of shiny surfaces area out of the dense depth map to mimic commodity depth cameras' output. <ref type="bibr" target="#b7">[8]</ref> uses uniform grid patterns. The latter appears a simplified and artificial pattern. Real-life situations require a more practical tool.</p><p>As for input sparsity, <ref type="bibr" target="#b3">[4]</ref> stacks sparse depth maps and corresponding RGB images together to build a four-channel (RGB-D) input before fed into a ResNet based depth estimation network. This treatment produces better results than monocular depth estimation with only RGB images. Other studies involve a two-branch encoder-decoder based framework which is similar to those used in RGB-D segmentation tasks <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref>. Their approaches do not apply special treatments to the sparse depth branch. They work well on the dataset where sparsity is not extremely severe, e.g. KITTI depth completion benchmark <ref type="bibr" target="#b5">[6]</ref>. In most of the two-branch frameworks, features from different sources are extracted independently and fused through direct concatenations or additions, or using features from RGB branch to provide an extra guidance to refine depth prediction results.</p><p>Canonical Correlation Analysis is a standard statistical technique for learning the shared subspace across several original data spaces. For two modalities, from the shared subspace, each representation is the most predictive to the other representation and the most predictable by the other <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>. To overcome the constraints of traditional CCA where the projections must be linear, deep canonical correlation analysis (DCCA) <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref> has been proposed. DCCA uses deep neural network to learn more complex non-linear projections between multiple modalities. CCA, DCCA, and other variants have been widely used on multi-modal representation learning problems <ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref>.</p><p>The one-dimensional CCA method suffers from the singularity problem of covariance matrices in the case of high-dimensional space with small sample size (SSS). Existing works have extended CCA to a two-dimensional way to avoid the SSS problem. <ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref> use a similar approach to building full-rank covariance matrices inspired by 2DPCA <ref type="bibr" target="#b31">[32]</ref> and 2DLDA [33] on the face recognition task. However, those studies do not approximate complex non-linear projections as <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref> attempt. Our CFCNet is the first to integrate two-dimensional CCA into deep learning frameworks to overcome the intrinsic problem of applying DCCA to modern computer vision tasks, detailed in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Approach</head><p>Our goal is to leverage the relationship of the sparse depth and their corresponding pixels in RGB images in order to optimize the performance of the depth completion task. We try to complement the missing depth components using cues from RGB domain. Since CCA could learn the shared subspace with its predictive characteristics, we estimate the missing depth component using features from RGB domain through CCA. However, traditional CCA has SSS problem in modern computer vision task, detailed in Section 3.2. We further propose the 2D 2 CCA to capture similar semantics from both RGB/depth encoders. After encoders learning the semantically similar features, we use a transformer network to transform features from RGB to depth domain. This design not only enables the reconstruction of missing depth features from complementary RGB information but also ensures semantics similarity and the same numerical range of the two data sources. Based on this structure, the decoder in CFCNet is capable of using the reconstructed depth features along with the observable depth features to recover the dense depth map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Network Architecture</head><p>Proposed CFCNet structure is in <ref type="figure" target="#fig_0">Figure 2</ref>. CFCNet takes in sparse depth map, sparse RGB, and complementary RGB. We use our Sparsity-aware Attentional Convolutions (SAConv, as shown in <ref type="figure" target="#fig_1">Figure 3</ref>) in VGG16-like encoders. SAConv is inspired by local attention mask <ref type="bibr" target="#b33">[34]</ref>. Harley et al. <ref type="bibr" target="#b33">[34]</ref> introduces the segmentation-aware mask to let convolution operators "focus" on the signals consistent with the segmentation mask. In order to propagate information from reliable sources, we use sparsity masks to make convolution operations attend on the signals from reliable locations. Difference of our SAConv and the local attention mask is that SAConv does not apply mask normalization. We avoid mask normalization because it affect the stability of our later 2D 2 CCA calculations due to the numerically small extracted features it produces after several times normalization. Also, similar to <ref type="bibr" target="#b5">[6]</ref>, we use maxpooling operation on masks after every SAConv to keep track of the visibility. If there is at least one nonzero value visible to a convolutional kernel, the maxpooling would evaluate the value at the position to 1.</p><p>Most multi-modal deep learning approaches simply concatenate or elementwisely add bottleneck features. However, when the extracted semantics and range of feature value differs among elements, direct concatenation and addition on multi-modal data source would not always yield better performance than single-modal data source, as seen in <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b16">17]</ref>. To avoid this problem. We use encoders to extract higher-level semantics from two branches. We propose 2D 2 CCA, detailed in 3.2, to ensure the extracted features from two branches are maximally correlated. The intuition is that we want to capture the same semantics from the RGB and depth domains. Next, we use a transformer network   to transform extracted features from RGB domain to depth domain, making extracted features from different sources share the same numerical range. During the training phase, we use features of sparse depth and corresponding sparse RGB image to calculate the 2D 2 CCA loss and transformer loss.</p><p>We use a symmetric decoder structure to decode the embedded features. For the input, we concatenate the sparse depth features with the reconstructed missing depth features. The reconstructed missing depth features are extracted from complementary RGB image through the RGB encoder and the transformer. To ensure single-stage training, we adopt weight-sharing strategies as shown in <ref type="figure" target="#fig_0">Figure 2</ref>. The numbers under the heat maps represent the channel numbers. The example demonstrates that the 2D 2 CCA is able to capture similar semantics from different sources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">2D Deep Canonical Correlation Analysis (2D 2 CCA)</head><p>Existing CCA based techniques introduced in Section 2 have limitations in modern computer vision tasks. Since modern computer vision studies usually use very deep networks to extract information from images of relatively large resolution, the batch size is limited by GPU-memory use. Meanwhile, the latent feature representations in networks are high-dimensional, since the batch size is limited, using DCCA with one-dimensional vector representation would lead to SSS problem. Therefore, We propose a novel 2D deep canonical correlation analysis(2D 2 CCA) to overcome the limitations.</p><p>We denote the completed depth map as D with its corresponding RGB image as I. Sparse depth map in the input and the corresponding sparse RGB image are denoted as sD and sI. RGB/Depth encoders are denoted as f I and f D where the parameters of the encoders are denoted as θ I and θ D respectively. As described in Section 3.1, f I and f D use the SAConv to propagate information from reliable points to extract features from sparse inputs. We generate 3D feature grids embedding pair ( F sD ∈ R m×n×C , F sI ∈ R m×n×C ) for each sparse depth map/image pair (sD, sI) by defining F sD = f D (sD; θ D ) and F sI = f I (sI; θ I ). Inside each feature grid pair, there are C feature map pairs F i sD ∈ R m×n , F i sI ∈ R m×n , ∀i &lt; C, and C = 512 in our network. Rather than analyzing the global correlation between any possible pairs of (F i sD , F j sI ), ∀i = j, we analyze the channelwise canonical correlation between the same channel number F i sD , F i sI . This channelwise correlation analysis will result in getting features with similar semantic meanings for each modality, as shown in <ref type="figure" target="#fig_6">Figure 6</ref>, which guides f I to embed more valuable information related to depth completion.</p><p>Using 1-dimensional feature representation would lead to SSS problem in modern deep learning based computer vision task. We introduce the 2-dimensional approach similar to <ref type="bibr" target="#b31">[32]</ref> to generate full-rank covariance matrixΣ sD,sI ∈ R m×n , which is calculated aŝ</p><formula xml:id="formula_0">Σ sD,sI = 1 C C−1 ∑ i=0 F i sD − E [F sD ] F i sI − E [F sI ] T ,<label>(1)</label></formula><p>in which we define E [F] = 1 C ∑ C−1 i=0 F i . Besides, we generate covariance matricesΣ sD (and respectivê Σ sI ) with the regularization constant r 1 and identity matrix I aŝ</p><formula xml:id="formula_1">Σ sD = 1 C C−1 ∑ i=0 F i sD − E [F sD ] F i sD − E [F sD ] T + r 1 I.<label>(2)</label></formula><p>The correlation between F sD and F sI is calculated as</p><formula xml:id="formula_2">corr(F sD , F sI ) = (Σ − 1 2 sD )(Σ sD,sI )(Σ − 1 2 sI ) tr .<label>(3)</label></formula><p>The higher value of corr(F sD , F sI ) represents the higher correlation between two feature blocks. Since corr(F sD , F sI ) is an non-negative scalar, we use −corr(F sD , F sI ) as the optimization objective to guide training of two feature encoders. To compute the gradient of corr(F sD , F sI ) with respect to θ D and θ I , we can compute its gradient with respect to F sD and F sI and then do the back propagation. The detail is showed following. Regarding to the gradient computation, we define M = (Σ </p><formula xml:id="formula_3">∂ corr(F sD , F sI ) ∂ F sI = 1 C (2∇ sDsD F sD + ∇ sDsI F sI ) ,<label>(4)</label></formula><p>where ∇ sDsI =Σ </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Loss Function</head><p>We denote our channelwise 2D 2 CCA loss as L 2D 2 CCA = −corr(F sD , F sI ). We denote the transformed component from sparse RGB to depth domain asF sD . The transformer loss describes the numerical similarity between RGB and depth domain. We use L 2 norm to measure the numerical similarity. Our transformer loss is L trans = F sD −F sD 2 2 .</p><p>We also build another encoder and another transformer network which share weights with the encoder and transformer network for the spare RGB. The input of the encoder is a complementary RGB image. We use features extracted from complementary RGB image to predict features of non-observable depth using transformer network. For the complementary RGB image, we denote the extracted feature and transformed component as F cI andF cD . Later, we concatenate F sD andF cD , both of which are 512-channel. We got an 1024-channel bottleneck feature on depth domain. We pass this bottleneck feature into the decoder described in Section 3.1. The output from the decoder is a completed dense depth mapD. To compare the inconsistency between the groundtruth D gt and the completed depth map, we use pixelwise L 2 norm. Thus our reconstruction loss is L recon = D gt −D 2 2 . Also, since bottleneck features have limited expressiveness, if the sparsity of inputs is severe, e.g. only 0.1% sampled points of the whole resolution, the completed depth maps usually have griding effects. To resolve the griding effects, we introduce the smoothness term as in <ref type="bibr" target="#b35">[36]</ref> into our loss function. L smooth = ∇ 2D 1 , where ∇ 2 denotes the second-order gradients. Our final total loss function with weights becomes L total = L 2D 2 CCA + w t L trans + w r L recon + w s L smooth .</p><p>4 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset and Experiment Details</head><p>Implementation details. We use PyTorch to implement the network. Our encoders are similar to VGG16, without the fully-connected layers. We use ReLU on extracted features after every SAConv operation. Downsampling is applied to both the features and masks in encoders. The transformer network is a 2-layer network, size 3 × 3, stride 1, and 512-dimension, with our SAConv. The decoder is also a VGG16-like network using deconvolution to upsample. We use SGD optimizer. We conclude all the hyperparameter tuning in the supplemental material.</p><p>Datasets. We have done extensive experiments on outdoor scene datasets such as KITTI odometry dataset <ref type="bibr" target="#b11">[12]</ref> and Cityscape depth dataset <ref type="bibr" target="#b36">[37]</ref>, and on indoor scene datasets such as NYUv2 <ref type="bibr" target="#b37">[38]</ref> and SLAM RGBD datasets as ICL_NUM <ref type="bibr" target="#b38">[39]</ref> and TUM <ref type="bibr" target="#b39">[40]</ref>.</p><p>• KITTI dataset. The KITTI dataset contains both RGB and LiDAR measurements, total 22 sequences for autonomous driving use. We use the official split, where 46K images are for training and 46K for testing. We adopt the same settings described in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b40">41]</ref> which drops the upper part of the images and resizes the images to 912×228.</p><p>• Cityscape dataset. The Cityscape dataset contains RGB and depth maps calculated from stereo matching of outdoor scenes. We use the official training/validation dataset split. The training set contains 23K images from 41 sequences and the testing set contains 3 sequences. We center crop the images to the size of 900×335 to avoid the upper sky and lower car logo.</p><p>• NYUv2 dataset. The NYUv2 dataset contains 464 sequences of indoor RGB and depth data using Kinect. We use the official dataset split and follow <ref type="bibr" target="#b3">[4]</ref> to sample 50K images as training data. The testing data contains 654 images.</p><p>• SLAM RGBD dataset. We use the sequences of ICL-NUIM <ref type="bibr" target="#b41">[42]</ref> and TUM RGBD SLAM datasets from stereo camera. <ref type="bibr" target="#b39">[40]</ref>. The former is synthetic, and the latter was acquired with Kinect. We use the same testing sequences as described in <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sparsifiers.</head><p>A sparsifier describes the strategy of sampling the dense/semi-dense depth maps in the dataset to make them become the sparse depth input for the training and evaluation purposes. We define three sparsifiers to simulate different sparse patterns existing in the real-world applications. Uniform sparsifier uniformly samples the dense depth map, simulating the scanning effect caused by LiDAR which is nearly uniform. Stereo sparsifier only samples the depth measurements on the edge or textured objects in the scene to simulate the sparse patterns generated by stereo matching or direct VSLAM. ORB sparsifier only maintains the depth measurements according to the location of ORB features in the corresponding RGB images. ORB sparsifier simulates the output sparse depth map from feature based VSLAM. We set a sample number for uniform and stereo sparsifiers to control the sparsity. Since the ORB feature number varies in different images, we do not predefine a sample number but take all the depth at the ORB feature positions.</p><p>Error metrics. We use the error metrics the same as in most previous works. (1) RMSE: root mean square error (2) MAE: mean absolute error (3) δ i : percentage of predicted pixels where the relative error is within 1.25 i . Most of related works adopt i = 1, 2, 3. RMSE and MAE both measure error in meters in our experiments.</p><p>Ablation studies. To examine the effectiveness of multi-modal approach, we evaluate the network performance using four types of inputs, i.e. (1) dense RGB images; (2) sparse depth; (3) dense RGB image + sparse depth; (4) complementary RGB image + sparse depth. The evaluation results are demonstrated in <ref type="table" target="#tab_0">Table 1</ref>. We could observe that the networks with single-modal input perform worse than those with multi-modal input, which validates our multi-modal design. Besides, we observe that using dense RGB with sparse depth has similar but worse performance than using complementary RGB with sparse depth. The sparse depth inputs are precise. However, if we extract RGB domain features for the locations where we already have precise depth information, it would cause ambiguity thus the performance is worse than using complementary RGB information. We also conduct ablation studies for different loss combinations in our supplementary material on KITTI and NYUv2 dataset.</p><p>Furthermore, we conduct the ablation study with different sparsity on NYUv2 dataset. The stereo sparsifier is used to sample from dense depth maps to generate sparse depth data for training and testing. We show how different sparsity can affect the predicted depth map quality. The results are in <ref type="table" target="#tab_1">Table 2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Outdoor scene -KITTI odometry and Cityscapes</head><p>For KITTI and Cityscapes these two outdoor datasets, we use the uniform sparsifier. For the KITTI dataset, we sample 500 points as sparse depth the same as some previous works. We compare with some state-of-the-art works, <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b43">44]</ref>. We follow the evaluation settings in these works, randomly choose 3000 images to calculate the numerical results. The results are in <ref type="table" target="#tab_2">Table 3</ref>. Next, we conduct experiments using both KITTI and Cityscape datasets. Some monocular depth prediction works use Cityscape dataset for training and KITTI dataset for testing. We choose this setting and use 100 uniformly sampled sparse depth as inputs. The results are shown in <ref type="table" target="#tab_3">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Indoor scene -NYUv2 and SLAM RGBD datasets</head><p>For NYUv2 indoor scene dataset, we use the stereo sparsifier to sample points. We compare to the state-of-the-art <ref type="bibr" target="#b3">[4]</ref> with different sparsity using their publicly released code. The results are shown in <ref type="table" target="#tab_4">Table 5</ref>.   Next, we conduct experiments on SLAM RGBD dataset. We follow the setting in the state-of-the-art, CNN-SLAM <ref type="bibr" target="#b0">[1]</ref>, and do the cross-dataset evaluation. We train the model on NYUv2 using ORB sparsifier and evaluate on the SLAM RGBD dataset. We use the metric in CNN-SLAM, calculating the percentage of accurate estimations. Accurate estimations mean the error is within ±10% of the groundtruth. The results are in <ref type="table" target="#tab_5">Table 6</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we directly analyze the relationship between the sparse depth information and their corresponding pixels in RGB images. To better fuse information, we propose 2D 2 CCA to ensure the most similar semantics are captured from two branches and use the complementary RGB information to complement the missing depth. Extensive experiments using total four outdoor/indoor scene datasets show our results achieve state-of-the-art.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Our network architecture. Here ⊕ is for concatenation operation. The input 0 -1 sparse mask represents the sparse pattern of depth measurements. The complementary mask is complementary to the sparse mask. We separate a full RGB image into a sparse RGB and a complementary RGB by the mask and feed them with masks into networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Our SAConv. The is for Hadamard product. The ⊗ is for convolution. The + is for elementwise addition. The kernel size is 3 × 3 and stride is 1 for both convolution and maxpooling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Samples of sparsified depth maps for experiments with different sparsifiers. (a) The source dense depth map from NYUv2. (b) With uniform sparsifier (500 points). (c) With stereo sparsifier (500 points). (d) With ORB sparsifier.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>The visualizations of F I and F D using an example from NYUv2 dataset. Visuals are heat maps of the extracted features. Brighter color means a larger feature value. The values within a single map were normalized to [0, 1]. (a) The feed-forward heat maps at the first iteration. (b) The feed-forward heat maps after being trained with 10000 iterations. The figure shows the heat maps of the first 6 channels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>decompose M as M = USV T using SVD decomposition. Then we define</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>− 1 2 sD UV TΣ − 1 2 sRGB and ∇ sDsD = − 1 2Σ − 1 2 sD UDU TΣ − 1 2</head><label>22121</label><figDesc>sD . ∂ corr(F sD ,F sI ) ∂ F sDfollows the similar calculations as ∂ corr(F sD ,F sI )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Visual results on KITTI dataset. (a) The RGB image (b) 500 points sparse depth as inputs. (c) Our Completed depth maps. (d) Results from [4].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Ablation study of using different data sources on NYUv2 dataset. Dense RGB means we feed in full RGB images. sD means sparse depth generated by 100-point stereo sparsifier . cRGB means complementary RGB image.</figDesc><table><row><cell>Input data</cell><cell>MAE</cell><cell>RMSE</cell><cell>δ 1</cell><cell>δ 2</cell><cell>δ 3</cell></row><row><cell>Dense RGB</cell><cell>0.576</cell><cell>0.740</cell><cell>63.5</cell><cell>89.0</cell><cell>97.0</cell></row><row><cell>sD(100 pts)</cell><cell>0.524</cell><cell>0.700</cell><cell>68.1</cell><cell>90.2</cell><cell>97.0</cell></row><row><cell>Dense RGB+sD(100 pts)</cell><cell>0.479</cell><cell>0.638</cell><cell>73.0</cell><cell>92.4</cell><cell>97.7</cell></row><row><cell>cRGB+sD(100 pts)</cell><cell>0.473</cell><cell>0.631</cell><cell>72.4</cell><cell>92.6</cell><cell>98.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Ablation study of different sample numbers on NYUv2 using stereo sparsifier.</figDesc><table><row><cell>Sample#</cell><cell>MAE</cell><cell>RMSE</cell><cell>δ 1</cell><cell>δ 2</cell><cell>δ 3</cell></row><row><cell>50</cell><cell>0.547</cell><cell>0.715</cell><cell>65.5</cell><cell>90.1</cell><cell>97.4</cell></row><row><cell>100</cell><cell>0.426</cell><cell>0.580</cell><cell>77.5</cell><cell>94.1</cell><cell>98.4</cell></row><row><cell>200</cell><cell>0.385</cell><cell>0.531</cell><cell>80.9</cell><cell>95.1</cell><cell>98.7</cell></row><row><cell>500</cell><cell>0.342</cell><cell>0.476</cell><cell>83.0</cell><cell>96.1</cell><cell>99.0</cell></row><row><cell>1000</cell><cell>0.290</cell><cell>0.419</cell><cell>87.0</cell><cell>97.0</cell><cell>99.2</cell></row><row><cell>2000</cell><cell>0.242</cell><cell>0.352</cell><cell>91.3</cell><cell>98.2</cell><cell>99.6</cell></row><row><cell>5000</cell><cell>0.222</cell><cell>0.323</cell><cell>93.3</cell><cell>98.9</cell><cell>99.8</cell></row><row><cell>10000</cell><cell>0.151</cell><cell>0.231</cell><cell>96.6</cell><cell>99.5</cell><cell>99.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>500 points sparse depth completion on KITTI dataset.</figDesc><table><row><cell></cell><cell>MAE</cell><cell>RMSE</cell><cell>δ 1</cell><cell>δ 2</cell><cell>δ 3</cell></row><row><cell>Ma et al.[4]</cell><cell>-</cell><cell>3.378</cell><cell>93.5</cell><cell>97.6</cell><cell>98.9</cell></row><row><cell>SPN [43]</cell><cell>-</cell><cell>3.243</cell><cell>94.3</cell><cell>97.8</cell><cell>99.1</cell></row><row><cell>CSPN[41]</cell><cell>-</cell><cell>3.029</cell><cell>95.5</cell><cell>98.0</cell><cell>99.0</cell></row><row><cell>CSPN+UNet[41]</cell><cell>-</cell><cell>2.977</cell><cell>95.7</cell><cell>98.0</cell><cell>99.1</cell></row><row><cell>PnP[44]</cell><cell>1.024</cell><cell>2.975</cell><cell>94.9</cell><cell>98.0</cell><cell>99.0</cell></row><row><cell>CFCNet w/o smoothness</cell><cell>1.233</cell><cell>2.967</cell><cell>94.1</cell><cell>98.1</cell><cell>99.3</cell></row><row><cell>CFCNet w/ smoothness</cell><cell>1.197</cell><cell>2.964</cell><cell>94.0</cell><cell>98.0</cell><cell>99.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Depth evaluation results: cap 50m means only taking the depth that smaller than 50m into consideration when doing the evaluation. CS-&gt;K means we train the network on the Cityscape dataset and we do the evaluation on the KITTI dataset. Comparing methods all train train/test with 100 pts.</figDesc><table><row><cell>Methods</cell><cell>Input</cell><cell>Dataset</cell><cell>RMSE</cell><cell>δ 1</cell><cell>δ 2</cell><cell>δ 3</cell></row><row><cell>Zhou et al. [36]</cell><cell>RGB</cell><cell>CS→K</cell><cell>7.580</cell><cell>57.7</cell><cell>84.0</cell><cell>93.7</cell></row><row><cell>Godard et al. [45]</cell><cell>RGB</cell><cell>CS→K</cell><cell>14.445</cell><cell>5.3</cell><cell>32.6</cell><cell>86.2</cell></row><row><cell>Aleotti et al. [46]</cell><cell>RGB</cell><cell>CS→K</cell><cell>14.051</cell><cell>6.3</cell><cell>39.4</cell><cell>87.6</cell></row><row><cell>CFCNet(50 pts)</cell><cell>RGB+sD</cell><cell>CS →K</cell><cell>7.841</cell><cell>78.3</cell><cell>92.7</cell><cell>97.0</cell></row><row><cell>CFCNet(100 pts)</cell><cell>RGB+sD</cell><cell>CS→K</cell><cell>5.827</cell><cell>82.6</cell><cell>94.7</cell><cell>97.9</cell></row><row><cell>Zhou et al. [36](cap 50m)</cell><cell>RGB</cell><cell>CS→K</cell><cell>6.148</cell><cell>59.0</cell><cell>85.2</cell><cell>94.5</cell></row><row><cell>CFCNet(50 pts, cap 50m)</cell><cell>RGB+sD</cell><cell>CS →K</cell><cell>6.334</cell><cell>79.2</cell><cell>93.2</cell><cell>97.3</cell></row><row><cell>CFCNet(100 pts, cap 50m)</cell><cell>RGB+sD</cell><cell>CS →K</cell><cell>4.524</cell><cell>83.7</cell><cell>95.2</cell><cell>98.1</cell></row><row><cell>CFCNet(50 pts, cap 50m)</cell><cell>RGB+sD</cell><cell>CS→CS</cell><cell>9.019</cell><cell>82.8</cell><cell>94.1</cell><cell>97.2</cell></row><row><cell>CFCNet(100 pts, cap 50m)</cell><cell>RGB+sD</cell><cell>CS→CS</cell><cell>6.887</cell><cell>88.9</cell><cell>96.1</cell><cell>98.1</cell></row><row><cell>CFCNet(100 pts, cap 50m)</cell><cell>RGB+sD</cell><cell>K→K</cell><cell>3.157</cell><cell>91.0</cell><cell>97.1</cell><cell>98.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Comparisons on NYUv2 dataset using stereo sparsifier.</figDesc><table><row><cell>Sample#</cell><cell>Methods</cell><cell>MAE</cell><cell>RMSE</cell><cell>δ 1</cell><cell>δ 2</cell><cell>δ 3</cell></row><row><cell>100</cell><cell>[4]</cell><cell>0.473</cell><cell>0.629</cell><cell>71.5</cell><cell>92.4</cell><cell>98.0</cell></row><row><cell>100</cell><cell>CFCNet</cell><cell>0.426</cell><cell>0.580</cell><cell>77.5</cell><cell>94.1</cell><cell>98.4</cell></row><row><cell>200</cell><cell>[4]</cell><cell>0.451</cell><cell>0.603</cell><cell>73.0</cell><cell>93.5</cell><cell>98.4</cell></row><row><cell>200</cell><cell>CFCNet</cell><cell>0.385</cell><cell>0.531</cell><cell>80.9</cell><cell>95.1</cell><cell>98.7</cell></row><row><cell>500</cell><cell>[4]</cell><cell>0.384</cell><cell>0.529</cell><cell>79.2</cell><cell>94.9</cell><cell>98.6</cell></row><row><cell>500</cell><cell>CFCNet</cell><cell>0.342</cell><cell>0.476</cell><cell>83.0</cell><cell>96.1</cell><cell>99.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Comparison in terms of percentage of correctly estimated depth on two SLAM RGBD datasets, ICL-NUIM and TUM. (TUM/seq1 name: fr3/long office household, TUM/seq2 name: fr3/nostructure texture near withloop, TUM/seq3 name: fr3/structure texture far)</figDesc><table><row><cell>Sequence#</cell><cell>CFCNet</cell><cell>CNN-SLAM[1]</cell><cell>Laina[47]</cell><cell>Remode[48]</cell></row><row><cell>ICL/office0</cell><cell>41.97</cell><cell>19.41</cell><cell>17.19</cell><cell>4.47</cell></row><row><cell>ICL/office1</cell><cell>43.86</cell><cell>29.15</cell><cell>20.83</cell><cell>3.13</cell></row><row><cell>ICL/office2</cell><cell>63.64</cell><cell>37.22</cell><cell>30.63</cell><cell>16.70</cell></row><row><cell>ICL/living0</cell><cell>51.76</cell><cell>12.84</cell><cell>15.00</cell><cell>4.47</cell></row><row><cell>ICL/living1</cell><cell>64.34</cell><cell>13.03</cell><cell>11.44</cell><cell>2.42</cell></row><row><cell>ICL/living2</cell><cell>59.07</cell><cell>26.56</cell><cell>33.01</cell><cell>8.68</cell></row><row><cell>TUM/seq1</cell><cell>54.70</cell><cell>12.47</cell><cell>12.98</cell><cell>9.54</cell></row><row><cell>TUM/seq2</cell><cell>66.30</cell><cell>24.07</cell><cell>15.41</cell><cell>12.65</cell></row><row><cell>TUM/seq3</cell><cell>74.61</cell><cell>27.39</cell><cell>9.45</cell><cell>6.73</cell></row><row><cell>Average</cell><cell>57.81</cell><cell>22.46</cell><cell>18.44</cell><cell>7.64</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cnn-slam: Real-time dense monocular slam with learned depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Tateno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6243" to="6252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Depth-aware cnn for rgb-d segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="135" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Disn: Deep implicit surface network for high-quality single-view 3d reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiangeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duygu</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radomir</forename><surname>Mech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="492" to="502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Sparse-to-dense: Depth prediction from sparse depth samples and a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangchang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sertac</forename><surname>Karaman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Dfusenet: Deep fusion of rgb and sparse depth information for image guided dense depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shreyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ty</forename><surname>Shivakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">W</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camillo</forename><forename type="middle">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.00761</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sparsity invariant cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on 3D Vision (3DV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Deeplidar: Deep surface normal guided depth prediction for outdoor scene from sparse lidar data and single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxiong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingdi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00488</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Estimating depth from rgb and sparse sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilad</forename><surname>Drozdov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="167" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Dense depth posterior (ddp) from single image and sparse range</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.10034</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Dfinenet: Ego-motion estimation and depth refinement from sparse, noisy depth input with rgb guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ty</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">D</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Camillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.06397</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A modern approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Forsyth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision: A Modern Approach</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Orb-slam: a versatile and accurate monocular slam system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raul</forename><surname>Mur-Artal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose Maria Martinez</forename><surname>Montiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">D</forename><surname>Tardos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on robotics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1147" to="1163" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Parse geometry from a line: Monocular depth estimation with partial laser observation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiyi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarath</forename><surname>Kodagoda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5059" to="5066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep depth completion of a single rgb-d image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="175" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Depth coefficients for depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunfei</forename><surname>Saif Imran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Morris</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.05421</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sparse and dense data with cnns: Depth completion and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Jaritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raoul</forename><forename type="middle">De</forename><surname>Charette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilie</forename><surname>Wirbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Perrotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fawzi</forename><surname>Nashashibi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Relations between two sets of variates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harold</forename><surname>Hotelling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="162" to="190" />
		</imprint>
	</monogr>
	<note>in Breakthroughs in statistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">An introduction to multivariate statistical analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theodore</forename><surname>Wilbur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anderson</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1962" />
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep canonical correlation analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Galen</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raman</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Bilmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1247" to="1255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On deep multi-view representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raman</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1083" to="1092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A comprehensive survey on pose-invariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxing</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on intelligent systems and technology</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">37</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Canonical correlation analysis networks for two-view image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weifeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">385</biblScope>
			<biblScope unit="page" from="338" to="352" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unsupervised learning of acoustic features via deep canonical correlation analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raman</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><forename type="middle">A</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4590" to="4594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multi-view deep network for cross-view classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meina</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4847" to="4855" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep correlation for matching images and text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krystian</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3441" to="3450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep multimodal learning for audio-visual speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youssef</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Etienne</forename><surname>Marcheret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaibhava</forename><surname>Goel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2130" to="2134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Image sentiment analysis using latent correlations among visual, textual, and sentiment views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie</forename><surname>Katsurai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shin&amp;apos;ichi</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2837" to="2841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Application of two-dimensional canonical correlation analysis for face image processing and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kukharev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kamenskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition and Image Analysis</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="210" to="219" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">2dcca: A novel method for small sample size face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cai-Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Zhen-Hai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="43" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Two-dimensional canonical correlation analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sun</forename><surname>Ho Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="735" to="738" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Two-dimensional pca: a new approach to appearance-based face representation and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><forename type="middle">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing-Yu</forename><surname>Frangi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A novel statistical linear discriminant analysis for image matrix: twodimensional fisherfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baozong</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Signal Processing</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1419" to="1422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Segmentation-aware convolutional networks using local attention masks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas Kokkinos Adam W</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multimodal deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhan</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="689" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1851" to="1858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="746" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A benchmark for rgb-d visual odometry, 3d reconstruction and slam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Whelan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew J</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and automation (ICRA)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1524" to="1531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A benchmark for the evaluation of rgb-d slam systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolas</forename><surname>Engelhard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfram</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="573" to="580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning depth with convolutional spatial propagation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinjing</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A benchmark for RGB-D visual odometry, 3D reconstruction and SLAM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Whelan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning affinity via spatial propagation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sifei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwei</forename><surname>Shalini De Mello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyu</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1520" to="1530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Plugand-play: Improve depth estimation via sparse data propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsun-Hsuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu-En</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan-Ting</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chen</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Unsupervised monocular depth estimation with left-right consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clément</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="270" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Generative adversarial networks for unsupervised monocular depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filippo</forename><surname>Aleotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Tosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Mattoccia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deeper depth prediction with fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="239" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Remode: Probabilistic, monocular dense reconstruction in real time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matia</forename><surname>Pizzoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Forster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2609" to="2616" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
