<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Taking a Deeper Look at Pedestrians</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hosang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Taking a Deeper Look at Pedestrians</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Using additional data at training time our strongest convnet model is competitive even to detectors that use additional data (optical flow) at test time.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we study the use of convolutional neural networks (convnets) for the task of pedestrian detection. Despite their recent diverse successes, convnets historically underperform compared to other pedestrian detectors. We deliberately omit explicitly modelling the problem into the network (e.g. parts or occlusion modelling) and show that we can reach competitive performance without bells and whistles. In a wide range of experiments we analyse small and big convnets, their architectural choices, parameters, and the influence of different training data, including pretraining on surrogate tasks.</p><p>We present the best convnet detectors on the Caltech and KITTI dataset. On Caltech our convnets reach top performance both for the Caltech1x and Caltech10x training setup. Using additional data at training time our strongest convnet model is competitive even to detectors that use additional data (optical flow) at test time.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In recent years the field of computer vision has seen an explosion of success stories involving convolutional neural networks (convnets). Such architectures currently provide top results for general object classification <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b35">36]</ref>, general object detection <ref type="bibr" target="#b39">[40]</ref>, feature matching <ref type="bibr" target="#b15">[16]</ref>, stereo matching <ref type="bibr" target="#b44">[45]</ref>, scene recognition <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b7">8]</ref>, pose estimation <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b6">7]</ref>, action recognition <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b37">38]</ref> and many other tasks <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b2">3]</ref>. Pedestrian detection is a canonical case of object detection with relevant applications in car safety, surveillance, and robotics. A diverse set of ideas has been explored for this problem <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b4">5]</ref> and established benchmark datasets are available <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b16">17]</ref>. We would like to know if the success of convnets is transferable to the pedestrian detection task.</p><p>Previous work on neural networks for pedestrian detection has relied on special-purpose designs, e.g. handcrafted features, part and occlusion modelling. Although these proposed methods perform ably, current top methods are all based on decision trees learned via Adaboost   <ref type="figure">Figure 1</ref>: Comparison of convnet methods on the Caltech test set (see <ref type="bibr">section 7)</ref>. Our CifarNet and AlexNet results significantly improve over previous convnets, and matches the best reported results (SpatialPooling+, which additionally uses optical flow). <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b43">44]</ref>. In this work we revisit the question, and show that both small and large vanilla convnets can reach top performance on the challenging Caltech pedestrians dataset. We provide extensive experiments regarding the details of training, network parameters, and different proposal methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Related work</head><p>Despite the popularity of the task of pedestrian detection, only few works have applied deep neural networks to this task: we are aware of only six.</p><p>The first paper using convnets for pedestrian detection <ref type="bibr" target="#b36">[37]</ref> focuses on how to handle the limited training data (they use the INRIA dataset, which provides 614 positives and 1218 negative images for training). First, each layer is initialized using a form of convolutional sparse coding, and the entire network is subsequently fine-tuned for the detection task. They propose an architecture that uses features from the last and second last layer for detection. This method is named ConvNet <ref type="bibr" target="#b36">[37]</ref>.</p><p>A different line of work extends a deformable parts model (DPM) <ref type="bibr" target="#b14">[15]</ref> with a stack of Restricted Boltzmann Machines (RBMs) trained to reason about parts and occlusion (DBN-Isol) <ref type="bibr" target="#b29">[30]</ref>. This model was extended to account for person-to-person relations (DBN-Mut) <ref type="bibr" target="#b31">[32]</ref> and finally to jointly optimize all these aspects: JointDeep <ref type="bibr" target="#b30">[31]</ref> jointly optimizes features, parts deformations, occlusions, and person-to-person relations.</p><p>The MultiSDP <ref type="bibr" target="#b45">[46]</ref> network feeds each layer with contextual features computed at different scales around the candidate pedestrian detection. Finally SDN <ref type="bibr" target="#b26">[27]</ref>, the current best performing convnet for pedestrian detection, uses additional "switchable layers" (RBM variants) to automatically learn both low-level features and high-level parts (e.g. "head", "legs", etc.).</p><p>Note that none of the existing papers rely on a "straightforward" convolutional network similar to the original Le-Net <ref type="bibr" target="#b25">[26]</ref> (layers of convolutions, non-linearities, pooling, inner products, and a softmax on top). We will revisit this decision in this paper.</p><p>Object detection Other than pedestrian detection, related convnets have been used for detection of ImageNet <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b38">39]</ref> and Pascal VOC categories <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b1">2]</ref>. The most successful general object detectors are based on variants of the R-CNN framework <ref type="bibr" target="#b18">[19]</ref>. Given an input image, a reduced set of detection proposals is created, and these are then evaluated via a convnet. This is essentially a two-stage cascade sliding window method. See <ref type="bibr" target="#b20">[21]</ref> for a review of recent proposal methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Detection proposals</head><p>The most popular proposal method for generic objects is SelectiveSearch <ref type="bibr" target="#b41">[42]</ref>. The recent review <ref type="bibr" target="#b20">[21]</ref> also points out EdgeBoxes <ref type="bibr" target="#b48">[49]</ref> as a fast and effective method. For pedestrian detection DBN-Isol and DBN-Mut use DPM <ref type="bibr" target="#b14">[15]</ref> for proposals. JointDeep, MultiSDP, and SDN use a HOG+CSS+linear SVM detector (similar to <ref type="bibr" target="#b42">[43]</ref>) for proposals. Only ConvNet <ref type="bibr" target="#b36">[37]</ref> applies a convnet in a sliding fashion.</p><p>Decision forests Most methods proposed for pedestrian detection do not use convnets for detection.</p><p>Leaving aside methods that use optical flow, the current top performing methods (on Caltech and KITTI datasets) are SquaresChnFtrs <ref type="bibr" target="#b4">[5]</ref>, InformedHaar <ref type="bibr" target="#b46">[47]</ref>, SpatialPooling <ref type="bibr" target="#b33">[34]</ref>, LDCF <ref type="bibr" target="#b27">[28]</ref>, and Regionlets <ref type="bibr" target="#b43">[44]</ref>. All of them are boosted decision forests, and can be considered variants of the integral channels features architecture <ref type="bibr" target="#b10">[11]</ref>. Regionlets and SpatialPooling use an large set of features, including HOG, LBP and CSS, while SquaresChnFtrs, InformedHaar, and LDCF build over HOG+LUV. On the Caltech benchmark, the best convnet (SDN) is outperformed by all aforementioned methods. <ref type="bibr" target="#b0">1</ref> Input to convnets It is important to highlight that ConvNet <ref type="bibr" target="#b36">[37]</ref> learns to predict from YUV input pixels, whereas all other methods use additional hand-crafted features. DBN-Isol and DBN-Mut use HOG features as input. MultiSDP uses HOG+CSS features as input.</p><p>JointDeep and SDN uses YUV+Gradients as input (and HOG+CSS for the detection proposals). We will show in our experiments that good performance can be reached using RGB alone, but we also show that more sophisticated inputs systematically improve detection quality. Our data indicates that the antagonism "hand-crafted features versus convnets" is an illusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Contributions</head><p>In this paper we propose to revisit pedestrian detection with convolutional neural networks by carefully exploring the design space (number of layers, filter sizes, etc.), and the critical implementation choices (training data preprocessing, effect of detections proposal, etc.). We show that both small (10 5 parameters) and large (6 · 10 7 parameters) networks can reach good performance when trained from scratch (even when using the same data as previous methods). We also show the benefits of using extended and external data, which leads to the strongest single-frame detector on Caltech. We report the best known performance for a convnet on the challenging Caltech dataset (improving by more than 10 percent points), and the first convnet results on the KITTI dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Training data</head><p>It is well known that for convnets the volume of training data is quite important to reach good performance. Below are the datasets we consider along the paper.</p><p>Caltech The Caltech dataset and its associated benchmark <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b4">5]</ref> is one of the most popular pedestrian detection datasets. It consists of videos captured from a car traversing U.S. streets under good weather conditions. The standard training set in the "Reasonable" setting consists of 4 250 frames with ∼ 2 · 10 3 annotated pedestrians, and the test set covers 4 024 frames with ∼ 1 · 10 3 pedestrians.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Caltech validation set</head><p>In our experiments we also use Caltech training data for validation. For those experiments we use one of the suggested validation splits <ref type="bibr" target="#b11">[12]</ref>: the first five training videos are used for validation training and the sixth training video for validation testing.</p><p>Caltech10x Because the Caltech dataset videos are fully annotated, the amount of training data can be increased by resampling the videos. Inspired by <ref type="bibr" target="#b27">[28]</ref>, we increase the training data tenfold by sampling one out of three frames (instead of one out of thirty frames in the standard setup). This yields ∼ 2 · 10 4 annotated pedestrians for training, extracted from 42 782 frames.</p><p>KITTI The KITTI dataset <ref type="bibr" target="#b16">[17]</ref> consists of videos captured from a car traversing German streets, also under good weather conditions. Although similar in appearance to Caltech, it has been shown to have different statistics (see <ref type="bibr">[5,</ref> supplementary material]). Its training set contains 4 445 pedestrians (4 024 taller than 40 pixels) over 7 481 frames, and its test set 7 518 frames.</p><p>ImageNet, Places In section 5 we will consider using large convnets that can exploit pre-training for surrogate tasks. We consider two such tasks (and their associated datasets), the ImageNet 2012 classification of a thousand object categories <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b18">19]</ref> and the classification of 205 scene categories <ref type="bibr" target="#b47">[48]</ref>. The datasets provide 1.2 · 10 6 and 2.5 · 10 6 annotated images for training, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">From decision forests to neural networks</head><p>Before diving into the experiments, it is worth noting that the proposal method we are using can be converted into a convnet so that the overall system can be seen as a cascade of two neural networks.</p><p>SquaresChnFtrs <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> is a decision forest, where each tree node pools and thresholds information from one out of several feature channels. As mentioned in section 1.1 it is common practice to learn pedestrian detection convnets on handcrafted features, thus the feature channels need not be part of the conversion. In this case, a decision node can be realised using (i) a fully connected layer with constant non-zero weights corresponding to the original pooling region and zero weights elsewhere, (ii) a bias term that applies the threshold, (iii) and a sigmoid non-linearity that yields a decision. A two-layer network is sufficient to model a level-2 decision tree given the three simulated node outputs. Finally, the weighted sum over the tree decisions can be modelled with yet another fully-connected layer.</p><p>The mapping from SquaresChnFtrs to a deep neural network is exact: evaluating the same inputs it will return the exact same outputs. What is special about the resulting network is that it has not been trained by back-propagation, but by Adaboost <ref type="bibr" target="#b5">[6]</ref>. This network already performs better than the best known convnet on Caltech, SDN. Unfortunately, experiments to soften the non-linearities and use back-propagation to fine-tune the model parameters did not show significant improvements.  <ref type="figure">Figure 2</ref>: Illustration of the CifarNet, ∼10 5 parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Vanilla convolutional networks</head><p>In our experience many convnet architectures and training hyper-parameters do not enable effective learning for diverse and challenging tasks. It is thus considered best practice to start exploration from architectures and parameters that are known to work well and progressively adapt it to the task at hand. This is the strategy of the following sections.</p><p>In this section we first consider CifarNet, a small network designed to solve the CIFAR-10 classification problem (10 objects categories, (5 + 1) · 10 5 colour images of 32×32 pixels) <ref type="bibr" target="#b23">[24]</ref>. In section 5 we consider AlexNet, a network that has 600 times more parameters than CifarNet and designed to solve the ILSVRC2012 classification problem (1 000 objects categories, (1.2 + 0.15) · 10 6 colour images of ∼VGA resolution). Both of these networks were introduced in <ref type="bibr" target="#b24">[25]</ref> and are re-implemented in the open source Caffe project <ref type="bibr" target="#b21">[22]</ref> </p><formula xml:id="formula_0">2 .</formula><p>Although pedestrian detection is quite a different task than CIFAR-10, we decide to start our exploration from the CifarNet, which provides fair performance on CIFAR-10. Its architecture is depicted in figure 2, unless otherwise specified we use raw RGB input.</p><p>We first discuss how to use the CifarNet network (section 4.1). This naive approach already improves over the best known convnets (section 4.2). Sections 4.3 and 4.4 explore the design space around CifarNet and further push the detection quality. All models in this section are trained using Caltech data only (see section 2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">How to use CifarNet?</head><p>Given an initial network specification, there are still several design choices that affect the final detection quality. We discuss some of them in the following paragraphs.</p><p>Detection proposals Unless otherwise specified we use the SquaresChnFtrs <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> detector to generate proposals because, at the time of writing, it is the best performing pedestrian detector (on Caltech) with source code available. In <ref type="figure" target="#fig_2">figure 3</ref> we compare SquaresChnFtrs against     EdgeBoxes <ref type="bibr" target="#b48">[49]</ref>, a state of the art class-agnostic proposal method. Using class-specific proposals allows to reduce the number of proposals by three orders of magnitude.</p><formula xml:id="formula_1">Positives Negatives MR GT Random 83.1% GT IoU &lt; 0.5 37.1% GT IoU &lt; 0.3 37.2% GT, IoU &gt; 0.5 IoU &lt; 0.5 42.1% GT, IoU &gt; 0.5 IoU &lt; 0.3 41.3% GT, IoU &gt; 0.75 IoU &lt; 0.5 39.9%</formula><p>Thresholds for positive and negative samples Given both training proposals and ground truth (GT) annotations, we now consider which training label to assign to each proposal. A proposal is considered to be a positive example if it exceeds a certain Intersection-over-Union (IoU) threshold for at least one GT annotation. It is considered negative if it does not exceed a second IoU threshold for any GT annotation, and is ignored otherwise. We find that using GT annotations as positives is beneficial (i.e. not applying significant jitter).</p><p>Model window size A typical choice for pedestrian detectors is a model window of 128 × 64 pixels in which the pedestrian occupies an area of 96 × 48 <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>. It is unclear that this is the ideal input size for convnets. Despite CifarNet being designed to operate over 32×32 pixels, table 2 shows that a model size of 128 × 64 pixels indeed works best. We experimented with other variants (stretching versus cropping, larger context border) with no clear improvement.</p><p>Training batch In a detection setup, training samples are typically highly imbalanced towards the background class. Although in our validation setup the imbalance is limited (see <ref type="table" target="#tab_4">table 3</ref>), we found it beneficial throughout our experiments to enforce a strict ratio of positive to negative examples per batch of the stochastic gradient descend optimisation. The final performance is not sensitive to this parameter as long as some ratio (vs. None) is maintained. We use a ratio of 1 : 5.</p><p>In the supplementary material we detail all other training parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">How far can we get with the CifarNet?</head><p>Given the parameter selection on the validation set from previous sections, how does CifarNet compare to previously reported convnet results on the Caltech test set? <ref type="table" target="#tab_5">In table 4</ref> and figure 1, we see that our naive network right away improves over the best known convnet (30.7% MR versus SDN 37.9% MR).</p><p>To decouple the contribution of our strong SquaresChnFtrs proposals to the CifarNet performance, we also train a CifarNet using the proposal from JointDeep <ref type="bibr" target="#b30">[31]</ref>. When using the same detection proposals at training and test time, the vanilla CifarNet already improves over both custom-designed JointDeep and SDN.</p><p>Our CifarNet results are surprisingly close to the best known pedestrian detector trained on Caltech1x (30.7% MR versus SpatialPooling 29.2% MR <ref type="bibr" target="#b33">[34]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Exploring different architectures</head><p>Encouraged by our initial results, we proceed to explore different parameters for the CifarNet architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Number and size of convolutional filters</head><p>Using the Caltech validation set we perform a swipe of convolutional filter sizes (3×3, 5×5, or 7×7 pixels) and number of filters at each layer <ref type="bibr">(16, 32,</ref>   full table in the supplementary material. We observe that using large filter sizes hurts quality, while the varying the number of filters shows less impact. Although some fluctuation in miss-rate is observed, overall there is no clear trend indicating that a configuration is clearly better than another. Thus, for sake of simplicity, we keep using CifarNet (32-32-64 filters of 5×5 pixel) in the subsequent experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Number and type of layers</head><p>In table 5 we evaluate the effect of changing the number and type of layers, while keeping other CifarNet parameters fix. Besides convolutional layers (CONV) and fully-connected layers (FC), we also consider locally-connected layers (LC) <ref type="bibr" target="#b0">[1]</ref>, and concatenating features across layers (CONCAT23) (used in ConvNet <ref type="bibr" target="#b36">[37]</ref>). None of the considered architecture changes improves over the original three convolutional layers of CifarNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Input channels</head><p>As discussed in section 1.1, the majority of previous convnets for pedestrian detection use gradient and colour fea-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Large convolutional network</head><p>One appealing characteristic of convnets is their ability to scale in size of training data volume. In this section we explore larger networks trained with more data.</p><p>We base our experiments on the R-CNN <ref type="bibr" target="#b18">[19]</ref> approach, which is currently one of the best performer on the Pascal VOC detection task <ref type="bibr" target="#b13">[14]</ref>. We are thus curious to evaluate its performance for pedestrian detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Surrogate tasks for improved detections</head><p>The R-CNN approach ("Regions with CNN features") wraps the large network previously trained for the ImageNet classification task <ref type="bibr" target="#b24">[25]</ref>, which we refer to as AlexNet (see <ref type="figure" target="#fig_3">figure 4</ref>). We also use "AlexNet" as shorthand for "R-CNN with AlexNet" with the distinction made clear by the context. During R-CNN training AlexNet is fine-tuned for the (pedestrian) detection task, and in a second step, the softmax output is replaced by a linear SVM. Unless otherwise specified, we use the default parameters of the open source, Caffe based, R-CNN implementation <ref type="bibr" target="#b2">3</ref> . Like in the previous sections, we use SquaresChnFtrs for detection proposals.</p><p>Pre-training If we only train the top layer SVM, without fine-tuning the lower layers of AlexNet, we obtain 39.8% MR on the Caltech test set. This is already surprisingly close to the best known convnet for the task <ref type="figure">(SDN</ref>   R-CNN recipe for detection (train AlexNet on ImageNet, fine-tune for the task of interest). In table 7 we investigate the influence of the pre-training task by considering AlexNets that have been trained for scene recognition <ref type="bibr" target="#b47">[48]</ref> ("Places", see section 2) and on both Places and ImageNet ("Hybrid"). "Places" provides results close to ImageNet, suggesting that the exact pre-training task is not critical and that there is nothing special about ImageNet.</p><p>Caltech10x Due to the large number of parameters of AlexNet, we consider providing additional training data using Caltech10x for fine-tuning the network (see section 2). Despite the strong correlation across training samples, we do observe further improvement (see <ref type="table" target="#tab_11">table 7</ref>). Interestingly, the bulk of the improvement is due to more pedestrians (Positives10x, uses positives from Caltech10x and negatives from Caltech1x). Our top result, 23.3% MR, makes our AlexNet setup the best reported single-frame detector on Caltech (i.e. no optical flow).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Caltech-only training</head><p>To compare with CifarNet, and to verify whether pretraining is necessary at all, we train AlexNet "from scratch" using solely the Caltech training data. We collect results in table 7.</p><p>Training AlexNet solely on Caltech, yields 32.4% MR, which improves over the proposals (SquaresChnFtrs 34.8% MR) and the previous best known convnet on Caltech (SDN 39.8% MR). Using Caltech10x further improves the performance, down to 27.5% MR.</p><p>Although these numbers are inferior than the ones obtained with ImageNet pre-training (23.3% MR, see table 7), we can get surprisingly competitive results using only pedestrian data despite the 10 7 free parameters of the AlexNet model. AlexNet with Caltech10x is second best known single-frame pedestrian detector on Caltech (best known is LDCF 24.8% MR, which also uses Caltech10x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AlexNet</head><p>Fine  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Additional experiments</head><p>How many layers? So far all experiments use the default parameters of R-CNN. Previous works have reported that, depending on the task, using features from lower AlexNet layers can provide better results <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b2">3]</ref>. <ref type="table" target="#tab_13">Table 8</ref> reports Caltech validation results when training the SVM output layer on top of layers four to seven (see <ref type="figure" target="#fig_3">figure 4)</ref>. We report results when using the default parameters and parameters that have been optimised by grid search (detailed grid search included in supplementary material). We observe a negligible difference between default and optimized parameter (at most 1 percent points  Effect of proposal method When comparing the performance of AlexNet fine-tuned on Caltech1x to the proposal method, we see an improvement of 9 pp (percent points) in miss-rate. In <ref type="table" target="#tab_15">table 9</ref> we study the impact of using weaker or stronger proposals. Both ACF <ref type="bibr" target="#b9">[10]</ref> and SquaresChnFtrs <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> provide source code, allowing us to generate training proposals. Katamari <ref type="bibr" target="#b4">[5]</ref> and SpatialPooling+ <ref type="bibr" target="#b33">[34]</ref> are current top performers on the Caltech dataset, both using optical flow, i.e. additional information at test time. There is a ∼10 pp gap between the detectors ACF, SquaresChnFtrs, and Katamari/SpatialPooling, allowing us to cover different operating points. The results of table 9 indicate that, despite the 10 pp gap, there is no noticeable difference between AlexNet models trained with ACF or SquaresChnFtrs. It is seems that as long as the proposals are not random (see top row of table 1), the obtained quality is rather stable. The results also indicate that the quality improvement from AlexNet saturates around ∼ 22% MR. Using stronger proposals does not lead to further improvement. This means that the discriminative power of our trained AlexNet is on par with the best known models on the Caltech dataset, but does not overtake them.</p><p>KITTI test set In <ref type="figure" target="#fig_4">figure 5</ref> we show performance of the AlexNet in context of the KITTI pedestrian detection benchmark <ref type="bibr" target="#b16">[17]</ref>. The network is pre-trained on ImageNet and fine-tuned using KITTI training data. SquaresChnFtrs reaches 44.4% AP (average precision), which the AlexNet can improve to 46.9% AP. These are the first published results for convnets on the KITTI pedestrian detection dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Error analysis</head><p>Results from the previous section are encouraging, but not as good as could be expected from looking at improvements on Pascal VOC. So what bounds performance? The proposal method? The localization quality of the convnet?</p><p>Looking at the highest scoring false positives paints a picture of localization errors of the proposal method, the R-CNN, and even the ground truth. To quantify this effect we rerun the Caltech evaluation but remove all false positives that touch an annotation. This experiment provides an </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>34.8%</head><p>LDCF <ref type="bibr" target="#b27">[28]</ref> 24.8%</p><p>Katamari <ref type="bibr" target="#b4">[5]</ref> 22.5% SP+: SpatialPooling+ <ref type="bibr" target="#b32">[33]</ref> 21.9%  upper bound on performance when solving localisation issues in detectors and doing perfect non-maximum suppression. We see a surprisingly consistent improvement for all methods of about 2% MR. This means that the intuition we gathered from looking at false positives is wrong and actually almost all of the mistakes that worsen the MR are actually background windows that are mistaken for pedestrians. What is striking about this result is that this is not just the case for our R-CNN experiments on detection proposals but also for methods that are trained as a sliding window detector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Small or big convnet?</head><p>Since we have analysed the CifarNet and AlexNet separately, we compare their performance in this section side by  side. <ref type="table" target="#tab_2">Table 10</ref> shows performance on the Caltech test set for models that have been trained only on Caltech1x and Cal-tech10x. With less training data the CifarNet reaches 30.7% MR, performing 2 percent points better than the AlexNet. On Caltech10x, we find the CifarNet performance improved to 28.4%, while the AlexNet improves to 27.1% MR. The trend confirms the intuition that models with lower capacity saturate earlier when increasing the amount of training data than models with higher capacity. We can also conclude that the AlexNet would profit from better regularisation when training on Caltech1x.</p><p>Timing The runtime during detection is about 3ms per proposal window. This is too slow for sliding window detection, but given a fast proposal method that has high recall with less than 100 windows per image, scoring takes about 300ms per image. In our experience SquaresChnFtrs runs in 2s per image, so proposing detections takes most of the detection time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Takeaways</head><p>Previous work suggests that convnets for pedestrian detection underperform, despite having involved architectures (see <ref type="bibr" target="#b4">[5]</ref> for a survey of pedestrian detection). In this paper we showed that neither has to be the case. We present a wide range of experiments with two off-the-shelf models that reach competitive performance: the small CifarNet and the big AlexNet.</p><p>We present two networks that are trained on Caltech only, which outperform all previously published convnets on Caltech. The CifarNet shows better performance than related work, even when using the same training data as the respective methods (section 4.2). Despite its size, the AlexNet also improves over all convnets even when it is trained on Caltech only (section 5.2).</p><p>We push the state of the art for pedestrian detectors that have been trained on Caltech1x and Caltech10x. The Ci-   farNet is the best single-frame pedestrian detector that has been trained on Caltech1x (section 4.2), while AlexNet is the best single-frame pedestrian detector trained on Cal-tech10x (section 5.2).</p><p>In <ref type="figure" target="#fig_6">figure 6</ref>, we include include all published methods on Caltech into the comparison, which also adds methods that use additional information at test time. The AlexNet that has been pre-trained on ImageNet reaches competitive results to the best published methods, but without using additional information at test time (section 5.1).</p><p>We report first results for convnets on the KITTI pedestrian detection benchmark. The AlexNet improves over the proposal method alone, delivering encouraging results to further push KITTI performance with convnets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>We have presented extensive and systematic experimental evidence on the effectiveness of convnets for pedestrian detection. Compared to previous convnets applied to pedestrian detection our approach avoids custom designs. When using the exact same proposals and training data as previous approaches our "vanilla" networks outperform previous results.</p><p>We have shown that with pre-training on surrogate tasks, convnets can reach top performance on this task. Interestingly we have shown that even without pre-training competitive results can be achieved, and this result is quite insensitive to the model size (from 10 5 to 10 7 parameters). Our experiments also detail which parameters are most critical to achieve top performance. We report the best known res-ults for convnets on both the challenging Caltech and KITTI datasets.</p><p>Our experience with convnets indicate that they show good promise on pedestrian detection, and that reported best practices do transfer to said task. That being said, on this more mature field we do not observe the large improvement seen on datasets such as Pascal VOC and ImageNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. CifarNet training, the devil is in the details</head><p>Training neural networks is sensitive to a large number of parameters, including the learning rate, how the network weights are initialised, the type of regularisation applied to the weights, and the training batch size. It is difficult to isolate the effects of the individual parameters, and the best parameters will largely depend on the specific setup. We report here the parameters we used.</p><p>We train CifarNet via stochastic gradient descent (SGD) with a learning rate of 0.005, a momentum of 0.9, and a batch size of 128. After 60 epochs, we reduce the learning rate by a factor of 0.1 and train for an additional 10 epochs. Reducing the learning rate even further did not improve the classification accuracy. The other learning rate policies we explored yielded inferior performance (e.g. gradually reducing the learning rate each training iteration). Careful tuning of the learning rate while adjusting the batch size was critical.</p><p>Other than the softmax classification loss, the training loss includes a L2 regularisation of the network weights. In the objective function, this regularization term has a weight of 0.005 for all layers but the last one (softmax weights), which receives weight 1. This parameter is referred in Caffe as "weight decay".</p><p>The network weights are initialised by drawing values from a Gaussian distribution with standard deviation σ = 0.01, with the exception of the first layer, for which we set σ = 0.0001. <ref type="table" target="#tab_2">Table 11</ref> shows the detection quality of different variants of CifarNet obtained by changing the number and size of the convolutional filters of each layer. See related section 4.3.1 of the main paper. Since different training rounds have different random initial weights, we train four networks for each parameter set and average the results. We report both mean and standard deviation of the miss rate on our validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Grid search around CifarNet</head><p>We observe that using either too small or too large filter sizes throughout the network hurts quality. The network width also seems to matter, a network too narrow or too wide can negatively impact classification accuracy. All and all the "middle-section" of the table shows only small fluctuations in miss-rate (specially when considering the variance).</p><p>In addition to filter size and layer width, we also experimented with different types of pooling layers (max-pooling versus mean-pooling), see figure 2 of main paper. Other than on the first layer, replacing mean-pooling with maxpooling hurts performance.</p><p>The results of table 2 indicate that there is no set of parameters close to CifarNet with a clear advantage over the default CifarNet parameters. When going too far from Ci-farNet parameters, classification accuracy plunges. <ref type="table" target="#tab_2">Table 12</ref> presents the swipe of parameters used to construct the "Best parameters" entries in table 8 of the main paper. We vary the criterion to select negative samples and the SVM regularization parameters. Defaults are parameters are IoU &lt; 0.5, and C = 10 −3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Grid search for AlexNet</head><p>Overall we notice that neither parameter is very sensitive (1 ∼ 2 percent points fluctuations). When C is far from optimal large degradation is observed (10 per cent points). As seen in table 8 of the main paper the gap between default and tuned parameters is rather small (1 ∼ 2 percent points).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Datasets statistics</head><p>In <ref type="figure">figure 7</ref> we plot the height distribution for pedestrians in Caltech and KITTI training sets. Although the datasets are visually similar, the height distributions are somewhat dissimilar (for reference ImageNet and Pascal distributions are more look alike among each other).</p><p>It was shown in <ref type="bibr" target="#b4">[5]</ref> that models trained in each dataset, do not transfer well across each other (compared to models trained on the smaller INRIA dataset).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Proposals statistics</head><p>In figures 8 and 9 we show statistics of different detectors on the Caltech test set, including the ones we use as proposals in our experiments. These figures complement table 9 of the main paper.</p><p>Our initial experiments indicated that it is important to keep a low number of average proposals per image in order to reduce the false positives rate (post re-scoring). This is in contrast to common practice when using class-agnostic proposal methods, where using more windows is considered better because they provide higher recall <ref type="bibr" target="#b20">[21]</ref>. We filter proposals via a threshold on the detection score.</p><p>As can be seen in figure 8 a recall higher than 90% can be achieved with only ∼3 proposals per image on average (for Intersection-over-Union threshold above 0.5, the evaluation criterion). The average number of proposals per image is quite low because most frames of the Caltech test set do not contain any pedestrian.</p><p>In <ref type="figure" target="#fig_10">figure 9</ref> we show the number of false positives at different overlap levels with the ground truth annotations. The bump around 0.5 IoU, most visible for SpatialPooling and LDCF, is an artefact of the non-maximum suppression method used by each method. Both these method obtain high quality detection, thus they must assign (very) lowscores to these false positives windows. To further improve quality the re-scoring method must do the same.    When using a method for proposals one desires to have high recall with high overlap with the ground truth ( <ref type="figure" target="#fig_8">figure  8)</ref>, as well has having false positives with low overlap with the ground truth ( <ref type="figure" target="#fig_10">figure 9</ref>). False positive proposals overlapping true pedestrians will have pieces of persons, which might confuse the re-scoring classifier. Classifying fully centred persons versus random background is assumed to be easier task.</p><p>In <ref type="table" target="#tab_15">table 9</ref> of the main paper we see that AlexNet reaches top detection quality by improving over LDCF, SquaresChnFtrs, and Katamari.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Recall of ground truth annotations versusIntersection-over-Union threshold on the Caltech test set. The legend indicates the average number of detection proposals per image for each curve. A pedestrian detector (SquaresChnFtrs<ref type="bibr" target="#b4">[5]</ref>) generates much better proposals than a state of the art generic method (EdgeBoxes<ref type="bibr" target="#b48">[49]</ref>).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Illustration of the AlexNet architecture, ∼6 · 10 7 parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>AlexNet over on KITTI test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Comparison of our key results (thick lines) with published methods on Caltech test set. Methods using optical flow are dashed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>12 :Figure 7 :</head><label>127</label><figDesc>Detection quality (MR) as function of the maximal IoU threshold to consider a proposal as negative example and the SVM regularization parameter C. (MR: log-average miss-rate on Caltech validation set) Histogram of pedestrian heights in different datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Recall of ground truth versus IoU threshold, for a selection of detection methods. The curves are cumulative distributions. The detections have been filtered by score to reach ∼3 proposals per image on average (number indicated in the legend).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>Distribution of overlap between false positives and ground truth of those false positives that do overlap with the ground truth. The curves are histogram with coarse IoU bins. Number in the legend indicates the average number of proposals per image (after filtering to reach ∼3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Effect of positive and negative training sets on the detection quality. MR: log-average miss-rate on Caltech validation set. GT: ground truth bounding boxes.</figDesc><table><row><cell>Window size</cell><cell>MR</cell></row><row><cell>32 × 32</cell><cell>50.6%</cell></row><row><cell>64 × 32</cell><cell>48.2%</cell></row><row><cell>128 × 64</cell><cell>39.9%</cell></row><row><cell>128 × 128</cell><cell>49.4%</cell></row><row><cell>227 × 227</cell><cell>54.9%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell>Ratio</cell><cell>MR</cell></row><row><cell cols="2">N one 41.4%</cell></row><row><cell cols="2">1 : 10 40.6%</cell></row><row><cell>1 : 5</cell><cell>39.9%</cell></row><row><cell>1 : 1</cell><cell>39.8%</cell></row><row><cell>Effect of the</cell><cell></cell></row><row><cell>window size on the de-</cell><cell></cell></row><row><cell>tection quality. MR: see</cell><cell></cell></row><row><cell>table 1.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Detection quality as a function of the strictly enforced ratio of positives:negatives in each training batch. None: no ratio enforced.</figDesc><table /><note>MR: see table 1.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>or 64 filters). We include the Detection quality as a function of the method and the proposals used for training and testing (MR: log-average miss-rate on Caltech test set). When using the exact same training data as JointDeep, our vanilla CifarNet already improves over the previous best known convnet on Caltech (SDN).</figDesc><table><row><cell></cell><cell>Method</cell><cell>Proposal</cell><cell>Test MR</cell></row><row><cell cols="2">Proposals of [31]</cell><cell>-</cell><cell>45.5%</cell></row><row><cell cols="2">JointDeep</cell><cell>Proposals of [31]</cell><cell>39.3% [31]</cell></row><row><cell></cell><cell>SDN</cell><cell>Proposals of [31]</cell><cell>37.9% [27]</cell></row><row><cell></cell><cell>CifarNet</cell><cell>Proposals of [31]</cell><cell>36.5%</cell></row><row><cell cols="2">SquaresChnFtrs</cell><cell>-</cell><cell>34.8% [5]</cell></row><row><cell></cell><cell>CifarNet</cell><cell cols="2">SquaresChnFtrs 30 .7 %</cell></row><row><cell># layers</cell><cell>Architecture</cell><cell></cell><cell>MR</cell></row><row><cell></cell><cell cols="3">CONV1 CONV2 CONV3 (CifarNet, fig. 2)</cell><cell>37 .1 %</cell></row><row><cell>3</cell><cell cols="2">CONV1 CONV2 LC</cell><cell>43.2%</cell></row><row><cell></cell><cell cols="2">CONV1 CONV2 FC</cell><cell>47.6%</cell></row><row><cell></cell><cell cols="2">CONV1 CONV2 CONV3 FC</cell><cell>39.6%</cell></row><row><cell>4</cell><cell cols="2">CONV1 CONV2 CONV3 LC CONV1 CONV2 FC1 FC2</cell><cell>40.5% 43.2%</cell></row><row><cell></cell><cell cols="2">CONV1 CONV2 CONV3 CONV4</cell><cell>43.3%</cell></row><row><cell cols="4">DAG CONV1 CONV2 CONV3 CONCAT23 FC</cell><cell>38.4%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Detection quality of different network architectures (MR: log-average miss-rate on Caltech validation set), sorted by number of layers before soft-max. DAG: directed acyclic graph.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Detection quality when changing the input channels network architectures. Results in MR; log-average miss-rate on Caltech validation set. G indicates luminance channel gradient, HOG indicates G plus G spread over six orientation bins (hard-binning). These are the same input channels used by our SquaresChnFtrs proposal method.</figDesc><table /><note>tures as input, instead of raw RGB. In table 6 we evaluate the effect of different input features over CifarNet. It seems that HOG+L channel provide a small advantage over RGB. For purposes of direct comparison with the large net- works, in the next sections we keep using raw RGB as input for our CifarNet experiments. We report the CifarNet test set results in section 6.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc><ref type="bibr" target="#b36">37</ref>.9% MR). When fine-tuning all layers on Caltech, the test set performance increases dramatically, reaching 25.9% MR. This confirms the effectiveness of the general</figDesc><table><row><cell>Filter size</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>11×11×3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>3×3×384</cell><cell>3×3×384</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Image size 227×227×3 Input image Stride 4</cell><cell>3×3 max pool stride 2 55×55×96 contrast norm.</cell><cell>3×3 max pool stride 2 27×27×256 contrast norm.</cell><cell>13×13×384 1</cell><cell>13×13×384 1</cell><cell>stride 2 max pool 3×3 13×13×256</cell><cell>4096 units</cell><cell>4096 units</cell><cell>C class softmax</cell></row><row><cell></cell><cell>5×5×96</cell><cell>3×3×256</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1</cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>27×27×96</cell><cell>13×13×256</cell><cell></cell><cell></cell><cell>6×6×256</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Layer 1</cell><cell>Layer 2</cell><cell>Layer 3</cell><cell>Layer 4</cell><cell>Layer 5</cell><cell>Layer 6</cell><cell>Layer 7</cell><cell>Output</cell></row><row><cell></cell><cell>(Convolution)</cell><cell>(Convolution)</cell><cell>(Convolution)</cell><cell>(Convolution)</cell><cell>(Convolution)</cell><cell>(Fully connected)</cell><cell>(Fully connected)</cell><cell>(Softmax)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table /><note>Detection quality when using different training data in different training stages of the AlexNet: initial train- ing of the convnet, optional fine-tuning of the convnet, and the SVM training. Positives10x: positives from Cal- tech10x and negatives from Caltech1x. Detection proposals provided by SquaresChnFtrs, result included for compar- ison. See section 5.1 and 5.2 for details.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>2% 32.5% 33.4% 42.7% Best 32.0% 31.8% 32.5% 42.4%</figDesc><table><row><cell>Parameters</cell><cell>fc7</cell><cell>fc6</cell><cell>pool5 conv4</cell></row><row><cell>Default</cell><cell>32.</cell><cell></cell><cell></cell></row></table><note>). Results for default parameters exhibit a slight trend of better perform- ance for higher levels. These validation set results indicate that, for pedestrian detection, the R-CNN default parameters are a good choice overall.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc>Detection quality when training the R-CNN SVM over different layers of the finetuned CNN. Results in MR; log-average miss-rate on Caltech validation set. "Best parameters" are found by exhaustive search on the validation set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 9 :</head><label>9</label><figDesc></figDesc><table><row><cell>Architecture</cell><cell>#</cell><cell cols="2">Test MR</cell></row><row><cell>training</cell><cell cols="3">parameters Caltech1x Caltech10x</cell></row><row><cell>CifarNet</cell><cell>∼ 10 5</cell><cell>30.7%</cell><cell>28.4%</cell></row><row><cell>MediumNet</cell><cell>∼ 10 6</cell><cell>−</cell><cell>27.9%</cell></row><row><cell>AlexNet</cell><cell>∼ 10 7</cell><cell>32.4%</cell><cell>27.5%</cell></row><row><cell cols="2">SquaresChnFtrs [5]</cell><cell>34.8%</cell><cell></cell></row></table><note>Effect of proposal methods on detection quality of R-CNN. 1×/10× indicates fine-tuning on Caltech or Cal- tech10x. Test MR: log-average miss rate on Caltech test set. ∆: the improvement in MR of the rescored proposals over the test proposals alone.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 10 :</head><label>10</label><figDesc></figDesc><table /><note>Selection of results (presented in previous sec- tions) when training different networks using Caltech train- ing data only. MR: log-average miss-rate on Caltech test set. See section 6.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head></head><label></label><figDesc>48.4 ± 1.7 44.4 ± 1.0 43.6 ± 0.8 45.1 ± 1.1 45.2 ± 0.7 42.3 ± 1.3 46.6 ± 2.1 45.1 5 , 5 , 5 42.7 ± 4.2 41.1 ± 1.3 39.1 ± 1.0 38.9 ± 1.5 37.8 ± 1.6 38.3 ± 2.5 38.5 ± 1.3 39.5 7, 5, 3 43.3 ± 2.9 38.7 ± 2.4 38.6 ± 2.1 38.8 ± 0.9 40.2 ± 2.0 37.9 ± 1.7 39.7 ± 0.7 39.6 ± 2.5 40.2 ± 0.9 40.8 ± 2.6 38.4 ± 0.9 40.8 ± 1.5 40.0 ± 0.4 41.7 ± 2.5 40.8 ± 2.7 41.6 ± 3.0 43.3 ± 6.1 40.5 ± 2.9 39.8 ± 2.5 47.3 ± 2.5 41.6 ± 2.0 42.5</figDesc><table><row><cell>Sizes</cell><cell># filters</cell><cell>16, 16, 16</cell><cell>32 , 32 , 64</cell><cell>32, 64, 32</cell><cell>64, 32, 32</cell><cell>32, 32, 32</cell><cell>64, 64, 64</cell><cell>64, 32, 16</cell><cell>Mean</cell></row><row><cell cols="2">3, 3, 3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">7, 5, 5 43.5 7, 7, 5 43.5 Mean 44.3</cell><cell>41.2</cell><cell>41.1</cell><cell>40.4</cell><cell>40.8</cell><cell>41.2</cell><cell>41.6</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 11 :</head><label>11</label><figDesc>Detection quality (MR%) as the number of filters per layer (columns) and filter sizes per layer (rows). CifarNet parameters are highlighted in italic. (MR: log-average miss-rate on Caltech validation set). 01% 33.62% 32.30% 32.22% 32.04% 32.42% 32.24% 32.26% 32.40% 0.4 36.01% 33.72% 32.43% 32.09% 32.16% 32.33% 32.23% 32.30% 32.20% 0.5 36.07% 33.90% 32.51% 32.03% 32.18% 32.53% 32.20% 32.28% 33.15% 0.6 36.50% 33.96% 32.43% 32.19% 32.24% 32.45% 32.29% 33.06% 34.61% 0.7 36.55% 34.32% 32.36% 32.05% 32.15% 32.55% 32.82% 33.83% 36.13% 16% 32.49% 32.01% 31.88% 32.03% 32.18% 32.50% 32.40% 32.48% 0.4 37.16% 32.54% 32.07% 31.89% 32.14% 31.92% 32.46% 32.51% 32.56% 0.5 37.41% 32.61% 32.17% 32.07% 32.04% 31.84% 32.57% 33.12% 33.18% 0.6 37.54% 32.68% 32.14% 32.12% 32.22% 31.90% 32.93% 34.02% 35.85% 0.7 38.06% 32.67% 32.10% 31.89% 32.23% 32.32% 33.92% 35.92% 38.72% 37% 36.77% 33.16% 32.75% 32.77% 33.29% 33.37% 34.28% 35.16% 0.4 55.89% 36.82% 33.17% 32.52% 32.82% 33.16% 32.79% 34.12% 35.42% 0.5 56.24% 37.09% 33.21% 32.65% 32.69% 33.14% 33.26% 34.95% 36.39% 0.6 56.68% 37.19% 33.40% 32.66% 32.83% 33.44% 34.17% 35.66% 38.28% 0.7 57.93% 37.60% 33.81% 32.85% 33.27% 34.23% 35.76% 38.98% 42.68% 29% 64.90% 48.26% 44.67% 44.83% 43.66% 42.71% 43.36% 45.48% 0.4 82.29% 65.06% 48.66% 44.69% 44.67% 43.06% 42.41% 42.74% 44.81% 0.5 82.22% 65.23% 48.87% 44.68% 44.34% 42.98% 42.57% 43.30% 44.98% 0.6 82.22% 65.30% 48.69% 44.89% 44.39% 43.63% 42.92% 44.27% 46.35% 0.7 82.39% 65.96% 50.47% 45.62% 45.32% 44.86% 44.84% 46.31% 50.13%</figDesc><table><row><cell>neg overlap</cell><cell>C</cell><cell>10 −6</cell><cell>10 −5.5</cell><cell>10 −5</cell><cell>10 −4.5</cell><cell>10 −4</cell><cell>10 −3.5</cell><cell>10 −3</cell><cell>10 −2.5</cell><cell>10 −2</cell></row><row><cell>0.3</cell><cell></cell><cell cols="4">36.(a) layer fc7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>neg overlap</cell><cell>C</cell><cell>10 −6</cell><cell>10 −5.5</cell><cell>10 −5</cell><cell>10 −4.5</cell><cell>10 −4</cell><cell>10 −3.5</cell><cell>10 −3</cell><cell>10 −2.5</cell><cell>10 −2</cell></row><row><cell>0.3</cell><cell></cell><cell cols="4">37.(b) layer fc6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>neg overlap</cell><cell>C</cell><cell>10 −6</cell><cell>10 −5.5</cell><cell>10 −5</cell><cell>10 −4.5</cell><cell>10 −4</cell><cell>10 −3.5</cell><cell>10 −3</cell><cell>10 −2.5</cell><cell>10 −2</cell></row><row><cell>0.3</cell><cell></cell><cell cols="5">55.(c) layer pool5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>neg overlap</cell><cell>C</cell><cell>10 −6</cell><cell>10 −5.5</cell><cell>10 −5</cell><cell>10 −4.5</cell><cell>10 −4</cell><cell>10 −3.5</cell><cell>10 −3</cell><cell>10 −2.5</cell><cell>10 −2</cell></row><row><cell>0.3</cell><cell></cell><cell cols="5">82.(d) layer conv4</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table</head><label></label><figDesc></figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Regionlets matches SpatialPooling on the KITTI benchmark, and thus by transitivity would improve over SDN on Caltech.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://caffe.berkeleyvision.org</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/rbgirshick/rcnn</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements We thank Shanshan Zhang for the help provided setting up some of the experiments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">DeepFace: Closing the Gap to Human-Level Performance in Face Verification</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Analyzing the performance of multilayer neural networks for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">From generic to specific deep representations for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">arXiv</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Seeking the strongest rigid detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Ten years of pedestrian detection, what have we learned</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, CVRSUAD workshop</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Delalleau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Marcotte</surname></persName>
		</author>
		<title level="m">Convex neural networks. In NIPS</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Articulated pose estimation with image-dependent preference on pairwise relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional neural network-based place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jacobson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Milford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">arXiv</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fast feature pyramids for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Appel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Integral channel features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Pedestrian detection: An evaluation of the state of the art. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Monocular pedestrian detection: Survey and experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Gavrila</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge -a retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Descriptor matching with convolutional neural networks: a comparison to sift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">arXiv</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Survey of pedestrian detection for advanced driver assistance systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Geronimo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Sappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Graf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">How good are detection proposals, really</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno>arXiv, 2014. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Switchable deep network for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Local decorrelation for improved detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deepidnet: multi-stage and deformable deep convolutional neural networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">arXiv</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A discriminative deep model for pedestrian detection with occlusion handling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Joint deep learning for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Single-pedestrian detection aided by multi-pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Pedestrian detection with spatially pooled features and structured ensemble learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paisitkriangkrai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
		<idno>arXiv, 2014. 7</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Strengthening the effectiveness of pedestrian detection with spatially pooled features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paisitkriangkrai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Cnn features off-the-shelf: an astounding baseline for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">arXiv</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge. In arXiv, 2014. 1</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pedestrian detection with unsupervised multi-stage feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">arXiv</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<title level="m">Going deeper with convolutions</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>arXiv</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">New features and insights for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Walk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Majer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Regionlets for generic object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Computing the stereo matching cost with a convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zbontar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">arXiv</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Multi-stage contextual deep learning for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Informed haar-like features improve pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bauckhage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning deep features for scene recognition using places database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
