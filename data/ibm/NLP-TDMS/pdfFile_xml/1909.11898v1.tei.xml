<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fine-tune Bert for DocRED with Two-step Process</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Wang</surname></persName>
							<email>hongwang600@cs.ucsb.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara ‡ LogMeIn</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christfried</forename><surname>Focke</surname></persName>
							<email>christfried.focke@logmein.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara ‡ LogMeIn</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Sylvester</surname></persName>
							<email>rob.sylvester@logmein.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara ‡ LogMeIn</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nilesh</forename><surname>Mishra</surname></persName>
							<email>nilesh.mishra@logmein.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara ‡ LogMeIn</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Wang</surname></persName>
							<email>william@cs.ucsb.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara ‡ LogMeIn</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fine-tune Bert for DocRED with Two-step Process</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Modelling relations between multiple entities has attracted increasing attention recently, and a new dataset called DocRED has been collected in order to accelerate the research on the document-level relation extraction. Current baselines for this task uses BiLSTM to encode the whole document and are trained from scratch. We argue that such simple baselines are not strong enough to model to complex interaction between entities. In this paper, we further apply a pre-trained language model (BERT) to provide a stronger baseline for this task. We also find that solving this task in phases can further improve the performance. The first step is to predict whether or not two entities have a relation, the second step is to predict the specific relation 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The task of relation extraction aims to automatically identify relationships between entities. It has been proven to be essential for many downstream applications such as question answering <ref type="bibr" target="#b5">(Yih et al., 2015;</ref><ref type="bibr" target="#b6">Yu et al., 2017)</ref>. Previous research <ref type="bibr">(Socher et al., 2012;</ref><ref type="bibr" target="#b8">Zeng et al., 2014a</ref><ref type="bibr" target="#b7">Zeng et al., , 2015</ref><ref type="bibr">dos Santos et al., 2015;</ref><ref type="bibr" target="#b3">Xiao and Liu, 2016;</ref><ref type="bibr" target="#b0">Cai et al., 2016;</ref><ref type="bibr">Lin et al., 2016;</ref><ref type="bibr" target="#b2">Wu et al., 2017;</ref><ref type="bibr">Qin et al., 2018;</ref><ref type="bibr">Han et al., 2018;</ref><ref type="bibr" target="#b1">Wang et al., 2019)</ref> on relation extraction mainly focuses on sentence-level, i.e., predicting the relation for entities in a given sentence. Recently the largescale document-level relation extraction dataset DocRED <ref type="bibr" target="#b4">(Yao et al., 2019)</ref> was published, which requires the model to predict a relation for every pair of entities in a document. This setting is more challenging, since a large number of relational facts are expressed across multiple sen-tences, and modeling of complex interactions between entities is required.</p><p>In <ref type="bibr" target="#b4">(Yao et al., 2019)</ref> several baselines for the document-level Relation Extraction (RE) task are presented. The best model uses a BiLSTM <ref type="bibr">(Hochreiter and Schmidhuber, 1997)</ref> to encode the whole document, entities are represented by their average word embedding. A BiLinear layer is then applied to predict the relation for a given entity pair. However, we argue that a pre-trained language model, such as <ref type="bibr">BERT (Devlin et al., 2019)</ref>, can provide a further boost in performance, since it already captures important language features and may capture some common sense knowledge.</p><p>In this paper, we use BERT to encode the document. A BiLinear layer is applied to predict the relation between entity pairs. We fine-tune the whole model using annotated data in the DocRED dataset, which increases the F1 score by about 2%. We also found that modeling the document-level relation extraction through a two-step process can further improve the performance. The first step is to predict whether a pair of entities has a relation or not. The second step is to predict the specific relation for a given entity pair. Note that the model we use in the second step is trained with pairs that have relations annotated in DocRED.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model</head><p>In this section, we will first introduce the BERT model for document-level RE, then we will explain how to use the two-steps training process to further improve the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">BERT Model</head><p>Let [x 1 , x 2 , · · · , x n ] denote the document input, and [e 1 , e 2 , · · · , e m ] denote the m entities in the document. We use BERT to encode the document as follows:</p><formula xml:id="formula_0">[h 1 , h 2 , · · · , h n ] = BERT([x 1 , x 2 , · · · , x n ]),</formula><p>from which we can get the embeddings [h e 1 , h e 2 , · · · , h em ].</p><p>Then for each pair of entities (e i , e j ), we use BiLinear layer to predict its relation:</p><formula xml:id="formula_1">r i,j = BILINEAR(h e i , h e j ).</formula><p>The whole model structure is represented in <ref type="figure" target="#fig_0">Figure  1</ref>. We use BERT-base in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Two-step Training Process</head><p>In the DocRED dataset, there is no relation for most entity pairs, which causes a large label imbalance, i.e., most entity pairs belong to the N/A relation. To alleviate this problem, we use a twostep training process.</p><p>In the first step, we only identify whether or not there exists a relation between a given entity pair, i.e., we simplify the problem to a binary classification problem. We use BERT for this step as mentioned above, where all of the annotated data is used to train the model. Sub-sampling is applied to balance relational and N/A pairs in each batch.</p><p>In the second step, we learn a model to identify the specific relationship between a given pair of entities. The model structure is the same BERT model as in the first step. The difference lies in the training data and labels: We only use these relational facts (i.e., entity pairs with relations) to train the model, so that the model can learn to distinguish between these different relations. Empirically we found that the second step is relatively easy, as we achieve about 90% accuracy. The bottleneck of the problem lies in the first step, which is to distinguish whether there is a relation or not.</p><p>After the two-step training, the testing process is straight forward. For a given pair of entities, the model from the first step is first applied to predict whether there is a relation between them. If it predicts a relation, then the model from the second step is applied to predict a specific relation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>In this section, we will introduce the DocRED dataset, our implementation details of the BERT model, and the experimental results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">DocRED Dataset</head><p>The DocRED dataset is collected through distant supervision (Mintz et al., 2009) on Wikipedia documents and Wikidata. The named entity recognition is performed first on each document. Then the identified entities are linked to Wikidata items, and entities with same Knowledge Base (KB) ID are merged. Finally, the relations for entity pairs are obtained by querying Wikidata. Further processing, like named entity and coreference annotation, entity linking, and relation and supporting evidence collection, is conducted based on the collected distantly supervised data. We refer the readers to the original paper <ref type="bibr" target="#b4">(Yao et al., 2019)</ref> for more details.</p><p>The DocRED dataset has wide coverage over a variety of topics. The entity types include person, location, organization, time, number and miscellaneous entity names. The relation types include science, art, time, personal life, etc. In order to be successful on this dataset, a variety of reasoning types are required, including pattern recognition, logical reasoning, coreference reasoning, and common-sense reasoning. The DocRED dataset provides both annotated training data (sampled from collected distantly supervised and humanly labeled data) and distantly supervised data. In our experiments, we only use the annotated data. Statistics about the dataset are listed in <ref type="table" target="#tab_0">Table 1</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation Details</head><p>We use BERT-base in our experiments. The learning rate is set to 10 −5 . The embedding size of BERT model is 768. A transformation layer is used to project the BERT embedding into a low-dimensional space of size 128. In the lowdimension space, a BiLinear layer is applied to predict the relation for a given entity pair.</p><p>In the first step, we set the relation label for all relational instances to be 1, while the label for all N/A relations to be 0. We randomly sample N/A relations at a ratio 3 : 1 within a batch. In the second step, we train a new model using only relational instances, and the specific relation label is kept in this step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head><p>We compare the BERT model with several baselines presented in <ref type="bibr" target="#b4">(Yao et al., 2019)</ref> including a CNN <ref type="bibr" target="#b9">(Zeng et al., 2014b)</ref>, LSTM (Hochreiter and Schmidhuber, 1997), bidirectional LSTM (BiLSTM) <ref type="bibr" target="#b0">(Cai et al., 2016)</ref> and Context-Aware models. The first three models differ from the BERT model in the encoder, i.e., they use CNN, LSTM, and BiLSTM as encoder respectively. Details about Context-Aware model can be found in <ref type="bibr">(Sorokin and Gurevych, 2017)</ref>.</p><p>The main results are presented in <ref type="table" target="#tab_2">Table 2</ref>. We can see that we obtain a 2% F 1 improvement by using the BERT encoder, which indicates that it may contain useful information such as commonsense knowledge in order to solve this task. By using the two-step training process, performance is improved further improve. In our experiments, we find that the accuracy for the second step is above 90%, which means the bottleneck lies in the first step, e.g., predicting whether a relation exists for a given entity pair.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Complex interaction modeling</head><p>To test whether current model can capture the complex interaction between entities, we use a SentModel which encodes the document sentence by sentence. Then we locate each entity within a specific sentence and compute its embedding by averaging the word embedding of the entity name. In this way, there will be no interaction between sentences since we encode the whole document sentence by sentence. We present the results in <ref type="table" target="#tab_4">Table 3</ref>. Surprisingly, the SentModel can achieve very similar performance compared to the BiL-STM model which encodes the whole document as a sequence. Therefore, the current model fails to capture complex interactions among entities, and only local information around each entity is used to predict a relation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion &amp; Discussion</head><p>In this paper, we investigate the usage of BERT for document-level RE. We find that BERT can improve the performance significantly, which we think may benefit from the common sense knowledge learned during pre-training. We also find that using a two-step training process can further improve the performance. The difficulty of this dataset is to distinguish whether there exists a relation between a pair of entities, while identifying a specific relation seems to be less challenging. Another discovery is that current models fail to model complex interaction between entities, which we think is the key to solve the problem of documentlevel RE. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>BERT model for DocRED.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the dataset. # Doc, # Rel, # Inst, #Fact denote the number of documents, the number of relations, the number of relation instances and the number of relational facts respectively.</figDesc><table><row><cell cols="4">Setting # Doc # Rel # Inst</cell><cell>#Fact</cell></row><row><cell>Train</cell><cell>3,053</cell><cell>96</cell><cell cols="2">38,269 34,715</cell></row><row><cell>Dev</cell><cell>1,000</cell><cell>96</cell><cell cols="2">12,332 11,790</cell></row><row><cell>Test</cell><cell>1,000</cell><cell>96</cell><cell cols="2">12,842 12,101</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison of the BERT model with other baselines. We report F 1 score on the Dev and Test set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparison of the BiLSTM baseline withSentModel which encode the document sentence by sentence. We report the F1 score and AUC on the Dev set here.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Cícero Nogueira dos Santos, Bing Xiang, and Bowen Zhou. 2015. Classifying relations by ranking with convolutional neural networks. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, ACL 2015, July 26-31, 2015, Beijing, China, Volume 1: Long Papers, pages 626-634. The Association for Computer Linguistics. Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng. 2012. Semantic compositionality through recursive matrix-vector spaces. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL 2012, July 12-14, 2012, Jeju Island, Korea, pages 1201-1211. ACL. Daniil Sorokin and Iryna Gurevych. 2017. Contextaware representations for knowledge base relation extraction. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, EMNLP 2017, Copenhagen, Denmark, September 9-11, 2017, pages 1784-1789. Association for Computational Linguistics.</figDesc><table><row><cell>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and</cell></row><row><cell>Kristina Toutanova. 2019. BERT: pre-training of</cell></row><row><cell>deep bidirectional transformers for language under-</cell></row><row><cell>standing. In Proceedings of the 2019 Conference</cell></row><row><cell>of the North American Chapter of the Association</cell></row><row><cell>for Computational Linguistics: Human Language</cell></row><row><cell>Technologies, NAACL-HLT 2019, Minneapolis, MN,</cell></row><row><cell>USA, June 2-7, 2019, pages 4171-4186. Association</cell></row><row><cell>for Computational Linguistics.</cell></row><row><cell>Xu Han, Pengfei Yu, Zhiyuan Liu, Maosong Sun, and</cell></row><row><cell>Peng Li. 2018. Hierarchical relation extraction with</cell></row><row><cell>coarse-to-fine grained attention. In Proceedings of</cell></row><row><cell>the 2018 Conference on Empirical Methods in Nat-</cell></row><row><cell>ural Language Processing, Brussels, Belgium, Oc-</cell></row><row><cell>tober 31 -November 4, 2018, pages 2236-2245. As-</cell></row><row><cell>sociation for Computational Linguistics.</cell></row><row><cell>Sepp Hochreiter and Jürgen Schmidhuber. 1997.</cell></row><row><cell>Long short-term memory. Neural Computation,</cell></row><row><cell>9(8):1735-1780.</cell></row><row><cell>Yankai Lin, Shiqi Shen, Zhiyuan Liu, Huanbo Luan,</cell></row><row><cell>and Maosong Sun. 2016. Neural relation extraction</cell></row><row><cell>with selective attention over instances. In Proceed-</cell></row><row><cell>ings of the 54th Annual Meeting of the Association</cell></row><row><cell>for Computational Linguistics, ACL 2016, August 7-</cell></row><row><cell>12, 2016, Berlin, Germany, Volume 1: Long Papers.</cell></row><row><cell>The Association for Computer Linguistics.</cell></row><row><cell>Mike Mintz, Steven Bills, Rion Snow, and Daniel Ju-</cell></row><row><cell>rafsky. 2009. Distant supervision for relation extrac-</cell></row><row><cell>tion without labeled data. In ACL 2009, Proceedings</cell></row><row><cell>of the 47th Annual Meeting of the Association for</cell></row><row><cell>Computational Linguistics and the 4th International</cell></row><row><cell>Joint Conference on Natural Language Processing</cell></row><row><cell>of the AFNLP, 2-7 August 2009, Singapore, pages</cell></row><row><cell>1003-1011. The Association for Computer Linguis-</cell></row><row><cell>tics.</cell></row><row><cell>Pengda Qin, Weiran Xu, and William Yang Wang.</cell></row><row><cell>2018. Robust distant supervision relation extraction</cell></row><row><cell>via deep reinforcement learning. In Proceedings</cell></row><row><cell>of the 56th Annual Meeting of the Association for</cell></row><row><cell>Computational Linguistics, ACL 2018, Melbourne,</cell></row><row><cell>Australia, July 15-20, 2018, Volume 1: Long Pa-</cell></row><row><cell>pers, pages 2137-2147. Association for Computa-</cell></row><row><cell>tional Linguistics.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Code can be found in https://github.com/ hongwang600/DocRed</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent convolutional neural network for relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-08-07" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Long Papers. The Association for Computer Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Extracting multiple-relations in one-pass with pre-trained transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dakuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saloni</forename><surname>Potdar</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1132</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1371" to="1377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adversarial training for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bamman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><forename type="middle">J</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-09-09" />
			<biblScope unit="page" from="1778" to="1783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semantic relation classification via hierarchical recurrent neural network with attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minguang</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING 2016, 26th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers</title>
		<meeting><address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2016-12-11" />
			<biblScope unit="page" from="1254" to="1263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">DocRED: A large-scale document-level relation extraction dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semantic parsing via staged query graph generation: Question answering with knowledge base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gao</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P15-1128</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1321" to="1331" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Improved neural relation detection for knowledge base question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cícero</forename><surname>Kazi Saidul Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1053</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017-07-30" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="571" to="581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction via piecewise convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09-17" />
			<biblScope unit="page" from="1753" to="1762" />
		</imprint>
	</monogr>
	<note>The Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Relation classification via convolutional deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING 2014, 25th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2014-08-23" />
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Relation classification via convolutional deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING 2014, 25th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2014-08-23" />
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
