<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Masked Autoregressive Flow for Density Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papamakarios</surname></persName>
							<email>g.papamakarios@ed.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theo</forename><surname>Pavlakou</surname></persName>
							<email>theo.pavlakou@ed.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Murray</surname></persName>
							<email>i.murray@ed.ac.uk</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Masked Autoregressive Flow for Density Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Autoregressive models are among the best performing neural density estimators. We describe an approach for increasing the flexibility of an autoregressive model, based on modelling the random numbers that the model uses internally when generating data. By constructing a stack of autoregressive models, each modelling the random numbers of the next model in the stack, we obtain a type of normalizing flow suitable for density estimation, which we call Masked Autoregressive Flow. This type of flow is closely related to Inverse Autoregressive Flow and is a generalization of Real NVP. Masked Autoregressive Flow achieves state-of-the-art performance in a range of general-purpose density estimation tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The joint density p(x) of a set of variables x is a central object of interest in machine learning. Being able to access and manipulate p(x) enables a wide range of tasks to be performed, such as inference, prediction, data completion and data generation. As such, the problem of estimating p(x) from a set of examples {x n } is at the core of probabilistic unsupervised learning and generative modelling.</p><p>In recent years, using neural networks for density estimation has been particularly successful. Combining the flexibility and learning capacity of neural networks with prior knowledge about the structure of data to be modelled has led to impressive results in modelling natural images <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42]</ref> and audio data <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b39">40]</ref>. State-of-the-art neural density estimators have also been used for likelihood-free inference from simulated data <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b25">26]</ref>, variational inference <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b26">27]</ref>, and as surrogates for maximum entropy models <ref type="bibr" target="#b21">[22]</ref>.</p><p>Neural density estimators differ from other approaches to generative modelling-such as variational autoencoders <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b27">28]</ref> and generative adversarial networks <ref type="bibr" target="#b9">[10]</ref>-in that they readily provide exact density evaluations. As such, they are more suitable in applications where the focus is on explicitly evaluating densities, rather than generating synthetic data. For instance, density estimators can learn suitable priors for data from large unlabelled datasets, for use in standard Bayesian inference <ref type="bibr" target="#b42">[43]</ref>. In simulation-based likelihood-free inference, conditional density estimators can learn models for the likelihood <ref type="bibr" target="#b6">[7]</ref> or the posterior <ref type="bibr" target="#b25">[26]</ref> from simulated data. Density estimators can learn effective proposals for importance sampling <ref type="bibr" target="#b24">[25]</ref> or sequential Monte Carlo <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b23">24]</ref>; such proposals can be used in probabilistic programming environments to speed up inference <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>. Finally, conditional density estimators can be used as flexible inference networks for amortized variational inference and as part of variational autoencoders <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b27">28]</ref>.</p><p>A challenge in neural density estimation is to construct models that are flexible enough to represent complex densities, but have tractable density functions and learning algorithms. There are mainly two families of neural density estimators that are both flexible and tractable: autoregressive models <ref type="bibr" target="#b38">[39]</ref> and normalizing flows <ref type="bibr" target="#b26">[27]</ref>. Autoregressive models decompose the joint density as a product of conditionals, and model each conditional in turn. Normalizing flows transform a base density (e.g. a standard Gaussian) into the target density by an invertible transformation with tractable Jacobian.</p><p>Our starting point is the realization (as pointed out by Kingma et al. <ref type="bibr" target="#b15">[16]</ref>) that autoregressive models, when used to generate data, correspond to a differentiable transformation of an external source of randomness (typically obtained by random number generators). This transformation has a tractable Jacobian by design, and for certain autoregressive models it is also invertible, hence it precisely corresponds to a normalizing flow. Viewing an autoregressive model as a normalizing flow opens the possibility of increasing its flexibility by stacking multiple models of the same type, by having each model provide the source of randomness for the next model in the stack. The resulting stack of models is a normalizing flow that is more flexible than the original model, and that remains tractable.</p><p>In this paper we present Masked Autoregressive Flow (MAF), which is a particular implementation of the above normalizing flow that uses the Masked Autoencoder for Distribution Estimation (MADE) <ref type="bibr" target="#b8">[9]</ref> as a building block. The use of MADE enables density evaluations without the sequential loop that is typical of autoregressive models, and thus makes MAF fast to evaluate and train on parallel computing architectures such as Graphics Processing Units (GPUs). We show a close theoretical connection between MAF and Inverse Autoregressive Flow (IAF) <ref type="bibr" target="#b15">[16]</ref>, which has been designed for variational inference instead of density estimation, and show that both correspond to generalizations of the successful Real NVP <ref type="bibr" target="#b5">[6]</ref>. We experimentally evaluate MAF on a wide range of datasets, and we demonstrate that MAF outperforms RealNVP and achieves state-of-the-art performance on a variety of general-purpose density estimation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Autoregressive density estimation</head><p>Using the chain rule of probability, any joint density p(x) can be decomposed into a product of one-dimensional conditionals as p(x) = i p(x i | x 1:i−1 ). Autoregressive density estimators <ref type="bibr" target="#b38">[39]</ref> model each conditional p(x i | x 1:i−1 ) as a parametric density, whose parameters are a function of a hidden state h i . In recurrent architectures, h i is a function of the previous hidden state h i−1 and the i th input variable x i . The Real-valued Neural Autoregressive Density Estimator (RNADE) <ref type="bibr" target="#b35">[36]</ref> uses mixtures of Gaussian or Laplace densities for modelling the conditionals, and a simple linear rule for updating the hidden state. More flexible approaches for updating the hidden state are based on Long Short-Term Memory recurrent neural networks <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b41">42]</ref>.</p><p>A drawback of autoregressive models is that they are sensitive to the order of the variables. For example, the order of the variables matters when learning the density of <ref type="figure">Figure 1a</ref> if we assume a model with Gaussian conditionals. As <ref type="figure">Figure 1b</ref> shows, a model with order (x 1 , x 2 ) cannot learn this density, even though the same model with order (x 2 , x 1 ) can represent it perfectly. In practice is it hard to know which of the factorially many orders is the most suitable for the task at hand. Autoregressive models that are trained to work with an order chosen at random have been developed, and the predictions from different orders can then be combined in an ensemble <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b36">37]</ref>. Our approach (Section 3) can use a different order in each layer, and using random orders would also be possible.</p><p>Straightforward recurrent autoregressive models would update a hidden state sequentially for every variable, requiring D sequential computations to compute the probability p(x) of a D-dimensional vector, which is not well-suited for computation on parallel architectures such as GPUs. One way to enable parallel computation is to start with a fully-connected model with D inputs and D outputs, and drop out connections in order to ensure that output i will only be connected to inputs 1, 2, . . . , i−1. Output i can then be interpreted as computing the parameters of the i th conditional p(x i | x 1:i−1 ). By construction, the resulting model will satisfy the autoregressive property, and at the same time it will be able to calculate p(x) efficiently on a GPU. An example of this approach is the Masked Autoencoder for Distribution Estimation (MADE) <ref type="bibr" target="#b8">[9]</ref>, which drops out connections by multiplying the weight matrices of a fully-connected autoencoder with binary masks. Other mechanisms for dropping out connections include masked convolutions <ref type="bibr" target="#b41">[42]</ref> and causal convolutions <ref type="bibr" target="#b39">[40]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Normalizing flows</head><p>A normalizing flow <ref type="bibr" target="#b26">[27]</ref> represents p(x) as an invertible differentiable transformation f of a base density π u (u). That is, x = f (u) where u ∼ π u (u). The base density π u (u) is chosen such that it can be easily evaluated for any input u (a common choice for π u (u) is a standard Gaussian). Under the invertibility assumption for f , the density p(x) can be calculated as  <ref type="figure">Figure 1</ref>: (a) The density to be learnt, defined as p(</p><formula xml:id="formula_0">p(x) = π u f −1 (x) det ∂f −1 ∂x .<label>(1)</label></formula><formula xml:id="formula_1">x 1 , x 2 ) = N (x 2 | 0, 4)N x 1 | 1 4 x 2 2 , 1 . (b)</formula><p>The density learnt by a MADE with order (x 1 , x 2 ) and Gaussian conditionals. Scatter plot shows the train data transformed into random numbers u; the non-Gaussian distribution indicates that the model is a poor fit. (c) Learnt density and transformed train data of a 5 layer MAF with the same order (x 1 , x 2 ).</p><p>In order for Equation (1) to be tractable, the transformation f must be constructed such that (a) it is easy to invert, and (b) the determinant of its Jacobian is easy to compute. An important point is that if transformations f 1 and f 2 have the above properties, then their composition f 1 • f 2 also has these properties. In other words, the transformation f can be made deeper by composing multiple instances of it, and the result will still be a valid normalizing flow.</p><p>There have been various approaches in developing normalizing flows. An early example is Gaussianization <ref type="bibr" target="#b3">[4]</ref>, which is based on successive application of independent component analysis. Enforcing invertibility with nonsingular weight matrices has been proposed <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b28">29]</ref>, however in such approaches calculating the determinant of the Jacobian scales cubicly with data dimensionality in general. Planar/radial flows <ref type="bibr" target="#b26">[27]</ref> and Inverse Autoregressive Flow (IAF) <ref type="bibr" target="#b15">[16]</ref> are models whose Jacobian is tractable by design. However, they were developed primarily for variational inference and are not well-suited for density estimation, as they can only efficiently calculate the density of their own samples and not of externally provided datapoints. The Non-linear Independent Components Estimator (NICE) <ref type="bibr" target="#b4">[5]</ref> and its successor Real NVP <ref type="bibr" target="#b5">[6]</ref> have a tractable Jacobian and are also suitable for density estimation. IAF, NICE and Real NVP are discussed in more detail in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Masked Autoregressive Flow</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Autoregressive models as normalizing flows</head><p>Consider an autoregressive model whose conditionals are parameterized as single Gaussians. That is, the i th conditional is given by</p><formula xml:id="formula_2">p(x i | x 1:i−1 ) = N x i | µ i , (exp α i ) 2</formula><p>where µ i = f µi (x 1:i−1 ) and α i = f αi (x 1:i−1 ). (2)</p><p>In the above, f µi and f αi are unconstrained scalar functions that compute the mean and log standard deviation of the i th conditional given all previous variables. We can generate data from the above model using the following recursion:</p><formula xml:id="formula_3">x i = u i exp α i + µ i where µ i = f µi (x 1:i−1 ), α i = f αi (x 1:i−1 ) and u i ∼ N (0, 1). (3)</formula><p>In the above, u = (u 1 , u 2 , . . . , u I ) is the vector of random numbers the model uses internally to generate data, typically by making calls to a random number generator often called randn().</p><p>Equation <ref type="formula">(</ref>3) provides an alternative characterization of the autoregressive model as a transformation f from the space of random numbers u to the space of data x. That is, we can express the model as x = f (u) where u ∼ N (0, I).</p><p>By construction, f is easily invertible. Given a datapoint x, the random numbers u that were used to generate it are obtained by the following recursion:</p><formula xml:id="formula_4">u i = (x i − µ i ) exp(−α i ) where µ i = f µi (x 1:i−1 ) and α i = f αi (x 1:i−1 ).<label>(4)</label></formula><p>Due to the autoregressive structure, the Jacobian of f −1 is triangular by design, hence its absolute determinant can be easily obtained as follows:</p><formula xml:id="formula_5">det ∂f −1 ∂x = exp − i α i where α i = f αi (x 1:i−1 ).<label>(5)</label></formula><p>It follows that the autoregressive model can be equivalently interpreted as a normalizing flow, whose density p(x) can be obtained by substituting Equations (4) and <ref type="bibr" target="#b4">(5)</ref> into Equation (1). This observation was first pointed out by Kingma et al. <ref type="bibr" target="#b15">[16]</ref>.</p><p>A useful diagnostic for assessing whether an autoregressive model of the above type fits the target density well is to transform the train data {x n } into corresponding random numbers {u n } using Equation <ref type="bibr" target="#b3">(4)</ref>, and assess whether the u i 's come from independent standard normals. If the u i 's do not seem to come from independent standard normals, this is evidence that the model is a bad fit. For instance, <ref type="figure">Figure 1b</ref> shows that the scatter plot of the random numbers associated with the train data can look significantly non-Gaussian if the model fits the target density poorly.</p><p>Here we interpret autoregressive models as a flow, and improve the model fit by stacking multiple instances of the model into a deeper flow. Given autoregressive models M 1 , M 2 , . . . , M K , we model the density of the random numbers u 1 of M 1 with M 2 , model the random numbers u 2 of M 2 with M 3 and so on, finally modelling the random numbers u K of M K with a standard Gaussian. This stacking adds flexibility: for example, <ref type="figure">Figure 1c</ref> demonstrates that a flow of 5 autoregressive models is able to learn multimodal conditionals, even though each model has unimodal conditionals. Stacking has previously been used in a similar way to improve model fit of deep belief nets <ref type="bibr" target="#b11">[12]</ref> and deep mixtures of factor analyzers <ref type="bibr" target="#b31">[32]</ref>.</p><p>We choose to implement the set of functions {f µi , f αi } with masking, following the approach used by MADE <ref type="bibr" target="#b8">[9]</ref>. MADE is a feedforward network that takes x as input and outputs µ i and α i for all i with a single forward pass. The autoregressive property is enforced by multiplying the weight matrices of MADE with suitably constructed binary masks. In other words, we use MADE with Gaussian conditionals as the building layer of our flow. The benefit of using masking is that it enables transforming from data x to random numbers u and thus calculating p(x) in one forward pass through the flow, thus eliminating the need for sequential recursion as in Equation <ref type="formula" target="#formula_4">(4)</ref>. We call this implementation of stacking MADEs into a flow Masked Autoregressive Flow (MAF).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Relationship with Inverse Autoregressive Flow</head><p>Like MAF, Inverse Autoregressive Flow (IAF) <ref type="bibr" target="#b15">[16]</ref> is a normalizing flow which uses MADE as its component layer. Each layer of IAF is defined by the following recursion:</p><formula xml:id="formula_6">x i = u i exp α i + µ i where µ i = f µi (u 1:i−1 ) and α i = f αi (u 1:i−1 ).<label>(6)</label></formula><p>Similarly to MAF, functions {f µi , f αi } are computed using a MADE with Gaussian conditionals. The difference is architectural: in MAF µ i and α i are directly computed from previous data variables x 1:i−1 , whereas in IAF µ i and α i are directly computed from previous random numbers u 1:i−1 .</p><p>The consequence of the above is that MAF and IAF are different models with different computational trade-offs. MAF is capable of calculating the density p(x) of any datapoint x in one pass through the model, however sampling from it requires performing D sequential passes (where D is the dimensionality of x). In contrast, IAF can generate samples and calculate their density with one pass, however calculating the density p(x) of an externally provided datapoint x requires D passes to find the random numbers u associated with x. Hence, the design choice of whether to connect µ i and α i directly to x 1:i−1 (obtaining MAF) or to u 1:i−1 (obtaining IAF) depends on the intended usage. IAF is suitable as a recognition model for stochastic variational inference <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b27">28]</ref>, where it only ever needs to calculate the density of its own samples. In contrast, MAF is more suitable for density estimation, because each example requires only one pass through the model whereas IAF requires D.</p><p>A theoretical equivalence between MAF and IAF is that training a MAF with maximum likelihood corresponds to fitting an implicit IAF to the base density with stochastic variational inference. Let π x (x) be the data density we wish to learn, π u (u) be the base density, and f be the transformation from u to x as implemented by MAF. The density defined by MAF (with added subscript x for disambiguation) is</p><formula xml:id="formula_7">p x (x) = π u f −1 (x) det ∂f −1 ∂x .<label>(7)</label></formula><p>The inverse transformation f −1 from x to u can be seen as describing an implicit IAF with base density π x (x), which defines the following implicit density over the u space:</p><formula xml:id="formula_8">p u (u) = π x (f (u)) det ∂f ∂u .<label>(8)</label></formula><p>Training MAF by maximizing the total log likelihood n log p(x n ) on train data {x n } corresponds to fitting p x (x) to π x (x) by stochastically minimizing D KL (π x (x) p x (x)). In Section A of the appendix, we show that</p><formula xml:id="formula_9">D KL (π x (x) p x (x)) = D KL (p u (u) π u (u)).<label>(9)</label></formula><p>Hence, stochastically minimizing</p><formula xml:id="formula_10">D KL (π x (x) p x (x)) is equivalent to fitting p u (u) to π u (u) by minimizing D KL (p u (u) π u (u)).</formula><p>Since the latter is the loss function used in variational inference, and p u (u) can be seen as an IAF with base density π x (x) and transformation f −1 , it follows that training MAF as a density estimator of π x (x) is equivalent to performing stochastic variational inference with an implicit IAF, where the posterior is taken to be the base density π u (u) and the transformation f −1 implements the reparameterization trick <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b27">28]</ref>. This argument is presented in more detail in Section A of the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Relationship with Real NVP</head><p>Real NVP <ref type="bibr" target="#b5">[6]</ref> (NVP stands for Non Volume Preserving) is a normalizing flow obtained by stacking coupling layers. A coupling layer is an invertible transformation f from random numbers u to data x with a tractable Jacobian, defined by</p><formula xml:id="formula_11">x 1:d = u 1:d x d+1:D = u d+1:D exp α + µ where µ = f µ (u 1:d ) α = f α (u 1:d ).<label>(10)</label></formula><p>In the above, denotes elementwise multiplication, and the exp is applied to each element of α. The transformation copies the first d elements, and scales and shifts the remaining D−d elements, with the amount of scaling and shifting being a function of the first d elements. When stacking coupling layers into a flow, the elements are permuted across layers so that a different set of elements is copied each time. A special case of the coupling layer where α = 0 is used by NICE <ref type="bibr" target="#b4">[5]</ref>.</p><p>We can see that the coupling layer is a special case of both the autoregressive transformation used by MAF in Equation <ref type="formula">(3)</ref>, and the autoregressive transformation used by IAF in Equation <ref type="formula" target="#formula_6">(6)</ref>. Indeed, we can recover the coupling layer from the autoregressive transformation of MAF by setting µ i = α i = 0 for i ≤ d and making µ i and α i functions of only x 1:d for i &gt; d (for IAF we need to make µ i and α i functions of u 1:d instead for i &gt; d). In other words, both MAF and IAF can be seen as more flexible (but different) generalizations of Real NVP, where each element is individually scaled and shifted as a function of all previous elements. The advantage of Real NVP compared to MAF and IAF is that it can both generate data and estimate densities with one forward pass only, whereas MAF would need D passes to generate data and IAF would need D passes to estimate densities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Conditional MAF</head><p>Given a set of example pairs {(x n , y n )}, conditional density estimation is the task of estimating the conditional density p(x | y). Autoregressive modelling extends naturally to conditional density estimation. Each term in the chain rule of probability can be conditioned on side-information y, decomposing any conditional density as p(x | y) = i p(x i | x 1:i−1 , y). Therefore, we can turn any unconditional autoregressive model into a conditional one by augmenting its set of input variables with y and only modelling the conditionals that correspond to x. Any order of the variables can be chosen, as long as y comes before x. In masked autoregressive models, no connections need to be dropped from the y inputs to the rest of the network.</p><p>We can implement a conditional version of MAF by stacking MADEs that were made conditional using the above strategy. That is, in a conditional MAF, the vector y becomes an additional input for every layer. As a special case of MAF, Real NVP can be made conditional in the same way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation and setup</head><p>We systematically evaluate three types of density estimator (MADE, Real NVP and MAF) in terms of density estimation performance on a variety of datasets. Code for reproducing our experiments (which uses Theano <ref type="bibr" target="#b32">[33]</ref>) can be found at https://github.com/gpapamak/maf.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MADE.</head><p>We consider two versions: (a) a MADE with Gaussian conditionals, denoted simply by MADE, and (b) a MADE whose conditionals are each parameterized as a mixture of C Gaussians, denoted by MADE MoG. We used C = 10 in all our experiments. MADE can be seen either as a MADE MoG with C = 1, or as a MAF with only one autoregressive layer. Adding more Gaussian components per conditional or stacking MADEs to form a MAF are two alternative ways of increasing the flexibility of MADE, which we are interested in comparing.</p><p>Real NVP. We consider a general-purpose implementation of the coupling layer, which uses two feedforward neural networks, implementing the scaling function f α and the shifting function f µ respectively. Both networks have the same architecture, except that f α has hyperbolic tangent hidden units, whereas f µ has rectified linear hidden units (we found this combination to perform best). Both networks have a linear output. We consider Real NVPs with either 5 or 10 coupling layers, denoted by Real NVP <ref type="bibr" target="#b4">(5)</ref> and Real NVP (10) respectively, and in both cases the base density is a standard Gaussian. Successive coupling layers alternate between (a) copying the odd-indexed variables and transforming the even-indexed variables, and (b) copying the even-indexed variables and transforming the odd-indexed variables.</p><p>It is important to clarify that this is a general-purpose implementation of Real NVP which is different and thus not comparable to its original version <ref type="bibr" target="#b5">[6]</ref>, which was designed specifically for image data.</p><p>Here we are interested in comparing coupling layers with autoregressive layers as building blocks of normalizing flows for general-purpose density estimation tasks, and our design of Real NVP is such that a fair comparison between the two can be made.</p><p>MAF. We consider three versions: (a) a MAF with 5 autoregressive layers and a standard Gaussian as a base density π u (u), denoted by MAF <ref type="formula" target="#formula_5">(5)</ref> In all experiments, MADE and MADE MoG order the inputs using the order that comes with the dataset by default; no alternative orders were considered. MAF uses the default order for the first autoregressive layer (i.e. the layer that directly models the data) and reverses the order for each successive layer (the same was done for IAF by Kingma et al. <ref type="bibr" target="#b15">[16]</ref>).</p><p>MADE, MADE MoG and each layer in MAF is a feedforward neural network with masked weight matrices, such that the autoregressive property holds. The procedure for designing the masks (due to Germain et al. <ref type="bibr" target="#b8">[9]</ref>) is as follows. Each input or hidden unit is assigned a degree, which is an integer ranging from 1 to D, where D is the data dimensionality. The degree of an input is taken to be its index in the order. The D outputs have degrees that sequentially range from 0 to D −1. A unit is allowed to receive input only from units with lower or equal degree, which enforces the autoregressive property. In order for output i to be connected to all inputs with degree less than i, and thus make sure that no conditional independences are introduced, it is both necessary and sufficient that every hidden layer contains every degree. In all experiments except for CIFAR-10, we sequentially assign degrees within each hidden layer and use enough hidden units to make sure that all degrees appear. Because CIFAR-10 is high-dimensional, we used fewer hidden units than inputs and assigned degrees to hidden units uniformly at random (as was done by Germain et al. <ref type="bibr" target="#b8">[9]</ref>).</p><p>We added batch normalization <ref type="bibr" target="#b12">[13]</ref> after each coupling layer in Real NVP and after each autoregressive layer in MAF. Batch normalization is an elementwise scaling and shifting operation, which is easily invertible and has a tractable Jacobian, and thus it is suitable for use in a normalizing flow. We found that batch normalization in Real NVP and MAF reduces training time, increases stability during training and improves performance (a fact that was also observed by Dinh et al. <ref type="bibr" target="#b5">[6]</ref> for Real NVP). Section B of the appendix discusses our implementation of batch normalization and its use in normalizing flows.</p><p>All models were trained with the Adam optimizer <ref type="bibr" target="#b13">[14]</ref>, using a minibatch size of 100, and a step size of 10 −3 for MADE and MADE MoG, and of 10 −4 for Real NVP and MAF. A small amount of 2 regularization was added, with coefficient 10 −6 . Each model was trained with early stopping until no improvement occurred for 30 consecutive epochs on the validation set. For each model, we selected the number of hidden layers and number of hidden units based on validation performance (we gave the same options to all models), as described in Section D of the appendix. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Unconditional density estimation</head><p>Following Uria et al. <ref type="bibr" target="#b35">[36]</ref>, we perform unconditional density estimation on four UCI datasets (POWER, GAS, HEPMASS, MINIBOONE) and on a dataset of natural image patches (BSDS300).</p><p>UCI datasets. These datasets were taken from the UCI machine learning repository <ref type="bibr" target="#b20">[21]</ref>. We selected different datasets than Uria et al. <ref type="bibr" target="#b35">[36]</ref>, because the ones they used were much smaller, resulting in an expensive cross-validation procedure involving a separate hyperparameter search for each fold. However, our data preprocessing follows Uria et al. <ref type="bibr" target="#b35">[36]</ref>. The sample mean was subtracted from the data and each feature was divided by its sample standard deviation. Discrete-valued attributes were eliminated, as well as every attribute with a Pearson correlation coefficient greater than 0.98. These procedures are meant to avoid trivial high densities, which would make the comparison between approaches hard to interpret. Section D of the appendix gives more details about the UCI datasets and the individual preprocessing done on each of them.</p><p>Image patches. This dataset was obtained by extracting random 8×8 monochrome patches from the BSDS300 dataset of natural images <ref type="bibr" target="#b22">[23]</ref>. We used the same preprocessing as by Uria et al. <ref type="bibr" target="#b35">[36]</ref>. Uniform noise was added to dequantize pixel values, which was then rescaled to be in the range [0, 1]. The mean pixel value was subtracted from each patch, and the bottom-right pixel was discarded.  <ref type="bibr" target="#b36">[37]</ref>. The UCI datasets were used for the first time in the literature for density estimation, so no comparison with existing work can be made yet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Conditional density estimation</head><p>For conditional density estimation, we used the MNIST dataset of handwritten digits <ref type="bibr" target="#b19">[20]</ref> and the CIFAR-10 dataset of natural images <ref type="bibr" target="#b16">[17]</ref>. In both datasets, each datapoint comes from one of 10 distinct classes. We represent the class label as a 10-dimensional, one-hot encoded vector y, and we model the density p(x | y), where x represents an image. At test time, we evaluate the probability of a test image x by p(x) = y p(x | y)p(y), where p(y) = 1 10 is a uniform prior over the labels. For comparison, we also train every model as an unconditional density estimator and report both results.</p><p>For both MNIST and CIFAR-10, we use the same preprocessing as by Dinh et al. <ref type="bibr" target="#b5">[6]</ref>. We dequantize pixel values by adding uniform noise, and then rescale them to [0, 1]. We transform the rescaled pixel values into logit space by x → logit(λ + <ref type="figure" target="#fig_3">(1 − 2λ)</ref>x), where λ = 10 −6 for MNIST and λ = 0.05 for CIFAR-10, and perform density estimation in that space. In the case of CIFAR-10, we also augment the train set with horizontal flips of all train examples (as also done by Dinh et al. <ref type="bibr" target="#b5">[6]</ref>).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>We showed that we can improve MADE by modelling the density of its internal random numbers. Alternatively, MADE can be improved by increasing the flexibility of its conditionals. The comparison between MAF and MADE MoG showed that the best approach is dataset specific; in our experiments MAF outperformed MADE MoG in 5 out of 9 cases, which is strong evidence of its competitiveness. MADE MoG is a universal density approximator; with sufficiently many hidden units and Gaussian components, it can approximate any continuous density arbitrarily well.</p><p>It is an open question whether MAF with a Gaussian base density has a similar property (MAF MoG clearly does).</p><p>We also showed that the coupling layer used in Real NVP is a special case of the autoregressive layer used in MAF. In fact, MAF outperformed Real NVP in all our experiments. Real NVP has achieved impressive performance in image modelling by incorporating knowledge about image structure. Our results suggest that replacing coupling layers with autoregressive layers in the original version of Real NVP is a promising direction for further improving its performance. Real NVP maintains however the advantage over MAF (and autoregressive models in general) that samples from the model can be generated efficiently in parallel.</p><p>Density estimation is one of several types of generative modelling, with the focus on obtaining accurate densities. However, we know that accurate densities do not necessarily imply good performance in other tasks, such as in data generation <ref type="bibr" target="#b34">[35]</ref>. Alternative approaches to generative modelling include variational autoencoders <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b27">28]</ref>, which are capable of efficient inference of their (potentially interpretable) latent space, and generative adversarial networks <ref type="bibr" target="#b9">[10]</ref>, which are capable of high quality data generation. Choice of method should be informed by whether the application at hand calls for accurate densities, latent space inference or high quality samples. Masked Autoregressive Flow is a contribution towards the first of these goals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Equivalence between MAF and IAF</head><p>In this section, we present the equivalence between MAF and IAF in full mathematical detail. Let π x (x) be the true density the train data {x n } is sampled from. Suppose we have a MAF whose base density is π u (u), and whose transformation from u to x is f . The MAF defines the following density over the x space:</p><formula xml:id="formula_12">p x (x) = π u f −1 (x) det ∂f −1 ∂x .<label>(11)</label></formula><p>Using the definition of p x (x) in Equation <ref type="formula" target="#formula_0">(11)</ref>, we can write the Kullback-Leibler divergence from π x (x) to p x (x) as follows:</p><formula xml:id="formula_13">D KL (π x (x) p x (x)) = Eπ x(x) (log π x (x) − log p x (x)) (12) = Eπ x (x) log π x (x) − log π u f −1 (x) − log det ∂f −1 ∂x .<label>(13)</label></formula><p>The inverse transformation f −1 from x to u can be seen as describing an implicit IAF with base density π x (x), which would define the following density over the u space:</p><formula xml:id="formula_14">p u (u) = π x (f (u)) det ∂f ∂u .<label>(14)</label></formula><p>By making the change of variables x → u in Equation <ref type="formula" target="#formula_0">(13)</ref> and using the definition of p u (u) in Equation <ref type="formula" target="#formula_0">(14)</ref> we obtain</p><formula xml:id="formula_15">D KL (π x (x) p x (x)) = Ep u (u) log π x (f (u)) − log π u (u) + log det ∂f ∂u (15) = Ep u (u) (log p u (u) − log π u (u)).<label>(16)</label></formula><p>Equation <ref type="formula" target="#formula_0">(16)</ref> is the definition of the KL divergence from p u (u) to π u (u), hence</p><formula xml:id="formula_16">D KL (π x (x) p x (x)) = D KL (p u (u) π u (u)).<label>(17)</label></formula><p>Suppose now that we wish to fit the implicit density p u (u) to the base density π u (u) by minimizing the above KL. This corresponds exactly to the objective minimized when employing IAF as a recognition network in stochastic variational inference <ref type="bibr" target="#b15">[16]</ref>, where π u (u) would be the (typically intractable) posterior. The first step in stochastic variational inference would be to rewrite the expectation in Equation <ref type="formula" target="#formula_0">(16)</ref> with respect to the base distribution π x (x) used by IAF, which corresponds exactly to Equation <ref type="bibr" target="#b12">(13)</ref>. This is often referred to as the reparameterization trick <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b27">28]</ref>. The second step would be to approximate Equation <ref type="formula" target="#formula_0">(13)</ref> with Monte Carlo, using samples {x n } drawn from π x (x), as follows:</p><formula xml:id="formula_17">D KL (p u (u) π u (u)) = Eπ x (x) log π x (x) − log π u f −1 (x) − log det ∂f −1 ∂x (18) ≈ 1 N n log π x (x n ) − log π u f −1 (x n ) − log det ∂f −1 ∂x .<label>(19)</label></formula><p>Using the definition of p x (x) in Equation <ref type="formula" target="#formula_0">(11)</ref>, we can rewrite Equation <ref type="formula" target="#formula_0">(19)</ref> as</p><formula xml:id="formula_18">1 N n (log π x (x n ) − log p x (x n )) = − 1 N n log p x (x n ) + const.<label>(20)</label></formula><p>Since samples {x n } drawn from π x (x) correspond precisely to the train data for MAF, we can recognize in Equation <ref type="bibr" target="#b19">(20)</ref> the training objective for MAF. In conclusion, training a MAF by maximizing its total log likelihood n log p x (x n ) on train data {x n } is equivalent to variationally training an implicit IAF with MAF's base distribution π u (u) as its target.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Batch normalization</head><p>In our implementation of MAF, we inserted a batch normalization layer <ref type="bibr" target="#b12">[13]</ref> between every two autoregressive layers, and between the last autoregressive layer and the base distribution. We did the same for Real NVP (the original implementation of Real NVP also uses batch normalization layers between coupling layers <ref type="bibr" target="#b5">[6]</ref>). The purpose of a batch normalization layer is to normalize its inputs x to have approximately zero mean and unit variance. In this section, we describe in full detail our implementation of batch normalization and its use as a layer in normalizing flows.</p><p>A batch normalization layer can be thought of as a transformation between two vectors of the same dimensionality. For consistency with our notation for autoregressive and coupling layers, let x be the vector closer to the data, and u be the vector closer to the base distribution. Batch normalization implements the transformation x = f (u) defined by</p><formula xml:id="formula_19">x = (u − β) exp(−γ) (v + ) 1 2 + m.<label>(21)</label></formula><p>In the above, denotes elementwise multiplication. All other operations are to be understood elementwise. The inverse transformation f −1 is given by</p><formula xml:id="formula_20">u = (x − m) (v + ) − 1 2 exp γ + β,<label>(22)</label></formula><p>and the absolute determinant of its Jacobian is</p><formula xml:id="formula_21">det ∂f −1 ∂x = exp i γ i − 1 2 log(v i + ) .<label>(23)</label></formula><p>Vectors β and γ are parameters of the transformation that are learnt during training. In typical implementations of batch normalization, parameter γ is not exponentiated. In our implementation, we chose to exponentiate γ in order to ensure its positivity and simplify the expression of the log absolute determinant. Parameters m and v correspond to the mean and variance of x respectively. During training, we set m and v equal to the sample mean and variance of the current minibatch (we used minibatches of 100 examples). At validation and test time, we set them equal to the sample mean and variance of the entire train set. Other implementations use averages over minibatches <ref type="bibr" target="#b12">[13]</ref> or maintain running averages during training <ref type="bibr" target="#b5">[6]</ref>. Finally, is a hyperparameter that ensures numerical stability if any of the elements of v is near zero. In our experiments, we used = 10 −5 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Number of parameters</head><p>To get a better idea of the computational trade-offs between different model choices versus the performance gains they achieve, we compare the number of parameters for each model. We only count connection weights, as they contribute the most, and ignore biases and batch normalization parameters. We assume that masking reduces the number of connections by approximately half.</p><p>For all models, let D be the number of inputs, H be the number of units in a hidden layer and L be the number of hidden layers. We assume that all hidden layers have the same number of units (as we did in our experiments). For MAF MoG, let C be the number of components per conditional. For Real NVP and MAF, let K be the number of coupling layers/autoregressive layers respectively. <ref type="table" target="#tab_4">Table 3</ref> lists the number of parameters for each model.</p><p>For each extra component we add to MADE MoG, we increase the number of parameters by DH.</p><p>For each extra autoregressive layer we add to MAF, we increase the number of parameters by</p><formula xml:id="formula_22">3 2 DH + 1 2 (L − 1)H 2 .</formula><p>If we have one or two hidden layers L (as we did in our experiments) and assume that D is comparable to H, the number of extra parameters in both cases is about the same. In other words, increasing flexibility by stacking has a parameter cost that is similar to adding more components to the conditionals, as long as the number of hidden layers is small.</p><p>Comparing Real NVP with MAF, we can see that Real NVP has about 1.3 to 2 times more parameters than a MAF of comparable size. Given that our experiments show that Real NVP is less flexible than a MAF of comparable size, we can conclude that MAF makes better use of its available capacity. The number of parameters of Real NVP could be reduced by tying weights between the scaling and shifting networks.  </p><formula xml:id="formula_23">MADE 3 2 DH + 1 2 (L − 1)H 2 MADE MoG C + 1 2 DH + 1 2 (L − 1)H 2 Real NVP 2KDH + 2K(L − 1)H 2 MAF 3 2 KDH + 1 2 K(L − 1)H 2</formula><p>layer in Real NVP contains two feedforward neural networks, one for scaling and one for shifting, each of which also has L hidden layers of H hidden units each. For each dataset, we gave a number of options for L and H (the same options where given to all models) and for each model we selected the option that performed best on the validation set. <ref type="table" target="#tab_5">Table 4</ref> lists the combinations of L and H that were given as options for each dataset.</p><p>In terms of nonlinearity for the hidden units, MADE, MADE MoG and MAF used rectified linear units, except for the GAS datasets where we used hyperbolic tangent units. In the coupling layer of Real NVP, we used hyberbolic tangent hidden units for the scaling network and rectified linear hidden units for the shifting network. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Datasets</head><p>In the following paragraphs, we give a brief description of the four UCI datasets (POWER, GAS, HEPMASS, MINIBOONE) and of the way they were preprocessed.</p><p>POWER. The POWER dataset <ref type="bibr" target="#b0">[1]</ref> contains measurements of electric power consumption in a household over a period of 47 months. It is actually a time series but was treated as if each example were an i.i.d. sample from the marginal distribution. The time feature was turned into an integer for the number of minutes in the day and then uniform random noise was added to it. The date was discarded, along with the global reactive power parameter, which seemed to have many values at exactly zero, which could have caused arbitrarily large spikes in the learnt distribution. Uniform random noise was added to each feature in the interval [0, i ], where i is large enough to ensure that with high probability there are no identical values for the i th feature but small enough to not change the data values significantly.</p><p>GAS. Created by Fonollosa et al. <ref type="bibr" target="#b7">[8]</ref>, this dataset represents the readings of an array of 16 chemical sensors exposed to gas mixtures over a 12 hour period. Similarly to POWER, it is a time series but was treated as if each example were an i.i.d. sample from the marginal distribution. Only the data from the file ethylene_CO.txt was used, which corresponds to a mixture of ethylene and carbon monoxide. After removing strongly correlated attributes, the dimensionality was reduced to 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HEPMASS.</head><p>Used by Baldi et al. <ref type="bibr" target="#b1">[2]</ref>, this dataset describes particle collisions in high energy physics. Half of the data are examples of particle-producing collisions (positive), whereas the rest come from a background source (negative). Here we used the positive examples from the "1000" dataset, where the particle mass is 1000. Five features were removed because they had too many reoccurring values; values that repeat too often can result in spikes in the density and misleading results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MINIBOONE.</head><p>Used by Roe et al. <ref type="bibr" target="#b29">[30]</ref>, this dataset comes from the MiniBooNE experiment at Fermilab. Similarly to HEPMASS, it contains a number of positive examples (electron neutrinos) and a number of negative examples (muon neutrinos). Here we use the positive examples. These had some obvious outliers <ref type="bibr" target="#b10">(11)</ref> which had values at exactly −1000 for every column and were removed. Also, seven of the features had far too high a count for a particular value, e.g. 0.0, so these were removed as well. <ref type="table" target="#tab_6">Table 5</ref> lists the dimensionality and the number of train, validation and test examples for all seven datasets. The first three datasets in <ref type="table" target="#tab_6">Table 5</ref> were subsampled so that the product of the dimensionality and number of examples would be approximately 10M. For the four UCI datasets, 10% of the data was held out and used as test data and 10% of the remaining data was used as validation data. From the BSDS300 dataset we randomly extracted 1M patches for training, 50K patches for validation and 250K patches for testing. For MNIST and CIFAR-10 we held out 10% of the train data for validation. We augmented the CIFAR-10 train set with the horizontal flips of all remaining 45K train examples.  <ref type="bibr" target="#b4">(5)</ref>. To assess whether the differences are statistically significant, we performed a pairwise comparison, which is a more powerful statistical test. In particular, we calculated the difference in test log probability between every other model and MAF MoG (5) on each test example, and assessed whether this difference is significantly positive, which would indicate that MAF MoG (5) performs significantly better. The results of this comparison are shown in <ref type="table" target="#tab_7">Table 6</ref>. We can see that MAF MoG <ref type="formula" target="#formula_5">(5)</ref> is significantly better than all other models except for MAF <ref type="bibr" target="#b4">(5)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Bits per pixel</head><p>In the main text, the results for MNIST and CIFAR-10 were reported in log likelihoods in logit space, since this is the objective that the models were trained to optimize. For comparison with other results in the literature, in <ref type="table" target="#tab_8">Table 7</ref> we report the same results in bits per pixel. For CIFAR-10, different colour components count as different pixels (i.e. an image is thought of as having 32×32×3 pixels).</p><p>In order to calculate bits per pixel, we need to transform the densities returned by a model (which refer to logit space) back to image space in the range [0, 256]. Let x be an image of D pixels in logit space and z be the corresponding image in [0, 256] image space. The transformation from z to x is Real NVP <ref type="formula" target="#formula_5">(5)</ref> 1.87 ± 0.16 Real NVP <ref type="bibr" target="#b9">(10)</ref> 2.15 ± 0.21</p><formula xml:id="formula_24">x = logit λ + (1 − 2λ) z 256 ,<label>(24)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MAF (5)</head><p>0.07 ± 0.11 MAF (10)</p><p>0.55 ± 0.12 MAF MoG <ref type="bibr" target="#b4">(5)</ref> 0.00 ± 0.00 where λ = 10 −6 for MNIST and λ = 0.05 for CIFAR-10. If p(x) is the density in logit space as returned by the model, using the above transformation the density of z can be calculated as</p><formula xml:id="formula_25">p z (z) = p(x) 1 − 2λ 256 D i σ(x i )(1 − σ(x i )) −1 ,<label>(25)</label></formula><p>where σ(·) is the logistic sigmoid function. From that, we can calculate the bits per pixel b(x) of image x as follows:</p><formula xml:id="formula_26">b(x) = − log 2 p z (z) D<label>(26)</label></formula><p>= − log p(x) D log 2 − log 2 (1 − 2λ) + 8 + 1 D i (log 2 σ(x i ) + log 2 (1 − σ(x i ))).</p><p>The above equation was used to convert between the average log likelihoods reported in the main text and the results of <ref type="table" target="#tab_8">Table 7</ref>. The BSDS300 generated images are visually indistinguishable from the real ones. For MNIST and CIFAR-10, generated images lack the fidelity produced by modern image-based generative approaches, such as RealNVP <ref type="bibr" target="#b5">[6]</ref> or PixelCNN++ <ref type="bibr" target="#b30">[31]</ref>. This is because our version of MAF has no knowledge about image structure, as it was designed for general-purpose density estimation and not for realistic-looking image synthesis. However, if the latter is desired, it would be possible to incorporate image modelling techniques in the design of MAF (such as convolutions or a multi-scale architecture as used by Real NVP <ref type="bibr" target="#b5">[6]</ref>) in order to improve quality of generated images. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 Generated images</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>MADE with Gaussian conditionals (c) MAF with 5 layers</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>, (b) a MAF with 10 autoregressive layers and a standard Gaussian as a base density, denoted by MAF (10), and (c) a MAF with 5 autoregressive layers and a MADE MoG with C = 10 Gaussian components in each conditional as a base density, denoted by MAF MoG (5). MAF MoG (5) can be thought of as a MAF (5) stacked on top of a MADE MoG and trained jointly with it.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>D</head><label></label><figDesc>Additional experimental details D.1 Models MADE, MADE MoG and each autoregressive layer in MAF is a feedforward neural network (with masked weight matrices), with L hidden layers of H hidden units each. Similarly, each coupling</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figures 2 ,</head><label>2</label><figDesc>3 and 4  show generated images and real examples for BSDS300, MNIST and CIFAR-10 respectively. Images were generated by MAF MoG (5) for BSDS300, conditional MAF (5) for MNIST, and conditional MAF (10) for CIFAR-10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Generated and real images from BSDS300.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Class-conditional generated and real images from MNIST. Rows are different classes. Generated images are sorted by decreasing log likelihood from left to right.(a) Generated images (b) Real images Class-conditional generated and real images from CIFAR-10. Rows are different classes. Generated images are sorted by decreasing log likelihood from left to right.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Average test log likelihood (in nats) for unconditional density estimation. The best performing model for each dataset is shown in bold (multiple models are highlighted if the difference is not statistically significant according to a paired t-test). Error bars correspond to 2 standard deviations. ± 0.02 −3.58 ± 0.75 −27.93 ± 0.02 −37.24 ± 1.07 96.67 ± 0.25 MADE −3.08 ± 0.03 3.56 ± 0.04 −20.98 ± 0.02 −15.59 ± 0.50 148.85 ± 0.28 MADE MoG 0.40 ± 0.01 8.47 ± 0.02 −15.15 ± 0.02 −12.27 ± 0.47 153.71 ± 0.28 Real NVP (5) −0.02 ± 0.01 4.78 ± 1.80 −19.62 ± 0.02 −13.55 ± 0.49 152.97 ± 0.28 Real NVP (10) 0.17 ± 0.01 8.33 ± 0.14 −18.71 ± 0.02 −13.84 ± 0.52 153.28 ± 1.78 ± 0.02 −17.70 ± 0.02 −11.75 ± 0.44 155.69 ± 0.28 MAF (10) 0.24 ± 0.01 10.08 ± 0.02 −17.73 ± 0.02 −12.24 ± 0.45 154.93 ± 0.28 MAF MoG (5) 0.30 ± 0.01 9.59 ± 0.02 −17.39 ± 0.02 −11.68 ± 0.44 156.36 ± 0.28</figDesc><table><row><cell>POWER</cell><cell>GAS</cell><cell>HEPMASS</cell><cell>MINIBOONE</cell><cell>BSDS300</cell></row><row><cell>Gaussian −7.74 MAF (5) 0.14 ± 0.01</cell><cell>9.07</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 shows</head><label>1</label><figDesc></figDesc><table /><note>the performance of each model on each dataset. A Gaussian fitted to the train data is reported as a baseline. We can see that on 3 out of 5 datasets MAF is the best performing model, with MADE MoG being the best performing model on the other 2. On all datasets, MAF outperforms Real NVP. For the MINIBOONE dataset, due to overlapping error bars, a pairwise comparison was done to determine which model performs the best, the results of which are reported in Section E of the appendix. MAF MoG (5) achieves the best reported result on BSDS300 for a single model with 156.36 nats, followed by Deep RNADE [37] with 155.2. An ensemble of 32 Deep RNADEs was reported to achieve 157.0 nats</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Average test log likelihood (in nats) for conditional density estimation. The best performing model for each dataset is shown in bold. Error bars correspond to 2 standard deviations.</figDesc><table><row><cell></cell><cell cols="2">MNIST</cell><cell cols="2">CIFAR-10</cell></row><row><cell></cell><cell>unconditional</cell><cell>conditional</cell><cell>unconditional</cell><cell>conditional</cell></row><row><cell>Gaussian</cell><cell>−1366.9 ± 1.4</cell><cell>−1344.7 ± 1.8</cell><cell>2367 ± 29</cell><cell>2030 ± 41</cell></row><row><cell>MADE</cell><cell>−1380.8 ± 4.8</cell><cell>−1361.9 ± 1.9</cell><cell>147 ± 20</cell><cell>187 ± 20</cell></row><row><cell>MADE MoG</cell><cell>−1038.5 ± 1.8</cell><cell>−1030.3 ± 1.7</cell><cell>−397 ± 21</cell><cell>−119 ± 20</cell></row><row><cell>Real NVP (5)</cell><cell>−1323.2 ± 6.6</cell><cell>−1326.3 ± 5.8</cell><cell>2576 ± 27</cell><cell>2642 ± 26</cell></row><row><cell>Real NVP (10)</cell><cell>−1370.7 ± 10.1</cell><cell>−1371.3 ± 43.9</cell><cell>2568 ± 26</cell><cell>2475 ± 25</cell></row><row><cell>MAF (5)</cell><cell>−1300.5 ± 1.7</cell><cell>−1302.9 ± 1.7*</cell><cell>2936 ± 27</cell><cell>2983 ± 26*</cell></row><row><cell>MAF (10)</cell><cell>−1313.1 ± 2.0</cell><cell>−1316.8 ± 1.8*</cell><cell>3049 ± 26</cell><cell>3058 ± 26*</cell></row><row><cell>MAF MoG (5)</cell><cell>−1100.3 ± 1.6</cell><cell>−1092.3 ± 1.7</cell><cell>2911 ± 26</cell><cell>2936 ± 26</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc>shows the results on MNIST and CIFAR-10. The performance of a class-conditional Gaussian is reported as a baseline for the conditional case. Log likelihoods are calculated in logit space.</figDesc><table /><note>MADE MoG is the best performing model on MNIST, whereas MAF is the best performing model on CIFAR-10. On CIFAR-10, both MADE and MADE MoG performed significantly worse than the Gaussian baseline. MAF outperforms Real NVP in all cases. To facilitate comparison with the literature, Section E of the appendix reports results in bits/pixel. 1</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Approximate number of parameters for each model, as measured by number of connection weights. Biases and batch normalization parameters are ignored.</figDesc><table><row><cell># of parameters</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Number of hidden layers L and number of hidden units H given as options for each dataset. Each combination is reported in the format L × H.</figDesc><table><row><cell>POWER</cell><cell>GAS</cell><cell cols="2">HEPMASS MINIBOONE</cell><cell>BSDS300</cell><cell>MNIST</cell><cell>CIFAR-10</cell></row><row><cell>1 × 100</cell><cell>1 × 100</cell><cell>1 × 512</cell><cell>1 × 512</cell><cell>1 × 512</cell><cell>1 × 1024</cell><cell>1 × 1024</cell></row><row><cell>2 × 100</cell><cell>2 × 100</cell><cell>2 × 512</cell><cell>2 × 512</cell><cell>2 × 512</cell><cell></cell><cell>2 × 1024</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>1 × 1024</cell><cell></cell><cell>2 × 2048</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>2 × 1024</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Dimensionality D and number of examples N for each dataset.On the MINIBOONE dataset, the model with highest average test log likelihood is MAF MoG<ref type="bibr" target="#b4">(5)</ref>. However, due to the relatively small size of this dataset, the average test log likelihoods of some other models have overlapping error bars with that of MAF MoG</figDesc><table><row><cell>N</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Pairwise comparison results for MINIBOONE. Values correspond to average difference in log probability (in nats) from the best performing model, i.e. MAF MoG<ref type="bibr" target="#b4">(5)</ref>. Error bars correspond to 2 standard deviations. Significantly positive values indicate that MAF MoG (5) performs better.</figDesc><table><row><cell></cell><cell>MINIBOONE</cell></row><row><cell>Gaussian</cell><cell>25.55 ± 0.88</cell></row><row><cell>MADE</cell><cell>3.91 ± 0.20</cell></row><row><cell>MADE MoG</cell><cell>0.59 ± 0.16</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Bits per pixel for conditional density estimation (lower is better). The best performing model for each dataset is shown in bold. Error bars correspond to 2 standard deviations. MADE 2.04 ± 0.01 2.00 ± 0.01 5.67 ± 0.01 5.65 ± 0.01 MADE MoG 1.41 ± 0.01 1.39 ± 0.01 5.93 ± 0.01 5.80 ± 0.01 Real NVP (5) 1.93 ± 0.01 1.94 ± 0.01 4.53 ± 0.01 4.50 ± 0.01 Real NVP (10) 2.02 ± 0.02 2.02 ± 0.08 4.54 ± 0.01 4.58 ± 0.01</figDesc><table><row><cell></cell><cell>MNIST</cell><cell></cell><cell cols="2">CIFAR-10</cell></row><row><cell></cell><cell>unconditional</cell><cell>conditional</cell><cell>unconditional</cell><cell>conditional</cell></row><row><cell>Gaussian</cell><cell>2.01 ± 0.01</cell><cell>1.97 ± 0.01</cell><cell>4.63 ± 0.01</cell><cell>4.79 ± 0.02</cell></row><row><cell>MAF (5)</cell><cell>1.89 ± 0.01</cell><cell>1.89 ± 0.01*</cell><cell>4.36 ± 0.01</cell><cell>4.34 ± 0.01*</cell></row><row><cell>MAF (10)</cell><cell>1.91 ± 0.01</cell><cell>1.92 ± 0.01*</cell><cell>4.31 ± 0.01</cell><cell>4.30 ± 0.01*</cell></row><row><cell>MAF MoG (5)</cell><cell>1.52 ± 0.01</cell><cell>1.51 ± 0.01</cell><cell>4.37 ± 0.01</cell><cell>4.36 ± 0.01</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In earlier versions of the paper, results marked with * were reported incorrectly, due to an error in the calculation of the test log likelihood of conditional MAF. Thus error has been corrected in the current version.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">(a) Generated images (b) Real images</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Maria Gorinova for useful comments, and Johann Brehmer for discovering the error in the calculation of the test log likelihood for conditional MAF. George Papamakarios and Theo Pavlakou were supported by the Centre for Doctoral Training in Data Science, funded by EPSRC (grant EP/L016427/1) and the University of Edinburgh. George Papamakarios was also supported by Microsoft Research through its PhD Scholarship Programme.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Individual household electric power consumption data set</title>
		<ptr target="http://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption.Accessedon15" />
		<imprint>
			<date type="published" when="2017-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Baldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cranmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Faucett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sadowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Whiteson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.07913</idno>
		<title level="m">Parameterized machine learning for high-energy physics</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Density modeling of images using a generalized normalization transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ballé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Laparra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4nd International Conference on Learning Representations</title>
		<meeting>the 4nd International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Gaussianization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Gopinath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 13</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="423" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">NICE: Non-linear Independent Components Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.8516</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Density estimation using Real NVP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Learning Representations</title>
		<meeting>the 5th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Approximate Bayesian computation via regression density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Nott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Sisson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="34" to="48" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Reservoir computing compensates slow response of chemosensor arrays exposed to fast varying gas concentrations in continuous monitoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fonollosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sheik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Huerta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors and Actuators B: Chemical</title>
		<imprint>
			<biblScope unit="volume">215</biblScope>
			<biblScope unit="page" from="618" to="629" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">MADE: Masked Autoencoder for Distribution Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="881" to="889" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Neural adaptive sequential Monte Carlo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2629" to="2637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning Representations</title>
		<meeting>the 3rd International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Auto-encoding variational Bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd International Conference on Learning Representations</title>
		<meeting>the 2nd International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<title level="m">Improved variational inference with Inverse Autoregressive Flow. Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="4743" to="4751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Picture: A probabilistic programming language for scene perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mansinghka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4390" to="4399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Inference compilation and universal probabilistic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Baydin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the 20th International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The MNIST database of handwritten digits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">UCI machine learning repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lichman</surname></persName>
		</author>
		<ptr target="http://archive.ics.uci.edu/ml" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Maximum entropy flow networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Loaiza-Ganem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Cunningham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Learning Representations</title>
		<meeting>the 5th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="416" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Inference networks for sequential Monte Carlo in graphical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Paige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on Machine Learning</title>
		<meeting>the 33rd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Distilling intractable generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papamakarios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Probabilistic Integration Workshop at Neural Information Processing Systems 28</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fast -free inference of simulation models with Bayesian conditional density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papamakarios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Variational inference with normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1530" to="1538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning</title>
		<meeting>the 31st International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1278" to="1286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">High-dimensional probability estimation with deep density models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Rippel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1302.5125</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Boosted decision trees as an alternative to artificial neural networks for particle identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">P</forename><surname>Roe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Stancu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mcgregor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment</title>
		<imprint>
			<biblScope unit="volume">543</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="577" to="584" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">PixelCNN++: Improving the PixelCNN with discretized logistic mixture likelihood and other modifications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.05517</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep mixtures of factor analysers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Machine Learning</title>
		<meeting>the 29th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="505" to="512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Theano: A Python framework for fast computation of mathematical expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theano</forename><surname>Development Team</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.02688</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Generative image modeling using spatial LSTMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1927" to="1935" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A note on the evaluation of generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4nd International Conference on Learning Representations</title>
		<meeting>the 4nd International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">RNADE: The real-valued neural autoregressive density-estimator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Uria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="2175" to="2183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A deep and tractable density estimator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Uria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning</title>
		<meeting>the 31st International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="467" to="475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Modelling acoustic feature dependencies with artificial neural networks: Trajectory-RNADE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Uria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Renals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Valentini-Botinhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bridle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4465" to="4469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Neural autoregressive distribution estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Uria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-A</forename><surname>Côté</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">205</biblScope>
			<biblScope unit="page" from="1" to="37" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03499</idno>
		<title level="m">WaveNet: A generative model for raw audio</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Conditional image generation with PixelCNN decoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="4790" to="4798" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Pixel recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on Machine Learning</title>
		<meeting>the 33rd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1747" to="1756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">From learning models of natural image patches to whole image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13rd International Conference on Computer Vision</title>
		<meeting>the 13rd International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="479" to="486" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
