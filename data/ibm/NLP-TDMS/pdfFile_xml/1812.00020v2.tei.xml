<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TextureNet: Consistent Local Parametrizations for Learning from High-Resolution Signals on Meshes 3D Textured Model Orientation Field High-res Patch High-res Network Feature Sampled Points Geodesic Patch Label TextureNet Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haotian</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nießner</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Technical University of Munich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Guibas</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">TextureNet: Consistent Local Parametrizations for Learning from High-Resolution Signals on Meshes 3D Textured Model Orientation Field High-res Patch High-res Network Feature Sampled Points Geodesic Patch Label TextureNet Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: TextureNet takes as input a 3D textured mesh. The mesh is parameterized with a consistent 4-way rotationally symmetric (4-RoSy) field, which is used to extract oriented patches from the texture at a set of sample points. Networks of 4-RoSy convolutional operators extract features from the patches and used for 3D semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>We introduce, TextureNet, a neural network architecture designed to extract features from high-resolution signals associated with 3D surface meshes (e.g., color texture maps). The key idea is to utilize a 4-rotational symmetric (4-RoSy) field to define a domain for convolution on a surface. Though 4-RoSy fields have several properties favorable for convolution on surfaces (low distortion, few singularities, consistent parameterization, etc.), orientations are ambiguous up to 4-fold rotation at any sample point. So, we introduce a new convolutional operator invariant to the 4-RoSy ambiguity and use it in a network to extract features from high-resolution signals on geodesic neighborhoods of a surface. In comparison to alternatives, such as PointNetbased methods which lack a notion of orientation, the coherent structure given by these neighborhoods results in significantly stronger features. As an example application, we demonstrate the benefits of our architecture for 3D semantic segmentation of textured 3D meshes. The results show that our method outperforms all existing methods on the basis of mean IoU by a significant margin in both geometry-only (6.4%) and RGB+Geometry (6.9-8.2%) settings.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In recent years, there has been tremendous progress in RGB-D scanning methods that allow reliable tracking and reconstruction of 3D surfaces using hand-held, consumer-grade devices <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b10">11]</ref>. Though these methods are now able to reconstruct high-resolution textured 3D meshes suitable for visualization, understanding the 3D semantics of the scanned scenes is still a relatively open research problem.</p><p>There has been a lot of recent work on semantic segmentation of 3D data using convolutional neural networks (CNNs). Typically, features extracted from the scanned inputs (e.g., positions, normals, height above ground, colors, etc.) are projected onto a coarse sampling of 3D locations, and then a network of 3D convolutional filters is trained to extract features for semantic classification -e.g., using convolutions over voxels <ref type="bibr" target="#b45">[45,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b12">13]</ref>, octrees <ref type="bibr" target="#b34">[35]</ref>, point clouds <ref type="bibr">[31,</ref><ref type="bibr" target="#b32">33]</ref>, or mesh vertices <ref type="bibr" target="#b25">[26]</ref>. The advantage of this approach over 2D image-based methods is that convolutions operate directly on 3D data, and thus are relatively unaffected by view-dependent effects of images, such as perspective, occlusion, lighting, and background clutter. However, the resolution of current 3D representations is generally quite low (2cm is typical), and so the ability of 3D CNNs to discriminate fine-scale semantic patterns is usually far below their color image counterparts <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b15">16]</ref>.</p><p>To address this issue, we propose a new convolutional neural network, TextureNet, that extracts features directly from high-resolution signals associated with 3D surface meshes. Given a map that associates high-resolution signals with a 3D mesh surface (e.g., RGB photographic texture), we define convolutional filters that operate on those signals within domains defined by geodesic surface neighborhoods. This approach combines the advantages of feature extraction from high-resolution signals (as in <ref type="bibr" target="#b9">[10]</ref>) with the advantages of view-independent convolution on 3D surface domains (as in <ref type="bibr" target="#b41">[41]</ref>). This combination is important for the example in labeling the chair in <ref type="figure">Figure 1</ref>, whose surface fabric is easily recognizable in a color texture map.</p><p>During our investigation of this approach, we had to address several research issues, the most significant of which is how to define on geodesic neighborhoods of a mesh. One approach could be to compute a global UV parameterization for the entire surface and then define convolutional operators directly in UV space; however, that approach would induce significant deformation due to flattening, not always follow surface features, and/or produce seams at surface cuts. Another approach could be to compute UV parameterizations for local neighborhoods independently; however, then adjacent neighborhoods might not be oriented consistently, reducing the ability of a network to learn orientation-dependent features. Instead, we compute a 4-RoSy (four-fold rotationally symmetric) field on the surface using QuadriFlow <ref type="bibr" target="#b17">[18]</ref> and define a new 4-RoSy convolutional operator that explicitly accounts for the 4fold rotational ambiguity of the cross field parameterization. Here, 4-RoSy field is a set of tangent directions associated with vertices, where neighboring directions are parallel to each other by rotating one of them around surface normal by 360K/4 degrees (K ∈ Z). Since the 4-RoSy field from QuadriFlow has no seams, aligns to shape features, induces relatively little distortion, has few singularities, and consistently orients adjacent neighborhoods (up to 4-way rotation), it provides a favorable trade-off between distortion and orientation invariance.</p><p>Results on 3D semantic segmentation benchmarks show an improvement of 4-RoSy convolution on surfaces over alternative geometry-only approaches (by 6.4%), plus significantly further improvement when applied to high-resolution color signals (by 6.9-8.2% ). With ablation studies, we verify the importance of the consistent orientation of a 4-RoSy field and demonstrate that our sampling and convolution operator works better than other alternatives.</p><p>Overall, our core research contributions are:</p><p>• a novel learning-based method for extracting features from high-resolution signals living on surfaces embedded in 3D, based on consistent local parameterizations, • a new 4-RoSy convolutional operator designed for cross fields on general surfaces in 3D, • a new deep network architecture, TextureNet, composed of 4-RoSy convolutional operators, • an extensive experimental investigation of alternative convolutional operators for semantic segmentation of surfaces in 3D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>3D Deep Learning. With the availability of 3D shape databases <ref type="bibr" target="#b45">[45,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b37">38]</ref> and real-world labeled 3D scanning data <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b5">6]</ref>, there is significant interest in deep learning on three-dimensional data. Early work developed CNNs operating on 3D volumetric grids <ref type="bibr" target="#b45">[45,</ref><ref type="bibr" target="#b26">27]</ref>. They have been used for 3D shape classification <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b34">35]</ref>, semantic segmentation <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13]</ref>, object completion <ref type="bibr" target="#b11">[12]</ref>, and scene completion <ref type="bibr" target="#b12">[13]</ref>. More recently, researchers have developed methods that can take a 3D point cloud as input to a neural network and predict object classes or semantic point labels <ref type="bibr">[31,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b1">2]</ref>. AtlasNet <ref type="bibr" target="#b13">[14]</ref> learns to generate surfaces of the 3D shape. In our work, we utilize a sparse point sampled data representation, however, we exploit high resolution signals on geometric surface structures with a new 4-RoSy surface convolution kernel.</p><p>Convolutions on Meshes. Several researchers have proposed methods for applying convolutional neural networks intrinsically on manifold meshes. FeaStNet <ref type="bibr" target="#b42">[42]</ref> proposes a graph operator that establishes correspondences between filter weights. Jiang et al. <ref type="bibr" target="#b20">[21]</ref> applies differential operators on unstructured spherical grids. GCNN <ref type="bibr" target="#b25">[26]</ref> proposes using discrete patch operators on tangent planes parameterized by radius and angles. However, the orientation of their selected geodesic patches is arbitrary, and the parameterization is highly distorted or inconsistent at regions with high Gaussian curvature. ACNN <ref type="bibr" target="#b2">[3]</ref> observes this limitation and introduces the anisotropic heat kernels derived from principal curvatures. MoNet <ref type="bibr" target="#b27">[28]</ref> further generalizes the architecture with the learnable gaussian kernels for convolutions. The principal curvature based frame selection method is adopted by Xu et al. <ref type="bibr" target="#b46">[46]</ref> for segmentation of nonrigid surfaces, by Tatarchenko et al. <ref type="bibr" target="#b41">[41]</ref> for semantic segmentation of point clouds, and by ADD <ref type="bibr" target="#b3">[4]</ref> for shape correspondence in the spectral domain. It naturally removes orientation ambiguity but fails to consider frame inconsistency problem, which is critical when performing feature aggregation. Its problems are particularly pronounced in indoor scenes (which often have many planar regions where principal curvature is undetermined) and in real-world scans (which often have noisy and uneven sampling where consistent principal curvatures are difficult to predict). In contrast, we define a 4-RoSy field that provides consistent orientations for neighboring convolution domains.</p><p>Multi-view and 2D-3D Joint Learning. Other researchers have investigated how to incorporate features from RGB inputs to 3D deep networks. The typical approach is to simply assign color values to voxels, points, or mesh vertices and treat them as additional feature channels. However, given that geometry and RGB data are at vastly different resolutions, this approach leads to significant downsampling of the color signal and thus does not  <ref type="figure">Figure 2</ref>: TextureNet architecture. We propose a UNet <ref type="bibr" target="#b35">[36]</ref> architecture for hierarchical feature extraction. The key innovation in the architecture is the texture convolution layer. We efficiently query the local geodesic patch for each surface point, associate each neighborhood with a local, orientation-consistent texture coordinate. This allows us to extract the local 3D surface features as well as high-resolution signals such as associated RGB input.</p><p>take full advantage of the high-frequency patterns therein. An alternative approach is to combine features extracted from RGB images in a multi-view CNN <ref type="bibr" target="#b40">[40]</ref>. This approach has been used for 3D semantic segmentation in 3DMV <ref type="bibr" target="#b9">[10]</ref>, where features are extracted from 2D RGB images and then back-projected into a 3D voxel grid where they are merged and further processed with 3D voxel convolutions. Like our approach, 3DMV processes high-resolution RGB signals; however it convolves them in a 2D image plane, where occlusions and background clutter are confounding. In contrast, our method directly convolves high-resolution signals intrinsically on the 3D surface which is view-independent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>Our approach performs convolutions on high-resolution signals with geodesic convolutions directly on 3D surface meshes. The input is a 3D mesh associated with a highresolution surface signal (e.g., a color texture map), and the outputs are learned features for a dense set of sample points that can be used for semantic segmentation and other tasks.</p><p>Our main contribution is defining a smooth, consistently oriented domain for surface convolutions based on fourway rotationally symmetric (4-RoSy) fields. We observe that 3D surfaces can be mapped with low-distortion to twodimensional parameterizations anchored at dense sample points with locally consistent orientations and few singularities if we allow for a four-way ambiguity in the orientation at the sample points. We leverage that observation in TextureNet by computing a 4-RoSy field and point sampling using QuadriFlow <ref type="bibr" target="#b17">[18]</ref> and then building a network using new 4-RoSy convolutional filters (TextureConv) that are invariant to the four-way rotational ambiguity.</p><p>We utilize this network design to learn and extract features from high-resolution signals on surfaces by extracting surface patches with high-resolution signals oriented by the 4-RoSy field at each sample point. The surface patches are convolved by a few TextureConv layers, pooled at sample points, and then convolved further with TextureConv layers in a UNet <ref type="bibr" target="#b35">[36]</ref> architecture, as shown in <ref type="figure">figure 2</ref>. For down-sampling and up-sampling, we use the furthest point sampling and three-nearest neighbor interpolation method proposed by PointNet++ <ref type="bibr" target="#b32">[33]</ref>. The output of the network is a set of features associated with point samples that can be used for classification and other tasks. The following sections describe the main components of the network in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">High-Resolution Signal Representation</head><p>Our network takes as input a high-resolution signal associated with a 3D surface mesh. In the first steps of processing, it generates a set of sample points on the mesh and defines a parameterized high-resolution patch for each sample (Section 3.2) as follow: For each sample point p i , we first compute its geodesic neighborhood Ω ρ (p i ) (Eq. 1) with radius ρ. Then, we sample an NxN point cloud {q xy | − N/2 ≤ x, y &lt; N/2}. The texture coordinate for q xy is ((x+0.5)d, (y+0.5)d) -d is the distance between the adjacent pixels in the texture patch. In practice, we select N = 10 and d = 4mm. Finally, we use our newly proposed "TextureConv" and max-pooling operators (Section 3.3) to extract the high-res feature f i for each point p i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">4-RoSy Surface parameterization</head><p>A critical aspect of our network is to define a consistently-oriented geodesic surface parameterization for any position on a 3D mesh. Starting with some basic definitions, for a sampled point p on the surface, we can locally parameterize its tangent plane by two orthogonal tangent vectors i and j. Also, for any point q on the surface, there exists a shortest path on the surface connecting p and q, e.g., the orange path in figure 3(a). By unfolding it to the tangent plane, we can map q along the shortest path to q * . Using these constructs, we define the local texture coordinate q in p's neighborhood as</p><formula xml:id="formula_0">t p (q) = i T j T (q * − p).</formula><p>We additionally define the local geodesic neighborhood of p with receptive field ρ as</p><formula xml:id="formula_1">Ω ρ (p) = {q | ||t p (q)|| ∞ &lt; ρ}.</formula><p>(1) The selection for the set of mesh sampled positions {p} and their tangent vectors i and j is critical for the success of learning on a surface domain. Ideally, we would select points whose spacing is uniform and whose tangent directions are consistently oriented at neighbors, such that the underlying parameterization has no distortions or seams, as shown in <ref type="figure">Figure 4</ref>(a). With those properties, we could learn convolutional operators with translation invariance exactly as we would for images. Unfortunately, these properties are only achievable if the surface is a flat plane. For a general 3D surface, we can only hope to select a set of point samples and tangent vectors that minimize deviations between spacings of points and distortions of local surface parameterizations. <ref type="figure">Figure 4</ref>(b) shows an example where harmonic surface parameterization introduces large-scale distortiona 2D convolution would include a large receptive field at the nose but a small one at the neck. <ref type="figure">Figure 4</ref>(c) shows a geometry image <ref type="bibr" target="#b14">[15]</ref> parameterization with high distortion in the orientation -convolutions on such a map would have randomly distorted and irregular receptive fields, making it difficult for a network to learn canonical features. Unfortunately, a smoothly varying direction field on the surface is usually hard to obtain. According to the study of the direction field design <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b22">23]</ref>, the best-known approach to mitigate the distortion is to compute a four-way rotationally symmetric (4-RoSy) orientation field, which minimizes the deviation by incorporating directional ambiguity. Additionally, the orientation field needs a consistent definition among different geometries, and the most intuitive way is to make it align with the shape features like the principal curvatures. Fortunately, the extrinsic energy is used by <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b17">18]</ref>   to realize it. Therefore, we compute the extrinsic 4-Rosy orientation field at a uniform distribution of point samples using QuadriFlow <ref type="bibr" target="#b17">[18]</ref> and use it to define the tangent vectors at any position on the surface. Because of the directional ambiguity, we randomly pick one direction from the cross as i and compute j = n × i for any position.</p><p>Although there is a 4-way rotational ambiguity in this local parameterization of the surface (which will be addressed with a new convolutional operator in the next section), the resulting 4-RoSy field provides a way to extract geodesic neighborhoods consistently across the entire surface, even near singularities. <ref type="figure" target="#fig_3">Figure 5</ref> (a,b,c) shows the ambiguity of possible unfolded neighborhoods at a singularity. Since QuadriFlow <ref type="bibr" target="#b17">[18]</ref> treats singularities as faces rather than vertices, all sampled positions have the well-defined orientation field. More importantly, the parameterization of every geodesic neighborhood is well-defined with our shortest path patch parameterization. For example, only <ref type="figure" target="#fig_3">Figure 5</ref>(a) is a valid parameterization for the purple spot, while the location for the blue and orange spots in <ref type="figure" target="#fig_3">Figures 5(b)</ref> and (c) are unfolded along the paths that are not the shortest. Unfolding a geodesic neighborhood around the singularity also causes another potential issue that a seam cut is usually required, leading to a gap at the 3-singularity or multiplesurface coverage at the 5-singularity. For example, there is a gap at the bottom-right corner in <ref type="figure" target="#fig_3">Figure 5</ref>(a) caused by the seam cut shown as the green dot line. Fortunately, the location of the seam is also well-defined with our shortestpath definition: it must be the shortest geodesic path going through the singularity. Therefore, our definition of the local neighborhood guarantees a canonical way of surface parameterization even around corners and singularities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">4-RoSy Surface Convolution Operator</head><p>TextureNet is a network architecture composed of convolutional operators acting on geodesic neighborhoods of sample points with 4-RoSy parameterizations. The input to each convolutional layer is three-fold: 1) a set of 3D sample points associated with features (e.g., RGB, normals, or features computed from high-resolution surface patches or pre- vious layers); 2) a coordinate system stored as two tangent vectors representing the 4-RoSy cross field for each point sample; and 3) a coarse triangle mesh, where each face is associated with the set of extracted sampled points and connectivity indices that support fast geodesic patch query and texture coordinate computation for the samples inside a geodesic neighborhood, much like the PTex <ref type="bibr" target="#b4">[5]</ref> representation for textures.</p><p>Our key contribution in this section is the design of a convolution operator suitable for 4-RoSy fields. The problem is that we cannot use traditional 3x3 convolution kernels on domains parameterized with 4-RoSy fields without inducing inconsistent feature aggregation at higher levels. <ref type="figure">Figure 6</ref> demonstrates the problem for a simple example. <ref type="figure">Figure 6</ref>(a) shows 3x3 convolution in a traditional flat domain. <ref type="figure">Figure 6</ref>(b) shows the frames defined by our 4-RoSy orientation field of the 3D cube where red spots represent the singularities. Although the cross-field in the orange patch is consistent under the 4-RoSy metric, the frames are not parallel when they are unfolded into a plane (figure 6(c)). Aggregation of features inside such a patch is therefore problematic.</p><p>"TextureConv" is our solution to remove the directional ambiguity. It consists of four layers (in <ref type="figure">figure 2)</ref>, including geodesic patch search, texture space grouping, convolution and aggregation. To extract the geodesic patch for each input point Ω ρ (p), we use breadth-first search with the priority queue to extract the face set in the order of geodesic distance from face center to p. We estimate the texture coordinate at the face center as well as its local tangent coordinate system, recorded as (t f , i f , j f ). In order to expand the search tree from face u to v, we can approximate the texture coordinate at the face center as t</p><formula xml:id="formula_2">v = t u +(i u , j u ) T (c v −c u ),</formula><p>where c f represents the center position of the face f . i v and j v can be computed by rotating the coordinate system around the shared edge from face u to v. After having the face set inside the geodesic patch, we can find the sampled points set associated with these faces. We estimate the texture coordinate of every sampled point q associated with each face f as t p (q) = t f + (i f , j f ) T (q − c f ). By testing ||t p (q)|| ∞ &lt; ρ, we can determine the sampled points inside the geodesic patch Ω ρ (p).</p><p>The texture space grouping layer segments the local neighborhood into 3x3 patches in the texture space, each of which is a square with edge length as 2ρ/3, as shown in figure 2 (after the "grouping arrow"). We could directly borrow the image convolution method linearly transform each point feature with 9 different weights according to their belonging patch. However, we propose a 4-RoSy convolution kernel to deal with the directional ambiguity. As shown in <ref type="figure">figure 2</ref>, all sampled points can be categorized as at the corners ({p 1 j }), edges ({p 2 j }) or the center ({p 3 j }). Each sampled point feature is convolved with a 1x1 convolution as h 1 , h 2 or h 3 based on its category. The extracted 4-rosy feature removes the ambiguity and allows higher-level feature aggregation. The channel-wise aggregation operator g can be max-pooling or average-pooling followed by the ReLu layer. In the task for semantic segmentation, we choose max-pooling since it is better at preserving salient signals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Evaluation</head><p>To investigate the performance of TextureNet, we ran a series of 3D semantic segmentation experiments for indoor scenes. In all experiments, we train and test on the standard splits of the ScanNet <ref type="bibr" target="#b8">[9]</ref> and Matterport3D <ref type="bibr" target="#b8">[9]</ref> datasets. Following previous works, we report mean class intersectionover-union (mIoU) results for ScanNet and mean class accuracy for Matterport3D.</p><p>Comparison to State-of-the-Art. Our main result is a comparison of TextureNet to state-of-the-art methods for 3D semantic segmentation. For this experiment, all methods utilize both color and geometry in their native formats. Specifically, PointNet++ <ref type="bibr" target="#b32">[33]</ref>, Tangent Convolution <ref type="bibr" target="#b41">[41]</ref>, SplatNet <ref type="bibr" target="#b38">[39]</ref> use points with per-point normals and colors; 3DMV <ref type="bibr" target="#b9">[10]</ref> uses 2D image features back-projected onto voxels; and Ours uses high-res 10x10 texture patches extracted from geodesic neighborhoods at sample points. <ref type="table">Table 1</ref> reports the mean IoU scores for all 20 classes of the ScanNet benchmark on the ScanNet (v2) and mean class accuracy on Matterport3D datasets. They show that Tex-tureNet (Ours) provides the best results on 18/20 classes for Scannet and 12/20 classes for Matterport3D. Overall, the mean class IoU for Ours is 8.2% higher than the previous state-of-the-art (3DMV) on ScanNet (48.4% vs. 56.6%), and our mean class accuracy is 6.9% higher on Matter-port3D (56.1% vs. 63.0%).</p><p>Qualitative visual comparisons of the results shown in <ref type="figure">Figures 7-9</ref> suggest that the differences between methods are often where high-resolution surface patterns are discriminating (e.g., the curtain and pillows in the top row of <ref type="figure">Figure 7</ref>) and where geodesic neighborhoods are more in-  <ref type="figure">Figure 7</ref>: Visualization on ScanNet (v2) <ref type="bibr" target="#b8">[9]</ref>. In the first row, we correctly predicts the lamp, pillow, picture, and part of the cabinet, while other methods fail. In the second row, we predict the window and the trash bin correctly, while 3DMV <ref type="bibr" target="#b9">[10]</ref> predicts part of the window as the trash bin and other methods fail. The third row (zoom-in) highlights the differences.</p><p>(a) Ground Truth (b) Ball (c) Ours <ref type="figure">Figure 8</ref>: Visual results using different neighborhoods. With euclidean ball as a neighborhood, part of the table is predicted as the chair, since they belong to the same euclidean ball. This issue is solved by extracting features from the geodesic patches.</p><p>formative than Euclidean ones (e.g., the lamp next to the bed). <ref type="figure">Figure 8</ref> shows a case where convolutions with the geodesic neighborhoods clearly outperform their Euclidean counterparts. In <ref type="figure">Figure 8(b)</ref>, part of the table is predicted as chair, probably because it is in a Euclidean ball covering nearby chairs. This problem is solved with our method based on geodesic patch neighborhoods. As shown in <ref type="figure">Figure 8(c)</ref>, the table and the chairs are clearly segmented.</p><p>Effect of 4-RoSy Surface Parameterization. Our second experiment is designed to test how different surface parameterizations affect semantic segmentation performance -i.e., how does the choice of the orientation field affect the learning process? The simplest choice is to pick an arbitrary direction on the tangent plane as the x-axis, similar to GCNN <ref type="bibr" target="#b25">[26]</ref>, <ref type="figure" target="#fig_5">(Figure 10(a)</ref>). A second option adopted by Tangent Convolution <ref type="bibr" target="#b41">[41]</ref>   <ref type="table">Table 2</ref>: Mean IoU for different direction fields on ScanNet (v2). The input is a pointcloud with a normal and rgb color for each point. Random refers to randomly picking an arbitrary direction for each sampled point. Intrinsic refers to solving for a 4-rosy field with intrinsic energy. EigenVec refers to solving for a direction field with the principal curvature. Extrinsic is our method, which solves a 4-rosy field with extrinsic energy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PointNet++</head><p>TangentConv SplatNet Ours GT <ref type="figure">Figure 9</ref>: Visual results on Matterport3D <ref type="bibr" target="#b5">[6]</ref>. In all examples, our method is better at predicting the door, the toilet, the sink, the bathtub, and the curtain.</p><p>Euclidean ball centered at p and parameterizes the tangent plane by two eigenvectors corresponding to the largest two eigenvalues of the covariance matrix q (p − q)(p − q) T . A critical problem of this formulation is that the principal directions cannot be robustly analyzed at planar regions or noisy surfaces <ref type="figure" target="#fig_5">(Figure 10(b)</ref>). It also introduces inconsistency to the coordinate systems of the neighboring points, which vexes the feature aggregation at higher levels. A third alternative is to use the intrinsic energy function <ref type="bibr" target="#b19">[20]</ref> or other widely used direction field synthesis technique <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b22">23]</ref>, which is not geometry-aware and therefore variant to 3D rigid transformation <ref type="figure" target="#fig_5">(Figure 10(c)</ref>). Our choice is to use the extrinsic energy to synthesize the direction field <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20]</ref>, which is globally consistent and only variant to geometry itself <ref type="figure" target="#fig_5">(Figure 10(d)</ref>).</p><p>To test the impact of this choice, we compare all of these alternative direction fields to create the local neighborhood parameterizations for our architecture and compare the results of 3D semantic segmentation on ScanNet (v1) test set. As shown in <ref type="table">Table 2</ref>, the choice for random direction field performs worst since it does not provide consistent parameterization. The tangent convolution suffers from the same issue, but gets a better result since it aligns with the shape features. The intrinsic parameterization aligns with the shape features, but is not a canonical parameterization -for example, different rigid transformations of the same shape lead to different parameterizations. The extrinsic energy provides a canonical and consistent surface parameterization. As a result, the extrinsic 4-rosy orientation field achieves the best results.</p><p>Effect of 4-RoSy Surface Convolution. Our third experiment is designed to test how the choice for the surface convolution operator affects learning. In  <ref type="table" target="#tab_4">Table 4</ref>: Mean Class IoU with different texture convolution operators on ScanNet (v2). The input is the pointcloud for the first row (Geometry) and the pointcloud associated with the normal and rgb signal for the second row (NRGB).</p><p>ing, respectively. GCNN 1 and GCNN are geodesic convolutional neural networks <ref type="bibr" target="#b25">[26]</ref> with N ρ = 3, N θ = 1 and N ρ = N θ = 3 respectively. ACNN represents anisotropic convolutional neural networks <ref type="bibr" target="#b2">[3]</ref> with N ρ = 3, N θ = 1. RoSy 1 means a 3x3 convolution along the direction of the 1-rosy orientation field. RoSy 4 picks an arbitrary direction from the cross in the 4-rosy field. RoSy 4 (m) applies 3x3 convolution for each direction of the cross in the 4-rosy field, aggregated by max pooling. Ours(A) and Ours represent our method with average and max pooling aggregation. We find that GCNN, ACNN and RoSy 4 produce the lowest IoUs, because they suffer from inconsistency of frames when features are aggregated. GCNN 1 does not suffer from this issue since there is only a single bin in the angle dimension. RoSy 4 (m) uses the max-pooling to canonicalize the feature extraction, which is independent of the orientation selection, and produces better results than RoSy 4 . RoSy 1 achieves a higher score by generating a more globally consistent orientation field with higher distortion. From this study, the combination of 4-rosy orientation field and our TextureNet is the best option for the segmentation task among these methods. Since we precompute the local parametrization, our training efficiency is similar to that of GCNN. Please refer to Supplemental D for the detailed performance with each class.</p><p>Effect of High-Resolution Color. Our fourth experiment tests how much convolving with high-resolution surface colors affects semantic segmentation. <ref type="table">Table 3</ref> compares the performance of our network with uncolored sampled points (XYZ), sampled points with the per-point surface normal and color (NRGB), and with the per-point normal and the 10x10 color texture patch (Highres) as input. According to <ref type="table" target="#tab_4">Table 4</ref>, our network is already superior with only XYZ or additional NRGB because of the convolution operator. We find that providing TextureNet with Highres colors improves the mean class IoU by 3.3%. As expected, the impact is stronger from some semantic classes than otherse.g., the IoUs for the bookshelf and picture classes increase 63.1→71.3% and 15.8→21.1%, respectively. We show additional comparison to O-CNN <ref type="bibr" target="#b43">[43]</ref> which enables highres signals for voxels in Supplemental E.</p><p>Comparisons Using Only Surface Geometry. As a final experiment, we evaluate the value of the proposed 3D network for semantic segmentation of inputs with only surface geometry (without color). During experiments on Scan-Net, TextureNet achieves 50.6% mIoU, which is 6.4% better than the previous state-of-the-art. In comparison, Scan-Net <ref type="bibr">[</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>TextureNet bridges the gap between 2D image convolution and 3D deep learning using 4-RoSy surface parameterizations. We propose a new method for learning from high-resolution signals on 3D meshes by computing local geodesic neighborhoods with consistent 4-RoSy coordinate systems. We design a network of 4-RoSy texture convolution operators that are able to learn surface features that significantly improve over the state-of-the-art performance for 3D semantic segmentation of 3D surfaces with color (by 6.9-8.2%). Code and data will be publicly available. Topics for further work include investigating the utility of Tex-tureNet for extracting features from other high-resolution signals on meshes (e.g., displacement maps, bump maps, curvature maps, etc.) and applications of TextureNet to other computer vision tasks (e.g., instance detection, pose estimation, part decomposition, texture synthesis, etc.).</p><p>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplemental</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Comparison to 2D Convolution on Texture Atlas</head><p>We did an additional experiment to compare our convolution operator with traditional image convolutions on a color texture atlas created with a standard UV parameterization, as shown in <ref type="figure">Figure 11</ref>. For this experiment, we trained a state-of-the-art network (DenseNet <ref type="bibr" target="#b16">[17]</ref>) on the semantic labels mapped to the texture map image. The results with that method are not very good -the mean class IoU is only 12.2%, as compared to 56.6% with our method. We conjecture the reason is that UV parameterizations are not consistent across examples and convolutions are affected by texture seams. <ref type="figure">Figure 11</ref>: An example of the texture image.</p><p>We additionally tried an as-rigid-as-possible parameterization, which achieves 16.8 IoU (ours is 58.1). The poor performance is mainly due to convolutions over regions with seams, large distortions, and inconsistent orientations -i.e., the main problems that our 4-rosy approach aims to resolve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation of Neighborhood Selection Methods</head><p>The next experiment tests whether the geodesic neighborhoods used by TextureNet convolutional operators are better than volumetric ones used by PointNet++. To test this, we compare the performance of the original Point-Net++ network which takes the Euclidean ball as the neighborhood, with slightly modified versions which take a cuboid or our geodesic patch as a neighborhood. As shown in <ref type="table">Table 5</ref>, the geodesic patch achieves a slightly higher score. This might be due to the reason that it is easier for the network to learn the boundary on the 2D subsurface than on the 3D space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Effect of Point Sampling Method</head><p>The next experiment tests the impact of our proposed point sampling method. While PointNet++ <ref type="bibr" target="#b32">[33]</ref> adopts the furthest point sampling method to preprocess the data, we use QuadriFlow <ref type="bibr" target="#b17">[18]</ref> to sample the points on the surface. It maintains uniform edge length in surface parametrization, and therefore usually provides more uniformly distributed samples on the surface considering the geodesic distance. <ref type="figure" target="#fig_6">Figure 12</ref> shows the proportion of each class in the Scan-Net dataset with QuadriFlow and furthest point sampling.</p><p>We use TextureNet to learn the semantic labels with their input and our samples. <ref type="table">Table 6</ref> shows the class IoU for the prediction. With more samples for minor classes like the counter, desk, and curtain, our sampling method performs better. <ref type="figure" target="#fig_6">Figure 12</ref> shows the visualization of different sampling results. Visually, our sampling method leads to more uniformly distributed points on the surface. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Class Distribution with Different Sampling</head><p>Ours FPS <ref type="figure">Figure 13</ref>: Class distribution with different sampling. The y-axis represents the portion of each class across all scenes. Except for classes of wall, floor, and bookshelf, our method achieves more samples than the furthest sampling method. As a result, PointNet++ achieves better results in most classes with our sampling method. <ref type="table">Table 7</ref> provides detailed results for the performance of different surface convolution operators on ScanNet dataset <ref type="bibr" target="#b8">[9]</ref> with input as the point cloud or the point cloud associated with the normal and RGB color for each point (expanding on  <ref type="table">Table 5</ref>: PointNet++ prediction using different neighborhood. The input is the sampled positions computed with our sampling method. Ball represents the euclidean ball. CubeX represents a tangent cuboid with the same volume as that of the ball, but has the width and length X times of the ball radius. Ours is using the geodesic patch with the same radius of the ball.  <ref type="table">Table 6</ref>: PointNet++ prediction taking the positions of the pointcloud from different sampling methods including the furthest point sampling (FPS) and Quadriflow (Quad). lutional neural networks <ref type="bibr" target="#b25">[26]</ref> with N ρ = 3, N θ = 1 and N ρ = N θ = 3 respectively. ACNN represents anisotropic convolutional neural networks <ref type="bibr" target="#b2">[3]</ref> with N ρ = 3 = N θ = 3. RoSy 1 refers to a 3x3 convolution along the direction of the 1-rosy orientation field. RoSy 4 picks an arbitrary direction from the cross in the 4-rosy field. RoSy 4 (m) applies 3x3 convolution for each direction of the cross in the 4-rosy field, aggregated by max pooling. Ours(A) and Ours represent our method with average-pooling and max-pooling aggregation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Further Results on Effect of 4-RoSy Surface Convolution</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Comparison to Octree-based Approaches</head><p>Existing volume-based octree methods have been used mostly for stand-alone objects from ShapeNet. For larger scenes, memory is a severe limitation. As a test, we tried O-CNN <ref type="bibr" target="#b43">[43]</ref> on chunks of radius 1.35m 3 using a 12GB GPU, which fits 6 conv/deconv layers and a feature dimension of 256 at resolution 256 3 . This test yielded a mean IoU of 30.8 with NRGB and 27.8 with pure geometry. In contrast, the surface-based convolution of TextureNet is much more efficient (2D rather than 3D), allowing for a total of 18 conv/deconv layers with max feature dimension of 1024, and achieves 58.1 with high-res color, 54.8 with NRGB, and 50.6 with pure geometry.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Further Comparisons Using Only Surface Geometry</head><p>This section provides more detailed results for the experiment described in the last paragraph of Section 4 of the paper, where we evaluate the value of the proposed 3D network for semantic segmentation of inputs with only surface geometry (without color). During experiments on Scan-Net, TextureNet achieves 50.6% mIoU, which is 6.4% bet-ter than the previous state-of-the-art. In comparison, Scan-Net <ref type="bibr">[</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Effect of 4-RoSy convolution on traditional image convolution</head><p>We also compared our 4-RoSy operator with the traditional image convolution on the MNIST dataset <ref type="bibr" target="#b23">[24]</ref>. We use a simple network containing two MLP layers and two fully connected layers. The performance of the original network is 99.1%. By replacing the convolution with our 4-RoSy operator in the MLP layers, we achieve 98.5% classification accuracy. Therefore, our 4-RoSy kernel is comparable to the traditional convolutions even on the standard images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Visual comparison of Different Resolutions</head><p>In <ref type="figure">Figure 14</ref>, we show the predictions of TextureNet with different color resolutions as input. The first column is the 3D model. The second column shows the ground truth semantic labels. The high-res signals of the red regions are shown in the third column. The last two columns are predictions from TextureNet with per-point color (low-res) or high-res texture patch as input. As a result, TextureNet performs better given the input with high-res signals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Visualization of the Semantic Segmentation</head><p>We compare TextureNet with the state-of-the-art method on ScanNet Dataset <ref type="bibr" target="#b8">[9]</ref> and Matterport3D Dataset. On both datasets, we outperform existing methods (see the main paper). <ref type="figure" target="#fig_3">Figure 15 and 16</ref> show examples of prediction from <ref type="table">Table 7</ref>: Texture Convolution Operator Comparison. The input is the pointcloud in (a) and the pointcloud associated with the normal and the rgb color for each point in (b). PN + (A) and PN + represent PointNet++ with average-pooling and maxpooling, respectively. GCNN 1 and GCNN are geodesic convolutional neural networks <ref type="bibr" target="#b25">[26]</ref> with N ρ = 3, N θ = 1 and N ρ = N θ = 3 respectively. ACNN represents anisotropic convolutional neural networks <ref type="bibr" target="#b2">[3]</ref> with N ρ = 3, N θ = 1. RoSy 1 means a 3x3 convolution along the direction of the 1-rosy orientation field. RoSy 4 picks an arbitrary direction from the cross in the 4rosy field. RoSy 4 (m) applies 3x3 convolution for each direction of the cross in the 4-rosy field, aggregated by maxpooling. Ours(A) and Ours represent our method with average-pooling and max-pooling aggregation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>wall floor cab bed chair sofa  <ref type="table">Table 8</ref>: Geometry-only: comparison to the state-of-the-art for 3D convolution with pure geometry as input; i.e., no RGB information used in any of these experiments. We can show that our method also outperforms existing geometry-only approaches.</p><p>several methods on ScanNet. <ref type="figure">Figure 17</ref> show examples of prediction from different methods on Matterport3D Dataset.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>13</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3D Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GT Low-res High-res Color Signal</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>(a) Local texture coordinate. (b) Visualization of geodesic neighborhoods Ω ρ (ρ = 20 cm) of a set of randomly sampled vertices. (a) With appropriate method like Quadriflow, we can get the surface parameterization aligning to shape features with negligible distortions. (b) Harmonic parameterizations leads to high distortion in the scale. (c) Geometry images [15] result in high distortion in the orientation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>the blue line (c) Cut the orange line</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>At the singularity of the cube, (a)-(c) provides three different ways of unfolding the local neighborhood. Such ambiguity is removed around the singularity by our texture coordinate definition using the shortest path. For the purple point, (a) is a valid neighborhood, while the blue points in (b) and orange points in (c) are unfolded along the paths which are not the shortest. Similarly, the ambiguity of the gap location is removed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(a) Image Coordinate (b) 3D parametrization Inconsistent (c) Inconsistent Frame Figure 6: (a) Traditional convolution kernel on a regular grid. (b) Frames defined by the orientation field on a 3D cube. (c) For the patch highlighted in orange in (b), multilayer feature aggregation would be problematic with traditional convolution due to the frame inconsistency caused by the directional ambiguity of the orientation field.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 10 :</head><label>10</label><figDesc>Direction fields from different methods. (a) Random directions lead to inconsistent frames. (b) Eigenvectors suffer from the same issue at flat area. (c) Intrinsic-energy based orientation field does not align to the shape features. (d) Our extrinsic-based method generates consistent orientation fields aligned with surface features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 12 :</head><label>12</label><figDesc>Visualization of Different Sampling methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 14 :Figure 15 :</head><label>1415</label><figDesc>Visual comparison of Different Resolutions. The column row is the 3D model. The second column shows the ground truth semantic labels. The high-res signals of the red regions are shown in the third column. The last two columns are predictions from TextureNet with per-point color (low-res) or high-res texture patch as input. Visualization of the Semantic Segmentation on ScanNet Dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 16 :Figure 17 :</head><label>1617</label><figDesc>Visualization of the Semantic Segmentation on ScanNet Dataset. Visualization of the Semantic Segmentation on Matterport Dataset. 17</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Input wall floor cab bed chair sofa table door wind shf pic cntr desk curt fridg show toil sink bath other avg PN + [33] 66.4 91.5 27.8 56.3 64.0 52.7 37.3 28.3 36.1 59.2 6.7 28.0 26.2 45.4 25.6 22.0 63.5 38.8 54.4 20.0 42.5 SplatNet [39] 69.9 92.5 31.1 51.1 65.6 51.0 38.3 19.7 26.7 60.6 0.0 24.5 32.8 40.5 0.0 24.9 59.3 27.1 47.2 22.7 39.3 Tangent [41] 63.3 91.8 36.9 64.6 64.5 56.2 42.7 27.9 35.2 47.4 14.7 35.3 28.2 25.8 28.3 29.4 61.9 48.7 43.7 29.8 43.8 3DMV [10] 60.2 79.6 42.4 53.8 60.6 50.7 41.3 37.8 53.9 64.3 21.4 31.0 43.3 57.4 53.7 20.8 69.3 47.2 48.4 30.1 48.4 bed chair sofa table door wind shf pic cntr desk curt ceil fridg show toil sink bath other avg PN + [33] 80.1 81.3 34.1 71.8 59.7 63.5 58.1 49.6 28.7 1.1 34.3 10.1 0.0 68.8 79.3 0.0 29.0 70.4 29.4 62.1 8.5 43.8 SplatNet [39] 90.8 95.7 30.3 19.9 77.6 36.9 19.8 33.6 15.8 15.7 0.0 0.0 0.0 12.3 75.7 0.0 0.0 10.6 4.1 20.3 1.7 26.7 Tangent [41] 56.0 87.7 41.5 73.6 60.7 69.3 38.1 55.0 30.7 33.9 50.6 38.5 19.7 48.0 45.1 22.6 35.9 50.7 49.3 56.4 16.] benchmarks. PN + , SplatNet, and Tangent Convolution use points with per-point normal and color as input. 3DMV uses 2D images and voxels. Ours uses grid points with high-res 10x10 texture patches.</figDesc><table><row><cell>Ours</cell><cell cols="5">68.0 93.5 49.4 66.4 71.9 63.6 46.4 39.6 56.8 67.1 22.5 44.5 41.1 67.8 41.2 53.5 79.4 56.5 67.2 35.6 56.6</cell></row><row><cell></cell><cell></cell><cell cols="2">(a) ScanNet (v2) (mean class IoU)</cell><cell></cell><cell></cell></row><row><cell>Input</cell><cell cols="5">wall floor cab 6 46.8</cell></row><row><cell cols="6">3DMV [10] 79.6 95.5 59.7 82.3 70.5 73.3 48.5 64.3 55.7 8.3 55.4 34.8 2.4 80.1 94.8 4.7 54.0 71.1 47.5 76.7 19.9 56.1</cell></row><row><cell>Ours</cell><cell cols="5">63.6 91.3 47.6 82.4 66.5 64.5 45.5 69.4 60.9 30.5 77.0 42.3 44.3 75.2 92.3 49.1 66.0 80.1 60.6 86.4 27.5 63.0</cell></row><row><cell></cell><cell></cell><cell cols="2">(b) Matterport3D (mean class accuracy)</cell><cell></cell><cell></cell></row><row><cell cols="6">Table 1: Comparison with the state-of-the-art methods for 3D semantic segmentation on the (a) ScanNet v2, and (b) Mat-</cell></row><row><cell>terport3D [6GT</cell><cell>PointNet++</cell><cell>3DMV</cell><cell>TangentConv</cell><cell>SplatNet</cell><cell>Ours</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>considers a set of points q in a 6 Input wall floor cab bed chair sofa table door wind bkshf pic cntr desk curt fridg show toil sink bath other ave Random 37.6 92.5 37.0 63.7 28.5 56.9 27.6 15.3 31.0 47.6 16.5 36.6 53.3 51.2 15.4 24.7 59.3 47.6 53.3 27.0 41.1 Intrinsic 47.4 91.9 35.3 62.5 55.8 44.8 37.5 29.8 40.5 40.9 16.7 41.5 39.9 42.1 20.4 24.3 85.6 44.5 58.3 29.5 44.4 EigenVec 45.3 79.0 32.2 53.4 59.8 40.4 32.2 28.8 40.5 43.4 17.8 39.5 32.7 40.6 22.5 25.0 82.4 48.1 54.8 32.6 42.5 Extrinsic 69.8 92.3 44.8 69.4 75.8 67.1 56.8 39.4 41.1 63.1 15.8 57.4 46.5 48.3 36.9 40.0 78.1 54.0 65.4 34.4 54.8</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 ,Table 3 :</head><label>43</label><figDesc>PN + (A) and PN + represent PointNet++ with average and max pool-7 Input wall floor cab bed chair sofa table door wind bkshf pic cntr desk curt fridg show toil sink bath other ave XYZ 64.8 90.0 39.3 65.8 74.8 66.6 50.5 33.9 35.6 58.0 14.0 54.3 42.1 45.4 30.9 43.0 67.7 47.9 55.8 32.2 50.6 NRGB 69.8 92.3 44.8 69.4 75.8 67.1 56.8 39.4 41.1 63.1 15.8 57.4 46.5 48.3 36.9 40.0 78.1 54.0 65.4 34.4 54.8 Highres 75.0 94.4 46.8 67.3 78.1 64.0 63.5 44.8 46.0 71.3 21.1 44.4 47.5 52.5 35.2 51.3 80.3 51.7 67.6 40.2 58.1 Mean IoU for different color inputs on ScanNet (v2). XYZ represents our network using raw point input; i.e., geometry only. NRGB represents our network taking input as the sampled points with per-point normal and color. Highres represents our network taking per-point normal and the 10x10 surface texture patch for each sampled point.</figDesc><table><row><cell cols="4">Input PN + (A) PN + GCNN 1 GCNN ACNN</cell></row><row><cell cols="2">Geometry 32.6 43.5</cell><cell>48.7</cell><cell>24.6 29.7</cell></row><row><cell>NRGB</cell><cell>38.1 48.2</cell><cell>49.6</cell><cell>27.0 32.4</cell></row><row><cell cols="4">Input RoSy 1 RoSy 4 RoSy 1 (m) Ours(A) Ours</cell></row><row><cell cols="2">Geometry 37.8 30.8</cell><cell>40.3</cell><cell>38.0 50.6</cell></row><row><cell>NRGB</cell><cell>47.8 34.5</cell><cell>42.6</cell><cell>39.1 54.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>9] = 30.6%, Tangent Convolution [41] = 40.9%, Point-Net++ [33] = 43.5%, and SplatNet [39] = 44.2%. Detailed class IoU results are provided in Supplemental F.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc>of the paper). PN + (A) and PN + represent PointNet++ with average-pooling and maxpooling, respectively. GCNN 1 and GCNN are geodesic convo-Input wall floor cab bed chair sofa table door wind bkshf pic cntr desk curt fridg show toil sink bath other ave Ball 68.1 96.2 34.9 41.2 61.8 43.0 24.1 5.0 19.2 41.7 0.0 4.7 11.8 17.7 20.1 30.8 72.2 43.7 55.2 8.7 35.0 Cube1 65.3 95.8 29.0 57.0 61.2 46.2 42.7 17.8 11.8 35.1 0.7 37.3 39.0 55.4 8.5 43.9 63.0 30.6 52.4 15.0 40.4 Cube2 58.7 90.0 61.6 62.6 59.3 50.4 40.2 31.3 15.1 45.6 1.9 29.4 23.9 53.1 18.2 41.8 81.7 34.1 51.8 25.2 43.9 Cube4 32.7 86.8 59.6 49.1 51.3 33.7 30.0 27.0 11.8 33.8 0.9 20.9 19.5 40.3 15.1 29.8 54.1 27.7 41.7 17.0 34.2 Ours 61.5 95.0 40.1 60.0 74.9 52.8 46.1 31.6 19.7 50.3 5.9 33.9 25.9 58.2 30.0 48.6 85.2 47.1 48.8 28.5 47.2</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Input wall floor cab bed chair sofa table door wind bkshf pic cntr desk curt fridg show toil sink bath other ave FPS 70.2 92.3 43.1 63.7 67.7 62.5 50.8 23.4 42.5 65.2 15.4 54.7 44.3 45.0 40.1 33.5 71.6 54.3 62.4 28.7 51.6 Quad 69.8 92.3 44.8 69.4 75.8 67.1 56.8 39.4 41.1 63.1 15.8 57.4 46.5 48.3 36.9 40.0 78.1 54.0 65.4 34.4 54.8</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>9] = 30.6%, Tangent Convolution [41] = 40.9%, Point-Net++ [33] = 43.5%, and SplatNet [39] = 44.2%. Detailed class IoU results are provided inTable 8.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>table door wind shf pic cntr desk curt fridg show toil sink bath other avg ScanNet [9] 43.7 78.6 31.1 36.6 52.4 34.8 30.0 18.9 18.2 50.1 10.2 21.1 34.2 0.0 24.5 15.2 46.0 31.8 20.3 14.5 30.6 PN + [33] 64.1 82.2 31.4 51.6 64.5 51.4 44.9 23.3 30.4 68.2 3.7 26.2 34.2 65.1 23.4 18.3 61.8 31.5 75.4 18.8 43.5 SplatNet [39] 67.4 85.8 32.3 45.1 71.9 51.0 40.7 15.1 25.2 62.3 0.0 23.2 39.9 56.1 0.0 24.2 62.6 23 67.4 25.7 40.9 Tangent [41] 62.0 83.6 39.3 58.4 67.6 57.3 47.9 27.6 28.5 55.0 8.3 36.1 33.9 38.7 26.2 28.0 60.5 39.3 59.0 27.8 44.2 Ours 64.8 90.0 39.3 65.8 74.8 66.6 50.5 33.9 35.6 58.0 14.0 54.3 42.1 45.4 30.9 43.0 67.7 47.9 55.8 32.2 50.6</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work is supported in part by Google, Intel, Amozon, a Vannevar Bush faculty fellowship, a TUM Foundation Fellowship, a TUM-IAS Rudolf Mößbauer Fellowship, the ERC Starting Grant Scan2CAD, and the NSF grants VEC-1539014/1539099, CHS-1528025 and IIS-1763268. It makes use of data from Matterport.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Joint 2d-3d-semantic data for indoor scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.01105</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Atzmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.10091</idno>
		<title level="m">Point convolutional neural networks by extension operators</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning shape correspondence with anisotropic convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodolà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3189" to="3197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Anisotropic diffusion descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodolà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="431" to="441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Ptex: Per-face texture mapping for production rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Burley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lacewell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1155" to="1164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.06158</idno>
		<title level="m">Matterport3d: Learning from rgb-d data in indoor environments</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012</idno>
		<title level="m">An information-rich 3d model repository</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A volumetric method for building complex models from range images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 23rd annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="303" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Scannet: Richly-annotated 3d reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Joint 3d-multi-view prediction for 3d semantic scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.10409</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bundlefusion: Real-time globally consistent 3d reconstruction using on-the-fly surface reintegration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">76</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Shape completion using 3d-encoder-predictor cnns and shape synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Scancomplete: Large-scale scene completion and semantic segmentation for 3d scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bokeloh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A papier-mâché approach to learning 3d surface generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Groueix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aubry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="216" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Geometry images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Gortler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hoppe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="355" to="361" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
	<note>Computer Vision (ICCV</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Quadriflow: A scalable and robust method for quadrangulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Shewchuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="147" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Kinectfusion: real-time 3d reconstruction and interaction using a moving depth camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hilliges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Molyneaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hodges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th annual ACM symposium on User interface software and technology</title>
		<meeting>the 24th annual ACM symposium on User interface software and technology</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="559" to="568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Instant field-aligned meshes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tarini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Panozzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<idno>189:1-189:15</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2015-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kashinath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niessner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.02039</idno>
		<title level="m">Spherical cnns on unstructured grids</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Very high frame rate volumetric integration of depth images on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kähler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Prisacariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1241" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Metric-driven RoSy field design and remeshing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-K</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Palacios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="95" to="108" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Mnist handwritten digit database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Burges</surname></persName>
		</author>
		<ptr target="http://yann.le-cun.com/exdb/mnist" />
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
		<respStmt>
			<orgName>AT&amp;T Labs</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Geodesic convolutional neural networks on riemannian manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision workshops</title>
		<meeting>the IEEE international conference on computer vision workshops</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="37" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Voxnet: A 3d convolutional neural network for real-time object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Robots and Systems (IROS), 2015 IEEE/RSJ International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="922" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Kinectfusion: Real-time dense surface mapping and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hilliges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Molyneaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hodges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th IEEE international symposium on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="127" to="136" />
		</imprint>
	</monogr>
	<note>Mixed and augmented reality (ISMAR)</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Real-time 3d reconstruction at scale using voxel hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stamminger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (ToG)</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">169</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Volumetric and multi-view cnns for object classification on 3d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5099" to="5108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">n-symmetry direction field design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Vallet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lévy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Octnet: Learning deep 3d representations at high resolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Ulusoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sun rgb-d: A rgbd scene understanding benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="567" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Semantic scene completion from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="190" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Splatnet: Sparse lattice networks for point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2530" to="2539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multiview convolutional neural networks for 3d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="945" to="953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Tangent convolutions for dense prediction in 3d</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3887" to="3896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Feastnet: Featuresteered graph convolutions for 3d shape analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Boyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2598" to="2606" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">O-cnn: Octree-based convolutional neural networks for 3d shape analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y.</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">72</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Elasticfusion: Real-time dense slam and light source estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Whelan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">F</forename><surname>Salas-Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leutenegger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="1697" to="1716" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Directionally convolutional networks for 3d shape segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2698" to="2707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Operator wall floor cab bed chair sofa table door wind bkshf pic cntr desk curt fridg show toil sink bath other ave PN + (A) 55</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
		</imprint>
	</monogr>
	<note>7 80.2 23.1 41.6 54.1 55.9 68.6 11.2 20.0 41.1 5.3 37.5</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">(a) Pointcloud Operator wall floor cab bed chair sofa table door wind bkshf pic cntr desk curt fridg show toil sink bath other ave PN + (A) 66</title>
		<idno>66.6 50.5 33.9 35.6 58.0 14.0 54.3 42.1 45.4 30.9 43.0 67.7 47.9 55.8 32.2 50.6</idno>
		<imprint>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
	<note>5 18.9 34.9 61.2 43.0 50.2 23.8 38.0 Ours 64.8 90.0 39.3 65.8 74. 6 94.7 29.9 50.5 64.9 52.9 56.5 17.4 19.7 45.0 0.0 36.5 30.4 21.5 13.5 19.1 49.6 30.3 45.6 16.6 38.1</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
