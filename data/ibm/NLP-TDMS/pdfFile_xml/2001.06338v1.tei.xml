<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient Facial Feature Learning with Wide Ensemble-based Convolutional Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henrique</forename><surname>Siqueira</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Knowledge Technology Department of Informatics</orgName>
								<orgName type="institution">University of Hamburg</orgName>
								<address>
									<addrLine>Vogt-Koelln-Str. 30</addrLine>
									<postCode>22527</postCode>
									<settlement>Hamburg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Magg</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Knowledge Technology Department of Informatics</orgName>
								<orgName type="institution">University of Hamburg</orgName>
								<address>
									<addrLine>Vogt-Koelln-Str. 30</addrLine>
									<postCode>22527</postCode>
									<settlement>Hamburg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Wermter</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Knowledge Technology Department of Informatics</orgName>
								<orgName type="institution">University of Hamburg</orgName>
								<address>
									<addrLine>Vogt-Koelln-Str. 30</addrLine>
									<postCode>22527</postCode>
									<settlement>Hamburg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Efficient Facial Feature Learning with Wide Ensemble-based Convolutional Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Ensemble methods, traditionally built with independently trained de-correlated models, have proven to be efficient methods for reducing the remaining residual generalization error, which results in robust and accurate methods for realworld applications. In the context of deep learning, however, training an ensemble of deep networks is costly and generates high redundancy which is inefficient. In this paper, we present experiments on Ensembles with Shared Representations (ESRs) based on convolutional networks to demonstrate, quantitatively and qualitatively, their data processing efficiency and scalability to large-scale datasets of facial expressions. We show that redundancy and computational load can be dramatically reduced by varying the branching level of the ESR without loss of diversity and generalization power, which are both important for ensemble performance. Experiments on large-scale datasets suggest that ESRs reduce the remaining residual generalization error on the AffectNet and FER+ datasets, reach human-level performance, and outperform state-of-the-art methods on facial expression recognition in the wild using emotion and affect concepts.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>"We get resourcefulness from having many resources; not from having one very smart one" <ref type="bibr" target="#b28">(Minsky 2014)</ref>. In machine learning, ensemble methods refer to a set of models where an inference is made collectively based on individual predictions <ref type="bibr" target="#b9">(Dietterich 2000)</ref>. A well-trained ensemble can reduce the remaining residual generalization error, which results in predictions being more accurate than any single model in the ensemble. Traditional ensemble (TE) methods are built by independently training several models on the same or different data. They can be composed of a single type of machine learning method such as an ensemble of neural networks <ref type="bibr" target="#b14">(Hansen and Salamon 1990)</ref>, but the diversity is often higher when an ensemble is built from a library of different methods <ref type="bibr" target="#b5">(Caruana et al. 2004)</ref>.</p><p>At present, ensembling of deep networks is an important resource but requires high computational power. To make this training-intensive technology accessible to everyone, recent studies have explored ways to reduce redundancy in Copyright c 2020, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. ensembling. <ref type="bibr" target="#b26">Meshgi, Oba, and Ishii (2018)</ref> have exploited concepts from active learning to reduce training time and redundancy. Rather than using the whole dataset for training, their ensemble method is trained on the most informative samples that maximize learning based on the query by committee algorithm <ref type="bibr" target="#b32">(Seung, Opper, and Sompolinsky 1992)</ref>.</p><p>Another approach adopted a divide-and-conquer strategy <ref type="bibr" target="#b23">(Li et al. 2019)</ref> where the input space is decomposed into multiple regions, and each region is used to train one convolutional neural network of the ensemble. Despite their progress on reducing redundancy, their approaches fall within the "explicit" ensemble methods, i.e., consist of independent models. Therefore, redundancy of low-level visual features is still high, and unnecessary computational resources have to be allocated for processing such features.</p><p>In the so-called "implicit" ensemble methods, a single network may generalize as well as an ensemble by distilling its knowledge <ref type="bibr" target="#b16">(Hinton, Vinyals, and Dean 2015)</ref>. By training a convolutional neural network (CNN) with the outputs of an ensemble of CNNs, <ref type="bibr" target="#b33">Shen, He, and Xue (2019)</ref> have reduced inference time and redundancy while maintaining generalization power and similar intermediate representations under an adversarial training strategy. However, training time is greatly increased with their approach since a trained traditional ensemble is a fundamental pre-requisite.</p><p>Ensemble with Shared Representations (ESR), proposed in our previous work <ref type="bibr" target="#b34">(Siqueira et al. 2018)</ref>, offers the best of the two worlds. It is neither a fully implicit nor a fully explicit ensemble method. As depicted in <ref type="figure" target="#fig_0">Figure 1</ref>, the shared layers represent the implicit part. They are responsible for the reduction of redundancy, training, and inference time. The low-level features learned by them are shared with the ensemble of convolutional branches. The latter characterizes the explicit part and carries the diversity of the ensemble. The level to start the ensemble of branches plays an important role in the computational load and generalization power as well as for redundancy and diversity. However, the effect of the branching level is still an open question that needs careful analysis. In the context of facial expression recognition, for instance, starting branching too early (level 1) may result in high redundancy of low-level facial features where all branches have to learn skin textures and so forth. other hand, branching too late may drastically decrease diversity in the ensemble where features from the shared layers no longer correspond to spatial facial features (level 5). We hypothesize that the optimal branching level may be located between the extremes, where the abstraction level of the facial features is high including smiling and frowning but have yet to be encoded into emotion concepts.</p><p>Another aspect that needs further understanding is the scalability of ESRs to large-scale datasets of facial expressions of emotion. Can ESRs reduce the remaining residual generalization error when training data is abundant? After reviewing prior work on facial expression recognition, we address these questions. In this paper, the effects of varying the branching level are extensively examined, quantitatively and qualitatively, first, on a small-scale but clean and well-structured dataset of facial expressions in the lab. Subsequently, experiments using a single GeForce GTX 1080 on large-scale benchmarks for facial expression recognition in the wild demonstrate the affordability and scalability of ESRs, followed by conclusions and future research. For reproducibility purposes, source code of our experiments, the ESR implementation in PyTorch, trained networks and supplementary material are available in our GitHub repository 1 . 1 Source code: https://github.com/knowledgetechnologyuhh/ Efficient-Facial-Feature-Learning-with-Wide-Ensemble-based-Convolutional-Neural-Networks</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prior Work on Facial Expression Recognition</head><p>Early approaches for automatic facial expression recognition have followed the general pipeline to tackle computer vision problems, which consist of pre-processing the facial images, appearance and/or geometric hand-crafted feature extraction and, in the final stage, the classification of such features <ref type="bibr" target="#b35">(Tian, Kanade, and Cohn 2005)</ref>. These methods are usually fast and accurate in indoor environments but frequently drop in performance under real-world conditions <ref type="bibr" target="#b29">(Mollahosseini, Hasani, and Mahoor 2019)</ref>.</p><p>The rapid progress in deep learning motivated researchers to develop facial expression recognition systems using deep neural networks. Since these networks can automatically learn features from data, hand-feature engineering was left out in the pipeline. Besides that, feature learning allows deep networks to learn a broader range of facial features than earlier approaches, including variation to rotations, and illumination changes. Indeed, as investigated by <ref type="bibr" target="#b20">Khorrami, Paine, and Huang (2015)</ref>, it has turned out that the features learned by a CNN trained for facial expression recognition reflect the facial features of emotion suggested by the psychologist Paul Ekman during his study of universal facial expressions of emotion <ref type="bibr" target="#b11">(Ekman 1989)</ref>. Recent approaches rely on wellestablished networks for object recognition such as AlexNet, MobileNet, ResNet, and VGGNet <ref type="bibr" target="#b2">(Barsoum et al. 2016;</ref><ref type="bibr" target="#b15">Hewitt and Gunes 2018;</ref><ref type="bibr" target="#b29">Mollahosseini, Hasani, and Mahoor 2019)</ref>. In visual perceptual tasks, certain features previously learned can be transferred among related tasks and the use of pre-trained networks often speed up learning and culminate in better accuracy than training them from scratch. These approaches represent the state of the art in the datasets utilized in our experiments (for a review, see <ref type="bibr" target="#b30">(Poria et al. 2017)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ensembles with Shared Representations</head><p>Ensembles with shared representations exploit the fundamental properties of convolutional networks. A convolutional layer learns local patterns from the previous layer by convolving small filters over its input feature space (Chollet 2018). Thus, the patterns learned by convolutional layers are translation-invariant. Another property is the capability to learn spatial hierarchies of patterns by stacking multiple convolutional layers. Consider the intermediate representations exhibited in <ref type="figure" target="#fig_0">Figure 1</ref>. Early layers learn simple and local visual patterns such as oriented lines, edges, and colors. These low-level abstractions of input space are the reason for early feature maps resembling a face with emphasis on certain regions. Subsequent layers hierarchically combine local patterns from previous layers into increasingly complex concepts such as nose, mouth, and eyes. The level of abstraction increases as you go deeper into the network until the point where feature maps are no longer visually interpretable. Finally, the last layer encodes these representations into semantic concepts, for instance, concepts of emotion.</p><p>These properties are the foundations of ESRs and play a crucial role in reducing redundancy of visual features in the ensemble. An ESR consists of two building blocks. (1) The base of the network (gray blocks in <ref type="figure" target="#fig_0">Figure 1</ref>) is an array of convolutional layers for low-and middle-level feature learn-ing. (2) These informative features are then shared with independent convolutional branches (purple blocks) that constitute the ensemble. From this point, each branch can learn distinctive features while competing for a common resource -the shared layers. This competitive training emerges from the minimization of a combined loss function defined as the summation of the loss functions of each branch as follows:</p><formula xml:id="formula_0">L esr = b i L[P (f (x i ) = y i |x i , θ shared , θ b ), y i ],<label>(1)</label></formula><p>where b denotes the branch index, (x i , y i ) random samples from the training set, θ shared the parameters of the shared layers from the base of the network that acts as a regularizer for ESRs, and θ b the parameters of a convolutional branch that composes the ensemble. Because novel convolutional branches are added in sequence while training, as outlined in Algorithm 1, the shared layers turn out to be an efficient transfer learning mechanism that guides and accelerates learning as the ensemble grows. Besides that, the shared representations are conditioned to learn features that are suitable to different branches in the ensemble due to the inductive transfer learning from the combination of multiple loss functions from each convolutional branch. During inference time, a given input is classified by the ensemble through a collective decision such as plurality and majority voting. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Datasets</head><p>Over the last two decades, a number of datasets of facial expressions have been collected for research on affective computing <ref type="bibr" target="#b29">(Mollahosseini, Hasani, and Mahoor 2019)</ref>. Among the attributes that characterize these datasets (e.g. the number of subjects and representations of emotion), the nature of the facial expressions is critical for developing and assessing automatic facial expression recognition systems.</p><p>Some of the datasets rely on posed or simulated facial expressions of emotion. They are supported by Ekman and Friesen's work <ref type="bibr" target="#b10">(Ekman and Friesen 1976;</ref><ref type="bibr" target="#b11">Ekman 1989</ref>) on universals in facial expressions of emotion. The arguments about universality suggest that when we feel certain emotions, some facial movements manifest regardless of age, culture, race, or sex. For example, when you are angry in a traffic jam, you scowl; when you are happy after an acceptance notification, you smile. These visible facial movements have been mapped latter to the Facial Action Code (FAC) <ref type="bibr" target="#b10">(Ekman and Friesen 1976)</ref>, where every single appearance change (action unit, AU) was categorized. These datasets are occasionally called in-the-lab datasets. As the name states, facial images are collected in controlled indoor environments where experimental variables (e.g. scene lighting and camera-view points) are accurately adjusted. They usually provide clean and high-quality data. Although posed emotional expressions are considered more expressive than natural expressions in everyday life <ref type="bibr" target="#b21">(Koolagudi and Rao 2012)</ref>, the datasets are well structured and carefully annotated from emotions to FAC <ref type="bibr" target="#b25">(Lucey et al. 2010</ref>).</p><p>On the other end of the spectrum, there are the in-thewild datasets with spontaneous facial expressions. Over a century, since Charles Darwin published The Expression of the Emotions in Man and Animals <ref type="bibr" target="#b8">(Darwin 1872)</ref>, the universality of emotional expressions has been called into question by distinguished psychologists including William James <ref type="bibr" target="#b19">(James 1884;</ref><ref type="bibr" target="#b18">James et al. 1890</ref>), James A. <ref type="bibr" target="#b31">Russell (Russell 2003)</ref> and Lisa F. <ref type="bibr" target="#b0">Barrett (Barrett and Russell 2015;</ref><ref type="bibr" target="#b1">Barrett 2017;</ref><ref type="bibr" target="#b12">Gendron, Crivelli, and Barrett 2018)</ref>. Their theses converge to the same conclusion: diversity of emotional expressions is the norm, not the uniformity. According to <ref type="bibr" target="#b18">James et al. (1890)</ref>, any categorization of an emotional expression can be seen "as true and as 'natural' as any other". Nevertheless, Russell argues for the minimum universality in his core affect theory, where emotions are described in an orthogonal dimensional space of arousal and valence levels. Therefore, even though we cannot claim that in-the-wild datasets contain emotional facial expressions, they do provide large and rich data of facial configurations captured in a vast range of environmental conditions. These variations are crucial to develop robust facial expression recognition systems. In most cases, the data is gathered from films or the Internet and annotated based on affect concepts and/or emotion concepts <ref type="bibr" target="#b29">(Mollahosseini, Hasani, and Mahoor 2019)</ref>.</p><p>We trained and tested the ensemble with shared representations on in-the-lab and in-the-wild datasets for a couple of reasons. The former allows us to evaluate ESRs' inference performance when training data is scarce and to conduct a descriptive analysis of their predictions based on the FAC system. On the other hand, the latter permits us to asses the scalability of ESRs to large-scale datasets and to test their inference performance in a more challenging scenario which includes, among other aspects, a vast intraclass variation, rotations, occlusions, and a heavily imbalanced label distribution. Together, they provide evidence on how flexible and robust ensembles with shared representations are in dealing with different shortcomings on facial expression recognition in the lab and in the wild. A few samples from the datasets used in our experiments are shown in <ref type="figure" target="#fig_2">Figure 2</ref> and the technical details are described as follows. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>In-the-Lab Dataset</head><p>The Extended Cohn-Kanade (CK+) dataset <ref type="bibr" target="#b25">(Lucey et al. 2010)</ref> has been vastly used to develop action unit detection and facial expression recognition systems. 123 subjects between 18 and 50 years old from different races, sex, and ethnic groups were told to portray a series of facial configurations based on FAC. The onset facial expressions were recorded from frontal and 30-degree camera-view points, and their peaks were carefully annotated and validated in terms of 30 action units and 8 discrete emotion concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>In-the-Wild Datasets</head><p>AffectNet <ref type="bibr" target="#b29">(Mollahosseini, Hasani, and Mahoor 2019)</ref> is the largest dataset of facial expressions in the wild publicly available. It contains more than one million images retrieved from the Internet using emotion keywords from different languages, where half of them were manually annotated by human experts using 8 discrete emotions, arousal and valence levels. In addition to its heterogeneity, the heavily imbalanced label distribution (e.g., contempt constitutes only 1% of the annotated images) and the strong baselines pose a real challenge for the affective computing community. FER+ <ref type="bibr" target="#b2">(Barsoum et al. 2016</ref>) derives from the reannotation of the Facial Expression Recognition 2013 (FER-2013) dataset <ref type="bibr" target="#b13">(Goodfellow et al. 2015)</ref> due to the originally high degree of noise presented in the annotations. FER-2013 was created by querying facial images from Google's image search engine using 184 emotion keywords. Each of the 35,887 facial images was then re-labeled by 10 annotators using crowd-sourcing, and the contempt category was added to the dataset as one of the possible 8 emotion labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Redundancy and Diversity Analysis</head><p>We start this section describing the methodology adopted to explore the impact of the branching level on redundancy, and diversity of ESRs. After discussing training strategies and architectural design, we present quantitative results on computational load, redundancy and recognition performance. We conclude this section by presenting evidence that ESRs converge faster than a TE while preserving diversity, by analyzing convergence graphs and saliency maps via Grad-CAM (Selvaraju et al. 2017) at different training milestones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methodology</head><p>We followed the subject-independent 10-fold crossvalidation for comparison purposes based on our previous work <ref type="bibr" target="#b34">(Siqueira et al. 2018)</ref>. First, we extracted the first and last three frames from each sequence on CK+, converted them to gray-scale, cropped the faces using the <ref type="bibr">Viola and Jones's algorithm (2004)</ref>, and resized the facial images to 96 x 96 pixels. The first frame was labeled as neutral, whereas the last three frames received one of the seven basic emotion labels. Subsequently, the images were separated into 10 folds according to the subject's id available in the metadata. Each fold was populated with facial images from a subject by iterating the subject id and the fold id, which resulted in 12 subjects and 130.8 facial images on average for each fold. With the folds populated, we run the experiment 10 times. In each trial t, we selected fold-(t) for testing, fold-(t + 1) for validating, and only the first four folds from the remaining eight folds for training, i.e., 523.2 images on average on the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training ESRs at Different Branching Levels</head><p>How does the branching level affect computational load, redundancy and recognition performance on ESRs? This research question was addressed by training several ESRs at different branching levels and analyzing the impact on those aspects. Two baselines were defined according to our previous research <ref type="bibr" target="#b34">(Siqueira et al. 2018)</ref>. After an exhaustive search among different convolutional architectures and training strategies, the network with the best mean test accuracy on CK+ was selected as the first baseline. The network comprises five convolutional layers, each followed by a batch normalization layer. A max-pooling layer was also added after the second, third, and fourth batch normalization layers. On top, a global average pooling layer transforms the last feature maps into a vector and forwards it to the dense output layer for facial expression recognition. The ReLU activation function was applied after batch normalization layers as suggested by <ref type="bibr" target="#b17">Ioffe and Szegedy (2015)</ref>. A detailed architectural diagram of the network used in our experiments is presented at the top of <ref type="figure">Figure 3</ref>. The second baseline is a traditional ensemble with four of such networks.</p><p>The single network was trained on four folds using stochastic gradient descent (SGD) to minimize the crossentropy loss, whereas different training strategies using SGD were tested to build ensembles with complementary representations of the data. Given the small number of training samples, the traditional ensemble was trained using bagging <ref type="bibr" target="#b4">(Breiman 1996)</ref> due to its efficiency in dealing with the variance problem <ref type="bibr" target="#b9">(Dietterich 2000)</ref>. Since we have four folds for training, we built an ensemble of four networks where each network was trained on three folds following a leaveone-fold-out scheme. The shared layers of ESRs allow us to test some variations of bagging. After adding a new convolutional branch to the ESR, the shared layers (lr sl ) and already trained branches (lr tb ) continue learning on additional data using (1) the same initial learning rate (fixed lr.; lr sl = lr tb = 0.1), (2) a smaller learning rate (varied lr.; lr sl = 0.1 and lr tb = 0.02), or (3) not training at all (frozen Shared Layers Ensemble <ref type="bibr">Input (1,</ref><ref type="bibr">96,</ref><ref type="bibr">96)</ref> Conv2D (1, 32, 5, 1) BatchNorm2D <ref type="formula">(32)</ref> Conv2D (64, 64, 3, 1) BatchNorm2D <ref type="formula">(64)</ref> MaxPool2D <ref type="formula">(2, 2)</ref> Conv2D (32, 64, 3, 1) BatchNorm2D <ref type="formula">(64)</ref> MaxPool2D <ref type="formula">(2, 2)</ref> Conv2D (64, 64, 3, 1) BatchNorm2D <ref type="formula">(64)</ref> AdaptiveAvgPool2D <ref type="formula" target="#formula_0">(1)</ref> Conv2D (64, 64, 3, 1) BatchNorm2D (64) MaxPool2D (2, 2) Linear <ref type="bibr">(64,</ref><ref type="bibr">8)</ref> In the lab <ref type="bibr">Input (3,</ref><ref type="bibr">96,</ref><ref type="bibr">96)</ref> Conv2D (3, 64, 5, 1) BatchNorm2D <ref type="formula">(64)</ref> Conv2D (128, 128, 3, 1) BatchNorm2D <ref type="formula" target="#formula_0">(128)</ref> Conv2D (128, 128, 3, 1) BatchNorm2D <ref type="formula" target="#formula_0">(128)</ref> MaxPool2D <ref type="formula">(2, 2)</ref> Conv2D (64, 128, 3, 1) BatchNorm2D <ref type="formula" target="#formula_0">(128)</ref> MaxPool2D <ref type="formula">(2, 2)</ref> Conv2D (128, 128, 3, 1) BatchNorm2D <ref type="formula" target="#formula_0">(128)</ref> Conv2D (256, 256, 3, 1) BatchNorm2D <ref type="formula">(256)</ref> Conv2D (256, 512, 3, 1, 1) BatchNorm2D <ref type="formula" target="#formula_0">(512)</ref> AdaptiveAvgPool2D <ref type="formula" target="#formula_0">(1)</ref> Conv2D (128, 256, 3, 1) BatchNorm2D <ref type="formula">(256)</ref> MaxPool2D <ref type="formula">(2, 2)</ref> Linear <ref type="formula">(</ref> layers; lr sl = lr tb = 0.0). We adopted a momentum factor of 0.9 on SGD and a learning rate decay with a multiplicative factor of 0.5 applied after every 250 epochs. Finally, we also included the interleaved training strategy adopted in our previous work <ref type="bibr" target="#b34">(Siqueira et al. 2018)</ref> in this experiment where all branches were trained iteratively on random mini-batches from the four folds. Data augmentation was randomly applied in all of the cases including brightness and contrast changes, horizontal flips, rotations up to 30 degrees, transla-  tions, and rescaling. <ref type="figure">Figure 4</ref> displays the mean accuracy on the CK+ test set with increasing branching level for every approach as well as the baselines (dashed lines). Consistent with ensemble literature, the ensemble methods achieved higher accuracies than the single network. The interleaved approach, however, demonstrated inferior performance among the ensembles. We believe the poorer performance might have been caused by low diversity in the ensemble. In interleaved training, diversity derives only from different starting points and different data augmentation executions on shuffled mini-batches. The accuracies obtained by the ESRs especially at level 3 were as high as the traditional ensemble method but the advantage is evident in the number of trainable parameters used by each approach, as shown in <ref type="table" target="#tab_0">Table 1</ref>  <ref type="figure">Figure 5</ref>: Comparison of the accuracy (%) on the validation set of CK+ over epoch between the ensemble with shared representations and the traditional ensemble. On the right, Grad-CAM visualization at different training milestones including before any weight update (epoch 0). The saliency maps were generated using the jet colormap, where red regions indicate facial features that contributed the most to the high activation of the output neurons, in this example, happy or fear. Best viewed in color. less trainable parameters than traditional ensembles with a substantial decrease of 32% at level 3 and 54% at level 4, while achieving the same generalization power confirmed by the paired t-test in <ref type="table" target="#tab_1">Table 2</ref>. Positive markers indicate statistically significant differences (p &lt; 0.05). The improvement in recognition performance is clear when compared with a single network. The high p-value between the ESR with four branches at level 3 and TE indicates that the generalization abilities are equivalent while the redundancy and computational load are significantly reduced by ESR-4 Lvl. 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transfer Learning and Diversity Analysis</head><p>Training time is an important factor when training deep neural networks, especially, ensembles of them. <ref type="figure">Figure 5</ref> shows both how transfer learning in ESRs accelerates and guides the learning of new branches, as well as the diversity analysis of learned facial features. The graphs, on the left, compare the convergence of the ESR and TE over epoch for every branch, or network, added to the ensemble. The convergence curve follows the same pattern as the ensemble size increases in TEs since any new model is trained from scratch. On the other hand, the convergence speed of additional branches in ESRs increases due to the prior knowledge stored in the shared layers. Even after the first update, the ESR was already twice as accurate as TEs, and this gap was only closed around epoch 50.</p><p>These quantitative results suggest that the shared layers learned informative facial features of emotion concepts. To support our claim with visual evidence, we generated saliency maps with respect to the ESR and TE predictions at different training milestones using Grad-CAM. Note, on the right, that the learning progress of facial features advanced at the same pace while the ensemble size was one (both methods are identical). When training the second branch, however, the ESR already learned after the first update that the region around the mouth is relevant for recognizing happy facial expressions, whereas the TE took around 50 epochs to discover the same pattern.</p><p>In fact, AU-12 from the FAC must be visible on happy facial expressions in CK+ <ref type="bibr" target="#b25">(Lucey et al. 2010)</ref>. When the facial muscle underlying AU-12 (i.e., Zygomatic Major) is fired, it pulls the corner of the lips up, producing a smile. The smile is one of the most discriminative and repetitive facial features presented in CK+ that distinguishes happy facial expressions among other categories. Fear, for instance, is categorized from the combination of more complex facial features, which would require more time for the ESR and TE to learn such features. Nevertheless, the ESR learned around epoch 50 that frowning is one of the features necessary to recognize fear. This appearance change is produced by the frontalis muscle that covers the forehead and, when activated, can raise the inner brow (AU-1). The other feature is the lip stretcher coded by AU-20 and presented in fear facial expressions in CK+. In general, the TE needs more training epochs than ESR to learn informative facial features. Finally, note that the diversity of features of the ESR is as high as the TE. In the happy facial expression example, while branches 2 and 4 captured the smile after training, branch 3 focused on the nasolabial furrows and branch 1 captured the wrinkles in the outer eyes caused by raising the cheeks (AU-6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training ESRs on Large-Scale Datasets</head><p>Nowadays, training ensembles on large-scale datasets became impracticable for those who have limited computational resources because even a single deep neural network may take over a month of training using several GPUs in large data centers <ref type="bibr" target="#b6">(Chollet 2017)</ref>. This section supports that ESRs are affordable for ensembling on large-scale datasets. Besides short training time and low computational cost, ESRs can reduce the remaining residual generalization error which led to higher accuracies than state-of-the-art methods in facial expression recognition benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methodology</head><p>Along with data, benchmark datasets usually provide standard experimental protocols and baseline results. AffectNet and FER+ have divided the dataset into training, validation, and test sets, and published them for the scientific community, except for the test set of the former. Meanwhile, researchers have utilized the validation set for evaluation and comparisons, as suggested by the AffectNet authors. We followed the same methodologies as the state-of-the-art methods for fair comparisons on both datasets. In experiments on AffectNet, the best inference performance on the validation set is reported, whereas the mean and standard deviation of the test accuracies after five trials are used as an evaluation metric for FER+ <ref type="bibr" target="#b2">(Barsoum et al. 2016</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation on the AffectNet Dataset</head><p>As the body of features increases, the memory capacity of the neural network shall also increase to account for the higher volume of patterns. Therefore, the architecture used from this point is based on the previous ESR but with a few more convolutional layers, and batch normalization layers, as well as more convolutional filters per layer. In order to preserve the spatial information of the features, we adopted the same spatial reduction rate of the feature maps from the previous experiment by adding a max-pooling layer after every two convolutional layers. The ensemble of convolution branches begins in an equivalent spatial level to ESR-4 Lvl. 3, where the shape of the feature maps are similar, as depicted in <ref type="figure">Figure 3</ref>.</p><p>Discrete emotion perception. One of the challenges of facial expression recognition on AffectNet is the imbalance problem. We coped with this problem by training the branches of the ESR on balanced subsets from the whole training set containing up to 5000 samples of each emotion. Through an empirical analysis, subsets with fewer samples of each category resulted in lower performance, while more samples provided no significant gain in accuracy. The  <ref type="figure">Figure 6</ref>: Normalized confusion matrix of the ensemble predictions on the AffectNet dataset and the emotion label distribution.</p><p>stochastic gradient descent was used to minimize the crossentropy loss function with an initial learning rate of 0.1, a momentum of 0.9, and a learning rate decay with a multiplicative factor of 0.5 applied after every 10 epochs. Convolutional branches were added to the ensemble until no significant gain in accuracy was achieved by the collective classification. Trained branches were continually updated on additional training data with a lower initial learning rate of 0.01 for their adaptation to the representational changes in the shared layers.</p><p>Our results are reported in <ref type="table" target="#tab_3">Table 3</ref> and <ref type="figure">Figure 6</ref>. The ESR with 9 convolutional branches (ESR-9) achieved the highest recognition performance on AffectNet in comparison to state-of-the-art methods. It is important to note that no single branch in the ensemble achieved an accuracy higher than 58.0%, only the collective classification made by the ensemble reached 59.3% of accuracy, which reveals that the remaining residual generalization error was reduced by the ESR. In the confusion matrix, we can see that ESR-9 was more accurate in the recognition of the happiness category but under-represented categories such as fear, disgust, and contempt were still well recognized given the disparity of  <ref type="bibr" target="#b15">(Hewitt and Gunes 2018)</ref> 0.37 0.41 MobileNet (Hewitt and Gunes 2018) 0.38 0.42 AlexNet <ref type="bibr" target="#b15">(Hewitt and Gunes 2018)</ref> 0.39 0.41 AlexNet <ref type="bibr" target="#b29">(Mollahosseini et al. 2019)</ref> 0.41 0.37 VGG16-Based <ref type="bibr" target="#b24">(Lindt et al. 2019)</ref> 0.41 0.45 the label distribution. Finally, even though <ref type="bibr" target="#b23">Li et al. (2019)</ref>'s ensemble has obtained an accuracy of 58.8%, the contempt category was removed from their experiments, which greatly reduced the complexity of the task since the chance of the network for learning undesired features decreased. Continuous affect perception. Predicting arousal and valence levels of facial images in a continuous space is a complex task where disagreement levels between human annotators are usually higher than in discrete emotion annotations. Thus, we trained the ESR in a curriculum learning fashion <ref type="bibr" target="#b3">(Bengio et al. 2009</ref>), where ESR-9, trained for discrete facial expression recognition, was fine-tuned for arousal and valence prediction. We assumed that some facial features learned by ESR-9 from the previous task would lead the network to learn faster and become more accurate for inferring affect concepts than training it from scratch. For example, a smile detector usually learned after a few training epochs, as shown in our experiments on the in-the-lab dataset, can be associated with positive arousal and positive valance levels. Instead of replacing the output layer of ESR-9 to account for arousal and valence predictions, we added two neurons on top of each branch, as shown in <ref type="figure">Figure 3</ref>, and trained only the weights connected to those neurons. Since the relation of discrete emotions and continuous affect is non-linear, we applied the ReLU function to the second last layer that is related to discrete emotion concepts.</p><p>We followed the same training procedure as in our previous experiments where each branch is sequentially trained on a balanced subset with up to 5000 samples from each quadrant of the arousal and valence circumplex to reduce bias. However, since arousal and valence prediction in the continuous domain is a regression problem, we minimize the root-mean-square error using stochastic gradient descent with a momentum of 0.9 and a learning rate of 0.01. Trained branches were continually updated with a lower learning rate of 0.001. The results are reported in <ref type="table" target="#tab_4">Table 4</ref>. ESR-9 outperformed state-of-the-art methods based on established pretrained deep neural networks for visual classification tasks with a significant margin on both of the arousal and valence dimensions by achieving 0.33 and 0.36 RMSEs, respectively. Moreover, since only the output layer was trained, ESR-9 can still perform discrete emotion perception which resulted in a great drop in computational load and redundancy. In comparison to <ref type="bibr" target="#b29">Mollahosseini, Hasani, and Mahoor (2019)</ref>'s approach which has approximately 180M parameters in total, ESR-9 has 9 times fewer parameters (≈ 20M ).</p><p>In their work, three AlexNets were trained, one for each of the three facial expression perception problems. Finally, ESR-9 reached the performance of human experts in facial expression annotations which have a disagreement level of 0.36 and 0.34 RMSEs for arousal and valence prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fine-tuning on the FER+ Dataset</head><p>In our experiments on FER+, we rescaled the images from 48 x 48 pixels to 96 x 96 pixels and fine-tuned ESR-9 trained on AffectNet. Before any training on FER+, ESR-9 achieved a test accuracy of 57.92%, similar to its performance on Af-fectNet. This cross-dataset evaluation indicates that ESR-9 generalizes well to different data distributions. It is important to note that facial images from FER+ are gray-scale images with low resolution and are not as centralized as Af-fectNet's images. These aspects may deteriorate certain facial features in images relevant for emotion perception until a point where they can no longer be detected, as argued by <ref type="bibr" target="#b35">Tian, Kanade, and Cohn (2005)</ref>. We fine-tuned ESR-9 using the stochastic gradient descent with a momentum of 0.9, an initial learning rate of 0.1, and a learning rate decay with a multiplicative factor of 0.75 applied after every 10 epochs. The learning rate decay was increased due to the faster convergence of ESR-9 on FER+. Trained branches were continually updated with a lower initial learning rate of 0.02.</p><p>After fine-tuning each branch sequentially on random subsets with up to 5000 training samples per emotion category on FER+, ESR-9 reached an average test accuracy of 87.153% with a very low standard deviation of 0.097%, outperforming the current state-of-the-art method <ref type="bibr" target="#b2">(Barsoum et al. 2016)</ref>. Our results are reported in <ref type="table" target="#tab_5">Table 5</ref> and <ref type="figure" target="#fig_4">Figure 7</ref>. Also, ESR-9 generalized reasonably well to underrepresented categories. When compared to <ref type="bibr" target="#b2">Barsoum et al. (2016)</ref>'s approach, for instance, ESR-9 correctly recognized 20.0% of the contempt test samples and 56.2% of the disgust test samples, while their approach recognized only 4.17%, and 26.32% respectively. The bias towards neutral classifications was also reduced in almost all categories. While our approach misclassified 40% of contempt samples as neutral and had no misclassification of disgust samples as neutral, their approach misclassified 54.17% of contempt and 10.53% of disgust samples. The bias problem in facial expression recognition is not solely related to the unbalance problem, but also to the inherent subjectivity of emotion perception on faces where humans may perceive different emotions in the same facial expression as illustrated in <ref type="figure" target="#fig_5">Figure 8</ref>  <ref type="bibr" target="#b1">(Barrett 2017;</ref><ref type="bibr" target="#b29">Mollahosseini, Hasani, and Mahoor 2019)</ref>.</p><p>It is relevant to mention that we extensively investigated the effects of varying the maximum number of training samples for each emotion category on FER+. When trained with lower upper bounds, ESRs increased recognition on underrepresented categories but the overall accuracy decreased. If the upper bound is too high, the diversity of the ensemble decreased. In this experiment, an upper-bound of 5000 samples for each category was the "optimal" value to achieve high overall accuracy and a relatively high correct classification of under-represented categories. Finally, our findings suggest that our approach to ESRs is an important contribution to alleviate the bias problem in machine learning. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Approach</head><p>Acc ↓ ESR-9 (Our network)</p><p>87.15 ± 0.1% SHCNN <ref type="bibr" target="#b27">(Miao et al. 2019)</ref> 86.54% VGG16-PLD <ref type="bibr" target="#b2">(Barsoum et al. 2016)</ref> 84.99 ± 0.37% VGG16-CEL <ref type="bibr" target="#b2">(Barsoum et al. 2016)</ref> 84.72 ± 0.24% TFE-JL <ref type="bibr" target="#b22">(Li et al. 2018)</ref> 84.3% VGG16-ML <ref type="bibr" target="#b2">(Barsoum et al. 2016)</ref> 83.97 ± 0.36% VGG16-MV <ref type="bibr" target="#b2">(Barsoum et al. 2016)</ref> 83.85 ± 0.63% ResNet18 + FC <ref type="bibr" target="#b22">(Li et al. 2018)</ref> 83.4% ResNet18 <ref type="bibr" target="#b22">(Li et al. 2018)</ref> 83.1% </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>Referring to Minsky at the beginning of this paper, one may think that single deep neural networks trained on large-scale datasets are enough to build rich, robust and highly accurate perceptual models. However, in certain domains where label distribution is unbalanced, for instance, those networks tend to become highly biased to the most representative categories. We demonstrated that ensembles with shared representations cope with this problem by training "many resources" (i. e., convolutional branches) on balanced subsets from the training data. Together, through the collective classification made by the ensemble, ESRs outperformed stateof-the-art deep neural networks on AffectNet and FER+ with low redundancy and an efficient transfer learning mechanism from the shared layers. Moreover, we showed that the branching level directly impacts ensemble diversity, generalization, and computational load. Artificial neural networks, when trained under continual learning settings, typically suffer from a phenomenon called catastrophic forgetting. Correct classified samples become misclassified when the network is continually trained on a different data distribution due to its inability to keep learned information. The same phenomenon occurs when training additional branches in ESRs having a direct impact on the generalization performance. To address the effects of catastrophic forgetting on ESRs, learning rates of the trained branches and shared layers should be carefully defined. High differences in learning rates may cause trained branches to forget learned information, whereas similar learning rates may foster co-adaptation between branches and decrease ensemble diversity. In the future, we will investigate approaches to overcome catastrophic forgetting in ensembles with shared representations. Despite reaching human-level performance in facial expression recognition on AffectNet, human-level affect inference under real-world conditions is far to be reached. To do so, computational models closer to recent findings that are changing and enhancing our understanding of emotions under the theory of psychological construction (Barrett and Russell 2015) should be developed. It is important to take into consideration not only cross-modal learning of emotional expressions but also temporal and contextual information during emotional episodes. As the next step, we will implement a model closer to the theory of constructed emotion (Barrett 2017) by adopting ESR-9's high-level representations as "proto concepts" of facial expressions to guide learning of emotion concepts in a hybrid neural system based on an intermediate view between empiricism and nativism of the cognition theory <ref type="bibr" target="#b36">(Ullman 2019)</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>On the arXiv:2001.06338v1 [cs.CV] 17 Jan 2020 Ensemble with Shared Representations (ESR). Illustration of the experiments to investigate the effect of branching level on computational load and generalization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1 :</head><label>1</label><figDesc>Training ESRs. initialize the shared layers with θ shared for b to maximum ensemble size do initialize the convolutional branch B b with θ b add the branch B b to the network ESR sample a subset D from a training set D foreach mini-batch (x i , y i ) ∼ D do perform the forward phase initialize the combined loss function L esr to 0.0 foreach existing branch B b in ESR do compute the loss L b with respect to B b add L b to L esr end perform the backward phase optimize ESR end end</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Experimental datasets. Extended Cohn-Kanade (CK+), AffectNet and FER+, from the top to the bottom.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>On the top, the architecture used in the in-the-lab experiments and an illustration of ESR-4 Lvl. 3 on the left. On the bottom, the architecture used in the in-the-wild experiments and an illustration of ESR-9 on the right. The latter architecture was designed to be equivalent to the former with respect to the spatial information of the features. The ReLU activation function is applied after batch normalization layers. The last linear layer in the bottom architecture was added for the affect perception experiment only. Each color represents a different type of layer and the PyTorch nomenclature was followed for reproducibility. Accuracy on the Extended Cohn-Kanade dataset increasing branching level for different training strategies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Normalized confusion matrix of the ensemble predictions on FER+ and the emotion label distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Subjective perception of facial expressions. Samples annotated as fear by one expert human annotator perceived differently by another expert. Adapted from (Mollahosseini, Hasani, and Mahoor 2019).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Test accuracy (%) of the most accurate networks and baselines on CK+ and their number of parameters.</figDesc><table><row><cell>Approach</cell><cell>#</cell><cell>Accuracy</cell></row><row><cell>Single Network</cell><cell>131.208</cell><cell>85.5 ± 3.5%</cell></row><row><cell cols="2">Traditional Ensemble 524.832</cell><cell>89.2 ± 1.2%</cell></row><row><cell>ESR-4 Lvl. 3</cell><cell cols="2">355.104 89.4 ± 2.2%</cell></row><row><cell>ESR-4 Lvl. 4</cell><cell>243.936</cell><cell>88.5 ± 3.8%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell cols="4">: Paired t-test (p-values) to compare Single Network,</cell></row><row><cell cols="4">Trad. Ensemble, ESR-4 Lvl. 3, and ESR-4 Lvl. 4 on CK+.</cell></row><row><cell></cell><cell>TE</cell><cell>Lvl. 3</cell><cell>Lvl. 4</cell></row><row><cell>Single Network</cell><cell cols="3">0.004 0.005 0.043</cell></row><row><cell>Trad. Ensemble (TE)</cell><cell>−</cell><cell cols="2">0.956 0.614</cell></row><row><cell>Lvl. 3</cell><cell>−</cell><cell>−</cell><cell>0.514</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Accuracy (%) on AffectNet for discrete emotions and number of emotion labels used in the experiments.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Approach</cell><cell></cell><cell></cell><cell></cell><cell>#</cell><cell cols="2">Acc ↓</cell></row><row><cell></cell><cell></cell><cell cols="7">ESR-9 (Our network)</cell><cell></cell><cell cols="3">8 59.3%</cell></row><row><cell cols="13">AlexNet-WL (Mollahosseini et al. 2019) 8 58.0%</cell></row><row><cell></cell><cell cols="9">VGGNet (Hewitt and Gunes 2018)</cell><cell cols="3">8 58.0%</cell></row><row><cell></cell><cell cols="9">MobileNet (Hewitt and Gunes 2018)</cell><cell cols="3">8 56.0%</cell></row><row><cell></cell><cell cols="9">AlexNet (Hewitt and Gunes 2018)</cell><cell cols="3">8 56.0%</cell></row><row><cell cols="13">AlexNet-US (Mollahosseini et al. 2019) 8 47.0%</cell></row><row><cell cols="13">AlexNet-DS (Mollahosseini et al. 2019) 8 40.0%</cell></row><row><cell></cell><cell></cell><cell cols="7">gACNN (Li et al. 2019)</cell><cell></cell><cell cols="3">7 58.8%</cell></row><row><cell></cell><cell cols="9">IPA2LT (Zeng, Shan, and Chen 2018)</cell><cell cols="3">7 57.3%</cell></row><row><cell></cell><cell></cell><cell cols="7">pACNN (Li et al. 2019)</cell><cell></cell><cell cols="3">7 55.3%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">AffectNet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>#</cell></row><row><cell></cell><cell>Ne</cell><cell>58.0</cell><cell>3.4</cell><cell>9.4</cell><cell>9.8</cell><cell>2.8</cell><cell>3.2</cell><cell>6.4</cell><cell>7.0</cell><cell cols="2">Ne</cell><cell>75,374</cell></row><row><cell></cell><cell>Ha</cell><cell>4.0</cell><cell>77.4</cell><cell>1.2</cell><cell>2.8</cell><cell>0.4</cell><cell>2.0</cell><cell>0.4</cell><cell>11.8</cell><cell>Ha</cell><cell></cell><cell>134,915</cell></row><row><cell></cell><cell>Sa</cell><cell>13.6</cell><cell>1.6</cell><cell>61.4</cell><cell>3.6</cell><cell>4.8</cell><cell>5.4</cell><cell>8.4</cell><cell>1.2</cell><cell>Sa</cell><cell></cell><cell>25,959</cell></row><row><cell>Target</cell><cell>Su Fe</cell><cell>9.6 3.8</cell><cell>7.8 1.6</cell><cell>3.4 8.4</cell><cell>55.4 13.4</cell><cell>17.8 63.6</cell><cell>2.4 2.4</cell><cell>2.4 6.4</cell><cell>1.2 0.4</cell><cell>Su Fe</cell><cell></cell><cell>14,590 6,878</cell></row><row><cell></cell><cell>Di</cell><cell>4.8</cell><cell>4.8</cell><cell>6.8</cell><cell>3.0</cell><cell>5.0</cell><cell>53.8</cell><cell>19.2</cell><cell>2.6</cell><cell>Di</cell><cell></cell><cell>4,303</cell></row><row><cell></cell><cell>An</cell><cell>12.8</cell><cell>1.4</cell><cell>6.8</cell><cell>4.2</cell><cell>4.4</cell><cell>9.4</cell><cell>59.0</cell><cell>2.0</cell><cell cols="2">An</cell><cell>25,382</cell></row><row><cell></cell><cell>Co</cell><cell>16.4</cell><cell>18.6</cell><cell>3.6</cell><cell>3.2</cell><cell>1.0</cell><cell>4.4</cell><cell>7.4</cell><cell>45.4</cell><cell cols="2">Co</cell><cell>4,250</cell></row><row><cell></cell><cell></cell><cell cols="8">Ne Ha Sa Su Fe Di An Co</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="6">Ensemble Prediction</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Root-mean-square error (RMSE) for arousal (aro) and valence (val) prediction on the AffectNet dataset.</figDesc><table><row><cell>Approach</cell><cell>RMSE Aro ↓ Val</cell></row><row><cell>ESR-9 (Our network)</cell><cell>0.33 0.36</cell></row><row><cell>VGGNet</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Mean and standard deviation of the test accuracy on FER+. Some authors only reported the best accuracy.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work has received funding from the European Union's Horizon 2020 research and innovation program under the Marie Skłodowska-Curie grant agreement No. 721619 for the SOCRATES project. The authors thank Prof. Dr. Thomas Hellström for his insightful questions that motivated the development of this paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">An introduction to psychological construction. The psychological construction of emotion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">F</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Russell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">How emotions are made: The secret life of the brain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">F</forename><surname>Barrett</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<pubPlace>Houghton Mifflin Harcourt</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Training deep networks for facial expression recognition with crowd-sourced label distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Barsoum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Ferrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="279" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bagging predictors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="123" to="140" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Ensemble selection from libraries of models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Crew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ksikes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="18" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Deep Learning with Python and Keras: The Handbook by the Developer of the Keras Library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>MITP-Verlag GmbH &amp; Co. KG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">The expression of the emotions in man and animals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Darwin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1872" />
			<publisher>John Murray</publisher>
			<pubPlace>UK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Ensemble methods in machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multiple Classifier Systems</title>
		<meeting><address><addrLine>Berlin, Heidelberg; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Measuring facial movement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">V</forename><surname>Friesen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Envir. Psycho. and Nonverbal Behavior</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="56" to="75" />
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The argument and evidence about universals in facial expressions. Handbook of Social Psychophysiology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<biblScope unit="page" from="143" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Universality reconsidered: Diversity in making meaning of facial expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gendron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Crivelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">F</forename><surname>Barrett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Directions in Psychological Science</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="211" to="219" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Challenges in representation learning: A report on three machine learning contests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Carrier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hamner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cukierski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Thaler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ramaiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Athanasakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Milakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Grozea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Romaszko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="59" to="63" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Special Issue on Deep Learning of Representations</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neural network ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Salamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on PAMI</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="993" to="1001" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Cnn-based facial affect analysis on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hewitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gunes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.08775</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<editor>Bach, F., and Blei, D.</editor>
		<meeting>the 32nd International Conference on Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">The principles of psychology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Burkhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bowers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">K</forename><surname>Skrupskelis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1890" />
			<publisher>Macmillan London</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">What is an emotion? Mind</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>James</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1884" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="188" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Do deep neural networks learn facial action units when doing expression recognition?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Khorrami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Paine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE on CVPR -Workshops</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Emotion recognition from speech: a review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Koolagudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJST</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="99" to="117" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Facial expression recognition with identity and emotion joint learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Affective Computing</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Occlusion aware facial expression recognition using cnn with attention mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on IP</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2439" to="2450" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Facial expression editing with continuous emotion labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lindt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Siqueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wermter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th IEEE Int. Conf. on FG 2019</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotionspecified expression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ambadar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Society Conference on CVPR -Workshops</title>
		<imprint>
			<biblScope unit="page" from="94" to="101" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Efficient diverse ensemble for discriminative co-tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Meshgi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Recognizing facial expressions using a shallow convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="78000" to="78011" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minsky</surname></persName>
		</author>
		<ptr target="https://www.youtube.com/watch?v=RZ3ahBm3dCk" />
		<title level="m">Is the singularity near? Available at</title>
		<imprint>
			<date type="published" when="2014-07-23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Affectnet: A database for facial expression, valence, and arousal computing in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mollahosseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="18" to="31" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A review of affective computing: From unimodal analysis to multimodal fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bajpai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hussain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="98" to="125" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
	<note>Core affect and the psychological construction of emotion</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Query by committee</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Seung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Opper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sompolinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 5th Workshop on Comp. Learning Theory</title>
		<meeting>of the 5th Workshop on Comp. Learning Theory</meeting>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="287" to="294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Meal: Multi-model ensemble via adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI Conference on Artificial Intelligence</title>
		<meeting>AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">An ensemble with shared representations based on convolutional networks for continually learning facial expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Siqueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Magg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wermter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ Int. Conf. on IROS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1563" to="1568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Facial Expression Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="247" to="275" />
			<pubPlace>New York, NY; New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Using neuroscience to develop artificial intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">363</biblScope>
			<biblScope unit="issue">6428</biblScope>
			<biblScope unit="page" from="692" to="693" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Robust real-time face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="154" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Facial expression recognition with inconsistently annotated datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
