<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">See Better Before Looking Closer: Weakly Supervised Data Augmentation Network for Fine-Grained Visual Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Qi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Lu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">See Better Before Looking Closer: Weakly Supervised Data Augmentation Network for Fine-Grained Visual Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Data augmentation is usually adopted to increase the amount of training data, prevent overfitting and improve the performance of deep models. However, in practice, random data augmentation, such as random image cropping, is low-efficiency and might introduce many uncontrolled background noises. In this paper, we propose Weakly Supervised Data Augmentation Network (WS-DAN) to explore the potential of data augmentation. Specifically, for each training image, we first generate attention maps to represent the object's discriminative parts by weakly supervised learning. Next, we augment the image guided by these attention maps, including attention cropping and attention dropping. The proposed WS-DAN improves the classification accuracy in two folds. In the first stage, images can be seen better since more discriminative parts' features will be extracted. In the second stage, attention regions provide accurate location of object, which ensures our model to look at the object closer and further improve the performance. Comprehensive experiments in common fine-grained visual classification datasets show that our WS-DAN surpasses the state-ofthe-art methods, which demonstrates its effectiveness.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Data augmentation is a frequently-used strategy that can improve the generalization of deep models since it increases the number of training data by introducing more data variances. It has been proven to be effective in most of the computer vision tasks, such as object classification, detection, and segmentation. There are various kinds of data augmentation for deep models, including image cropping, rotation and color distortion. Previous works usually choose random data augmentation to pre-process their training data. For instance, random image cropping can generate images with different translations and scales so as to increase the robustness of deep models. However, the cropped regions are parts of the object. We randomly choose one of the part regions, then drop it to generate more discriminative object's parts or crop it to extract more detailed part feature. Look Closer: The whole object is localized from attention maps and enlarged to further improve the accuracy. randomly sampled and a high percentage of them contain many background noises, which might lower the training efficiency, affect the quality of the extracted features and cancel out its benefits. To improve the efficiency of data augmentation, the model should be aware of spatial information of target objects.</p><p>Fine-Grained Visual Classification (FGVC) aims to classify the subordinate-level categories under a basic-level category, such as species of the bird, model of the car and type of the aircraft. FGVC is challenging because of three main reasons: (1) High intra-class variances. Objects that belong to the same category usually present significantly different poses and viewpoints; (2) Low inter-class variances. Objects that belong to different categories might be very similar apart from some minor differences, e.g. the color styles of a bird's head can usually determine its category; (3) Limited training data. Labeling fine-grained categories usually requires specialized knowledge and a large amount of annotation time. Because of the these reasons, it is hard to obtain accurate classification results only by the state-of-theart coarse-grained Convolutional Neural Networks (CNN), such as VGG <ref type="bibr" target="#b22">[23]</ref>, ResNet <ref type="bibr" target="#b6">[7]</ref> and Inception <ref type="bibr" target="#b25">[26]</ref>.</p><p>As is pointed out in recent works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b29">30]</ref>, the key steps of FGVC is extracting more discriminative local features in multiple object's parts. However, object's parts are difficult to be defined and vary from object to object. Moreover, labeling these object's parts requires additional human cost. In this work, we utilize weakly supervised learning to locate the discriminative object's parts only by imagelevel annotation. Instead of proposing region of interest bounding boxes <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5]</ref>, we represent object's parts or visual pattern by attention maps which are generated by the convolutions. We also propose bilinear attention pooling and attention regularization loss to weakly supervise the attention generation process. Compared with other part localization model <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b29">30]</ref>, our model can be more easily to locate a large number of object's parts (more than 10) so as to achieve better performance.</p><p>After obtaining the locations of objects' parts, we propose attention-guided data augmentation to effectively augment the training data and solve the above mentioned high intra-class variances and low inter-class variances issues. For different fine-grained categories, objects are usually very similar, apart from few differences. Attention cropping can distinguish them by cropping and resizing the part regions to extract more discriminative local features. For the same fine-grained category, if the model only focuses on few object's parts, it is very likely to predict the wrong category when these parts are occluded because of pose and viewpoint variances. Therefore, it is crucial to extract the local features from different object's parts. Our attention dropping randomly erases one region of objects' part out of the image in order to encourage the network to extract discriminative features from other object's parts. Thus, through the attention-guided data augmentation, our model can extract more discriminative features in multiple object's parts, which means the object can be seen better.</p><p>Another benefit of our attention-guided data augmentation is that we can accurately locate objects, which makes our model look at objects closer and refine the predictions. For each testing image, object category will be coarse-tofine predicted. The model first predicts object's region and coarse-stage probability of category from the raw image. Subsequently, the object's region is enlarged and the finestage probability is predicted.</p><p>In summary, the main contributions of this work are:</p><p>1. We propose Weakly Supervised Attention Learning to generate attention maps to represent the spatial distribution of discriminative object's parts, and extract a sequential local features to solve the fine-grained vi-sual classification problem.</p><p>2. Based on attention maps, we propose attention-guided data augmentation to improve the efficiency of data augmentation, including attention cropping and attention dropping. Attention cropping randomly crops and resizes one of the attention part to enhance the local feature representation. Attention dropping randomly erases one of the attention region out of the image in order to encourage the model to extract the feature from multiple discriminative parts.</p><p>3. We utilize attention maps to accurately locate the whole object and enlarge it to further improve the classification accuracy.</p><p>Experiments in common fine-grained visual classification datasets demonstrates our data augmentation can significantly improve the accuracy of fine-grained classification and object localization, which surpasses the state-of-the-art methods and baselines.</p><p>The rest of the paper is organized as follows. We first review the related works, including data augmentation and fine-grained visual classification in Section 2, then describe the proposed Weakly Supervised Data Augmentation Network (WS-DAN) in Section 3. In Section 4, comprehensive experiments are conducted to demonstrate the effectiveness of WS-DAN. Finally, conclusion is drawn in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>In this section, we review the related works of finegrained image classification, data augmentation and weakly supervised localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Fine-grained Visual Classification</head><p>A variety of methods have been developed to distinguish different fine-grained categories. Convolutional Neural Networks were proposed to solve the large scale image classification problem. However, according to the experiments results in <ref type="table" target="#tab_7">Table 4</ref>, these basic models can only achieve the moderate performance, because it is relatively difficult for them to focus on the subtle differences of object's parts without special design.</p><p>To focus on the local features, many methods rely on the annotations of parts location or attribute. Part R-CNN <ref type="bibr" target="#b34">[35]</ref> extended R-CNN <ref type="bibr" target="#b5">[6]</ref> to detect objects and localize their parts under a geometric prior, then predicted a fine-grained category from a pose-normalized representation. Lin et al. proposed a feedback-control framework Deep LAC <ref type="bibr" target="#b12">[13]</ref> to back-propagate alignment and classification errors to localization; they also proposed a valve linkage function (VLF) to connect the localization and classification modules.</p><p>To reduce aditional location labeling cost, methods that only require image-level annotation draw more attention.</p><p>Different feature pooling methods have been proposed. Lin et al. proposed bilinear pooling <ref type="bibr" target="#b15">[16]</ref> and improved bilinear pooling <ref type="bibr" target="#b14">[15]</ref>, where two features are combined at each location using the outer product, which considers their pairwise interactions. MPN-COV <ref type="bibr" target="#b11">[12]</ref> improved second-order pooling by matrix square and achieved the state-of-the-art accuracy.</p><p>Spatial Transformer Network (ST-CNN) <ref type="bibr" target="#b8">[9]</ref> aims to achieve accurate classification performance by first learning a proper geometric transformation and align the image before classifying. The method can also locate several object's parts at the same time. Fu et al. proposed Recurrent Attention CNN (RA-CNN) <ref type="bibr" target="#b4">[5]</ref> to recurrently predict the location of one attention area and extract the corresponding feature, while the method only focuses on one local part, so they combine three scale feature, i.e. three object's parts, to predict the final category. To generate multiple attention locations at the same time, Zheng et al. proposed Multi-Attention CNN (MA-CNN) <ref type="bibr" target="#b37">[38]</ref>, which simultaneously locates several body parts. And they proposed channel grouping loss to generate multiple parts by clustering. However, the number of object's parts is limited (2 or 4), which might constrain their accuracy. In this paper, we propose bilinear attention pooling which combines attention layers with featur layers, whose number of attention regions is more easily to be increased and improve the classification accuracy as shown in <ref type="table" target="#tab_12">Table 9</ref>.</p><p>Also, metric learning has been introduced in FGVC task.  <ref type="bibr" target="#b3">[4]</ref> to combine the pairwise confusion loss with cross entropy loss to learn features with greater generalization, thereby preventing overfitting. In our model, attention regularization loss is proposed to regular the attention regions and corresponding local features, which improves the identities of object's parts and the classification accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Data Augmentation</head><p>Our data augmentation method focuses on image's spatial augmentation. Before our work, random spatial image augmentation methods <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b23">24]</ref>, such as image cropping and image dropping, have been proposed and proved to be effective in improving the robustness of deep models. Maxdrop <ref type="bibr" target="#b18">[19]</ref> aims to remove the maximally activated features to encourage the network to consider the less prominent features. The drawback is that Max-drop can only remove one discriminative region of each image, which limits their performance. Cutout <ref type="bibr" target="#b2">[3]</ref> and Hide-and-Seek <ref type="bibr" target="#b23">[24]</ref> improve the robustness of CNNs by randomly masking many square regions out of training images. However, a great many of the erased regions are unrelavant background, or the whole ob-ject might be erased out, especially for small objects.</p><p>Random data augmentation suffers from low efficiency and generating much uncontrolled noisy data. To overcome these issue, a few methods have been proposed to take data distribution into consideration, which is more effective than a random data augmentation. Cubuk et al. proposed Au-toAugmentation <ref type="bibr" target="#b1">[2]</ref> to create a search space of data augmentation policies. It can automatically design a specific policy so as to obtain state-of-the-art validation accuracy for target dataset. Peng et al. proposed Adversarial Data Augmentation <ref type="bibr" target="#b19">[20]</ref> to jointly optimize data augmentation and deep model. They designed an augmentation network to online generate hard data and improve the robustness of the deep model. However, their data-specific augmentation is significantly complicated than random augmentation. Our attention-guided data augmentation is more simple and can generate part-level augmentation to boost the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Weakly Supervised Learning for Localization</head><p>Weakly supervised learning is an umbrella term that covers a variety of studies that attempt to construct predictive models by learning with weak supervision <ref type="bibr" target="#b39">[40]</ref>. Accurately locating the object or its parts only by image-level supervision is very challenging. Early works <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b33">34]</ref> usually generate class-specific localization maps by Global Average Pooling (GAP) <ref type="bibr" target="#b13">[14]</ref>. The activation area can reflect the location of an object. However, training by softmax cross entropy loss usually leads the model to pay attention to the most discriminative location, whose output bounding box just covers part of the object. To locate the whole object. Singh et al. <ref type="bibr" target="#b23">[24]</ref> randomly hides the patches of input images so as to force the network to find other discriminative parts. However, the process is inefficient for the lack of high-level guidance. Zhang et al. proposed Adversarial Complementary Learning (ACoL) <ref type="bibr" target="#b35">[36]</ref> approach to discover entire objects by training two adversary complementary classifiers, which can locate different object's parts and discover the complementary regions that belong to the same object. Nevertheless, there are only two complementary regions in their implementation, which limits accuracy. Our attention-guided data augmentation encourages the model to pay attention to multiple object's parts, extract more discriminative features and achieve significantly performance in object localizing, as shown in <ref type="table" target="#tab_11">Table 8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>In this section, we describe the proposed WS-DAN in detail, including weakly supervised attention learning, attention-guided data augmentation and object localization &amp; refinement. The overview structure of proposed WS-DAN is illustrated in <ref type="figure" target="#fig_2">Fig 2.</ref> (a) Training process. (A) Weakly Supervised Attention Learning. For each training image, attention maps will be generated to represent the object's discriminative parts by weakly supervised attention learning. (B) Attention-Guided Data Augmentation. One attention map is randomly selected to augment this image, including attention cropping and attention dropping. Finally, the raw and augmented data will be trained as input data.</p><p>(b) Testing process. Firstly, object's categories probability and attention maps will be outputed from raw image by (A). Secondly, object will be located according to (C) and then be enlarged to refine the categories probability. Finally, the above two probabilities will be combined as the final prediction. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Weakly Supervised Attention Learning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Spatial Representation</head><p>We first predict the parts' regions of objects. During training and testing, the object's location annotation (e.g. bounding boxes or keypoints) is not available. In our method, we adopt weakly supervised learning to predict objects' location distribution only by their category annotations.</p><p>We extract the feature of image I by CNN and denote F ∈ R H×W ×N as feature maps, where H, W and C represents feature layer's height, width and the number of channels respectively. The distributions of objects' parts are represented by Attention Maps A ∈ R H×W ×M which is ob-tained from F by</p><formula xml:id="formula_0">A = f (F ) = M k=1 A k<label>(1)</label></formula><p>where f (·) is a convolutional function. A k ∈ R H×W represents one of the object's part or visual pattern, such as the head of a bird, the wheel of a car or the wing of an aircraft. M is the number of attention maps. Atteniton maps will be utilized to augment training data in Section 3.2. We propose regions of object's parts by attention maps rather than SS <ref type="bibr" target="#b26">[27]</ref> or RPN <ref type="bibr" target="#b20">[21]</ref> since the former is more flexible and can be more easily trained end-to-end in FGVC task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Bilinear Attention Pooling</head><p>After representing object's parts by attention maps A, inspired by Bilinear Pooling <ref type="bibr" target="#b15">[16]</ref> that aggregates feature representation from two-stream network layers, we propose Bilinear Attention Pooling (BAP) to extract features from these parts. We element-wise multiply feature maps F by each attention map A k in order to generate M part feature maps F k , as shown in Equ 2. </p><formula xml:id="formula_1">F k = A k F (k = 1, 2, ..., M )<label>(2)</label></formula><p>where denotes element-wise multiplication for two tensors. Then, we further extract discriminative local feature by additional feature extraction function g(·), such as Global Average Pooling (GAP), Global Maximum Pooling (GMP) or convolutions, in order to obtain k th attention feature</p><formula xml:id="formula_2">f k ∈ R 1×N , f k = g(F k )<label>(3)</label></formula><p>Object's feature is represented by part feature matrix P ∈ R M ×N which is stacked by these part features f k . Let Γ(A, F ) indicates bilinear attention pooling between attention maps A and feature maps F . It can be represented in Equ 4,</p><formula xml:id="formula_3">P = Γ(A, F ) =     g(a 1 F ) g(a 2 F ) ... g(a M F )     =     f 1 f 2 ... f M    <label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Attention Regularization</head><p>For each fine-grained category, we expect that attention map A k can represent the same k th object's part. Inspired by center loss <ref type="bibr" target="#b31">[32]</ref> proposed to solve face recognition problem, we propose attention regularization loss to weakly supervised the attention learning process. We penalize the variances of features that belong to the same object's part, which means that part feature f k will get close to the a global feature center c k ∈ R 1×N and attention map A k will be activated in the same k th object's part. The loss function can be represented by L A in Equ 5,</p><formula xml:id="formula_4">L A = M k=1 f k − c k 2 2 (5)</formula><p>where c k is its part's feature center. c k can be initialized from zero and updated by moving average,</p><formula xml:id="formula_5">c k ← c k + β(f k − c k )<label>(6)</label></formula><p>where β controls the update rate of c k , and attention regularization loss only applies for raw images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Attention-guided Data Augmentation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Augmentation Map</head><p>Random data augmentation is low efficient, especially when the size of the object is small, and suffers from introducing a high percentage of background noises. With attention maps, data can be more efficiently augmented. For each training image, we randomly choose one of its attention map A k to guide the data augmentation process, and normalize it as k th Augmentation Map A * k ∈ R H×W .</p><formula xml:id="formula_6">A * k = A k − min(A k ) max(A k ) − min(A k )<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Attention Cropping</head><p>With augmentation map A * k , we can zoom into this part's region and extract more detailed local features. Specifically, we first obtain the Crop Mask C k from A * k by setting element A * k (i, j) which is greater than threshold θ c ∈ [0, 1] to 1, and others to 0, as represented in Equ 8.</p><formula xml:id="formula_7">C k (i, j) = 1, if A * k (i, j) &gt; θ c 0, otherwise.<label>(8)</label></formula><p>We then find a bounding box B k that can cover the whole selected positive region of C k and enlarge this region from raw image as the augmented input data, As illustrated in <ref type="figure" target="#fig_0">Fig 1.</ref> Since the scale of object's part increases, object can be seen better for extracting more fine-grained features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Attention Dropping</head><p>Attention regularization loss supervises each attention map A k ∈ R H×W to represent the same k th object's part, while different attention maps might focus on the similar object's part. To encourage attention maps represent multiple discriminative object's parts, we propose attention dropping. Specifically, we obtain attention Drop Mask D k by setting element A * k (i, j) which is greater than threshold θ d ∈ [0, 1] to 0, and others to 1, as shown in Equ 9,</p><formula xml:id="formula_8">D k (i, j) = 0, if A * k (i, j) &gt; θ d 1, otherwise.<label>(9)</label></formula><p>The k th part region will be dropped by masking image I with D k . The augmented image is illustrated in <ref type="figure" target="#fig_0">Fig 1.</ref> Since k th object's part is eliminated from the image, the network will be encouraged to propose other discriminative parts, which means the object can also be seen better: the robustness of classification and the accuracy of localization will be improved, as shown in <ref type="table" target="#tab_3">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Object Localization and Refinement</head><p>The attention-guided data augmentation can also contributes to previous weakly supervised attention learning process, which means the location of object can be more accurately predicted. In the testing process, after the model outputs the coarse-stage classification result and corresponding attention maps for the raw image, we can predict the whole region of the object and enlarge it to predict finegrained result by the same network model. Object Map A m that indicates the location of object is calculated by Equ 10.</p><formula xml:id="formula_9">A m = 1 M M k=1 A k<label>(10)</label></formula><p>The final classification result is averaged by the coarsegrained prediction and fine-grained prediction. The detailed process of Coarse-to-Fine prediction is described as Algorithm 1, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we show comprehensive experiments to verify the effectiveness of WS-DAN. Firstly, we explore the contribution of each proposed module. Then we compare our model with the state-of-the-art methods on four publicly available fine-grained visual classification datasets. Following this, we perform additional experiments to demonstrate the effect of the number of attention maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Experiments Settings</head><p>Datasets We compare our method with the state-of-theart on four FGVC datasets, including CUB-200-2011 <ref type="bibr" target="#b27">[28]</ref>, FGVC-Aircraft <ref type="bibr" target="#b17">[18]</ref>, Stanford Cars <ref type="bibr" target="#b10">[11]</ref> and Stanford Dog <ref type="bibr" target="#b9">[10]</ref>. Specific information of each dataset is shown in <ref type="table">Table</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implement Details</head><p>In the following experiments, we adopt Inception v3 <ref type="bibr" target="#b25">[26]</ref> as the backbone and choose Mix6e layer as feature maps. Attention maps are obtained by 1 × 1 Convolutional kernel. We adopt GAP as the feature pooling function g(·). The attention cropping and dropping threshold θ c and θ d are both set to 0.5.</p><p>We train the models using Stochastic Gradient Descent (SGD) with the momentum of 0.9, epoch number of 80, weight decay of 0.00001, and a mini-batch size of 16 on a P100 GPU. The initial learning rate is set to 0.001, with exponential decay of 0.9 after every 2 epochs. The code will be released in the near feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Accuracy Contribution</head><p>As described above, our WS-DAN mainly consists of four components, including Weakly Supervised Attention Learning, Attention Cropping, Attention Dropping, and Object Localization and Refinement (Loc.&amp; Refinement). We perform an experiment in CUB-200-2011 dataset and demonstrate that each component is effective to improve the accuracy, as shown in <ref type="table" target="#tab_3">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparision with Random Data Augmentation</head><p>We conduct this experiment to demonstrate the effectiveness of attention-guided data augmentation compared with random data augmentation. The baseline method is our  Weakly Supervised Attention Learning model. For clarity, we evaluate the performance only for the first stage. In this experiment, we evaluate the accuracy of object localization by Mean Intersection-over-Union (mIoU). Higher score of mIoU means more accurately in locating the object. The results are shown in <ref type="table" target="#tab_5">Table 3</ref>. We can see that attention-guided data augmentation is more efficient than random data augmentation.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Comparison with Stage-of-the-Art Methods</head><p>Fine-grained Classification Results We compare our method with state-of-the-art methods on above mentioned fine-grained classification datasets. The results are respectively shown in <ref type="table" target="#tab_7">Table 4</ref>, <ref type="table" target="#tab_8">Table 5</ref>, <ref type="table" target="#tab_9">Table 6</ref> and <ref type="table" target="#tab_10">Table 7</ref>. It can be seen that our WS-DAN achieves the state-of-art accuracy on all these fine-grained datasets. Espetially, we significantly improve the accuracy compared with the backbone Inception v3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Object Localization Results</head><p>The recent method ACoL <ref type="bibr" target="#b35">[36]</ref>   <ref type="bibr" target="#b22">[23]</ref> 77.8 ResNet-101 <ref type="bibr" target="#b6">[7]</ref> 83.5 Inception-V3 <ref type="bibr" target="#b25">[26]</ref> 83.7 B-CNN <ref type="bibr" target="#b15">[16]</ref> 84.1 ST-CNN <ref type="bibr" target="#b8">[9]</ref> 84.1 PDFR <ref type="bibr" target="#b36">[37]</ref> 84.5 RA-CNN <ref type="bibr" target="#b4">[5]</ref> 85.4 GP-256 <ref type="bibr" target="#b30">[31]</ref> 85.8 MA-CNN <ref type="bibr" target="#b37">[38]</ref> 86.5 MAMC <ref type="bibr" target="#b24">[25]</ref> 86.5 PC <ref type="bibr" target="#b3">[4]</ref> 86.9 DFL-CNN <ref type="bibr" target="#b28">[29]</ref> 87.4 NTS-Net <ref type="bibr" target="#b32">[33]</ref> 87.5 MPN-COV <ref type="bibr" target="#b11">[12]</ref> 88.7 WS-DAN 89.4  <ref type="bibr" target="#b22">[23]</ref> 80.5 ResNet-101 <ref type="bibr" target="#b6">[7]</ref> 87.2 Inception-V3 <ref type="bibr" target="#b25">[26]</ref> 87.4 B-CNN <ref type="bibr" target="#b15">[16]</ref> 84.1 RA-CNN <ref type="bibr" target="#b4">[5]</ref> 88.4 PC <ref type="bibr" target="#b3">[4]</ref> 89.2 GP-256 <ref type="bibr" target="#b30">[31]</ref> 89.8 MA-CNN <ref type="bibr" target="#b37">[38]</ref> 89.9 MPN-COV <ref type="bibr" target="#b11">[12]</ref> 91.4 NTS-Net <ref type="bibr" target="#b32">[33]</ref> 91.4 DFL-CNN <ref type="bibr" target="#b28">[29]</ref> 92.0 WS-DAN 93.0  <ref type="bibr" target="#b22">[23]</ref> 85.7 ResNet-101 <ref type="bibr" target="#b6">[7]</ref> 91.2 Inception-V3 <ref type="bibr" target="#b25">[26]</ref> 90.8 RA-CNN <ref type="bibr" target="#b4">[5]</ref> 92.5 MA-CNN <ref type="bibr" target="#b37">[38]</ref> 92.8 GP-256 <ref type="bibr" target="#b30">[31]</ref> 92.8 PC <ref type="bibr" target="#b3">[4]</ref> 92.9 MAMC <ref type="bibr" target="#b24">[25]</ref> 93.0 MPN-COV <ref type="bibr" target="#b11">[12]</ref> 93.3 DFL-CNN <ref type="bibr" target="#b28">[29]</ref> 93.8 NTS-Net <ref type="bibr" target="#b32">[33]</ref> 93.9 WS-DAN 94.5 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Effect of Number of Attention Maps</head><p>More object's parts usually contribute to the better performance. Similar conclusion has been made in MA-CNN <ref type="bibr" target="#b37">[38]</ref> and ST-CNN <ref type="bibr" target="#b8">[9]</ref>. We perform this experiment about the effectiveness of the number of attention maps M . <ref type="table" target="#tab_12">Table 9</ref> shows that with the increasing of M , the classification accuracy also rises. When M reaches to around 32, the performance gradually becomes stable, and the final ac-Method Accuracy(%) VGG-19 <ref type="bibr" target="#b22">[23]</ref> 76.7 ResNet-101 <ref type="bibr" target="#b6">[7]</ref> 85.8 Inception-V3 <ref type="bibr" target="#b25">[26]</ref> 88.9 NAC <ref type="bibr" target="#b21">[22]</ref> 68.6 PC <ref type="bibr" target="#b3">[4]</ref> 83.8 FCAN <ref type="bibr" target="#b16">[17]</ref> 84.2 MAMC <ref type="bibr" target="#b24">[25]</ref> 85.2 RA-CNN <ref type="bibr" target="#b4">[5]</ref> 87.3 WS-DAN 92.2   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Visualization of Augmented Data</head><p>In <ref type="figure" target="#fig_5">Fig 4,</ref> we visualize the augmented images by random data augmentation and attention-guided data augmentation in CUB-200-2011 and FGVC-Aircraft datasets. Intuitively, random data augmentation introduces much background into the training data. Attention-guided data augmentation is more efficient in cropping or dropping because of the guidance of location of objects' parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose Weakly Supervised Data Attention Network to significantly. We combine Weakly Supervised Learning (WSL) with Data Augmentation (DA). WSL provides objects' spatial distribution for DA and DA encourages the attention learning process of WSL. They benefit from each other and promote the model to extract (a) Comparision between Attention Cropping and Random Cropping. Random cropping is very likely to include a high percentage of background as input image, while attention cropping knows exactly where to crop to see better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(b) Comparision between Attention Dropping and Random Dropping.</head><p>Random dropping might erase the whole object out of the image or just erase background. Attention dropping is more efficient for erasing the discriminative object parts and promoting multiple attention. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>See Better: Attention maps represent discriminative</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Sun et al. proposed Multiple Attention Multiple Class (MAMC) loss [25] that pulls positive features closer to the anchor, while pushes negative features away. Dubey et al. proposed PC</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Overview training and testing process of Weakly Supservised Data Augmentation Network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>The process of Bilinear Attention Pooling. The network backbone(e.g. Inception v3 [26]) first generates feature maps (a) and attention maps (b) respectively . Each attention map represents a specific object's part; (c) Part feature maps are generated by element-wise multiplying each attention map with feature maps. Then, part features are extracted by convolutional or pooling operation; (d)The final feature matrix consists of all these part features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Algorithm 1</head><label>1</label><figDesc>Object Localization and Refinement Require: Trained WS-DAN model W Require: Raw image I r 1: Predict coarse-grained probability p 1 : p 1 = W (I r ) and output attention maps A; 2: Calculate object map A m of A by Equ. 10; 3: Obtain the bounding box B of object from A m ; 4: Enlarge the region in B as I o ; 5:Predict fine-grained probability p 2 : p 2 = W (I o ); 6: Calculate the final prediction: p = (p 1 + p 2 )/2; 7: return p</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Visualization of augmented images of random and attention-guided data augmentation. more discriminative image feature and from multiple local regions, which ensures WS-DAN to surpass the state-ofthe-art methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Introduction to four common fine-grained visual classification datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Contribution of proposed components and their combinations.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Comparison with random data augmentation in CUB-200-2011 dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>provided the performances of image-based object localization on CUB-200-2011 bird dataset. To compare with it, we evaluate our method on the same dataset with the same metric, i.e. calculating the localization error (failure percentage) of the images whose bounding boxes have less 50% IoU with the ground truth. Since most of the images of FGVC-Aircrafts and Stanford Cars datasets occupy the whole space, we only evaluate the localization error rate in CUB-200-2011 and Stanford Dogs datasets.The experimental results are shown inTable 8. Our model surpasses the state-of-the-art methods and baselines</figDesc><table><row><cell>Methods</cell><cell>Accuracy(%)</cell></row><row><cell>VGG-19</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Comparison with state-of-the-art methods on CUB-200-2011 testing dataset.</figDesc><table><row><cell>Methods</cell><cell>Accuracy(%)</cell></row><row><cell>VGG-19</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Comparison with state-of-the-art methods on FGVC-Aircraft testing dataset.</figDesc><table><row><cell>Methods</cell><cell>Accuracy(%)</cell></row><row><cell>VGG-19</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Comparison with state-of-the-art methods on Stanford Cars testing dataset.</figDesc><table /><note>by a large margin.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Comparison with state-of-the-art methods on Stanford Dog testing dataset.</figDesc><table><row><cell>Method</cell><cell>CUB-200-2011(%)</cell><cell>Stanford Dogs(%)</cell></row><row><cell>GoogLeNet</cell><cell>59.0</cell><cell>30.7</cell></row><row><cell>VGGnet-ACoL [36]</cell><cell>54.1</cell><cell>-</cell></row><row><cell>ResNet-101 [7]</cell><cell>42.1</cell><cell>29.6</cell></row><row><cell>Inception V3 [8]</cell><cell>40.8</cell><cell>28.8</cell></row><row><cell>WS-DAN</cell><cell>18.3</cell><cell>19.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Object Localization error rate on CUB-200-2011 and Stanford Dogs datasets. The localization error rate of our model is significantly lower than other methods.curacy reaches to 89.4%. Our feature pooling model makes it easy to set an arbitrary number of object's parts. We can achieve a more accurate result by increasing the number of attention maps.</figDesc><table><row><cell># Attention Maps</cell><cell>Accuracy(%)</cell></row><row><cell>4</cell><cell>86.6</cell></row><row><cell>8</cell><cell>87.7</cell></row><row><cell>16</cell><cell>88.3</cell></row><row><cell>32</cell><cell>89.4</cell></row><row><cell>64</cell><cell>89.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 :</head><label>9</label><figDesc>The effect of number of attention maps evaluated on CUB-200-2011 dataset.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Weakly supervised deep detection networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2846" to="2854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Autoaugment: Learning augmentation policies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno>abs/1805.09501</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno>abs/1708.04552</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pairwise confusion for fine-grained visual classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Farrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Naik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Look closer to see better: Recurrent attention convolutional neural network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="4476" to="4484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Novel dataset for fine-grained image categorization: Stanford dogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jayadevaprakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<title level="m">3d object representations for fine-grained categorization. ICCVW</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="554" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Towards faster training of global covariance pooling networks by iterative matrix square root normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deep lac: Deep localization, alignment and classification for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1666" to="1674" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno>abs/1312.4400</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improved bilinear pooling with cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<meeting><address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Bilinear cnn models for fine-grained visual recognition. ICCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Fully convolutional attention localization networks: Efficient attention localization for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<idno>abs/1603.06765</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Fine-grained visual classification of aircraft</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Analysis on the dropout effect in convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="189" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Jointly optimize data augmentation and network training: Adversarial data augmentation in human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2226" to="2234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-07" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodner</surname></persName>
		</author>
		<title level="m">Neural activation constellations: Unsupervised part model discovery with convolutional networks. ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1143" to="1151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hide-and-seek: Forcing a network to be meticulous for weakly-supervised object and action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3544" to="3553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multi-attention multi-class constraint for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="834" to="850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page" from="2818" to="2826" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E A</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="171" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">The Caltech-UCSD Birds-200-2011 Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno>- port CNS-TR-2011-001</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Re</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning a discriminative filter bank within a cnn for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Mask-cnn: Localizing parts and selecting descriptors for fine-grained bird species categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="704" to="714" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Grassmann pooling as compact homogeneous bilinear pooling for fine-grained visual classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A discriminative feature learning approach for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">9911</biblScope>
			<biblScope unit="page" from="499" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning to navigate for fine-grained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
		<title level="m">Topdown neural attention by excitation backprop. IJCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Partbased r-cnns for fine-grained category detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="834" to="849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Adversarial complementary learning for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Picking deep filter responses for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1134" to="1142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning multi-attention convolutional neural network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">À</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A brief introduction to weakly supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">National Science Review</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="44" to="53" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
