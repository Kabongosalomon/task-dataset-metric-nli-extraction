<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Video Object Segmentation with Language Referring Expressions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Khoreva</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Bosch Center for Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Video Object Segmentation with Language Referring Expressions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most state-of-the-art semi-supervised video object segmentation methods rely on a pixel-accurate mask of a target object provided for the first frame of a video. However, obtaining a detailed segmentation mask is expensive and time-consuming. In this work we explore an alternative way of identifying a target object, namely by employing language referring expressions. Besides being a more practical and natural way of pointing out a target object, using language specifications can help to avoid drift as well as make the system more robust to complex dynamics and appearance variations. Leveraging recent advances of language grounding models designed for images, we propose an approach to extend them to video data, ensuring temporally coherent predictions. To evaluate our approach we augment the popular video object segmentation benchmarks, DAVIS16 and DAVIS17 with language descriptions of target objects. We show that our language-supervised approach performs on par with the methods which have access to a pixel-level mask of the target object on DAVIS16 and is competitive to methods using scribbles on the challenging DAVIS17 dataset.</p><p>Query: "A man in a red sweatshirt performing breakdance" <ref type="figure">Figure 1</ref>: Examples of the proposed approach. Classical semi-supervised video object segmentation relies on an expensive pixel-level mask annotation of a target object in the first frame of a video. We explore a more natural and more practical way of pointing out a target object by providing a language referring expression.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Video object segmentation has recently witnessed growing interest <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b40">41]</ref>. Segmenting objects at pixel level provides a finer understanding of video and is relevant for many applications, e.g. augmented reality, video editing, and rotoscoping.</p><p>Ideally, one would like to obtain a pixel-accurate segmentation of objects in video with no human input during test time. However, the current state-of-the-art unsupervised video object segmentation methods <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b46">47]</ref> have troubles segmenting the target objects in videos containing multiple objects and cluttered backgrounds without any guidance from the user. Hence, many recent works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b49">50]</ref> employ a semi-supervised approach, where a pixel-level mask of the target object is manually annotated in the first frame and the task is to accurately segment the object in successive frames. Although this setting has proven to be successful, it can be prohibitive for many applications. It is tedious and time-consuming for the user to provide a pixel-accurate segmentation and usually takes more than a minute to annotate a single instance ( <ref type="bibr" target="#b25">[26]</ref> reports 79s for polygon annotations, precisely delineating an object would take even more). To make video object segmentation more applicable in practice, instead of costly pixel-level masks <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b1">2]</ref> propose to employ point clicks or scribbles to specify the target object in the first frame. This is much faster and takes an annotator on average 7.5s to label an object with point clicks <ref type="bibr" target="#b31">[32]</ref> and 10s with scribbles <ref type="bibr" target="#b24">[25]</ref>. However, on small touchscreen devices, such as tablets or phones, providing precise clicks or drawing scribbles using fingers could be cumbersome and inconvenient for the user.</p><p>To overcome these limitations we propose a new task -segmenting objects in video using language referring expressions -which is a more natural way of human-computer interaction. It is much easier for a user to say: "Segment the man in a red sweatshirt performing breakdance" (see <ref type="figure">Figure 1</ref>), than to provide a tedious pixel-level segmentation mask or struggle with drawing a scribble which does not straddle the object boundary. Moreover, employing language specifications can make the system more robust to background clutter, help to avoid drift and better adapt to the complex dynamics inherent to videos, while not over-fitting to a particular view in the first frame (see <ref type="table">Table 4</ref>).</p><p>We aim to investigate the capabilities and limitations of existing techniques on the proposed task and explore how far one can go while leveraging the advances in imagelevel language grounding and pixel-level segmentation in videos. We start by analyzing the performance of the state-of-the-art language grounding models <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b55">56]</ref> for localization of objects in videos via bounding boxes. We discover that they suffer from a number of issues, predicting temporally inconsistent and jittery boxes, and show a way to enhance their predictions by enforcing temporal coherency (see <ref type="figure">Figure 3</ref>). Next we propose a convnet-based framework that utilizes referring expressions for video object segmentation task, where the output of the grounding model (bounding box) is used as a guidance for pixel-wise segmentation of the object. We also show that video object segmentation using the mask annotation on the first frame can be further improved by using language supervision, highlighting the complementarity of both modalities.</p><p>To evaluate the proposed approach we extend the popular benchmarks for segmenting single and multiple objects in videos, DAVIS <ref type="bibr" target="#b15">16</ref>  <ref type="bibr" target="#b37">[38]</ref> and DAVIS <ref type="bibr" target="#b16">17</ref>  <ref type="bibr" target="#b41">[42]</ref>, with language descriptions of the target objects. We collect the annotations using two different settings, asking the annotators to provide a description of the target object based on the first frame only as well as on the full video. Future work may choose which setting they prefer to use. On average each video has been annotated with 7.5 referring expressions and it takes the annotator around 5s to provide a referring expression for a target object.</p><p>Our language-supervised approach performs on par with semi-supervised methods which have access to the pixel-accurate object mask on DAVIS <ref type="bibr" target="#b15">16</ref> and shows comparable results to the techniques that employ scribbles on the challenging DAVIS 17 dataset.</p><p>In summary, our contributions are the following. We present a new task of segmenting objects in video using natural language referring expressions for which we augment two well-known video segmentation benchmarks with textual descriptions of target objects. We conduct an extensive analysis of the performance of the state-ofthe-art language grounding models on video data and propose a way to improve their temporal coherency. To the best of our knowledge we are the first to perform an analysis of transferability of image-based grounding models to video. We show that high quality video object segmentation results can be obtained by employing language referring expressions, allowing a more natural and practical human-computer interaction. Moreover, we show that language descriptions are complementary to visual forms of supervision, such as masks, and can be exploited as an additional source of guidance for object segmentation. Thus, while proposing the new task and accompanying dataset, our work contributes the necessary benchmark analysis, a very competitive baseline and valuable insights for future work. We hope our findings would further promote the research in the field of video object segmentation via language expressions and help to discover better techniques that can be used in realistic scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Grounding natural language expressions</head><p>There has been an increasing interest in the task of grounding natural language expressions over the last few years <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b22">23]</ref>. We group the existing works by the type of visual domain: images and video. Image domain. Grounding natural language expressions is a task of localizing a given expression in an image with a bounding box <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b33">34]</ref> or a segmentation mask <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b22">23]</ref>. Referring expression comprehension is a closely related task, where the goal is to localize the non-ambiguous referring expression. Most existing approaches rely on external bounding box proposals which are scored to determine the top scoring box as the correct region <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b55">56]</ref>. A few recent works explore methods of inferring object regions by proposal generation network <ref type="bibr" target="#b3">[4]</ref> or efficient subwindow search <ref type="bibr" target="#b54">[55]</ref>. Multiple existing approaches model relationships between objects present in the scene <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b13">14]</ref>. In this work we choose two state-of-the-art grounding models for experimentation and analysis <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b55">56]</ref>. DBNet <ref type="bibr" target="#b57">[58]</ref> frames grounding as a classification task, where an expression and an image region serve as input and a binary classification decision is an output. A key component of this approach is utilization of negative expressions and image regions to ensure discriminative training. DBNet currently leads on Visual Genome <ref type="bibr" target="#b21">[22]</ref>. MattNet <ref type="bibr" target="#b55">[56]</ref> is a modular network which "softly" decomposes referring expressions in three parts: subject, location, and relationship, each of which is processed by a different visual module. This allows MattNet to process referring expressions of general forms, as each module can be "enabled" or "disabled" depending on the expression. MattNet achieves top performance on RefCOCO(g/+) <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b33">34]</ref> both in terms of bounding box localization and pixel-wise segmentation accuracy. Video domain. The progress made in image-level natural language grounding leads to an increasing interest in application to video. The recent work of <ref type="bibr" target="#b23">[24]</ref> studies object tracking in video using language expressions. They introduce a dynamic convolutional layer, where a language query is used to predict visual convolutional filters. <ref type="bibr" target="#b0">[1]</ref> addresses object tracking in video with the language descriptions and human gaze as input. Our work falls in the same line of research, as we are exploring natural language as input for video object segmentation. To the best of our knowledge, this is the first work to apply natural language to this task. A concurrent work by <ref type="bibr" target="#b9">[10]</ref> has addressed a task of actor/action segmentation in video based on sentence input. Their work focuses on seven classes of actors (adult, baby, etc.) and mostly action-oriented descriptions. In contrast, we consider arbitrary objects and unconstrained referring expressions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Video Object Segmentation</head><p>Video object segmentation has witnessed considerable progress <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b49">50]</ref>. In the following, we group the related work into unsupervised and semi-supervised.</p><p>Unsupervised methods. Unsupervised methods assume no human input on the video during test time. They aim to group pixels consistent in both appearance and motion and extract the most salient spatio-temporal object tube. Several techniques exploit object proposals <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b20">21]</ref>, saliency <ref type="bibr" target="#b8">[9]</ref> and optical flow <ref type="bibr" target="#b36">[37]</ref>. Convnet-based approaches <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b46">47]</ref> cast video object segmentation as a foreground/background classification problem and feed to the network both appearance and motion cues. Because these methods do not have any knowledge of the target object, they have difficulties in videos with multiple moving and dominant objects and cluttered backgrounds.</p><p>Semi-supervised methods. Semi-supervised methods assume human input for the first frame, either by providing a pixel-accurate mask <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b2">3]</ref>, clicks <ref type="bibr" target="#b31">[32]</ref> or scribbles <ref type="bibr" target="#b40">[41]</ref>, and then propagate the information to the successive frames. Existing approaches focus on leveraging superpixels <ref type="bibr" target="#b52">[53]</ref>, constructing graphical models <ref type="bibr" target="#b47">[48]</ref>, utilizing object proposals <ref type="bibr" target="#b39">[40]</ref> or employing optical flow and long-term trajectories <ref type="bibr" target="#b51">[52]</ref>. Lately, convnets have been considered for the task <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b49">50]</ref>. These methods usually build the architecture upon the semantic segmentation networks <ref type="bibr" target="#b28">[29]</ref> and process each frame of the video individually. <ref type="bibr" target="#b2">[3]</ref> proposes to fine-tune a pre-trained generic object segmentation network on the first frame mask of the test video to make it sensitive to the target object. <ref type="bibr" target="#b38">[39]</ref> employs a similar strategy, but also provides a temporal context by feeding the previous frame mask to the network. Several methods extend the work of <ref type="bibr" target="#b2">[3]</ref> by incorporating the semantic information <ref type="bibr" target="#b32">[33]</ref> or by integrating online adaptation <ref type="bibr" target="#b49">[50]</ref>. <ref type="bibr" target="#b14">[15]</ref> proposes to employ a recurrent network to exploit the long-term temporal information.</p><p>The above methods employ a pixel-level mask on the first frame. However, for many applications, particularly on small touchscreen devices, it can be prohibitive to provide a pixel-accurate segmentation. Hence, there has been a growing interest to integrate cheaper forms of supervision, such as point clicks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b31">32]</ref> or scribbles <ref type="bibr" target="#b40">[41]</ref>, into convnetbased techniques. In spirit with these approaches, we aim to reduce the annotation effort on the first frame by using language referring expressions to specify the object. Our approach also builds upon convnets and exploits both linguistic and visual modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section we provide an overview of the proposed approach. Given a video V = {f 1 , ..., f N } with N frames and a textual query of the target object Q, our aim is to obtain a pixel-level segmentation mask of the target object in every frame that it appears.</p><p>We leverage recent advances in grounding referring expressions in images <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b55">56]</ref> and pixel-level segmentation in videos <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b16">17]</ref>. Our method consists of two main steps (see <ref type="figure">Figure 2</ref>). Using as input the textual query Q provided by the user, we first generate target object bounding box proposals for every frame of the video by exploiting referring expression grounding models, designed for images only. Applying these models off-the-shelf results in temporally inconsistent and jittery box predictions (see <ref type="figure">Figure  Query</ref>: "A girl in white dancing"</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Referring Expression Grounding</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Temporal Consistency</head><p>Pixel-wise Segmentation <ref type="figure">Figure 2</ref>: System overview. We first localize the target object via grounding model using the given referring expression and enforce temporal consistency of bounding boxes across frames. Next we apply a segmentation convnet to recover detailed object masks.</p><p>3). Therefore, to mitigate this issue and make them more applicable for video data, we next employ temporal consistency, which enforces bounding boxes to be coherent across frames. As a second step, using as guidance the obtained box predictions of the target object on every frame of the video we apply a convnet-based pixel-wise segmentation model to recover detailed object masks in each frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Grounding objects in video by referring expressions</head><p>As discussed in §2, the task of natural language grounding is to automatically localize a region described by a given language expression. It is typically formulated as measuring the compatibility between a set of object proposals O = {o i } M i=1 and a given textual query Q. The grounding model provides as output a set of matching scores S = {s i } M i=1 between a box proposal and a textual query Q. The box proposal with the highest matching score is selected as the predicted region.</p><p>We employ two state-of-the-art referring expression grounding models -DBNet <ref type="bibr" target="#b57">[58]</ref> and MattNet <ref type="bibr" target="#b55">[56]</ref>, to localize the object in each frame. Mask R-CNN <ref type="bibr" target="#b11">[12]</ref> bounding box proposals are exploited as an initial set of proposals for both models, although originally DBNet has been designed to utilize EdgeBox proposals <ref type="bibr" target="#b7">[8]</ref>. However, using the grounding models designed for images and picking the highest scoring proposal for each video frame lead to temporally incoherent results. Even with simple textual queries for adjacent frames that from a human perspective look very much alike, the referring model often outputs inconsistent predictions (see <ref type="figure">Figure 3</ref>). This indicates the inherent instability of the grounding models trained on the image domain. To resolve this problem we propose to re-rank the object proposals by exploiting temporal structure along with the original matching scores given by a grounding model.</p><p>Temporal consistency. The goal of the temporal smoothing step is to improve temporal consistency and to reduce id-switches for target object predictions across frames. Since objects tend to move smoothly through space and in time, there should be little changes from frame to frame and the box proposals should have high overlap between neighboring frames. By finding temporally coherent tracks of an object that are spreadout in time, we can focus on the predictions that consistently appear throughout the video and give less emphasis to objects that appear for only a short period of time.</p><p>The grounding model provides the likeliness of each box proposal to be the target object by outputting a matching score s i . Then each box proposal is re-ranked based on its overlap with the proposals in other frames, the original objectness score given by <ref type="bibr" target="#b11">[12]</ref> and its matching score from the grounding model. Specifically, for each proposal we compute a new score:ŝ i = s i * ( M j=1,j =i r ij * d j * s j /t ij ), where r ij measures an intersection-over-union ratio between box proposals i and j, t ij denotes the temporal distance between two proposals (t ij = |f i − f j |) and d j is the original objectness score. Then, in each frame we select the proposals with the highest new score. The new scoring rewards temporally coherent predictions which likely belong to the target object and form a spatio-temporal tube. This step allows to improve temporal coherence boosting grounding and video segmentation performance (see <ref type="table">Table 1</ref> in §5 and <ref type="table">Table  5</ref> in §6) while being computational efficient (takes only a fraction of second).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Pixel-level video object segmentation</head><p>We next show how to output pixel-level object masks, exploiting the bounding boxes from grounding as a guidance for the segmentation network. The boxes are used as the input to the network to guide the network towards the target object, providing its rough location and extent. The task of the network is to obtain a pixel-level foreground/background segmentation mask using appearance and motion cues. Approach. We model pixel-level segmentation as a box refinement task. The bounding box is transformed into a binary image (255 for the interior of the box, 0 for the background) and concatenated with the RGB channels of the input image and optical flow magnitude, forming a 5-channel input for the network. Thus we ask the network to learn to refine the provided boxes into accurate masks. Fusing appearance and motion cues allows to better exploit video data and handle better both static and moving objects.</p><p>We make one single pass over the video, applying the model per-frame. The network does not keep a notion of the specific appearance of the object in contrast to <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b2">3]</ref>, where the model is fine-tuned during the test time to learn the appearance of the target object. Neither do we do an online adaptation as in <ref type="bibr" target="#b49">[50]</ref>, where the model is updated on its previous predictions while processing video frames. This makes the system more efficient during the inference time, which is more suitable for real-world applications.</p><p>Similar to <ref type="bibr" target="#b38">[39]</ref>, we train the network on static images, employing the saliency segmentation dataset <ref type="bibr" target="#b6">[7]</ref> which contains a diverse set of objects. The bounding box is obtained from the ground truth masks. To make the system robust during test time to sloppy boxes from the grounding model, we augment the ground truth box by randomly jittering its coordinates (uniformly, ±20% of the original box width and height). We synthesize optical flow from static images by applying affine transformations for both background and foreground object to simulate the camera and object motion in the neighboring frames, as in <ref type="bibr" target="#b19">[20]</ref>. This simple strategy allows us to train on diverse set of static images, while exploiting motion information during test time. We train the network on many triplets of RGB images, synthesized flow magnitude images and loose boxes in order for the model generalize well to different localization quality of boxes given by the grounding model and different dynamics of the object.</p><p>During inference we use the state-of-the-art optical flow estimation method Flow-Net2.0 <ref type="bibr" target="#b15">[16]</ref>. We compute the optical flow magnitude by subtracting the median motion for each frame and averaging the magnitude of the forward and backward flow. The obtained image is further scaled to [0; 255] to maintain the same range as RGB channels. Network. As our network architecture we use ResNet-101 <ref type="bibr" target="#b12">[13]</ref>. We adapt the network to the segmentation task following the procedure of <ref type="bibr" target="#b28">[29]</ref> and employing atrous convolutions <ref type="bibr" target="#b4">[5]</ref> with hybrid rates <ref type="bibr" target="#b50">[51]</ref> within the last two blocks of ResNet to enlarge the Query: "A woman with a stroller." Query: "A girl riding a horse."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>W/o temporal consistency</head><p>With temporal consistency <ref type="figure">Figure 3</ref>: Qualitative results of language grounding with and w/o temporal consistency on DAVIS <ref type="bibr" target="#b16">17</ref> . The results are obtained using MattNet <ref type="bibr" target="#b55">[56]</ref> trained on RefCOCO <ref type="bibr" target="#b56">[57]</ref>.</p><p>receptive field as well as to alleviate the "gridding" issue. After the last block, we apply spatial pyramid pooling <ref type="bibr" target="#b4">[5]</ref>, which aggregates features at multiple scales by applying atrous convolutions with different rates, and augment it with the image-level features <ref type="bibr" target="#b27">[28]</ref> to exploit better global context. The network is trained using a standard cross-entropy loss (all pixels are equally weighted). The final logits are upsampled to the ground truth resolution to preserve finer details for back-propagation. For network initialization we use a model pre-trained on ImageNet <ref type="bibr" target="#b12">[13]</ref>. The new layers are initialized using the "Xavier" strategy <ref type="bibr" target="#b10">[11]</ref>. The network is trained on MSRA <ref type="bibr" target="#b6">[7]</ref> for segmentation. To avoid the domain shift we fine-tune the model on the training sets of DAVIS <ref type="bibr" target="#b15">16</ref>  <ref type="bibr" target="#b37">[38]</ref> and DAVIS <ref type="bibr" target="#b16">17</ref>  <ref type="bibr" target="#b41">[42]</ref> respectively. We employ SGD with a polynomial learning policy with initial learning rate of 0.001, crop size of 513 × 513, random scale data augmentation (from 0.5 to 2.0) and left-right flipping during training. The network is trained for 20k iterations on MSRA and 20k iterations on the training set of DAVIS <ref type="bibr" target="#b15">16</ref> /DAVIS 17 . During inference we employ test time augmentation as in <ref type="bibr" target="#b4">[5]</ref>. Other sources of supervision. Additionally we consider variants of the proposed model using different sources of supervision. Our approach is flexible and can take advantage of the first frame mask annotation as well as language. We describe how language can be used on top of the mask supervision, improving the robustness of the system against occlusions and dynamic backgrounds (see §6 for results). Mask. Here we discuss a variant that uses only the first frame mask supervision during test time. The network is initialized with the bounding box obtained from the object mask in the 1st frame and for successive frames uses the prediction from the preceding frame warped with the optical flow (as in <ref type="bibr" target="#b38">[39]</ref>) to get the input box for the next frame. Following <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b2">3]</ref> we fine-tune the model for 1k iterations on an augmented set obtained from the first frame image and mask, to learn the specific properties of the object. Mask + Language. We show that using language supervision is complementary to the first frame mask. Instead of relying on the preceding frame prediction as in the previous paragraph, we use the bounding boxes obtained from the grounding model after the temporal consistency step. We initialize with the ground truth box in the first frame and fine-tune the network on the 1st frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Collecting referring expressions for video</head><p>Our task is to localize and provide a pixel-level mask of an object on all video frames given a language referring expression obtained either by looking at the first frame only ID 1: "A man in a grey t-shirt and yellow trousers" ID 1: "A man in a grey shirt walking through the crossing" ID 2: "A woman in a black shirt" ID 2: "A woman walking through the crossing" or the full video. To validate our approach we employ two popular video object segmentation datasets, DAVIS <ref type="bibr" target="#b15">16</ref>  <ref type="bibr" target="#b37">[38]</ref> and DAVIS 17 <ref type="bibr" target="#b41">[42]</ref>. These two datasets introduce various challenges, containing videos with single or multiple salient objects, crowded scenes, similar looking instances, occlusions, camera view changes, fast motion, etc. DAVIS <ref type="bibr" target="#b15">16</ref>  <ref type="bibr" target="#b37">[38]</ref> consists of 30 training and 20 test videos of diverse object categories with all frames annotated with pixel-level accuracy. Note that in this dataset only a single object is annotated per video. For the multiple object video segmentation task we consider DAVIS <ref type="bibr" target="#b16">17</ref> . Compared to DAVIS <ref type="bibr" target="#b15">16</ref> , this is a more challenging dataset, with multiple objects annotated per video and more complex scenes with more distractors, occlusions, smaller objects, and fine structures. Overall, DAVIS <ref type="bibr" target="#b16">17</ref> consists of a training set with 60 videos, and a validation/test-dev/test-challenge set with 30 sequences each.</p><p>As our goal is to segment objects in videos using language specifications, we augment all objects annotated with mask labels in DAVIS <ref type="bibr" target="#b15">16</ref> and DAVIS 17 with non-ambiguous referring expressions. We follow the work of <ref type="bibr" target="#b33">[34]</ref> and ask the annotator to provide a language description of the object, which has a mask annotation, by looking only at the first frame of the video. Then another annotator is given the first frame and the corresponding description, and asked to identify the referred object. If the annotator is unable to correctly identify the object, the description is corrected to remove ambiguity and to specify the object uniquely. We have collected two referring expressions per target object annotated by non-computer vision experts (Annotator 1, 2).</p><p>However, by looking only at the 1st frame, the obtained referring expressions may potentially be invalid for an entire video. (We actually quantified that only∼ 15% of the collected descriptions become invalid over time and it does not affect strongly segmentation results as temporal consistency step helps to disambiguate some of such cases, see the supp. material for details.) Besides, in many applications, such as video editing or video-based advertisement, the user has access to a full video. Providing a language query which is valid for all frames might decrease the editing time and result in more coherent predictions. Thus, on DAVIS 17 we asked the workers to provide a description of the object by looking at the full video. We have collected one expression of the full video type per target object. Future work may choose to use either setting.</p><p>The average length for the first frame/full video expressions is 5.5/6.3 words. For DAVIS <ref type="bibr" target="#b16">17</ref> first frame annotations we notice that descriptions given by Annotator 1 are longer than the ones by Annotator 2 (6.4 vs. 4.6 words). We evaluate the effect of description length on the grounding performance in §5. Besides, the expressions relevant to a full video mention verbs more often than the first frame descriptions (44% vs. 25%). This is intuitive, as referring to an object which changes its appearance and ID 1: "A girl with blonde hair dressed in blue". ID 1: "A brown camel in the front". ID 1: "A black scooter ridden by a man". ID 2: "A man in a suit riding a scooter". position over time may require mentioning its actions. Adjectives are present in over 50% for all annotations. Most of them refer to colors (over 70%), shapes and sizes (7%) and spatial/ordering words (6% first frame vs. 13% full video expressions). The full video expressions also have a higher number of adverbs and prepositions, and overall are more complex than the ones provided for the first frame, see <ref type="figure">Figure 4</ref> for examples.</p><p>Overall augmented DAVIS 16/17 contains ∼ 1.2k referring expressions for more than 400 objects on 150 videos with ∼ 10k frames. We believe the collected data will be of interest to segmentation as well as vision and language communities, providing an opportunity to explore language as alternative input for video object segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation of natural language grounding in video</head><p>In this section we discuss the performance of natural language grounding models on video data. We experiment with DBNet <ref type="bibr" target="#b57">[58]</ref> and MattNet <ref type="bibr" target="#b56">[57]</ref>. DBNet is trained on Visual Genome <ref type="bibr" target="#b21">[22]</ref> which contains images from MS COCO <ref type="bibr" target="#b25">[26]</ref> and YFCC100M <ref type="bibr" target="#b44">[45]</ref>, and spans thousands of object categories. MattNet is trained on referring expressions for MS COCO images <ref type="bibr" target="#b25">[26]</ref>, specifically RefCOCO and RefCOCO+ <ref type="bibr" target="#b56">[57]</ref>. Unlike RefCOCO which has no restrictions on the expressions, RefCOCO+ contains no spatial words and rather focuses on object appearance. Both aforementioned models rely on external bounding box proposals, such as EdgeBox <ref type="bibr" target="#b7">[8]</ref> or Mask R-CNN <ref type="bibr" target="#b11">[12]</ref>.</p><p>We carry out most of our evaluation on DAVIS <ref type="bibr" target="#b15">16</ref> and DAVIS <ref type="bibr" target="#b16">17</ref> with the referring expressions introduced in §4. To evaluate the localization quality we employ the intersection-over-union overlap (IoU) of the top scored box proposal with the ground truth bounding box, averaged across all queries. <ref type="table">Table 1</ref> reports performance of the grounding models on DAVIS <ref type="bibr" target="#b15">16</ref> and DAVIS 17 referring expressions. In the following we summarize our key observations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">DAVIS 16 /DAVIS 17 referring expression grounding</head><p>(1) We see the effect of replacing EdgeBox with Mask R-CNN object proposals for DBNet model (54.1 to 64.9). Employing better proposals significantly improves the quality of this grounding method, thus we rely on Mask R-CNN proposals in all the following experiments. (2) We note the stability of grounding performance across two annotations (see ∆(A1,A2)), showing that the grounding methods are quite robust to Attribute-based analysis. Next we perform a more detailed analysis of the grounding models on DAVIS <ref type="bibr" target="#b16">17</ref> . We split the textual queries/videos into subsets where a certain attribute is present and report the averaged results for the subsets. <ref type="table" target="#tab_2">Table 2</ref> presents attribute-based grounding performance on first-frame based expressions averaged across annotators. To estimate the upper bound performance and the impact of imperfect bounding box proposals we add an Oracle comparison, where performance is reported on the ground-truth object boxes. We summarize our findings in the following.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Video object segmentation results</head><p>In this section we present single and multiple video object segmentation results using natural language referring expressions on two datasets: DAVIS <ref type="bibr" target="#b15">16</ref>  <ref type="bibr" target="#b37">[38]</ref> and DAVIS 17 <ref type="bibr" target="#b41">[42]</ref>. In addition, we experiment with fusing two complementary sources of information, employing both the pixel-level mask and language supervision on the first frame. All results here are obtained using the bounding boxes given by the MattNet model <ref type="bibr" target="#b55">[56]</ref> trained on RefCOCO <ref type="bibr" target="#b56">[57]</ref> after the temporal consistency step (see §3.1). For evaluation we use the IoU measure (also called Jaccard index -J) between the ground truth and the predicted segmentation, averaged across all video sequences and all frames. For DAVIS <ref type="bibr" target="#b16">17</ref> we also employ the J&amp;F measure proposed in <ref type="bibr" target="#b41">[42]</ref>. <ref type="table" target="#tab_5">Table 3</ref> compares our results to previous work on DAVIS <ref type="bibr" target="#b15">16</ref>  <ref type="bibr" target="#b37">[38]</ref>. As we employ MattNet <ref type="bibr" target="#b55">[56]</ref>, which exploits Mask R-CNN <ref type="bibr" target="#b11">[12]</ref> box proposals, we also would like to compare to its segments. We report the oracle Mask R-CNN results, where on each frame the segment with the highest ground truth overlap was chosen. Even with the oracle assignment of segments, <ref type="bibr" target="#b11">[12]</ref> under-performs compared to our segmentation model (71.5 vs. 83.1). This shows that for very detailed mask annotations (as in DAVIS 16/17 ) a more complex segmentation module than the Mask R-CNN segmentation head is required (which itself is a shallow FCN with reduced output resolution, resulting in coarse masks).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">DAVIS 16 single object segmentation</head><p>Our method, while only exploiting language, shows competitive performance, on par with techniques which use a pixel-level mask on the first frame (82. <ref type="bibr" target="#b7">8</ref>   <ref type="table">Table 4</ref>: Attribute-based results with different forms of supervision on DAVIS <ref type="bibr" target="#b15">16</ref> , val set. AC: appearance change, LR: low resolution, SV: scale variation, SC: shape complexity, CS: camera shake, DB: dynamic background, BC: background clutter, FM: fast motion, MB: motion blur, DEF: deformation, OCC: occlusions. See §6.1 for more details.</p><p>OnAVOS <ref type="bibr" target="#b49">[50]</ref>). This shows that high quality results can be obtained via a more natural way of human-computer interaction -referring to an object via language, making video segmentation techniques more applicable in practice. Compared to mask supervision employing language results in a runtime speed up: it is ∼ 15 times faster to specify the object with language (79s <ref type="bibr" target="#b25">[26]</ref> vs. 5s) plus online tuning is not needed for good performance ( <ref type="bibr" target="#b32">[33]</ref> reports 10min for online tuning with 80.2 vs. our 82.8). Note that <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b49">50]</ref> show superior results to our approach (∼ 86 mIoU). However, they employ additional cues by incorporating semantic information <ref type="bibr" target="#b32">[33]</ref> or doing online adaptation <ref type="bibr" target="#b49">[50]</ref>. Potentially, these techniques can also be applied to our method, though it is out of scope of this paper.  Compared to the approaches which use point click supervision <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b31">32]</ref>, our method shows superior performance (82.8 vs. 80.6 and 80.9). This indicates that language can be successfully utilized as an alternative and cheaper form of supervision for video object segmentation, on par with clicks and scribbles. Maks and language. In <ref type="table" target="#tab_5">Table 3</ref> we also report the results for variants using only mask supervision on the the first frame or combining both mask and language (see §3.2 for details). Notice that employing either mask or language results in comparable performance (82.8 vs. 83.1), while fusing both modalities leads to a further improvement (82.8 vs. 84.5). This shows that referring expressions are complementary to visual forms of supervision and can be exploited as an additional source of guidance for segmentation, on top of not only pixel-level masks, but potentially scribbles and point clicks. <ref type="table">Table 4</ref> presents a more detailed evaluation using video attributes. We report the averaged results on a subset of sequences where a certain challenging attribute is present. Note that using language alone leads to more robust performance for videos with low resolution, camera shake and background clutter without the need for an expensive pixel-level mask. When utilizing both mask and language we observe that the system becomes consistently more robust to various video challenges (e.g. fast motion, occlusions, motion blur, etc.) and compares favorably to mask only on all attributes, except appearance change. Overall, employing language can help the model to better handle occlusions, avoid drift and better adapt to complex dynamics inherent to video.  <ref type="table">Table 5</ref>: Ablation study on DAVIS <ref type="bibr" target="#b15">16</ref> .</p><p>Ablation study. We validate the contributions of the components in our method (see §3) by presenting an ablation study in <ref type="table">Table  5</ref> on DAVIS <ref type="bibr" target="#b15">16</ref> , training set. Augmenting the ground truth boxes by random jittering makes the system more robust to sloppy boxes at test time (82.5 vs. 80.6), while employing motion cues allows to better handle moving objects (80.6 vs. 75.9). Temporal consistency step helps to provide more temporally coherent boxes (4.3 mIoU point boost for grounding, see <ref type="table">Table 1</ref>) and hence improve the final segmentation quality (75.9 vs. 72.5). Exploiting the proposed network architecture versus using the network proposed in <ref type="bibr" target="#b38">[39]</ref> results in 3.7 point boost (75.9 vs. 72.2), providing more detailed object masks. Overall, all components introduced in our approach lead to the state-of-the-art results on DAVIS 16 . <ref type="table" target="#tab_8">Table 6</ref> presents results on DAVIS 17 <ref type="bibr" target="#b41">[42]</ref>. The lower numbers in comparison with <ref type="table" target="#tab_5">Table  3</ref> indicate that DAVIS 17 is significantly more difficult than DAVIS <ref type="bibr" target="#b15">16</ref> . Even when employing mask supervision on the first frame the dataset presents a challenging task and there is much room for improvement. The semi-supervised methods perform well on foreground-background segmentation, but have problems separating multiple foreground objects, handling small objects and preserving the correct object identities <ref type="bibr" target="#b41">[42]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">DAVIS 17 multiple object segmentation</head><p>Compared to mask supervision using language descriptions significantly underperforms. We believe that one of the main problems is a relatively unstable behavior of the underlying grounding model. There are a lot of identity switches, that are heavily penalized by the evaluation metric as every pixel should be assigned to one instance. We conducted an oracle experiment assigning Mask R-CNN box proposals to the correct object ids and then performing segmentation (denoted "Oracle -Grounding"). We observe a significant increase in performance (37.3 to 54.9), making the results competitive to mask supervision. If we utilize Mask R-CNN segment proposals for oracle case, the result is 2.1 points lower than using our segmentation model on top. The underlying choice of proposals for the grounding model could also have its effect. If the object is not detected by Mask R-CNN, the grounding model has no chances to recover the correct instance. To evaluate the influence of proposals we conduct an oracle experiment where the ground truth boxes are exploited in the grounding model (denoted "Oracle -Box proposals"). With oracle boxes we observe an increase in performance (37.3 to 42.1), however, recovering the correct identities still poses a problem for grounding.</p><p>Another factor influencing the results is the domain shift between the training and test data. Both Mask R-CNN and MattNet are trained on MS COCO <ref type="bibr" target="#b25">[26]</ref>, and have troubles recovering instances not belonging to 80 COCO categories. We split the DAVIS 17 validation set into COCO and non-COCO objects/language queries <ref type="bibr">(43 vs. 18</ref>) and eval-uate separately on two subsets. As in §5, we observe much higher results for COCO queries <ref type="bibr">(45 to 27.5)</ref>, indicating the problem of generalization from training to test data.</p><p>The method which exploits scribble supervision <ref type="bibr" target="#b40">[41]</ref> performs on par with our approach. Note that even for scribble supervision the task remains difficult.  Mask and language. In <ref type="table" target="#tab_8">Table 6</ref> we also report the results for variants of our approach using only mask supervision or combining mask and language. Employing language on top of mask leads to an increase in performance over using mask only (58 to 59), again showing complementarity of both sources of supervision. <ref type="figure">Figure A1</ref> provides qualitative results of our method using only language as supervision. We observe successful handling of similar looking objects, fast motion, deformations and partial occlusions. Discussion. Our results indicate that language alone can be successfully used as an alternative and a more natural form of supervision. Particularly, high quality results can be achieved for videos with the salient target object. Videos with multiple similar looking objects pose a challenge for grounding models, as they have problems preserving object identities across frames. Experimentally we show that better proposals, grounding and proximity of training and test data can further boost the performance for videos with multiple objects. Language is complementary to mask supervision and can be exploited as an additional source of guidance for segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this work we propose the task of video object segmentation using language referring expressions. We propose an approach to address this new task as well as extend two well-known video object segmentation benchmarks with textual descriptions of target objects. Our experiments indicate that language alone can be successfully exploited to obtain high quality segmentations of objects in videos. While allowing a more natural human-computer interaction, using guidance from language descriptions can also make video segmentation more robust to occlusions, complex dynamics and cluttered backgrounds. We show that classical semi-supervised video object segmentation which uses the mask annotation on the first frame can be further improved by the use of language descriptions. We believe there is a lot of potential in fusing lingual (referring expressions) and visual (clicks, scribbles or masks) forms of supervision for object segmentation in video. We hope that our results encourage more research on video object segmentation with referring expressions and foster discovery of new techniques applicable in realistic settings, which discard tedious pixel-level annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>This supplementary material provides additional quantitative and qualitative results and is structured as follows.</p><p>Section A discusses two types of referring expressions -1st frame vs. full videoand the effect of 1st frame annotations being invalid for the whole video. It also provides additional examples of the collected referring expressions for video object segmentation task (see <ref type="figure">Figure A2</ref>).</p><p>Section B provides additional evaluation of natural language grounding models on the Lingual ImageNet Videos <ref type="bibr" target="#b23">[24]</ref> and compares results with the work of <ref type="bibr" target="#b23">[24]</ref>  <ref type="table" target="#tab_10">(Table  B1)</ref>.</p><p>Section C provides additional evaluation metrics for DAVIS <ref type="bibr" target="#b15">16</ref>  <ref type="table" target="#tab_2">(Table C2</ref>) and comparisons of different grounding models, effect of temporal consistency and annotation types on video object segmentation task <ref type="table" target="#tab_5">(Table C3</ref>). We also include more qualitative examples for Language, Mask and Mask + Language approaches (see <ref type="figure" target="#fig_1">Figures C3-C5)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Referring expressions for video object segmentation</head><p>As our goal is to segment objects in videos using language specifications, we augment all objects annotated with mask labels in DAVIS <ref type="bibr" target="#b15">16</ref>  <ref type="bibr" target="#b37">[38]</ref> and DAVIS <ref type="bibr" target="#b16">17</ref>  <ref type="bibr" target="#b41">[42]</ref> with nonamb-iguous referring expressions.</p><p>Original query: "A brown camel" vs.</p><p>Corrected: "A brown camel in the front" <ref type="figure">Figure A1</ref>: Predictions for the ambiguous query and its correction.</p><p>We collected referring expression annotations using two different settings, asking the annotators to provide a description of the target object based on the first frame only as well as on the full video. Future work may choose which setting they prefer more.</p><p>We experiment with both annotation types. While the first type is more similar to image-based referring expressions, the second type has different trends, tending to be more complex/long due to increased complexity of the video. We report the grounding ( <ref type="table">Table 1</ref> in the main paper) and VOS results <ref type="table" target="#tab_5">(Table C3</ref>) with both types, showing that DBNet <ref type="bibr" target="#b57">[58]</ref> benefits from the "full video" descriptions, while MattNet <ref type="bibr" target="#b56">[57]</ref> has difficulties coping with more complex language.</p><p>Concerned that the referring expressions obtained by only looking at the 1st frame might be potentially invalid for the entire video, on DAVIS 17 we ask a user to mark which 1st frame expressions become ambiguous/invalid over time, and to correct them to be valid for the full video (e.g. <ref type="figure">Fig A1)</ref>. Only ∼15% of all descriptions were marked invalid. Though some descriptions become ambiguous/invalid over time, it does not impact strongly the results (original 36.9 vs. corrected 37.1 mIoU). One of the reasons is that temporal consistency helps to disambiguate some of such cases ( <ref type="figure">Fig A1)</ref>. Another reason is that invalid descriptions might still contain valid info (e.g. "a boy in red on the left", the boy is no longer on the left, but still in red).</p><p>We present additional examples of collected referring expressions in <ref type="figure">Figure A2</ref>.   For the natural language grounding task we additionally consider Lingual Im-ageNet Videos <ref type="bibr" target="#b23">[24]</ref>, which provides referring expression annotations for a subset of the ImageNet Video Object Detection dataset <ref type="bibr" target="#b42">[43]</ref>. The dataset is split into a training and a validation set, each consisting of 50 videos. The performance on Lingual ImageNet <ref type="bibr" target="#b23">[24]</ref> is measured in terms of the AUC (area under the curve) score metric, following <ref type="bibr" target="#b23">[24]</ref>.</p><p>Here we compare to <ref type="bibr" target="#b23">[24]</ref>, who perform tracking of objects using language specifications. <ref type="table" target="#tab_10">Table B1</ref> presents grounding results reported by <ref type="bibr" target="#b23">[24]</ref>, including tracking by language only, tracking given the ground-truth bounding box on the first frame, and the combined approach. Our method is based on language input only, specifically, we report the results after the temporal consistency step applied to DBNet and MattNet  <ref type="table" target="#tab_2">Table C2</ref>: Comparison of video object segmentation results on DAVIS <ref type="bibr" target="#b15">16</ref> , validation set.</p><p>predictions. As we see both models significantly outperform <ref type="bibr" target="#b23">[24]</ref>, even when <ref type="bibr" target="#b23">[24]</ref> has access to the ground-truth bounding box on the first frame.</p><p>C Video object segmentation C.1 Additional metrics for DAVIS <ref type="bibr" target="#b15">16</ref> We report video object segmentation results for the DAVIS 16 benchmark in <ref type="table" target="#tab_2">Table C2</ref>, using evaluation metrics proposed in <ref type="bibr" target="#b37">[38]</ref>. Three measures are used: region similarity in terms of intersection-over-union (J, higher is better), contour accuracy (F , higher is better), and temporal instability of the masks (T , lower is better). See <ref type="bibr" target="#b37">[38]</ref> for more details. Note that using only language supervision results in a smaller decay over time for J and F measures and a better overall temporal stability T compared to employing pixel-level mask supervision on the first frame.  <ref type="table" target="#tab_5">Table C3</ref>: Effect of different grounding models, temporal consistency and annotation types on video object segmentation on DAVIS 17 , validation set.</p><p>C.2 Effect of grounding models, temporal consistency and annotation types on video object segmentation <ref type="table" target="#tab_5">Table C3</ref> reports the effect of different grounding models, temporal consistency step for grounding and employing the first frame versus the full video descriptions on video object segmentation. We compare DBNet versus MattNet (trained on RefCOCO <ref type="bibr" target="#b56">[57]</ref>) as a base grounding model for video object segmentation task. Exploiting MattNet grounding boxes results in a better performance compared to DBNet (37.3 vs. <ref type="bibr">35.4)</ref>. Overall the temporal consistency step has a positive impact on video object segmentation performance across different grounding models (for MattNet 35.4 → 37.3 and for DBNet 32.6 → 35.4).</p><p>We also compare the segmentation performance from first frame versus full video descriptions in <ref type="table" target="#tab_5">Table C3</ref>. Employing the full video versus the first frame descriptions results in a minor improvement for DBNet <ref type="bibr">(35.4 vs. 35.5)</ref>, however has a negative effect for MattNet (37.3 vs. 35.5). The same diverging has been observed for language grounding results in the main paper when comparing results on expressions provided for the first frame versus expressions provided for the full video in <ref type="table" target="#tab_2">Table 2</ref>. We attribute this to the fact that DBNet is trained on the more diverse Visual Genome descriptions and can handle better more complex full video expressions. <ref type="figure">Figure C3</ref> provides more qualitative examples of Language-only supervision for video object segmentation on DAVIS <ref type="bibr" target="#b15">16</ref> and DAVIS 17 , validation sets. We observe successful handling of shape deformations, fast motion as well as partial and full occlusions. <ref type="figure">Figure C4</ref> shows examples of Mask + Language supervision on DAVIS 17 , validation set. We observe high quality instance level segmentation of multiple similar looking objects. <ref type="figure" target="#fig_1">Figure C5</ref> shows comparison of Language versus Mask supervision on DAVIS 16 and DAVIS 17 , validation sets. Note that using only language supervision results in a more robust performance for videos with similar looking instances and camera view changes in comparison to employing pixel-level masks. <ref type="figure">Figure C4</ref>: Video object segmentation qualitative results using Mask + Language as supervision on DAVIS 17 , val set. Frames sampled along the video duration. In the last row we visualize a failure case of the proposed approach. <ref type="figure" target="#fig_1">Figure C5</ref>: Video object segmentation results using Language versus Mask on the 1st frame as supervision on DAVIS <ref type="bibr" target="#b15">16</ref> and DAVIS 17 , val sets. Using language only results in a more robust performance for videos with similar looking instances and camera view changes in comparison to employing pixel-level masks. Frames sampled along the video duration. The videos are chosen with the highest mIoU difference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Qualitative results for video object segmentation</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>ID 3 :Figure 4 :</head><label>34</label><figDesc>"A white truck on the road" ID 3: "A white truck moving from the left to right"First frame annotationFull video annotation Example of annotations provided for the 1st frame vs. the full video. Full video annotations include descriptions of activities and overall are more complex.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Video object segmentation qualitative results using only referring expressions as supervision on DAVIS<ref type="bibr" target="#b15">16</ref> and DAVIS 17 , val sets. Frames sampled along the video.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>ID 5 :</head><label>5</label><figDesc>"A man in a yellow t-shirt" ID 5: "A person rolling over longboard" First frame annotation Full video annotation Figure A2: Example of collected annotations provided for the first frame (left) vs. the full video (right). Full video annotations include descriptions of activities and overall are more complex than the ones provided for the first frame.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>As MattNet is trained on MS COCO images and both models rely on MS COCObased Mask R-CNN proposals, we compare performance for expressions which include COCO versus non-COCO objects. Both models drop in performance on non-COCO expressions, showing the impact of the domain shift to DAVIS 17 (e.g. for MattNet 59.6 vs. 36.9). Even DBNet which is trained on a larger training corpus suffers from the same effect (55.5 vs. 37.3). (2) We label the DAVIS 17 expressions as "spatial" if they include some of the spatial words (e.g. left, right). Such queries are significantly harder for all models (e.g. for MattNet 33.8 vs. 58.5). (3) Verbs are important as they allow to disambiguate an object in a video based on its actions. Presence of verbs in expres-</figDesc><table><row><cell>Method</cell><cell>Train. data</cell><cell>Obj. prop.</cell><cell>mIoU CO.˜CO. Sp.˜Sp. Ve.˜Ve. S Expr. length M L</cell><cell>Num. obj. 1 2-3 &gt;3</cell></row><row><cell cols="3">DBNet Vis.Gen. Mask</cell><cell cols="2">55.5 37.3 36.5 55.7 37.4 52.0 61.8 49.2 33.6 79.5 49.3 22.6</cell></row><row><cell cols="2">MattNet RefCOCO</cell><cell>R-CNN</cell><cell cols="2">59.6 36.9 33.8 58.5 55.8 51.7 63.9 50.2 49.1 86.1 51.2 16.1</cell></row><row><cell cols="3">DBNet Vis.Gen. Oracle MattNet RefCOCO</cell><cell cols="2">79.3 59.0 47.7 81.7 70.3 77.6 84.8 69.9 67.9 100 73.8 37.2 73.2 46.6 42.2 72.5 74.7 62.9 79.0 61.1 59.0 100 64.5 23.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Grounding performance breakdown for different attributes on DAVIS 17 , val set.</figDesc><table><row><cell>Results obtained after the temporal consistency, using average between two annotators</cell></row><row><cell>(1st frame based). Attributes: COCO/non-COCO, Spatial/non-Spatial, Verbs/no Verbs,</cell></row><row><cell>Expression length (Short, Medium, Long) and Number of objects.</cell></row></table><note>sions is a challenging factor for DBNet trained on Visual Genome, while MattNet does significantly better (37.4 vs. 55.8). (4) Expression length is also an important factor. We quantize our expressions into Short (&lt;4 words), Medium (4-6 words) and Long (&gt;6 words). All models demonstrate similar drop in performance as expression length in- creases (e.g. for MattNet 63.9 → 50.2 → 49.1). (5) Videos with more objects are more difficult, as these objects also tend to be very similar, such as e.g. fish in a tank (e.g. for MattNet 86.1 → 51.2 → 16.1). (6) From the Oracle performance on COCO versus non-COCO expressions, we see that all models are able to significantly improve their performance even for non-COCO objects (e.g. for DBNet 37.3 to 59.0). DBNet benefits more than MattNet from Oracle boxes, showing its higher potential to generalize to a new domain given better proposals.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>vs. 81.7 for Supervision AC LR SV SC CS DB BC FM MB DEF OCC Language 80.1 79.0 74.4 77.6 85.7 66.4 85.0 77.7 78.1 84.3 80.1 Mask 81.2 78.1 75.9 79.0 85.6 68.0 82.8 79.0 79.9 85.6 80.5 Mask + Lang. 81.0 79.0 76.8 80.4 86.8 72.2 84.4 79.5 80.4 85.9 82.3</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Comparison of video object segmentation results on DAVIS 16 , val set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Comparison of semi-supervised video object segmentation methods on DAVIS 17 , val set. Numbers in italic are reported on subsets of DAVIS 17 containing/non-containing COCO objects.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>B Language grounding results on Lingual ImageNet Videos</figDesc><table><row><cell>Method</cell><cell cols="2">Supervision AUC score</cell></row><row><cell>Tracking by language [24]</cell><cell>Language Box Box + Language</cell><cell>26.3 47.9 49.4</cell></row><row><cell>DBNet</cell><cell>Language</cell><cell>54.0</cell></row><row><cell>MattNet</cell><cell>Language</cell><cell>60.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table B1 :</head><label>B1</label><figDesc>Comparison of grounding models on Lingual ImageNet Videos, val set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Recall ↑ Decay ↓ Mean ↑ Recall ↑ Decay ↓ Mean ↓</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>DAVIS16</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Supervision</cell><cell>Method</cell><cell cols="2">Region, J</cell><cell></cell><cell cols="2">Boundary, F</cell><cell></cell><cell>Temp. stab., T</cell></row><row><cell></cell><cell cols="3">Mean ↑ Oracle Mask R-CNN [12] 71.5</cell><cell>87.3</cell><cell>5.9</cell><cell>72.4</cell><cell>84.6</cell><cell>6.8</cell><cell>24.8</cell></row><row><cell></cell><cell></cell><cell>NLC [9]</cell><cell>55.1</cell><cell>55.8</cell><cell>12.6</cell><cell>52.3</cell><cell>51.9</cell><cell>11.4</cell><cell>42.5</cell></row><row><cell></cell><cell></cell><cell>FST[36]</cell><cell>55.8</cell><cell>64.9</cell><cell>0.0</cell><cell>51.1</cell><cell>51.6</cell><cell>2.9</cell><cell>36.6</cell></row><row><cell></cell><cell></cell><cell>SegFlow[6]</cell><cell>67.4</cell><cell>81.4</cell><cell>6.2</cell><cell>66.7</cell><cell>77.1</cell><cell>5.1</cell><cell>28.2</cell></row><row><cell></cell><cell>Unsupervised</cell><cell>MP-Net [46]</cell><cell>70.0</cell><cell>85.0</cell><cell>1.3</cell><cell>65.9</cell><cell>79.2</cell><cell>2.5</cell><cell>57.2</cell></row><row><cell></cell><cell></cell><cell>FusionSeg [17]</cell><cell>70.7</cell><cell>83.5</cell><cell>1.5</cell><cell>65.3</cell><cell>73.8</cell><cell>1.8</cell><cell>32.8</cell></row><row><cell></cell><cell></cell><cell>LVO [47]</cell><cell>75.9</cell><cell>89.1</cell><cell>0.0</cell><cell>72.1</cell><cell>8.4</cell><cell>1.3</cell><cell>26.5</cell></row><row><cell></cell><cell></cell><cell>ARP [21]</cell><cell>76.2</cell><cell>91.1</cell><cell>7.0</cell><cell>70.6</cell><cell>83.5</cell><cell>7.9</cell><cell>39.3</cell></row><row><cell></cell><cell></cell><cell>FCP [40]</cell><cell>58.4</cell><cell>71.5</cell><cell>-2.0</cell><cell>49.2</cell><cell>49.5</cell><cell>-1.1</cell><cell>30.6</cell></row><row><cell></cell><cell></cell><cell>BVS [31]</cell><cell>60.0</cell><cell>66.9</cell><cell>28.9</cell><cell>58.8</cell><cell>67.9</cell><cell>21.3</cell><cell>34.7</cell></row><row><cell></cell><cell></cell><cell>ObjFlow [48]</cell><cell>68.0</cell><cell>75.6</cell><cell>26.4</cell><cell>63.4</cell><cell>70.4</cell><cell>27.2</cell><cell>22.2</cell></row><row><cell></cell><cell></cell><cell>PLM [44]</cell><cell>70.2</cell><cell>86.3</cell><cell>11.2</cell><cell>62.5</cell><cell>73.2</cell><cell>14.7</cell><cell>31.8</cell></row><row><cell>Semi-supervised</cell><cell>1st frame mask</cell><cell cols="2">VPN [18] CTN [19] SegFlow [6] MaskTrack [39] 79.7 70.2 73.5 76.1 OSVOS [3] 79.8</cell><cell>82.3 87.4 90.6 93.1 93.6</cell><cell>12.4 15.6 12.1 8.9 14.9</cell><cell>65.5 69.3 76.0 75.4 80.6</cell><cell>69.0 79.6 85.5 87.1 92.6</cell><cell>14.4 12.9 10.4 9.0 15.0</cell><cell>32.4 22.0 18.9 21.8 37.8</cell></row><row><cell></cell><cell></cell><cell>MaskRNN [15]</cell><cell>80.4</cell><cell>96.0</cell><cell>4.4</cell><cell>82.3</cell><cell>93.2</cell><cell>8.8</cell><cell>19.0</cell></row><row><cell></cell><cell></cell><cell>OnAVOS 1 [50]</cell><cell>81.7</cell><cell>92.2</cell><cell>11.9</cell><cell>81.1</cell><cell>88.2</cell><cell>11.2</cell><cell>27.3</cell></row><row><cell></cell><cell></cell><cell>Our</cell><cell>83.1</cell><cell>95.1</cell><cell>9.8</cell><cell>85.7</cell><cell>94.4</cell><cell>9.6</cell><cell>24.0</cell></row><row><cell></cell><cell>Language</cell><cell>Our</cell><cell>82.8</cell><cell>94.1</cell><cell>3.2</cell><cell>85.4</cell><cell>94.7</cell><cell>3.4</cell><cell>22.6</cell></row><row><cell></cell><cell>Mask + Lang.</cell><cell>Our</cell><cell>84.5</cell><cell>96.3</cell><cell>8.2</cell><cell>86.9</cell><cell>95.9</cell><cell>8.7</cell><cell>24.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>Annotation type Grounding Temporal consistency mIoU J&amp;F</figDesc><table><row><cell>1st frame Full video</cell><cell>DBNet MattNet DBNet MattNet</cell><cell>-! -! ! !</cell><cell>32.6 35.4 35.4 37.3 35.5 35.5</cell><cell>34.7 37.6 38.5 39.3 37.7 37.1</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">OSVOS S reports 86.0 mIoU by employing semantic segmentation as additional supervision. 2 OnAVOS gives 86.1 mIoU by exploiting online adaptation on successive frames.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">OnAVOS reports 64.5 mIoU by performing online adaptation on successive frames.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">OnAVOS gives 86.1 mIoU by online adaptation on successive frames.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Object referring in videos with language and human gaze</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Balajee Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Interactive video object segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Benard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gygli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.00269</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">One-shot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Query-guided regression network with context policy for phrase grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kovvuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00915</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Segflow: Joint learning for video object segmentation and optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Global contrast based salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Fast edge detection using structured forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Video segmentation by non-local consensus voting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Faktor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Actor and action video segmentation from a sentence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gavrilyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>AISTATS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mask R-CNN</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Modeling relationships in referential expressions with compositional modular networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Maskrnn: Instance level video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">T</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Fusionseg: Learning to combine motion and appearance for fully automatic segmention of generic objects in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gadde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.05478</idno>
		<title level="m">Video propagation networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Online video object segmentation via convolutional trident network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">D</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.09554</idno>
		<title level="m">Lucid data dreaming for multiple object tracking</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Primary object segmentation in videos based on region augmentation and reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07332</idno>
		<title level="m">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Referring image segmentation via recurrent refinement networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Tracking by natural language specification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Scribblesup: Scribble-supervised convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Recurrent multimodal interaction for referring image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<idno>arxiv:1506.04579</idno>
		<title level="m">Parsenet: Looking wider to see better</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Comprehension-guided referring expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Bilateral space video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Maerki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Deep extreme cut: From extreme points to object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<idno>arxiv: 1709.06031</idno>
		<title level="m">Video object segmentation without temporal information</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Generation and comprehension of unambiguous object descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jonathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Camburu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Modeling context between objects for referring expression understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Nagaraja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Fast object segmentation in unconstrained video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Papazoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Fast object segmentation in unconstrained video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Papazoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Learning video object segmentation from static images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Fully connected object proposals for video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.00557</idno>
		<title level="m">The 2018 davis challenge on video object segmentation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00675</idno>
		<title level="m">The 2017 davis challenge on video object segmentation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<title level="m">ImageNet Large Scale Visual Recognition Challenge. IJCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Pixel-level matching for video object segmentation using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shin Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rameau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>So Kweon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Friedland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Elizalde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Poland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Yfcc100m: the new data in multimedia research</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Learning motion patterns in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Learning video object segmentation with visual memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Video segmentation via object flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Online adaptation of convolutional neural networks for the 2017 davis challenge on video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DAVIS Challenge -CVPR Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Online adaptation of convolutional neural networks for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Understanding convolution for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cottrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08502</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Super-trajectory for video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08634</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Jots: Joint online tracking and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Track and segment: An iterative unsupervised approach for video object proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Interpretable and globally optimal prediction for textual grounding using image concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Hwu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schwing</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Mattnet: Modular attention network for referring expression comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Modeling context in referring expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Poirson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Discriminative bimodal networks for visual localization and detection with natural language queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<idno>ID 4</idno>
	</analytic>
	<monogr>
		<title level="m">A black bicycle&quot; ID 1: &quot;A bicycle moving on the road</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>A longboard. A guy riding a bicycle&quot; ID 1: &quot;A red car</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<idno>ID 1</idno>
		<title level="m">A man jumping across fences</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A dog running in the garden</title>
		<idno>ID 1</idno>
	</analytic>
	<monogr>
		<title level="m">A goat walking on rocks</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
				<idno>ID 1</idno>
		<title level="m">A red and white car</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">A woman riding a horse</title>
		<idno>ID 1</idno>
	</analytic>
	<monogr>
		<title level="m">A horse doing high-jumps</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">A bald man with black belt in the center</title>
		<idno>ID 1</idno>
	</analytic>
	<monogr>
		<title level="m">A man with blue belt on the right</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>A red bmx bike</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">A green motorbike</title>
		<idno>ID 1</idno>
	</analytic>
	<monogr>
		<title level="m">A man riding a motorbike</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Video object segmentation qualitative results using only Language as supervision on DAVIS 16 and DAVIS 17 , val sets. Frames sampled along the video duration. ID 1: &quot;A man wearing a cap</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C3</forename><surname>Figure</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>ID 2: &quot;A black bike</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">A brown and white colored piglet</title>
		<idno>ID 1</idno>
	</analytic>
	<monogr>
		<title level="m">An adult pig on the right</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>A brown piglet in the middle</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
				<idno>ID 1</idno>
		<title level="m">An orange goldfish in the center next to the largest fish</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>The smallest goldfish. ID 4: &quot;A small goldfish in the end&quot;. ID 5: &quot;A goldfish on the bottom</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Language supervision, ID 1: &quot;A brown camel in the front</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Pixel-level mask supervision Language supervision, ID 1: &quot;A silver car</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Pixel-level mask supervision Language supervision</title>
		<idno>ID 1</idno>
	</analytic>
	<monogr>
		<title level="m">A black car</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Pixel-level mask supervision Language supervision</title>
		<idno>ID 1</idno>
	</analytic>
	<monogr>
		<title level="m">A man in a suit riding a scooter</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>A black scooter ridden by a man</note>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Pixel-level mask supervision</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
