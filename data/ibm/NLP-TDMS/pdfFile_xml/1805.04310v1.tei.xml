<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Weakly and Semi Supervised Human Body Part Parsing via Pose-Guided Knowledge Transfer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao-Shu</forename><surname>Fang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guansong</forename><surname>Lu</surname></persName>
							<email>sjtuluguansong@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Fang</surname></persName>
							<email>fxlfang@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Xie</surname></persName>
							<email>jianwen@ucla.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
							<email>yuwingtai@tencent.com</email>
							<affiliation key="aff3">
								<orgName type="department">Tencent YouTu</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
							<email>lucewu@sjtu.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghai</forename><surname>Jiao</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Tong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Weakly and Semi Supervised Human Body Part Parsing via Pose-Guided Knowledge Transfer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Human body part parsing, or human semantic part segmentation, is fundamental to many computer vision tasks. In conventional semantic segmentation methods, the ground truth segmentations are provided, and fully convolutional networks (FCN) are trained in an end-to-end scheme. Although these methods have demonstrated impressive results, their performance highly depends on the quantity and quality of training data. In this paper, we present a novel method to generate synthetic human part segmentation data using easily-obtained human keypoint annotations. Our key idea is to exploit the anatomical similarity among human to transfer the parsing results of a person to another person with similar pose. Using these estimated results as additional training data, our semi-supervised model outperforms its strong-supervised counterpart by 6 mIOU on the PASCAL-Person-Part dataset [6], and we achieve stateof-the-art human parsing results. Our approach is general and can be readily extended to other object/animal parsing task assuming that their anatomical similarity can be annotated by keypoints. The proposed model and accompanying source code will be made publicly available. * This work was done when Xiaolin Fang was an intern at MVIG lab of Shanghai Jiao Tong University.</p><p>† The corresponding author is Cewu Lu, email: lucewu@sjtu.edu.cn. Cewu Lu is also a member of SJTU-SenseTime lab and AI research institute of SJTU.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The task of human body part parsing retrieves a semantic segmentation of body parts from the image of a person. Such pixel-level body part segmentations are not only crucial for activity understanding, but might also facilitate various vision tasks such as robotic manipulation <ref type="bibr" target="#b10">[11]</ref>, affordances reasoning <ref type="bibr" target="#b18">[19]</ref> and recognizing human-object interactions <ref type="bibr" target="#b13">[14]</ref>. In recent years, deep learning has been suc-  cessfully applied to the human part parsing problem <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b31">32]</ref>.</p><p>To fully exploit the power of deep convolutional neural networks, large-scale datasets are indispensable <ref type="bibr" target="#b8">[9]</ref>. However, semantic labeling of body parts on a pixel-level is labor intensive. For the task of human body part parsing, the largest dataset <ref type="bibr" target="#b5">[6]</ref> contains less than 2,000 labeled images for training, which is order of magnitudes less than the amount of training data in common benchmarks for image classification, semantic segmentation and keypoint estimation <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b11">12]</ref>. The small amount of data may lead to overfitting and degrade the performance in real world scenarios.</p><p>On the other hand, an abundance of human keypoints annotations <ref type="bibr" target="#b0">[1]</ref> is readily available. Current state-of-the-art pose estimation algorithms <ref type="bibr" target="#b26">[27]</ref> have also performed well in natural scene. The human keypoints encode structural information of the human body and we believe that such high-level human knowledge can be transfered to the task of human body part parsing.</p><p>However, despite the availability of keypoints annotations, few methods have investigated to utilize keypoints as augmented training data for human body part parsing. The main problem is that human keypoints are a sparse representation, while the human body part parsing requires an enormous amount of training data with dense pixel-wise annotations. Consequently, a end-to-end method which only relies on keypoint annotations as labels may not achieve high performance.</p><p>In this paper, we propose a novel approach to augment training samples for human parsing. Due to physical anatomical constraints, humans that share the same pose will have a similar morphology. As shown in <ref type="figure" target="#fig_0">Fig 1,</ref> given a person, we can use his/her pose annotation to search for the corresponding parsing results with similar poses. These collected parsing results are then averaged and form a strong part-level prior. In this way, we can convert the sparse keypoint representation into a dense body part segmentation prior. With the strong part-level prior, we combine it with the input image and forward them through a refinement network to generate an accurate part segmentation result. The generated part segmentation can be used as extra data to train a human parsing network.</p><p>We conduct exhaustive experiments on the PASCAL-Part dataset <ref type="bibr" target="#b5">[6]</ref>. Our semi supervised method achieves stateof-the-art performance using a simple VGG-16 <ref type="bibr" target="#b29">[30]</ref> based network, surpassing the performance of ResNet-101 <ref type="bibr" target="#b15">[16]</ref> based counterpart trained on limited part segmentation annotations. When utilizing a model based on the deeper ResNet-101, our proposed method outperforms the stateof-the-art results by 3 mAP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>This paper is closely related to the following areas: semantic part segmentation, joint pose and body part estimation, and weakly supervised learning.</p><p>Human body part parsing. In this subtask of semantic segmentation, fully convolutional network (FCN) <ref type="bibr" target="#b25">[26]</ref> and its variants <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b31">32]</ref> have demonstrated promising results. In <ref type="bibr" target="#b3">[4]</ref>, Chen et al. proposed atrous convolution to capture object features at different scales and they further combined the convolutional neural network (CNN) with a Conditional Random Field (CRF) to improve the accuracy. In <ref type="bibr" target="#b4">[5]</ref>, the authors proposed an attention mechanism that softly combines the segmentation predictions at different scales according to the context. To tackle the problem of scale and location variance, Xia et al. <ref type="bibr" target="#b31">[32]</ref> developed a model that adaptively zoom the input image into the proper scale to refine the parsing results.</p><p>Another approach to human parsing is the usage of re-current networks with long short term memory (LSTM) units <ref type="bibr" target="#b16">[17]</ref>. The LSTM network can innately incorporate local and global spatial dependencies into their feature learning. In <ref type="bibr" target="#b22">[23]</ref>, Liang et al. proposed the local-global LSTM network to incorporate spatial dependencies at different distances to improve the learning of features. In <ref type="bibr" target="#b21">[22]</ref>, the authors proposed a Graph LSTM network to fully utilize the local structures (e.g., boundaries) of images. Their network takes arbitrary-shaped superpixels as input and propagates information from one superpixel node to all its neighboring superpixel nodes. To further explore the multi-level correlations among image regions, Liang et al. <ref type="bibr" target="#b20">[21]</ref> proposed a structure-evolving LSTM that can learn graph structure during the optimization of LSTM network. These LSTM networks achieved competitive performance on human body part parsing.</p><p>Utilizing Pose for Human Parsing. Recent works try to utilize the human pose information to provide high-level structure for human body part parsing. Promising methods include pose-guided human parsing <ref type="bibr" target="#b33">[34]</ref>, joint pose estimation and semantic part segmentation <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b32">33]</ref> and selfsupervised structure-sensitive learning <ref type="bibr" target="#b14">[15]</ref>. These methods focus on using pose information to regularize part segmentation results, and they lie in the area of strong supervision. Our method differs from theirs in that we aim to transfer the part segmentation annotations to unlabeled data based on pose similarity and generate extra training samples, which emphasizes semi-supervision.</p><p>Weak Supervision for Semantic Segmentation. In <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b1">2]</ref>, the idea is to utilize weakly supervised methods for semantic segmentation. In particular, Chen et al. <ref type="bibr" target="#b6">[7]</ref> proposed to learn segmentation priors based on visual subcategories. <ref type="bibr">Dai</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Definition</head><p>Our goal is to utilize pose information to weakly supervise the training of a human parsing network. Consider a semantic part segmentation problem where we have a dataset notes its corresponding part segmentation label, K i ∈ R v×2 denotes its keypoints annotation, and i is training example index. Each example contains at most u body parts and v keypoints. In practice, N is usually small since labeling the human semantic part is very labor intensive. For a standard semantic part segmentation problem, the objective function can be written as:</p><formula xml:id="formula_0">D s = {I i , S i , K i } N i=1 of N labeled training examples. Let I i ∈ R h×w×3 denote an input image, S i ∈ R h×w×u de-</formula><formula xml:id="formula_1">E(Φ) = i j e[f j p (I i ; Φ), S i (j)],<label>(1)</label></formula><p>where j is the pixel index of the image I i , S i (j) is the semantic label at pixel j, f p (·) is the fully convolutional network, f j p (I i ; Φ) is the per-pixel labeling produced by the network given parameters Φ, and e[·] is the per-pixel loss function.</p><p>Similarly, we can consider another dataset</p><formula xml:id="formula_2">D p = {I i , K i } M i=1 of M examples with only keypoints annotations where M N .</formula><p>Our goal is to generate part segmentations for all images in D p and utilize these as additional training data when training a FCN by minimizing the objective function in Eqn. 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Overview</head><p>To utilize the keypoints annotations, we directly generate the pixel-wise part segmentations based on keypoints annotations. Given a target image I t and keypoints annotation K t where (I t , K t ) ∈ D p , we first find a subset of images in D s that have the most similar keypoints annotations (Sec. 3.3). Then, for the clustered images, we apply pose-guided morphing to their part segmentations to align them with the target pose (Sec. 3.4). The aligned body part segmentations are averaged to generate the part-level prior for the target image I t . Finally, a refinement network is applied to estimate the body part segmentation of I t (Sec. 3.5). The training method of our refinement network will be detailed in Sec. 3.6. The generated part segmentations can be used as extra training data for human part segmentation (Sec. 3.7). <ref type="figure" target="#fig_1">Figure 2</ref> gives an overview of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Discovering Pose-Similar Clusters</head><p>In order to measure similarity between different poses, we first need to normalize and align all the poses. We normalize the pose size by fixing their torsos to the same length. Then, the hip keypoints are used as the reference coordinate to align with the origin. We measure the Euclidean distances between K t and every keypoint annotations in D s . The top-k persons in D s with the smallest distances are chosen and form the pose-similar cluster, which serves as the basis for the following part-level prior generation step. The influence of k will be evaluated in Sec. 4.3.</p><p>Intuitively, given an image I t with only keypoint annotations K t , one may think of an alternative solution to obtain the part-parsing prior by solely morphing the part segmentation result of the person that has the closest pose to K t . However, due to the differences between human bodies or possible occlusions, a part segmentation result with distinct boundary may not fit well to another one. Thus, instead of finding the person with the most similar pose, we find several persons that have similar poses and generate part-level prior by averaging their morphed part segmentation results. Such averaged part-level prior can be regarded as a probability map for each body part. It denotes the possibility for each pixel of whether it belongs to a body part based on real data distribution. In Sec. 4.3, we show that using the averaged prior achieves better performance than using only the parsing result of the closest neighbor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Generating Part-Level Prior</head><p>The discovered pose-similar cluster forms a solid basis for generating the part-level prior. However, for each cluster, there are some inevitable intra-cluster pose variations, which makes the part parsing results misaligned. Thus, we  introduce the pose-guided morphing method.</p><p>For n persons in the same pose-similar cluster, let us denote their part parsing results as S = {S 1 , ..., S n }, their keypoints annotations as K = {K 1 , ..., K n } and the morphed part parsing results as S = { S 1 , ..., S n }. By comparing the poses in K with the target pose K t , we can compute the transformation parameters θ and then use them to transform S to obtain the morphed part parsing results S. We use the affine transformation to morph the part segmentations. This procedure can be expressed as:</p><formula xml:id="formula_3">S = {T (S i ; θ i ) | 1 ≤ i ≤ n , S i ∈ S},<label>(2)</label></formula><p>where</p><formula xml:id="formula_4">θ i = g(K t , K i ), K i ∈ K,</formula><p>T (·) is the affine transformation with parameters θ, and g(·) computes θ according to pose K i and the target pose K t . For the part parsing annotations, we represent them as the combination of several binary masks. Each mask represent the appearance of a corresponding body part. The morphing procedure is conducted on each body part independently. Consider the left upper arm as an example. For the left upper arm segment G 1 = x 1 of pose K 1 and the same segment G 2 = x 2 of pose K t , we have transformation relationship</p><formula xml:id="formula_5">x 1 1 = Q x 2 1 = A b 0 1 x 2 1 ,<label>(3)</label></formula><p>where Q is the affine transformation matrix we need to calculate. Since both x 1 and x 2 are known, we can easily compute the result of A and b. Then, the morphed part segmentation mask can be obtained by  where {x t i , y t i } and {x s i , y s i } are the coordinates before and after transformation. <ref type="figure" target="#fig_3">Figure 3</ref> illustrates our pose-guided morphing method.</p><formula xml:id="formula_6">x t i y t i = A x s i y s i + b,<label>(4)</label></formula><p>After the pose-guided morphing, the transformed part segmentations S are averaged to form the part-level prior</p><formula xml:id="formula_7">P t = 1 n n i=1 S i .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Prior Refinement</head><p>Finally, we feed forward our part-level prior through a refinement network together with the original image.</p><p>With a coarse prior, the search space for our refinement network is significantly reduced, and thus it can achieve superior results than directly making predictions from a single image input. The discriminative power of a CNN can eliminate the uncertainty at the body part boundary of the part parsing prior based on local image evidence, thus leading to high-quality parsing results. For each image I t with body part prior P t , we estimate the part segmentation result S t by</p><formula xml:id="formula_8">S t = f r (I t , P t ; Ψ),<label>(5)</label></formula><p>where f r is the refinement network with parameters Ψ. In the next section, we will elaborate on the learning of Ψ. This estimated part segmentation result can be used as extra training data to train the FCN. The semi-supervised regime will be further discussed in Section 3.7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Training of Refinement Network</head><p>Previous sections are conducted under the assumption that we have a well-trained refinement network. In this section, we will explain the training algorithm. <ref type="figure" target="#fig_4">Fig. 4</ref> depicts a schematic overview of the proposed training pipeline.</p><p>The refinement network is a variant of "U-Net" proposed in <ref type="bibr" target="#b17">[18]</ref>, which is in the form of an auto-encoder network with skip connections. For such a network, the input will be progressively down-sampled until a bottleneck layer and then gradually up-sampled to restore the input size. To preserve the local image evidence, skip connections are introduced between each layer i and layer n − i, assuming the network has n layers in total. Each skip connection concatenates the feature maps at layer i with those in layer n−i. We refer readers to <ref type="bibr" target="#b17">[18]</ref> for the detailed network structures. The input for our network is an image as well as a set of masks. Each mask is a heatmap ranging from 0 to 1 that represents the probability map for a specific body part. The output for this network is also a set of masks that have the same representation. The label is a set of binary masks indicating the appearance of each body part. Our objective function is the L1 distance between the output and the label. Both input and output size are set as 256 × 256.</p><p>To train the refinement network, we utilize the data in D s with both semantic part segmentation annotations and pose annotations. Given an image I m ∈ D s , similar to the pipeline for generating part-level prior in Sec. 3.4, we first generate the part parsing prior P m given I m . The only difference is that when discovering pose-similar cluster, we find n nearest neighbours each time and randomly pick k of them to generate part-level prior. This can be regarded as a kind of data augmentation to improve the generalization ability of the refinement network. The impact of n will be discussed in Sec. 4.3. Formally, with the part-level prior P m and semantic part segmentation ground truth S m for image I m , we train our refinement network by minimizing the cost function:</p><formula xml:id="formula_9">E(Ψ) = j S m (j) − f j r (I m , P m ; Ψ) 1 .<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7.">Semi-Supervised Training for Parsing Network</head><p>In previous sections, we have presented our method to generate pixel-wise part segmentation labels based on keypoints annotations. Now we can train the parsing network for part segmentation in a semi-supervised manner. For our parsing network, we use the VGG-16 based model proposed in <ref type="bibr" target="#b4">[5]</ref> due to its effective performance and simple structure. In this network, multi-scale inputs are applied to a shared VGG-16 based DeepLab model <ref type="bibr" target="#b3">[4]</ref> for predictions. A soft attention mechanism is employed to weight the outputs of the FCN over scales. The training for this network follows the standard process of optimizing the per-pixel regression problem, which is formulated as Eqn. 1. For the loss function, we use the multinomial logistic loss. During training, the input image is resized and padded to 320 × 320.</p><p>We consider updating the parameter Φ of network f p by minimizing the objective function in Eqn. 1 on D s with ground truth labels and D p with generated part segmentation labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we first introduce related datasets and implementation details in our experiments. Then, we re-port our results and comparisons with state-of-the-art performance. Finally, we conduct extensive experiments to validate the effectiveness of our proposed semi-supervision method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>Pascal-Person-Part Dataset <ref type="bibr" target="#b5">[6]</ref> is a dataset for human semantic part segmentation. It contains 1,716 images for training and 1,817 images for testing. The dataset contains detailed pixel-wise annotations for body parts, including hands, ears, etc. The keypoint annotations for this dataset have been made available by <ref type="bibr" target="#b32">[33]</ref>. Horse-Cow Dataset <ref type="bibr" target="#b30">[31]</ref> is a part segmentation benchmark for horse and cow images. It contains 294 training images and 227 testing images. The keypoint annotations for this dataset are provided by <ref type="bibr" target="#b2">[3]</ref>. In addition, 317 extra keypoint annotations are provided by <ref type="bibr" target="#b2">[3]</ref>, which have no corresponding pixel-wise part parsing annotations. We take these keypoint annotations as extra training data for our network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>In our experiments, we merge the part segmentation annotations in <ref type="bibr" target="#b5">[6]</ref> to be Head, Torso, Left/Right Upper Arms, Left/Right Lower Arms, Left/Right Upper Legs and Left/Right Lower Legs, resulting in 10 body parts. The pose-guided morphing is conducted on the mask of each body part respectively. In order to be consistent with previous works, after the morphing, we merge the masks of each left/right pair by max-pooling and get six body part classes.</p><p>For our semi-supervision setting, we first train the refinement network and then fix it to generate synthetic part segmentation data using keypoints annotations in <ref type="bibr" target="#b0">[1]</ref>. Note that for simplicity, we only synthesize part segmentation labels for the single person case, and it would easy to extend to a multi-person scenario. To train our refinement network with single person data, we crop those people with at least upper body keypoints annotations in the training list of the part parsing dataset <ref type="bibr" target="#b5">[6]</ref> and get 2,004 images with a single person and corresponding part segmentation label. We randomly sample 100 persons as validation set for the refinement network to set hyper-parameters.</p><p>To train the refinement network, we apply random jittering by first resizing input images to 286 × 286 and then randomly cropping to 256 × 256. The batch size is set to <ref type="figure">Figure 5</ref>. Qualitative comparison of the refinement network trained with and without part-level prior. For each image group, from left to right: input image, FCN prediction without prior, corresponding part-level prior for the input image, prediction from refinement network trained with prior.</p><p>1. We use the Adam optimizer with an initial learning rate of 2e−4 and β 1 of 0.5. To train our parsing network, we follow the same setting as <ref type="bibr" target="#b4">[5]</ref> by setting the learning rate as 1e-3 and decayed by 0.1 after 2,000 iterations. The learning rate for the last layer is 10 times larger than previous layers. The batch size is set to 30. We use the SGD solver with a momentum of 0.9 and weight decay of 5e−4. All experiments are conducted on a single Nvidia Titan X GPU. Our refinement network is trained from scratch for 200 epochs, which takes around 18 hours. The parsing network is initialized with a model pre-trained on COCO dataset <ref type="bibr" target="#b24">[25]</ref> which is provided by the author. We train it for 15K iterations, and it takes around 30 hours for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results and Comparisons</head><p>Evaluation of Annotation Number. We first compare the performance of the parsing network trained with a different number of annotations on validation set and the results are reported in <ref type="table">Table 1</ref>. When all annotations of semantic part segmentations are used, the result is the original implementation of the baseline model <ref type="bibr" target="#b4">[5]</ref>, and our reproduction is slightly (0.5 mIOU) higher. Then we gradually add the data with keypoint annotations. As can be seen, the performance increases in line with the number of keypoint annotations. This suggests that our semi supervision method is effective and scalable. <ref type="figure" target="#fig_6">Fig. 6</ref> gives some qualitative comparisons between the predictions of the baseline model and our semisupervised model.</p><p>Significance of Part-level Prior. We evaluate the significance of the part-level prior. First, we compare the performance of refinement network trained with or without partlevel prior on the cropped single person part segmentation validation set. Without prior, the refinement network is a regular FCN trained with limited training data. The vali- dation accuracy for both cases is shown in <ref type="figure">Fig. 7</ref>. As we can see, the accuracy of the refinement network with partlevel prior is much higher than the case without prior. In <ref type="figure">Fig. 5</ref>, we visualize some predictions from our refinement network trained with and without part-level priors, and the corresponding priors are also visualized.</p><p>Prior Generation Strategy. During our training process, the quality of part-level prior is important and directly affects the refined part segmentation results. We compare different prior generation strategies and report the final results in <ref type="table">Table 2</ref>. We first explore using the skeleton label map <ref type="bibr" target="#b32">[33]</ref>as prior, which draws a stick with width 7 between neighboring joints, and the result is 58.26 mIOU. Comparing to this method, our proposed part-level prior has a considerable improvement, which indicates the importance of knowledge transfer during prior generation. For part-level prior, we compare the impact of the size k of pose-similar cluster. As aforementioned in Sec. 3.3, if we only choose the person with the nearest pose and take his/her morphed part parsing result as our prior, the performance is limited. But if we choose too many people to generate our part-level prior, the quality of the final results would also decline. We claim that this is because our part segmentation dataset is small and the intra-cluster part appearance variance would increase as the cluster size increases. Then, we explore the influence of adding data augmentation during the training of refinement network. As we can see, randomly sample 3 candidates to generate partlevel prior from a larger pool with size 5 is beneficial for training since it increases the sample variances. For the remaining experiments, we set k as 3 and n as 5.</p><p>Comparisons with State-of-the-Art. In <ref type="table">Table 3</ref>, we report comparisons with the state-of-the-art results on the  <ref type="figure">Figure 7</ref>. Validation accuracy of refinement network with and without part-level prior. The left curve is the case trained without prior and the right one is the case with prior. The performance of refinement network is much better with the part-level prior.</p><p>Pascal-Person-Part dataset <ref type="bibr" target="#b5">[6]</ref>. With additional training data generated by our refinement network, our VGG16 based model achieves the state-of-the-art performance. Although we focus on exploiting human keypoints annotations as extra supervision, our method applies to other baseline models and can benefit from other parallel improvements. To prove that, we replace our baseline model with ResNet-101 <ref type="bibr" target="#b15">[16]</ref> based DeepLabv2 <ref type="bibr" target="#b3">[4]</ref>   <ref type="table">Table 3</ref>. Comparison of semantic object parsing performance with several state-of-the-art methods on the PASCAL-Person-Part dataset <ref type="bibr" target="#b5">[6]</ref>. "+ms" denotes testing with multi-scale inputs. Note that we only perform single scale testing for our VGG-16 entry since the base network <ref type="bibr" target="#b4">[5]</ref> has explicitly utilized multi-scale features.</p><p>Comparisons between two Networks. To see how the refinement network can assist the training of the parsing network, we visualize some predictions of both networks in <ref type="figure">Fig. 8</ref>. In this experiment, the parsing network we use has already been trained under the semi supervision setting. As shown in <ref type="figure">Fig. 8</ref>, due to the guidance of the strong partlevel prior, the refinement network makes fewer mistakes on the structure predictions (e.g., the upper legs in (a) and the upper arms in (b)), and produces tighter masks (e.g., the legs in (c)). These improvements obtained by using the partlevel prior will be transferred to the parsing network during the semi-supervised training. On the other hand, by leveraging a large number of training data, the parsing network can figure out which are important or irrelevant features and produce predictions that are less noisy (e.g.,(b) and (d)).</p><p>Training on Pose Estimation Results. Since single person pose estimation is mature enough to be deployed, what if we replace the ground-truth keypoint annotations with pose estimation results? To answer that, we use the pose estimator <ref type="bibr" target="#b12">[13]</ref> trained on MPII dataset to estimate human poses in COCO dataset <ref type="bibr" target="#b24">[25]</ref>. Same as previous, we crop those people with full body annotations and collect 10K images. The semi-supervised result achieves 61.8 mIOU, which is on par with the results trained on ground-truth annotations. It shows that our system is robust to noise and suggests a promising strategy to substantially improve the performance of human semantic part segmentation without extra costs.</p><p>Extension to other Categories. To show the potential of extending our method to other categories, we also perform experiments on the Horse-Cow Dataset <ref type="bibr" target="#b30">[31]</ref>. The results are reported in <ref type="table">Table 4</ref>. Our baseline model, which is the attention model <ref type="bibr" target="#b4">[5]</ref>, has an mIOU of 71.55 for the Horse and an IOU of 68.84 for the Cow. By leveraging the keypoint annotations provided by <ref type="bibr" target="#b2">[3]</ref>, we gain improvements of 3.14 and 3.39 mIOU for Horse and Cow respectively. The consis-  <ref type="table">Table 4</ref>. Comparison of object parsing performance with three state-of-the-art methods over the Horse-Cow object parsing dataset <ref type="bibr" target="#b30">[31]</ref>. We also list the performance of Attention <ref type="bibr" target="#b4">[5]</ref>, which is the baseline model for our VGG-16 entry. tent improvements across different categories indicate that our method is general and applicable to other segmentation tasks where their anatomical similarity can be annotated by keypoints. Finally, by replacing our baseline model with the deeper ResNet-101 based model, we achieve the most stateof-the-art result on the Horse-Cow dataset, yielding the final performances of 76.99 and 74.22 mIOU for these two categories respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose a novel strategy to utilize the keypoint annotations to train deep network for human body part parsing. Our method exploits the constraints on biological morphology to transfer the part parsing annotations among different persons with similar poses. Experimental results show that it is effective and general. By utilizing a large number of keypoint annotations, we achieve the most state-of-the-art result on human part segmentation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Due to the morphological constraints, persons that share the same pose should have similar semantic part segmentations. For a person with only keypoints annotation, we search for persons in the human body part parsing dataset that have similar poses and then transfer their part parsing annotations to the target person. The transferred annotations form a strong part-level prior for the target person.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Overview of our method. Given an image with keypoint annotations, we first search for the corresponding part segmentation with similar pose (a). Then, we apply a pose-guided morphing to the retrieved part segmentation masks and compute a part-level prior (b). A refinement network is applied to refine the prior based on local image evidence. The semantic part segmentation results can be used as extra training data to train a human parsing network without keypoint annotations (c). See text for more details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Pose-guided morphing for body part parsing. For each body part, we compute an affine transformation matrix Q according to the corresponding pose segment. The local body part is then transformed according to the estimated transformation matrix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Training pipeline for our refinement network. We elaborate on the details in the text.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>MPII [ 1 ]</head><label>1</label><figDesc>is a challenging benchmark for person pose estimation. It contains around 25K images and over 40K people with pose annotations. The training set consists of over 28K people and we select those with full body annotations as extra training data for human part segmentation. After filtering, there remain 10K images with single person pose annotations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Qualitative comparison on the PASCAL-Person-Part dataset between the baseline model and our semi-supervised model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>model and follow the same training setting. Without additional keypoints annotations, this baseline achieves 59.6 mAP. Under our semi-supervised train-Figure 8. Qualitative comparison between the predictions of the refinement network and the parsing network. "ref.net" denotes the predictions of the refinement network, and "parsing net" denotes the predictions of the parsing network.ing, this model achieves 64.28 mIOU. Note that this result is obtained by single scale testing. By performing multiscale testing (scale = 0.5, 0.75, 1), we can further achieve 67.6 mIOU, which outperforms previous best result by 3 mIOU.Method head torso u-arms l-arms u-legs l-legs Bkg AvgDeepLab-LargeFOV-CRF<ref type="bibr" target="#b3">[4]</ref> 80.13 55.56 36.43 38.72 35.50 30.82 93.52 52.95 Attention [5] 81.47 59.06 44.15 42.50 38.28 35.62 93.65 56.39 HAZN [32] 80.79 80.76 45.65 43.11 41.21 37.74 93.78 57.54 Graph LSTM [23] 82.69 62.68 46.88 47.71 45.66 40.93 94.59 60.16 Structure-evolving LSTM [21] 82.89 67.15 51.42 48.72 51.72 45.91 97.18 63.57 Joint (VGG-16, +ms) [33] 80.21 61.36 47.53 43.94 41.77 38.00 93.64 58.06 Joint (ResNet-101, +ms) [33] 85.50 67.87 54.72 54.30 48.25 44.76 95.32 64.39</figDesc><table><row><cell>Ours (VGG-16)</cell><cell cols="4">84.06 67.03 51.66 50.15 45.33 44.26 95.73 62.60</cell></row><row><cell>Ours (ResNet-101)</cell><cell cols="4">84.83 68.64 53.11 53.01 48.40 46.76 95.22 64.28</cell></row><row><cell>Ours (ResNet-101, +ms)</cell><cell cols="4">87.15 72.28 57.07 56.21 52.43 50.36 97.72 67.60</cell></row><row><cell></cell><cell></cell><cell>(a)</cell><cell></cell><cell>(b)</cell></row><row><cell></cell><cell></cell><cell>(c)</cell><cell></cell><cell>(d)</cell></row><row><cell></cell><cell>image</cell><cell>ref. net parsing net</cell><cell>image</cell><cell>ref. net</cell><cell>parsing net</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>70.75 84.49 63.91 51.73 72.36 Graph LSTM [23] 91.73 72.89 86.34 69.04 53.76 74.75 Structure-evolving LSTM [21] 92.51 74.89 87.55 71.93 57.45 76.87 Attention [5] 90.48 68.91 83.34 64.20 50.74 71.55 Ours(VGG-16) 91.62 72.75 87.24 69.52 52.32 74.69 Ours(ResNet-101) 93.39 75.63 88.39 72.61 54.95 76.99 75.18 83.33 57.42 29.37 67.20 Graph LSTM [23] 91.54 73.88 85.92 63.67 35.22 70.05 Structure-evolving LSTM [21] 92.88 77.75 87.91 67.60 42.86 73.80 Attention [5] 91.06 74.33 84.38 60.60 33.85 68.84 Ours(VGG-16) 92.27 78.36 88.20 64.76 37.58 72.23 Ours(ResNet-101) 93.70 80.30 89.53 66.92 40.65 74.22</figDesc><table><row><cell></cell><cell></cell><cell>Horse</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Bkg</cell><cell>head body</cell><cell>leg</cell><cell>tail</cell><cell>Avg</cell></row><row><cell>HAZN [32]</cell><cell cols="2">90.94 Cow</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Bkg</cell><cell>head body</cell><cell>leg</cell><cell>tail</cell><cell>Avg</cell></row><row><cell>HAZN [32]</cell><cell>90.71</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement:</head><p>This work is supported in part by the National Natural Science Foundation of China under Grants 61772332. This work is also support in part by Sensetime Ltd</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">What&apos;s the point: Semantic segmentation with point supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bearman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Detection, attribute classification and action recognition of people using poselets (in submission)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">T&apos;PAMI</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Attention to scale: Scale-aware semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Detect what you can: Detecting and representing objects using holistic models and body parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Enriching visual knowledge bases via object discovery and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Boxsup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Towards unified human parsing and pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Interactive grasp learning based on human demonstration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ekvall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kragic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">RMPE: Regional multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Actions and attributes from wholes and parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Look into person: Self-supervised structure-sensitive learning and a new benchmark for human parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.05446</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Image-to-image translation with conditional adversarial networks. arxiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning human activities and object affordances from rgb-d videos. The International</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="951" to="970" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Human pose estimation using a joint pixel-wise and part-wise formulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03055</idno>
		<title level="m">Interpretable structure-evolving lstm</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semantic object parsing with graph lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semantic object parsing with local-global long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Scribblesup: Scribble-supervised convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Weakly-and semi-supervised learning of a dcnn for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.02734</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Fully convolutional multi-class multiple instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7144</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Semantic part segmentation using compositional model combining shape and appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Zoom better to see clearer: Human and object parsing with hierarchical auto-zoom net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Joint multi-person pose estimation and semantic part segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.03383</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pose-guided human parsing by an and/or graph using pose-context features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
