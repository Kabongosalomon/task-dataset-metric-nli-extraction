<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Data Diversification: A Simple Strategy For Neural Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan-Phi</forename><surname>Nguyen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute for Infocomm Research (I</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
							<email>srjoty@ntu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Salesforce Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Kui</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Institute for Infocomm Research (I</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ai</forename><forename type="middle">Ti</forename><surname>Aw</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Institute for Infocomm Research (I</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Data Diversification: A Simple Strategy For Neural Machine Translation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce Data Diversification: a simple but effective strategy to boost neural machine translation (NMT) performance. It diversifies the training data by using the predictions of multiple forward and backward models and then merging them with the original dataset on which the final NMT model is trained. Our method is applicable to all NMT models. It does not require extra monolingual data like back-translation, nor does it add more computations and parameters like ensembles of models. Our method achieves state-of-the-art BLEU scores of 30.7 and 43.7 in the WMT'14 English-German and English-French translation tasks, respectively. It also substantially improves on 8 other translation tasks: 4 IWSLT tasks (English-German and English-French) and 4 low-resource translation tasks (English-Nepali and English-Sinhala). We demonstrate that our method is more effective than knowledge distillation and dual learning, it exhibits strong correlation with ensembles of models, and it trades perplexity off for better BLEU score.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The invention of novel architectures for neural machine translation (NMT) has been fundamental to the progress of the field. From the traditional recurrent approaches <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b16">17]</ref>, NMT has advanced to self-attention method <ref type="bibr" target="#b27">[28]</ref>, which is more efficient and powerful and has set the standard for many other NLP tasks <ref type="bibr" target="#b2">[3]</ref>. Another parallel line of research is to devise effective methods to improve NMT without intensive modification to model architecture, which we shall refer to as non-intrusive extensions. Examples of these include the use of sub-word units to solve the out-of-vocabulary (OOV) problem <ref type="bibr" target="#b21">[22]</ref> or exploiting extra monolingual data to perform semi-supervised learning using back-translation <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b3">4]</ref>. One major advantage of these methods is the applicability to most existing NMT models as well as potentially future architectural advancements with little change. Thus, nonintrusive extensions are used in practice to avoid the overhead cost of developing new architectures and enhance the capability of existing state-of-the-art models.</p><p>In this paper, we propose Data Diversification 1 , a simple but effective way to improve machine translation consistently and significantly. In this method, we first train multiple models on both backward (target→source) and forward (source→target) translation tasks. Then, we use these models to generate a diverse set of synthetic training data from both lingual sides to augment the original data. Our approach is inspired from and a combination of multiple well-known strategies: back-translation, ensemble of models, data augmentation and knowledge distillation for NMT.</p><p>Our method establishes the state of the art (SOTA) in the WMT'14 English-German and English-French translation tasks with 30.7 and 43.7 BLEU scores, respectively. <ref type="bibr" target="#b1">2</ref> Furthermore, it gives 1.0-2.0 BLEU gains in 4 IWSLT tasks (English↔German and English↔French) and 4 low-resource tasks (English↔Sinhala and English↔Nepali). We demonstrate that data diversification outperforms other related methods -knowledge distillation <ref type="bibr" target="#b13">[14]</ref> and dual learning <ref type="bibr" target="#b30">[31]</ref>, and is complementary to back-translation <ref type="bibr" target="#b20">[21]</ref> in semi-supervised setup. Our analysis further reveals that the method is correlated with ensembles of models and it sacrifices perplexity for better BLEU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Novel Architectures The invention of novel neural architectures has been fundamental to scientific progress in NMT. Often they go through further refinements and modifications. For instance, Shaw et al. <ref type="bibr" target="#b22">[23]</ref> and Ahmed et al. <ref type="bibr" target="#b0">[1]</ref> propose minor modifications to improve the original Transformer <ref type="bibr" target="#b27">[28]</ref> with slight performance gains. Ott et al. <ref type="bibr" target="#b17">[18]</ref> propose scaling the training process to 128 GPUs to achieve more significant improvements. Wu et al. <ref type="bibr" target="#b31">[32]</ref> repeat the cycle with dynamic convolution. Side by side, researchers also look for other complementary strategies to improve the performance of NMT systems, which are orthogonal to the advancements in model architectures. Semi-supervised NMT Semi-supervised learning offers considerable capabilities to NMT models. Back-translation <ref type="bibr" target="#b20">[21]</ref> is a simple but effective way to exploit extra monolingual data. Another effective strategy is to use pretrained models. Zhu et al. <ref type="bibr" target="#b33">[34]</ref> recently propose a novel way to incorporate pretrained BERT <ref type="bibr" target="#b2">[3]</ref> to improve NMT. Nonetheless, the drawback of both approaches is that they require huge extra monolingual data to train/pretrain. Acquiring enormous datasets is sometimes expensive, especially for low-resource scenarios (languages or domains). Moreover, in the case of using pretrained BERT, the packaged translation model incurs the additional computational cost of the pretrained model. <ref type="table" target="#tab_0">Table 1</ref> summarizes different types of costs for training and inference of different approaches to improve NMT. Developing new architectures, like dynamic convolution <ref type="bibr" target="#b31">[32]</ref>, offers virtually no measurable compromise for training and inference, but it may take time for new models to be refined and mature. On the other hand, semi-supervised methods are often simpler, but require significantly more training data. In particular, Edunov et al. <ref type="bibr" target="#b3">[4]</ref> use back-translation with 50× more training data. NMT+BERT <ref type="bibr" target="#b33">[34]</ref> requires 60× more computations and 25× more data to train (including the pre-training stage). It also needs 3× more computations and parameters during inference. Evolved-Transformer <ref type="bibr" target="#b24">[25]</ref>, an evolution-based technique, requires more than 15,000 times more FLOPs to train. This may not be practical for common practitioners.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Resource Trade-offs</head><p>On the other hand, our data diversification method is simple as back-translation, but it requires no extra monolingual data. It also has the same inference efficiency as the "New Architectures" approach. However, it has to make compromise with extra computations in training. <ref type="bibr" target="#b2">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data diversification</head><p>Let D = (S, T ) be the parallel training data, where S denotes the source-side corpus and T denotes the target-side corpus. Also, let M S→T and M T →S be the forward and backward NMT models, Algorithm 1 Data Diversification: Given a dataset D = (S, T ), a diversification factor k, the number of rounds N ; return a trained source-target translation modelM S→T . 1: procedure TRAIN(D = (S, T ))</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2:</head><p>Train randomly initialized M on D = (S, T ) until convergence <ref type="bibr">3:</ref> return</p><formula xml:id="formula_0">M 1: procedure DATADIVERSE(D = (S, T ), k, N ) 2: D 0 ← D</formula><p>Assign original dataset to round-0 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3:</head><p>for r ∈ 1, . . . , N do 4:</p><formula xml:id="formula_1">D r = (S r , T r ) ← D r−1 5: for i ∈ 1, . . . , k do 6: M i S→T,r ← TRAIN(D r−1 = (S r−1 , T r−1 ))</formula><p>Train forward model <ref type="bibr" target="#b6">7</ref>:</p><formula xml:id="formula_2">M i T →S,r ← TRAIN(D r−1 = (T r−1 , S r−1 )) Train backward model 8: D r ← D r ∪ (S, M i S→T,r (S))</formula><p>Add forward data 9:</p><formula xml:id="formula_3">D r ← D r ∪ (M i T →S,r (T ), T ) Add backward data 10:M S→T ← T rain(D N )</formula><p>Train the final model <ref type="bibr">11:</ref> returnM S→T which translate from source to target and from target to source, respectively. In our case, we use the Transformer <ref type="bibr" target="#b27">[28]</ref> as the base architecture. In addition, given a corpus X l in language l and an NMT model M l→l which translates from language l to languagel, we denote the corpus M l→l (X l ) as the translation of corpus X l produced by the model M l→l . The translation may be conducted following the standard procedures such as maximum likelihood and beam search inference.</p><p>Our data diversification strategy trains the models in N rounds. In the first round, we train k forward models (M 1 S→T,1 , ..., M k S→T,1 ) and k backward models (M 1 T →S,1 , .., M k T →S,1 ), where k denotes a diversification factor. Then, we use the forward models to translate the source-side corpus S of the original data to generate synthetic training data. In other words, we obtain multiple synthetic targetside corpora as (M 1 S→T,1 (S), ..., M k S→T,1 (S)). Likewise, the backward models are used to translate the target-side original corpus T to synthetic source-side corpora as (M 1 T →S,1 (T ), ..., M k T →S,1 (T )). After that, we augment the original data with the newly generated synthetic data, which is summed up to the new round-1 data D 1 as follows:</p><formula xml:id="formula_4">D 1 = (S, T ) ∪ k i=1 (S, M i S→T,1 (S)) ∪ k i=1 (M i T →S,1 (T ), T )<label>(1)</label></formula><p>After that, if the number of rounds N &gt; 1, we continue training round-2 models (M 1 S→T,2 , ..., M k S→T,2 ) and (M 1 T →S,2 , .., M k T →S,2 ) on the augmented data D 1 . The similar process continues until the final augmented dataset D N is generated. Eventually, we train the final model M S→T on the dataset D N . For a clearer presentation, Algorithm 1 summarizes the process concretely.</p><p>In the experiments, unless specified otherwise, we use the default setup of k = 3 and N = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Relation with existing methods</head><p>Our method shares certain similarities with a variety of existing techniques, namely, data augmentation, back-translation, ensemble of models, knowledge distillation and multi-agent dual learning.</p><p>Data augmentation Our approach is genuinely a data augmentation method. Fadaee et al. <ref type="bibr" target="#b5">[6]</ref> proposed an augmentation strategy which targets rare words to improve low-resource translation. Wang et al. <ref type="bibr" target="#b29">[30]</ref> suggested to simply replace random words with other words in the vocabularies. Our approach is distinct from these methods in that it does not randomly corrupt the data and train the model on the augmented data on the fly. Instead, it transforms the data into synthetic translations, which follow different model distributions.</p><p>Back-translation Our method is similar to back-translation, which has been employed to generate synthetic data from target-side extra monolingual data. Sennrich et al. <ref type="bibr" target="#b20">[21]</ref> were the first to propose such strategy, while Edunov et al. <ref type="bibr" target="#b3">[4]</ref> refined it at scale. Our method's main advantage is that it does not require any extra monolingual data. Our technique also differs from previous work in that it additionally employs forward translation, which we have shown to be important (see §5.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ensemble of models</head><p>Using multiple models to average the predictions and reduce variance is a typical feature of ensemble methods <ref type="bibr" target="#b18">[19]</ref>. However, the drawback is that the testing parameters and computations are multiple times more than an individual model. While our diversification approach correlates with model ensembles ( §5.1), it does not suffer this disadvantage.</p><p>Knowledge distillation Knowledge distillation <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b7">8]</ref> involves pre-training a big teacher model and using its predictions (forward translation) to train a smaller student as the final model. In comparison to that, our method additionally employs back-translation and involves multiple backward and forward "teachers". We use all backward, forward, as well as the original data to train the final model without any parameter reduction. We also repeat the process multiple times. In this context, our method also differs from the ensemble knowledge distillation method <ref type="bibr" target="#b6">[7]</ref>, which uses the teachers to jointly generate a single version of data. Our method on the other hand uses the teachers to individually generate various versions of synthetic data.</p><p>Multi-agent dual learning Multi-agent dual learning <ref type="bibr" target="#b30">[31]</ref> involves leveraging duality with multiple forward and backward agents. Similar to <ref type="bibr" target="#b6">[7]</ref>, this method combines multiple agents in an ensembling manner to form forward (F α ) and backward (G β ) teachers. Then, it simultaneously optimizes the reconstruction losses ∆ x (x, G β (F α (x))) and ∆ y (y, F α (G β (y))) to train the final dual models. As a result, the two models are coupled and entangled. On the other hand, our method does not combine the agents in this way, nor does it optimize any reconstruction objective.</p><p>Our approach is also related but substantially different from the mixture of experts for diverse MT <ref type="bibr" target="#b23">[24]</ref>, iterative back-translation <ref type="bibr" target="#b11">[12]</ref> and copied monolingual data for NMT <ref type="bibr" target="#b1">[2]</ref>; see the Appendix for further details about these comparisons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we present experiments to demonstrate that our data diversification approach improves translation quality in many translation tasks, encompassing WMT and IWSLT tasks, and high-and low-resource translation tasks. Due to page limit, we describe the setup for each experiment briefly in the respective subsections and give more details in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">WMT'14 English-German and English-French translation tasks</head><p>Setup. We conduct experiments on the standard WMT'14 English-German (En-De) and English-French (En-Fr) translation tasks. The training datasets contain about 4.5M and 35M sentence pairs respectively. The sentences are encoded with Byte-Pair Encoding (BPE) <ref type="bibr" target="#b21">[22]</ref> with 32K operations.We use newstest2013 as the development set, and newstest2014 for testing. Both tasks are considered high-resource tasks as the amount of parallel training data is relatively large. We use the Transformer <ref type="bibr" target="#b27">[28]</ref> as our NMT model and follow the same configurations as suggested by Ott et al. <ref type="bibr" target="#b17">[18]</ref>. When augmenting the datasets, we filter out the duplicate pairs, which results in training datasets of 27M and 136M pairs for En-De and En-Fr, respectively. <ref type="bibr" target="#b3">4</ref> We do not use any extra monolingual data.</p><p>Results. From the results on WMT newstest2014 testset in <ref type="table" target="#tab_1">Table 2</ref>, we observe that the scale Transformer <ref type="bibr" target="#b17">[18]</ref>, which originally gives 29.3 BLEU in the En-De task, now gives 30.7 BLEU with our data diversification strategy, setting a new SOTA. Our approach yields an improvement of 1.4 BLEU over the without-diversification model and 1.0 BLEU over the previous SOTA reported on this task by Wu et al. <ref type="bibr" target="#b31">[32]</ref>. <ref type="bibr" target="#b4">5</ref> Our approach also outperforms other non-intrusive extensions, such as multi-agent dual learning and knowledge distillation by a good margin (0.7-3.1 BLEU). Similar observation can be drawn for WMT'14 En-Fr task. Our strategy establishes a new SOTA of 43.7 BLEU, exceeding the previous (reported) SOTA by 0.5 BLEU. It is important to mention that while our method increases the overall training time (including the time to train the base models), training a single Transformer model for the same amount of time only leads to overfitting.  <ref type="bibr" target="#b13">[14]</ref> 27.6 38.6 Distill (T=S) <ref type="bibr" target="#b13">[14]</ref> 28.4 42.1 Ens-Distill <ref type="bibr" target="#b6">[7]</ref> 28.9 42.5  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our Data Diversification with</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">IWSLT translation tasks</head><p>Setup. We evaluate our approach in IWSLT'14 English-German (En-De) and German-English (De-En), IWSLT'13 English-French (En-Fr) and French-English (Fr-En) translation tasks. The IWSLT'14 En-De training set contains about 160K sentence pairs. We randomly sample 5% of the training data for validation and combine multiple test sets IWSLT14.TED.{dev2010, dev2012, tst2010, tst1011, tst2012} for testing. The IWSLT'13 En-Fr dataset has about 200K training sentence pairs. We use the IWSLT15.TED.tst2012 set for validation and the IWSLT15.TED.tst2013 set for testing. We use BPE for all four tasks. We compare our approach against two baselines that do not use our data diversification: Transformer <ref type="bibr" target="#b27">[28]</ref> and Dynamic Convolution <ref type="bibr" target="#b31">[32]</ref>. <ref type="table" target="#tab_3">Table 3</ref> we see that our method substantially and consistently boosts the performance in all the four translation tasks. In the En-De task, our method achieves up to 30.6 BLEU, which is 2 BLEU above the Transformer baseline. Similar trend can also be seen in the remaining De-En, En-Fr, Fr-en tasks. The results also show that our method is agnostic to model architecture, with both the Transformer and Dynamic Convolution achieving high gains. In contrary, other methods like knowledge distillation and multi-agent dual learning show minimal improvements on these tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results. From</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Low-resource translation tasks</head><p>Having demonstrated the effectiveness of our approach in high-resource languages like English, German and French, we now evaluate our approach performs on low-resource languages. For this, we use the English-Nepali and English-Sinhala low-resource setup proposed by Guzmán et al. <ref type="bibr" target="#b10">[11]</ref>. Both Nepali (Ne) and Sinhala (Si) are challenging domains since the data sources are particularly scarce and the vocabularies and grammars are vastly different from high-resource language like English.</p><p>Setup. We evaluate our method on the supervised setup of the four low-resource translation tasks: En-Ne, Ne-En, En-Si, and Si-En. We compare our approach against the baseline in Guzmán et al. <ref type="bibr" target="#b10">[11]</ref>. The English-Nepali and English-Sinhala parallel datasets contain about 500K and 400K sentence pairs respectively. We replicate the same setup as done by Guzmán et al. <ref type="bibr" target="#b10">[11]</ref> and use their dev set for development and devtest set for testing. We use k = 3 in our data diversification experiments. Results. From the results in <ref type="table" target="#tab_4">Table 4</ref>, we can notice that our method consistently improves the performance by more than 1 BLEU in all four tested tasks. Specifically, the method achieves 5.7, 8.9, 2.2, and 8.2 BLEU for En-Ne, Ne-En, En-Si and Si-En tasks, respectively. In absolute terms, these are 1.4, 1.3, 2.2 and 1.5 BLEU improvements over the baseline model <ref type="bibr" target="#b10">[11]</ref>. Without any monolingual data involved, our method establishes a new state of the art in all four low-resource tasks.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Understanding data diversification</head><p>We propose several logical hypotheses to explain why and how data diversification works as well as provide a deeper insight to its mechanism. We conduct a series of experimental analysis to confirm or reject such hypotheses. As a result, certain hypotheses are confirmed by the experiments, while some others, though being intuitive, are experimentally rejected. In this section, we explain the hypotheses that are empirically verified, while we elaborate the failed hypotheses in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Ensemble effects</head><p>Hypothesis Data diversification exhibits a strong correlation with ensemble of models.</p><p>Experiments To show this, we perform inference with an ensemble of seven (7) models and compare its performance with ours. We evaluate this setup on the WMT'14 En-De, and IWSLT'14 En-De, De-En, IWSLT'13 En-Fr and Fr-En translation tasks. The results are reported in <ref type="table" target="#tab_5">Table 5</ref>. We notice that the ensemble of models outdoes the single-model baseline by 1.3 BLEU in WMT'14 and 1.0-2.0 BLEU in IWSLT tasks. These results are particularly comparable to those achieved by our technique. This suggests that our method may exhibit an ensembling effect. However, note that an ensemble of models has a major drawback that it requires N (7 in this case) times more computations and parameters to perform inference. In contrary, our method does not have this disadvantage.</p><p>Explanation Intuitively, different models (initialized with different random seeds) trained on the original dataset converge to different local optima. As such, individual models tend to have high variance. Ensembles of models are known to help reduce variance, thus improves the performance. Formally, suppose a single-model M i ∈ {M 1 , ..., M N } estimates a model distribution p Mi , which is close to the data generating distribution p data . An ensemble of models averages multiple p Mi (for i = 1, . . . , N ), which leads to a model distribution that is closer to p data and improves generalization.</p><p>Our strategy may achieve the same effect by forcing a single-modelM to learn from the original data distribution p data as well as multiple synthetic distributions D i ∼ p Mi for i = 1, . . . , N , simultaneously. Following Jensen's Inequality <ref type="bibr" target="#b12">[13]</ref>, our method optimizes the upper bound:</p><formula xml:id="formula_5">E Y ∼U (M1(X),...,M N (X)),X∼p data log pM (Y |X) ≤ j log[ 1 N N i pM (y i j |y i &lt;j , X)]<label>(2)</label></formula><p>where U is uniform sampling, y i j = argmax yj p Mi (y j |y &lt;j , X). Let max y k j 1 N N i p Mi (y k j |y &lt;j , X) be the token-level probability of an ensemble of models M i and V be the vocabulary. Experimentally, we observe that the final modelM tends to outperform when the following condition is met:  Condition 3 can be met naturally at the beginning of the training process, but is not guaranteed at the end. We provide further analysis and supporting experiments in the Appendix.</p><formula xml:id="formula_6">E X∼p data 1 N N i pM (y i j |y i &lt;j , X) ≤ E X∼p data max y k j 1 N N i p Mi (y k j |y &lt;j , X) with y k j ∈ V (3)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Perplexity vs. BLEU score</head><p>Hypothesis Data diversification sacrifices perplexity for better BLEU score.</p><p>Experiments We tested this hypothesis as follows. We recorded the validation perplexity when the models fully converge for the baseline setup and for our data diversification method. We report the results in <ref type="figure" target="#fig_0">Figure 1a</ref>  Explanation Common wisdom tells that the lower perplexity often leads to better BLEU scores. In fact, our NMT models are trained to minimize perplexity (equivalently, cross entropy loss). However, existing research <ref type="bibr" target="#b26">[27]</ref> also suggests that sometimes sacrificing perplexity may result in better generalization and performance. As shown in <ref type="figure" target="#fig_0">Figure 1a</ref>, our models consistently show higher perplexity compared to the baseline in all the tasks, though we did not have intention to do so. As a result, the BLEU score is also consistently higher than the baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Initial parameters vs. diversity</head><p>Hypothesis Models with different initial parameters increase diversity in the augmented data, while the ones with fixed initial parameters decrease it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments and Explanation</head><p>With the intuition that diversity in training data improves translation quality, we speculated that the initialization of model parameters plays a crucial role in data diversification. Since neural networks are susceptible to initialization, it is possible that different initial parameters may lead the models to different convergence paths <ref type="bibr" target="#b8">[9]</ref> and thus different model distributions, while models with the same initialization are more likely to converge in similar paths. To verify this, we did an experiment with initializing all the constituent models (M i S→T,n , M i T →S,n ,M S→T ) with the same initial parameters to suppress data diversity. We conducted this experiment on the IWSLT'14 English-German and German-English tasks. We used a diversification factor of k = 1 only in this case. The results are shown in <ref type="figure" target="#fig_1">Figure 1b</ref>. Apparently, the BLEU scores of the fixed (same) initialization drop compared to the randomized counterpart in both language pairs. However, its performance is still significantly higher than the single-model baseline. This suggests that initialization is not the only contributing factor to diversity. Indeed, even though we are using the same initial checkpoint, each constituent model is trained on a different dataset and and learns to estimate a different distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Forward-translation is important</head><p>Hypothesis Forward-translation is as vital as back-translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments and Explanation</head><p>We separate our method into forward and backward diversification, in which we only train the final model (M S→T ) with the original data augmented by either the translations of the forward models (M i S→T,n ) or those of the backward models (M i T →S,n ) separately. We compare those variants with the bidirectionally diversified model and the single-model baseline. As shown in <ref type="table" target="#tab_6">Table 6</ref>, the forward and backward methods perform worse than the bidirectional counterpart but still better than the baseline. However, it is worth noting that diversification with forward models outperforms the one with backward models, as recent research has focused mainly on back-translation where only backward models are used <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b3">4]</ref>. Our finding is similar to Zhang and Zong <ref type="bibr" target="#b32">[33]</ref>, where the authors used source-side monolingual data to improve BLEU score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Unaffected by the translationese effect</head><p>Data diversification is built on the foundation of back-translation <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b3">4]</ref>. However, in a recent work, Edunov et al. <ref type="bibr" target="#b4">[5]</ref> point out that back-translation suffers from the translationese effect <ref type="bibr" target="#b28">[29]</ref>, where back-translation only improves the performance when the source sentences are translationese but does not offer any improvement when the sentences are natural text. <ref type="bibr" target="#b6">7</ref> In other words, back-translation can be ineffective in practice because our goal is to translate natural text, not translated text. Thus, it is important to test whether our data diversification method also suffers from this effect.</p><p>To verify this, we measure the BLEU improvements of data diversification over the baseline <ref type="bibr" target="#b17">[18]</ref> in the WMT'14 English-German setup ( §4.1) in all 3 scenarios laid out by Edunov et al. <ref type="bibr" target="#b4">[5]</ref>: (i) natural source → translationese target (X → Y * ), (ii) translationese source → natural target (X * → Y ), and (iii) translationese of translationese of source to translationese of target (X * * → Y * ). We use the same test sets provided by Edunov et al. <ref type="bibr" target="#b4">[5]</ref> to conduct this experiment. <ref type="bibr" target="#b7">8</ref> As shown in <ref type="table" target="#tab_7">Table 7</ref>, out method consistently outperforms the baseline in all three scenarios while Edunov et al. <ref type="bibr" target="#b4">[5]</ref> show that back-translation <ref type="bibr" target="#b20">[21]</ref> improves only in the X * → Y scenario. Thus, our method is not effected by the translationese effect. Our explanation for this is that the mentioned back-translation technique <ref type="bibr" target="#b20">[21]</ref> is a semi-supervised setup that uses extra natural monolingual data in the target. In our method, however, back-translation is conducted on the translationese part (target side) of the parallel data, and does not enjoy the introduction of extra natural text, which only benefits the X * → Y scenario. Otherwise speaking, back-translation with and without monolingual data are two distinct setups that should not be confused or identically interpreted. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Study on hyperparameters and back-translation</head><p>Effect of different k and N We first conduct experiments with the two hyper-parameters in our method -the diversification factor k and the number of rounds N , to investigate how they affect the performance. Particularly, we test the effect of different N on the IWSLT'14 En-De and De-En tasks, while the effect of different k is tested on the WMT'14 En-De task. As shown in <ref type="table" target="#tab_8">Table 8</ref>, increasing N improves the performance but the gain margin is insignificant. Meanwhile, <ref type="table" target="#tab_9">Table 9</ref> shows that increasing k significantly improves the performance until a specific saturation point. Note that increasing N costs more than increasing k while its gain may not be significant.</p><p>Complementary to back-translation Our method is also complementary to back-translation (BT) <ref type="bibr" target="#b20">[21]</ref>. To demonstrate this, we conducted experiments on the IWSLT'14 En-De and De-En tasks with <ref type="bibr" target="#b6">7</ref> Translationese refers to the unique characteristics of translated text (e.g., simplification, explicitation, normalization) compared to text originally written in a given language. This happens because when translating a text, translators usually have to make trade-offs between fidelity (to the source) and fluency (in the target). <ref type="bibr" target="#b7">8</ref> Note that our setup uses the WMT'14 training set while Edunov et al. <ref type="bibr" target="#b4">[5]</ref> use the WMT'18 training set.   <ref type="table" target="#tab_0">Table  10</ref>, using back-translation improves the baseline performance significantly. However, using our data diversification strategy with such monolingual data boosts the performance further with additional +1.0 BLEU over the back-translation baselines. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We have proposed a simple yet effective method to improve translation performance in many standard machine translation tasks. The approach achieves state-of-the-art in the WMT'14 English-German translation task with 30.7 BLEU. It also improves in IWSLT'14 English-German, German-English, IWSLT'13 English-French and French-English tasks by 1.0-2.0 BLEU. Furthermore, it outperforms the baselines in the low-resource tasks: English-Nepali, Nepali-English, English-Sinhala, Sinhala-English. Our experimental analysis reveals that our approach exhibits a strong correlation with ensembles of models. It also trades perplexity off for better BLEU score. We have also shown that the method is complementary to back-translation with extra monolingual data as it improves the back-translation performance significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>Our work has a potential positive impact on the application of machine translation in a variety of languages. It helps boost performance in both high-and low-resource languages. The method is simple to implement and applicable to virtually all existing machine translation systems. Future commercial and humanitarian translation services can benefit from our work and bring knowledge of one language to another, especially for uncommon language speakers such as Nepalese and Sri Lankan. On the other hand, our work needs to train multiple models, which requires more computational power or longer time to train the final model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>In the following supplementary materials, we discuss the other hypotheses that are not supported by the experiments. After that, we present mathematically our data diversification method is correlated to ensembles of models. Finally, we describe the training setup for our back-translation experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Further comparison</head><p>We continue to differentiate our method from other existing works. First, Shen et al. <ref type="bibr" target="#b23">[24]</ref> also seek to generate diverse set of translations using mixture of experts, not to improve translation quality like ours. In this method, multiple experts are tied into a single NMT model to be trained to generate diverse translations through EM optimization. It does not employ data augmentation, neither forward nor backward translations. Our method does not train multiple peer models with EM training either.</p><p>Second, iterative back-translation <ref type="bibr" target="#b11">[12]</ref> employs back-translation to augment the data in multiple rounds. In each round, a forward (or backward) model takes turn to play the "back-translation" role to train the backward (or forward) model. The role is switched in the next round. In our approach, both directions are involved in each round and multiple models to the achieve ensembling effect.</p><p>Third, Currey et al. <ref type="bibr" target="#b1">[2]</ref> propose to generate bitext from the target-side monolingual data by just copying the target sentence into the source sentence. In other words, source and target are identical. Our method does not use copying practice nor any extra monolingual data. We use the Transformer <ref type="bibr" target="#b27">[28]</ref> as our NMT model and follow the same configurations as suggested by Ott et al. <ref type="bibr" target="#b17">[18]</ref>. The model has 6 layers, each of which has model dimension d model = 1024, feed-forward dimension d f f n = 4096, and 16 attention heads. Adam optimizer <ref type="bibr" target="#b14">[15]</ref> was used with the similar learning rate schedule as Ott et al. <ref type="bibr" target="#b17">[18]</ref> -0.001 learning rate, 4,000 warm-up steps, and a batch size of 450K tokens. We use a dropout rate of 0.3 for En-De and 0.1 for En-Fr. We train the models for 45,000 updates. The data generation process costs 30% the time to train the baseline.</p><p>For data diversification, we use a diversification factor k = 3 for En-De and k = 2 for En-Fr. When augmenting the datasets, we filter out the duplicate pairs, which results in training datasets of 27M and 136M sentence pairs for En-De and En-Fr, respectively. <ref type="bibr" target="#b8">9</ref> Note that we do not use any extra monolingual data. For inference, we average the last 5 checkpoints of the final model and use a beam size of 5 and a length penalty of 0.6. We measure the performance in standard tokenized BLEU.</p><p>IWSLT setup. We also show the effectiveness of our approach in IWSLT'14 English-German (En-De) and German-English (De-En), IWSLT'13 English-French (En-Fr) and French-English (Fr-En) translation tasks. The IWSLT'14 En-De training set contains about 160K sentence pairs. We randomly sample 5% of the training data for validation and combine multiple test sets IWSLT14.TED.{dev2010, dev2012, tst2010, tst1011, tst2012} for testing. The IWSLT'13 En-Fr dataset has about 200K training sentence pairs. We use the IWSLT15.TED.tst2012 set for validation and the IWSLT15.TED.tst2013 set for testing. We use BPE for all four tasks. This results in a shared vocabulary of 10,000 tokens for English-German pair and 32,000 tokens for English-French pair.</p><p>We compare our approach against two baselines that do not use our data diversification: Transformer <ref type="bibr" target="#b27">[28]</ref> and Dynamic Convolution <ref type="bibr" target="#b31">[32]</ref>. In order to make a fair comparison, for the baselines and our approach, we use the base setup of the Transformer model. Specifically, the models have 6 layers, each with model dimensions d model = 512, feed-forward dimensions d f f n = 1024, and 4 attention heads. We use a dropout of 0.3 for all our IWSLT experiments. The models are trained for 500K updates and selected based on the validation loss. Note that we do not perform checkpoint averaging for these tasks, rather we run the experiments for 5 times with different random seeds and report the mean BLEU scores to provide more consistent and stable results. For inference, we use a beam size of 5, a length penalty of 1.0 for En-De, 0.2 for En-Fr, and 2.5 for Fr-En pair.</p><p>Low-resource setup. We evaluate our data diversification strategy on the supervised setups of the four low-resource translation tasks: En-Ne, Ne-En, En-Si, and Si-En. We compare our approach against the baseline proposed in <ref type="bibr" target="#b10">[11]</ref>. The English-Nepali parallel dataset contains about 500K sentence pairs, while the English-Sinhala dataset has about 400K pairs. We use the provided dev set for development and devtest set for testing.</p><p>In terms of training parameters, we replicate the same setup as done by Guzmán et al. <ref type="bibr" target="#b10">[11]</ref>. Specifically, we use the base Transformer model with 5 layers, each of which has 2 attention heads, d model = 512, d f f n = 2048. We use a dropout rate of 0.4, label smoothing of 0.2, weight decay of 10 −4 . We train the models for 100 epochs with batch size of 16,000 tokens. We select the inference models and length penalty based on the validation loss. The Nepali and Sinhala corpora are tokenized using the Indic NLP library. <ref type="bibr" target="#b9">10</ref> We reuse the provided shared vocabulary of 5000 tokens built by BPE learned with the sentencepiece library. <ref type="bibr" target="#b10">11</ref> For inference, we use beam search with a beam size of 5, and a length penalty of 1.2 for Ne-En and Si-En tasks, 0.9 for En-Ne and 0.5 for En-Si. We report tokenized BLEU for from-English tasks and detokenized SacredBLEU <ref type="bibr" target="#b19">[20]</ref> for to-English tasks. We use k = 3 in our data diversification experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Diversity analysis</head><p>As mentioned, by training multiple models with different random seeds, the generated translations from the training set yield only 14% and 22% duplicates for En-De and En-Fr, respectively. These results may be surprising as we might expect more duplicates. Therefore, we performed further diversity analysis.</p><p>To evaluate the diversity the teacher models brought in data diversification, we compare them using the BLEU/Pairwise-BLEU benchmark proposed by Shen et al. <ref type="bibr" target="#b23">[24]</ref>. This benchmark measures the diversity and quality of multiple hypotheses, generated by multiple models or a mixture of experts, given a source sentence. Specifically, we use our forward models trained on WMT'14 English-German and English-French and measure the BLEU and Pairwise-BLEU scores in the provided test set <ref type="bibr" target="#b23">[24]</ref>. The results are reported in <ref type="table" target="#tab_0">Table 11</ref>, where we compare the diversity and quality of our method against the mixture of experts method provided by Shen et al. <ref type="bibr" target="#b23">[24]</ref> (hMup) and other baselines, such as sampling <ref type="bibr" target="#b9">[10]</ref>, diverse bearm search <ref type="bibr" target="#b15">[16]</ref>. All methods, ours and the baselines, use the big Transformer model.</p><p>As it can be seen in En-De experiments, our method is less diverse than the mixture of experts (hMup) <ref type="bibr" target="#b23">[24]</ref> and diverse beam search <ref type="bibr" target="#b15">[16]</ref> (57.1 versus 50.2 and 53.7 Pairwise-BLEU). However, translations of our method are of better quality (69.5 BLEU), which is very close to human performance. Meanwhile, our method achieve similar quality to beam, but yields better diversity than this approach. The same conclusion can be derived from the WMT'14 English-French experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Failed Hypotheses</head><p>In addition to the successful hypotheses that we described in the paper, we speculated other possible hypotheses that were eventually not supported by the experiments despite being intuitive. We present them in this section for better understanding of the approach.</p><p>Effects of Dropout. First, given that parameter initialization affects diversity, it is logical to assume that Dropout will magnify the diversification effects. In other words, we expected that removing  dropout would result in less performance boost offered by our method than when dropout is enabled. However, our empirical results did not support this.</p><p>We ran experiments to test whether non-zero dropout magnify the improvements of our method over the baseline. We trained the single-model baseline and our data diversification's teacher models (both backward, forward models) as well as the student model in cases of dropout = 0.3 and dropout = 0.0 in the IWSLT'14 English-German and German-English tasks. We used factor k = 1 in these experiments. As reported in <ref type="table" target="#tab_0">Table 12</ref>, the no-dropout versions perform much worse than the non-zero dropout versions in all experiments. However, the gains made by our data diversification with dropout are not particularly higher than the non-dropout counterpart. This suggests that dropout may not contribute to the diversity of the synthetic data.</p><p>Effects of Beam Search. We hypothesized that beam search would generate more diverse synthetic translations of the original dataset, thus increases the diversity and improves generalization. We tested this hypothesis by using greedy decoding (beam size = 1) to generate the synthetic data and compare its performance against beam search (beam size = 5) counterparts. We again used the IWSLT'14 English-German and German-English as a testbed. Note that for testing with the final model, we used the same beam search (beam size = 5) procedure for both cases. As shown in <ref type="table" target="#tab_0">Table 13</ref>, the performance of greedy decoding is not particularly reduced compared to the beam search versions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Correlation with Ensembling</head><p>In this section, we show that our data diversification method is optimizing its model distribution to be close to the ensemble distribution under the condition that the final model is randomly initialized. We also show that such condition can be easily met in our experiments. Specifically, let the constituent models of an ensemble of models be M i ∈ {M 1 , ..., M N } and each model M i produces a model distribution probability p Mi (Y |X) (for i = 1, . . . , N ) of the true probability P (Y |X), X and Y = (y 1 , ..., y m ) be the source and target sentences sampled from the data generating distribution p data ,M be the final model in our data diversification strategy. In addition, without loss of generality, we assume that the final modelM is only trained on the data generated by forward models M i S→T,r . We also have token-level probability P (y j |y &lt;j , X) such that:</p><formula xml:id="formula_7">log P (Y |X) = j log P (y j |y &lt;j , X)<label>(4)</label></formula><p>For the sake of brevity, we omit X in P (y j |y &lt;j , X) and use only P (y j |y &lt;j ) in the remaining of description. With this, our data diversification method maximizes the following expectation:</p><formula xml:id="formula_8">E Y ∼U (M1(X),...,M N (X)),X∼p data log pM (Y |X) = E Y ∼Yc j log pM (y j |y &lt;j ) = Y ∼Yc 1 N j log pM (y j |y &lt;j ) = j 1 N Y ∼Yc log pM (y j |y &lt;j )<label>(5)</label></formula><p>where U denotes uniform sampling, Y c = U (M 1 (X), ..., M N (X)). According to Jensen's Inequality <ref type="bibr" target="#b12">[13]</ref>, we have: </p><p>where y k j ∈ V and V is the vocabulary. In addition, with the above notations, by maximizing the expectation 5, we expect our method to automatically push the final model distribution pM , with the term 1 Through experiments we will discuss later, we observe that our method is able to achieve high performance gain under the following conditions:</p><p>• Both sides of Eqn. 7 are tight, meaning they are almost equal. This can be realized when the teacher models are well-trained from the parallel data.</p><p>• The following inequality needs to be maintained and the training process should stop when the inequality no longer holds:</p><formula xml:id="formula_10">E 1 N N i pM (y i j |y i &lt;j ) ≤ E max y k j 1 N N i p Mi (y k j |y &lt;j ) ≤ E 1 N N i p Mi (y i j |y i &lt;j ) ≤ 1 (8)</formula><p>The left-side equality of condition 8 happens when all y i j are identical ∀i ∈ {1, ..., N } and y i j = y e j ∀y j . In the scenario of our experiments, condition 8 is easy to be met. First, when the constituent models M i are well-trained (but not overfitted), the confidence of the models is also high. This results in the expectation E max y k j 1 N N i p Mi (y k j |y &lt;j ) and E 1 N N i p Mi (y i j |y i &lt;j ) comparably close to 1.0, compared to uniform probability of 1/|V | with V is the target vocabulary and |V | N . To test this condition, we empirically compute the average values of both terms over the IWSLT'14 English-German and IWSLT'13 English-French tasks. As reported in <ref type="table" target="#tab_0">Table 14</ref>, the average probability E max y k j 1 N N i p Mi (y k j |y &lt;j ) is around 0.74-0.77, while the average probability E 1 N N i p Mi (y i j |y i &lt;j ) is always higher but close to the former term (0.76-0.79). Both of these terms are much larger than 1/|V | = 3 × 10 −5 .</p><p>Second, as modelM is randomly initialized, it is logical that E 1 N N i pM (y i j |y i &lt;j ) ≈ 1/|V |. Thus, under our experimental setup, it is likely that condition 8 is met. However, the condition can be broken when the final modelM is trained until it overfits on the augmented data. This results in 1 N N i pM (y i j |y i &lt;j ) ≤ E max y k j 1 N N i p Mi (y k j ) . This scenario is possible because the modelM is trained on many series of tokens y i j with absolute confidence of 1, despite the fact that model M i produces y i j with a relative confidence p Mi (y i j |y i &lt;j ) &lt; 1. In other words, our method does not restrict the confidence of the final modelM for the synthetic data up to the confidence of their teachers M i . Breaking this condition may cause a performance drop. As demonstrated in <ref type="table" target="#tab_0">Table 14</ref>, the performances significantly drop as we overfit the diversified modelM so that it is more confident in the predictions of M i than M i themselves (i.e., 0.82 versus 0.76 probability for En-De). Therefore, it is recommended to maintain a final model's confidence on the synthetic data as high and close to, but not higher than the teacher models' confidence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Details on Back-translation experiments</head><p>In this section, we describe the complete training setup for the back-translation experiments presented in Section 6. First, for the back-translation baselines <ref type="bibr" target="#b20">[21]</ref>, we train the backward models following the same setup for the baseline Transformer presented in Section 4.1 for the WMT'14 En-De experiment and Section 4.2 for IWSLT'14 De-En and En-De experiments. For IWSLT experiments, we use the target-side corpora (En and De) from the WMT'14 English-German dataset to augment the IWSLT De-En and En-De tasks. We use the BPE code built from the original parallel data to transform these monolingual corpora into BPE subwords. This results in a total dataset of 4.66M sentence pairs, which is 29 times larger than the original IWSLT datasets. For the final model trained on the augmented data, we use the big Transformer with the same hyper-parameters as described in Section 4.1. However, note that we use the same shared vocabulary from the baseline setup for the back-translation experiments. On the other hand, for the WMT'14 English-German experiment, we use the German corpus derived from News Crawl 2009, which contains 6.4M sentences. Similarly, we use the same BPE code and shared vocabulary built from the parallel data to transform and encode this monolingual corpus. The process produces a total dataset of 10.9M sentence pairs, which is 2.4 times larger than the original dataset. We use the big Transformer setup for all the WMT experiments.</p><p>Second, in terms of back-translation models trained with our data diversification strategy, we use the existing k backward models to generate k diverse sets of source sentences from the provided monolingual data. After that, we combine these datasets with the diverse dataset built by our method from the original parallel data. Then, we train the final model on this dataset. In addition, model setups and training parameters are identical to those used for the back-translation baselines.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>( a )</head><label>a</label><figDesc>Validation perplexity vs BLEU scores. (b) Random vs fixed parameters initialization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Relationship between validation perplexity vs the BLEU scores (1a) and the effects of random initialization (1b) in the IWSLT En-De, De-En, En-Fr, Fr-En and WMT'14 En-De tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>for WMT'14 En-De, IWSLT'14 En-De, De-En, IWSLT'13 En-Fr and Fr-En tasks. The left axis of the figure shows Perplexity (PPL) values for the models, which compares the dark blue (baseline) and red (our) bars. Meanwhile, the right axis shows the respective BLEU scores for the models as reflected by the faded bars.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>p</head><label></label><figDesc>j = argmax yj p Mi (y j |y &lt;j ). Meanwhile, we define max y k j Mi (y k j ) as the averaged probability for output token y e j = argmax yj 1 N N i p Mi (y j ) of an ensemble of models. Then, since the maximum function is convex, we have the following inequality: Mi (y i j |y i &lt;j )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>i</head><label></label><figDesc>pM (y i j |y i &lt;j ) close to the average model distribution of 1 N N i p Mi (y i j |y i &lt;j (the right-hand side of Eqn. 7).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Estimated method comparison. |Θ| denotes the number of parameters, while |D| denotes the size of actual training data required.</figDesc><table><row><cell>Method</cell><cell cols="2">Training</cell><cell></cell><cell cols="2">Inference</cell></row><row><cell></cell><cell>FLOPs</cell><cell>|Θ|</cell><cell>|D|</cell><cell cols="2">FLOPs |Θ|</cell></row><row><cell></cell><cell cols="2">New Architectures</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Transformer</cell><cell>1×</cell><cell>1×</cell><cell>1×</cell><cell>1×</cell><cell>1×</cell></row><row><cell>Dynamic Conv</cell><cell>1×</cell><cell>1×</cell><cell>1×</cell><cell>1×</cell><cell>1×</cell></row><row><cell></cell><cell cols="2">Semi-supervised</cell><cell></cell><cell></cell><cell></cell></row><row><cell>NMT+BERT</cell><cell>&gt; 60×</cell><cell cols="2">3× &gt; 25×</cell><cell>3×</cell><cell>3×</cell></row><row><cell>Back-translation</cell><cell>2×</cell><cell cols="2">1× &gt; 50×</cell><cell>1×</cell><cell>1×</cell></row><row><cell></cell><cell cols="2">Evolution-based</cell><cell></cell><cell></cell><cell></cell></row><row><cell>So et al. [25]</cell><cell cols="2">&gt; 15000× 1×</cell><cell>1×</cell><cell>1×</cell><cell>1×</cell></row><row><cell></cell><cell cols="3">Our Data Diversification</cell><cell></cell><cell></cell></row><row><cell>Default Setup</cell><cell>7×</cell><cell>1×</cell><cell>1×</cell><cell>1×</cell><cell>1×</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>BLEU scores on newstest2014 for WMT'14 English-German (En-De) and English-French (En-Fr) translation tasks. Distill (T&gt;S) (resp. T=S) indicates the teacher model is larger than (resp. equal to) the student model.</figDesc><table><row><cell>Method</cell><cell cols="2">WMT'14</cell></row><row><cell></cell><cell cols="2">En-De En-Fr</cell></row><row><cell>Transformer [28]  †</cell><cell cols="2">28.4 41.8</cell></row><row><cell>Trans+Rel. Pos [23]  †</cell><cell cols="2">29.2 41.5</cell></row><row><cell cols="3">Scale Transformer [18] 29.3 42.7 6</cell></row><row><cell>Dynamic Conv [32]  †</cell><cell cols="2">29.7 43.2</cell></row><row><cell>Transformer with</cell><cell></cell><cell></cell></row><row><cell>Multi-Agent [31]  †</cell><cell>30.0</cell><cell>-</cell></row><row><cell>Distill (T&gt;S)</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>BLEU scores on IWSLT'14 English-German (En-De), German-English (De-En), and IWSLT'13 English-French (En-Fr) and French-English (Fr-En) translation tasks. Superscript † denotes the numbers are reported from the paper, others are based on our runs. Dynamic Conv 30.6 37.2 45.2 44.9</figDesc><table><row><cell>Method</cell><cell>IWSLT'14</cell><cell cols="2">IWSLT'13</cell></row><row><cell></cell><cell cols="3">En-De De-En En-Fr Fr-En</cell></row><row><cell>Baselines</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Transformer</cell><cell cols="3">28.6 34.7 44.0 43.3</cell></row><row><cell cols="4">Dynamic Conv 28.7 35.0 43.8 43.5</cell></row><row><cell cols="2">Transformer with</cell><cell></cell><cell></cell></row><row><cell>Multi-Agent  †</cell><cell>28.9 34.7</cell><cell>-</cell><cell>-</cell></row><row><cell>Distill (T&gt;S)</cell><cell cols="3">28.0 33.6 43.4 42.9</cell></row><row><cell>Distill (T=S)</cell><cell cols="3">28.5 34.1 44.1 43.4</cell></row><row><cell>Ens-Distill</cell><cell cols="3">28.8 34.7 44.3 43.9</cell></row><row><cell cols="2">Our Data Diversification with</cell><cell></cell><cell></cell></row><row><cell>Transformer</cell><cell cols="3">30.6 37.0 45.5 45.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Performances on low-resource translations. As done by Guzmán et al.<ref type="bibr" target="#b10">[11]</ref>, the from-English pairs are measured in tokenized BLEU, while to-English are measured in detokenized SacreBLEU.</figDesc><table><row><cell>Method</cell><cell cols="4">En-Ne Ne-En En-Si Si-En</cell></row><row><cell>Guzmán et al. [11]</cell><cell>4.3</cell><cell>7.6</cell><cell>1.0</cell><cell>6.7</cell></row><row><cell>Data Diversification</cell><cell>5.7</cell><cell>8.9</cell><cell>2.2</cell><cell>8.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Diversification preserves the effects of ensembling, but does not change |Θ| and flops.</figDesc><table><row><cell></cell><cell>|Θ| IWSLT'14</cell><cell>IWSLT'13 WMT</cell></row><row><cell></cell><cell cols="2">flops En-De De-En En-Fr Fr-En En-De</cell></row><row><cell cols="3">Baseline 1x 28.6 34.7 44.0 43.3 29.3</cell></row><row><cell cols="3">Ensemble 7x 30.2 36.5 45.5 44.9 30.3</cell></row><row><cell>Ours</cell><cell cols="2">1x 30.6 37.0 45.5 45.0 30.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>BLEU scores for forward and backward diversification in comparison to bidirectional diversification and the baseline on IWSLT'14 En-De and De-En tasks.</figDesc><table><row><cell cols="4">Task Baseline Backward Forward Bidirectional</cell></row><row><cell>En-De 28.6</cell><cell>29.2</cell><cell>29.86</cell><cell>30.6</cell></row><row><cell>De-En 34.7</cell><cell>35.8</cell><cell>35.94</cell><cell>37.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>BLEU evaluation of our method and the baseline on the translationese effect<ref type="bibr" target="#b4">[5]</ref>, in the WMT'14 English-German setup.</figDesc><table><row><cell>Baseline [18]</cell><cell>31.35</cell><cell>28.47</cell><cell>38.59</cell></row><row><cell>Our method</cell><cell>33.47</cell><cell>30.38</cell><cell>41.03</cell></row></table><note>WMT'14 En-De X → Y* X* → Y X * * → Y*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>BLEU scores for different rounds N IWSLT'14 N = 1 N = 2 N = 3</figDesc><table><row><cell>En-De</cell><cell>30.4 30.6 30.6</cell></row><row><cell>De-En</cell><cell>36.8 36.9 37.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>BLEU scores for different factors k. WMT'14 k = 1 k = 2 k = 3 k = 4 k = 5 k = 6 En-De 29.8 30.1 30.7 30.7 30.7 30.6 extra monolingual data extracted from the WMT'14 En-De corpus. In addition, we also compare our method against the back-translation baseline in the WMT'14 En-De task with extra monolingual data from News Crawl 2009. We use the big Transformer as the final model in all our back-translation experiments. Further details of these experiments are provided in the Appendix. As reported in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>BLEU scores for models with and without back-translation (BT) on the IWSLT'14 English-German (En-De), German-English (De-En) and WMT'14 En-De tasks. Column |D| shows the total data used in back-translation compared to the original parallel data.</figDesc><table><row><cell>Task</cell><cell cols="2">No back-translation</cell><cell cols="3">With back-translation</cell></row><row><cell></cell><cell>Baseline</cell><cell>Ours</cell><cell>|D|</cell><cell cols="2">Baseline Ours</cell></row><row><cell>IWSLT'14 En-De</cell><cell>28.6</cell><cell>30.6</cell><cell>29×</cell><cell>30.0</cell><cell>31.8</cell></row><row><cell>IWSLT'14 De-En</cell><cell>34.7</cell><cell>37.0</cell><cell>29×</cell><cell>37.1</cell><cell>38.5</cell></row><row><cell>WMT'14 En-De</cell><cell>29.3</cell><cell>30.7</cell><cell>2.4×</cell><cell>30.8</cell><cell>31.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>We use newstest2013 as the development set, and newstest2014 for testing. Both tasks are considered as high-resource tasks as the amount of parallel training data is relatively large.</figDesc><table /><note>C Experimental setup detailsWMT'14 setup. We conduct experiments on the WMT'14 English-German (En-De) and English- French (En-Fr) translation tasks. The training datasets contain about 4.5 million and 35 million sentence pairs respectively. The sentences are encoded with Byte-Pair Encoding (BPE) [22] with 32,000 operations, which results in a shared-vocabulary of 32,768 tokens for En-De and 45,000 tokens for En-Fr.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 11 :</head><label>11</label><figDesc>WMT'14 English-German (En-De) and English-French (En-Fr) diversity performances in BLEU and Pairwise-BLEU scores, tested on the test set provided by Shen et al.<ref type="bibr" target="#b23">[24]</ref>. Lower Pairwise-BLEU means more diversity, higher BLEU means better quality.</figDesc><table><row><cell>Method</cell><cell cols="2">Pairwise-BLEU</cell><cell cols="2">BLEU</cell></row><row><cell></cell><cell cols="4">En-De En-Fr En-De En-Fr</cell></row><row><cell>Sampling</cell><cell>24.1</cell><cell>32.0</cell><cell>37.8</cell><cell>46.5</cell></row><row><cell>Beam</cell><cell>73.0</cell><cell>77.1</cell><cell>69.9</cell><cell>79.8</cell></row><row><cell>Div-beam</cell><cell>53.7</cell><cell>64.9</cell><cell>60.0</cell><cell>72.5</cell></row><row><cell>hMup</cell><cell>50.2</cell><cell>64.0</cell><cell>63.8</cell><cell>74.6</cell></row><row><cell>Human</cell><cell>35.5</cell><cell>46.5</cell><cell>69.6</cell><cell>76.9</cell></row><row><cell>Ours</cell><cell>57.1</cell><cell>70.1</cell><cell>69.5</cell><cell>77.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 12 :</head><label>12</label><figDesc>Improvements of data diversification under conditions with-and without-dropout in the IWSLT'14 English-German and German-English.</figDesc><table><row><cell>Task</cell><cell cols="2">Baseline Ours</cell><cell>Gain</cell></row><row><cell></cell><cell cols="2">Dropout = 0.3</cell></row><row><cell>En-De</cell><cell>28.6</cell><cell cols="2">30.1 +1.5 (5%)</cell></row><row><cell>De-En</cell><cell>34.7</cell><cell cols="2">36.5 +1.8 (5%)</cell></row><row><cell></cell><cell cols="2">Dropout = 0.0</cell></row><row><cell>En-De</cell><cell>25.7</cell><cell cols="2">27.5 +1.8 (6%)</cell></row><row><cell>De-En</cell><cell>30.7</cell><cell cols="2">32.5 +1.8 (5%)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 13 :</head><label>13</label><figDesc>Improvements of data diversification under conditions maximum likelihood and beam search in the IWSLT'14 English-German and German-English.</figDesc><table><row><cell>Task</cell><cell>Baseline</cell><cell>Ours</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Beam=1 Beam=5</cell></row><row><cell>En-De</cell><cell>28.6</cell><cell>30.3</cell><cell>30.4</cell></row><row><cell>De-En</cell><cell>34.7</cell><cell>36.6</cell><cell>36.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 14 :</head><label>14</label><figDesc>The average value of E 1 in the IWSLT'14 English-German, German-English, IWSLT'13 English-French and French-English tasks.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>N</cell><cell cols="3">N i p Mi (y i j |y i &lt;j ) , E max y k j</cell><cell>1 N</cell><cell>N i p Mi (y k j |y &lt;j ) and</cell></row><row><cell>E 1 N</cell><cell cols="7">N i pM (y i j |y i &lt;j ) En-De De-En En-Fr Fr-En</cell></row><row><cell></cell><cell cols="4">Teacher models M i</cell><cell></cell><cell></cell></row><row><cell></cell><cell>E 1 N</cell><cell cols="3">N i p Mi (y i j |y i &lt;j )</cell><cell></cell><cell>0.76</cell><cell>0.78</cell><cell>0.76</cell><cell>0.79</cell></row><row><cell></cell><cell cols="2">E max y k j</cell><cell>1 N</cell><cell cols="2">N i p Mi (y k j |y &lt;j )</cell><cell>0.75</cell><cell>0.76</cell><cell>0.74</cell><cell>0.77</cell></row><row><cell></cell><cell cols="3">Test BLEU</cell><cell></cell><cell></cell><cell>28.6</cell><cell>34.7</cell><cell>44.0</cell><cell>43.3</cell></row><row><cell></cell><cell cols="4">Diversified ModelM</cell><cell></cell><cell></cell></row><row><cell></cell><cell>E 1 N</cell><cell cols="3">N i pM (y i j |y i &lt;j )</cell><cell></cell><cell>0.74</cell><cell>0.74</cell><cell>0.73</cell><cell>0.75</cell></row><row><cell></cell><cell cols="3">Test BLEU</cell><cell></cell><cell></cell><cell>30.6</cell><cell>37.0</cell><cell>45.5</cell><cell>45.0</cell></row><row><cell></cell><cell cols="5">Overfitted Diversified ModelM</cell><cell></cell></row><row><cell></cell><cell>E 1 N</cell><cell cols="3">N i pM (y i j |y i &lt;j )</cell><cell></cell><cell>0.82</cell><cell>0.86</cell><cell>0.84</cell><cell>0.89</cell></row><row><cell></cell><cell cols="3">Test BLEU</cell><cell></cell><cell></cell><cell>28.6</cell><cell>34.8</cell><cell>43.8</cell><cell>43.3</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Code: https://github.com/nxphi47/data_diversification 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada. arXiv:1911.01986v4 [cs.CL] 4 Oct 2020</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">As of submission deadline, we report SOTA in the standard WMT'14 setup without monolingual data.<ref type="bibr" target="#b2">3</ref> Parameters |Θ| do not increase as we can discard the intermediate models after using them.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">There were 14% and 22% duplicates, respectively. We provide a diversity analysis in the Appendix.<ref type="bibr" target="#b4">5</ref> We could not reproduce the results reported by Wu et al.<ref type="bibr" target="#b31">[32]</ref> using their code. We only achieved 29.2 BLEU for this baseline, while our method applied to it gives 30.1 BLEU.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">Ott et al.<ref type="bibr" target="#b17">[18]</ref> reported 43.2 BLEU in En-Fr. However, we could achieve only 42.7 using their code, based on which our data diversification gives 1.0 BLEU gain.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">There were 14% and 22% duplicates, respectively. We provide a diversity analysis in the Appendix.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">https://github.com/anoopkunchukuttan/indic_nlp_library 11 https://github.com/google/sentencepiece</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Weighted transformer network for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karim</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno>abs/1711.02132</idno>
		<ptr target="http://arxiv.org/abs/1711.02132" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Copied monolingual data improves low-resource neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Currey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Valerio Miceli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Barone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heafield</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-4715</idno>
		<ptr target="https://www.aclweb.org/anthology/W17-4715" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Conference on Machine Translation</title>
		<meeting>the Second Conference on Machine Translation<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-09" />
			<biblScope unit="page" from="148" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
		<ptr target="https://www.aclweb.org/anthology/N19-1423" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Understanding back-translation at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1045</idno>
		<ptr target="https://www.aclweb.org/anthology/D18-1045" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-11" />
			<biblScope unit="page" from="489" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On the evaluation of machine translation systems trained with back-translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.253</idno>
		<ptr target="https://www.aclweb.org/anthology/2020.acl-main.253" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07" />
			<biblScope unit="page" from="2836" to="2846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Data augmentation for low-resource neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marzieh</forename><surname>Fadaee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arianna</forename><surname>Bisazza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-2090</idno>
		<ptr target="https://www.aclweb.org/anthology/P17-2090" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-07" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="567" to="573" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Ensemble distillation for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Freitag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Al-Onaizan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baskaran</forename><surname>Sankaran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.01802</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Born again neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommaso</forename><surname>Furlanello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v80/furlanello18a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<editor>Jennifer Dy and Andreas Krause</editor>
		<meeting>the 35th International Conference on Machine Learning<address><addrLine>Stockholmsmässan, Stockholm Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="10" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT Press</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Generating sequences with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.0850</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The FLORES evaluation datasets for lowresource machine translation: Nepali-English and Sinhala-English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng-Jen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Pino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1632</idno>
		<ptr target="https://www.aclweb.org/anthology/D19-1632" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11" />
			<biblScope unit="page" from="6098" to="6111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Iterative backtranslation for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duy</forename><surname>Vu Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Haffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohn</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-2703</idno>
		<ptr target="https://www.aclweb.org/anthology/W18-2703" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Neural Machine Translation and Generation</title>
		<meeting>the 2nd Workshop on Neural Machine Translation and Generation<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-07" />
			<biblScope unit="page" from="18" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Sur les fonctions convexes et les inégalités entre les valeurs moyennes. Acta mathematica</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan Ludwig William Valdemar</forename><surname>Jensen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1906" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="175" to="193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Sequence-level knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1139</idno>
		<ptr target="https://www.aclweb.org/anthology/D16-1139" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016-11" />
			<biblScope unit="page" from="1317" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.6980" />
	</analytic>
	<monogr>
		<title level="m">ICLR (Poster)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">A simple, fast diverse decoding algorithm for neural generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.08562</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Scaling neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation (WMT)</title>
		<meeting>the Third Conference on Machine Translation (WMT)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">When networks disagree: Ensemble methods for hybrid neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon N</forename><surname>Perrone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cooper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BROWN UNIV PROVIDENCE RI INST FOR BRAIN AND NEURAL SYSTEMS</title>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A call for clarity in reporting BLEU scores</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-6319</idno>
		<ptr target="https://www.aclweb.org/anthology/W18-6319" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation: Research Papers</title>
		<meeting>the Third Conference on Machine Translation: Research Papers<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-10" />
			<biblScope unit="page" from="186" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improving neural machine translation models with monolingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1009</idno>
		<ptr target="https://www.aclweb.org/anthology/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016-08" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="16" to="1009" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1162</idno>
		<ptr target="http://www.aclweb.org/anthology/P16-1162" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Self-attention with relative position representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-2074</idno>
		<ptr target="http://aclweb.org/anthology/N18-2074" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="464" to="468" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Mixture models for diverse machine translation: Tricks of the trade</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianxiao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The evolved transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Liang</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v97/so19a.html" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="5877" to="5886" />
			<date type="published" when="2019-06-15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><forename type="middle">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Decoding with largescale neural language models improves translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinggong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victoria</forename><surname>Fossum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/D13-1140" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="1387" to="1392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">On the features of translationese. Digital Scholarship in the Humanities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vered</forename><surname>Volansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Ordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuly</forename><surname>Wintner</surname></persName>
		</author>
		<idno type="DOI">10.1093/llc/fqt031</idno>
		<ptr target="https://doi.org/10.1093/llc/fqt031" />
		<imprint>
			<date type="published" when="2013-07" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="98" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">SwitchOut: an efficient data augmentation algorithm for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1100</idno>
		<ptr target="https://www.aclweb.org/anthology/D18-1100" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-11" />
			<biblScope unit="page" from="856" to="861" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multi-agent dual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiren</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HyGhN2A5tm" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pay less attention with lightweight and dynamic convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SkVhlh09tX" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Exploiting source-side monolingual data in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1160</idno>
		<ptr target="https://www.aclweb.org/anthology/D16-1160" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016-11" />
			<biblScope unit="page" from="1535" to="1545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Incorporating bert into neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhua</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieyan</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Hyl7ygStwB" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
