<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HorizonNet: Learning Room Layout with 1D Representation and Pano Stretch Data Augmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Tsing Hua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Wei</forename><surname>Hsiao</surname></persName>
							<email>chiweihsiao@gapp.nthu.edu.twsunmin@ee.nthu.edu.tw</email>
							<affiliation key="aff0">
								<orgName type="institution">National Tsing Hua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Tsing Hua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwann-Tzong</forename><surname>Chen</surname></persName>
							<email>htchen@cs.nthu.edu.tw</email>
							<affiliation key="aff0">
								<orgName type="institution">National Tsing Hua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">HorizonNet: Learning Room Layout with 1D Representation and Pano Stretch Data Augmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Figure 1: Some examples of 3D reconstructed room layouts by our HorizonNet.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a new approach to the problem of estimating the 3D room layout from a single panoramic image. We represent room layout as three 1D vectors that encode, at each image column, the boundary positions of floor-wall and ceiling-wall, and the existence of wall-wall boundary. The proposed network, HorizonNet, trained for predicting 1D layout, outperforms previous state-of-the-art approaches. The designed post-processing procedure for recovering 3D room layouts from 1D predictions can automatically infer the room shape with low computation cost-it takes less than 20ms for a panorama image while prior works might need dozens of seconds. We also propose Pano Stretch Data Augmentation, which can diversify panorama data and be applied to other panorama-related learning tasks. Due to the limited data available for non-cuboid layout, we relabel 65 general layout from the current dataset for finetuning. Our approach shows good performance on general layouts by qualitative results and cross-validation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The goal of this work is to predict the room layout from a panoramic image. Most of the state-of-the-art methods solve this problem by adopting more effective deep network architectures for their models to learn from different cues in the image. Assumptions about the room structures are often made to constrain the solution space so that the predictions of the deep model would not deviate from the common cases too much. Post-processing steps can further be performed to refine the predictions. Given a number of images with annotated layouts for training, state-ofthe-art methods are able to achieve good results on the test data. However, acquiring high-quality room-layout annotations for panoramic images is labor-demanding. The annotations done by different people might be inconsistent due to ambiguities about the locations of wall boundaries, especially for well-decorated rooms. Moreover, currently available datasets do not include more images of complex room layouts. The annotation for a complex layout would just be approximated as a cuboid-shaped or L-shaped layout, introducing even more ambiguities for training and testing.</p><p>Two important and correlated issues may be further addressed for improving state-of-the-art methods. The first issue is the lack of more training and validation data with precise annotations. The second issue is that, without more annotated data for training, the deep networks cannot be too large, otherwise the test accuracy might be low due to overfitting. Collecting more data to train a more sophisticated model is indeed beneficial and doable, but a more efficient way to improve the performance should also be welcome. We argue that, if we have some better understanding of the problem and make good use of domain knowledge, we may improve the performance without acquiring a lot more annotated data or using a larger deep network. Data augmentation is a common procedure in deep learning to generate more data for training. Standard data augmentation heuristics such as random cropping or luminance change for image classification or object detection might not be effective for layout prediction. Our idea is to take account of the underlying geometric constraints and design a better data augmentation mechanism specifically for training layoutpredicting deep networks. On the other hand, instead of increasing the model complexity, we aim to enhance the model by devising a compact representation with respect to the geometric constraints. We can, therefore, remove redundant degrees of freedom and force the model to focus more on learning critical properties for layout prediction.</p><p>We characterize our contributions as follows:</p><p>• We introduce a 1D O(W ) representation that encodes the whole-room layout for a panoramic scene. Training with such a representation allows our method to outperform previous state-of-the-art results, yet requires fewer parameters and less computation time.</p><p>• We propose a data augmentation mechanism called Pano Stretch Data Augmentation, which generates panorama images on the fly during training and improves the accuracy under all settings in our experiments. This data augmentation mechanism also has the potential for boosting other tasks (e.g., semantic segmentation, object detection) that directly work on a panorama.</p><p>• We show that leveraging RNNs in a layout prediction task is helpful for improving the accuracy. RNNs are able to capture the long-range geometric pattern of room layouts.</p><p>• Owing to the 1D representation and our efficient postprocessing procedure, the computation cost of our model is very low, and the model can be easily extended to handle complex scenes with layouts other than cuboid-shaped or L-shaped.</p><p>Code and data are available at: https://sunset1995. github.io/HorizonNet/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Room layout estimation from a single-view RGB image is an active research topic over the past decade. Many approaches have been developed in this field. Most of them exploit the Manhattan world assumption that the room layouts, and even the furniture, are aligned with the three principal axes <ref type="bibr" target="#b2">[3]</ref>. The Manhattan world assumption imposes constraints on the layout estimation problem, and, based on the assumption, the Manhattan aligned vanishing points could also be used to rectify the image and extract features for inferring the layout.</p><p>Delage et al. <ref type="bibr" target="#b5">[6]</ref> train a dynamic Bayesian network to recognize the floor-wall boundary in each column of the perspective image. Many approaches search the Manhattan aligned layout based on extracted geometric cues. Lee et al. <ref type="bibr" target="#b17">[18]</ref> test the hypothesis using Orientation Map (OM) while Hedau et al. <ref type="bibr" target="#b11">[12]</ref> using Geometric Context (GC) <ref type="bibr" target="#b13">[14]</ref>. Hedau et al. <ref type="bibr" target="#b9">[10]</ref> further jointly inference the room layout with 3D objects, e.g. beds. Similar strategies have also been used by later methods, such as introducing an improved scoring function <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref>, generating layout hypothesis with Manhattan junction <ref type="bibr" target="#b21">[22]</ref>, and modeling the interaction between objects and layout <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b34">35]</ref>.</p><p>The aforementioned methods only deal with perspective images. Zhang et al. <ref type="bibr" target="#b31">[32]</ref> propose to estimate the layout from a 360 • H-FOV panoramic image. They extend the previous methods of vanishing point detection, hypothesis generation, and scoring hypotheses based on OM, GC and object interaction, and apply all of them to panoramas. Xu et al. <ref type="bibr" target="#b27">[28]</ref> also use the OM, GC, object detection, and object orientation to reconstruct 3D layout. Yang et al. <ref type="bibr" target="#b28">[29]</ref> use superpixels and Manhattan aligned line segments as features, and formulate the problem by constraint graphs. The method of <ref type="bibr" target="#b30">[31]</ref> follows a similar approach using more geometric and semantic features. Other approaches attempt to recover the floor plan from a panorama using image gradient cues <ref type="bibr" target="#b20">[21]</ref> or from multiple panorama images <ref type="bibr" target="#b1">[2]</ref>.</p><p>Recent methods rely more on deep networks to improve layout estimation. Most of them leverage dense prediction models to classify geometric or semantic label for each pixel. For perspective images, common ways are to predict the boundary probability map <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b22">23]</ref>, classes of boundaries <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b22">23]</ref>, classes of layout surface <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b14">15]</ref>, and corner keypoints heatmaps <ref type="bibr" target="#b16">[17]</ref>. The predicted dense maps can be post-processed to generate layouts. A few deep learning methods have been developed for panorama-based layout estimation. Zou et al. <ref type="bibr" target="#b35">[36]</ref> predict the corner probability map and boundary map directly from a panorama. They also extend Stanford 2D-3D dataset <ref type="bibr" target="#b0">[1]</ref> with annotated layouts for training and evaluation. Fernandez-Labrador et al. <ref type="bibr" target="#b8">[9]</ref> train the deep network on perspective images. During testing, they stitch the predicted perspective boundary maps into a panorama and combine them with geometric cues to infer the layout. Two concurrent works DuLa-Net <ref type="bibr" target="#b29">[30]</ref> and CFL <ref type="bibr" target="#b7">[8]</ref> show improved quantitative results with the ability to produce general room shape not limited to cuboid shape. DuLa-Net <ref type="bibr" target="#b29">[30]</ref> combines the surface semantic mask from conventional equirectangular view and the projected floor and ceiling view. CFL <ref type="bibr" target="#b7">[8]</ref> proposes convolution kernel specialized for equirectangular image.</p><p>Unlike all the existing methods that use neural networks to perform dense prediction for layout estimation, we leverage the property of aligned panorama image to predict the positions of floor-wall and ceiling-wall boundaries, as well as the existence of wall-wall boundary for each column of an equirectangular image. Our model only produces three values for each column of an image, and thus the output size of the model is reduced from O(HW ) to O(W ). The proposed output representation is similar to <ref type="bibr" target="#b5">[6]</ref> but they only predict floor-wall boundary for each column of a perspective image using a Dynamic Bayesian Network. In contrast, our work can handle panoramas and recognize floor-wall, ceiling-wall and wall-wall boundaries using a deep neural network. Existing works <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b7">8]</ref> on the same task learn to make dense O(HW ) predictions over the entire image while our model predicts only three values for each image column. RoomNet <ref type="bibr" target="#b16">[17]</ref> imitates RNN's recurrent structure with "time steps" equal to refinement steps. We use RNN where each "time step" is responsible for estimating the result across a few image columns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>The goal of our approach is to estimate Manhattan room layout from a panoramic image that covers 360 • H-FOV. Unlike conventional dense prediction (target output size = O(HW )) for layout estimation using deep learning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b32">33]</ref>, we formulate the problem as regressing the boundaries and classifying the corner for each column of image (target output size = O(W )). The proposed HorizonNet trained for predicting the O(W ) target is presented in Sec. 3.1. In Sec. 3.2, we introduce a simple yet fast and effective post-processing procedure to derive the layout from output of HorizonNet. Finally in Sec. 3.3, we introduce Pano Stretch Data Augmentation which effectively augments the training data on-the-fly by stretching the image and ground-truth layout along x or z axis ( <ref type="figure">Fig. 5)</ref>.</p><p>All training and test images are pre-processed by the panoramic image alignment algorithm mentioned in <ref type="bibr" target="#b35">[36]</ref>. Our approach exploits the properties of the aligned panoramas that the wall-wall boundaries are vertical lines under equirectangular projection. Therefore, we can use only one value to indicate the column position of wall-wall boundary instead of two (each for a boundary endpoint). <ref type="figure" target="#fig_1">Fig. 2</ref> shows an overview of our network, which comprises a feature extractor and a recurrent neural network. The network takes a single panorama image with the dimension of 3 × 512 × 1024 (channel, height, width) as input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">HorizonNet</head><p>1D Layout Representation: The size of network output is 3 × 1 × 1024. As illustrated in <ref type="figure" target="#fig_2">Fig. 3</ref>, two of the three output channels represent the ceiling-wall (y c ) and the floorwall (y f ) boundary position of each image column, and the other one (y w ) represents the existence of wall-wall boundary (i.e. corner). The values of y c and y f are normalized to [−π/2, π/2]. Since defining y w as a binary-valued vector with 0/1 labels would make it too sparse to detect (only 4 out of 1024 non-zero values for simple cuboid layout), we set y w (i) = c dx where i indicates the ith column, dx is the distance from the ith column to the nearest column where wall-wall boundary exists, and c is a constant. To check the robustness of our method against the choice of c, we have tried 0.6, 0.8, 0.9, 0.96, 0.99 and get similar results. Therefore, we stick to c = 0.96 for all the experiments. One benefit of using 1D representation is that it is less affected by zero dominant backgrounds. 2D whole-image representations of boundaries and corners would result in 95% zero values even after smoothing <ref type="bibr" target="#b35">[36]</ref>. Our 1D boundaries representation introduces no zero backgrounds because the prediction for each component of y c or y f is simply a realvalued regression to the ground truth. The 1D wall-wall (corners) representation also changes the peak-background ratio of ground truth from 2N 512·1024 to N 1024 where N is the number of wall-wall corners. Therefore, the 1D wall-wall representation is also less affected by zero-dominated background. In addition, computation of 1D compact output is more efficient compared to 2D whole-image output. As depicted in Sec. 3.2, recovering the layout from our three 1D representations is simple, fast, and effective.</p><p>Feature Extractor: We adopt ResNet-50 <ref type="bibr" target="#b10">[11]</ref> as our feature extractor. The output of each block of ResNet-50 has half spatial resolution compared to that of the previous block. To capture both low-level and high-level features, each block of the ResNet-50 contains a sequence of convolution layers in which the number of channels and the height is reduced by a factor of 8 (= 2×2×2) and 16 (= 4×2×2), respectively. More specifically, each block contains three convolution layers with 4 × 1, 2 × 1, 2 × 1 kernel size and stride, and the number of channels after each Conv is reduced by a factor of 2. All the extracted features from each layer are upsampled to the same width 256 (a quarter of input image width) and reshaped to the same height. The final concatenated feature map is of size 1024 × 1 × 256. The activation function after each Conv is ReLU except the final layer in which we use Sigmoid for y w and an identity function for y c , y f . We have tried various settings for the feature extractor, including deeper ResNet-101, different designs of the convolution layers after each ResNet block, and upsampling to the image width 1024, and find that the results are similar. Therefore, we stick to the simpler and computationally efficient setting.   Recurrent Neural Network for Capturing Global Information: Recurrent neural networks (RNNs) are capable of learning patterns and long-term dependencies from sequential data. Geometrically speaking, any corner of a room can be roughly inferred from the positions of other corners; therefore, we use the capability of RNN to capture global information and long-term dependencies. Intuitively, because LSTM <ref type="bibr" target="#b12">[13]</ref>, a type of RNN architecture, stores information about its prediction for other regions in the cell state, it has the ability to predict for occluded area accurately based on the geometric patterns of the entire room. In our model, RNN is used to predict y c , y f , y w column by column. That is, the sequence length of RNN is proportional to the image width. In our experiment, RNN predicts for four columns instead of one column per time step, which requires less computational time without loss of accuracy. As the y c , y f , y w of a column is related to both its left and right neighbors, we adopt the bidirectional RNN <ref type="bibr" target="#b24">[25]</ref> to capture the information from both sides. <ref type="figure" target="#fig_5">Fig. 7</ref> and <ref type="table">Table 1</ref> demon-strate the difference between models with or without RNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Post-processing</head><p>We recover general room layouts that are not limited to cuboid under following assumptions: i) intersecting walls are perpendicular to each other (Manhattan world assumption); ii) all rooms have the one-floor-one-ceiling layout where floor and ceiling are parallel to each other; iii) camera height is 1.6 meters following <ref type="bibr" target="#b31">[32]</ref>; iv) the pre-processing step correctly align the floor orthogonal to y-axis.</p><p>As described in Sec. 3.1, raw outputs of our deep model y f , y c , y w ∈ R 1024 contain the layout information for each image column. Each value in y f and y c is the position of floor-wall boundary and ceiling-wall boundary at the corresponding image column. y w represents the probability of wall-wall existence of each image column.</p><p>Recovering the Floor and Ceiling Planes: For each column of the image, we can use the corresponding values in y f , y c to vote for the ceiling-floor distance. Based on the assumed camera height, we can project the floor-wall boundary y f from image to 3D XY Z position (they all shared the same Y ). The ceiling-wall boundary y c shares the same 3D X, Z position with the y f on the same image column, and therefore the distance between floor and ceiling can be calculated. We take the average of results calculated from all image columns as the final floor-ceiling distance. Recovering Wall Planes: We first find the prominent peaks on the estimated wall-wall probability y w with two criteria: i) the signal should be larger than any other signal within 5°H-FOV, and ii) the signal should be larger than 0.05. <ref type="figure">Fig. 4a</ref> shows the projected y c (red points) on ceiling plane. The green lines are the detected prominent peaks which split the ceiling-wall boundary (red points) into multiple parts. To handle possibly failed horizontal alignment in the pre-processing step, we calculate the first principal component of each part, then rotate the scene by the aver-age angle of all first principal components (top right figure in <ref type="figure">Fig. 4a</ref>). So now we have two types of walls: i) Xaxis orthogonal walls and ii) Z-axis orthogonal walls. We construct the walls from low to high variance suggested by the first principal component. Adjacency walls are forced to be orthogonal to each other, thus only walls whose two adjacent walls are not yet constructed have the freedom to decide the orthogonal type. We use a simple voting strategy: each projected red point votes for all planes within 0.16 meters (bottom right figure in <ref type="figure">Fig. 4a</ref>). The most voted plane is selected. Two special cases are depicted in <ref type="figure">Fig 4b</ref> which occur when the two adjacency walls are already constructed and they are orthogonal to each other. Finally, the XY Z positions of all corners are decided according to the intersection of three adjacent Manhattan junction planes.</p><p>The time complexity of our post-processing procedure is O(W ), where W is the image width. Thus the postprocessing can be efficiently done; in average, it takes less than 20ms to finish. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Occluded Corner</head><p>Camera Center Camera Center False Negative (b) Two special cases: Instead of voting for a wall, we add a corner according to the two prominent peaks and the positions of two walls. <ref type="figure">Figure 4</ref>: Visualization of wall planes recovering. <ref type="figure">Fig. 4a</ref> is an example that the pre-processing algorithm fails to correctly align the horizontal rotation of panorama.  <ref type="table">(Table 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Pano Stretch Data Augmentation</head><p>For a 360 • H-FOV panoramic image, we propose to stretch along axes in 3D space to augment training data. To achieve this goal, we first represent each pixel under UV space as (u, v) where u ∈ [−π, π], v ∈ [−π/2, π/2]. The coordinate (u, v) can be easily computed as the column and row of an equirectangular image, subject to a rotation angle of the camera. Here we introduce an additional variable d, which denotes the depth of a pixel. We will show that d can be eliminated later so our final equation does not depend on it.</p><p>We project the pixels to 3D space and multiply their x, y, z by k x , k y , k z . The equation of stretched x , y , z are shown in Eq. 1.</p><formula xml:id="formula_0">   x = k x · x = k x · d · cos(v) · cos(u) ; y = k y · y = k y · d · sin(v) ; z = k z · z = k z · d · cos(v) · sin(u) .<label>(1)</label></formula><p>We can then project the stretched points back to the sphere by Eq. 2 for further equirectangular projection. atan2 in the equation is 2-argument arctangent. The depth d is eliminated since it exists in both terms of atan2. We fix k y = 1 because setting k y to a value other than one is equivalent to multiplying k x , k z by the same value.</p><formula xml:id="formula_1">     u = atan2(k z · sin(u), k x · cos(u)) ; v = atan2(k y · sin(v), k 2</formula><p>x cos 2 (u) + k 2 z sin 2 (u) · cos(v) ) .</p><p>(2) In our implementation, we do the inverse mapping by Eq. 3. For each pixel in the target image, we compute the corresponding coordinate and sample its value from the source image via bilinear interpolation. <ref type="figure">Fig. 5</ref> shows a visualization sample. u = atan2(k x · sin(u ), k z · cos(u )) ; v = arctan(k z · tan(v ) · csc(u ) · sin(u)) .</p><p>(3)</p><p>Note that our Pano Stretch Data Augmentation procedure could also be used on other tasks (e.g., ground-truth map of semantic segmentation, bounding box for object detection) that directly work on panoramas. The augmentation procedure has the potential to boost the accuracy of those tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We train and evaluate our model using the same dataset as LayoutNet <ref type="bibr" target="#b35">[36]</ref>. The dataset consists of PanoContext dataset <ref type="bibr" target="#b31">[32]</ref> and the extended Stanford 2D-3D dataset <ref type="bibr" target="#b0">[1]</ref> annotated by <ref type="bibr" target="#b35">[36]</ref>. To train our model, we generate 3 × 1 × 1024 ground truth from the annotation. We follow the same training/validation/test split of LayoutNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Training Details</head><p>The Adam optimizer <ref type="bibr" target="#b15">[16]</ref> is employed to train the network for 300 epochs with batch size 24 and learning rate 0.0003. The L1 Loss is used for the ceiling-wall boundary (y c ) and floor-wall boundary (y f ). The Binary Cross-Entropy Loss is used for the wall-wall corner (y w ). The network is implemented in PyTorch <ref type="bibr" target="#b19">[20]</ref>. It takes four hours to finish the training on three NVIDIA GTX 1080 Ti GPUs.</p><p>The data augmentation techniques we adopt include standard left-right flipping, panoramic horizontal rotation, and luminance change. Moreover, we exploit the proposed Pano Stretch Data Augmentation (Sec. 3.3) during training. The stretching factors k x , k z are sampled from uniform distribution U <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, and then take the reciprocals of sampled values with probability 0.5. The process time of Pano Stretch Data Augmentation is roughly 130ms per 512 × 1024 RGB image. Therefore, it is feasible to be applied on-the-fly during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Cuboid Room Results</head><p>We generate cuboid room by only selecting the four most prominent peaks in the post-processing step (Sec. 3.2). Quantitative Results: Our approach is evaluated on three standard metrics: i) 3D IoU: intersection over union between 3D layout constructed from our prediction and the ground truth; ii) Corner Error: average Euclidean distance between predicted corners and ground-truth corners (normalized by image diagonal length); iii) Pixel Error: pixelwise error between predicted surface classes and groundtruth surface classes. The quantitative results of different training and testing settings are summarized in <ref type="table">Table 1 and Table 2</ref>. To clarify the difference, the input resolution of DuLa-Net <ref type="bibr" target="#b29">[30]</ref> and CFL <ref type="bibr" target="#b7">[8]</ref> are 256 × 512 while LayoutNet <ref type="bibr" target="#b35">[36]</ref> and ours are 512 × 1024. Other than conventional augmentation technique, CFL <ref type="bibr" target="#b7">[8]</ref> is trained with Random Erasing while ours is trained with the proposed Pano Stretch. DuLa-Net <ref type="bibr" target="#b29">[30]</ref> did not report corner errors and pixel errors. Our approach achieves state-of-the-art performance and outperforms existing methods under all settings. Qualitative Results: The qualitative results are shown in <ref type="figure">Fig. 6</ref>. We present the results from the best to the worst based on their corner errors. Please see more results in the supplemental materials. Computation time: The 1D layout representation is easy to compute. Forward passing a single 512 x 1024 RGB image takes 8ms and 50ms for our HorizonNet with and without RNN respectively. The post-processing step for extracting layout from our 1D representation takes only 12ms. We evaluate the result on a single NVIDIA Titan X GPU and an Intel i7-5820K 3.30GHz CPU. The reported execution time is averaged across all the testing data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>Ablation experiments are presented in <ref type="table">Table 3</ref>. We report the result averaged across all the testing instances. For a fair comparison, we also experiment with dense O(HW ) prediction following LayoutNet <ref type="bibr" target="#b35">[36]</ref> but replace the U-Net <ref type="bibr" target="#b23">[24]</ref> with the same backbone as our architecture. <ref type="bibr" target="#b0">1</ref> The results of this setting are presented in the first two rows. We do not try dense O(HW ) output with RNN since it would consume <ref type="figure">Figure 6</ref>: Qualitative results of cuboid layout estimation. The results are separately sampled from four groups that comprise results with the best 0-25%, 25-50%, 50-75% and 75-100% corner errors (displayed from the first to the fourth columns). The green lines are ground truth layout while the orange lines are estimated. The images in the first row are from PanoContext dataset <ref type="bibr" target="#b31">[32]</ref> while second row are from Stanford 2D-3D dataset <ref type="bibr" target="#b0">[1]</ref>. too many computing resources. We can see that learning on our 1D O(W ) layout representation is better than conventional dense O(HW ) layout representation. We observe that training with the proposed Pano Stretch Data Augmentation can always boost the performance. Note that the proposed data augmentation method can also be adopted in other tasks on panoramas and has the potential to increase their accuracy as well. See supplemental material for the experiment using Pano Stretch Data Augmentation on semantic segmentation task.</p><p>For the rows where RNN columns are unchecked, the RNN components shown in <ref type="figure" target="#fig_1">Fig 2 are</ref> replaced by fully connected layers. Our experiments show that using RNN in network architecture also improves performance. <ref type="figure" target="#fig_5">Fig. 7</ref> shows some representative results with and without RNN. The raw output of the model with RNN is highly consistent with the Manhattan world even without post-processing, which </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Non-cuboid Room Results</head><p>Since the non-cuboid rooms in PanoContext and Stanford 2D-3D dataset are labeled as cuboids, our model is never trained to recognize non-cuboid layouts and concave corners. This bias makes our model tend to predict complex-shaped rooms as cuboids. To estimate general room layouts, we re-label 65 rooms from the training split to fine-tune our trained model. We fine-tune our model for 300 epochs with learning rate 5e−5 and batch size 2.</p><p>To quantitatively evaluate the fine-tuning result on general-shaped rooms, we use 13-fold cross validation on the 65 re-annotated non-cuboid data. The results are sum-  <ref type="table">Table 3</ref>. Ablation study demonstrates the effectiveness of each component in our approach. We show that all of our proposed designs can improve the quantitative result. Besides, our proposed 1D layout representation significantly reduces the number of parameters. FPS is measured for forward-pass of a 3 × 512 × 1024 image on an NVIDIA TITAN X GPU. marized in <ref type="table">Table 4</ref>. We depict some examples of reconstructed non-cuboid layouts from the testing and validation splits in <ref type="figure" target="#fig_6">Fig.1 and Fig.8</ref>. See supplemental material for more reconstructed layouts. The results show that our approach can work well on general room layout even with corners occluded by other walls.  <ref type="table">Table 4</ref>. Quantitative results on the 65 re-annotated noncuboid datas. The result of fine-tuning is evaluated by 13fold validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have presented a new 1D representation for the task of estimating room layout from a panorama. The proposed HorizonNet trained with such 1D representation outperforms previous state-of-the-art methods and requires fewer computation resources. Our post-processing method which recovers 3D layout from the model output is fast and effective, and it also works for complex room layouts even with occluded corners. The proposed Pano Stretch Data Augmentation further improves our results, and can also be applied to the training procedure of other panorama tasks for potential improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Pano Stretch Augmentation for Semantic Segmentation</head><p>We evaluate the potential advantage of the proposed Pano Stretch Data Augmentation on semantic segmentation task. We train and test on Stanford 2D3D <ref type="bibr" target="#b0">[1]</ref>   <ref type="table">Table 5</ref>. We evaluate the effect of Pano Stretch Augmentation on semantic segmentation task using the standard metric -mIoU (%). The result implies that the new augmentation technique has the potential to mitigate the "lack of training data" problem for other tasks like semantic segmentation.</p><p>B. More Qualitative Results of Cuboid Room Layout Reconstruction <ref type="figure">Figure 9</ref>: Qualitative results of cuboid layout estimation on PanoContext <ref type="bibr" target="#b31">[32]</ref> dataset. The results in the first to the fourth rows are separately sampled from four groups that comprise results with the best 0-25%, 25-50%, 50-75% and 75-100% corner errors, and the four results with the worst corner errors are displayed in the last row. The green lines are ground truth layout while the orange lines are estimated layout.              </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>An illustration of the HorizonNet architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Visualization of our 1D ground truth representations. y w denotes the existence probability of wall-wall boundary. y c , y f (plotted in green and blue) denote the positions of the ceiling-wall boundary and floor-wall boundary respectively. For better visualization, we plot y w , y c , y f with line width greater than one pixel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>with the Floor and Ceiling Planes Projected boundary (a) Depicting how we recover the wall planes from our model output.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>k x = 1 Figure 5 :</head><label>15</label><figDesc>.0, k z = 1.0 (original) k x = 2.0, k z = 1.0 k x = 1.0, k z = 2.0 k x = 2.0, k z = 2.0 Visualization of the proposed Pano Stretch Data Augmentation. The image and ground-truth layout (green lines) are stretched along x or z axis (the effect of scaling y can be covered by x and z). This can augment the data by changing the room's length and width. This augmentation strategy improves our quantitative results under all experiment settings</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Visualization of model outputs with and without RNN. We plot the ground truth (green), outputs of the model with RNN (yellow), and outputs of the model without RNN (magenta). Both predictions are raw network outputs without post-processing. The model with RNN performs better than the model without RNN in images contain ceiling beam, black missing polar region caused by smaller camera V-FOV, and occluded area.demonstrates the ability of RNN to capture the geometric pattern of the entire room.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Qualitative results of non-cuboid layout estimation. The occluded walls are filled with black. The blue lines in the equirectangular images are the estimated room layout boundary.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 :</head><label>10</label><figDesc>Qualitative results of cuboid layout estimation on Stanford 2D-3D<ref type="bibr" target="#b0">[1]</ref> dataset. The results in the first to the fourth rows are separately sampled from four groups that comprise results with the best 0-25%, 25-50%, 50-75% and 75-100% corner errors, and the four results with the worst corner errors are displayed in the last row. The green lines are ground truth layout while the orange lines are estimated layout.C. More Qualitative Results of Non-Cuboid Room Layout Reconstruction</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 11 :</head><label>11</label><figDesc>Green lines are original ground truth annotation. Blue lines are room layout estimated by our model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 12 :</head><label>12</label><figDesc>Green lines are original ground truth annotation. Blue lines are room layout estimated by our model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 13 :</head><label>13</label><figDesc>Green lines are original ground truth annotation. Blue lines are room layout estimated by our model. The occlusion walls are filled with black.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 14 :</head><label>14</label><figDesc>Green lines are original ground truth annotation. Blue lines are room layout estimated by our model. The occlusion walls are filled with black.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 15 :</head><label>15</label><figDesc>Green lines are original ground truth annotation. Blue lines are room layout estimated by our model. The occlusion walls are filled with black.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 16 :</head><label>16</label><figDesc>Green lines are original ground truth annotation. Blue lines are room layout estimated by our model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 17 :</head><label>17</label><figDesc>Green lines are original ground truth annotation. Blue lines are room layout estimated by our model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 18 :</head><label>18</label><figDesc>Green lines are original ground truth annotation. Blue lines are room layout estimated by our model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 19 :</head><label>19</label><figDesc>Green lines are original ground truth annotation. Blue lines are room layout estimated by our model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 20 :</head><label>20</label><figDesc>Green lines are original ground truth annotation. Blue lines are room layout estimated by our model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 21 :</head><label>21</label><figDesc>Green lines are original ground truth annotation. Blue lines are room layout estimated by our model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 22 :</head><label>22</label><figDesc>Green lines are original ground truth annotation. Blue lines are room layout estimated by our model. The occlusion wall is filled with black.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 23 :</head><label>23</label><figDesc>Green lines are original ground truth annotation. Blue lines are room layout estimated by our model. The occlusion wall is filled with black.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>semantic segmentation benchmark. We train PSPNet [34] on subsampled training set and test on the whole testing set. The results are summarized in Table 5. stretch aug. 31.5 34.9 37.1 40.7 44.2 44.8 w/ pano stretch aug. 33.2 36.1 38.4 41.9 44.3 44.9</figDesc><table><row><cell># of training images</cell><cell>20</cell><cell>50</cell><cell>100 200 500 1040</cell></row><row><cell>wo/ pano</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">To output dense (full-image) probability map, we change the Conv layer after each ResNet block from reducing both height and channels to reducing only channels, and then upsample to the same spatial dimension as the input image. Finally, the processed features of four blocks are concatenated and passed through a Conv layer to generate the final result.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement: This research was partially supported by iStaging and by MOST grants 106-2221-E-007-080-MY3, 107-2218-E-007-047, and 108-2634-F-001-007.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Joint 2D-3D-Semantic Data for Indoor Scene Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-02" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Piecewise planar and compact floorplan reconstruction from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Cabral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasutaka</forename><surname>Furukawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="628" to="635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Manhattan world: Compass direction from a single image by bayesian inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Coughlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Proceedings of the Seventh IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1999" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="941" to="947" />
		</imprint>
	</monogr>
	<note>Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Delay: Robust spatial layout estimation for cluttered indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saumitro</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="616" to="624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Understanding bayesian rooms using composite 3d object models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Del Pero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Bowdish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Kermgard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kobus</forename><surname>Barnard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="153" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A dynamic bayesian network model for autonomous 3d reconstruction from a single indoor image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erick</forename><surname>Delage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2418" to="2428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Fernandez-Labrador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><forename type="middle">M</forename><surname>Facil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Perez-Yus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cedric</forename><surname>Demonceaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose J</forename><surname>Guerrero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.09879</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Panoroom: From the sphere to the 3d layout</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Fernandez-Labrador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Fcil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cdric</forename><surname>Perez-Yus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Demonceaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jos J</forename><surname>Civera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guerrero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.08094</idno>
		<title level="m">Corners for layout: End-to-end layout recovery from 360 images</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Fernandez-Labrador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Perez-Yus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gonzalo</forename><surname>Lopez-Nicolas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose J</forename><surname>Guerrero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.08294</idno>
		<title level="m">Layouts from panoramic images with geometry and deep learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Estimating spatial layout of rooms using volumetric reasoning about objects and surfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1288" to="1296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-27" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Recovering the spatial layout of cluttered rooms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varsha</forename><surname>Hedau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th international conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1849" to="1856" />
		</imprint>
	</monogr>
	<note>Computer vision</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Recovering surface layout from an image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="151" to="172" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Izadinia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven M</forename><surname>Seitz</surname></persName>
		</author>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Roomnet: End-to-end room layout estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Yu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2017 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4875" to="4884" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Geometric reasoning for single image structure recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>David C Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2136" to="2143" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning informative edge maps for indoor scene layout prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="936" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Omnidirectional image capture on mobile devices for fast automatic generation of 2.5 d indoor maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Pintore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valeria</forename><surname>Garro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Ganovelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrico</forename><surname>Gobbetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Agus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applications of Computer Vision (WACV), 2016 IEEE Winter Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Manhattan junction catalogue for spatial reasoning of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srikumar</forename><surname>Ramalingam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jaishanker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arpit</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3065" to="3072" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A coarse-to-fine indoor layout estimation (cfile) method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuzhuo</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangwen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C-C Jay</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="36" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unet: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kuldip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Efficient exact inference for 3d indoor scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="299" to="313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Efficient structured prediction for 3d indoor scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>R Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2815" to="2822" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pano2cad: Room layout from a single panorama image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Björn</forename><surname>Stenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Kerola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Tung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applications of Computer Vision (WACV), 2017 IEEE Winter Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="354" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Efficient 3d room shape recovery from a single panorama</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5422" to="5430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Dula-net: A dual-projection network for estimating room layouts from a single rgb panorama</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shang-Ta</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu-En</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Han</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Wonka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Kuo</forename><surname>Chu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.11977</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Automatic 3d indoor scene modeling from single panorama</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiyang</forename><surname>Shi Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Sing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3926" to="3934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Panocontext: A whole-room 3d context model for panoramic scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="668" to="686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Physics inspired optimization on semantic transfer features: An alternative method for room layout estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anbang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiwen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.00383</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Scene parsing by integrating function, geometry and appearance models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3119" to="3126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Layoutnet: Reconstructing the 3d room layout from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuhang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Colburn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2051" to="2059" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
