<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Weakly Supervised Complementary Parts Models for Fine-Grained Image Classification from the Bottom Up</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weifeng</forename><surname>Ge</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Deepwise AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangru</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Deepwise AI Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Weakly Supervised Complementary Parts Models for Fine-Grained Image Classification from the Bottom Up</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Given a training dataset composed of images and corresponding category labels, deep convolutional neural networks show a strong ability in mining discriminative parts for image classification. However, deep convolutional neural networks trained with image level labels only tend to focus on the most discriminative parts while missing other object parts, which could provide complementary information. In this paper, we approach this problem from a different perspective. We build complementary parts models in a weakly supervised manner to retrieve information suppressed by dominant object parts detected by convolutional neural networks. Given image level labels only, we first extract rough object instances by performing weakly supervised object detection and instance segmentation using Mask R-CNN and CRF-based segmentation. Then we estimate and search for the best parts model for each object instance under the principle of preserving as much diversity as possible. In the last stage, we build a bi-directional long short-term memory (LSTM) network to fuze and encode the partial information of these complementary parts into a comprehensive feature for image classification. Experimental results indicate that the proposed method not only achieves significant improvement over our baseline models, but also outperforms state-of-the-art algorithms by a large margin (6.7%, 2.8%, 5.2% respectively) on Stanford Dogs 120, Caltech-UCSD Birds 2011-200 and Caltech 256.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep neural networks have demonstrated its ability to learn representative features for image classification <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b16">17]</ref>. Given training data, image classification <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b24">25]</ref> often builds a feature extractor that accepts an input image and a subsequent classifier that generates prediction probability for the image. This is a common pipeline in many high-level vision tasks, such as object detection <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16]</ref>, tracking <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b37">38]</ref>, and scene under- * These authors have equal contribution. standing <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b30">31]</ref>.</p><p>Although a model trained with the aforementioned pipeline can achieve competitive results on many image classification benchmarks, its performance gain primarily comes from the model's capacity to discover the most discriminative parts in the input image. To better understand a trained deep neural network and obtain insights about this phenomenon, many techniques <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b55">55,</ref><ref type="bibr" target="#b1">2]</ref> have been proposed to visualize the intermediate results of deep networks. In <ref type="figure" target="#fig_0">Fig 1,</ref> it can be found that deep convolutional neural networks trained with image labels only tend to focus on the most discriminative parts while missing other object parts. However, focusing on the most discriminative parts alone can have limitations. Some image classification tasks need to grasp object descriptions that are as complete as possible. A complete object description does not have to come in one piece, but could be assembled together using multiple partial descriptions. To remove redundancies, such partial descriptions should be complementary to each other. Image classification tasks, that could benefit from such complete descriptions, include fine-grained classification tasks on Stanford Dogs 120 <ref type="bibr" target="#b20">[21]</ref> and CUB 2011-200 <ref type="bibr" target="#b47">[48]</ref>, where appearances of different object parts collectively contribute to the final classification performance.</p><p>According to the above analysis, we approach image classification from a different perspective and propose a new pipeline that aims to mine complementary parts instead of the aforementioned most discriminative parts, and fuse the mined complementary parts before making final classification decisions. Object Detection Phase. Object detection <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16]</ref> is able to localize objects by performing a huge number of classifications at a large number of locations. In <ref type="figure" target="#fig_0">Fig 1,</ref> the red bounding boxes are the ground truth, the green ones are positive object proposals, and the blue ones are negative proposals. The differences between the positive and negative proposals are whether they contain sufficient information (overlap ratio with the ground truth bounding box) to describe objects. If we look at the activation map in <ref type="figure" target="#fig_0">Fig 1,</ref> it is obvious that the positive bounding boxes spread much wider than the core regions. As a result, we hypoth-esize that the positive object proposals that lay around the core regions can be helpful for image classification since they contain partial information of the objects in the image. However, the challenges in improving image classifi- cation by detection are two-fold. First, how can we perform object detection without groundtruth bounding box annotations? Second, how can we exploit object detection results to boost the performance of image classification? In this paper, we attempt to tackle these two challenges in a weakly supervised manner.</p><p>To avoid missing any important object parts, we propose a weakly supervised object detection pipeline regularized by iterative object instance segmentation. We start by training a deep classification neural network that produces a class activation map (CAM) as in <ref type="bibr" target="#b55">[55]</ref>. Then the activations in CAM are taken as the pixelwise probabilities of the corresponding class. A conditional random field (CRF) <ref type="bibr" target="#b39">[40]</ref> then incorporates low level pairwise appearance information to perform unsupervised object instance segmentation. To refine object locations and pixel labels, a Mask R-CNN <ref type="bibr" target="#b15">[16]</ref> is trained using the object instance masks from the CRF. Results from the Mask R-CNN are used as a pixel probability map to replace the CAM in the CRF. We alternate Mask R-CNN and CRF regularization a few times to generate the final object instance masks. Image Classification Phase. Directly reporting classification results in the object detection phase gives rise to inferior performance because object detection algorithms make much effort to determine location in addition to class labels. In order to mine representative object parts with the help of object detection, we utilize the proposals generated in the previous object detection phase and build a complementary parts model, which consists of a subset of the proposals that cover as much complementary object information as possible. At the end, we exploit a bi-directional long short-term memory network to encode the deep features of the object parts for final image classification.</p><p>The proposed weakly supervised complementary parts model has been efficiently implemented in Caffe <ref type="bibr" target="#b19">[20]</ref>. Experimental results demonstrate state-of-the-art performance on multiple image classification tasks, including fine-grained classification on Stanford Dogs 120 <ref type="bibr" target="#b20">[21]</ref> and Caltech-UCSD Birds 200-2011 <ref type="bibr" target="#b47">[48]</ref>, and generic classifica-tion on Caltech 256 <ref type="bibr" target="#b14">[15]</ref>.</p><p>In summary, this paper has the following contributions:</p><p>• We introduce a new representation for image classification, called weakly supervised complementary parts model, that attempts to grasp complete object descriptions using a selected subset of object proposals. It is an important step forward in exploiting weakly supervised detection to boost image classification performance.</p><p>• We develop a novel pipeline for weakly supervised object detection and instance segmentation. Specifically, we iterate the following two steps, object detection and segmentation using Mask R-CNN, and instance segmentation enhancement using CRF.</p><p>• To encode complementary information in different object parts, we exploit a bi-directional long short-term memory network to make the final classification decision. Experimental results demonstrate that we achieve state-of-the-art performance on multiple image classification tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Weakly Supervised Object Detection and Segmentation.</p><p>Weakly supervised object detection and segmentation respectively locates and segments objects with image label only <ref type="bibr" target="#b4">[5]</ref>. In <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b5">6]</ref>, the object detection is solved as a classification problem by specific pooling layers in CNNs. The method in <ref type="bibr" target="#b44">[45]</ref> proposed an iterative bottom-up and topdown framework to expand object regions and optimize segmentation network iteratively. Ge et al. in <ref type="bibr" target="#b11">[12]</ref> progressively mine the object locations and pixel labels with the filtering and fusion of multiple evidences. While here we perform the weakly supervised object instance detection and segmentation by feeding a coarse segmentation mask and proposal for Mask R-CNN <ref type="bibr" target="#b15">[16]</ref> using CAM <ref type="bibr" target="#b55">[55]</ref> and rectifying the object locations and masks with CRF <ref type="bibr" target="#b39">[40]</ref> iteratively. In this way, we avoid losing important object parts for subsequent object parts modeling. Part Based Fine-grained Image Classification. Learning a diverse collection of discriminative parts in a supervised <ref type="bibr" target="#b52">[52,</ref><ref type="bibr" target="#b50">51]</ref> or unsupervised manner <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b53">53,</ref><ref type="bibr" target="#b25">26]</ref> is very popular in fine-grained image classification. Many works <ref type="bibr" target="#b52">[52,</ref><ref type="bibr" target="#b50">51]</ref> have been done to build object part models with part bounding box annotations. The method in <ref type="bibr" target="#b52">[52]</ref> builds two deformable part models <ref type="bibr" target="#b9">[10]</ref> to localize objects and discriminative parts. Zhang et al. in <ref type="bibr" target="#b50">[51]</ref> treats objects and semantic parts equally by assigning them in different object classes with R-CNN <ref type="bibr" target="#b13">[14]</ref>. Another line of works <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b53">53,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b44">45]</ref> estimate the part location in a unsupervised setting. In <ref type="bibr" target="#b34">[35]</ref>, parts are discovered based the neural activation, and then are optimized using a EM similar algorithm. The work in <ref type="bibr" target="#b34">[35]</ref> extracts the highlight responses in CNN as the part prior to initialize convolutional filters, and then learn discriminative patch detectors end-to-end.</p><p>In this paper, we do not aim to build strong part detectors to provide local appearance information for the final classification decision. The goal of our complementary parts model is to efficiently utilize the rich information hidden in the object proposals produced during object detection phase. Every object proposal contains enough information to classify the object, and their information are complementary with each other to formulate a more complete description about objects. Context Encoding with LSTM. LSTM network shows its powerfulness in encoding the context information for image classification. In <ref type="bibr" target="#b25">[26]</ref>, Lam et al. address fine-grained image classification by mining informative image parts using a heuristic network, a successor network and a single layer LSTM. The heuristic network is responsible for extracting features from proposals and the successor network is responsible for predicting the new proposal offset. A single layer LSTM is used to fuse the information both for final object class prediction and also for the offset prediction. Attentional regions is discovered recurrently by incorporating a LSTM sub-network for multi-label image classification in <ref type="bibr" target="#b46">[47]</ref>. The LSTM sub-network sequentially predict semantic labeling scores on the located regions and captures the spatial dependencies at the same time.</p><p>LSTM is used in our complementary part model to integrate the rich information hidden in different object proposals detected. Different from the single direction LSTM in <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b46">47]</ref>, we exploit a bi-directional LSTM to learn deep hierachical representation of all image patches. Experimental results show this strategy improve the performance substantially compared to the single layer LSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Weakly Supervised Complementary Parts Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>Given an image I and its corresponding image label c, the method proposed in this paper aims to mine discriminative parts M of an object that capture complementary information via object detection and then fuse the mined complementary parts for image classification. This is a reversal of a current trend <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b28">29]</ref>, which fine-tunes image classification models for object detection. Since we do not have labeled part locations but image level labels only, we formulate our problem in a weakly supervised manner. We adopt an iterative refinement pipeline to improve the estimation of object parts. Then we build a classifier utilizing the rich context representation focusing on object parts to boost classification performance. We decompose our pipeline into three stages, as shown in <ref type="figure">Fig 2,</ref> namely, weakly supervised object detection and instance segmentation, complementary part model mining and image classification with context encoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Weakly Supervised Object Detection and Instance Segmentation</head><p>Coarse Object Mask Initialization. Given an image I and its image label c, the feature map of the last convolutional layer of a classification network is denoted as φ (I, θ) ∈ R K×h×w , where θ represents the parameters of network φ, K is the number of channels, h and w are the height and width of the feature map respectively. Next, global average pooling is performed on φ to obtain the pooled feature F k =</p><p>x,y φ k (x, y). The classification layer is added at the end and thus, the class activation map (CAM) for class c is given as follows,</p><formula xml:id="formula_0">M c (x, y) = k w c k φ k (x, y),<label>(1)</label></formula><p>where w c k is the weight corresponding to class c for the k-th channel in the global average pooling layer. The obtained class activation map M c is upsampled to the original image size R H×W through bilinear interpolation. Since an image could have multiple object instances, multiple locally maximum responses could be observed on the class activation map M c . We apply multi-region level set segmentation <ref type="bibr" target="#b2">[3]</ref> to this map to segment candidate object instances. Next, for each instance, we normalize the class activation to the range, [0, 1]. Suppose we have n object instances in CAM, we set up an object probability map F ∈ R (n+1)×H×W according to the normalized CAM. The first n object probability maps denote the probability of a certain object existing in the image and the (n + 1)-th probability map represents the probability of the background. The background probability map is calculated as</p><formula xml:id="formula_1">F n+1 i∈R H×W = max(1 − n ι=1 F ι i∈R H×W , 0).<label>(2)</label></formula><p>Then a conditional random field (CRF) <ref type="bibr" target="#b39">[40]</ref> is used to extract higher-quality object instances. In order to apply CRFs, a label map L is generated according to the following formula,</p><formula xml:id="formula_2">L i∈R H×W = λ, arg max λ F λ i∈R H×W &gt; σ c 0, otherwise<label>(3)</label></formula><p>where σ c is always set to 0.8, a fixed threshold used to determine how certain a pixel belongs to an object or background. The label map L is then fed into a CRF to generate object instance segments, that are treated as pseudo groundtruth annotations for Mask-RCNN training. The parameters in the CRF are the same as in <ref type="bibr" target="#b22">[23]</ref>. <ref type="figure" target="#fig_0">Fig 2 stage 1</ref> shows the whole process of object instance segmentation. Softmax  <ref type="bibr" target="#b17">[18]</ref> are stacked together to fuse and encode the partial information provided by different object parts.</p><formula xml:id="formula_3">+ C 0 ′ h 1 , C 1 h 2 , C 2 h 3 , C 3 h n+1 , C n+1 h n+2 , C n+2 h 1 ′ , C 1 ′ h 2 ′ , C 2 ′ h n ′ , C n ′ h n+1 ′ , C n+1 ′ h n+2 ′ , C n+2 ′</formula><p>Jointly Detect and Segment Object Instances. Given a set of segmented object instances, S = [S 1 , S 2 , ...S n ] of I, and their corresponding class labels, generated in the previous stage, we obtain the minimum bounding box of each segment to form a set of proposals, P = [P 1 , P 2 , ...P n ]. The proposals P, segments S and their corresponding class labels are used for training Mask R-CNN for further proposal and mask refinement. In this way, we turn object detection and instance segmentation into fully supervised learning. We train Mask R-CNN with the same setting as in <ref type="bibr" target="#b15">[16]</ref>. CRF-Based Segmentation. Suppose there are m object proposals, P = [P 1 , P 2 , ..., P m ], and their corresponding segments, S = [S 1 , S 2 , ..., S m ] for image class c, whose classification score is above σ 0 , a threshold used to remove outlier proposals. Then, a non-maximum suppression (NMS) procedure is applied to m proposals with overlapping threshold τ . Suppose n object proposals remain</p><formula xml:id="formula_4">afterwards, O = [O 1 , O 2 , ..., O n ], where n m.</formula><p>Most existing research utilizes NMS to suppress a large number of proposals sharing the same class label in order to obtain a small number of distinct object proposals. However, in our weakly supervised setting, proposals suppressed in the NMS process actually contain rich object parts information as shown in <ref type="figure">Fig 2.</ref> Specifically, each proposal P i ∈ P suppressed by object proposal O j can be considered as a complementary part of O j . Therefore, the suppressed proposals, P i , can be used to further refine O j . We implement this idea by initializing a class probability map F ∈ R (n+1)×H×W . For each proposal P i suppressed by O j , we add the probability map of its proposal segmentation mask S i to the corresponding locations on F j by bilinear interpolation. The class probability map is then normalized to [0, 1]. For the (n + 1)-th probability map for the back-ground, it is defined as</p><formula xml:id="formula_5">F ,n+1 i∈R H×W = max(1 − n ι=1 F ,ι i∈R H×W , 0).<label>(4)</label></formula><p>Given the class probability maps F , CRF is applied again to refine and rectify instance segmentation results as described in the previous stage. Iterative Instance Refinement. We alternate CRF-based segmentation and Mask R-CNN based detection and instance segmentation several times to gradually refine the localization and segmentation of object instances. <ref type="figure">Fig 2  shows</ref> the iterative instance refinement process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Complementary Parts Model</head><p>Model Definition. According to the analysis in the previous stage, given a detected object O i , its corresponding suppressed proposals, P ,i = P ,i 1 , P ,i 2 , ..., P ,i k , may contain useful object information and can localize correct object position. Then, it is necessary to identify the most informative proposals for the following classification task. In this section, we propose a complementary parts model A for image classification. This model is defined by a root part covering the entire object as well as its context, a center part covering the core region of the object and a fixed number of surrounding proposals that cover different object parts but still keep enough discriminative information.</p><p>A complementary parts model for an object with n parts is defined as a (n + 1)-tuple A = [A 1 , ..., A n , A n+1 ], where A 1 is the object center part, A n+1 is the root part, and A i is the i-th part. Each part model is defined by a tuple A i = [φ i , u i ], where φ i is the feature of the i-th part, u i is a R 4 dimensional tuple that describes the geometric information of a part, namely part center and part size (x i , y i , w i , h i ). A potential parts model without any missing parts is called an object hypothesis. To make object parts complementary to each other, the differences in their appearance features or locations should be as large as possible while the combination of parts scores should also be as large as possible. Such criteria serve as constraints during the search for discriminative parts that are complementary to each other. The score S (A) of an object hypothesis is given by the summed score of all object parts minus appearance similarities and spatial overlap between different parts.</p><formula xml:id="formula_6">S (A) = n+1 ι=1 f (φ ι ) − λ 0 n p=1 n+1 q=p+1 [d s (φ p , φ q ) + β 0 IoU (u p , u q )] ,<label>(5)</label></formula><p>where f (φ k ) is the score of the k-th part in the classification branch of Mask R-CNN, d s (φ p , φ q ) = φ p − φ q 2 is the semantic similarity and IoU (u p , u q ) is the spatial overlap between parts p and q, and there are two constant parameters λ 0 = 0.01 and β 0 = 0.1. Given a set of object hypotheses, we can choose a hypothesis that achieves the maximum score as the final object part model. Searching for the optimal subset of proposals maximizing the above score is a combinatorial optimization problem, which is computationally expensive. In the following, we seek an approximate solution using a fast heuristic algorithm. Part Location Initialization. To initialize a parts model, we simplify part estimation by designing a grid-based object parts template that follows two basic rules. First, every part should contain enough discriminative information; Second, the differences between part pairs should be as large as possible. As shown in <ref type="figure">Fig 2,</ref> deep convolutional neural networks have demonstrated its ability in localizing the most discriminative parts of an object. Thus, we set the root part A n+1 to be the object proposal O i that represents the entire object. Then, a s × s(= n) grid centered at A n+1 is created. The size of each grid cell is wn+1 s × hn+1 s , where w n+1 and h n+1 are the width and height of the root part A n+1 . The center grid cell is assigned to the object center part. The rest of the grid cells are assigned to part A i , where i ∈ [2, 3, ..., n]. Then, we initialize each part A i ∈ A to be the proposal P j ∈ P closest to the assigned grid cell. Parts Model Search. For a model with n object parts (we exclude the (n + 1)-th part as it is a root part) and k candidate suppressed proposals, the objective function is defined as</p><formula xml:id="formula_7">A = arg max A∈S A S (A) ,<label>(6)</label></formula><p>where K = C n k , k n is the total number of object hypothesises, S A = A 1 , A 1 , ..., A K is the set of object hypotheses. As mentioned earlier, directly searching for an optimal parts model can be intractable. Thus, we adopt a greedy search strategy to search for A. Specifically, we sequentially go through every A i in A and find the optimal object part for A i in P that minimizes A. The overall time complexity is reduced from exponential to linear (O(nk)). In <ref type="figure">Fig 2,</ref> we can see that the object hypotheses generated during the search process cover different parts of the object and do not focus on the core region only.  Stacked LSTM for Feature Fusion. Here we propose a stacked LSTM module φ l (·; θ l ) for feature fusion and performance boosting, which is shown in be the dimension of the hidden state. We use softmax to generate the class probability vector for each part A i , f (φ l (I (A i ) ; θ l )) ∈ R C×1 . The loss function for final im-age classification is defined as follows,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Image Classification with Context Encoding</head><formula xml:id="formula_8">ℎ −1 tanh tanh −1 −1 −1 ሚ −1 ℎ −1 −1 ℎ +1 tanh tanh +1 +1 +1 ሚ +1 ℎ +1 +1 ℎ tanh tanh ሚ ℎ tanh tanh +1 ′ +1 ′ +1 ′ ሚ +1 ′ ℎ +1 ′ +1 ′ ℎ +1 ′ tanh tanh ′ ′ ′ ሚ ′ ℎ ′ ′ ℎ ′ tanh tanh −1 ′ −1 ′ −1 ′ ሚ −1 ′ ℎ −1 ′ FC Softmax</formula><formula xml:id="formula_9">L(I, y I ) = − C k=1 y k log f k (φ l (I; θ l )) − n+2 i=1 C k=1 γ i y k log f k (φ l (I (A i ) ; θ l )) ,<label>(7)</label></formula><p>where f k (φ l <ref type="figure">(I; θ l )</ref>) is the probability that image I belongs to the k-th class, f k (φ l (I (A i ) ; θ l )) is the probability that image patch I (A i ) belongs to the k-th class, and γ i is a constant weight for the i-th patch. Here we have two settings: first, the single loss sets γ i = 0 (i = 2, ..., n + 2), and keeps only one loss at the start of the sequence; second, the multiple losses sets γ i = 1 (i = 2, ..., n + 2). Experimental results indicate that, in comparison to a single loss for the last output from the second LSTM, multiple losses used here improve classification accuracy by a significant margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>All experiments have been conducted on NVIDIA TITAN X(Maxwell) GPUs with 12GB memory using Caffe <ref type="bibr" target="#b19">[20]</ref>. No annotated parts are used. n is set to 9 for all experiments.</p><p>In the mask initialization stage, we fine-tune from Ima-geNet pre-trained GoogleNet with batch normalization <ref type="bibr" target="#b18">[19]</ref> on target datasets. The initial learning rate is 0.001 and is divided by 10 after every 40000 iterations with the standard SGD optimizer. Training converges after 70000 iterations. In the Mask R-CNN refinement process, we adopt ResNet-50 with Feature Pyramid Network (FPN) as the backbone and pre-train the network on the COCO dataset following the same setting described in <ref type="bibr" target="#b15">[16]</ref>. We then fine-tune the model on our target datasets. During training, image-centric training is used and the input images are resized such that their shorter side is 800 pixels. Each mini-batch contains 1 image per GPU and each image has 512 sampled ROIs. The model is trained on 4 GPUs for 150k iterations with an initial learning rate 0.001, which is divided by 10 at 120k iterations. We use the standard SGD optimizer and a weight decay of 0.0001. The momentum is set to 0.9. Unless specified, the settings we use for different algorithms follow their original settings respectively <ref type="bibr" target="#b55">[55,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b15">16]</ref>. For the last stage, we adopt GoogleNet with batch normalization <ref type="bibr" target="#b18">[19]</ref> as the backbone network for Stanford Dogs 120 and Caltech-UCSD Birds 2011-200 datasets and the Caltech256 dataset. First, we fine-tune the pretrained network on the target dataset with the generated object parts.  The parameters are the same as those used in the first stage. Next, we build a Stacked LSTM module and treat the features of the n + 2 image patches as training sequences. We train the model with 4 GPUs and set the learning rate to 0.001, which is decreased by a factor of 10 for every 8000 iterations. We adopt the standard SGD optimizer, momentum is set to 0.9, and the weight decay is 0.0002. Training converges at 16000 iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Fine-grained Image Classification</head><p>Stanford Dogs 120. Stanford Dogs 120 contains 120 categories of dogs. There are 12000 images for training, and 8580 images for testing. The training procedure follows the steps described in Section 4.1.</p><p>To perform fair comparisons with existing state-of-theart algorithms, we divide our experiments into two groups. The first group consists of algorithms that use the original training data only and the second group is composed of methods that use extra training data. In each group, we set our baseline accordingly. In the first group, we directly fine-tune the GoogleNet pretrained on ImageNet with the input image size set to 448 x 448, which is adopted by other algorithms <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b38">39]</ref> in the comparison and the classification accuracy achieved is 85.2%. This serves as our baseline model and we then add the proposed stacked LSTM over a complementary parts model. Our stacked LSTM is trained with both single loss and multiple losses, which achieves a classification accuracy of 92.4% and 93.9% respectively. Both of our proposed variants outerperform existing stateof-the-art by a clear margin. In the second group, we perform selective joint fine-tuning (SJFT) with images retrieved from ImageNet, and the input image size is set to 224 x 224 to obtain our baseline network. The classification accuracy of our baseline is 92.1%, 1.8% higher than the SJFT with ResNet-152 counterpart. With our stacked Method Accuracy(%)</p><p>MAMC <ref type="bibr" target="#b38">[39]</ref> 85.2 Inception-v3 <ref type="bibr" target="#b23">[24]</ref> 85.9 RA-CNN <ref type="bibr" target="#b10">[11]</ref> 87.3 FCAN <ref type="bibr" target="#b29">[30]</ref> 88 the first group, no extra training data is used. Our baseline model in this group is a directly fine-tuned GoogleNet model that achieves a classification accuracy of 82.6%. We then add the Stacked LSTM module and train the model with both single loss and multiple losses, which achieves a classification accuracy of 87.6% and 90.3% respectively, outperforming all other algorithms in this comparison <ref type="bibr" target="#b54">[54,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b26">27]</ref>. Compared to HSNet, our model does not use any parts annotations in the training stage while HSNet is trained with groundtruth parts annotations. In the second group, our baseline model still uses GoogleNet as the backbone and performs SJFT with images retrieved from ImageNet. It achieves a classification accuracy of 82.8%. By adding the Stacked LSTM module, the accuracy of the model trained with single loss is 87.7% and the model trained with multiple losses is 90.4%. When the top performing result in the first group is compared to that of the second group, it can be concluded that SJFT contributes little to the performance gain (0.1% gains) and our proposed method is effective and solid, contributing much to the final performance (7.7% higher than the baseline). It is worth noting that, in <ref type="bibr" target="#b3">[4]</ref>, a subset of ImageNet and iNaturalist <ref type="bibr" target="#b43">[44]</ref> most similar to CUB200 are used for training, and in <ref type="bibr" target="#b23">[24]</ref>, a large amount of web data are also used in the training phase. In <ref type="table">Table 3</ref>, as described previously, we conduct our experiments under two settings. For the first setting, no extra training data is used. We fine-tune the pretrained GoogleNet on the target dataset and treat the fine-tuned model as our baseline model, which achieves a classification accuracy of 84.1%. By adding our proposed Stacked LSTM module, the accuracy is increased by a large margin to 90.1% for Single Loss and to 93.5% for multiple losses respectively, outerperforming all methods listed in the table. Also, it is 4.1% higher than its ResNet-152 counterpart. For the second setting, we adopt SJFT <ref type="bibr" target="#b12">[13]</ref> with GoogleNet as our baseline model, which achieves a classification accuracy of 86.3%. Then we add our proposed Stacked LSTM module and the final performance is increased by 3.8% for single loss and 8.0% for multiple losses. Our method with GoogleNet as backbone network outerperfoms current state-of-the-art by 5.2%, demonstrating that our proposed algorithm is solid and effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Generic Object Recognition</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>Ablation Study on Complementary Parts Mining. The ablation study is performed on the CUB200 dataset with GoogleNet as the backbone network. The classification accuracy of our reference model with n = 9 parts on this dataset is 90.3%. First, when the number of parts n is set to <ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16</ref>, and 20 in our model, the corresponding classification accuracy is respectively 85.3%, 87.9%, 89.1%, 90.3%, 87.6%, 86.8% and 85.9%. Obviously the best result is achieved when n = 9. Second, if we use object features only in our reference model, the classification accuracy drops to 90.0%. Third, if we use image features only, the performance drops to 82.8%. Fourth, if we simply use the uniform grid cells as the object parts without further optimization, the performance drops to 78.3%, which indicates our search for the best parts model plays an important role in escalating the performance. Fifth, instead of a grid-based object parts initialization, we randomly sample n = 9 suppressed object proposals around the bounding box of the surviving proposal, and the performance drops to 86.9%. Lastly, we discover that the part order in LSTM does not matter. We randomly shuffle the part order during training and testing, and the classification accuracy remains the same.</p><p>Ablation Study on Context Fusion.We perform an ablation study on Stanford Dogs 120 for the context fusion stage. We first replace the multiple losses with the single loss and the accuracy drops from 93.9% to 92.4%. This suggests that multiple losses help regularize the training process and produce more discriminative features for image classification. We then keep the multiple losses setting in subsequent experiments. Second, the Stacked LSTM module is removed and we conduct experiments with two settings, a feature concatenation module and an averaging module. In the feature concatenation module, the features of all the n + 2 parts are concatenated. And in the averaging module, the classification output of multiple features are summed. The classification accuracies achieved are decreased by 5.8% and 8.7% respectively. The performance drop suggests that fusing n + 2 image patches through LSTM is helpful for final image classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Inference Time Complexity.</head><p>The inference time of our implementation is summarised as follows: in the complementary parts model search phase, the time for processing an image with its shorter edge set to 800 pixels is around 277ms; in the context encoding phase, the running time on an image of size 448 × 448 is about 63ms, and on an image of size 224 × 224 is about 27ms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we have presented a new pipeline for finegrained image classification, which is based on a complementary part model. Different from previous work which focuses on learning the most discriminative parts for image classification, our scheme mines complementary parts that contain partial object descriptions in a weakly supervised manner. After getting object parts that contain rich information, we fuse all the mined partial object descriptions with bi-directional stacked LSTM to encode these complementary information for classification. Experimental results indicate that the proposed method is effective and outperforms existing state-of-theart by a large margin. Nevertheless, how to build the complementary part model in a more efficient and accurate way remains an open problem for further investigation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Visualization of class activation map (CAM<ref type="bibr" target="#b55">[55]</ref>) and weakly supervised object detections.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>CNN</head><label></label><figDesc>Feature Extractor Fine-tuning. Given an input image I and the parts model A = [A 1 , ..., A n , A n+1 ] constructed in the previous stage, the image patches corresponding to the parts are denoted as I (A) = [I (A 1 ) , I (A 2 ) , ..., I (A n ) , I (A n+1 )]. During image classification, random crops of images are often used to train the model. Thus, apart from the (n+1) patches, we ap-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Context encoded image classification based on LSTMs. Two standard LSTMs<ref type="bibr" target="#b17">[18]</ref> are stacked together. They have opposite scanning orders.pend a random crop of the original image as the (n + 2)-nd image patch. The motivation for adding a randomly cropped patch is to include more context information during training since those patches corresponding to object parts primarily focus on the object itself. Every patch shares the same label with the original image it is cropped from. All patches from all the original training images form a new training set, which is used to fine-tune a CNN model pretrained on Ima-geNet. This fine-tuned model serves as the feature extractor for all image patches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig 3 .</head><label>3</label><figDesc>First, the (n + 2) patches from a complementary parts model are fed through the CNN feature extractor φ c (·; θ c ) trained in the previous step. The output from this step is denoted as Ψ (I) = [φ c (I; θ c ) , φ c (I (A 1 ) ; θ c ) , ..., φ c (I (A n+2 ) ; θ c )]. Next, we build a two-layer stacked LSTM to fuse the extracted features Ψ (I). The hidden state of the first LSTM is fed into the second LSTM layer, but the second LSTM follows the reversed order of the first one. Let D(= 256)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Example intermediate results of Mask R-CNN training are shown in Fig 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Label Map for CRF (d) CRF Segmentation (e) Initialized Mask (f) Detections (h) Object Heatmap (i) Label Map for CRF (j) CRF Result (g) Masks</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 .</head><label>4</label><figDesc>Example intermediate results for training Mask R-CNN. First row: pseudo object mask and object bounding box are generated with CAM and CRF refinement. Second row: With previous pseudo groundtruth generated, object mask and object bounding box are further refined with Mask R-CNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Weakly Supervised Instance Detection and Segmentation NMS</head><label></label><figDesc></figDesc><table><row><cell cols="3">Conv 1 Stage 1: Redundant Object Proposals Pool 1 Conv 2 Class Activation Map Initial Segmentation CRFs Stage 2: Complementary Parts Model</cell><cell>Pool 2</cell><cell>Conv 3</cell><cell>Pool 3 Mask R-CNN Conv 4 Pool 4</cell><cell>ROI Align ROI Align</cell><cell>Detection Mask Constrained Object Part Model RCNN Conv Probability Map</cell><cell>Instance Segmentation CRFs</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Searching</cell></row><row><cell></cell><cell>CNN</cell><cell>CNN</cell><cell cols="2">CNN</cell><cell></cell><cell></cell><cell>CNN</cell><cell>CNN</cell></row><row><cell>C 0</cell><cell>LSTM 1</cell><cell>LSTM 1</cell><cell cols="2">LSTM 1</cell><cell></cell><cell>…</cell><cell>LSTM 1</cell><cell>LSTM 1</cell></row><row><cell></cell><cell>LSTM 2</cell><cell>LSTM 2</cell><cell cols="2">LSTM 2</cell><cell></cell><cell></cell><cell>LSTM 2</cell><cell>LSTM 2</cell></row><row><cell></cell><cell>Softmax</cell><cell>Softmax</cell><cell cols="2">Softmax</cell><cell></cell><cell></cell><cell>Softmax</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Stage 3: Image Classification with Context Encoding</head><label></label><figDesc>The proposed image classification pipeline based on weakly supervised complementary parts model. From top to bottom: (a) Weakly Supervised Object Detection and Instance Segmentation: The first step initializes the segmentation probability map by CAM [55], and obtaining coarse instance segmentation maps by CRF [40]. Then the segments and bounding boxes are used as groundtruth annotations for training Mask R-CNN [16] in an iterative manner. (b) Complementary Parts Model: Search for complementary object proposals to form the object parts model. (c) Image Classification with Context Encoding: Two LSTMs</figDesc><table><row><cell></cell><cell>…</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Input</cell><cell>Weakly Supervised Object Detection</cell><cell>Detections</cell><cell>Complementary Part Model</cell><cell>Complementary Parts</cell></row></table><note>… Figure 2.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Classification results on Stanford Dogs 120. Two sections are divided by the horizontal separators, namely (from top to bottom) Experiments without SJFT and Experiments with SJFT.LSTM plugged in and trained with both single loss and multiple losses, the performance is further boosted to 96.3% and 97.1% respectively, surpassing the current state of the art by 6% and 6.8%.</figDesc><table><row><cell>.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Caltech 256 .</head><label>256</label><figDesc>There are 256 object categories and 1 background cluster class in Caltech 256. A minimum number of 80 images per category are provided for training, validation and testing. As a convention, results are reported with the number of training samples per category falling between 5 and 60. We follow the same convention and report the result with the number of training sample per category set to 60. In this experiment, GoogleNet is adopted as our backbone network and the input image size is 224 x 224. We train our model with mini-batch size set to 8 on each GPU.</figDesc><table><row><cell>Method</cell><cell>Accuracy(%)</cell></row><row><cell>ZF Net [50]</cell><cell>74.2±0.3</cell></row><row><cell>VGG-19 + VGG-16 [36]</cell><cell>86.2±0.3</cell></row><row><cell>VGG-19 + GoogleNet +AlexNet [22]</cell><cell>86.1</cell></row><row><cell>L 2 -SP [28]</cell><cell>87.9±0.2</cell></row><row><cell>GoogleNet (our baseline)</cell><cell>84.1±0.2</cell></row><row><cell>baseline + Stacked LSTM + Single-Loss</cell><cell>90.1±0.2</cell></row><row><cell>baseline + Stacked LSTM + Multi-Loss</cell><cell>93.5±0.2</cell></row><row><cell>SJFT with ResNet-152 [13]</cell><cell>89.1±0.2</cell></row><row><cell>SJFT with GoogleNet (our baseline)</cell><cell>86.3±0.2</cell></row><row><cell>baseline + Stacked LSTM + Single-Loss</cell><cell>90.1±0.2</cell></row><row><cell>baseline + Stacked LSTM + Multi-Loss</cell><cell>94.3±0.2</cell></row><row><cell cols="2">Table 3. Classification results on Caltech 256. Two sections are</cell></row><row><cell cols="2">divided by the horizontal separators, namely (from top to bottom)</cell></row><row><cell cols="2">Experiments without SJFT and Experiments with SJFT.</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On pixel-wise explanations for nonlinear classifier decisions by layer-wise relevance propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Klauschen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">130140</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Network dissection: Quantifying interpretability of deep visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3319" to="3327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Level set segmentation with multiple regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weickert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Large scale fine-grained categorization and domain-specific transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<idno>abs/1806.06193</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Weakly supervised cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Diba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Pazandeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="5131" to="5139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Wildcat: Weakly supervised learning of deep convnets for image classification, pointwise localization and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Weakly supervised learning of deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weldon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4743" to="4752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Blitznet: A real-time deep network for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dvornik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shmelkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A discriminatively trained, multiscale, deformable part model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>CVPR 2008. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Look closer to see better: Recurrent attention convolutional neural network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="4476" to="4484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-evidence filtering and fusion for multi-label classification, object detection and semantic segmentation based on weakly supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Borrowing treasures from the wealthy: Deep transfer learning through selective joint fine-tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Caltech-256 object category dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Holub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
	<note>Computer Vision (ICCV</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Multimedia</title>
		<meeting>the 22nd ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Novel dataset for fine-grained image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jayadevaprakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">First Workshop on Fine-Grained Visual Categorization, IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Colorado Springs, CO</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning to select pretrained deep representations with bayesian evidence framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-27" />
			<biblScope unit="page" from="5318" to="5326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>J. Shawe-Taylor, R. S. Zemel, P. L. Bartlett, F. Pereira, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">The unreasonable effectiveness of noisy data for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<idno>abs/1511.06789</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fine-grained recognition as hsnet search for informative image parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mahasseni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07-21" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fine-grained recognition as hsnet search for informative image parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mahasseni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07-21" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="6497" to="6506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Explicit inductive bias for transfer learning with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Davoine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4" to="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Fully convolutional attention localization networks: Efficient attention localization for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<idno>abs/1603.06765</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Beyond holistic object recognition: Enriching image understanding with part states</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Features for multi-target multicamera tracking and re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Neural activation constellations: Unsupervised part model discovery with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1143" to="1151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning spatialaware regressions for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multi-attention multi-class constraint for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018 -15th European Conference</title>
		<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="834" to="850" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XVI</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">An introduction to conditional random fields. Foundations and Trends R in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Robust object tracking based on temporal and spatial deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The inaturalist species classification and detection dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Salt Lake City, UT</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Weakly-supervised semantic segmentation by iteratively mining common object features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning a discriminative filter bank within a cnn for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Multi-label image recognition by recurrently discovering attentional regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="464" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Caltech-UCSD Birds 200</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<idno>CNS-TR-2010-001</idno>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Hierarchical bilinear pooling for fine-grained visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2014 -13th European Conference</title>
		<meeting><address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Partbased r-cnns for fine-grained category detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<biblScope unit="page" from="834" to="849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Deformable part descriptors for fine-grained recognition and attribute prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Farrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="729" to="736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning multi-attention convolutional neural network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-10-22" />
			<biblScope unit="page" from="5219" to="5227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Learning multi-attention convolutional neural network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-10-22" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="5219" to="5227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
