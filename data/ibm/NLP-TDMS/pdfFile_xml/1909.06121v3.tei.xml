<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dual Graph Convolutional Network for Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Engineering Science</orgName>
								<orgName type="laboratory">Torr Vision Group</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangtai</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
							<email>aarnab@robots.ox.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Engineering Science</orgName>
								<orgName type="laboratory">Torr Vision Group</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuiyuan</forename><surname>Yang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">DeepMotion AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuiyuanyang@deepmotion</forename><surname>Ai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhai</forename><surname>Tong</surname></persName>
							<email>yhtong@pku.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Engineering Science</orgName>
								<orgName type="laboratory">Torr Vision Group</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Dual Graph Convolutional Network for Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>ZHANG ET AL.: DUAL GRAPH CONVOLUTIONAL NETWORK 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Exploiting long-range contextual information is key for pixel-wise prediction tasks such as semantic segmentation. In contrast to previous work that uses multi-scale feature fusion or dilated convolutions, we propose a novel graph-convolutional network (GCN) to address this problem. Our Dual Graph Convolutional Network (DGCNet) models the global context of the input feature by modelling two orthogonal graphs in a single framework. The first component models spatial relationships between pixels in the image, whilst the second models interdependencies along the channel dimensions of the network's feature map. This is done efficiently by projecting the feature into a new, lower-dimensional space where all pairwise interactions can be modelled, before reprojecting into the original space. Our simple method provides substantial benefits over a strong baseline and achieves state-of-the-art results on both Cityscapes (82.0% mean IoU) and Pascal Context (53.7% mean IoU) datasets. Code and models are made available to foster any further research</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semantic segmentation is a fundamental problem in computer vision, and aims to assign an object class label to each pixel in an image. It has numerous applications including autonomous driving, augmented-and virtual reality and medical diagnosis.</p><p>An inherent challenge in semantic segmentation is that pixels are difficult to classify when considered in isolation, as local image evidence is ambiguous and noisy. Therefore, segmentation systems must be able to effectively capture contextual information in order to reason about occlusions, small objects and model object co-occurrences in a scene.  Current state-of-the-art methods are all based on deep learning using fully convolutional networks (FCNs) <ref type="bibr" target="#b31">[32]</ref>. However, the receptive field of an FCN grows slowly (only linearly) with increasing depth in the network, and its limited receptive field is not able to capture longer-range relationships between pixels in an image. Dilated convolutions <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b48">49]</ref> have been proposed to remedy this. However, the resulting feature representation is dominated by large objects in the image, and consequently, performance on small objects is poor. Another direction has been to fuse multiscale features within the network <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b54">55]</ref> or to use LSTMs to propagate information spatially <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b39">40]</ref>. Recently, several methods based on self-attention <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b49">50]</ref> have also been used to learn an affinity map at each spatial position that propagates information to its neighbours. However, the memory requirements of the large affinity matrix renders these methods unsuitable for high resolution imagery (such as the Cityscapes dataset <ref type="bibr" target="#b8">[9]</ref>).</p><p>In this paper, we use graph-convolutional networks (GCNs) <ref type="bibr" target="#b18">[19]</ref> to effectively and efficiently model contextual information for semantic segmentation. GCNs have recently been applied to scene understanding tasks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b52">53]</ref>, as they are able to globally propagate information through the whole image in a manner that is conditional on the input. This provides greater representational power than methods based on Conditional Random Fields <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b56">57]</ref> which have historically been employed for semantic segmentation <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b37">38]</ref>.</p><p>As shown in <ref type="figure" target="#fig_1">Fig. 1</ref>, our proposed method consists of two primary components: the coordinate space GCN explicitly models the spatial relationships between pixels in the image, enabling our network to produce coherent predictions that consider all objects in the image, whilst the feature space GCN models interdependencies along the channel dimensions of the network's feature map. Assuming that filters in later layers of a CNN are responsive to object parts and other high-level features <ref type="bibr" target="#b50">[51]</ref>, the feature space GCN captures correlations between more abstract features in the image like object parts. After reasoning, these two complementary relation-aware feature are distributed back to the original coordinate space and added to the original feature.</p><p>Using our proposed approach, we establish new state-of-the-arts on the Cityscapes <ref type="bibr" target="#b8">[9]</ref> (82.0% mean IoU) and Pascal Context <ref type="bibr" target="#b32">[33]</ref> (53.7% mean IoU) datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Following the success of deep neural networks for image classification <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b40">41]</ref>, recent works in semantic segmentation all leverage fully-convolutional networks (FCNs) <ref type="bibr" target="#b31">[32]</ref>. A limitation of standard FCNs is their small receptive field which prevents them from taking all the contextual information in the scene into account. The DeepLab series of papers <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref> proposed atrous or dilated convolutions and atrous spatial pyramid pooling (ASPP) to increase the effective receptive field. DenseASPP improved on <ref type="bibr" target="#b5">[6]</ref> by densely connecting convolutional layers with different dilation rates. PSPNet <ref type="bibr" target="#b54">[55]</ref>, on the other hand, used a pyramid pooling module to fuse convolutional features from multiple scales. Similarly, encoder-decoder network structures <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b36">37]</ref> combine features from early-and late-stages in the network to fuse mid-level and high-level semantic features. Deeplab-v3+ <ref type="bibr" target="#b6">[7]</ref> also followed this approach by fusing lower-level features into its decoder. Di et al. <ref type="bibr" target="#b26">[27]</ref> also recursively, locally fused feature maps of every two levels in a feature pyramid into one.</p><p>Another approach has been proposed to more explicitly account for the relations between all pixels in the image. DAG-RNN <ref type="bibr" target="#b39">[40]</ref> models a directed acyclic graph with a recurrent network that propagates information. PSANet <ref type="bibr" target="#b55">[56]</ref> captures pixel-to-pixel relations using an attention module that takes the relative location of each pixel into account. On the other hand, EncNet <ref type="bibr" target="#b51">[52]</ref> and DFN <ref type="bibr" target="#b46">[47]</ref> use attention along the channel dimension of the convolutional feature map to account for global context such as the co-occurrences of different classes in the scene.</p><p>Following on from these approaches, graph neural networks <ref type="bibr" target="#b18">[19]</ref> have also been used to model long-range context in the scene. Non-local networks <ref type="bibr" target="#b42">[43]</ref> applied this to video understanding and object detection by learning an affinity map between all pixels in the image or video frames. This allowed the network to effectively increase its receptive field to the whole image. The non-local operator has been applied to segmentation by OCNet <ref type="bibr" target="#b49">[50]</ref> and DANet <ref type="bibr" target="#b13">[14]</ref> recently. However, these methods have a (sometimes prohibitively) high memory cost as the affinity matrix grows quadratically with the number of pixels in the image. To bypass this problem, several works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref> have modelled the dependencies between regions of the image rather than individual pixels. This is done by aggregating features from the original "co-ordinate space" to a lower-dimensional intermediate representation, performing graph convolutions in this space, and then reprojecting back onto the original co-ordinate space.</p><p>However, differently from these recent GCN methods, we propose the Dual Graph Convolutional Network (DGCNet) to model the global context of the input feature by considering two orthogonal graphs in a single general framework. Specifically, with different mapping strategies, we first project the feature into a new coordinate space and a non-coordinate (feature) space where global relational reasoning can be computed efficiently. After reasoning, two complementary relation-aware features are distributed back to the original coordinate space and added to the original feature. The refined feature thus contains rich global contextual information and can be further provided to the following layers to learn better task-specific representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>In this section, we first revisit the graph convolutional network in Sec. 3.1 and then introduce the formulation of our proposed DGCNet in Sec. 3.2 and 3.3. Finally, we detail the resulting </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminaries</head><p>Revisiting the graph convolution. Assume an input tensor X ∈ R N×D , where D is the feature dimension and N = H × W is the number of locations defined on regular grid coordinates Ω = {1, ..., H} × {1, ...,W }. In standard convolution, information is only exchanged among positions in a small neighborhood defined by the filter size (e.g., typically 3 × 3). In order to create a large receptive field and capture long range dependencies, one needs to stack numerous layers after each other, as done in common architectures <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b40">41]</ref>. Graph convolution <ref type="bibr" target="#b18">[19]</ref>, is a highly efficient, effective and differentiable module that generalises the neighborhood definition used in standard convolution, and allows long-range information exchange in a single layer. This is done by defining edges E among nodes V in a graph G. Formally, following <ref type="bibr" target="#b18">[19]</ref>, graph convolution is defined as</p><formula xml:id="formula_0">X = σ (AXW),<label>(1)</label></formula><p>where σ (·) is the non-linear activation function, A ∈ R N×N is the adjacency matrix characterising the neighbourhood relations of the graph and W ∈ R D×D is the weight matrix. Clearly, the graph definition and structure play a key role in determining the information propagation. Our proposed framework is motivated by building orthogonal graph spaces via different graph projection strategies to learn a better task-specific representation. As summarised in <ref type="figure" target="#fig_2">Fig. 2</ref>, we now describe how we propagate information in the coordinate space in Sec. 3.2, and in feature space in Sec. 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Graph convolution in coordinate space</head><p>Coordinate space projection. We first project the input feature into a new coordinate space Ω S . In general, we adopt a spatial downsampling operation H S to transform the input feature</p><formula xml:id="formula_1">X to a new feature V S ∈ R H×W d 2 ×D in the new coordinate space Ω S , where the d denotes the downsample rate, V S = H S X.<label>(2)</label></formula><p>We consider two different operations for H S : <ref type="bibr" target="#b0">(1)</ref>. Parameter-free operation. We take the downsampling operation H S to be average pooling which requires no additional learnable parameters. <ref type="bibr" target="#b1">(2)</ref>. Parameterized operation. For efficiency, a downsampling rate of d is achieved by chaining log 2 (d) depth-wise convolution layers, each with a stride of 2 and kernel size of 3 × 3. Coordinate graph convolution. After projecting the features into the new coordinate space Ω S , we can build a lightweight fully-connected graph with adjacency matrix A S ∈ R H d × W d for diffusing information across nodes. Note that the nodes of the graph aggregate information from a "cluster" of pixels, and the edges measure the similarity between these clusters.</p><p>The global relational reasoning is performed on the downsampled feature V S to model the interaction between the features of the corresponding nodes. In particular, we adopt three learnable linear transformations (δ , ψ, υ) on the feature V S to produce the message, M S</p><formula xml:id="formula_2">M S = f (δ (V S ), ψ(V S ) )υ(V S )W S ,<label>(3)</label></formula><p>where f (δ (V S ), ψ(V S ) ) gives the adjacency matrix in Eq. 1, f is the dot-product operation.</p><p>Reprojection. After the reasoning, we map the new features M S back into the original coordinate space (R H×W ×D ) to be compatible with a regular CNN. Opposite to the downsample operation H S used in graph projection, we simply perform upsampling as the reprojection operation. In particular, nearest neighbour interpolation, interp(·), is adopted to resize M S to the original spatial input size (N). Hence, the output map is computed asX</p><formula xml:id="formula_3">S = ξ (interp(M S )), where ξ (·) is a 1 × 1 convolution that transforms M S into the channel dimension D.</formula><p>Discussion. Our coordinate GCN is built in a coarser spatial grid and its size is determined by the downsampling rate d (we usually set d = 8, the effect of changing the downsample rate is analysed in Section 4.2.1). Compared to the Non-local operator <ref type="bibr" target="#b42">[43]</ref> that needs to build a large fully-connected graph with adjacency matrix A ∈ R HW ×HW , our method is significantly more efficient. Moreover, by re-ordering Eq. 3 to M S = δ (V S )(ψ(V S ) υ(V S ))W S (following the associative rule), we can obtain large savings in terms of memory and computation (from O((HW ) 2 ) to O(HW )).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Graph convolution in feature space</head><p>Given that the coordinate space GCN explicitly models the spatial relationships between pixels in the image, we now consider projecting the input feature into the feature space F. The coordinate space GCN enables our network to produce coherent predictions that consider all objects in the image, whilst the feature space GCN models interdependencies along the channel dimensions of the network's feature map. Assuming that filters in later layers of a FCN are responsive to object parts and other high-level features <ref type="bibr" target="#b50">[51]</ref>, then the feature space GCN captures correlations between more abstract features in the image like object parts. Feature space projection. In practice, we first reduce the dimension of the input feature X with function θ (X) ∈ R N×D 1 and formulate the projection function ϕ(X) = H F ∈ R N×D 2 as a linear combination of input X such that the new features can aggregate information from multiple regions. Formally, the input feature X is projected to a new feature V F in the feature space F via the projection function H F . Thus we have</p><formula xml:id="formula_4">V F = H F θ (X) = ϕ(X)θ (X),<label>(4)</label></formula><p>where both functions of θ (·) and ϕ(·) are implemented with 1 × 1 convolutional layer. This results in a new feature V F ∈ R D 2 ×D 1 , which consists of D 2 nodes, each of dimension D 1 . Feature graph convolution. After projection, we can build a fully-connected graph with adjacency matrix A F ∈ R D 2 ×D 2 in the feature space F, where each node contains the feature descriptor. Following Eq. 1, we have</p><formula xml:id="formula_5">M F = (I − A F )V F W F ,<label>(5)</label></formula><p>where W F ∈ R D 1 ×D 1 denotes the layer-specific trainable edge weights. We consider Laplacian smoothing <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b21">22]</ref> by updating the adjacency matrix to (I − A F ) to propagate the node features over the graph. The identity matrix I serves as a residual sum connection in our implementation that alleviates optimisation difficulties. Both adjacency matrix A F and W F are randomly initialised and optimised by gradient descent during training in an end-to-end fashion.</p><p>Reprojection. As in Sec. 3.2, after the reasoning, we map the new features M F back into the original coordinate space with outputX F ∈ R N×D to be compatible with regular convolutional neural networks,X</p><formula xml:id="formula_6">F = φ (H F M F ).<label>(6)</label></formula><p>This is done by first reusing the projection matrix H F and then performing a linear projection (e.g., 1 × 1 convolution layer) to transformM F into the original coordinate space. As a result, we have the featureX F with feature dimension of D at each grid coordinate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">DGCNet</head><p>The final refined feature is computed asX = X +X S +X F , where "+" denotes point-wise summation. To this end, we can easily incorporate our proposed module into existing backbone CNN architectures (e.g., ResNet-101). <ref type="figure" target="#fig_2">Figure 2</ref> shows the schematic illustration of our proposed DGCNet . Implementation of DGCNet. We insert our proposed module between two 3 × 3 convolution layers (both layers output D = 512 channels) appended at the end of a Fully Convolutional Network (FCN) for the task of semantic segmentation. Specifically, we use an ImageNet pretrained ResNet-101 as backbone network, removing the last pooling and FC layer. Our proposed module is then random initialised. Dilated convolution and multi-grid strategies <ref type="bibr" target="#b13">[14]</ref> are adopted in last two stages of the backbone. We simply set D 1 = D 2 and D 2 = D 4 in our implementation. We add a synchronised batch normalisation (BN) layer and ReLU nonlinearity after each convolution layer in our proposed module, except for the convolution layers in the coordinate space GCN (i.e., there are no BN and ReLU operations in the coordinate space GCN defined in Sec. 3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>To evaluate our proposed method, we carry out comprehensive experiments on the Cityscapes <ref type="bibr" target="#b8">[9]</ref> and PASCAL Context <ref type="bibr" target="#b32">[33]</ref> datasets, where we achieve state-of-art performance. We describe our experimental setup in Sec. 4.1, before presenting experiments on the Cityscapes dataset (on which we also perform ablation studies) in Sec. 4.2 and finally Pascal Context in Sec. 4.3.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental setup</head><p>Cityscapes: This dataset <ref type="bibr" target="#b8">[9]</ref> densely annotates 19 object categories in urban scenes captured from cameras mounted on a car. It contains 5000 finely annotated images, split into 2975, 500 and 1525 for training, validation and testing respectively. The images are all captured at a high resolution of 2048 × 1024. PASCAL Context: This dataset <ref type="bibr" target="#b32">[33]</ref> provides detailed semantic labels for whole scenes (both "thing" and "stuff" classes <ref type="bibr" target="#b12">[13]</ref>), and contains 4998 images for training and 5105 images for validation. Following previous works which we compare to, we evaluate on the most frequent 59 classes with along with one background category (60 classes) <ref type="bibr" target="#b32">[33]</ref>. Implementation details: We implement our method using Pytorch. Following <ref type="bibr" target="#b54">[55]</ref>, we use momentum and adopt a polynomial learning rate decay schedule where the initial learning rate is multiplied by (1 − iter total_iter ) 0.9 . The initial learning rate is set to 0.01 for Cityscapes and 0.001 for Pascal Context. Momentum and weight decay coefficients are set to 0.9 and 0.0001 respectively. For data augmentation, we apply random cropping (crop size with 769) and random left-right flipping during training for Cityscapes. For the Pascal Context dataset, our crop size is 480. We also use synchronised batch normalisation for better estimation of the batch statistics. Metrics: Following the common procedure of <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b57">58]</ref>, we report the mean Intersection over Union (IoU), averaged over all classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiments on Cityscapes</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Ablation Studies</head><p>Effect of proposed modules: As shown in <ref type="table" target="#tab_0">Table 1a</ref>, our proposed GCN modules substantially improve performance. Compared to our baseline FCN with dilated convolutions (with the ResNet-101 backbone), appending the Channel-GCN module obtains a mean IoU of 79.3%, which is an improvement of 4.1%. Similarly, the Spatial-GCN module on its own improves the baseline by 3.6%. The best results are obtained by combining the two modules together, resulting in a mean IoU of 80.5%. The effect of our modules are visualised in <ref type="figure" target="#fig_5">Fig. 6</ref>. The contextual information captured by our graph module improves the consistency of our results leading to fewer artifacts in the prediction. Effect of additional training and inference strategies: It is common to use additional "tricks" to improve results for semantic segmentation benchmarks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b54">55]</ref>. <ref type="table" target="#tab_0">Table  1b</ref> shows that our proposed GCN module is complementary to them, and incorporating these strategies improves our results too. Specifically, we consider 1) Online Hard Example Mining (OHEM) <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b49">50]</ref> where the loss is only computed on the K pixels with the highest loss in the image. Following <ref type="bibr" target="#b49">[50]</ref>, we used K = 10 5 in a 769 × 769 cropped training image 2) Multi-Grid <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b58">59</ref>] employs a hierarchy of convolutional filters of different dilation rates <ref type="bibr">(4, 8 and 16)</ref> in the last ResNet block. 3) Multi-Scale (MS) ensembling is commonly used at inference time to improve results <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b55">56]</ref> We average the segmentation probability maps from 6 image scales{0.75, 1 1.25, 1.5, 1.75, 2} during inference.</p><p>As shown in <ref type="table">Table.</ref> 1b each of these strategies provides consistent improvements to our overall results. Using these strategies, we compare to the state-of-art in the following subsection.</p><p>Computational cost analysis <ref type="table" target="#tab_1">Table 2a</ref> shows that our proposed method costs significantly fewer floating point operations than the related work of DANet <ref type="bibr" target="#b13">[14]</ref>, but still achieves higher performance. Effect of mapping strategies: As mentioned in Sec. 3.2, different mapping strategies are possible to build the coordinate space graph. We present the effect of two strategies -average pooling and strided convolution -for different downsampling ratios, d, in Tab. 2b. It is interesting to observe that the model with average pooling achieves similar performance as strided convolution, even slightly outperforming it for d = 4. One reason may be that the model with the parameter-free mapping strategy can overfit less than the one using the strided convolution operation. Note that both strategies are robust to the choice of d, and similar performance is obtained for d = 4, 8 and 16. The final model that we use for comparing to ResNet-101 79.1 BiSeNet <ref type="bibr" target="#b45">[46]</ref> ResNet-101 78.9 PSANet <ref type="bibr" target="#b55">[56]</ref> ResNet-101 80.1 DFN <ref type="bibr" target="#b46">[47]</ref> ResNet-101 79.3 DepthSeg <ref type="bibr" target="#b19">[20]</ref> ResNet-101 78.2 DenseASPP <ref type="bibr" target="#b44">[45]</ref> ResNet-101 80.6 GloRe <ref type="bibr" target="#b7">[8]</ref> ResNet-101 80.9 DANet <ref type="bibr" target="#b13">[14]</ref> ResNet-101 81.5 OCNet <ref type="bibr" target="#b49">[50]</ref> ResNet    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experiments on Pascal Context</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We proposed a graph-convolutional module to model the contextual relationships in an image, which is critical for dense prediction tasks such as semantic segmentation. Our method consists of two branches, one to capture context along the spatial dimensions, and another along the channel dimensions, in a convolutional feature map. Our proposed approach provides significant improvements over a strong baseline, and achieves state-of-art results on the Cityscapes and Pascal Context datasets. Future work is to address other dense prediction tasks such as instance segmentation and depth estimation.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Feature</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Our proposed DGCNet exploits contextual information across the whole image by proposing a graph convolutional network to efficiently propagate information along both the spatial and channel dimensions of a convolutional feature map.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of our proposed DGCNet. Our method consists of two branches, which each consist of a graph convolutional network (GCN) to model contextual information in the spatial-and channel-dimensions in a convolutional feature map, X.network architecture in Sec. 3.4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Cityscapes results compared with Dilated FCN ResNet101 baseline<ref type="bibr" target="#b48">[49]</ref>. Red boxes show our method can handle inconsistent results within the same object. Best view in color. More results in supplementary material.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc><ref type="bibr" target="#b5">[6]</ref> 70.4 97.9 81.3 90.3 48.8 47.4 49.6 57.9 67.3 91.9 69.4 94.2 79.8 59.8 93.7 56.5 67.5 57.5 57.7 68.8 RefineNet [29] 73.6 98.2 83.3 91.3 47.8 50.4 56.1 66.9 71.3 92.3 70.3 94.8 80.9 63.3 94.5 64.6 76.1 64.3 .5 85.5 92.8 58.6 55.5 65 73.5 77.9 93.3 72 95.2 84.8 68.5 95.4 70.9 78.8 68.7 65.9 73.8 ResNet-38 [44] 78.4 98.5 85.7 93.1 55.5 59.1 67.1 74.8 78.7 93.7 72.6 95.5 86.6 69.2 95.7 64.5 78.8 74.1 ] 80.6 98.7 87.1 93.4 60.7 62.7 65.6 74.6 78.5 93.6 72.5 95.4 86.2 71.9 96.0 78.0 90.3 80.7 69.7 .6 86.1 93.5 56.1 63.3 69.7 77.3 81.3 93.9 72.9 95.7 87.3 72.9 96.2 76.8 89.4 86.5 72.2 78.2 Ours 82.0 98.7 87.4 93.9 62.4 63.4 70.8 78.7 81.3 94.0 73.3 95.8 87.8 73.7 96.4 76.0 91.6 81.6 71.5 78.2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Comparison of our results on Pascal Context to the state-of-art EncNet [52] method. Note how our results are more consistent and have fewer artifacts. Best view in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Ablation studies on (a) the proposed components of our network and (b) additional training and inference strategies. All methods use the ResNet-101 backbone, and are evaluated using the mean IoU on the Cityscapes validation set. Refer to Sec. 4.2.1 for additional details.</figDesc><table><row><cell></cell><cell>Backbone</cell><cell>Coord. GCN Feature GCN</cell><cell>mIoU (%)</cell><cell>OHEM</cell><cell>Multi-grid</cell><cell>MS</cell><cell>mIoU (%)</cell></row><row><cell>Dilated FCN</cell><cell>ResNet-101</cell><cell></cell><cell>75.2</cell><cell>DGCNet</cell><cell></cell><cell></cell><cell>79.5</cell></row><row><cell>GCN</cell><cell>ResNet-101</cell><cell></cell><cell>78.8</cell><cell>DGCNet</cell><cell></cell><cell></cell><cell>79.8</cell></row><row><cell>GCN</cell><cell>ResNet-101</cell><cell></cell><cell>79.3</cell><cell>DGCNet</cell><cell></cell><cell></cell><cell>80.5</cell></row><row><cell>DGCNet</cell><cell>ResNet-101</cell><cell></cell><cell>80.5</cell><cell>DGCNet</cell><cell></cell><cell></cell><cell>81.8</cell></row><row><cell cols="4">(a) Comparison of different graph modules.</cell><cell cols="4">(b) Additional training and inference strategies.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Ablation studies on (a) computational cost (input size [1 × 512 × 128 × 128] and (b) graph projection strategy. All methods are evaluated using the mean IoU at a single scale, using the ResNet-101 backbone on the Cityscapes validation set.</figDesc><table><row><cell></cell><cell>GFLOPs</cell><cell>#Params</cell><cell>mIoU</cell><cell>Downsample rate</cell><cell>d=4</cell><cell>d=8</cell><cell>d=16</cell></row><row><cell>DA module [14]</cell><cell>24.87</cell><cell>1 496 224</cell><cell>79.8</cell><cell>Avg. pooling</cell><cell>80.2</cell><cell>80.5</cell><cell>80.5</cell></row><row><cell>DGC module (Ours)</cell><cell>14.15</cell><cell>1 240 704</cell><cell>80.5</cell><cell>Stride conv.</cell><cell>80.0</cell><cell>80.5</cell><cell>80.5</cell></row><row><cell cols="3">(a) Comparison on computation cost.</cell><cell></cell><cell cols="4">(b) Ablation on graph projection strategies.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>State-of-the-art comparison on the Cityscapes test set.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>mIoU (%)</cell></row><row><cell>PSPNet [55]</cell><cell>ResNet-101</cell><cell>78.4</cell></row><row><cell>PSANet [56]</cell><cell>ResNet-101</cell><cell>78.6</cell></row><row><cell>OCNet [50]</cell><cell>ResNet-101</cell><cell>80.1</cell></row><row><cell>DGCNet (Ours)  †</cell><cell>ResNet-101</cell><cell>80.9</cell></row><row><cell>SAC [54]</cell><cell>ResNet-101</cell><cell>78.1</cell></row><row><cell>AAF [18]</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparison to other methods on Pascal Context<ref type="bibr" target="#b32">[33]</ref> dataset.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>mIoU (%)</cell></row><row><cell>FCN8-s [32]</cell><cell>VGG-16</cell><cell>37.8</cell></row><row><cell>HO CRF [1]</cell><cell>VGG-16</cell><cell>41.3</cell></row><row><cell>Piecewise [28]</cell><cell>VGG-16</cell><cell>43.3</cell></row><row><cell>DeepLab-v2 (COCO) [6]</cell><cell>ResNet-101</cell><cell>45.7</cell></row><row><cell>RefineNet [29]</cell><cell>ResNet-101</cell><cell>47.3</cell></row><row><cell>PSPNet [55]</cell><cell>ResNet-101</cell><cell>47.8</cell></row><row><cell>Ding et al. [11]</cell><cell>ResNet-101</cell><cell>51.6</cell></row><row><cell>EncNet [52] (SS)</cell><cell>ResNet-50</cell><cell>49.0</cell></row><row><cell>EncNet [52] (MS)</cell><cell>ResNet-101</cell><cell>51.7</cell></row><row><cell>SGR [26]</cell><cell>ResNet-101</cell><cell>52.5</cell></row><row><cell>DANet [14]</cell><cell>ResNet-50</cell><cell>50.1</cell></row><row><cell>DANet [14]</cell><cell>ResNet-101</cell><cell>52.6</cell></row><row><cell>Dilated FCN baseline</cell><cell>ResNet-50</cell><cell>44.3</cell></row><row><cell>DGCNet (SS)</cell><cell>ResNet-50</cell><cell>50.1</cell></row><row><cell>DGCNet (SS)</cell><cell>ResNet-101</cell><cell>53.0</cell></row><row><cell>DGCNet (MS)</cell><cell>ResNet-101</cell><cell>53.7</cell></row><row><cell cols="2">SS: Single scale. MS: Multi scale</cell><cell></cell></row><row><cell>the state-of-the-art uses strided convolutions with d = 8.</cell><cell></cell><cell></cell></row><row><cell>4.2.2 Comparisons with state-of-the-art</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5</head><label>5</label><figDesc>compares our approach with existing methods on the Cityscapes test set. Following common practice towards obtaining the highest performance, we use the inference strategies described in the previous section. For fair comparison, Tab. 5 shows methods that are only trained using the fine annotations from Cityscapes, and evaluated on the evaluation server. We achieve a mean IoU of 80.7% when only using the training set, thus outperforming PSANet [56] by 2.1% and OC-Net<ref type="bibr" target="#b49">[50]</ref> by 0.8%. Training with both train-fine and val-fine sets achieves an IoU of 82.0%. In both scenarios, we obtain state-of-the-art results. Detailed per-class results are provided in the supplementary material, which shows that our method achieves the highest IoU in 16 out of the 19 classes.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4</head><label>4</label><figDesc>shows our results on Pascal Context. We follow prior work<ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b51">52]</ref> to use the semantic labels of the most frequent 59 object categories plus background (therefore, there Comparison of our results on Pascal Context to the state-of-art EncNet<ref type="bibr" target="#b51">[52]</ref> method. Note that our results are more consistent and have fewer artifacts. Best view in color. More results in supplementary material.</figDesc><table><row><cell>input</cell></row></table><note>are 60 classes in total). The Dilated-FCN baseline achieves a mean IoU of 44.3% with the ResNet-50 backbone. Our proposed DGCNet significantly improves this baseline, achieving an IoU of 50.1% with the same ResNet-50 backbone under single scale evaluation (SS), which outperforms previous work using the same backbone (49.0) [52]. With the ResNet- 101 backbone, DGCNet achieves an IoU of 53.0%. Moreover, our performance further improves to 53.7% when multiscale inference (MS) is adopted, surpassing the previous state-of-the-art [14] by a large margin.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Per-class results on Cityscapes testing set. Our methods outperforms existing approaches and achieves 82.0 % in Mean IoU and achieves the highest IoU in 16 out of the 19 classes.</figDesc><table><row><cell>Input</cell><cell>EncNet</cell><cell>DGCNet</cell><cell>Ground Truth</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by EPSRC Programme Grant Seebibyte EP/M013774/1, ERC grant ERC-2012-AdG 321162-HELIOS and EPSRC/MURI grant EP/N019474/1. We gratefully acknowledge the use of the University of Oxford Advanced Research Computing (ARC) facility in carrying out this work. We would also like to acknowledge the Royal Academy of Engineering, FiveAI and the support of DeepMotion AI Research for providing the computing resources in carrying out this research.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head> <ref type="figure">Figure 5</ref><p>: Cityscapes results compared with Dilated FCN ResNet101 baseline <ref type="bibr" target="#b48">[49]</ref>. Best view in color.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Higher order conditional random fields in deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadeep</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Conditional random fields meet deep neural networks for semantic segmentation: Combining probabilistic graphical models with deep learning for structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadeep</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardino</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Måns</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Savchynskyy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fredrik</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Scene labeling with lstm recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonmin</forename><surname>Byeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">M</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Raue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Liwicki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<title level="m">Semantic image segmentation with deep convolutional nets and fully connected CRFs. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.12814</idno>
		<title level="m">Jiashi Feng, and Yannis Kalantidis. Graph-based global reasoning networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Boxsup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1635" to="1643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Context contrasted feature and gated multi-scale aggregation for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henghui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ai</forename><forename type="middle">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Finding pictures of objects in large collections of images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Forsyth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><forename type="middle">M</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hayit</forename><surname>Fleck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Greenspan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chad</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Carson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International workshop on object representation in computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="335" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijie</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.02983</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multiscale conditional random fields for image labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><forename type="middle">Á</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carreira-Perpiñán</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Strip pooling: Rethinking spatial pooling for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinbin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adaptive affinity fields for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jyh-Jing</forename><surname>Tsung-Wei Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Recurrent scene parsing with perspective understanding in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Holistic, instance-level human parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Global aggregation then local distribution in fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangtai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ansheng</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maoke</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuiyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhai</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Beyond grids: Learning graph representations for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Symbolic graph reasoning meets convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multi-scale context intertwining for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Efficient piecewise training of deep structured models for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">D</forename><surname>Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Refinenet: Multi-path refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollãąr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.04579</idno>
		<title level="m">Parsenet: Looking wider to see better</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The role of context for object detection and semantic segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roozbeh</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam-Gyu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong-Whan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonwoo</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Large kernel matters-improve semantic segmentation by global convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiming</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Full-resolution residual networks for semantic segmentation in street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Pohlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4151" to="4160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">. U-Net</forename></persName>
		</author>
		<title level="m">Convolutional networks for biomedical image segmentation. International Conference on Medical Image Computing and Computer Assisted Intervention</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Textonboost: Joint appearance, shape and context modeling for multi-class object recognition and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Criminisi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Training region-based object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Scene segmentation with dagrecurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">Very deep convolutional networks for largescale image recognition. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Understanding convolution for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panqu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehua</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodi</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrison</forename><surname>Cottrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Wider or deeper: Revisiting the resnet model for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Denseaspp for semantic segmentation in street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maoke</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuiyuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Bisenet: Bilateral segmentation network for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nong</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning a discriminative feature network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nong</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Bisenet: Bilateral segmentation network for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nong</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Ocnet: Object context network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.00916</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Context encoding for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristin</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambrish</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Agrawal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Dynamic graph message passing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Scale-adaptive convolutions for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jintao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Psanet: Point-wise spatial attention network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Conditional random fields as recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadeep</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardino</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhav</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dalong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.05442</idno>
		<title level="m">Semantic understanding of scenes through the ADE20K dataset</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Improving Semantic Segmentation via Video Propagation and Label Relaxation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Sapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fitsum</forename><forename type="middle">A</forename><surname>Reda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">J</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shawn</forename><surname>Newsam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
