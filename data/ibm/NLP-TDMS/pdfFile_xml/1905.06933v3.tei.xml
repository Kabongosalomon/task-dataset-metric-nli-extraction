<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dynamically Fused Graph Network for Multi-hop Reasoning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunxuan</forename><surname>Xiao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanru</forename><surname>Qu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename><surname>Lin Qiu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename><surname>Hao Zhou</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">ByteDance AI Lab</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">ByteDance AI Lab</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
							<email>wnzhang@sjtu.edu.cnlqiu</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Dynamically Fused Graph Network for Multi-hop Reasoning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Text-based question answering (TBQA) has been studied extensively in recent years. Most existing approaches focus on finding the answer to a question within a single paragraph. However, many difficult questions require multiple supporting evidence from scattered text across two or more documents. In this paper, we propose the Dynamically Fused Graph Network (DFGN), a novel method to answer those questions requiring multiple scattered evidence and reasoning over them. Inspired by human's step-by-step reasoning behavior, DFGN includes a dynamic fusion layer that starts from the entities mentioned in the given query, explores along the entity graph dynamically built from the text, and gradually finds relevant supporting entities from the given documents. We evaluate DFGN on HotpotQA, a public TBQA dataset requiring multi-hop reasoning. DFGN achieves competitive results on the public board. Furthermore, our analysis shows DFGN could produce interpretable reasoning chains.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Question answering (QA) has been a popular topic in natural language processing. QA provides a quantifiable way to evaluate an NLP system's capability on language understanding and reasoning <ref type="bibr" target="#b4">(Hermann et al., 2015;</ref><ref type="bibr" target="#b14">Rajpurkar et al., 2016</ref><ref type="bibr" target="#b13">Rajpurkar et al., , 2018</ref>. Most previous work focus on finding evidence and answers from a single paragraph <ref type="bibr" target="#b16">(Seo et al., 2016;</ref><ref type="bibr" target="#b8">Liu et al., 2017;</ref><ref type="bibr" target="#b22">Wang et al., 2017)</ref>. It rarely tests deep reasoning capabilities of the underlying model. In fact, <ref type="bibr" target="#b10">Min et al. (2018)</ref> observe that most questions in existing QA benchmarks can be answered by retrieving † These authors contributed equally. The order of authorship is decided through dice rolling. Work done while Lin Qiu was a research intern in ByteDance AI Lab.</p><p>The Sum of All Fears is a best-selling thriller novel by Tom Clancy ... It was the fourth of Clancy's Jack Ryan books to be turned into a film ...</p><p>Dr. John Patrick Jack Ryan Sr., KCVO (Hon.), Ph.D. is a fictional character created by Tom Clancy who appears in many of his novels and their respective film adaptations ... Net Force Explorers is a series of young adult novels created by Tom Clancy and Steve Pieczenik as a spin-off of the military fiction series ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question: What fiction character created by Tom Clancy was turned into a film in 2002?</head><p>Answer: Jack Ryan Input Paragraphs:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original Entity Graph</head><p>Second Mask Applied First Mask Applied <ref type="figure">Figure 1</ref>: Example of multi-hop text-based QA. One question and three document paragraphs are given. Our proposed DFGN conducts multi-step reasoning over the facts by constructing an entity graph from multiple paragraphs, predicting a dynamic mask to select a subgraph, propagating information along the graph, and finally transfer the information from the graph back to the text in order to localize the answer. Nodes are entity occurrences, with the color denoting the underlying entity. Edges are constructed from co-occurrences. The gray circles are selected by DFGN in each step. a small set of sentences without reasoning. To address this issue, there are several recently proposed QA datasets particularly designed to evaluate a system's multi-hop reasoning capabilities, including WikiHop <ref type="bibr" target="#b23">(Welbl et al., 2018)</ref>, ComplexWe-bQuestions <ref type="bibr" target="#b20">(Talmor and Berant, 2018)</ref>, and Hot-potQA .</p><p>In this paper, we study the problem of multi-hop text-based QA, which requires multi-hop reasoning among evidence scattered around multiple raw documents. In particular, a query utterance and a set of accompanying documents are given, but not all of them are relevant. The answer can only be obtained by selecting two or more evidence from the documents and inferring among them (see <ref type="bibr">Figure</ref> 1 for an example). This setup is versatile and does not rely on any additional predefined knowledge base. Therefore the models are expected to generalize well and to answer questions in open domains.</p><p>There are two main challenges to answer questions of this kind. Firstly, since not every document contain relevant information, multi-hop textbased QA requires filtering out noises from multiple paragraphs and extracting useful information. To address this, recent studies propose to build entity graphs from input paragraphs and apply graph neural networks (GNNs) to aggregate the information through entity graphs <ref type="bibr" target="#b3">(Dhingra et al., 2018;</ref><ref type="bibr" target="#b1">De Cao et al., 2018;</ref><ref type="bibr" target="#b17">Song et al., 2018a)</ref>. However, all of the existing work apply GNNs based on a static global entity graph of each QA pair, which can be considered as performing implicit reasoning. Instead of them, we argue that the queryguided multi-hop reasoning should be explicitly performed on a dynamic local entity graph tailored according to the query.</p><p>Secondly, previous work on multi-hop QA (e.g. WikiHop) usually aggregates document information to an entity graph, and answers are then directly selected on entities of the entity graph. However, in a more realistic setting, the answers may even not reside in entities of the extracted entity graph. Thus, existing approaches can hardly be directly applied to open-domain multi-hop QA tasks like HotpotQA.</p><p>In this paper, we propose Dynamically Fused Graph Network (DFGN), a novel method to address the aforementioned concerns for multi-hop text-based QA. For the first challenge, DFGN constructs a dynamic entity graph based on entity mentions in the query and documents. This process iterates in multiple rounds to achieve multihop reasoning. In each round, DFGN generates and reasons on a dynamic graph, where irrelevant entities are masked out while only reasoning sources are preserved, via a mask prediction module. <ref type="figure">Figure 1</ref> shows how DFGN works on a multi-hop text-based QA example in HotpotQA. The mask prediction module is learned in an endto-end fashion, alleviating the error propagation problem.</p><p>To solve the second challenge, we propose a fusion process in DFGN to solve the unrestricted QA challenge. We not only aggregate information from documents to the entity graph (doc2graph), but also propagate the information of the entity graph back to document representations (graph2doc). The fusion process is iteratively performed at each hop through the document tokens and entities, and the final resulting answer is then obtained from document tokens. The fusion process of doc2graph and graph2doc along with the dynamic entity graph jointly improve the interaction between the information of documents and the entity graph, leading to a less noisy entity graph and thus more accurate answers.</p><p>As one merit, DFGN's predicted masks implicitly induce reasoning chains, which can explain the reasoning results. Since the ground truth reasoning chain is very hard to define and label for open-domain corpus, we propose a feasible way to weakly supervise the mask learning. We propose a new metric to evaluate the quality of predicted reasoning chains and constructed entity graphs.</p><p>Our contributions are summarized as follows:</p><p>• We propose DFGN, a novel method for the multi-hop text-based QA problem. • We provide a way to explain and evaluate the reasoning chains via interpreting the entity graph masks predicted by DFGN. The mask prediction module is additionally weakly trained. • We provide an experimental study on a public dataset (HotpotQA) to demonstrate that our proposed DFGN is competitive against stateof-the-art unpublished work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Text-based Question Answering Depending on whether the supporting information is structured or not, QA tasks can be categorized into knowledge-based (KBQA), text-based (TBQA), mixed, and others. In KBQA, the supporting information is from structured knowledge bases (KBs), while the queries can be either structure or natural language utterances. For example, SimpleQuestions is one large scale dataset of this kind <ref type="bibr" target="#b0">(Bordes et al., 2015)</ref>. In contrast, TBQA's supporting information is raw text, and hence the query is also text. SQuAD <ref type="bibr" target="#b14">(Rajpurkar et al., 2016)</ref> and HotpotQA  are two such datasets. There are also mixed QA tasks which combine both text and KBs, e.g. WikiHop (Welbl   <ref type="bibr" target="#b20">(Talmor and Berant, 2018)</ref>. In this paper, we focus on TBQA, since TBQA tests a system's end-to-end capability of extracting relevant facts from raw language and reasoning about them. Depending on the complexity in underlying reasoning, QA problems can be categorized into single-hop and multi-hop ones. Single-hop QA only requires one fact extracted from the underlying information, no matter structured or unstructured, e.g. "which city is the capital of California". The SQuAD dataset belongs to this type <ref type="bibr" target="#b14">(Rajpurkar et al., 2016)</ref>. On the contrary, multi-hop QA requires identifying multiple related facts and reasoning about them, e.g. "what is the capital city of the largest state in the U.S.". Example tasks and benchmarks of this kind include WikiHop, Com-plexWebQuestions, and HotpotQA. Many IR techniques can be applied to answer single-hop questions <ref type="bibr" target="#b14">(Rajpurkar et al., 2016)</ref>. However, these IR techniques are hardly introduced in multi-hop QA, since a single fact can only partially match a question.</p><p>Note that existing multi-hop QA datasets Wik-iHop and ComplexWebQuestions, are constructed using existing KBs and constrained by the schema of the KBs they use. For example, the answers are limited in entities in WikiHop rather than formed by free texts in HotpotQA (see <ref type="figure" target="#fig_0">Figure 2</ref> for an example). In this work, we focus on multi-hop textbased QA, so we only evaluate on HotpotQA.</p><p>Multi-hop Reasoning for QA Popular GNN frameworks, e.g. graph convolution network (Kipf and Welling, 2017), graph attention network <ref type="bibr" target="#b21">(Veličković et al., 2018)</ref>, and graph recurrent network <ref type="bibr" target="#b18">(Song et al., 2018b)</ref>, have been previously studied and show promising results in QA tasks requiring reasoning <ref type="bibr" target="#b3">(Dhingra et al., 2018;</ref><ref type="bibr" target="#b1">De Cao et al., 2018;</ref><ref type="bibr" target="#b17">Song et al., 2018a)</ref>.</p><p>Coref-GRN extracts and aggregates entity information in different references from scattered paragraphs <ref type="bibr" target="#b3">(Dhingra et al., 2018)</ref>. Coref-GRN utilizes co-reference resolution to detect different mentions of the same entity. These mentions are combined with a graph recurrent neural network (GRN) <ref type="bibr" target="#b18">(Song et al., 2018b)</ref> to produce aggregated entity representations. MHQA-GRN <ref type="bibr" target="#b17">(Song et al., 2018a)</ref> follows Coref-GRN and refines the graph construction procedure with more connections: sliding-window, same entity, and co-reference, which shows further improvements. Entity-GCN <ref type="bibr" target="#b1">(De Cao et al., 2018)</ref> proposes to distinguish different relations in the graphs through a relational graph convolutional neural network (GCN) (Kipf and Welling, 2017). Coref-GRN, MHQA-GRN and Entity-GCN explore the graph construction problem in answering real-world questions. However, it is yet to investigate how to effectively reason about the constructed graphs, which is the main problem studied in this work.</p><p>Another group of sequential models deals with multi-hop reasoning following Memory Networks <ref type="bibr" target="#b19">(Sukhbaatar et al., 2015)</ref>. Such models construct representations for queries and memory cells for contexts, then make interactions between them in a multi-hop manner. <ref type="bibr" target="#b11">Munkhdalai and Yu (2017)</ref> and <ref type="bibr" target="#b12">Onishi et al. (2016)</ref> incorporate a hypothesis testing loop to update the query representation at each reasoning step and select the best answer among the candidate entities at the last step. IR-Net <ref type="bibr" target="#b25">(Zhou et al., 2018)</ref> generates a subject state and a relation state at each step, computing the similarity score between all the entities and relations given by the dataset KB. The ones with the highest score at each time step are linked together to form an interpretable reasoning chain. However, these models perform reasoning on simple synthetic datasets with a limited number of entities and relations, which are quite different with largescale QA dataset with complex questions. Also, the supervision of entity-level reasoning chains in synthetic datasets can be easily given following some patterns while they are not available in Hot-potQA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dynamically Fused Graph Network</head><p>We describe dynamically fused graph network (DFGN) in this section. Our intuition is drawn from the human reasoning process for QA. One starts from an entity of interest in the query, focuses on the words surrounding the start entities, connects to some related entity either found in the neighborhood or linked by the same surface mention, repeats the step to form a reasoning chain, and lands on some entity or snippets likely to be the answer. To mimic human reasoning behavior, we develop five components in our proposed QA system ( <ref type="figure" target="#fig_1">Fig. 3)</ref>: a paragraph selection subnetwork, a module for entity graph construction, an encoding layer, a fusion block for multi-hop reasoning, and a final prediction layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Paragraph Selection</head><p>For each question, we assume that N p paragraphs are given (e.g. N p = 10 in HotpotQA). Since not every piece of text is relevant to the question, we train a sub-network to select relevant paragraphs. The sub-network is based on a pre-trained BERT model <ref type="bibr" target="#b2">(Devlin et al., 2018)</ref> followed by a sentence classification layer with sigmoid prediction. The selector network takes a query Q and a paragraph as input and outputs a relevance score between 0 and 1. Training labels are constructed by assigning 1's to the paragraphs with at least one supporting sentence for each Q&amp;A pair. During inference, paragraphs with predicted scores greater than η (= 0.1 in experiments) are selected and concate-  nated together as the context C. η is properly chosen to ensure the selector reaches a significantly high recall of relevant paragraphs. Q and C are further processed by upper layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Constructing Entity Graph</head><p>We do not assume a global knowledge base. Instead, we use the Stanford corenlp toolkit <ref type="bibr" target="#b9">(Manning et al., 2014)</ref> to recognize named entities from the context C. The number of extracted entities is denoted as N . The entity graph is constructed with the entities as nodes and edges built as follows. The edges are added 1. for every pair of entities appear in the same sentence in C (sentencelevel links); 2. for every pair of entities with the same mention text in C (context-level links); and 3. between a central entity node and other entities within the same paragraph (paragraph-level links). The central entities are extracted from the title sentence for each paragraph. Notice the context-level links ensures that entities across multiple documents are connected in a certain way. We do not apply co-reference resolution for pronouns because it introduces both additional useful and erroneous links.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Encoding Query and Context</head><p>We concatenate the query Q with the context C and pass the resulting sequence to a pre-trained BERT model to obtain representations Q = [q 1 , . . . , q L ] ∈ R L×d 1 and C = [c 1 , . . . , c M ] ∈ R M ×d 1 , where L,M are lengths of query and context, and d 1 is the size of BERT hidden states.</p><p>In experiments, we find concatenating queries and contexts performs better than passing them separately to BERT. The representations are further passed through a bi-attention layer <ref type="bibr" target="#b16">(Seo et al., 2016)</ref> to enhance cross interactions between the query and the context. In practice, we find adding the bi-attention layer achieves better performance than the BERT encoding only. The output representation are Q 0 ∈ R L×d 2 and C 0 ∈ R M ×d 2 , where d 2 is the output embedding size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Reasoning with the Fusion Block</head><p>With the embeddings calculated for the query Q and context C, the remaining challenge is how to identify supporting entities and the text span of potential answers. We propose a fusion block to mimic human's one-step reasoning behaviorstarting from Q 0 and C 0 and finding one-step supporting entities. A fusion block achieves the following: 1. passing information from tokens to entities by computing entity embeddings from tokens (Doc2Graph flow); 2. propagating information on entity graph; and 3. passing information from entity graph to document tokens since the final prediction is on tokens (Graph2Doc flow). <ref type="figure">Fig. 4</ref> depicts the inside structure of the fusion block in DFGN.</p><p>Document to Graph Flow. Since each entity is recognized via the NER tool, the text spans associated with the entities are utilized to compute entity embeddings (Doc2Graph). To this end, we construct a binary matrix M, where M i,j is 1 if i-th token in the context is within the span of the j-th entity. M is used to select the text span associated with an entity. The token embeddings calculated from the above section (which is a matrix containing only selected columns of C t−1 ) is passed into a mean-max pooling to calculate entity embeddings E t−1 = [e t−1,1 , . . . , e t−1,N ]. E t−1 will be of size 2d 2 ×N , where N is the number of entities, and each of the 2d 2 dimensions will produce both mean-pooling and max-pooling results. This module is denoted as Tok2Ent.</p><p>Dynamic Graph Attention. After obtaining entity embeddings from the input context C t−1 , we apply a graph neural network to propagate node information to their neighbors. We propose a dynamic graph attention mechanism to mimic human's step-by-step exploring and reasoning behavior. In each reasoning step, we assume every node has some information to disseminate to  <ref type="figure">Figure 4</ref>: Reasoning with the fusion block in DFGN neighbors. The more relevant to the query, the neighbor nodes receive more information from nearby. We first identify nodes relevant to the query by creating a soft mask on entities. It serves as an information gatekeeper, i.e. only those entity nodes pertaining to the query are allowed to disseminate information. We use an attention network between the query embeddings and the entity embeddings to predict a soft mask m t , which aims to signify the start entities in the t-th reasoning step:</p><formula xml:id="formula_0">q (t−1) = MeanPooling(Q (t−1) ) (1) γ (t) i =q (t−1) V (t) e (t−1) i / d 2 (2) m (t) = σ([γ (t) 1 , · · · , γ (t) N ])<label>(3)</label></formula><formula xml:id="formula_1">E (t−1) = [m (t) 1 e (t−1) 1 , . . . , m (t) N e (t−1) N ]<label>(4)</label></formula><p>where V t is a linear projection matrix, and σ is the sigmoid function. By multiplying the soft mask and the initial entity embeddings, the desired start entities will be encouraged and others will be penalized. As a result, this step of information propagation is restricted to a dynamic sub-part of the entity graph. The next step is to disseminate information across the dynamic sub-graph.</p><p>Inspired by GAT <ref type="bibr" target="#b21">(Veličković et al., 2018)</ref>, we compute attention score α between two entities by:</p><formula xml:id="formula_2">h (t) i = U tẽ (t−1) i + b t (5) β (t) i,j = LeakyReLU(W t [h (t) i , h (t) j ]) (6) α (t) i,j = exp(β (t) i,j ) k exp(β (t) i,k )<label>(7)</label></formula><p>where U t ∈ R d 2 ×2d 2 , W t ∈ R 2d 2 are linear projection parameters. Here the i-th row of α rep-resents the proportion of information that will be assigned to the neighbors of entity i. Note that the information flow in our model is different from most previous GATs. In dynamic graph attention, each node sums over its column, which forms a new entity state containing the total information it received from the neighbors:</p><formula xml:id="formula_3">e (t) i = ReLU( j∈B i α (t) j,i h (t) j )<label>(8)</label></formula><p>where B i is the set of neighbors of entity i. Then we obtain the updated entity embeddings</p><formula xml:id="formula_4">E (t) = [e (t) 1 , . . . , e (t) N ].</formula><p>Updating Query. A reasoning chain contains multiple steps, and the newly visited entities by one step will be the start entities of the next step.</p><p>In order to predict the expected start entities for the next step, we introduce a query update mechanism, where the query embeddings are updated by the entity embeddings of the current step. In our implementation, we utilize a bi-attention network <ref type="bibr" target="#b16">(Seo et al., 2016)</ref> to update the query embeddings:</p><formula xml:id="formula_5">Q (t) = Bi-Attention(Q (t−1) , E (t) )<label>(9)</label></formula><p>Graph to Document Flow. Using Tok2Ent and dynamic graph attention, we realize a reasoning step at the entity level. However, the unrestricted answer still cannot be backtraced. To address this, we develop a Graph2Doc module to keep information flowing from entity back to tokens in the context. Therefore the text span pertaining to the answers can be localized in the context. Using the same binary matrix M as described above, the previous token embeddings in C t−1 are concatenated with the associated entity embedding corresponding to the token. Each row in M corresponds to one token, therefore we use it to select one entity's embedding from E t if the token participates in the entity's mention. This information is further processed with a LSTM layer (Hochreiter and Schmidhuber, 1997) to produce the nextlevel context representation:</p><formula xml:id="formula_6">C (t) = LSTM([C (t−1) , ME (t) ])<label>(10)</label></formula><p>where ; refers to concatenation and C (t) ∈ R M ×d 2 serves as the input of the next fusion block. At this time, the reasoning information of current subgraph has been propagated onto the whole context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Prediction</head><p>We follow the same structure of prediction layers as . The framework has four output dimensions, including 1. supporting sentences, 2. the start position of the answer, 3. the end position of the answer, and 4. the answer type. We use a cascade structure to solve the output dependency, where four isomorphic LSTMs F i are stacked layer by layer. The context representation of the last fusion block is sent to the first LSTM F 0 . Each F i outputs a logit O ∈ R M ×d 2 and computes a cross entropy loss over these logits.</p><formula xml:id="formula_7">O sup = F 0 (C (t) )<label>(11)</label></formula><formula xml:id="formula_8">O start = F 1 ([C (t) , O sup ]) (12) O end = F 2 ([C (t) , O sup , O start ]) (13) O type = F 3 ([C (t) , O sup , O end ])<label>(14)</label></formula><p>We jointly optimize these four cross entropy losses. Each loss term is weighted by a coefficient.</p><formula xml:id="formula_9">L = L start + L end + λ s L sup + λ t L type (15)</formula><p>Weak Supervision. In addition, we introduce a weakly supervised signal to induce the soft masks at each fusion block to match the heuristic masks. For each training case, the heuristic masks contain a start mask detected from the query, and additional BFS masks obtained by applying breadthfirst search (BFS) on the adjacent matrices give the start mask. A binary cross entropy loss between the predicted soft masks and the heuristics is then added to the objective. We skip those cases whose start masks cannot be detected from the queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate our Dynamically Fused Graph Network (DFGN) on HotpotQA  in the distractor setting. For the full wiki setting where the entire Wikipedia articles are given as input, we consider the bottleneck is about information retrieval, thus we do not include the full wiki setting in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation Details</head><p>In paragraph selection stage, we use the uncased version of BERT Tokenizer <ref type="bibr" target="#b2">(Devlin et al., 2018)</ref> to tokenize all passages and questions. The encoding vectors of sentence pairs are generated from a pre-trained BERT model <ref type="bibr" target="#b2">(Devlin et al., 2018)</ref>. We set a relatively low threshold during selection to  <ref type="table">Table 2</ref>: Ablation study of question answering performances in the development set of HotpotQA in the distractor setting. We use a DFGN with 2-layer fusion blocks as the origin model. The upper part is the model ablation results and the lower part is the dataset ablation results.</p><p>keep a high recall (97%) and a reasonable precision (69%) on supporting facts. In graph construction stage, we use a pretrained NER model from Stanford CoreNLP Toolkits 1 <ref type="bibr" target="#b9">(Manning et al., 2014)</ref> to extract named entities. The maximum number of entities in a graph is set to be 40. Each entity node in the entity graphs has an average degree of 3.52.</p><p>In the encoding stage, we also use a pre-trained BERT model as the encoder, thus d 1 is 768. All the hidden state dimensions d 2 are set to 300. We set the dropout rate for all hidden units of LSTM and dynamic graph attention to 0.3 and 0.5 respectively. For optimization, we use Adam Optimizer <ref type="bibr" target="#b6">(Kingma and Ba, 2015)</ref> with an initial learning rate of 1e −4 . 1 https://nlp.stanford.edu/software/ CRF-NER.shtml</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Main Results</head><p>We first present a comparison between baseline models and our DFGN 2 . <ref type="table">Table 1</ref> shows the performance of different models in the private test set of HotpotQA. From the table we can see that our model achieves the second best result on the leaderboard now 3 (on March 1st). Besides, the answer performance and the joint performance of our model are competitive against state-of-the-art unpublished models. We also include the result of our model with a revised entity graph whose entities are recognized by a BERT NER model <ref type="bibr" target="#b2">(Devlin et al., 2018)</ref>. We fine-tune the pre-trained BERT model on the dataset of the CoNLL'03 NER shared task <ref type="bibr" target="#b15">(Sang and De Meulder, 2003)</ref> and use it to extract named entities from the input paragraphs. The results show that our model achieves a 1.5% gain in the joint F1-score with the entity graph built from a better entity recognizer.</p><p>To evaluate the performance of different components in our DFGN, we perform ablation study on both model components and dataset segments. Here we follow the experiment setting in  to perform the dataset ablation study, where we only use golden paragraphs or supporting facts as the input context. The ablation results of QA performances in the development set of HotpotQA are shown in <ref type="table">Table 2</ref>. From the table we can see that each of our model components can provide from 1% to 2% relative gain over the QA performance. Particularly, using a 1-layer fusion block leads to an obvious performance loss, which implies the significance of performing multi-hop reasoning in HotpotQA. Besides, the dataset abla-tion results show that our model is not very sensitive to the noisy paragraphs comparing with the baseline model which can achieve a more than 5% performance gain in the "gold paragraphs only" and "supporting facts only" settings. .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation on Graph Construction and Reasoning Chains</head><p>The chain of reasoning is a directed path on the entity graph, so high-quality entity graphs are the basis of good reasoning. Since the limited accuracy of NER model and the incompleteness of our graph construction, 31.3% of the cases in the development set are unable to perform a complete reasoning process, where at least one supporting sentence is not reachable through the entity graph, i.e. no entity is recognized by NER model in this sentence. We name such cases as "missing supporting entity", and the ratio of such cases can evaluate the quality of graph construction. We focus on the rest 68.7% good cases in the following analysis.</p><p>In the following, we first give several definitions before presenting ESP (Entity-level Support) scores.</p><p>Path A path is a sequence of entities visited by the fusion blocks, denoting as P = [e p 1 , . . . , e p t+1 ] (suppose t-layer fusion blocks).</p><p>Path Score The score of a path is acquired by multiplying corresponding soft masks and attention scores along the path, i.e. score(P <ref type="formula" target="#formula_2">(7)</ref>).</p><formula xml:id="formula_10">) = t i=1 m (i) p i α (i) p i ,p i+1 (Eq. (3),</formula><p>Hit Given a path and a supporting sentence, if at least one entity of the supporting sentence is visited by the path, we call this supporting sentence is hit 4 .</p><p>Given a case with m supporting sentences, we select the top-k paths with the highest scores as the predicted reasoning chains. For each supporting sentence, we use the k paths to calculate how many supporting sentences are hit.</p><p>In the following, we introduce two metrics to evaluate the quality of multi-hop reasoning through entity-level supporting (ESP) scores.  ESP Recall For a case with m supporting sentences and h of them are hit, this case has a recall score of h/m. The averaged recall of the whole dataset is the ESP Recall.</p><p>We train a DFGN with 2 fusion blocks to select paths with top-k scores. In the development set, the average number of paths of length 2 is 174.7. We choose k as 1, 2, 5, 10 to compute ESP EM and ESP Recall scores. As we can see in <ref type="table" target="#tab_5">Table 3</ref>, regarding the supporting sentences as the ground truth of reasoning chains, our framework can predict reliable information flow. The most informative flow can cover the supporting facts and help produce reliable reasoning results. Here we present the results from two versions of the entity graphs. The results with a maximum number of nodes ≤ 40 are from the entity graph whose entities are extracted by Stanford CoreNLP. The results with a maximum number of nodes ≤ 80 are from the entity graph whose entities are extracted by the aforementioned BERT NER model. Since the BERT NER model performs better, we use a larger maximum number of nodes.</p><p>In addition, as the size of an entity graph gets larger, the expansion of reasoning chain space makes a Hit even more difficult. However, the BERT NER model still keeps comparative and even better performance on metrics of EM and Recall. Thus the entity graph built from the BERT NER model is better than the previous version.</p><p>Supporting Fact 1: "Farrukhzad Khosrau V was briefly king of the Sasanian Empire from March 631 to ..." Supporting Fact 2: "The Sasanian Empire, which succeeded the Parthian Empire, was recognised as ... the Roman-Byzantine Empire, for a period of more than 400 years." Q2: From March 631 to April 631, Farrukhzad Khosrau V was the king of an empire that succeeded which empire? Answer: the Parthian Empire Prediction: Parthian Empire Top 1 Reasoning Chain: n/a Supporting Fact 1: "Barrack buster is the colloquial name given to several improvised mortars, developed in the 1990s by the engineering group of the Provisional Irish Republican Army (IRA)." Supporting Fact 2: " On 20 March 1994, a British Army Lynx helicopter was shot down by the Provisional Irish Republican Army (IRA) in Northern Ireland." Q1: Who used a Barrack buster to shoot down a British Army Lynx helicopter? Answer: IRA Prediction: IRA Top 1 Reasoning Chain: British Army Lynx, Provisional Irish Republican Army, IRA</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mask1 Mask2 End</head><p>Supporting Fact 1: "George Archainbaud <ref type="bibr">(May 7, 1890</ref><ref type="bibr">? February 20, 1959</ref>) was a French-born American film and television director." Supporting Fact 2: "Ralph Murphy <ref type="bibr">(May 1, 1895</ref><ref type="bibr">? February 10, 1967</ref> was an American film director." Q3: Who died first, George Archainbaud or Ralph Murphy? Answer: George Archainbaud Prediction: Ralph Murphy Top 1 Reasoning Chain: Ralph Murphy, May 1, 1895, Ralph Murphy <ref type="figure">Figure 5</ref>: Case study of three samples in the development set. We train a DFGN with 2-layer fusion blocks to produce the results. The numbers on the left side indicate the importance scores of the predicted masks. The text on the right side include the queries, answers, predictions, predicted top-1 reasoning chains and the supporting facts of three samples with the recognized entities highlighted by different colors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Case Study</head><p>We present a case study in <ref type="figure">Figure 5</ref>. The first case illustrates the reasoning process in a DFGN with 2-layer fusion blocks. At the first step, by comparing the query with entities, our model generates Mask1 as the start entity mask of reasoning, where "Barrack" and "British Army Lynx" are detected as the start entities of two reasoning chains. Information of two start entities is then passed to their neighbors on the entity graph. At the second step, mentions of the same entity "IRA" are detected by Mask2, serving as a bridge for propagating information across two paragraphs. Finally, two reasoning chains are linked together by the bridge entity "IRA", which is exactly the answer.</p><p>The second case in <ref type="figure">Figure 5</ref> is a bad case. Due to the malfunction of the NER module, the only start entity, "Farrukhzad Khosrau V", was not successfully detected. Without the start entities, the reasoning chains cannot be established, and the further information flow in the entity graph is blocked at the first step.</p><p>The third case in <ref type="figure">Figure 5</ref> is also a bad case, which includes a query of the Comparison query type. Due to the lack of numerical computation ability of our model, it fails to give a correct answer, although the query is just a simple compar-ison between two days "February 20, 1959" and "February 10, 1967". It is an essential problem to incorporate numerical operations for further improving the performance in cases of the comparison query type.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Comparison between HotpotQA (left) and WikiHop (right). In HotpotQA, the questions are proposed by crowd workers and the blue words in paragraphs are labeled supporting facts corresponding to the question. In WikiHop, the questions and answers are formed with relations and entities in the underlying KB respectively, thus the questions are inherently restricted by the KB schema. The colored words and phrases are entities in the KB. et al., 2018) and ComplexWebQuestions</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Overview of DFGN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Australia at the 2012 Winter Youth OlympicsAustralia competed at the 2012 Winter Youth Olympics in Innsbruck. The chef de mission of the team will be former Olympic champion Alisa Camplin, the first time a woman is the chef de mission of any Australian Olympic team. The Australian team will consist of 13 athletes in 8 sports.Alisa Peta Camplin OAM (born 10 November 1974) is an Australian aerial skier who won gold at the 2002 Winter Olympics, the second ever winter Olympic gold medal for Australia. At the 2006 Winter Olympics, Camplin finished third to receive a bronze medal. She is the first Australian skier to win medals at consecutive Winter Olympics, making her one of Australia's best skiers. They provide sunset views over the Arabian Sea.Mumbai (also known as Bombay, the official name until 1995) is the capital city of the Indian state of Maharashtra. It is the most populous city in India ?The Arabian Sea is a region of the northern Indian Ocean bounded on the north by Pakistan and Iran, on the west by northeastern Somalia and the Arabian Peninsula, and on the east by India ?</figDesc><table><row><cell>Paragraph 1: Paragraph 2: Alisa Camplin Distractor Paragraphs 3 -10 ... Q: The first woman to be the chef de mission of an Australian Olympic</cell><cell>The Hanging Gardens, in Mumbai, also known as Pherozeshah Mehta Gardens, are terraced gardens ? Q: (Hanging gardens of Mumbai, country, ?)</cell></row><row><cell>team won gold medal in which winter Olympics ?</cell><cell>Options: {Iran, India, Pakistan, Somalia, ? }</cell></row><row><cell>A: 2002 Winter Olympics</cell><cell>A: India</cell></row><row><cell>HotpotQA</cell><cell>WikiHop</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>.60 59.02 20.32 64.49 10.83 40.16 GRN * 52.92 66.71 52.37 84.11 31.77 58.47 DFGN(Ours) 55.17 68.49 49.85 81.06 31.87 58.23 QFE * 53.86 68.06 57.75 84.49 34.63 59.61 DFGN(Ours) † 56.31 69.69 51.50 81.62 33.62 59.82 Table 1: Performance comparison on the private test set of HotpotQA in the distractor setting. Our DFGN is the second best result on the leaderboard before submission (on March 1st). The baseline model is from and the results with * is unpublished. DFGN(Ours) † refers to the same model with a revised entity graph, whose entities are recognized by a BERT NER model. Note that the result of DFGN(Ours) † is submitted to the leaderboard during the review process of our paper.</figDesc><table><row><cell>Model</cell><cell cols="2">Answer EM F1</cell><cell>Sup Fact EM F1</cell><cell>Joint EM</cell><cell>F1</cell></row><row><cell cols="2">Baseline Model 45Setting EM</cell><cell>F1</cell><cell></cell><cell></cell></row><row><cell>DFGN (2-layer)</cell><cell cols="2">55.42 69.23</cell><cell></cell><cell></cell></row><row><cell>-BFS Supervision</cell><cell cols="2">54.48 68.15</cell><cell></cell><cell></cell></row><row><cell>-Entity Mask</cell><cell cols="2">54.64 68.25</cell><cell></cell><cell></cell></row><row><cell>-Query Update</cell><cell cols="2">54.44 67.98</cell><cell></cell><cell></cell></row><row><cell>-E2T Process</cell><cell cols="2">53.91 67.45</cell><cell></cell><cell></cell></row><row><cell>-1 Fusion Block</cell><cell cols="2">54.14 67.70</cell><cell></cell><cell></cell></row><row><cell>-2 Fusion Blocks</cell><cell cols="2">53.44 67.11</cell><cell></cell><cell></cell></row><row><cell cols="3">-2 Fusion Blocks &amp; Bi-attn 50.03 62.83</cell><cell></cell><cell></cell></row><row><cell>gold paragraphs only</cell><cell cols="2">55.67 69.15</cell><cell></cell><cell></cell></row><row><cell>supporting facts only</cell><cell cols="2">57.57 71.67</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>4% 15.5% 29.8% 41.0% ESP EM(≤ 80) 7.1% 14.7% 29.9% 44.8% ESP Recall(≤ 40) 37.3% 46.1% 58.4% 66.4% ESP Recall(≤ 80) 34.9% 44.6% 59.1% 70.0%</figDesc><table><row><cell>k</cell><cell>1</cell><cell>2</cell><cell>5</cell><cell>10</cell></row><row><cell>ESP EM(≤ 40)</cell><cell>7.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Evaluation of reasoning chains by ESP scores on two versions of the entity graphs in the development set. ≤ 40 and ≤ 80 indicate to the maximum number of nodes in entity graphs. Note that ≤ 40 refers to the entity graph whose entities are extracted by Stanford CoreNLP, while ≤ 80 refers to the entity graph whose entities are extracted by the aforementioned BERT NER model.ESP EM (Exact Match)For a case with m supporting sentences, if all the m sentences are hit, we call this case exact match. The ESP EM score is the ratio of exactly matched cases.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Our code is available in https://github.com/ woshiyyya/DFGN-pytorch.3  The leaderboard can be found on https: //hotpotqa.github.io</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">A supporting sentence may contain irrelevant information, thus we do not have to visit all entities in a supporting sentence. Besides, due to the fusion mechanism of DFGN, the entity information will be propagated to the whole sentence. Therefore, we define a "hit" occurs when at least one entity of the supporting sentence is visited.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We introduce Dynamically Fused Graph Network (DFGN) to address multi-hop reasoning. Specifically, we propose a dynamic fusion reasoning block based on graph neural networks. Different from previous approaches in QA, DFGN is capable of predicting the sub-graphs dynamically at each reasoning step, and the entity-level reasoning is fused with token-level contexts. We evaluate DFGN on HotpotQA and achieve leading results. Besides, our analysis shows DFGN can produce reliable and explainable reasoning chains. In the future, we may incorporate new advances in building entity graphs from texts, and solve more difficult reasoning problems, e.g. the cases of comparison query type in HotpotQA.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Large-scale simple question answering with memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno>abs/1506.02075</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Question answering by reasoning across documents with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>De Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilker</forename><surname>Aziz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.09920</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural models for reasoning over multiple mentions using coreference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiao</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="42" to="48" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1693" to="1701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semisupervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.03556</idno>
		<title level="m">Stochastic answer networks for machine reading comprehension</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The stanford corenlp natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 52nd annual meeting of the association for computational linguistics: system demonstrations</title>
		<meeting>52nd annual meeting of the association for computational linguistics: system demonstrations</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Efficient and robust question answering from minimal context over documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1725" to="1735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Reasoning with memory augmented neural networks for language comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsendsuren</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Who did what: A large-scale person-centered cloze dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeshi</forename><surname>Onishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcallester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2230" to="2235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Know what you dont know: Unanswerable questions for squad</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="784" to="789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Introduction to the conll-2003 shared task: Language-independent named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik F Tjong Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fien</forename><surname>De Meulder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL</title>
		<meeting>the Seventh Conference on Natural Language Learning at HLT-NAACL</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bidirectional attention flow for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Exploring graph-structured passage representation for multihop reading comprehension with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.02040</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A graph-to-sequence model for amrto-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1616" to="1626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2440" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The web as a knowledge-base for answering complex questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="641" to="651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Gated self-matching networks for reading comprehension and question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="189" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Constructing datasets for multi-hop reading comprehension across documents. Transactions of the Association of Computational Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="287" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hotpotqa: A dataset for diverse, explainable multi-hop question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2369" to="2380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An interpretable reasoning network for multirelation question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mantong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2010" to="2022" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
