<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-view Convolutional Neural Networks for 3D Shape Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<settlement>Amherst</settlement>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
							<email>smaji@cs.umass.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<settlement>Amherst</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kalogerakis</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<settlement>Amherst</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Learned-Miller</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<settlement>Amherst</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-view Convolutional Neural Networks for 3D Shape Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A longstanding question in computer vision concerns the representation of 3D shapes for recognition: should 3D shapes be represented with descriptors operating on their native 3D formats, such as voxel grid or polygon mesh, or can they be effectively represented with view-based descriptors? We address this question in the context of learning to recognize 3D shapes from a collection of their rendered views on 2D images. We first present a standard CNN architecture trained to recognize the shapes' rendered views independently of each other, and show that a 3D shape can be recognized even from a single view at an accuracy far higher than using state-of-the-art 3D shape descriptors. Recognition rates further increase when multiple views of the shapes are provided. In addition, we present a novel CNN architecture that combines information from multiple views of a 3D shape into a single and compact shape descriptor offering even better recognition performance. The same architecture can be applied to accurately recognize human hand-drawn sketches of shapes. We conclude that a collection of 2D views can be highly informative for 3D shape recognition and is amenable to emerging CNN architectures and their derivatives.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>One of the fundamental challenges of computer vision is to draw inferences about the three-dimensional (3D) world from two-dimensional (2D) images. Since one seldom has access to 3D object models, one must usually learn to recognize and reason about 3D objects based upon their 2D appearances from various viewpoints. Thus, computer vision researchers have typically developed object recognition algorithms from 2D features of 2D images, and used them to classify new 2D pictures of those objects.</p><p>But what if one does have access to 3D models of each object of interest? In this case, one can directly train recognition algorithms on 3D features such as voxel occupancy or surface curvature. The possibility of building such classifiers of 3D shapes directly from 3D representations has recently emerged due to the introduction of large 3D shape repositories, such as 3D Warehouse, TurboSquid, and Shapeways. For example, when Wu et al. <ref type="bibr" target="#b36">[37]</ref> introduced the ModelNet 3D shape database, they presented a classifier for 3D shapes using a deep belief network architecture trained on voxel representations. While intuitively, it seems logical to build 3D shape classifiers directly from 3D models, in this paper we present a seemingly counterintuitive result -that by building classifiers of 3D shapes from 2D image renderings of those shapes, we can actually dramatically outperform the classifiers built directly on the 3D representations. In particular, a convolutional neural network (CNN) trained on a fixed set of rendered views of a 3D shape and only provided with a single view at test time increases category recognition accuracy by a remarkable 8% (77% → 85%) over the best models <ref type="bibr" target="#b36">[37]</ref> trained on 3D representations. With more views provided at test time, its performance further increases.</p><p>One reason for this result is the relative efficiency of the 2D versus the 3D representations. In particular, while a full resolution 3D representation contains all of the information about an object, in order to use a voxel-based representation in a deep network that can be trained with available samples and in a reasonable amount of time, it would appear that the resolution needs to be significantly reduced. For example, 3D ShapeNets use a coarse representation of shape, a 30×30×30 grid of binary voxels. In contrast a single projection of the 3D model of the same input size corresponds to an image of 164×164 pixels, or slightly smaller if multiple projections are used. Indeed, there is an inherent trade-off between increasing the amount of explicit depth information (3D models) and increasing spatial resolution (projected 2D models).</p><p>Another advantage of using 2D representations is that we can leverage (i) advances in image descriptors <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b25">26]</ref> and (ii) massive image databases (such as ImageNet <ref type="bibr" target="#b8">[9]</ref>) to pre-train our CNN architectures. Because images are ubiquitous and large labeled datasets are abundant, we can learn a good deal about generic features for 2D image categorization and then fine-tune to specifics about 3D model projections. While it is possible that some day as much 3D  <ref type="figure">Figure 1</ref>. Multi-view CNN for 3D shape recognition (illustrated using the 1 st camera setup). At test time a 3D shape is rendered from 12 different views and are passed thorough CNN1 to extract view based features. These are then pooled across views and passed through CNN2 to obtain a compact shape descriptor.</p><p>training data will be available, for the time being this is a significant advantage of our representation.</p><p>Although the simple strategy of classifying views independently works remarkably well (Sect. 3.2), we present new ideas for how to "compile" the information in multiple 2D views of an object into a compact object descriptor using a new architecture called multi-view CNN ( <ref type="figure">Fig. 1</ref> and Sect. 3.3). This descriptor is at least as informative for classification (and for retrieval is slightly more informative) than the full collection of view-based descriptors of the object. Moreover it facilitates efficient retrieval using either a similar 3D object or a simple hand-drawn sketch, without resorting to slower methods that are based on pairwise comparisons of image descriptors. We present state-of-the-art results on 3D object classification, 3D object retrieval using 3D objects, and 3D object retrieval using sketches (Sect. 4).</p><p>Our multi-view CNN is related to "jittering" where transformed copies of the data are added during training to learn invariances to transformations such as rotation or translation. In the context of 3D recognition the views can be seen as jittered copies. The multi-view CNN learns to combine the views instead of averaging, and thus can use the more informative views of the object for prediction while ignoring others. Our experiments show that this improves performance (Sect. 4.1) and also lets us visualize informative views of the object by back-propagating the gradients of the network to the views ( <ref type="figure" target="#fig_1">Fig. 3</ref>). Even on traditional image classification tasks multi-view CNN can be a better alternative to jittering. For example, on the sketch recognition benchmark <ref type="bibr" target="#b10">[11]</ref> a multi-view CNN trained on jittered copies performs better than a standard CNN trained with the same jittered copies (Sect. 4.2).</p><p>Pre-trained CNN models, data, and the complete source code to reproduce the results in the paper are available at http://vis-www.cs.umass.edu/mvcnn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Our method is related to prior work on shape descriptors for 3D objects and image-based CNNs. Next we discuss representative work in these areas.</p><p>Shape descriptors. A large corpus of shape descriptors has been developed for drawing inferences about 3D objects in both the computer vision and graphics literature. Shape descriptors can be classified into two broad categories: 3D shape descriptors that directly work on the native 3D representations of objects, such as polygon meshes, voxel-based discretizations, point clouds, or implicit surfaces, and viewbased descriptors that describe the shape of a 3D object by "how it looks" in a collection of 2D projections.</p><p>With the exception of the recent work of Wu et al. <ref type="bibr" target="#b36">[37]</ref> which learns shape descriptors from the voxel-based representation of an object through 3D convolutional nets, previous 3D shape descriptors were largely "hand-designed" according to a particular geometric property of the shape surface or volume. For example, shapes can be represented with histograms or bag-of-features models constructed out of surface normals and curvatures <ref type="bibr" target="#b14">[15]</ref>, distances, angles, triangle areas or tetrahedra volumes gathered at sampled surface points <ref type="bibr" target="#b24">[25]</ref>, properties of spherical functions defined in volumetric grids <ref type="bibr" target="#b15">[16]</ref>, local shape diameters measured at densely sampled surface points <ref type="bibr" target="#b3">[4]</ref>, heat kernel signatures on polygon meshes <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b18">19]</ref>, or extensions of the SIFT and SURF feature descriptors to 3D voxel grids <ref type="bibr" target="#b16">[17]</ref>. Developing classifiers and other supervised machine learning algorithms on top of such 3D shape descriptors poses a number of challenges. First, the size of organized databases with annotated 3D models is rather limited compared to image datasets, e.g., ModelNet contains about 150K shapes (its 40 category benchmark contains about 4K shapes). In contrast, the ImageNet database <ref type="bibr" target="#b8">[9]</ref> already includes tens of millions of annotated images. Second, 3D shape descriptors tend to be very high-dimensional, making classifiers prone to overfitting due to the so-called 'curse of dimensionality'.</p><p>On the other hand view-based descriptors have a number of desirable properties: they are relatively low-dimensional, efficient to evaluate, and robust to 3D shape representation artifacts, such as holes, imperfect polygon mesh tesselations, noisy surfaces. The rendered shape views can also be directly compared with other 2D images, silhouettes or even hand-drawn sketches. An early example of a view-based approach is the work by Murase and Nayar <ref type="bibr" target="#b23">[24]</ref> that recognizes objects by matching their appearance in parametric eigenspaces formed by large sets of 2D renderings of 3D models under varying poses and illuminations. Another example, which is particularly popular in computer graphics setups, is the LightField descriptor <ref type="bibr" target="#b4">[5]</ref> that extracts a set of geometric and Fourier descriptors from object silhouettes rendered from several different viewpoints. Alternatively, the silhouette of an object can be decomposed into parts and then represented by a directed acyclic graph (shock graph) <ref type="bibr" target="#b22">[23]</ref>. Cyr and Kimia <ref type="bibr" target="#b7">[8]</ref> defined similarity metrics based on curve matching and grouped similar views, called aspect graphs of 3D models <ref type="bibr" target="#b17">[18]</ref>. Eitz et al. <ref type="bibr" target="#b11">[12]</ref> compared human sketches with line drawings of 3D models produced from several different views based on local Gabor filters, while Schneider et al. <ref type="bibr" target="#b29">[30]</ref> proposed using Fisher vectors <ref type="bibr" target="#b25">[26]</ref> on SIFT features <ref type="bibr" target="#b21">[22]</ref> for representing human sketches of shapes. These descriptors are largely "hand-engineered" and some do not generalize well across different domains.</p><p>Convolutional neural networks. Our work is also related to recent advances in image recognition using CNNs <ref type="bibr" target="#b19">[20]</ref>. In particular CNNs trained on the large datasets such as ImageNet have been shown to learn general purpose image descriptors for a number of vision tasks such as object detection, scene recognition, texture recognition and finegrained classification <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b6">7]</ref>. We show that these deep architectures can be adapted to specific domains including shaded illustrations of 3D objects, line drawings, and human sketches to produce descriptors that have superior performance compared to other view-based or 3D shape descriptors in a variety of setups. Furthermore, they are compact and efficient to compute. There has been existing work on recognizing 3D objects with CNNs <ref type="bibr" target="#b20">[21]</ref> using two concatenated views (binocular images) as input. Our network instead learns a shape representation that aggregates information from any number of input views without any specific ordering, and always outputs a compact shape descriptor of the same size. Furthermore, we leverage both image and shape datasets to train our network.</p><p>Although there is significant work on 3D and 2D shape descriptors, and estimating informative views of the objects (or, aspect graphs), there is relatively little work on learning to combine the view-based descriptors for 3D shape recognition. Most methods resort to simple strategies such as performing exhaustive pairwise comparisons of descriptors extracted from different views of each shape, or concatenating descriptors from ordered, consistent views. In contrast our multi-view CNN architecture learns to recognize 3D shapes from views of the shapes using image-based CNNs but in the context of other views via a view-pooling layer. As a result, information from multiple views is effectively accumulated into a single, compact shape descriptor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>As discussed above, our focus in this paper is on developing view-based descriptors for 3D shapes that are trainable, produce informative representations for recognition and retrieval tasks, and are efficient to compute.</p><p>Our view-based representations start from multiple views of a 3D shape, generated by a rendering engine. A simple way to use multiple views is to generate a 2D image descriptor per each view, and then use the individual descriptors directly for recognition tasks based on some voting or alignment scheme. For example, a naïve approach would be to average the individual descriptors, treating all the views as equally important. Alternatively, if the views are rendered in a reproducible order, one could also concatenate the 2D descriptors of all the views. Unfortunately, aligning a 3D shape to a canonical orientation is hard and sometimes ill-defined. In contrast to the above simple approaches, an aggregated representation combining features from multiple views is more desirable since it yields a single, compact descriptor representing the 3D shape.</p><p>Our approach is to learn to combine information from multiple views using a unified CNN architecture that includes a view-pooling layer ( <ref type="figure">Fig. 1</ref>). All the parameters of our CNN architecture are learned discriminatively to produce a single compact descriptor for the 3D shape. Compared to exhaustive pairwise comparisons between singleview representations of 3D shapes, our resulting descriptors can be directly used to compare 3D shapes leading to significantly higher computational efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Input: A Multi-view Representation</head><p>3D models in online databases are typically stored as polygon meshes, which are collections of points connected with edges forming faces. To generate rendered views of polygon meshes, we use the Phong reflection model <ref type="bibr" target="#b26">[27]</ref>. The mesh polygons are rendered under a perspective projection and the pixel color is determined by interpolating the reflected intensity of the polygon vertices. Shapes are uniformly scaled to fit into the viewing volume.</p><p>To create a multi-view shape representation, we need to setup viewpoints (virtual cameras) for rendering each mesh.</p><p>We experimented with two camera setups. For the 1 st camera setup, we assume that the input shapes are upright oriented along a consistent axis (e.g., z-axis). Most models in modern online repositories, such as the 3D Warehouse, satisfy this requirement, and some previous recognition methods also follow the same assumption <ref type="bibr" target="#b36">[37]</ref>. In this case, we create 12 rendered views by placing 12 virtual cameras around the mesh every 30 degrees (see <ref type="figure">Fig. 1</ref>). The cameras are elevated 30 degrees from the ground plane, pointing towards the centroid of the mesh. The centroid is calculated as the weighted average of the mesh face centers, where the weights are the face areas. For the 2 nd camera setup, we do not make use of the assumption about consistent upright orientation of shapes. In this case, we render from several more viewpoints since we do not know beforehand which ones yield good representative views of the object. The renderings are generated by placing 20 virtual cameras at the 20 vertices of an icosahedron enclosing the shape. All cameras point towards the centroid of the mesh. Then we generate 4 rendered views from each camera, using 0, 90, 180, 270 degrees rotation along the axis passing through the camera and the object centroid, yielding total 80 views.</p><p>We note that using different shading coefficients or illumination models did not affect our output descriptors due to the invariance of the learned filters to illumination changes, as also observed in image-based CNNs <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b9">10]</ref>. Adding more or different viewpoints is trivial, however, we found that the above camera setups were already enough to achieve high performance. Finally, rendering each mesh from all the viewpoints takes no more than ten milliseconds on modern graphics hardware.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Recognition with Multi-view Representations</head><p>We claim that our multi-view representation contains rich information about 3D shapes and can be applied to various types of tasks. In the first setting, we make use of existing 2D image features directly and produce a descriptor for each view. This is the most straightforward approach to utilize the multi-view representation. However, it results in multiple 2D image descriptors per 3D shape, one per view, which need to be integrated somehow for recognition tasks.</p><p>Image descriptors. We consider two types of image descriptors for each 2D view: a state-of-the-art "hand-crafted" image descriptor based on Fisher vectors <ref type="bibr" target="#b28">[29]</ref> with multiscale SIFT, as well as CNN activation features <ref type="bibr" target="#b9">[10]</ref>.</p><p>The Fisher vector image descriptor is implemented using VLFeat <ref type="bibr" target="#b35">[36]</ref>. For each image multi-scale SIFT descriptors are extracted densely. These are then projected to 80 dimensions with PCA, followed by Fisher vector pooling with a Gaussian mixture model with 64 components, square-root and 2 normalization.</p><p>For our CNN features we use the VGG-M network from <ref type="bibr" target="#b2">[3]</ref> which consists of mainly five convolutional layers conv 1,...,5 followed by three fully connected layers fc 6,..., <ref type="bibr" target="#b7">8</ref> and a softmax classification layer. The penultimate layer fc 7 (after ReLU non-linearity, 4096-dimensional) is used as image descriptor. The network is pre-trained on Ima-geNet images from 1k categories, and then fine-tuned on all 2D views of the 3D shapes in training set. As we show in our experiments, fine-tuning improves performance significantly. Both Fisher vectors and CNN features yield very good performance in classification and retrieval compared with popular 3D shape descriptors (e.g., SPH <ref type="bibr" target="#b15">[16]</ref>, LFD <ref type="bibr" target="#b4">[5]</ref>) as well as 3D ShapeNets <ref type="bibr" target="#b36">[37]</ref>.</p><p>Classification. We train one-vs-rest linear SVMs (each view is treated as a separate training sample) to classify shapes using their image features. At test time, we simply sum up the SVM decision values over all 12 views and return the class with the highest sum. Alternative approaches, e.g., averaging image descriptors, lead to worse accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Retrieval.</head><p>A distance or similarity measure is required for retrieval tasks. For shape x with n x image descriptors and shape y with n y image descriptors, the distance between them is defined in Eq. 1. Note that the distance between two 2D images is defined as the 2 distance between their feature vectors, i.e. x i − y j 2 .</p><formula xml:id="formula_0">d(x, y) = j min i x i − y j 2 2n y + i min j x i − y j 2 2n x<label>(1)</label></formula><p>To interpret this definition, we can first define the distance between a 2D image x i and a 3D shape y as d(x i , y) = min j x i − y j 2 . Then given all n x distances between x's 2D projections and y, the distance between these two shapes is computed by simple averaging. In Eq. 1, this idea is applied in both directions to ensure symmetry.</p><p>We investigated alternative distance measures, such as minimun distance among all n x · n y image pairs and the distance between average image descriptors, but they all led to inferior performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Multi-view CNN: Learning to Aggregate Views</head><p>Although having multiple separate descriptors for each 3D shape can be successful for classification and retrieval compared to existing 3D descriptors, it can be inconvenient and inefficient in many cases. For example, in Eq. 1, we need to compute all n x ×n y pairwise distances between images in order to compute distance between two 3D shapes. Simply averaging or concatenating the image descriptors leads to inferior performance. In this section, we focus on the problem of learning to aggregate multiple views in order to synthesize the information from all views into a single, compact 3D shape descriptor.</p><p>We design the multi-view CNN (MVCNN) on top of image-based CNNs <ref type="figure">(Fig. 1)</ref>. Each image in a 3D shape's multi-view representation is passed through the first part of the network (CNN 1 ) separately, aggregated at a viewpooling layer, and then sent through the remaining part of the network (CNN 2 ). All branches in the first part of the network share the same parameters in CNN 1 . We use element-wise maximum operation across the views in the view-pooling layer. An alternative is element-wise mean operation, but it is not as effective in our experiments. The view-pooling layer can be placed anywhere in the network. We show in our experiments that it should be placed close to the last convolutional layer (conv 5 ) for optimal classification and retrieval performance. View-pooling layers are closely related to max-pooling layers and maxout layers <ref type="bibr" target="#b13">[14]</ref>, with the only difference being the dimension that their pooling operations are carried out on. The MVCNN is a directed acyclic graphs and can be trained or fine-tuned using stochastic gradient descent with back-propagation.</p><p>Using fc 7 (after ReLU non-linearity) in an MVCNN as an aggregated shape descriptor, we achieve higher performance than using separate image descriptors from an image-based CNN directly, especially in retrieval (62.8% → 70.1%). Perhaps more importantly, the aggregated descriptor is readily available for a variety of tasks, e.g., shape classification and retrieval, and offers significant speed-ups against multiple image descriptors.</p><p>An MVCNN can also be used as a general framework to integrate perturbed image samples (also known as data jittering). We illustrate this capability of MVCNNs in the context of sketch recognition in Sect. 4.2.</p><p>Low-rank Mahalanobis metric. Our MVCNN is finetuned for classification, and thus retrieval performance is not directly optimized. Although we could train it with a different objective function suitable for retrieval, we found that a simpler approach can readily yield a significant retrieval performance boost (see row 12 in Tab. 1). We learn a Mahalanobis metric W that directly projects MVCNN descriptors φ ∈ R d to W φ ∈ R p , such that the 2 distances in the projected space are small between shapes of the same category, and large otherwise. We use the large-margin metric learning algorithm and implementation from <ref type="bibr" target="#b31">[32]</ref>, with p &lt; d to make the final descriptor compact (p = 128 in our experiments). The fact that we can readily use metric learning over the output shape descriptor demonstrates another advantage of using MVCNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">3D Shape Classification and Retrieval</head><p>We evaluate our shape descriptors on the Princeton Mod-elNet dataset <ref type="bibr" target="#b0">[1]</ref>. ModelNet currently contains 127,915 3D CAD models from 662 categories. <ref type="bibr" target="#b0">1</ref> A 40-class wellannotated subset containing 12,311 shapes from 40 common categories, ModelNet40, is provided on the ModelNet website. For our experiments, we use the same training and test split of ModelNet40 as in <ref type="bibr" target="#b36">[37]</ref>. <ref type="bibr" target="#b1">2</ref> Our shape descriptors are compared against the 3D ShapeNets by Wu et al. <ref type="bibr" target="#b36">[37]</ref>, the Spherical Harmonics descriptor (SPH) by Kazhdan et al. <ref type="bibr" target="#b15">[16]</ref>, the LightField descriptor (LFD) by Chen et al. <ref type="bibr" target="#b4">[5]</ref>, and Fisher vectors extracted on the same rendered views of the shapes used as input to our networks.</p><p>Results on shape classification and retrieval are summarized in Tab. 1. Precision-recall curves are provided in <ref type="figure">Fig. 2</ref>. Remarkably the Fisher vector baseline with just a single view achieves a classification accuracy of 78.8% outperforming the state-of-the-art learned 3D descriptors (77.3% <ref type="bibr" target="#b36">[37]</ref>). When all 12 views of the shape are available at test time (based on our first camera setup), we can also average the predictions over these views. Averaging increases the performance of Fisher vectors to 84.8%. The performance of Fisher vectors further supports our claim that 3D objects can be effectively represented using viewbased 2D representations. The trends in performance for shape retrieval are similar.</p><p>Using our CNN baseline trained on ImageNet in turn outperforms Fisher vectors by a significant margin. Finetuning the CNN on the rendered views of the training shapes of ModelNet40 further improves the performance. By using all 12 views of the shape, its classification accuracy reaches 88.6%, and mean average precision (mAP) for retrieval is also improved to 62.8%.</p><p>Our MVCNN outperforms all state-of-the-art descriptors as well as the Fisher vector and CNN baselines. With finetuning on the ModelNet40 training set, our model achieves 89.9% classification accuracy, and 70.1% mAP on retrieval using the first camera setup. If we do not make use of the assumption about consistent upright orientation of shapes (second camera setup), the performance remains still intact, achieving 90.1% classification accuracy and 70.4% retrieval mAP. MVCNN constitutes an absolute gain of 12.8% in classification accuracy compared to the state-ofthe-art learned 3D shape descriptor <ref type="bibr" target="#b36">[37]</ref> (77.3% → 90.1%). Similarly, retrieval mAP is improved by 21.2% (49.2% → 70.4%). Finally, learning a low-rank Mahalanobis metric improves retrieval mAP further while classification accuracy remains almost unchanged, and the resulting shape descriptors become more compact (d = 4096, p = 128).</p><p>We considered different locations to place the viewpooling layer in the MVCNN. Performance is not very sen- <ref type="bibr" target="#b0">1</ref> As of 09/24/2015. <ref type="bibr" target="#b1">2</ref> Based on our correspondence with the authors of <ref type="bibr" target="#b36">[37]</ref>, for each category the first 80 shapes in the "train" folder (or all shapes if there are fewer than 80) should be used for training, while the first 20 shapes in the "test" folder should be used for testing.  <ref type="figure">Figure 2</ref>. Precision-recall curves for various methods for 3D shape retrieval on the ModelNet40 dataset. Our method significantly outperforms the state-of-the-art on this task achieving 80.2% mAP.</p><p>sitive among the later few layers (conv 4 ∼fc 7 ); however any location prior to conv 4 decreases classification accuracies significantly. We find conv 5 offers slightly better accuracies (∼1%), and thus use it for all our experiments.</p><p>Saliency map among views. For each 3D shape S, our multi-view representation consists of a set of K 2D views {I 1 , I 2 . . . I K }. We would like to rank pixels in the 2D views w.r.t. their influence on the output score F c of the network (e.g. taken from fc 8 layer) for its ground truth class c. Following <ref type="bibr" target="#b32">[33]</ref>, saliency maps can be defined as the derivatives of F c w.r.t. the 2D views of the shape:</p><formula xml:id="formula_1">[w 1 , w 2 . . . w K ] = ∂F c ∂I 1 S , ∂F c ∂I 2 S . . . ∂F c ∂I K S<label>(2)</label></formula><p>For MVCNN, w in Eq. 2 can be computed using backpropagation with all the network parameters fixed, and can then be rearranged to form saliency maps for individual views. Examples of saliency maps are shown in <ref type="figure" target="#fig_1">Fig. 3.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Sketch Recognition: Jittering Revisited</head><p>Given the success of our aggregated descriptors on multiple views of a 3D object, it is logical to ask whether aggregating multiple views of a 2D image could also improve performance. Here we show that this is indeed the case by exploring its connection with data jittering in the context of sketch recognition.</p><p>Data jittering, or data augmentation, is a method to generate extra samples from a given image. It is the process of perturbing the image by transformations that change its appearance while leaving the high-level information (class  label, attributes, etc.) intact. Jittering can be applied at training time to augment training samples and to reduce overfitting, or at test time to provide more robust predictions. In particular, several authors <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b34">35]</ref> have used data jittering to improve the performance of deep representations on 2D image classification tasks. In these applications, jittering at training time usually includes random image translations (implemented as random crops), horizontal reflections, and color perturbations. At test time jittering usually only includes a few crops (e.g., four at the corners, one at the center and their horizontal reflections). We now examine whether we can get more benefit out of jittered views of an image by using the same feature aggregation scheme we developed for recognizing 3D shapes.</p><p>The human sketch dataset <ref type="bibr" target="#b10">[11]</ref> contains 20,000 handdrawn sketches of 250 object categories such as airplanes, apples, bridges, etc. The accuracy of humans in recognizing these hand-drawings is only 73% because of the low quality of some sketches. Schneider and Tuytelaars <ref type="bibr" target="#b29">[30]</ref> cleaned up the dataset by removing instances and categories that humans find hard to recognize. This cleaned dataset (SketchClean) contains 160 categories, on which humans can achieve 93% recognition accuracy. Using SIFT Fisher vectors with spatial pyramid pooling and linear SVMs, Schneider and Tuytelaars achieved 68.9% recognition accuracy on the original dataset and 79.0% on the SketchClean dataset. We split the SketchClean dataset randomly into training, validation and test set, <ref type="bibr" target="#b2">3</ref> and report classification accuracy on the test set in Tab. 2.</p><p>With an off-the-shelf CNN (VGG-M from <ref type="bibr" target="#b2">[3]</ref>), we are able to get 77.3% classification accuracy without any network fine-tuning. With fine-tuning on the training set, the accuracy can be further improved to 84.0%, significantly surpassing the Fisher vector approach. These numbers are <ref type="bibr" target="#b2">3</ref> The dataset does not come with a standard training/val/test split. achieved by using the penultimate layer (fc 7 ) in the network as image descriptors and linear SVMs.</p><p>Although it is impractical to get multiple views from 2D images, we can use jittering to mimic the effect of views. For each hand-drawn sketch, we do in-plane rotation with three angles: −45°, 0°, 45°, and also horizontal reflections (hence 6 samples per image). We apply the two CNN variants (regular CNN and MVCNN) discussed earlier for aggregating multiple views of 3D shapes, and get 85.5% (CNN w/o view-pooling) and 86.3% (MVCNN w/ viewpooling on fc 7 ) classification accuracy respectively. The latter also has the advantage of a single, more compact descriptor for each hand-drawn sketch.</p><p>With a deeper network architecture (VGG-VD, a network with 16 weight layers from <ref type="bibr" target="#b33">[34]</ref>), we achieve 87.2% accuracy, further advancing the classification performance, and closely approaching human performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Sketch-based 3D Shape Retrieval</head><p>Due to the growing number of online 3D repositories, many approaches have been investigated to perform efficient 3D shape retrieval. Most online repositories (e.g. 3D Warehouse, TurboSquid, Shapeways) provide only textbased search engines or hierarchical catalogs for 3D shape retrieval. However, it is hard to convey stylistic and geometric variations using only textual descriptions, thus sketchbased shape retrieval <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b11">12]</ref> has been proposed as an alternative for users to retrieve shapes with an approximate sketch of the desired 3D shape in mind. Sketchbased retrieval is challenging since it involves two heterogeneous data domains (hand-drawn sketches and 3D shapes), and sketches can be highly abstract and visually different from target 3D shapes. Here we demonstrate the potential strength of MVCNNs in sketch-based shape retrieval.</p><p>For this experiment, we construct a dataset containing  193 sketches and 790 CAD models from 10 categories existing in both SketchClean and ModelNet40. Following <ref type="bibr" target="#b11">[12]</ref>, we produce renderings of 3D shapes with a style similar to hand-drawn sketches (see <ref type="figure" target="#fig_2">Fig. 4</ref>). This is achieved by detecting Canny edges on the depth buffer (also known as z-buffer) from 12 viewpoints. These edge maps are then passed through CNNs to obtain image descriptors. Descriptors are also extracted from 6 perturbed samples of each query sketch in the manner described in Sect. 4.2. Finally we rank 3D shapes w.r.t. "average minimum distance" (Eq. 1) to the sketch descriptors. Representative retrieval results are shown in <ref type="figure" target="#fig_3">Fig. 5</ref>.</p><p>We are able to retrieve 3D objects from the same class with the query sketch, as well as being visually similar, especially in the top few matches. Our performance is 36.1% mAP on this dataset. Here we use the VGG-M network trained on ImageNet without any fine-tuning on either sketches or 3D shapes. With a fine-tuning procedure that optimizes a distance measure between hand-drawn sketches and 3D shapes, e.g., by using a Siamese Network <ref type="bibr" target="#b5">[6]</ref>, retrieval performance can be further improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>While the world is full of 3D shapes, as humans at least, we understand that world is mostly through 2D images. We have shown that using images of shapes as inputs to modern learning architectures, we can achieve performance better than any previously published results, including those that operate on direct 3D representations of shapes. While even a näive usage of these multiple 2D projections yields impressive discrimination performance, by building descriptors that are aggregations of information from multiple views, we can achieve compactness, efficiency, and better accuracy. In addition, by relating the content of 3D shapes to 2D representations like sketches, we can retrieve these 3D shapes at high accuracy using sketches, and leverage the implicit knowledge of 3D shapes contained in their 2D views.</p><p>There are a number of directions to explore in future work. One is to experiment with different combinations of 2D views. Which views are most informative? How many views are necessary for a given level of accuracy? Can informative views be selected on the fly?</p><p>Another obvious question is whether our view aggregating techniques can be used for building compact and discriminative descriptors for real-world 3D objects from multiple views, or automatically from video, rather than merely for 3D polygon mesh models. Such investigations could be immediately applicable to widely studied problems such as object recognition and face recognition.</p><p>Here we provide additional evaluations and visulizations of our multi-view CNN (MVCNN), including a) confusion matrix of 3D shape classification; b) additional view-based saliency maps; and c) examples of correctly and wrongly classified hand-drawn sketches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. 3D shape classification</head><p>Confusion matrix of 3D shape classification on Model-Net40 is given in <ref type="figure" target="#fig_4">Figure 6</ref>. Here MVCNN with fine-tuning on 12 views (row 11 in <ref type="table" target="#tab_1">Table 1</ref> of main submission) is used.</p><p>Top confusions occur at 1) flower pot → plant (45%), 2) table → desk (32%), 3) flower pot → vase (20%), 4) plant → flower (19%), and 5) stool → chair (15%). Distinctions between some of these pairs are ambiguous even for humans.  <ref type="table">lamp  laptop  mantel  monitor  night stand  person  piano  plant  radio  range hood  sink  sofa  stairs  stool  table  tent  toilet  tv stand  vase</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Image-specific class saliency visualization across views</head><p>Additional examples of saliency maps are shown in <ref type="figure">Figure 7</ref>. Note that the saliency maps tend to highlight a) the most canonical views accross views, e.g. the front view of the bench; and b) the most discriminative parts within views, e.g. the faucet and the sink hole of the bathtub.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Sketch classification</head><p>Examples of correctly and wrongly classified handdrawn sketches are shown in <ref type="figure" target="#fig_5">Figure 8</ref>. Most misclassified sketches contain visually similar components with the target class, e.g. spider and crab have a similar layout of legs, and some are difficult to recognize even for humans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Document changelog</head><p>v1 Initial version. v2 An updated ModelNet40 training/test split is used for experiments in order to be consistent with <ref type="bibr" target="#b36">[37]</ref>. Performance of most methods drops a bit because of the smaller training set (the full ModelNet40 was used in v1). Results with low-rank Mahalanobis metric learning are added. v3 A second camera setup without the upright orientation assumption is added. Some accuracy and mAP numbers are changed slightly because a small issue in mesh rendering related to specularities is fixed.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Top three views with the highest saliency are highlighted in blue and the relative magnitude of gradient energy for each view is shown on top. The saliency maps are computed by back-propagating the gradients of the class score onto the images via the view-pooling layer. Notice that the handles of the dresser are the most discriminative features. (Figures are enhanced for visibility.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Line-drawing style rendering from 3D shapes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Sketch-based 3D shape retrieval examples. Top matches are shown for each query, with mistakes highlighted in red.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Confusion matrix of ModelNet40 classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 .</head><label>8</label><figDesc>Examples of correctly and wrongly classified hand-drawn sketches. All sketches in each row are classified into the class labeled on top left. False positives are in red, with their ground truth classes labeled on top.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Classification and retrieval results on the ModelNet40 dataset. On the top are results using state-of-the-art 3D shape descriptors. Our view-based descriptors including Fisher vectors (FV) significantly outperform these even when a single view is available at test time (#Views = 1). When multiple views (#Views=12 or 80) are available at test time, the performance of view-based methods improve significantly. The multi-view CNN (MVCNN) architecture outperforms the view-based methods, especially for retrieval.</figDesc><table><row><cell>Method</cell><cell cols="3">Training Config. Pre-train Fine-tune #Views</cell><cell cols="2">Test Config. Classification (Accuracy) #Views</cell><cell>Retrieval (mAP)</cell></row><row><cell>(1) SPH [16]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>68.2%</cell><cell>33.3%</cell></row><row><cell>(2) LFD [5]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>75.5%</cell><cell>40.9%</cell></row><row><cell>(3) 3D ShapeNets [37]</cell><cell cols="2">ModelNet40 ModelNet40</cell><cell>-</cell><cell>-</cell><cell>77.3%</cell><cell>49.2%</cell></row><row><cell>(4) FV</cell><cell>-</cell><cell>ModelNet40</cell><cell>12</cell><cell>1</cell><cell>78.8%</cell><cell>37.5%</cell></row><row><cell>(5) FV, 12×</cell><cell>-</cell><cell>ModelNet40</cell><cell>12</cell><cell>12</cell><cell>84.8%</cell><cell>43.9%</cell></row><row><cell>(6) CNN</cell><cell>ImageNet1K</cell><cell>-</cell><cell>-</cell><cell>1</cell><cell>83.0%</cell><cell>44.1%</cell></row><row><cell>(7) CNN, f.t.</cell><cell cols="2">ImageNet1K ModelNet40</cell><cell>12</cell><cell>1</cell><cell>85.1%</cell><cell>61.7%</cell></row><row><cell>(8) CNN, 12×</cell><cell>ImageNet1K</cell><cell>-</cell><cell>-</cell><cell>12</cell><cell>87.5%</cell><cell>49.6%</cell></row><row><cell>(9) CNN, f.t.,12×</cell><cell cols="2">ImageNet1K ModelNet40</cell><cell>12</cell><cell>12</cell><cell>88.6%</cell><cell>62.8%</cell></row><row><cell>(10) MVCNN, 12×</cell><cell>ImageNet1K</cell><cell>-</cell><cell>-</cell><cell>12</cell><cell>88.1%</cell><cell>49.4%</cell></row><row><cell>(11) MVCNN, f.t., 12×</cell><cell cols="2">ImageNet1K ModelNet40</cell><cell>12</cell><cell>12</cell><cell>89.9%</cell><cell>70.1%</cell></row><row><cell cols="3">(12) MVCNN, f.t.+metric, 12× ImageNet1K ModelNet40</cell><cell>12</cell><cell>12</cell><cell>89.5%</cell><cell>80.2%</cell></row><row><cell>(13) MVCNN, 80×</cell><cell>ImageNet1K</cell><cell>-</cell><cell>80</cell><cell>80</cell><cell>84.3%</cell><cell>36.8%</cell></row><row><cell>(14) MVCNN, f.t., 80×</cell><cell cols="2">ImageNet1K ModelNet40</cell><cell>80</cell><cell>80</cell><cell>90.1%</cell><cell>70.4%</cell></row><row><cell cols="3">(15) MVCNN, f.t.+metric, 80× ImageNet1K ModelNet40</cell><cell>80</cell><cell>80</cell><cell>90.1%</cell><cell>79.5%</cell></row><row><cell cols="3">* f.t.=fine-tuning, metric=low-rank Mahalanobis metric learning</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Figure 7. Additional examples of saliency maps. Top three views with the highest saliency are highlighted in blue and the relative magnitudes of gradient energy for each view is shown on top.</figDesc><table><row><cell>snail</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>snake</cell><cell>telephone</cell><cell>squirrel</cell><cell>ear</cell><cell cols="2">pipe</cell><cell cols="2">monkey</cell></row><row><cell>0.15</cell><cell>0.08</cell><cell>0.09</cell><cell>0.22</cell><cell>0.17</cell><cell>0.06</cell><cell>0.06</cell><cell>0.05</cell><cell>0.04</cell><cell>0.06</cell><cell></cell><cell>0.04</cell><cell></cell><cell>0.08</cell></row><row><cell>butterfly</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>bee</cell><cell>bell</cell><cell>bee</cell><cell>octopus</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>17.05 guitar</cell><cell>4.76</cell><cell>7.15</cell><cell>5.26</cell><cell>3.76</cell><cell>4.94</cell><cell>13.81 lightbulb</cell><cell>4.59 violin</cell><cell>6.11 windmill</cell><cell cols="3">31.39 wine bottle violin 7.39</cell><cell cols="2">5.28 lightbulb</cell></row><row><cell>church</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>castle</cell><cell>castle</cell><cell>candle</cell><cell>house</cell><cell cols="2">crown</cell><cell></cell><cell></cell></row><row><cell>0.02 grapes</cell><cell>0.04</cell><cell>0.01</cell><cell>0.01</cell><cell>0.01</cell><cell>0.01</cell><cell>0.01 sea turtle</cell><cell>0.01</cell><cell>0.01</cell><cell>0.00</cell><cell></cell><cell>0.01</cell><cell></cell><cell>0.02</cell></row><row><cell>squirrel</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>dragon</cell><cell>monkey</cell><cell>angel</cell><cell>monkey</cell><cell cols="2">elephant</cell><cell></cell><cell></cell></row><row><cell>21.08 flying bird</cell><cell>53.82</cell><cell>28.73</cell><cell>6.31</cell><cell>10.59</cell><cell>6.79</cell><cell>8.56 squirrel</cell><cell>9.87 dragon</cell><cell>9.79 knife</cell><cell>5.92 duck</cell><cell>leaf</cell><cell>22.33</cell><cell>cat</cell><cell>26.07</cell></row><row><cell>laptop</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>book</cell><cell>suitcase</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>397.12 pen</cell><cell>222.53</cell><cell>459.61</cell><cell>74.91</cell><cell>48.79</cell><cell>37.11</cell><cell cols="3">47.61 wrist watch wrist watch syringe 42.08 18.74</cell><cell>250.39 cigarette</cell><cell cols="2">251.21</cell><cell cols="2">151.78</cell></row><row><cell>knife</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>pen</cell><cell>hammer</cell><cell>pen</cell><cell>scissors</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>51.45</cell><cell>26.05</cell><cell>30.05</cell><cell>11.75</cell><cell>26.89</cell><cell>128.24</cell><cell>97.69</cell><cell>55.04</cell><cell>32.19</cell><cell>12.62</cell><cell cols="2">16.42</cell><cell></cell><cell>37.77</cell></row><row><cell>strawberry</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>pineapple</cell><cell>pineapple</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>bicycle</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>wheelbarrow</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>4.12</cell><cell>9.95</cell><cell>3.94</cell><cell>1.07</cell><cell>1.11</cell><cell>0.61</cell><cell>1.01</cell><cell>0.56</cell><cell>1.28</cell><cell>0.00</cell><cell></cell><cell>3.81</cell><cell></cell><cell>6.00</cell></row><row><cell>crab</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>spider</cell><cell>cat</cell><cell>spider</cell><cell>spider</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>hammer</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>mushroom</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.01</cell><cell>0.01</cell><cell>0.01</cell><cell>0.01</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.01</cell><cell>0.02</cell><cell></cell><cell>0.01</cell><cell></cell><cell>0.00</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements We thank Yanjie Li for her help on rendering meshes. We thank NVIDIA for their generous donation of GPUs used in this research. Our work was partially supported by NSF (CHS-1422441).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<ptr target="http://modelnet.cs.princeton.edu/" />
		<title level="m">The Princeton ModelNet</title>
		<imprint>
			<date type="published" when="2015-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Shape Google: Geometric words and expressions for invariant shape retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ovsjanikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Data-driven suggestions for creativity support in 3D modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On visual similarity based 3D model retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ouhyoung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eurographics</title>
		<meeting>Eurographics</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Describing textures in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A similarity-based aspect-graph approach to 3D object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Cyr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">B</forename><surname>Kimia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">57</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">DeCAF: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno>abs/1310.1531</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">How do humans sketch objects?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Eitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Alexa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Sketch-based shape retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Eitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Boubekeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hildebrand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Alexa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>Maxout networks. ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Extended gaussian images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">K P</forename><surname>Horn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE</title>
		<meeting>of the IEEE</meeting>
		<imprint>
			<date type="published" when="1984" />
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="1671" to="1686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Rotation invariant spherical harmonic representation of 3D shape descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kazhdan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rusinkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Symposium of Geometry Processing</title>
		<meeting>Symposium of Geometry essing</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Hough transform and 3D SURF for robust three dimensional classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Knopp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Willems</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The singularities of the visual mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Koenderink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Van Doorn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological cybernetics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="51" to="59" />
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Intrinsic shape context descriptors for deformable shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Litman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS. 2012. 3, 4</title>
		<meeting>NIPS. 2012. 3, 4</meeting>
		<imprint>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning methods for generic object recognition with invariance to pose and lighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Object recognition from local scale-invariant features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">View-based 3-D object recognition using shock graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Macrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shokoufandeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dickinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Siddiqi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICPR</title>
		<meeting>ICPR</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Visual learning and recognition of 3-D objects from appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Murase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
		<idno>1995. 3</idno>
		<imprint>
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Shape distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Osada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chazelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dobkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Improving the Fisher kernel for large-scale image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Illumination for computer generated pictures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Phong</surname></persName>
		</author>
		<idno>1975. 3</idno>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">CNN features off-the-shelf: An astounding baseline for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Razavin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DeepVision workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Image classification with the Fisher vector: Theory and practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sketch classification and classification-driven analysis using Fisher vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<idno>174:1-174:9</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Discriminative sketch-based 3D model retrieval via robust shape matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fisher vector faces in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Deep inside convolutional networks: Visualising image classification models and saliency maps. CoRR, abs/1312</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">6034</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno>abs/1409.4842</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">VLFeat: An open and portable library of computer vision algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fulkerson</surname></persName>
		</author>
		<ptr target="http://www.vlfeat.org/" />
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">3D ShapeNets: A deep representation for volumetric shape modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Sketchbased 3D model retrieval using diffusion tensor fields of suggestive contours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schreck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kuijper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Multimedia</title>
		<meeting>International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
