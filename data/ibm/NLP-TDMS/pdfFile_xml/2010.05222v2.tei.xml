<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Partial FC: Training 10 Million Identities on a Single Machine</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-01-23">23 Jan 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>An</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DeepGlint</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuhan</forename><surname>Zhu</surname></persName>
							<email>zhuxuhan@bupt.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Xiao</surname></persName>
							<email>yangxiaoxtu@foxmail.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Xiangtan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lan</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DeepGlint</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DeepGlint</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DeepGlint</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Qin</surname></persName>
							<email>binqin@deepglint.com</email>
							<affiliation key="aff0">
								<orgName type="department">DeepGlint</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debing</forename><surname>Zhang</surname></persName>
							<email>debingzhangchina@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Fu</surname></persName>
							<email>fuying@bit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">DeepGlint</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Beijing Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Partial FC: Training 10 Million Identities on a Single Machine</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-01-23">23 Jan 2021</date>
						</imprint>
					</monogr>
					<note>arXiv:2010.05222v2 [cs.CV]</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Face recognition has been an active and vital topic among computer vision community for a long time. Previous researches mainly focus on loss functions used for facial feature extraction network, among which the improvements of softmax-based loss functions greatly promote the performance of face recognition. However, the contradiction between the drastically increasing number of face identities and the shortage of GPU memories is gradually becoming irreconcilable. In this paper, we thoroughly analyze the optimization goal of softmax-based loss functions and the difficulty of training massive identities. We find that the importance of negative classes in softmax function in face representation learning is not as high as we previously thought. The experiment demonstrates no loss of accuracy when training with only 10% randomly sampled classes for the softmax-based loss functions, compared with training with full classes using state-of-the-art models on mainstream benchmarks. We also implement a very efficient distributed sampling algorithm, taking into account model accuracy and training efficiency, which uses only eight NVIDIA RTX2080Ti to complete classification tasks with tens of millions of identities. The code of this paper has been made available https://github.com/ deepinsight/insightface/tree/master/recognition/partial fc.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Face recognition is playing an increasingly important role in modern life and has been widely used in residential security, face authentication , and criminal investigation. During the learning process of face recognition models, the features of each person in the dataset are mapped to so-called embedding space, where the features belonging to the same person are pulled together and the features belonging to different persons are pushed away on the Euclidean distance basis. A golden rule is that the more identities the dataset provides, the more information the model can learn, and, further, the stronger the ability can be acquired to distinguish these features <ref type="bibr" target="#b0">(Cao, Li, and Zhang 2018;</ref><ref type="bibr" target="#b1">Deng et al. 2019)</ref>. Many companies have training sets with millions and even tens of millions of face identities. For instance, Google's face dataset in 2015 already had 200 million images consisting of 8 million different identities <ref type="bibr" target="#b15">(Schroff, Kalenichenko, and Philbin 2015)</ref>.</p><p>Copyright © 2021, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. The softmax loss and its variants <ref type="bibr" target="#b19">(Wang et al. 2018b;</ref><ref type="bibr" target="#b1">Deng et al. 2019;</ref><ref type="bibr" target="#b18">Wang et al. 2018a;</ref><ref type="bibr" target="#b10">Liu et al. 2017</ref>) are widely used as objectives for face recognition. In general, they make global feature-to-class comparisons during the multiplication between the embedding features and the linear transformation matrix. In spite of that, when there are huge number of identities in the training set, the cost of storage and calculation of the final linear matrix easily exceed the current GPU capabilities, resulting in failure to train. <ref type="bibr">Zhang et al.</ref> reduces the amount of calculation by dynamically selecting active classes in each mini-batch and using only a subset of the classes (partial-classes) to approximate full-class softmax . <ref type="bibr">Deng et al. mitigate</ref> the memory pressure of each GPU through model parallel, and calculate the full-class softmax with very little communication <ref type="bibr" target="#b1">(Deng et al. 2019</ref>). The problem with selecting active classes is that when the number of identites is large to a certain extent, such as 10 million, the time consumption cannot be ignored when retrieving the active classes through features. As for model parallel, the merit of memory savings of distributed GPUs has its bottleneck. When the number of identities grows, the increase of GPU amount can indeed alleviate the problem of storing weight matrix W , whereas the storage of final logits will put new burden on GPU memories.</p><p>In this paper, we propose an efficient face recognition training strategy that can accomplish ultra-large-scale face recognition training. Specifically, we first equally store the non-overlapping subsets of softmax linear transformation matrix on all GPUs in order. Then each GPU is accountable for calculating the sum of the dot product of sampled sub-matrix that is stored on its own and input features. After that each GPU gathers the local sum from other GPUs to approximate the full-class softmax function. By only communicating the sampled local sum, we approximate full-class softmax with only a small amount of communication. This method greatly reduces the communication, calculation, and storage costs on each GPU. Furthermore, we demonstrate the effectiveness of our strategy, which can promote the training efficiency several times that of the previous practice. By using 8 NVIDIA RTX2080Ti, datasets with 10 million of identities can be trained and 64 GPUs are able to train 100 millions identities. In order to verify the usefulness and robustness of our algorithm in academia, we clean and merge existing public face recognition dataset to obtain the largest publicly available face recognition training set Glint360K, which will be released. Experiment results from multiple datasets show that our method using only 10% classes to calculate softmax, can achieve on par accuracy with stateof-the-art works.</p><p>In summary, our contributions are mainly as follows: 1) We propose a softmax approximation algorithm, which can maintain the accuracy when using only 10% of the class centers. 2) We propose an efficient distributed training strategy that can easily train classification tasks with massive number of classes. 3) We clean, merge, and release the largest and cleanest face recognition dataset Glint360K. Baseline models trained on Glint360K with our proposed training strategy can easily achieve state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work Face Recognition</head><p>With the development of deep learning, deep neural networks has been playing an increasingly important role in the field of face recognition. The general pipeline is that the deep neural network extracts a feature for each input image. The learning process gradually narrows the gaps within the classes, and widens the gaps between the classes. At present, the most successful classifiers distinguish different identities by using the softmax classifier or its variants <ref type="bibr" target="#b19">(Wang et al. 2018b;</ref><ref type="bibr" target="#b1">Deng et al. 2019;</ref><ref type="bibr" target="#b10">Liu et al. 2017)</ref>. The field of face recognition now requires massive identities to train a model, e.g., 10 millions. For the methods based on softmax loss, the linear transformation matrix W will increase linearly followed the increase of the number of classes. When the number of identities is large to an extent, a single GPU cannot even carry such a weight matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acceleration for Softmax</head><p>For accelerating large-scale softmax in face recognition, there have been some approaches. HF-softmax (Goodman  <ref type="figure">Figure 2</ref>: (a) Increasing the number of classes and GPU, the total memory usage increases with the power of the number of GPU. (b) the W memory cost is constant, but logits will increase linearly, when in 16 servers with 8 GPUs case, the percentage of total GPU memory, is as high as 90%.</p><p>2001) dynamically selects a subset of active class centers for each mini-batch. The active class centers are selected by constructing a random hash forest in the embedding space and retrieving the approximate nearest class centers by features. However, all the class centers of this method are stored in RAM, the time cost for calculation in feature retrieval cannot be ignored. Softmax Dissection <ref type="bibr" target="#b4">(He et al. 2020</ref>) separates softmax loss into intra class objective and inter class objective and reduces the calculation of the redundancy of the inter class objective, but it can not be extended to other softmax-based losses. These methods are based on data parallel when using multi-GPU training. Even though only part of class centers are used to approximate softmax loss function, the inter-GPU communication is still costly, when applying gradients average for synchronizing SGD <ref type="bibr" target="#b9">(Li et al. 2014)</ref>. Besides, the number of selectable class centers are limited by the memory capacity of a single GPU. ArcFace <ref type="bibr" target="#b1">(Deng et al. 2019)</ref> proposes model parallel, which separates softmax weight matrix to different GPUs, then calculates full-class softmax loss with very little communication cost. They successfully trains 1 million identities with eight GPUs on a single machine. However, this method still has memory limitations. When the number of identities keeps increasing, the GPU memory consumption will finally exceed its capacity limit, although the number of GPUs increases in the same proportion. We will analyze the GPU memory usage of model parallel in detail in subsequent section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>In this section, we first detail the existing model parallel, analyzing its inter-device communication overhead, storage cost and memory limitations. Then we introduce our performance-lossless approximation method, and explain how this method works. Finally, we propose our distribution approximation method with the implementation details. Model parallel It is painful to train models with massive identities without using model parallel, subject to the memory capacity of a single graphics card. The bottleneck exists in storing the matrix of softmax weight W ∈ R d×C , where d denotes embedding feature dimension and C denotes the number of classes. A natural and straightforward approach to break the bottleneck is to partition W into k sub-matrices w of size d × C k and places the i-th sub-matrices on the i-th GPU. Consequently, to calculate the final softmax outputs, each GPU has to gather features from all other GPUs, as the weights are split up among different GPUs. The definition of softmax function is</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Problem Formulation</head><formula xml:id="formula_0">σ(X, i) = e w T i X C j=1 e w T j X .</formula><p>(1)</p><p>The calculation of the numerator can be done independently by each GPU as input feature X and corresponding weight sub-matrix w i are stored locally.</p><p>To calculate the denominator of the softmax function, sum of all e w T j X to be specific, information from all other GPUs have to be collected. Naturally, we can first calculate the local sum of each GPU, and then compute the global sum through communication. Compared with the naive data parallel, this implementation has negligible communication cost. The difference lies in the data to be communicated changed. Data parallel has to transmit the gradients of whole W to get all weights updated, whereas model parallel only communicates the local sum, whose cost can be ignored. To be specific, the size of communication overhead is equal to batch size multiplied by 4 bytes (Float32). We use collective communication primitives and matrix operations to describe the calculation process of the model parallel on the i-th GPU including forward as well as backward propagation, as shown in Algorithm 1. This method can greatly reduce inter-worker communication. Because the sizes of W , x i , and ∇X are d * C, N * d and N * d * k respectively, and on large-scale classification tasks, we typically assume Algorithm 1 The Model Parallel on the i-th GPU Input:</p><p>x i : features, located on i-th GPU; w i : i-th part matrix of W , located on i-th GPU; onehot i : onehot of x i , located on i-th GPU; Output:</p><p>∇x i : the gradient of x i ; ∇w i : the gradient of w i ; 1: / * collect features across all GPUs * / 2: X = allgather(x i ) 3: logits i = X * w i 4: / * calculate the local softmax denominator * / 5: den i = sum(e logitsi ) 6: / * get the global denominator across all GPUs * / 7: den = allreduce(den i ) 8: prob i = e logitsi /den 9: ∇logits i = prob i − onehot i 10: ∇w i = X T * ∇logits i 11: / * sync the gradients of features across all GPUs * / 12:</p><formula xml:id="formula_1">∇X = allreduce(∇logits i * w T i ) 13: ∇x i = get submatrix(i, ∇X) 14: return ∇x i , ∇w i ; C ≫ N * (k + 1), where N represents the mini-batch size on each GPU.</formula><p>Memory Limits Of Model Parallel The model parallel can completely solve the storage and communication problems of w, since no matter how big C is, we can easily add more GPUs. So that each GPU's memory size storing submatrix w remains unchanged, i.e.,</p><formula xml:id="formula_2">M em w = d × C ↑ k ↑ × 4 bytes.<label>(2)</label></formula><p>However, w is not the only one stored on GPU memories. The storage of predicted logits suffers from the increase of total batch size. We denote the logits storage on each GPU as logits = Xw, and then the memory consumption storing logits on each GPU is therefore equal to</p><formula xml:id="formula_3">M em logits = N k × C k × 4 bytes<label>(3)</label></formula><p>where N is the mini-batch size on each GPU, and k is the number of GPUs. Assuming that the batch size of each GPU is constant, when C increases, in order to keep C k unchanged, we have to increase k at the same time. Hence, the GPU memory occupied by logits will continue to increase, because the batch size of features increase synchronously with k. Assuming that only the classification layer is considered, each parameter will occupy 12 bytes, as we use momentum SGD optimization algorithm during training. In case CosFace <ref type="bibr" target="#b19">(Wang et al. 2018b)</ref> or ArcFace <ref type="bibr" target="#b1">(Deng et al. 2019</ref>) is used, each element in logits occupies 8 bytes. Hence, the overall GPU memory occupied by classification layer is calculated as</p><formula xml:id="formula_4">M em F C = 3 × M em W + 2 × M em logits .<label>(4)</label></formula><p>As shown in <ref type="figure">Figure 2</ref>, suppose the mini-batch size on each GPU is 64 and the embedding feature dimension is 512, then</p><formula xml:id="formula_5">!"#$%#&amp; !"#$%$&amp;'()*+##()',%'-# ,'.+%$&amp;'()*+##()',%'-# !"# ! !"# " $ ! %! % &amp; $ ! # #+/!*'(0$%1(!"#$%$&amp;'()*+##()',%'-# '()*+, ! $ " # $ " '()*+, " 2!3+%'(#24#'%()*+##()',%'-# !"# ! !"# " $ ! -'()*+, ! $ " -'()*+, " -$ " # . / $ &amp; -'()*+, " % &amp; 0%! 1/ . 23345678591'()*+, % &amp; $ % &amp;$ : ;&lt;(= % &gt; '()*+, % ?334567859@7A9B '()%*#! :: 1'()*+, % &gt; ;&lt;(= % C (DBE(+ % '%()$%#&amp; !"#$%&amp;"'</formula><p>!""#!$%&amp;' <ref type="figure">Figure 4</ref>: The structure of distributed implementation of our method. k means the number of GPUs. Allgather: Gather data from all GPUs and distribute the combined data to all GPUs. Allreduce: Sum up the data and distribute the results to all GPUs. 1 million classification task requires 8 GPUs and training 10 millions classification task requires at least 80 GPUs. We find that logits will take up ten times as much memory cost as w, which makes storing logtis new bottleneck to model parallel. The result shows that training tasks with massive identities cannot be solved by simply adding GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Approximate Strategy</head><p>Roles of positive and negative classes The most widely used classification loss function, softmax loss, can be described as</p><formula xml:id="formula_6">L = − 1 N N i=1 log e w T y i xi+by i C j=1 e w T j xi+bj = − 1 N N i=1 log e fy i C j=1 e fj ,<label>(5)</label></formula><p>where x i ∈ R d denotes the deep feature of the i-th sample, belonging to the y i -th class. w j ∈ R d denotes the j-th column of the weight W ∈ R d×C . The batch size and the class number are N and C, respectively.</p><p>f j is usually denoted as activation of a fully-connected layer with weight vector w j and bias b j . We fix the bias bj = 0 for simplicity, and as a result f j is given by</p><formula xml:id="formula_7">f j = w T j x = w T j x cos θ j ,<label>(6)</label></formula><p>where θ j is the angle between the weight w j and the feature x i . Following <ref type="bibr" target="#b19">(Wang et al. 2018b;</ref><ref type="bibr" target="#b10">Liu et al. 2017;</ref><ref type="bibr" target="#b1">Deng et al. 2019;</ref><ref type="bibr" target="#b18">Wang et al. 2018a</ref>), we fix the individual weight w j by l 2 normalisation, we also fix the feature x i by l 2 normalisation and rescale it to s. The normalisation step on features and weights makes the predictions only depend on the angle between the feature and the weight.</p><p>Naturally, each column of the linear transformation matrix is viewed as a class center, and the j-th column of the matrix corresponds to the class center of class j. we denote w yi as positive class center of x i , and the others are negative class centers.</p><p>Through the analysis of the softmax equation, we arrive at the following assumption. If we want to select a subset of class centers to approximate the softmax, positive class centers must be selected, whereas negative class centers only need to be selected from a subset of all. By doing so, the performance of the model can be maintained.</p><p>We use two experiments to prove this hypothesis. In each experiment, only a certain percentage of the class centers will be sampled to calculate the approximated softmax loss in each iteration. The first experiment will primarily select all positive classes corresponding to input features in the current batch, and then randomly sample the negative class centers. We call this sampling strategy as Positive Plus Randomly Negative (PPRN) for short in the follow section. The second is just making random selection from all class centers. Sampling rate is set to 0.1 and 0.5 for both experiments. We define the average cosine distance between x i and w yi as CA pcc during the training process, i.e.,</p><formula xml:id="formula_8">CA pcc = 1 n n i=1 cos θ i , with cos θ i ∈ [0, 1].<label>(7)</label></formula><p>The results of the experiments are shown in <ref type="figure">Figure 3</ref>. In <ref type="figure">Figure 3</ref> (a), we can find that at sampling rate of 0.1, fully random sampling results in an inferior model performance compared to PPRN. Because the averaged cosine angle between positive centers and features is our optimization goal. When training without sampling positive centers, the gradients of x i only learn the direction to push the sample away from negative centers but lack of the intra-class clustering objective. Nonetheless, this performance degradation will gradually decline as the sampling rate increases, since the probability of positive class is sampled is also increasing. According to <ref type="figure">Figure 3</ref> (b), model trained with PPRN with sampling rate of 0.1, 0.5 and 1.0 respectively have similar performance. To explain this phenomenon, one key point is that the predicted probability without sampling P i and the predicted probability with PPRN sampling strategyP i are very similar under certain circumstances. i.e.  </p><formula xml:id="formula_9">P i = e fi C j=0 e fj ,<label>(8)</label></formula><p>where S denotes the set of sampled classes and r denotes the sampling rate.</p><p>As the sampling strategy PPRN continuously optimizes the positive class centers, the probability of the positive class P gt and the sum of sampled classes P j continues to increase. This makes the gap between the probability of any negative classesP i and P i smaller and smaller. That is to say in the later stage of the training process, the optimization direction and amplitude of the negative class centers has little relationship with the sampling rate. Therefore, this is also the reason the sampling rates of 0.1, 0.5 and 1.0 can achieve very similar results.</p><p>Distributed Approximation As mentioned in the previous section, only a subset of the class centers can achieve a comparable performance. In order to train a training set with a larger number of identities, we propose a distributed approximation. The process of sampling subset class centers is straightforward: 1) First select the positive class centers; 2) Randomly sample negative class centers. In the case of model parallel, in order to balance the calculation and storage of each GPU, the number of class centers sampled on each GPU should be equal, so the sampling process has changed as follows: 1. Obtain the positive class centers on this GPU W will be evenly divided into different GPUs according to the order, such as W = [w 1 , w 2 , ..., w k ], k is the number of GPUs. When we know the label y i of the sample x i , its positive class center is the y i -th column of the W linear matrix. Therefore, the positive class centers w p i on this current GPU can be easily obtained by the label y of the features in current batch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Calculate the number of negative class centers</head><p>According to the previous information, the number of class centers stored on this GPU is |w i |, the number of positive class centers is |w p i |, then the number of negative class centers that need to be randomly sampled on this GPU is s i = (|w i | − |w p i |) * r, where r is the sampling rate for PPRN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Randomly sample negative classes</head><p>By randomly sampling s i negative class centers in the difference set between w i and w p i , we get the negative class centers w n i = random(w i − w p i , s i ) Finally, we get all the class centers to participate in the softmax calculation,</p><formula xml:id="formula_11">W s = [W p , W n ], where W p = [w p 1 , ..., w p k ], W n = [w n 1 , ..., w n k ].</formula><p>In fact, this methon is an approximate method to obtain the load balance of each GPU.</p><formula xml:id="formula_12">W s =random(W − W p ) ≈[random(w1 − w p 1 , s1), ..., random(w k − w p k , s k )]<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment Datasets and Settings</head><p>Training Dataset Our training datasets include CASIA <ref type="bibr" target="#b11">(Liu et al. 2015)</ref> and MS1MV2 <ref type="bibr" target="#b1">(Deng et al. 2019</ref>). Furthermore, we clean Celeb-500k <ref type="bibr" target="#b0">(Cao, Li, and Zhang 2018</ref>) and MS1MV2 to merge into a new training set, which we call Glint360K. The released dataset contains 17 million images of 360K individuals, which is the largest and cleanest training set by far in academia.</p><p>Testing Dataset We explore efficient face verification datasets (e.g., LFW <ref type="bibr" target="#b5">(Huang et al. 2008)</ref>, CFP-FP <ref type="bibr" target="#b16">(Sengupta et al. 2016)</ref>, AgeDB-30 <ref type="bibr" target="#b13">(Moschoglou et al. 2017)</ref>) to check the improvement from different settings. Besides, we report the performance of our method on the large-pose and large-age datasets (e.g., CPLFW <ref type="bibr" target="#b23">(Zheng and Deng 2018)</ref> and CFLFW <ref type="bibr" target="#b23">(Zheng, Deng, and Hu 2017)</ref>). In addition, we extensively test the proposed method on large-scale image datasets (e.g., MegaFace <ref type="formula">(</ref> Training Settings We use ResNet50 and ResNet100 <ref type="bibr" target="#b1">(Deng et al. 2019;</ref><ref type="bibr" target="#b3">He et al. 2016)</ref>, as our backbone network, and use two margin-base loss functions (i.e., CosFace and ArcFace). We set the feature scale s to 64 and cosine margin m of CosFace at 0.4 and arccos margin m of ArcFace at 0.5. We use a mini-batch size of 512 across 8 NVIDIA RTX2080Ti. The learning rate starts from 0.1. For CASIA, the learning rate is divided by 10 at 20K, 28K iterations   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effectiveness and Robustness</head><p>Effects on positive class centers We compare the results of PPRN and random sampling of all class centers under different sampling rates, as shown in <ref type="figure" target="#fig_3">Figure 5</ref>. The experiment shows that the accuracy of all random sampling of all class centers will plumb drastically when the sampling rate is small, while our method will maintain.</p><p>Effects on small-scaled trainset As shown in <ref type="table">Table 1</ref>. On a small-scaled training set, PPRN with sampling rate of 10%, has almost no adverse effect on accuracy, which proves that our method is effective even on a small data set.</p><p>Robustness on the number of identities As shown in <ref type="table" target="#tab_2">Table 2</ref>, We use two large training sets MS1MV2 and Glint360K to verify the number of identities in the training set effects on our sampling method. For IJB-B and IJB-C, when using MS1MV2, the accuracy difference between 10% sampling and full softmax in IJB-B and IJB-C is 0.4% and 0.4%, when using Glint360K, 10% sampling has no difference in IJB-B, and has only 0.1% difference in IJB-C. For MegaFace, when using MS1MV2, the identification and ver-ification accuracy difference between 10% and full softmax are 0.24% and 0.09%, when using Glint360K, the performance of 10% sampling rate and full softmax are comparable, in verification evaluation, 10% even outperforms full softmax, surpasses full softmax by +0.12%. This conclusion shows that our method is also work in larger-scale training sets, and if the number of identites increases greater than or equal to 300K, the performance of 10% sampling is comparable to full softmax.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Benchmark Results</head><p>Results on IJB-B and IJB-C We follow the testing protocol in ArcFace, and employ the face detection scores and the feature norms to re-weight faces within templates. The experiments on these two datasets (MS1MV2 and Glint360K) are used to prove that PPRN with sampling rate of 10% for softmax calculation has little lost on performance. As shown in <ref type="table" target="#tab_2">Table 2</ref>, when we apply PPRN with sampling rate of 10% on our large-scale training data (Glint360K), further improve the TAR (@FAR=1e-4) to 0.961 and 0.972 on IJB-B and IJB-C respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on MegaFace</head><p>We adopt the refined version of MegaFace <ref type="bibr" target="#b1">(Deng et al. 2019)</ref> to give a fair evaluation, we use MS1MV2 and Glint360K under the large protocol. As shown in   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training 100 millions identities</head><p>We set different number of identities and GPUs to test the training speed of our method and model parallel. In all experiments, we remove the influence of IO. We compare four settings, as shown in <ref type="table" target="#tab_5">Table 4</ref>: 1. 1 Million identities on 8 GPUs 8 GPU is more than enough to store one million class centers, we store W on the GPU. Because of the reduction in calculations brought by logits, our speed is 30% faster than model parallel.</p><p>2. 10 Million identities on 8 GPUs When the number of idintities is as large as 10 millions, the model parallel method can not work. We can still continue training, the training speed of our method is 900 images per second. 3. 10 Million identities on 64 GPUs When using model parallel, 64 GPUs will bring a large global batch size, which will increase the GPU memory of logits. 2048 is the largest batch size for model parallel. Compared with model parallel, the memory consumption of our method on each GPU is reduced from 9.6G to 6.7G, and training speed is 12600 images per second, which is 3 times faster than model parallel. 4. 100 Million identities on 64 GPUs With 64 GPUs, training 10 million identities is already at the limit of model parallel, but ours can easily expand to 20 million, 30 million, or even 100 million identities. when training 20 million, 30 million and 100 million identities, the training speed of our method are 10790, 8600 and 2000 images per second. We are the first to propose how to training 100 millions classes.  Compare with other sampling-based methods.</p><p>We compare our method with some current samplingbased methods. The methods are HF-Softmax proposed in ) and D-Softmax proposed in <ref type="bibr" target="#b4">(He et al. 2020</ref>). We adopt same dataset which is merged by MS1MV2 <ref type="bibr" target="#b1">(Deng et al. 2019</ref>) and MegaFace2 <ref type="bibr" target="#b14">(Nech and Kemelmacher-Shlizerman 2017)</ref> and same network as the work done by <ref type="bibr" target="#b4">He et al. 2020)</ref> for fair comparison. By using the same 1/64 sampling rate, our method outperforms all the methods in accuracy and speed, as shown in <ref type="table" target="#tab_6">Table 5</ref> and <ref type="table" target="#tab_8">Table 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we first systematically analyse the pros and cons of model parallel. Following this, for the issue that model parallel cannot train models with massive number of classes, we introduce PPRN sampling strategy. On one hand, by training only a subset of all classes in each iteration, the training speed can be very fast. More importantly, this training on partial classes method makes GPU memory no longer bottleneck in model parallel, which means we can make the e of massive identities from impossible to possible. Next we make broad experiment on verifying the effectiveness and robustness of PPRN across different models, loss functions, training sets and test sets. Last but not least, we release by far the largest and cleanest face recognition dataset Glint360K to accelerate the development in the field. When training on Glint360K, we achieve state-of-the-art performance with only 10% of classes used for training.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>S ae L &gt;S ã á S á ? (a) By using hash forest, the complexity of finding active classes is reduced from O(N) to O(logN). (b) Positive class is selected and negative classes are selected randomly, the complexity of sampling is O(1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) The curves CA pcc values. Solid lines means obtaining the whole positive class centers and sampling partial negative class centers at rate 0.5 and 0.1 (PPRN). Dotted lines means randomly sampling all class centers at rate 0.1 and 0.5. (b) The curves CA pcc values when sampling partial negative class centers at rate 0.1, 0.5 and 1.0 respectively (PPRN).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :,</head><label>5</label><figDesc>Verification results of PPRN (ours) and random sampling of all class centers on different val datasets. We employ ResNet50 as the backbone and CosFace loss on trainning dataset CASIA. (a) Verification result on LFW. (b) Verification result on AgeDB-30. |S| = C * r,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Kemelmacher-Shlizerman et al. 2016), IJB-B (Whitelam et al. 2017), IJB-C (Maze et al. 2018)) and InsightFace Recognition Test (IFRT) 1 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell cols="5">: The 1:1 verification accuracy on the LFW, AgeDB-30, CALFW, CPLFW, CFP-FP datasets. TAR@FAR=1e-4 is re-</cell></row><row><cell cols="5">ported on the IJB-B and IJB-C datasets. Identification and verification evaluation on MegaFace Challenge1 using FaceScrub as</cell></row><row><cell cols="5">the probe set. "Id" refers to the rank-1 face identification accuracy with 1M distractors, and "Ver" refers to the face verification</cell></row><row><cell cols="4">TAR@FPR=1e-6. r means the sampling rate.</cell></row><row><cell>Dataset</cell><cell cols="3">African Caucasian Indian Asian</cell><cell>All</cell></row><row><cell>CASIA-R100</cell><cell>39.67</cell><cell>53.93</cell><cell cols="2">47.81 16.17 37.53</cell></row><row><cell>VGG2-R50</cell><cell>49.20</cell><cell>65.93</cell><cell cols="2">56.22 27.15 47.13</cell></row><row><cell>MS1MV2-R50</cell><cell>71.97</cell><cell>83.24</cell><cell cols="2">79.66 22.94 56.20</cell></row><row><cell>MS1MV3-R134</cell><cell>81.08</cell><cell>89.06</cell><cell cols="2">87.53 38.40 74.76</cell></row><row><cell>Glint360K-R100(r=1.0)</cell><cell>89.50</cell><cell>94.23</cell><cell cols="2">93.54 65.07 88.67</cell></row><row><cell>Glint360K-R100(r=0.1)</cell><cell>90.45</cell><cell>94.60</cell><cell cols="2">93.96 63.91 88.23</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: The 1:1 verification accuracy on InsightFace</cell></row><row><cell>Recognition Test (IFRT), TAR@FAR=1e-6 is measured on</cell></row><row><cell>all-to-all 1:1 protocal. r means the sampling rate.</cell></row><row><cell>and the training process is finished at 32K iterations. For</cell></row><row><cell>MS1MV2, we divide the learning rate at 100K, 160K itera-</cell></row><row><cell>tions and finish at 180K iterations. For Glint360K, the learn-</cell></row><row><cell>ing rate is divided by 10 at 200k, 400k, 500k, 550k iterations</cell></row><row><cell>and finish at 600K iterations.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>Finally, Our method using our large-scale Glint360K dataset with PPRN with sampling rate of 10% achieves state-of-the-art verification accuracy of 99.13% on the MegaFace dataset.</figDesc><table><row><cell>Results on IFRT IFRT is a globalised fair benchmark</cell></row><row><cell>for face recognition algorithms, this test dataset contains</cell></row><row><cell>242143 identities and 1624305 images. IFRT evaluates the</cell></row><row><cell>algorithm performance on worldwide web pictures which</cell></row><row><cell>contains various sex, age and race groups. In Table 3, we</cell></row><row><cell>compare the performance of our method train on Glint360k.</cell></row><row><cell>The proposed Glint360k dataset obviously boosts the per-</cell></row><row><cell>formance compared to MS1MV3. Furthermore, the perfor-</cell></row><row><cell>mance of PPRN with sampling rate of 10% and full softmax</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Large-scale classification training comparison. The less memory occupied and the larger throughput, the better. OOM means the GPU memory overflows and the model cannot be trained. When model parallel is applied, storing weight matrices in RAM is useless, because all class centers still need to be loaded back into GPUs when calculating softmax loss.</figDesc><table><row><cell>Method</cell><cell>Ids</cell><cell>r</cell><cell>LFW</cell><cell cols="2">CFP AgeDB</cell></row><row><cell cols="5">HF-Softmax 1.3K 1/64 99.18 86.11</cell><cell>91.55</cell></row><row><cell cols="5">D-Softmax-K 1.3K 1/64 99.55 89.77</cell><cell>95.02</cell></row><row><cell>Ours</cell><cell cols="4">1.3K 1/64 99.60 95.52</cell><cell>95.63</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Comparison of our method and other existing sampling-based methods in terms of face verification accuracy on LFW, CFP and AgeDB. r means the sampling rate.</figDesc><table /><note>are still comparable.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Comparison of our method and other existing sampling-based methods in terms of training speed, the Total average time is computed as the average time for one forward-backward pass through the entire model.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/deepinsight/insightface/tree/master/IFRT</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Celeb-500K: A Large Training Dataset for Face Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2406" to="2410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">ArcFace: Additive Angular Margin Loss for Deep Face Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4690" to="4699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Classes for fast maximum entropy training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings (Cat. No.01CH37221)</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="561" to="564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Softmax Dissection: Towards Understanding Intra-and Inter-class Objective for Embedding Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10957" to="10964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Labeled Faces in the Wild: A Database forStudying Face Recognition in Unconstrained Environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mattar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Faces in &apos;Real-Life&apos; Images: Detection, Alignment, and Recognition</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">CurricularFace: Adaptive Curriculum Learning Loss for Deep Face Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5901" to="5910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The MegaFace Benchmark: 1 Million Faces for Recognition at Scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4873" to="4882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Group-Face: Learning Latent Groups and Constructing Group-Based Representations for Face Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-C</forename><surname>Roh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5621" to="5630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Scaling distributed machine learning with the parameter server</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Josifovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Shekita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-Y</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI&apos;14 Proceedings of the 11th USENIX conference on Operating Systems Design and Implementation</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="583" to="598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">SphereFace: Deep Hypersphere Embedding for Face Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6738" to="6746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep Learning Face Attributes in the Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3730" to="3738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">IARPA Janus Benchmark -C: Face Dataset and Protocol</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Maze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Duncan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Otto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Niggel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Grother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Biometrics (ICB)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="158" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">AgeDB: The First Manually Collected, In-the-Wild Age Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moschoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Papaioannou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1997" to="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Level Playing Field for Million Scale Face Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3406" to="3415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">FaceNet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Frontal to profile face verification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Circle Loss: A Unified Perspective of Pair Similarity Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6398" to="6407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Additive Margin Softmax for Face Verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="926" to="930" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">CosFace: Large Margin Cosine Loss for Deep Face Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5265" to="5274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<title level="m">FACE AUTHENTICATION METHOD AND DEVICE</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">IARPA Janus Benchmark-B Face Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Whitelam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Taborsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Maze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Duncan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Grother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="592" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Accelerated Training for Massive Classification via Dynamic Class Selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7566" to="7573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Cross-pose LFW: A database for studying cross-pose face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.08197</idno>
	</analytic>
	<monogr>
		<title level="m">Cross-Age LFW: A Database for Studying Cross-Age Face Recognition in Unconstrained Environments</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
		<respStmt>
			<orgName>Beijing University of Posts and Telecommunications</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
