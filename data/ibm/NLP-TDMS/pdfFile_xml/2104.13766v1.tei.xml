<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Boosting Co-teaching with Compression Regularization for Label Noise</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyi</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">ESAT-STADIUS</orgName>
								<address>
									<settlement>Leuven</settlement>
									<region>KU</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Shen</surname></persName>
							<email>xi.shen@enpc.fr</email>
							<affiliation key="aff1">
								<orgName type="laboratory">LIGM (UMR 8049)</orgName>
								<address>
									<addrLine>Ecole des Ponts</addrLine>
									<settlement>UPE</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shell</forename><forename type="middle">Xu</forename><surname>Hu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Upload AI LLC</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><forename type="middle">A K</forename><surname>Suykens</surname></persName>
							<email>johan.suykens@esat.kuleuven.be</email>
							<affiliation key="aff0">
								<orgName type="department">ESAT-STADIUS</orgName>
								<address>
									<settlement>Leuven</settlement>
									<region>KU</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Boosting Co-teaching with Compression Regularization for Label Noise</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we study the problem of learning image classification models in the presence of label noise. We revisit a simple compression regularization named Nested Dropout <ref type="bibr" target="#b21">[22]</ref>. We find that Nested Dropout <ref type="bibr" target="#b21">[22]</ref>, though originally proposed to perform fast information retrieval and adaptive data compression, can properly regularize a neural network to combat label noise. Moreover, owing to its simplicity, it can be easily combined with Coteaching [5] to further boost the performance.</p><p>Our final model remains simple yet effective: it achieves comparable or even better performance than the state-ofthe-art approaches on two real-world datasets with label noise which are Clothing1M [28] and ANIMAL-10N [24]. On Clothing1M [28], our approach obtains 74.9% accuracy which is slightly better than that of DivideMix <ref type="bibr" target="#b11">[12]</ref>. On ANIMAL-10N [24], we achieve 84.1% accuracy while the best public result by PLC [30] is 83.4%. We hope that our simple approach can be served as a strong baseline for learning with label noise. Our implementation is available at https://github.com/yingyichen-cyy/Nested-Co-teaching.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The availability of large-scale datasets with clean annotations has made indispensable contributions to the prosperity of deep learning. However, collecting these extensive high-quality data has always been a major challenge since the procedure is both expensive and time-consuming. Appealed by the inexpensive and convenient accesses to large but defective data, such as querying commercial search engines <ref type="bibr" target="#b13">[14]</ref>, downloading images from social media <ref type="bibr" target="#b15">[16]</ref> and other web crawling strategies <ref type="bibr" target="#b18">[19]</ref>, efforts have been made in literature to learn with imperfect data, among which learning with noisy labels has been attached great importance.</p><p>The problem of learning with noisy labels dates back to <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b20">21]</ref> and the mainstream solutions include adding regularization <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b17">18]</ref>, estimating the label transition matrix <ref type="bibr" target="#b19">[20]</ref>, training on selected or reweighted samples <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b28">29]</ref>, label correction <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b29">30]</ref> and other strategies categorized into the semi-supervised learning genre <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b11">12]</ref>. In addition to works that provide concise and theoretically sound frameworks to combat label noise, there are other existing works devoted to achieving the state-of-the-art performances on benchmark datasets which require an appropriate tuning on multiple hyper-parameters <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b29">30]</ref>.</p><p>With the above in mind, we propose a simple method that combines the regularization and training on selected samples paradigms together to improve state-of-the-art performance on two real-world datasets. To be specific, we revisit a compression regularization called Nested Dropout <ref type="bibr" target="#b21">[22]</ref>, which was originally proposed to learn ordered feature representations, such that it can be used to perform fast information retrieval and adaptive data compression. It has also been shown in the paper that the ordered feature representation has a strong connection to the PCA solution, that is, the feature channels can be associated to the eigenvectors of the covariance of input data. We find this property important in combating label noise since we are then able to conduct a signal-to-noise separation on the learned feature channels. We verify this intuition empirically, which shows that Nested Dropout gives rise to a strong baseline for learning with noisy labels.</p><p>To further take full advantage of this compression regularization, we combine it with a widely acknowledged method called Co-teaching <ref type="bibr" target="#b4">[5]</ref>. This is another strong baseline for learning with noisy labels. The basic idea is that two networks can be trained simultaneously where each network updates itself based on the small-loss mini-batch samples selected by its peer. The success of Co-teaching requires the two networks to be reliable enough to select clean samples for each other where we assume the smaller the loss, the cleaner the data <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29]</ref>. In this regard, we propose a two-stage solution:</p><p>• In stage one, two Nested Dropout networks are trained separately to provide reliable base networks for the subsequent stage;</p><p>• In stage two, the two trained networks are further finetuned with Co-teaching.</p><p>As such, we are able to boost the classical strategy with a simple compression regularization. The rest of the paper is organized as follows: Section 2 gives the architecture of the proposed method. Section 3 presents the experiments on an illustrative toy dataset and two real-world datasets, namely, Clothing1M <ref type="bibr" target="#b27">[28]</ref> and ANIMAL-10N <ref type="bibr" target="#b23">[24]</ref>. Empirical results demonstrate the effectiveness of our two-stage method given its superior performance comparing to several state-of-the-art approaches such as DivideMix <ref type="bibr" target="#b11">[12]</ref> and PLC <ref type="bibr" target="#b29">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Method</head><p>In this section, we present our approach. We find that the compression regularization is a simple yet effective technique to combat label noise. Specifically, we first focus on one compression regularization named Nested Dropout <ref type="bibr" target="#b21">[22]</ref> in Section 2.1, which may serve as a stronger substitute of Dropout <ref type="bibr" target="#b24">[25]</ref>. To take full advantage of Nested Dropout, we further combine it with one commonly accepted method called Co-teaching <ref type="bibr" target="#b4">[5]</ref> in Section 2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Nested Dropout</head><p>Nested Dropout <ref type="bibr" target="#b21">[22]</ref> is one regularization able to learn ordered representations where different dimensions have different degrees of importance. While applied, meaningless representations can be dropped, leading to a compressed network <ref type="bibr" target="#b3">[4]</ref>. Taking above into consideration, these ordered representations could be particularly adapted to learning with noisy labels since representations learned from noisy data are expected to be meaningless.</p><p>Let h ∈ R K×H×W be the hidden feature representation obtained by the feature network f , i.e. h = f (x) with x being the input. To obtain an ordered feature representation, in each training iteration, we only keep the first k dimensional feature of h and mask the rest to zeros, that is,</p><formula xml:id="formula_0">z = h 1:k , 0, . . . , 0 ∈ R K×···</formula><p>where k is sampled from a categorized distribution with parameters</p><formula xml:id="formula_1">p k ∝ exp − k 2 2 σ 2 nest , ∀k = 1, . . . , K .<label>(1)</label></formula><p>where σ nest is one major hyper-parameter in our method. In this manner, smaller k's are preferred when σ nest is small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Co-teaching</head><p>Co-teaching <ref type="bibr" target="#b4">[5]</ref> is a standard baseline for learning with label noise. The idea is to train two deep networks f 1 and f 2 simultaneously where each network selects its (1 -λ forget ) percent small-loss instances, i.e. D 1 and D 2 , respectively, where λ forget is the forget rate and it is an important hyperparameter in the Co-teaching architecture. Networks update themselves based on the sample set selected by their peers.</p><p>Considering that small-loss instances are more likely to be clean <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29]</ref>, we could obtain classifiers resistant to noisy labels by training them on these instances. However, the above comes with one premise that the classifier should be reliable enough so that the small-loss instances are indeed clean. In the original Co-teaching <ref type="bibr" target="#b4">[5]</ref>, it proposes to keep all the instances in the mini-batch at the beginning, and then gradually decrease the number of instances in D 1 and D 2 until the N -th epoch, after which the number of samples used to train the models becomes fixed. Compared to tuning the hyper-parameter N , we find it more stable to conduct standard training for each model until convergence then fine-tune both models with Co-teaching.</p><p>To combine with Nested Dropout, the training procedure comes with two stages: (i) train two Nested Dropout networks separately; (ii) fine-tune these two networks with Coteaching.</p><p>In the first stage, we set a learning rate warm-up to cope with the difficulty of training with Nested Dropout in early epochs resulting from the high probability of dropping most of the channels in the feature layer (i.e. large σ nest ). In the second stage, Nested Dropout is maintained during the training of each model except for selecting small-loss instances D 1 and D 2 . The final performance is the accuracy of the ensembled model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head><p>In this section, we present our experimental results. We first show how Nested Dropout deals with regression noise in a toy example in Section 3.1. In Section 3.2, we compare our method with state-of-the-art approaches on two realworld datasets: Clothing1M <ref type="bibr" target="#b27">[28]</ref> and ANIMAL-10N <ref type="bibr" target="#b23">[24]</ref>. Finally, an ablation study on ANIMAL-10N <ref type="bibr" target="#b23">[24]</ref> is given in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Toy example: a simple regression with noise</head><p>To gain an intuitive understanding on the reason why Nested Dropout <ref type="bibr" target="#b21">[22]</ref> is able to resist label noise, we present a simulated regression experiment. We generate a dataset of noisy observations from y i = x i + i for i = 1, . . . , 64 where x i is the evenly spaced value between [0, 10] and i ∼ N (0, 1) are independently sampled. We adopt a multilayer perceptron (MLP) composed of three linear layers with input and output dimensions being 1 → 64 → 128 → 1. Each layer is followed by a ReLU activation except the last one. For the model with Nested Dropout <ref type="bibr" target="#b21">[22]</ref>, we apply Nested Dropout <ref type="bibr" target="#b21">[22]</ref> to the last layer of this MLP and denote it by MLP+Nested. We set σ nest = 200 and use the (1) to sample the number of features for training. Results after 100k epochs are shown in <ref type="figure" target="#fig_1">Figure 1</ref>. It can be seen that MLP overfits to the label noise while MLP+Nested with the first k ∈ {1, 10} channels recovers the ground-truth y = x better. However, with the number of channels increasing,    <ref type="bibr" target="#b27">[28]</ref>. We report average accuracy as well as the standard deviation for three runs. Results with "*" are either using a balanced subset or a balanced loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Acc. (%) CE <ref type="bibr" target="#b26">[27]</ref> 67.2 F-correction <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b19">20]</ref> 68.9 Decoupling <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b16">17]</ref> 68.5 Co-teaching <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b4">5]</ref> 69.2 Co-teaching+ <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b28">29]</ref> 59.3 JoCoR <ref type="bibr" target="#b26">[27]</ref> 70.3 JO <ref type="bibr" target="#b25">[26]</ref> 72.2 Dropout* <ref type="bibr" target="#b24">[25]</ref> 72.8 PENCIL* <ref type="bibr" target="#b10">[11]</ref> 73.5 MLNT <ref type="bibr" target="#b12">[13]</ref> 73.5 PLC* <ref type="bibr" target="#b29">[30]</ref> 74.0 DivideMix* <ref type="bibr" target="#b11">[12]</ref> 74.8 Ours Nested* 73.1 ± 0.3 Nested + Co-teaching* 74.9 ± 0.2</p><p>MLP+Nested gradually overfits to the label noise due to over parameterization. This demonstrates that the first few channels in MLP+Nested contain the main data structure information, while channels towards the end are more likely to encode misled information that would overfit to the noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Comparison with state-of-the-art methods on real datasets</head><p>Datasets We conduct experiments on two real datasets: Clothing1M <ref type="bibr" target="#b27">[28]</ref> and ANIMAL-10N <ref type="bibr" target="#b23">[24]</ref>. The Cloth-ing1M <ref type="bibr" target="#b27">[28]</ref> dataset contains 1 million clothing images obtained from online shopping websites with 14 categories. The labels in this dataset are quite noisy with an unknown underlying structure. This dataset also provides 50k, 14k and 10k manually verified clean data for training, validation and testing, respectively. Note that the clean training set is not used during the training. Following <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b29">30]</ref>, in our experiment, we randomly sample a balanced subset which <ref type="table">Table 2</ref>: Test set accuracy (%) on ANIMAL-10N <ref type="bibr" target="#b23">[24]</ref>. We report average accuracy as well as the standard deviation for three runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Acc. (%) CE <ref type="bibr" target="#b23">[24]</ref> 79.4 ± 0.1 Dropout <ref type="bibr" target="#b24">[25]</ref> 81.3 ± 0.3 SELFIE <ref type="bibr" target="#b23">[24]</ref> 81.8 ± 0.1 PLC <ref type="bibr" target="#b29">[30]</ref> 83.4 ± 0.4 Ours Nested 81.3 ± 0.6 Nested + Co-teaching 84.1 ± 0.1 includes 260k images (18.5k images per category) from the noisy training set and use it as our training set and report the classification accuracy on the 10k clean test data. We adopt the standard data augmentation procedures to train ImageNet <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6]</ref>, including random horizontal flip, and resizing the image with a short edge of 256 and then randomly cropping a 224 × 224 patch from the resized image. The ANIMAL-10N is recently proposed by <ref type="bibr" target="#b23">[24]</ref>. It contains 10 animals with confusing appearance. The estimated label noise rate is 8%. There are 50k training and 5k testing images. We did not apply any data augmentation so that the setting is the same with <ref type="bibr" target="#b23">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation details</head><p>We implement our approach on Pytorch. Experiments on Clothing1M <ref type="bibr" target="#b27">[28]</ref> are with ResNet-18 <ref type="bibr" target="#b5">[6]</ref> pre-trained on ImageNet <ref type="bibr" target="#b1">[2]</ref> following <ref type="bibr" target="#b26">[27]</ref>. Note that Nested Dropout or Dropout is applied right before the linear classifier in the network. Models in stage one are optimised with SGD optimizer with a momentum of 0.9, a weight decay of 5e−4, an initial learning rate of 2e−2, and batch size of 448. During training, we run learning rate warm-up for 6000 iterations, then train the model for 30 epochs with the learning rate decayed by 0.1 after the 5th epoch. In stage two, we apply Co-teaching to fine-tune two well-trained models. SGD optimizer is utilized with the same settings only with an initial learning rate changing to 2e−3. Moreover, we set forget rate λ forget in the Co- <ref type="table">Table 3</ref>: Average test accuracy (%) with standard deviation (three runs) of different σ nest on ANIMAL-10N <ref type="bibr" target="#b23">[24]</ref>. The corresponding optimal number of channels k * for each model is also provided (entry "k * "). We report test accuracy of single model (entry "Acc.") as well as the accuracy with the combination of Co-teaching (entry "Co-teaching Acc.")  <ref type="bibr" target="#b22">[23]</ref> with batch normalization <ref type="bibr" target="#b6">[7]</ref> as in <ref type="bibr" target="#b23">[24]</ref>. The two Dropout layers in the original VGG-19 architecture are changed to Nested Dropouts when Nested is applied. The SGD optimizer is employed. Following <ref type="bibr" target="#b23">[24]</ref>, we train the network for 100 epochs and use an initial learning rate of 0.1, which is divided by 5 at 50% and 75% of the total number of epochs. In stage one, models are trained with learning rate warm-up for 6000 iterations. In stage two, no warm-up is applied, batch norms are freezed, forget rate λ forget is 0.2, initial learning rate is 4e−3 and decayed by 0.2 after the 5th epoch with 30 epochs in total.</p><formula xml:id="formula_2">σ nest k * Acc. (%) k * Co-</formula><p>Results on the Clothing1M <ref type="bibr" target="#b27">[28]</ref> We now compare our method to state-of-the-art approaches on Clothing1M <ref type="bibr" target="#b27">[28]</ref> in <ref type="table" target="#tab_0">Table 1</ref>. It is worth noting that <ref type="table" target="#tab_0">Table 1</ref> also includes very recent approaches such as DivideMix <ref type="bibr" target="#b11">[12]</ref> and PLC <ref type="bibr" target="#b29">[30]</ref>. Surprisingly, our single model with Nested Dropout <ref type="bibr" target="#b21">[22]</ref> not only surpasses the standard Dropout <ref type="bibr" target="#b24">[25]</ref>, but also achieves comparable performance to PENCIL <ref type="bibr" target="#b10">[11]</ref> and MLNT <ref type="bibr" target="#b10">[11]</ref>. Incorporating Co-teaching <ref type="bibr" target="#b4">[5]</ref> further boosts the performance to 74.9% and outperforms the state-of-theart DivideMix <ref type="bibr" target="#b11">[12]</ref>.</p><p>Results on the ANIMAL-10N <ref type="bibr" target="#b27">[28]</ref> Experimental results on ANIMAL-10N <ref type="bibr" target="#b27">[28]</ref> are given in <ref type="table">Table 2</ref>. The dataset is recently proposed, and we compare with two approaches that report performance on this dataset: SELFIE <ref type="bibr" target="#b23">[24]</ref> and PLC <ref type="bibr" target="#b29">[30]</ref>. It can be seen that our single model achieves comparable performance to Dropout as well as SELFIE <ref type="bibr" target="#b23">[24]</ref> and Co-teaching provides a consistent performance boost, which is similar to the results on the Clothing1M <ref type="bibr" target="#b27">[28]</ref>. Note that, our best performance by using Nested Dropout <ref type="bibr" target="#b21">[22]</ref> and Co-teaching <ref type="bibr" target="#b4">[5]</ref> achieves 84.1% accuracy outperforms recent approach PLC <ref type="bibr" target="#b29">[30]</ref> by 0.7%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Ablation study</head><p>In this section, we provide the ablation study of σ nest on ANIMAL-10N <ref type="bibr" target="#b23">[24]</ref>. The results are given in <ref type="table">Table 3</ref>. As we can see, the Nested Dropout <ref type="bibr" target="#b21">[22]</ref> provides consistent improvement compared to training with standard cross entropy loss (entry "CE") and the performance gain is also robust to the choices of the hyper-parameter σ nest . Moreover, fine-tuning with Co-teaching <ref type="bibr" target="#b4">[5]</ref> provides clear boost for all the models. We also show the optimal number of channels of each model (entry "k * ") in the table. Note that though two layers of Nested Dropout have been applied to the classifier of VGG-19, the optimal number of channels k * is recorded with regard to the last Nested Dropout layer for simplicity. Interestingly, the models trained with Nested Dropout <ref type="bibr" target="#b21">[22]</ref> achieve better performance but with only using less than 1% of channels compare to the models trained with standard cross entropy (entry "CE").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>In this paper, we investigated the problem of image classification in the presence of noisy labels. Specifically, we first demonstrated that a simple compression regularization called Nested Dropout <ref type="bibr" target="#b21">[22]</ref> can be used to combat label noise. Moreover, due to its simplicity, Nested Dropout <ref type="bibr" target="#b21">[22]</ref> can be easy combined with Co-teaching <ref type="bibr" target="#b4">[5]</ref> to further boost the performance. We validated our approach on two realworld noisy datasets and achieved state-of-the-art performance on both datasets. The proposed approach is simple comparing to many existing methods. Therefore, we hope that our approach can be served as a strong baseline for future research on learning with noisy label.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Comparisons of regression between MLP and MLP incorporated with Nested Dropout [22] on a synthetic noisy label dataset. (a) Training the regression with standard MLP; (b-d) Learning regression with MLP+Nested and plot the prediction results using only the first k channels where (b) k = 1, (c) k = 10, (d) k = 100.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Test set accuracy (%) on Clothing1M</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The research leading to these results has received funding from the European Research Council under the European Union's Horizon 2020 research and innovation program / ERC Advanced Grant E-DUALITY (787960). This paper reflects only the authors' views and the Union is not liable for any use that may be made of the contained information. This work was also supported in part by EU H2020 ICT-48 Network TAILOR (Foundations of Trustworthy AI -Integrating Reasoning, Learning and Optimization), Leuven.AI Institute.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning from noisy examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dana</forename><surname>Angluin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Laird</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deliang Fan, and Boqing Gong. A semi-supervised two-stage approach to learning from noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Aidan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><forename type="middle">Rao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Divyam</forename><surname>Kamalakara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Madaan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
		<title level="m">Learning sparse networks using targeted dropout. arXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Coteaching: Robust training of deep neural networks with extremely noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingrui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Recycling: Semi-supervised learning with noisy labels in deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyeongbo</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junggi</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngchul</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsung</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Woo-Jin</forename><surname>Seong Gyun Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Song</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>IEEE Access</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Selfpaced learning for latent variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>M Pawan Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koller</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Probabilistic End-to-end Noise Correction for Learning with Noisy Labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Kun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Jianxin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dividemix: Learning with noisy labels as semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning to learn from noisy labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongkang</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eirikur</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<title level="m">Webvision database: Visual learning and understanding from web data. arXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dimensionality-driven learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">E</forename><surname>Houle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Erfani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shutao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudanthi</forename><surname>Wijewickrema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Ashwin Bharambe, and Laurens Van Der Maaten. Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Decoupling&quot; when to update&quot; from&quot; how to update</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>Malach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nagarajan</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Inderjit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambuj</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tewari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Web crawling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Olston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Najork</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Now Publishers Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Making deep neural networks robust to label noise: A loss correction approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Patrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Rozza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhen</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Induction of decision trees. Machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Quinlan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning ordered representations with nested dropout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Rippel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gelbart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Selfie: Refurbishing unclean samples for robust deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwanjun</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minseok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae-Gil</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Joint optimization framework for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daiki</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daiki</forename><surname>Ikami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshihiko</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyoharu</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Combating noisy labels by agreement: A joint training method with co-regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>An</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning from massive noisy labeled data for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">How does disagreement help generalization against label corruption</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingrui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangchao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning with feature-dependent label noise: A progressive approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songzhu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayank</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
