<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unified Vision-Language Pre-Training for Image Captioning and VQA</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
							<email>luozhou@umich.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Palangi</surname></persName>
							<email>hpalangi@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
							<email>leizhang@microsoft.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Cloud &amp; AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Microsoft AI &amp; Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
							<email>jjcorso@umich.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
							<email>jfgao@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Unified Vision-Language Pre-Training for Image Captioning and VQA</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a unified Vision-Language Pre-training (VLP) model. The model is unified in that (1) it can be finetuned for either vision-language generation (e.g., image captioning) or understanding (e.g., visual question answering) tasks, and (2) it uses a shared multi-layer transformer network for both encoding and decoding, which differs from many existing methods where the encoder and decoder are implemented using separate models. The unified VLP model is pre-trained on a large amount of image-text pairs using the unsupervised learning objectives of two tasks: bidirectional and sequence-to-sequence (seq2seq) masked vision-language prediction. The two tasks differ solely in what context the prediction conditions on. This is controlled by utilizing specific self-attention masks for the shared transformer network. To the best of our knowledge, VLP is the first reported model that achieves state-of-the-art results on both vision-language generation and understanding tasks, as disparate as image captioning and visual question answering, across three challenging benchmark datasets: COCO Captions, Flickr30k Captions, and VQA 2.0. The code and the pre-trained models are available at https://github.com/LuoweiZhou/VLP.</p><p>A girl with an upside-down umbrella.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>…</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unified Encoder-Decoder</head><p>Cows on the high mountain pasture.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Inspired by the recent success of pre-trained language models such as <ref type="bibr">BERT (Devlin et al. 2018)</ref> and <ref type="bibr">GPT (Radford et al. 2018;</ref><ref type="bibr" target="#b16">Radford et al. 2019)</ref>, there is a growing interest in extending these models to learning cross-modal representations like image-text <ref type="bibr" target="#b21">Tan and Bansal 2019)</ref> and <ref type="bibr">video-text (Sun et al. 2019b;</ref><ref type="bibr" target="#b20">Sun et al. 2019a</ref>), for various vision-language tasks such as Visual Question Answering (VQA) and video captioning, where traditionally tedious task-specific feature designs and fine-tuning are required. <ref type="table">Table 1</ref> summarizes some of the recent works on visionlanguage pre-training where all the models are unexceptionally built upon Bidirectional Encoder Representations from Transformers (BERT) <ref type="bibr" target="#b5">(Devlin et al. 2018)</ref>. These models use a two-stage training scheme. The first stage, called pretraining, learns the contextualized vision-language representations by predicting the masked words or image regions based on their intra-modality or cross-modality relationships   <ref type="figure">Figure 1</ref>: We propose a unified encoder-decoder model for general vision-language pre-training. The pre-trained model is then fine-tuned for image captioning and visual question answering. Thanks to our vision-language pre-training, both training speed and overall accuracy have been significantly improved on the downstream tasks compared to random initialization or language-only pre-training. All the results are evaluated on the validation set of the corresponding dataset.</p><p>on large amounts of image-text pairs. Then, in the second stage, the pre-trained model is fine-tuned to adapt to a downstream task. <ref type="bibr">Although</ref>  Understanding-based only LXMERT <ref type="bibr" target="#b21">(Tan and Bansal 2019)</ref>, ViLBERT , UNITER , VisualBERT ), B2T2 <ref type="bibr" target="#b1">(Alberti et al. 2019</ref>), Unicoder-VL ),VL-BERT <ref type="bibr" target="#b20">(Su et al. 2019</ref>  <ref type="table">Table 1</ref>: Comparison between our method and other vision-language pre-training works. on individual downstream tasks using different pre-trained models, it remains challenging to pre-train a single, unified model that is universally applicable, via fine-tuning, to a wide range of vision-language tasks as disparate as vision-language generation (e.g., image captioning) and understanding (e.g., VQA). Most existing pre-trained models are either developed only for understanding tasks, as denoted by "understanding-based only" in Tab. 1, or designed as hybrid models that consist of multiple modality-specific encoders and decoders which have to be trained separately in order to support generation tasks. For example, VideoBERT and CBT in Tab. 1 perform pre-training only for the encoder, not for the decoder. This causes a discrepancy between the cross-modal representations learned by the encoder and the representation needed by the decoder for generation, which could hurt the generality of the model. In this paper, we strive to develop a new method of pre-training a unified representation for both encoding and decoding, eliminating the aforementioned discrepancy. In addition, we expect that such a unified representation would also allow more effective cross-task knowledge sharing, reducing the development cost by eliminating the need of pre-training different models for different types of tasks.</p><p>To this end, we propose a unified encoder-decoder model, called the Vision-Language Pre-training (VLP) model, which can be fine-tuned for both vision-language generation and understanding tasks. The VLP model uses a shared multi-layer Transformer network <ref type="bibr" target="#b21">(Vaswani et al. 2017)</ref> for encoding and decoding, pre-trained on large amounts of image-caption pairs <ref type="bibr" target="#b19">(Sharma et al. 2018)</ref>, and optimized for two unsupervised vision-language prediction tasks: bidirectional and sequence to sequence (seq2seq) masked language prediction. The two tasks differ solely in what context the prediction conditions on. This is controlled by utilizing specific self-attention masks for the shared Transformer network. In the bidirectional prediction task, the context of the masked caption word to be predicted consists of all the image regions and all the words on its right and left in the caption. In the seq2seq task, the context consists of all the image regions and the words on the left of the to-be-predicted word in the caption.</p><p>The proposed VLP has two main advantages in comparison with the BERT-based models in Tab. 1. First, VLP unifies the encoder and decoder and learns a more universal contextualized vision-language representation that can be more easily fine-tuned for vision-language generation and understanding tasks, as disparate as image captioning and VQA. Second, the unified pre-training procedure leads to a single model architecture for two distinct vision-language prediction tasks, i.e., bidirectional and seq2seq, alleviating the need for multiple pre-training models for different types of tasks without any significant performance loss in taskspecific metrics.</p><p>We validate VLP in our experiments on both the image captioning and VQA tasks using three challenging benchmarks: COCO Captions <ref type="bibr" target="#b4">(Chen et al. 2015)</ref>, Flickr30k Captions <ref type="bibr" target="#b25">(Young et al. 2014)</ref>, and VQA 2.0 dataset <ref type="bibr" target="#b8">(Goyal et al. 2017)</ref>. We observe that compared to the two cases where we do not use any pre-trained model or use only the pre-trained language model (i.e., BERT), using VLP significantly speedups the task-specific fine-tuning and leads to better taskspecific models, as shown in <ref type="figure">Fig. 1</ref>. More importantly, without any bells and whistles, our models achieve state-of-theart results on both tasks across all three datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Language Pre-training. Among numerous BERT variants in language pre-training, we review the two methods that are most relevant to our approach, namely Unified LM or UniLM <ref type="bibr" target="#b6">(Dong et al. 2019</ref>) and Multi-Task DNN (MT-DNN) <ref type="bibr" target="#b14">(Liu et al. 2019a</ref>). UniLM employs a shared Transformer network which is pre-trained on three language modeling objectives: unidirectional, bidirectional, and sequenceto-sequence. Each objective specifies different binary values in the self-attention mask to control what context is available to the language model. MT-DNN combines multi-task training and pre-training by attaching task-specific projection heads to the BERT network. Our work is inspired by these works and tailored for vision-language tasks in particular. Vision-Language Pre-training. This has become a nascent research area in the vision-language community. Related works include ViLBERT ) and LXMERT <ref type="bibr" target="#b21">(Tan and Bansal 2019)</ref>, both of which tackle understanding-based tasks only (e.g., VQA and Retrieval) and share the same two-stream BERT framework with a vision-language coattention module to fuse the information from both modal-ities. ViLBERT is tested on a variety of downstream tasks including VQA, referring expression, and image-to-text retrieval. LXMERT only focuses on a particular problem space (i.e., VQA and visual reasoning) and the generalization ability further compromises when the datasets from the downstream tasks are also exploited in the pre-training stage. The most similar work to ours is <ref type="bibr">VideoBERT (Sun et al. 2019b</ref>), which addresses generation-based tasks (e.g., video captioning) and understanding-based tasks (e.g., action classification). However, it separates the visual encoder and the language decoder and performs pre-training only on the encoder, leaving decoder uninitialized. In contrast, we propose a unified model for both encoding and decoding and fully leverage the benefit of pre-training. Image Captioning &amp; VQA. Most of the recent works on image captioning are built upon <ref type="bibr" target="#b2">(Anderson et al. 2018)</ref>, where a language model gets clues for sentence generation through dynamically attending on object regions in the image extracted from pre-trained object detectors. Follow-up works further capture the relationships among object regions by using Graph Convolutional Networks (GCNs) <ref type="bibr" target="#b24">(Yao et al. 2018)</ref>, incorporating language inductive bias (Yang et al. 2019), or enforcing region grounding between image and text . VQA is another prevalent research area in vision and language. Since its initial proposal <ref type="bibr" target="#b3">(Antol et al. 2015)</ref>, there has been a significant amount of works proposing model architectures to fuse question and image representations <ref type="bibr" target="#b11">(Kim, Jun, and Zhang 2018;</ref><ref type="bibr" target="#b2">Anderson et al. 2018;</ref>, new datasets or models to reduce the dataset bias <ref type="bibr" target="#b26">(Zhang et al. 2016;</ref><ref type="bibr" target="#b8">Goyal et al. 2017;</ref><ref type="bibr" target="#b0">Agrawal et al. 2017</ref>) and ground the answer in the question <ref type="bibr" target="#b13">(Lewis and Fan 2019)</ref>. We use our base architecture to perform both image captioning and VQA with minor model structure differences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vision-Language Pre-training</head><p>We denote the input image as I and the associated/target sentence description (words) as S. We extract a fixed number N of object regions from the image using an off-the-shelf object detector, denoted as {r 1 , . . . , r N } and the corresponding region features as R = [R 1 , . . . , R N ] ∈ R d×N , region object labels (probabilities) as C = [C 1 , . . . , C N ] ∈ R l×N , and region geometric information as G = [G 1 , . . . , G N ] ∈ R o×N , where d is the embedding size, l indicates the number of the object classes of the object detector, and o = 5 consists of four values for top left and bottom right corner coordinates of the region bounding box (normalized between 0 and 1) and one value for its relative area (i.e., ratio of the bounding box area to the image area, also between 0 and 1). The words in S are represented as one-hot vectors which are further encoded to word embeddings with embedding size e: y t ∈ R e where t ∈ {1, 2, . . . , T } and T indicates the length of the sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vision-Language Transformer Network</head><p>Our vision-language Transformer network, which unifies the Transformer encoder and decoder into a single model, is depicted in <ref type="figure" target="#fig_1">Fig. 2 (left)</ref>. The model input consists of the class-aware region embedding, word embedding and three special tokens. The region embedding is defined as:</p><formula xml:id="formula_0">r i = W r R i + W p [LayerNorm(W c C i )|LayerNorm(W g G i )]</formula><p>(1) where [·|·] indicates the concatenation on the feature dimension, LayerNorm represents Layer Normalization. The second term mimics the positional embedding in BERT, but adding extra region class information, and W r , W p , W c , W g are the embedding weights (the bias term and the nonlinearity term are omitted). Note that here we overload the notation of r i ∈ R d (i ∈ {1, 2, ..., N }) to also represent classaware region embeddings. In addition, we add segment embeddings to r i as in BERT where all the regions share the same segment embedding where the values depend on the objectives (i.e., seq2seq and bidirectional, see the following section).</p><p>The word embeddings are similarly defined as in (Devlin et al. 2018), adding up y t with positional embeddings and segment embeddings, which is again overloaded as y t . We define three special tokens </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-training Objectives</head><p>In the BERT masked language modeling objective, 15% of the input text tokens are first replaced with either a special [MASK] token, a random token or the original token, at random with chances equal to 80%, 10%, and 10%, respectively. Then, at the model output, the hidden state from the last Transformer block is projected to word likelihoods where the masked tokens are predicted in the form of a classification problem. Through this reconstruction, the model learns the dependencies in the context and forms a language model. We follow the same scheme and consider two specific objectives: the bidirectional objective (bidirectional) as in BERT and the sequence to sequence objective (seq2seq), inspired by <ref type="bibr" target="#b6">(Dong et al. 2019)</ref>.</p><p>As shown in <ref type="figure" target="#fig_1">Fig. 2 (right)</ref>, the only difference between the two objectives lie in the self-attention mask. The mask used for the bidirectional objective allows unrestricted message passing between the visual modality and the language modality while in seq2seq, the to-bepredicted word cannot attend to the words in the future, i.e., it satisfies the auto-regressive property. More formally, we define the input to the first Transformer block as H 0 = [r <ref type="bibr">[CLS]</ref> , r 1 , . . . , r N , y <ref type="bibr">[SEP]</ref> , y 1 , . . . , y T , y <ref type="bibr">[STOP]</ref> ] ∈ R d×U where U = N +T +3, and then the encoding at different levels of Transformer as H l = Transformer(H l−1 ), l ∈ [1, L]. We further define a self-attention mask as M ∈ R U ×U ) , where M jk = 0, allow to attend −∞, prevent from attending j, k = 1, . . . , U.</p><p>(2) For simplicity, we assume a single attention head in the selfattention module. Then, the self-attention output on H l−1 <ref type="bibr">[CLS]</ref> r <ref type="bibr">[CLS]</ref> hr <ref type="bibr">[CLS]</ref>   <ref type="bibr">[SEP]</ref> hr <ref type="bibr">[SEP]</ref> [SEP] y 1 hy 1 Tok 1 y <ref type="bibr">[MASK]</ref> hy <ref type="bibr">[MASK]</ref> [MASK] y 3 hy 3 Tok 3 RoI 2 y <ref type="bibr">[MASK]</ref> hy <ref type="bibr">[MASK]</ref> [MASK] Image <ref type="bibr">[STOP]</ref> y <ref type="bibr">[STOP]</ref> hy <ref type="bibr">[STOP]</ref> Self-attention mask ). The image is processed as N Region of Interests (RoIs) and region features are extracted according to Eq. 1. The sentence is tokenized and masked with [MASK] tokens for the later masked language modeling task. Our Unified Encoder-Decoder consists of 12 layers of Transformer blocks, each having a masked self-attention layer and feed-forward module, where the self-attention mask controls what input context the prediction conditions on. We implemented two selfattention masks depending on whether the objective is bidirectional or seq2seq. Better viewed in color.</p><p>can be formulated as:</p><formula xml:id="formula_1">A l =softmax( Q K √ d + M )V ,<label>(3)</label></formula><formula xml:id="formula_2">V =W l V H l−1 , Q = W l Q H l−1 , K = W l K H l−1 ,<label>(4)</label></formula><p>where W l V , W l Q , and W l K are the embedding weights (the bias terms are omitted). The intermediate variables V , Q, and K indicate values, queries and keys, respectively, as in the self-attention module <ref type="bibr" target="#b21">(Vaswani et al. 2017)</ref>. A l is further encoded by a feed-forward layer with a residual connection to form the output H l . During the pre-training, we alternate per-batch between the two objectives and the proportions of seq2seq and bidirectional are determined by hyperparameters λ and 1 − λ, respectively.</p><p>It is worth noting that in our experiments we find that incorporating the region class probabilities (C i ) into region feature (r i ) leads to better performance than having a masked region classification pretext as in <ref type="bibr" target="#b21">Tan and Bansal 2019)</ref>. Therefore, differing from existing works where masked region prediction tasks are used to refine the visual representation, we indirectly refine the visual representation by utilizing it for masked language reconstruction. We also choose not to use the Next Sentence Prediction task as in BERT, or in our context predicting the correspondence between image and text, because the task is not only weaker than seq2seq or bidirectional but also computationally expensive. This coincidentally agrees with a concurrent work of <ref type="bibr">RoBERTa (Liu et al. 2019b)</ref>. Sequence-to-sequence inference. Similar to the way seq2seq training is performed, we can directly apply VLP to sequence-to-sequence inference, in the form of beam search.</p><p>More details follow next in the Image Captioning section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fine-Tuning for Downstream Tasks Image Captioning</head><p>We fine-tune the pre-trained VLP model on the target dataset using the seq2seq objective. During inference, we first encode the image regions along with the special [CLS] and [SEP] tokens and then start the generation by feeding in a [MASK] token and sampling a word from the word likelihood output (e.g., greedy sampling). Then, the [MASK] token in the previous input sequence is replaced by the sampled word and a new [MASK] token is appended to the input sequence to trigger the next prediction. The generation terminates when the [STOP] token is chosen. Other inference approaches like beam search could apply as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visual Question Answering</head><p>We frame VQA as a multi-label classification problem. In this work we focus on open domain VQA where top k most frequent answers are selected as answer vocabulary and used as class labels. Following (Anderson et al. 2018) we set k to 3129.</p><p>During the fine-tuning, a multi-layer Perceptron (Lin-ear+ReLU+Linear+Sigmoid) on top of the element-wise product of the last hidden states of [CLS] and [SEP] is learned, similar to . We optimize the model output scores with respect to the soft answer labels using cross-entropy loss. Note that unlike <ref type="bibr" target="#b21">(Tan and Bansal 2019)</ref> where the task-specific objective (i.e., VQA) is exploited during pre-training by using the target datasets (from inten-     We further perform CIDEr optimization on COCO Captions through Self-Critical Sequence Training (SCST) <ref type="bibr" target="#b18">(Rennie et al. 2017)</ref>, as in most of the recent image captioning literatures. The results are in Tab. 3 where our full model sets new SotA on all the metrics. Boost from pre-training. Our full model leads our baseline model by a large margin on most of the metrics thanks to our pre-training. Some noticeable improvements include over 10% absolute gain on CIDEr metric on Flickr30k, and over 2% gain on CIDEr on COCO and B@4, METEOR on Flickr30k. Small datasets (i.e., Flickr30k) benefit the most as vision-language pre-training alleviates overfitting issues. Our model variants under the two extreme settings work well as expected on their "favorable" tasks, i.e., seq2seq pretraining alone improves downstream captioning tasks significantly and bidirectional pre-training benefits understanding tasks (i.e., VQA), but not the opposite. They set new SotAs on all metrics except the "Number" accuracy on VQA 2.0. The joint training organically combines the representations learned from the two rather different objectives and yields slightly compromised but decent accuracy on all the downstream tasks. That said, from an engineering perspective, if we can afford having separate pre-training models for generation task or understanding task, we will get the optimal model performance. If we value model architecture and parameter sharing, the joint model is a good trade-off. Impact of pre-training types. Depending on how the base Method B@4 M C S Region label as pretext 5.4 9.4 62.2 14.5 Region label probability as input 5.8 9.7 67.0 15.5 <ref type="table">Table 6</ref>: Comparison between having region class prediction pretext and feeding in class probabilities as a part of the model input. Results are on Conceptual Captions val set.</p><p>model Transformer is initialized, we define four "degrees" of pre-training from weakest to strongest as i) without any pre-training, i.e., base model is trained from scratch, ii) bidirectional language pre-training, i.e., base model is initialized from BERT weights (Devlin et al. 2018), iii) seq2seq and bidirectional language pre-training, i.e., base model is initialized from UniLM weights <ref type="bibr" target="#b6">(Dong et al. 2019</ref>) which is our baseline setting, and iv) our full Vision-Language Pretraining. The corresponding fine-tuning results on downstream tasks are presented in <ref type="figure">Fig. 1</ref> on the val set (full results see Appendix) and Tab. 4 on the test set. As shown from the figure, our vision-language pre-training significantly accelerates the learning process of downstream tasks and contributes to better overall accuracy. It is worth noting that the learning process of VQA is greatly shortened despite that the hidden states associated with tokens [CLS] and <ref type="bibr">[SEP]</ref> are not learned during the pre-training. This indicates that the contextualized vision-language representations can generalize to unseen domains and work reasonable well as a warm-start for new tasks. We also study how the pre-training types 1-3 influence our vision-language pre-training in terms of caption generation. The results on Conceptual Captions val set at epoch 20 are shown in Tab. 5. All the models are trained based on the unified VLP objective (λ = 0.75) for a fair comparison. We observe that initializing base model with weights transferred from pure language pre-training benefits visionlanguage pre-training. The training objectives of UniLM are closer to our seq2seq and bidirectional objectives than the ones in BERT and hence we hypothesize that this counts for the slightly larger improvement. Note that our intention here is to demonstrate how different weight initializations can influence pre-training performance rather than pursuing possibly high quantitative scores (with full seq2seq training, CIDEr could climb to 77.2 after training for 30 epochs). Region object labels as pretext. Existing works ) regard region object labels (probabilities) (C i ) as an important auxiliary to enrich image region </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GT sentences:</head><p>A man standing by a large air gondola that is docked in a station A train is parked as a man at the top of the stairs waits along side it. Small tram bus parked between two stair cases A man standing next to cable car and a flight of stairs A man getting ready to board the trolley car  features and here we follow a similar design. We can also instead use these labels for a masked region classification pretext as in <ref type="bibr" target="#b21">(Tan and Bansal 2019)</ref>. Here we have a comparison over the two design choices. "region label probability as input" is equivalent to our full model Unified VLP and "region label as pretext" is the implementation from <ref type="bibr" target="#b21">(Tan and Bansal 2019)</ref>. As shown in the results, predicting class labels as a pretext has a negative impact on the pre-training, in terms of captioning performance. We hypothesize that this is because the class labels from the off-the-shelf object detector might be noisy which compromises the learned feature representation. In contrast, our model refines the visual representation through a more reliable masked language modeling and could correct the errors exist in the class labels. Qualitative results and analyses. Qualitative examples on COCO Captions and VQA 2.0 are shown in <ref type="figure" target="#fig_4">Fig. 3</ref>. In the first two examples, our full model with vision-language pretraining captures more details in the image, such as "umbrellas" and "a blue wall" than the baseline methods. It also answers questions correctly. In the third example, all the methods dis-identify the gondola as a train due to their visual similarity. When it comes to the question answering, our methods all give correct answers while the GT answer is incorrect (note that there is a person in the gondola). In the fourth example, all the models mistakenly classify the activity as surfing while the correct one is kayaking/boating. This is consistent across both the caption model and the VQA model, which implies that the feature representations are indeed shared across tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>This paper presents a unified Vision-Language Pre-training (VLP) model that can be fine-tuned for both vision-language generation and understanding tasks. The model is pretrained on large amounts of image-text pairs based on two objectives: bidirectional and seq2seq vision-language prediction. The two disparate objectives are fulfilled under the same architecture with parameter sharing, avoiding the necessity of having separate pre-trained models for different types of downstream tasks (i.e., generation-based or understanding-based). In our comprehensive experiments on image captioning and VQA tasks, we demonstrate that the large-scale unsupervised pre-training can significantly speed up the learning on downstream tasks and improve model accuracy. Besides, compared to having separate pre-trained models, our unified model combines the representations   learned from different objectives and yields slightly compromised but decent (SotA) accuracy on all the downstream tasks. In our future work, we would like to apply VLP to more downstream tasks, such as text-image grounding and visual dialogue. Methodology-wise, we would want to see how multi-task fine-tuning can be applied to our framework to alleviate interference between different objectives.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Model architecture for pre-training. The input comprises of image input, sentence input, and three special tokens ([CLS], [SEP], [STOP]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>People in matching shirts standing under umbrellas in the sun People in the same colorful shirts have umbrellas. A large group of people with an umbrella outside. A group of men standing next to a lot of umbrellas A group of people that are under one umbrella Unified VLP (159.8): A group of people standing under umbrellas in the rain. Init from UniLM (59.0): A group of people standing around each other. Init from BERT (59.0): A group of people standing around each other.Question: Are they dressed the same? in front of a blue wall A man talks on a phone in a room with blue wallpaper A man holding a cell phone standing in front of blue wallpaper with designs and a large wall vent A man on a cell phone by a bright blue wall A man holding a phone to his ear Unified VLP (180.6): A man talking on a cell phone in front of a blue wall.Init from UniLM (126.9): A man talking on a cell phone while standing next to a blue wall.Init from BERT (59.6): A man talking on a cell phone while wearing a gray shirt.Question: Is the man taking his own picture?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Unified VLP (28.0): A red train is parked in a station.Init from UniLM (36.9): A red train with a man standing on the top of it.Init from BERT (21.3): A red train car sitting inside of a train station.Question: How many people are here? Two boaters are white water rafting through rough currents. Two people in a small boat in a body of water There are people on a boat tube in the water Two people riding a raft through some waves Two people in a canoe in some rapidsUnified VLP (7.5): A man riding a surfboard on top of a wave. Init from UniLM (7.6): A man and a boy are riding a surfboard on a wave. Init from BERT (5.4): A man riding a paddle board on top of a wave. Question: What is the person doing? Correct answer: kayaking/boating Unified VLP: surfing Init from UniLM: surfing Init from BERT: surfing</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Qualitative examples on COCO Captions and VQA 2.0. The first column indicates images from the COCO validation set. The second column shows the five human-annotated ground-truth (GT) captions. The third column indicates captions generated by three of our methods and the corresponding CIDEr scores, where only Unified VLP has vision-language pretraining. The last column shows VQA questions and correct answers associated with the image and answers generated by our models. The top two are successful cases and the bottom two are failed cases. See text for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>significant improvements have been reported arXiv:1909.11059v3 [cs.CV] 4 Dec 2019</figDesc><table><row><cell>Type</cell><cell>Method</cell><cell>Domain</cell><cell>Architecture</cell><cell>Downstream Tasks</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>[CLS],[SEP],[STOP], where [CLS] indicates the start of the visual input, [SEP] marks the boundary between the visual input and the sentence input, and [STOP] determines the end of the sentence. The [MASK] tokens indicate the masked words which will be explained in the next section.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell cols="4">COCO (w/ CIDEr optimization)</cell></row><row><cell>Method</cell><cell>B@4</cell><cell>M</cell><cell>C</cell><cell>S</cell></row><row><cell>BUTD</cell><cell cols="3">36.3 27.7 120.1</cell><cell>21.4</cell></row><row><cell>GCN-LSTM (spa)</cell><cell cols="3">38.2 28.5 127.6</cell><cell>22.0</cell></row><row><cell>SGAE (Yang et al. 2019)</cell><cell cols="3">38.4 28.4 127.8</cell><cell>22.1</cell></row><row><cell>AoANet*</cell><cell cols="3">38.9 29.2 129.8</cell><cell>22.4</cell></row><row><cell>Ours (Unified VLP)</cell><cell cols="3">39.5 29.3 129.3</cell><cell>23.2</cell></row></table><note>Results on COCO Captions test set (with cross-entropy optimization only, all single models), VQA 2.0 Test-Standard set and Flickr30k test set. * indicates unpublished works. B@4 represents for BLEU@4, M for METEOR, C for CIDEr, and S for SPICE. Results on previous works are obtained from the original papers. Top two results on each metric are in bold.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 3 :</head><label>3</label><figDesc>Results on COCO Captions test set (with CIDEr optimization, all single models). * indicates unpublished works. Top one result on each metric is in bold.sive human annotations), our pre-training does not have this requirement and is therefore more general.</figDesc><table><row><cell></cell><cell>take the model</cell></row><row><cell></cell><cell>output from fc6 layer as the region feature (R i ) and the class</cell></row><row><cell></cell><cell>likelihood on the 1600 object categories as region object la-</cell></row><row><cell></cell><cell>bels (C i ). Note that if not specified, the weights in our BERT</cell></row><row><cell></cell><cell>model are initialized from UniLM (Dong et al. 2019) pre-</cell></row><row><cell></cell><cell>trained on text corpora only. For caption inference, we use</cell></row><row><cell></cell><cell>greedy search on the validation set and beam search with</cell></row><row><cell></cell><cell>beam size 5 on the test set. We perform light model hyper-</cell></row><row><cell></cell><cell>parameter search with the configurations presented in Ap-</cell></row><row><cell></cell><cell>pendix. λ is set to 0.75 for CC pre-training from light model</cell></row><row><cell></cell><cell>validation (out of {0.25, 0.5, 0.75}), and set to 1 for image</cell></row><row><cell>Experiments and Results</cell><cell>captioning (i.e., full seq2seq) and 0 for VQA (i.e., full bidi-</cell></row><row><cell>Data preparation. We conduct pre-training on the Con-ceptual Captions (CC) dataset (Sharma et al. 2018) which has around 3 million web-accessible images with associ-ated captions. The datasets for downstream tasks include COCO Captions (Chen et al. 2015), VQA 2.0 (Goyal et al. 2017) and Flickr30k (Young et al. 2014). For COCO Captions and Flickr30k, we follow Karpathy's split 1 , which gives 113.2k/5k/5k and 29.8k/1k/1k images for train/val/test splits respectively. For VQA 2.0, we split the dataset with the official partition, i.e., 443.8k questions from 82.8k images for training, 214.4k questions from 40.5k images for valida-tion and report the results on Test-Standard set through the official evaluation server. We trim long sentences and pad short sentences to 20 words and all the words are tokenized and numericalized as in BERT (Devlin et al. 2018).</cell><cell>rectional). Model variants and metrics. To demonstrate the effective-ness of our vision-language pre-training, we first include a baseline model without this pre-training. We then include two extreme settings of our model with λ = 1 (seq2seq pre-training only) and λ = 0 (bidirectional pre-training only) to study how each objective individually works with different downstream tasks. Our full model conducts joint training on the two objectives. The fine-tuning procedure is performed the same regardless of the pre-training configurations. Re-garding evaluation metrics, we use standard language met-rics for image captioning, including Bleu@4, METEOR, CIDEr, and SPICE and the official measurement on accu-racy for VQA, over different answer types including Yes/No, Number, and Other. Comparisons against SotAs. Results comparing our meth-</cell></row><row><cell></cell><cell>ods and SotA methods on the test set are in Tab. 2. We in-</cell></row></table><note>1 cs.stanford.edu/people/karpathy/deepimagesent/caption datasets.zip Implementation details. Our Transformer backbone is the same as BERT-base (Devlin et al. 2018). The input of the network consists of image (regions) and the asso- ciated/target caption. We represent each input image as 100 object regions extracted from a variant of Faster R- CNN (Ren et al. 2015) pre-trained on Visual Genome (Kr- ishna et al. 2017; Anderson et al. 2018). We</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>Impact of different levels of pre-training on downstream tasks. All results are on the test set (Test-Dev for VQA 2.0). Top one result on each metric is in bold.</figDesc><table><row><cell>Method</cell><cell>B@4</cell><cell>M</cell><cell>C</cell><cell>S</cell></row><row><cell>From scratch</cell><cell cols="4">5.5 9.4 63.8 14.9</cell></row><row><cell>Init from BERT</cell><cell cols="4">5.7 9.7 66.7 15.3</cell></row><row><cell>Init from UniLM</cell><cell cols="4">5.8 9.7 67.0 15.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>: Impact of model weight initializations on pre-</cell></row><row><cell>training. Results are on Conceptual Captions val set on cap-</cell></row><row><cell>tion generation.</cell></row><row><cell>clude state-of-the-art published works (upper part of Tab. 2),</cell></row><row><cell>unpublished works that are currently in submission (middle</cell></row><row><cell>part), and our methods (lower part). All the image captioning</cell></row><row><cell>methods are single models, with cross-entropy optimization</cell></row><row><cell>only for a fair comparison. Our full model (Unified VLP)</cell></row><row><cell>outperforms SotA methods on three out of four metrics on</cell></row><row><cell>COCO, overall accuracy on VQA 2.0, and all four metrics</cell></row><row><cell>on Flickr30k. The improvements are particularly sound on</cell></row><row><cell>Flickr30k, where we get 5.1% absolute gain on CIDEr met-</cell></row><row><cell>ric and 2.8% on BLEU@4.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 :</head><label>7</label><figDesc>Results on COCO Captions, VQA 2.0, and Flickr30k validation set. B@4 represents for BLEU@4, M for METEOR, C for CIDEr, and S for SPICE. Top two results on each metric are in bold.</figDesc><table><row><cell>Dataset</cell><cell cols="3">Batch Size Learning Rate # of Epochs</cell><cell cols="2">GPUs Time per Epoch</cell></row><row><cell>CC</cell><cell>64(x8)</cell><cell>1e-4(x8)</cell><cell>30</cell><cell>8x V100</cell><cell>5hr</cell></row><row><cell>COCO</cell><cell>64(x8)</cell><cell>3e-5(x8)</cell><cell>30</cell><cell>8x V100</cell><cell>12min</cell></row><row><cell>VQA 2.0</cell><cell>64(x2)</cell><cell>2e-5(x2)</cell><cell>20</cell><cell>2x V100</cell><cell>32min</cell></row><row><cell>Flickr30k</cell><cell>64(x8)</cell><cell>3e-5(x8)</cell><cell>30</cell><cell>8x V100</cell><cell>3min</cell></row><row><cell>COCO (w/o pre-training)</cell><cell>64(x8)</cell><cell>3e-4(x8)</cell><cell>30</cell><cell>8x V100</cell><cell>12min</cell></row><row><cell>COCO (SCST training)</cell><cell>16(x4)</cell><cell>1e-6(x4)</cell><cell cols="2">30 4x Titan Xp</cell><cell>3hr</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc>Model hyper-parameters and training specifications.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement. The technical work was performed during Luowei's summer internship at Microsoft Research. Luowei Zhou and Jason Corso were partly supported by DARPA FA8750-17-2-0125 and NSF IIS 1522904 as part of their affiliation with University of Michigan. This article solely reflects the opinions and conclusions of its authors but not the DARPA or NSF. We thank Li Dong and Furu Wei for generously sharing us their UniLM source code. We thank Kezhen Chen for his helpful discussions.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on Downstream Tasks</head><p>We include the validation results on fine-tuning tasks in Tab. 7. Note that for VQA 2.0, all the methods here are only trained on the training set while for the results reported on the test set (Tab. 3 and Tab. 4 in the main paper), all the models are trained on both training set and validation set following the practice from early works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>Region proposal and feature. We use a variant of Faster RCNN model <ref type="bibr" target="#b17">(Ren et al. 2015)</ref> with ResNeXt-101 FPN backbone <ref type="bibr" target="#b22">(Xie et al. 2017)</ref> for region proposal and feature extraction. The Faster RCNN model is pre-trained on the Visual Genome dataset <ref type="bibr" target="#b12">(Krishna et al. 2017</ref>), following the same procedure in (Anderson et al. 2018) for joint object detection (1600 classes) and attribute classification. We set the number of regions per image to exact 100 as suggested in <ref type="bibr" target="#b10">(Jiang et al. 2018)</ref>. We take the output of the fc6 layer as the feature representation for each region, and fine-tune the fc7 layer. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Don&apos;t just assume; look and answer: Overcoming priors for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kembhavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4971" to="4980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Alberti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.05054</idno>
		<title level="m">Fusion of detected objects in text for visual question answering</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bottom-up and topdown attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anderson</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6077" to="6086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">VQA: Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Antol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00325</idno>
		<title level="m">Microsoft coco captions: Data collection and evaluation server</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11740</idno>
		<idno>arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Uniter: Learning universal image-text representations</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Unified language model pre-training for natural language understanding and generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.03197</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dynamic fusion with intra-and intermodality attention flow for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6639" to="6648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Making the v in vqa matter: Elevating the role of image understanding in visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6904" to="6913" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.06954</idno>
		<title level="m">Attention on attention for image captioning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.09956</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Pythia v0. 1: the winning entry to the vqa challenge</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bilinear attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1564" to="1574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Krishna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generative question answering: Learning to answer the whole question</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Lewis and Fan 2019</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unicoder-vl: A universal encoder for vision and language by cross-modal pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.06066</idno>
		<idno>arXiv:1907.11692</idno>
	</analytic>
	<monogr>
		<title level="m">Multi-task deep neural networks for natural language understanding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Visualbert: A simple and performant baseline for vision and language. Liu et al. 2019b. Roberta: A robustly optimized bert pretraining approach</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Neural baby talk</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7219" to="7228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.02265</idno>
		<ptr target="https://s3-us-west-2.amazonaws.com/openai-assets/researchcovers/languageunsupervised/languageunder-standingpaper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Language models are unsupervised multitask learners</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Self-critical sequence training for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Rennie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7008" to="7024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Sharma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2556" to="2565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Vl-bert: Pre-training of generic visual-linguistic representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08530</idno>
		<idno>arXiv:1906.05743</idno>
	</analytic>
	<monogr>
		<title level="m">Contrastive bidirectional transformer for temporal representation learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Sun et al. 2019a]. Sun et al. 2019b] Sun. Videobert: A joint model for video and language representation learning</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.07490</idno>
	</analytic>
	<monogr>
		<title level="m">Lxmert: Learning cross-modality encoder representations from transformers</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Advances in neural information processing systems</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5987" to="5995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Auto-encoding scene graphs for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10685" to="10694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Exploring visual relationship for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="684" to="699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions</title>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="67" to="78" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Yin and Yang: Balancing and answering binary visual questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">End-to-end dense video captioning with masked transformer</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8739" to="8748" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Grounded video description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
