<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Tensorizing Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2015-12-20">20 Dec 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Novikov</surname></persName>
							<email>novikov@bayesgroup.rupodoprikhin.dmitry@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Skolkovo Institute of Science and Technology</orgName>
								<address>
									<settlement>Moscow</settlement>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Institute of Numerical Mathematics</orgName>
								<orgName type="institution">Russian Academy of Sciences</orgName>
								<address>
									<settlement>Moscow</settlement>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Podoprikhin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Skolkovo Institute of Science and Technology</orgName>
								<address>
									<settlement>Moscow</settlement>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Osokin</surname></persName>
							<email>anton.osokin@inria.frvetrovd@yandex.ru</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">INRIA</orgName>
								<orgName type="institution" key="instit2">SIERRA project-team</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Vetrov</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Skolkovo Institute of Science and Technology</orgName>
								<address>
									<settlement>Moscow</settlement>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">National Research University Higher School of Economics</orgName>
								<address>
									<settlement>Moscow</settlement>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Tensorizing Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2015-12-20">20 Dec 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep neural networks currently demonstrate state-of-the-art performance in several domains. At the same time, models of this class are very demanding in terms of computational resources. In particular, a large amount of memory is required by commonly used fully-connected layers, making it hard to use the models on low-end devices and stopping the further increase of the model size. In this paper we convert the dense weight matrices of the fully-connected layers to the Tensor Train <ref type="bibr" target="#b16">[17]</ref> format such that the number of parameters is reduced by a huge factor and at the same time the expressive power of the layer is preserved. In particular, for the Very Deep VGG networks <ref type="bibr" target="#b20">[21]</ref> we report the compression factor of the dense weight matrix of a fully-connected layer up to 200000 times leading to the compression factor of the whole network up to 7 times.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep neural networks currently demonstrate state-of-the-art performance in many domains of largescale machine learning, such as computer vision, speech recognition, text processing, etc. These advances have become possible because of algorithmic advances, large amounts of available data, and modern hardware. For example, convolutional neural networks (CNNs) <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b20">21]</ref> show by a large margin superior performance on the task of image classification. These models have thousands of nodes and millions of learnable parameters and are trained using millions of images <ref type="bibr" target="#b18">[19]</ref> on powerful Graphics Processing Units (GPUs). The necessity of expensive hardware and long processing time are the factors that complicate the application of such models on conventional desktops and portable devices. Consequently, a large number of works tried to reduce both hardware requirements (e. g. memory demands) and running times (see Sec. 2). In this paper we consider probably the most frequently used layer of the neural networks: the fullyconnected layer. This layer consists in a linear transformation of a high-dimensional input signal to a high-dimensional output signal with a large dense matrix defining the transformation. For example, in modern CNNs the dimensions of the input and output signals of the fully-connected layers are of the order of thousands, bringing the number of parameters of the fully-connected layers up to millions. We use a compact multiliniear format -Tensor-Train (TT-format) <ref type="bibr" target="#b16">[17]</ref> -to represent the dense weight matrix of the fully-connected layers using few parameters while keeping enough flexibility to perform signal transformations. The resulting layer is compatible with the existing training algorithms for neural networks because all the derivatives required by the back-propagation algorithm <ref type="bibr" target="#b17">[18]</ref> can be computed using the properties of the TT-format. We call the resulting layer a TT-layer and refer to a network with one or more TT-layers as TensorNet. We apply our method to popular network architectures proposed for several datasets of different scales: MNIST <ref type="bibr" target="#b14">[15]</ref>, CIFAR-10 <ref type="bibr" target="#b11">[12]</ref>, ImageNet <ref type="bibr" target="#b12">[13]</ref>. We experimentally show that the networks with the TT-layers match the performance of their uncompressed counterparts but require up to 200 000 times less of parameters, decreasing the size of the whole network by a factor of 7.</p><p>The rest of the paper is organized as follows. We start with a review of the related work in Sec. 2. We introduce necessary notation and review the Tensor Train (TT) format in Sec. 3. In Sec. 4 we apply the TT-format to the weight matrix of a fully-connected layer and in Sec. 5 derive all the equations necessary for applying the back-propagation algorithm. In Sec. 6 we present the experimental evaluation of our ideas followed by a discussion in Sec. 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>With sufficient amount of training data, big models usually outperform smaller ones. However stateof-the-art neural networks reached the hardware limits both in terms the computational power and the memory. In particular, modern networks reached the memory limit with 89% <ref type="bibr" target="#b20">[21]</ref> or even 100% <ref type="bibr" target="#b24">[25]</ref> memory occupied by the weights of the fully-connected layers so it is not surprising that numerous attempts have been made to make the fully-connected layers more compact. One of the most straightforward approaches is to use a low-rank representation of the weight matrices. Recent studies show that the weight matrix of the fully-connected layer is highly redundant and by restricting its matrix rank it is possible to greatly reduce the number of parameters without significant drop in the predictive accuracy <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b24">25</ref>]. An alternative approach to the problem of model compression is to tie random subsets of weights using special hashing techniques <ref type="bibr" target="#b3">[4]</ref>. The authors reported the compression factor of 8 for a twolayered network on the MNIST dataset without loss of accuracy. Memory consumption can also be reduced by using lower numerical precision <ref type="bibr" target="#b0">[1]</ref> or allowing fewer possible carefully chosen parameter values <ref type="bibr" target="#b8">[9]</ref>. In our paper we generalize the low-rank ideas. Instead of searching for low-rank approximation of the weight matrix we treat it as multi-dimensional tensor and apply the Tensor Train decomposition algorithm <ref type="bibr" target="#b16">[17]</ref>. This framework has already been successfully applied to several data-processing tasks, e. g. <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b26">27]</ref>. Another possible advantage of our approach is the ability to use more hidden units than was available before. A recent work <ref type="bibr" target="#b1">[2]</ref> shows that it is possible to construct wide and shallow (i. e. not deep) neural networks with performance close to the state-of-the-art deep CNNs by training a shallow network on the outputs of a trained deep network. They report the improvement of performance with the increase of the layer size and used up to 30 000 hidden units while restricting the matrix rank of the weight matrix in order to be able to keep and to update it during the training. Restricting the TT-ranks of the weight matrix (in contrast to the matrix rank) allows to use much wider layers potentially leading to the greater expressive power of the model. We demonstrate this effect by training a very wide model (262 144 hidden units) on the CIFAR-10 dataset that outperforms other non-convolutional networks. Matrix and tensor decompositions were recently used to speed up the inference time of CNNs <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14]</ref>. While we focus on fully-connected layers, Lebedev et al. <ref type="bibr" target="#b13">[14]</ref> used the CP-decomposition to compress a 4-dimensional convolution kernel and then used the properties of the decomposition to speed up the inference time. This work shares the same spirit with our method and the approaches can be readily combined. Gilboa et al. exploit the properties of the Kronecker product of matrices to perform fast matrix-byvector multiplication <ref type="bibr" target="#b7">[8]</ref>. These matrices have the same structure as TT-matrices with unit TT-ranks. Compared to the Tucker format <ref type="bibr" target="#b22">[23]</ref> and the canonical format <ref type="bibr" target="#b2">[3]</ref>, the TT-format is immune to the curse of dimensionality and its algorithms are robust. Compared to the Hierarchical Tucker format <ref type="bibr" target="#b10">[11]</ref>, TT is quite similar but has simpler algorithms for basic operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">TT-format</head><p>Throughout this paper we work with arrays of different dimensionality. We refer to the onedimensional arrays as vectors, the two-dimensional arrays -matrices, the arrays of higher dimensions -tensors. Bold lower case letters (e. g. a) denote vectors, ordinary lower case letters (e. g. a(i) = a i ) -vector elements, bold upper case letters (e. g. A) -matrices, ordinary upper case letters (e. g. A(i, j)) -matrix elements, calligraphic bold upper case letters (e. g. A) -for tensors and ordinary calligraphic upper case letters (e. g. A(i) = A(i 1 , . . . , i d )) -tensor elements, where d is the dimensionality of the tensor A. We will call arrays explicit to highlight cases when they are stored explicitly, i. e. by enumeration of all the elements. A d-dimensional array (tensor) A is said to be represented in the TT-format <ref type="bibr" target="#b16">[17]</ref> if for each dimension k = 1, . . . , d and for each possible value of the k-th dimension index j k = 1, . . . , n k there exists a matrix G k [j k ] such that all the elements of A can be computed as the following matrix product:</p><formula xml:id="formula_0">A(j 1 , . . . , j d ) = G 1 [j 1 ]G 2 [j 2 ] · · · G d [j d ].</formula><p>(1) All the matrices G k [j k ] related to the same dimension k are restricted to be of the same size r k−1 × r k . The values r 0 and r d equal 1 in order to keep the matrix product (1) of size 1 × 1. In what follows we refer to the representation of a tensor in the TT-format as the TT-representation or the TT-decomposition. The sequence {r k } d k=0 is referred to as the TT-ranks of the TT-representation of A (or the ranks for short), its maximum -as the maximal TT-rank of the TT-representation of A: r = max k=0,...,d r k . The collections of the matrices (G k [j k ]) n k j k =1 corresponding to the same dimension (technically, 3-dimensional arrays G k ) are called the cores. Oseledets [17, Th. 2.1] shows that for an arbitrary tensor A a TT-representation exists but is not unique. The ranks among different TT-representations can vary and it's natural to seek a representation with the lowest ranks. We use the symbols <ref type="formula">(1)</ref> can be equivalently rewritten as the sum of the products of the elements of the cores:</p><formula xml:id="formula_1">G k [j k ](α k−1 , α k ) to denote the element of the matrix G k [j k ] in the position (α k−1 , α k ), where α k−1 = 1, . . . , r k−1 , α k = 1, . . . , r k . Equation</formula><formula xml:id="formula_2">A(j 1 , . . . , j d ) = α0,...,α d G 1 [j 1 ](α 0 , α 1 ) . . . G d [j d ](α d−1 , α d ).</formula><p>(</p><p>The representation of a tensor A via the explicit enumeration of all its elements requires to store d k=1 n k numbers compared with d k=1 n k r k−1 r k numbers if the tensor is stored in the TT-format. Thus, the TT-format is very efficient in terms of memory if the ranks are small. An attractive property of the TT-decomposition is the ability to efficiently perform several types of operations on tensors if they are in the TT-format: basic linear algebra operations, such as the addition of a constant and the multiplication by a constant, the summation and the entrywise product of tensors (the results of these operations are tensors in the TT-format generally with the increased ranks); computation of global characteristics of a tensor, such as the sum of all elements and the Frobenius norm. See <ref type="bibr" target="#b16">[17]</ref> for a detailed description of all the supported operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">TT-representations for vectors and matrices</head><p>The direct application of the TT-decomposition to a matrix (2-dimensional tensor) coincides with the low-rank matrix format and the direct TT-decomposition of a vector is equivalent to explicitly storing its elements. To be able to efficiently work with large vectors and matrices the TT-format for them is defined in a special manner.</p><formula xml:id="formula_4">Consider a vector b ∈ R N , where N = d k=1 n k . We can establish a bijection µ between the coordinate ℓ ∈ {1, . . . , N } of b and a d-dimensional vector- index µ(ℓ) = (µ 1 (ℓ), . . . , µ d (ℓ)) of the corresponding tensor B, where µ k (ℓ) ∈ {1, . . . , n k }.</formula><p>The tensor B is then defined by the corresponding vector elements: B(µ(ℓ)) = b ℓ . Building a TTrepresentation of B allows us to establish a compact format for the vector b. We refer to it as a TT-vector. Now we define a TT-representation of a matrix W ∈ R M×N , where M = d k=1 m k and N = d k=1 n k . Let bijections ν(t) = (ν 1 (t), . . . , ν d (t)) and µ(ℓ) = (µ 1 (ℓ), . . . , µ d (ℓ)) map row and column indices t and ℓ of the matrix W to the d-dimensional vector-indices whose k-th dimensions are of length m k and n k respectively, k = 1, . . . , d. From the matrix W we can form a d-dimensional tensor W whose k-th dimension is of length m k n k and is indexed by the tuple (ν k (t), µ k (ℓ)). The tensor W can then be converted into the TT-format:</p><formula xml:id="formula_5">W (t, ℓ) = W((ν 1 (t), µ 1 (ℓ)), . . . , (ν d (t), µ d (ℓ))) = G 1 [ν 1 (t), µ 1 (ℓ)] . . . G d [ν d (t), µ d (ℓ)], (3) where the matrices G k [ν k (t), µ k (ℓ)]</formula><p>, k = 1, . . . , d, serve as the cores with tuple (ν k (t), µ k (ℓ)) being an index. Note that a matrix in the TT-format is not restricted to be square. Although indexvectors ν(t) and µ(ℓ) are of the same length d, the sizes of the domains of the dimensions can vary. We call a matrix in the TT-format a TT-matrix.</p><p>All operations available for the TT-tensors are applicable to the TT-vectors and the TT-matrices as well (for example one can efficiently sum two TT-matrices and get the result in the TT-format). Additionally, the TT-format allows to efficiently perform the matrix-by-vector (matrix-by-matrix) product. If only one of the operands is in the TT-format, the result would be an explicit vector (matrix); if both operands are in the TT-format, the operation would be even more efficient and the result would be given in the TT-format as well (generally with the increased ranks). For the case of the TT-matrixby-explicit-vector product c = W b, the computational complexity is O(d r 2 m max{M, N }), where d is the number of the cores of the TT-matrix W , m = max k=1,...,d m k , r is the maximal rank and N = d k=1 n k is the length of the vector b. The ranks and, correspondingly, the efficiency of the TT-format for a vector (matrix) depend on the choice of the mapping µ(ℓ) (mappings ν(t) and µ(ℓ)) between vector (matrix) elements and the underlying tensor elements. In what follows we use a column-major MATLAB reshape command 1 to form a d-dimensional tensor from the data (e. g. from a multichannel image), but one can choose a different mapping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">TT-layer</head><p>In this section we introduce the TT-layer of a neural network. In short, the TT-layer is a fullyconnected layer with the weight matrix stored in the TT-format. We will refer to a neural network with one or more TT-layers as TensorNet. Fully-connected layers apply a linear transformation to an N -dimensional input vector x:</p><formula xml:id="formula_6">y = W x + b,<label>(4)</label></formula><p>where the weight matrix W ∈ R M×N and the bias vector b ∈ R M define the transformation.</p><p>A TT-layer consists in storing the weights W of the fully-connected layer in the TT-format, allowing to use hundreds of thousands (or even millions) of hidden units while having moderate number of parameters. To control the number of parameters one can vary the number of hidden units as well as the TT-ranks of the weight matrix. A TT-layer transforms a d-dimensional tensor X (formed from the corresponding vector x) to the ddimensional tensor Y (which correspond to the output vector y). We assume that the weight matrix W is represented in the TT-format with the cores G k [i k , j k ]. The linear transformation (4) of a fully-connected layer can be expressed in the tensor form:</p><formula xml:id="formula_7">Y(i 1 , . . . , i d ) = j1,...,j d G 1 [i 1 , j 1 ] . . . G d [i d , j d ] X (j 1 , . . . , j d ) + B(i 1 , . . . , i d ).<label>(5)</label></formula><p>Direct application of the TT-matrix-by-vector operation for the Eq. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Learning</head><p>Neural networks are usually trained with the stochastic gradient descent algorithm where the gradient is computed using the back-propagation procedure <ref type="bibr" target="#b17">[18]</ref>. Back-propagation allows to compute the gradient of a loss-function L with respect to all the parameters of the network. The method starts with the computation of the gradient of L w.r.t. the output of the last layer and proceeds sequentially through the layers in the reversed order while computing the gradient w.r.t. the parameters and the input of the layer making use of the gradients computed earlier. Applied to the fully-connected layers (4) the back-propagation method computes the gradients w.r.t. the input x and the parameters W and b given the gradients ∂L ∂y w.r.t to the output y:</p><formula xml:id="formula_8">∂L ∂x = W ⊺ ∂L ∂y , ∂L ∂W = ∂L ∂y x ⊺ , ∂L ∂b = ∂L ∂y .<label>(6)</label></formula><p>In what follows we derive the gradients required to use the back-propagation algorithm with the TTlayer. To compute the gradient of the loss function w.r.t. the bias vector b and w.r.t. the input vector x one can use equations <ref type="bibr" target="#b5">(6)</ref>. The latter can be applied using the matrix-by-vector product (where the matrix is in the TT-format) with the complexity of O(dr 2 n max{m, n} d ) = O(dr 2 n max{M, N }). To perform a step of stochastic gradient descent one can use equation <ref type="bibr" target="#b5">(6)</ref> to compute the gradient of the loss function w.r.t. the weight matrix W , convert the gradient matrix into the TT-format (with the TT-SVD algorithm <ref type="bibr" target="#b16">[17]</ref>) and then add this gradient (multiplied by a step size) to the current estimate of the weight matrix: W k+1 = W k + γ k ∂L ∂W . However, the direct computation of In what follows we use shortened notation for prefix and postfix sequences of indices:</p><formula xml:id="formula_9">i − k := (i 1 , . . . , i k−1 ), i + k := (i k+1 , . . . , i d ), i = (i − k , i k , i + k )</formula><p>. We also introduce notations for partial core products:</p><formula xml:id="formula_10">P − k [i − k , j − k ] := G 1 [i 1 , j 1 ] . . . G k−1 [i k−1 , j k−1 ], P + k [i + k , j + k ] := G k+1 [i k+1 , j k+1 ] . . . G d [i d , j d ].<label>(7)</label></formula><p>We now rewrite the definition of the TT-layer transformation (5) for any k = 2, . . . , d − 1:</p><formula xml:id="formula_11">Y(i) = Y(i − k , i k , i + k ) = j − k ,j k ,j + k P − k [i − k , j − k ]G k [i k , j k ]P + k [i + k , j + k ]X (j − k , j k , j + k ) + B(i). (8)</formula><p>The gradient of the loss function L w.r.t. to the k-th core in the position [ĩ k ,j k ] can be computed using the chain rule:</p><formula xml:id="formula_12">∂L ∂G k [ĩ k ,j k ] r k−1 × r k = i ∂L ∂Y(i) ∂Y(i) ∂G k [ĩ k ,j k ] .<label>(9)</label></formula><p>Given the gradient matrices ∂Y(i) ∂G k [ĩ k ,j k ] the summation (9) can be done explicitly in O(M r k−1 r k ) time, where M is the length of the output vector y.</p><p>We now show how to compute the matrix ∂Y(i) ∂G k [ĩ k ,j k ] for any values of the core index k ∈ {1, . . . , d} andĩ k ∈ {1, . . . , m k },j k ∈ {1, . . . , n k }. For any i = (i 1 , . . . , i d ) such that i k =ĩ k the value of Y(i) doesn't depend on the elements of G k [ĩ k ,j k ] making the corresponding gradient ∂Y(i)</p><formula xml:id="formula_13">∂G k [ĩ k ,j k ]</formula><p>equal zero. Similarly, any summand in the Eq. (8) such that j k =j k doesn't affect the gradient</p><formula xml:id="formula_14">∂Y(i) ∂G k [ĩ k ,j k ]</formula><p>. These observations allow us to consider only i k =ĩ k and j k =j k .</p><formula xml:id="formula_15">Y(i − k ,ĩ k , i + k )</formula><p>is a linear function of the core G k [ĩ k ,j k ] and its gradient equals the following expression:</p><formula xml:id="formula_16">∂Y(i − k ,ĩ k , i + k ) ∂G k [ĩ k ,j k ] = j − k ,j + k P − k [i − k , j − k ] ⊺ r k−1 ×1 P + k [i + k , j + k ] ⊺ 1×r k X (j − k ,j k , j + k ).<label>(10)</label></formula><p>We denote the partial sum vector as</p><formula xml:id="formula_17">R k [j − k ,j k , i + k ] ∈ R r k : R k [j 1 , . . . , j k−1 ,j k , i k+1 , . . . , i d ] = R k [j − k ,j k , i + k ] = j + k P + k [i + k , j + k ] X (j − k ,j k , j + k )</formula><p>.</p><formula xml:id="formula_18">Vectors R k [j − k ,j k , i + k ]</formula><p>for all the possible values of k, j − k ,j k and i + k can be computed via dynamic programming (by pushing sums w.r.t. each j k+1 , . . . , j d inside the equation and summing out one index at a time) in O(dr 2 m max{M, N }). Substituting these vectors into (10) and using   matrix rank uncompressed <ref type="figure">Figure 1</ref>: The experiment on the MNIST dataset. We use a two-layered neural network and substitute the first 1024 × 1024 fully-connected layer with the TT-layer (solid lines) and with the matrix rank decomposition based layer (dashed line). The solid lines of different colors correspond to different ways of reshaping the input and output vectors to tensors (the shapes are reported in the legend). To obtain the points of the plots we vary the maximal TT-rank or the matrix rank.</p><p>(again) dynamic programming yields us all the necesary matrices for summation <ref type="bibr" target="#b8">(9)</ref>. The overall computational complexity of the backward pass is O(d 2 r 4 m max{M, N }).</p><p>The presented algorithm reduces to a sequence of matrix-by-matrix products and permutations of dimensions and thus can be accelerated on a GPU device.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Parameters of the TT-layer</head><p>In this experiment we investigate the properties of the TT-layer and compare different strategies for setting its parameters: dimensions of the tensors representing the input/output of the layer and the TT-ranks of the compressed weight matrix. We run the experiment on the MNIST dataset <ref type="bibr" target="#b14">[15]</ref> for the task of handwritten-digit recognition. As a baseline we use a neural network with two fullyconnected layers (1024 hidden units) and rectified linear unit (ReLU) achieving 1.9% error on the test set. For more reshaping options we resize the original 28 × 28 images to 32 × 32.</p><p>We train several networks differing in the parameters of the single TT-layer. The networks contain the following layers: the TT-layer with weight matrix of size 1024×1024, ReLU, the fully-connected layer with the weight matrix of size 1024 × 10. We test different ways of reshaping the input/output tensors and try different ranks of the TT-layer. As a simple compression baseline in the place of the TT-layer we use the fully-connected layer such that the rank of the weight matrix is bounded (implemented as follows: the two consecutive fully-connected layers with weight matrices of sizes 1024 × r and r ×1024, where r controls the matrix rank and the compression factor). The results of the experiment are shown in <ref type="figure">Figure 1</ref>. We conclude that the TT-ranks provide much better flexibility than the matrix rank when applied at the same compression level. In addition, we observe that the TT-layers with too small number of values for each tensor dimension and with too few dimensions perform worse than their more balanced counterparts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison with HashedNet [4]</head><p>. We consider a two-layered neural network with 1024 hidden units and replace both fully-connected layers by the TT-layers. By setting all the TT-ranks in the network to 8 we achieved the test error of 1.6% with 12 602 parameters in total and by setting all the TT-ranks to 6 the test error of 1.9% with 7 698 parameters. Chen et al. <ref type="bibr" target="#b3">[4]</ref> report results on the same architecture. By tying random subsets of weights they compressed the network by the factor of 64 to the 12 720 parameters in total with the test error equal 2.79%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">CIFAR-10</head><p>CIFAR-10 dataset <ref type="bibr" target="#b11">[12]</ref> consists of 32 × 32 3-channel images assigned to 10 different classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck. The dataset contains 50000 train and 10000 test images. Following <ref type="bibr" target="#b9">[10]</ref> we preprocess the images by subtracting the mean and performing global contrast normalization and ZCA whitening. As a baseline we use the CIFAR-10 Quick <ref type="bibr" target="#b21">[22]</ref> CNN, which consists of convolutional, pooling and non-linearity layers followed by two fully-connected layers of sizes 1024 × 64 and 64 × 10. We fix the convolutional part of the network and substitute the fully-connected part by a 1024 × N TT-layer  <ref type="table">Table 2</ref>: Substituting the fully-connected layers with the TT-layers in vgg-16 and vgg-19 networks on the ImageNet dataset. FC stands for a fully-connected layer; TT stands for a TT-layer with all the TT-ranks equal " "; MR stands for a fully-connected layer with the matrix rank restricted to " ". We report the compression rate of the TT-layers matrices and of the whole network in the second, third and fourth columns. In addition, substituting the both fully-connected layers by the TT-layers yields the test error of 24.39% and reduces the number of parameters of the fully-connected layer matrices by the factor of 11.9 and the total parameter number by the factor of 1.7.</p><p>For comparison, in <ref type="bibr" target="#b5">[6]</ref> the fully-connected layers in a CIFAR-10 CNN were compressed by the factor of at most 4.7 times with the loss of about 2% in accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Wide and shallow network</head><p>With sufficient amount of hidden units, even a neural network with two fully-connected layers and sigmoid non-linearity can approximate any decision boundary <ref type="bibr" target="#b4">[5]</ref>. Traditionally, very wide shallow networks are not considered because of high computational and memory demands and the overfitting risk. TensorNet can potentially address both issues. We use a three-layered TensorNet of the following architecture: the TT-layer with the weight matrix of size 3 072 × 262 144, ReLU, the TT-layer with the weight matrix of size 262 144 × 4 096, ReLU, the fully-connected layer with the weight matrix of size 4 096 × 10. We report the test error of 31.47% which is (to the best of our knowledge) the best result achieved by a non-convolutional neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">ImageNet</head><p>In this experiment we evaluate the TT-layers on a large scale task. We consider the 1000-class ImageNet ILSVRC-2012 dataset <ref type="bibr" target="#b18">[19]</ref>, which consist of 1.2 million training images and 50 000 validation images. We use deep the CNNs vgg-16 and vgg-19 <ref type="bibr" target="#b20">[21]</ref> as the reference models 2 . Both networks consist of the two parts: the convolutional and the fully-connected parts. In the both networks the second part consist of 3 fully-connected layers with weight matrices of sizes 25088 × 4096, 4096 × 4096 and 4096 × 1000.</p><p>In each network we substitute the first fully-connected layer with the TT-layer.  <ref type="bibr" target="#b20">[21]</ref>. We train the TT-layer and the fully-connected layers on the training set. In <ref type="table">Table 2</ref> we vary the ranks of the TT-layer and report the compression factor of the TT-layers (vs. the original fully-connected layer), the resulting compression factor of the whole network, and the top 1 and top 5 errors on the validation set. In addition, we substitute the second fully-connected layer with the TT-layer. As a baseline compression method we constrain the matrix rank of the weight matrix of the first fully-connected layer using the approach of <ref type="bibr" target="#b1">[2]</ref>.  <ref type="table">Table 3</ref>: Inference time for a 25088 × 4096 fully-connected layer and its corresponding TT-layer with all the TT-ranks equal 4. The memory usage for feeding forward one image is 392MB for the fully-connected layer and 0.766MB for the TT-layer.</p><p>In <ref type="table">Table 2</ref> we observe that the TT-layer in the best case manages to reduce the number of the parameters in the matrix W of the largest fully-connected layer by a factor of 194 622 (from 25088× 4096 parameters to 528) while increasing the top 5 error from 11.2 to 11.5. The compression factor of the whole network remains at the level of 3.9 because the TT-layer stops being the storage bottleneck. By compressing the largest of the remaining layers the compression factor goes up to 7.4. The baseline method when providing similar compression rates significantly increases the error. For comparison, consider the results of <ref type="bibr" target="#b25">[26]</ref> obtained for the compression of the fully-connected layers of the Krizhevsky-type network <ref type="bibr" target="#b12">[13]</ref> with the Fastfood method. The model achieves compression factors of 2-3 without decreasing the network error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Implementation details</head><p>In all experiments we use our MATLAB extension 3 of the MatConvNet framework 4 <ref type="bibr" target="#b23">[24]</ref>. For the operations related to the TT-format we use the TT-Toolbox 5 implemented in MATLAB as well. The experiments were performed on a computer with a quad-core Intel Core i5-4460 CPU, 16 GB RAM and a single NVidia Geforce GTX 980 GPU. We report the running times and the memory usage at the forward pass of the TT-layer and the baseline fully-connected layer in <ref type="table">Table 3</ref>. We train all the networks with stochastic gradient descent with momentum (coefficient 0.9). We initialize all the parameters of the TT-and fully-connected layers with a Gaussian noise and put L2-regularization (weight 0.0005) on them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion and future work</head><p>Recent studies indicate high redundancy in the current neural network parametrization. To exploit this redundancy we propose to use the TT-decomposition framework on the weight matrix of a fully-connected layer and to use the cores of the decomposition as the parameters of the layer. This allows us to train the fully-connected layers compressed by up to 200 000× compared with the explicit parametrization without significant error increase. Our experiments show that it is possible to capture complex dependencies within the data by using much more compact representations. On the other hand it becomes possible to use much wider layers than was available before and the preliminary experiments on the CIFAR-10 dataset show that wide and shallow TensorNets achieve promising results (setting new state-of-the-art for non-convolutional neural networks). Another appealing property of the TT-layer is faster inference time (compared with the corresponding fully-connected layer). All in all a wide and shallow TensorNet can become a time and memory efficient model to use in real time applications and on mobile devices. The main limiting factor for an M × N fully-connected layer size is its parameters number M N . The limiting factor for an M ×N TT-layer is the maximal linear size max{M, N }. As a future work we plan to consider the inputs and outputs of layers in the TT-format thus completely eliminating the dependency on M and N and allowing billions of hidden units in a TT-layer.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>( 5 )</head><label>5</label><figDesc>yields the computational complexity of the forward pass O(dr 2 m max{m, n} d ) = O(dr 2 m max{M, N }).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>O(dr 2 m max{M, N }) O(r max{M, N }) FC backward pass O(M N ) O(M N ) TT backward pass O(d 2 r 4 m max{M, N }) O(r 3 max{M, N }) Table 1: Comparison of the asymptotic complexity and memory usage of an M × N TT-layer and an M × N fully-connected layer (FC). The input and output tensor shapes are m 1 × . . . × m d and n 1 × . . . × n d respectively (m = max k=1...d m k ) and r is the maximal TT-rank.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>∂L ∂W requires O(M N ) memory. A better way to learn the TensorNet parameters is to compute the gradient of the loss function directly w.r.t. the cores of the TT-representation of W .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>10</head><label>10</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>followed by ReLU and by a N × 10 fully-connected layer. With N = 3125 hidden units (contrary to 64 in the original network) we achieve the test error of 23.13% without fine-tuning which is slightly better than the test error of the baseline (23.25%). The TT-layer treated input and output vectors as 4 × 4 × 4 × 4 × 4 and 5 × 5 × 5 × 5 × 5 tensors respectively. All the TT-ranks equal 8, making the number of the parameters in the TT-layer equal 4 160. The compression rate of the TensorNet compared with the baseline w.r.t. all the parameters is 1.24.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>To do this we reshape the 25088-dimensional input vectors to the tensors of the size 2 × 7 × 8 × 8 × 7 × 4 and the 4096dimensional output vectors to the tensors of the size 4 × 4 × 4 × 4 × 4 × 4. The remaining fullyconnected layers are initialized randomly. The parameters of the convolutional parts are kept fixed as trained by Simonyan and Zisserman</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://www.mathworks.com/help/matlab/ref/reshape.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">After we had started to experiment on the vgg-16 network the vgg-* networks have been improved by the authors. Thus, we report the results on a slightly outdated version of vgg-16 and the up-to-date version of vgg-19.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/Bihaqo/TensorNet 4 http://www.vlfeat.org/matconvnet/ 5 https://github.com/oseledets/TT-Toolbox</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. We would like to thank Ivan Oseledets for valuable discussions. </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Experimental determination of precision requirements for back-propagation training of artificial neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Asanovi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Morgan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Computer Science Institute, Tech. Rep</title>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Do deep nets really need to be deep</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2654" to="2662" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Analysis of individual differences in multidimensional scaling via n-way generalization of Eckart-Young decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Caroll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="283" to="319" />
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Compressing neural networks with the hashing trick</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2285" to="2294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Approximation by superpositions of a sigmoidal function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cybenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<biblScope unit="page" from="303" to="314" />
		</imprint>
	</monogr>
	<note>Mathematics of control, signals and systems</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Predicting parameters in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shakibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>De Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="2148" to="2156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Exploiting linear structure within convolutional networks for efficient evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1269" to="1277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Scaling multidimensional inference for structured gaussian processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gilboa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Saati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Cunningham</surname></persName>
		</author>
		<idno>no. 1209.4120</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Compressing deep convolutional networks using vector quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<idno>no. 1412.6115</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Maxout networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1319" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A new scheme for the tensor representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hackbusch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kühn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Fourier Anal. Appl</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="706" to="722" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>Computer Science Department, University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Speeding-up convolutional neural networks using fine-tuned CP-decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lebedev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rakhuba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Oseledets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">The MNIST database of handwritten digits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Putting MRFs on a Tensor Train</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Novikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rodomanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Osokin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vetrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="811" to="819" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Tensor-Train decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">V</forename><surname>Oseledets</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Scientific Computing</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2295" to="2317" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="issue">6088</biblScope>
			<biblScope unit="page" from="533" to="536" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Low-rank matrix factorization for deep neural network training with high-dimensional output targets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sindhwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Arisoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ramabhadran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference of Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="6655" to="6659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Practical bayesian optimization of machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="2951" to="2959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Some mathematical notes on three-mode factor analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">R</forename><surname>Tucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="279" to="311" />
			<date type="published" when="1966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Matconvnet -convolutional neural networks for MATLAB</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the ACM Int. Conf. on Multimedia</title>
		<meeting>eeding of the ACM Int. Conf. on Multimedia</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Restructuring of deep neural network acoustic models with singular value decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2365" to="2369" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Deep fried convnets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Moczulski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno>no. 1412.7149</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Enabling high-dimensional hierarchical uncertainty quantification by ANOVA and tensor-train decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">V</forename><surname>Oseledets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Karniadakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Daniel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="63" to="76" />
		</imprint>
	</monogr>
	<note>Computer-Aided Design of Integrated Circuits and Systems</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
