<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Contrastive Code Representation Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paras</forename><surname>Jain</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ion Stoica 1</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Jain</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ion Stoica 1</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianjun</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ion Stoica 1</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ion Stoica 1</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ion Stoica 1</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Contrastive Code Representation Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent work learns contextual representations of source code by reconstructing tokens from their context. For downstream semantic understanding tasks like summarizing code in English, these representations should ideally capture program functionality. However, we show that the popular reconstruction-based BERT model is sensitive to source code edits, even when the edits preserve semantics. We propose ContraCode: a contrastive pre-training task that learns code functionality, not form. ContraCode pre-trains a neural network to identify functionally similar variants of a program among many non-equivalent distractors. We scalably generate these variants using an automated source-to-source compiler as a form of data augmentation. Contrastive pre-training improves JavaScript summarization and TypeScript type inference accuracy by 2% to 13%. We also propose a new zero-shot JavaScript code clone detection dataset, showing that ContraCode is both more robust and semantically meaningful. On it, we outperform RoBERTa by 39% AUROC in an adversarial setting and up to 5% on natural code.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Programmers increasingly rely on machine-aided programming tools to aid software development <ref type="bibr" target="#b37">(Kim et al., 2012)</ref>. These tools analyze or transform code automatically. Traditionally, code analysis uses hand-written rules, though the wide diversity of programs encountered in practice can limit their generality. Recent work uses machine learning to improve performance through richer language understanding, such as learning to detect bugs <ref type="bibr" target="#b52">(Pradel &amp; Sen, 2018)</ref> and predict performance <ref type="bibr" target="#b44">(Mendis et al., 2019)</ref>.</p><p>Still, program datasets suffer from scarce annotations due to the time and expertise needed to label code. Synthetic autogenerated labels are used for method naming <ref type="bibr" target="#b4">(Alon et al., 2019a;</ref><ref type="bibr" target="#b7">b)</ref> and bug detection <ref type="bibr">(Ferenc et al., 2018;</ref><ref type="bibr">Pradel &amp; Figure 1</ref>. Robust code clone detection: When trained on source code, BERT is not robust to simple label-preserving code edits like renaming variables. Adversarially selecting between possible edits lowers performance below random guessing. Contrastive pre-training with ContraCode learns a more robust representation of functionality that is consistent across code transforms.  <ref type="figure">Figure 2</ref>. For many learned analyses, programs with the same functionality should have similar representations. ContraCode learns such representations by pre-training an encoder to retrieve equivalent, transformed programs among many distractors. <ref type="figure">Figure 3</ref>. A UMAP visualization of JavaScript method representations learned by RoBERTa and ContraCode, in R 2 . Programs with the same functionality share color and number. RoBERTa's embeddings often do not cluster by functionality, suggesting that it is sensitive to implementation details. For example, many different programs overlap, and renaming the variables of Program 19 significantly changes the embedding. In contrast, variants of Program 19 cluster in ContraCode's embedding space.</p><p>Transformer <ref type="bibr" target="#b67">(Vaswani et al., 2017)</ref> on programs by reconstructing masked or replaced tokens from context. This objective is called masked language modeling (MLM).</p><p>However, we find that BERT is sensitive to implementation details. <ref type="figure">Figure 1</ref> shows the performance of two selfsupervised models on a binary classification task: detecting whether two programs solve the same problem. We mine these programs from the HackerRank interview preparation website. While BERT has comparable performance on the original user-submitted programs (0 edits), BERT's performance greatly degrades when the programs are adversarially transformed e.g. by renaming variables and deleting dead code (1-16 edits). These transforms do not change the functionality of the programs, so are label-preserving. Qualitatively, program representations are not invariant to edits <ref type="figure">(Figure 3</ref>). This could be because accurate reconstructions during pre-training mostly depend on syntactic and program implementation details.</p><p>Motivated by the sensitivity of supervised learning and reconstruction-based pre-training, we develop ContraCode: a self-supervised learning algorithm that explicitly optimizes for representations of program functionality. We hypothesize that programs with the same functionality should have similar underlying representations for downstream code understanding tasks. ContraCode generates syntactically diverse but functionally similar programs with sourceto-source compiler transformation techniques (e.g., dead code elimination, obfuscation and constant folding). Con-traCode uses these programs in a challenging discriminative pretext task that requires the model to identify equivalent programs out of a large dataset of distractors, illustrated in Figure 2. To solve this task, the model has to embed code semantics rather than syntax. In essence, we specify domain knowledge about desired invariances through code transformations. ContraCode improves robustness even under the most adversarial setting in <ref type="figure">Figure 1</ref>, and consistently improves downstream code understanding on other tasks. The contributions of our work include:</p><p>1. the novel use of compiler-based transformations as data augmentations for code, 2. the concept of program representation learning based on functional equivalence, and 3. a detailed analysis of architectures, code transforms and pre-training strategies, showing ContraCode improves type inference top-1 accuracy by 9%, learned inference by 2%-13%, summarization F1 score by up to 8% and clone detection AUROC by 2%-46%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Self-supervised learning (SSL) is a general representation learning strategy where some dimensions or attributes of a datapoint are predicted from the remaining parts. These methods are unsupervised in the sense that they do not rely on labels, but SSL tasks often adapt losses and architectures designed for supervised learning. Selfsupervised pre-training has yielded large improvements in both NLP <ref type="bibr" target="#b28">(Howard &amp; Ruder, 2018;</ref><ref type="bibr">Devlin et al., 2018;</ref><ref type="bibr" target="#b55">Radford et al., 2018;</ref><ref type="bibr" target="#b0">2019)</ref> and computer vision <ref type="bibr" target="#b41">(Mahajan et al., 2018)</ref> by improving generalization <ref type="bibr">(Erhan et al., 2010;</ref><ref type="bibr" target="#b23">Hao et al., 2019)</ref>. Weak visual features, such as orientation <ref type="bibr" target="#b18">(Gidaris et al., 2018)</ref>, color <ref type="bibr" target="#b75">(Zhang et al., 2016)</ref>, and context <ref type="bibr" target="#b49">(Pathak et al., 2016)</ref>, are meaningful signals for representations <ref type="bibr" target="#b41">(Mahajan et al., 2018)</ref>.</p><p>Contrastive learning unifies many past SSL approaches that compare pairs or collections of similar and dissimilar items <ref type="bibr" target="#b22">(Hadsell et al., 2006)</ref>. Rather than training the network to predict labels or reconstruct data, contrastive methods minimize the distance between the representations of similar examples (positives) while maximizing the distance between dissimilar examples (negatives). Examples include Siamese networks <ref type="bibr" target="#b13">(Bromley et al., 1994)</ref> and triplet losses <ref type="bibr" target="#b59">(Schroff et al., 2015)</ref>. Contrastive predictive coding <ref type="bibr" target="#b47">(Oord et al., 2018;</ref><ref type="bibr" target="#b26">Hénaff et al., 2019)</ref> learns to encode chunks of sequential data to predict future chunks with the InfoNCE loss, a variational lower bound on mutual information between views of the data <ref type="bibr" target="#b66">(Tian et al., 2019;</ref><ref type="bibr" target="#b72">Wu et al., 2020)</ref> inspired by noise-constrastive estimation <ref type="bibr" target="#b21">(Gutmann &amp; Hyvärinen, 2010)</ref>. In instance discrimination tasks <ref type="bibr" target="#b73">(Wu et al., 2018)</ref>, views and not pieces of an entire image are compared. SimCLR <ref type="bibr" target="#b14">(Chen et al., 2020a)</ref> and Momentum Contrast <ref type="bibr" target="#b24">(He et al., 2019;</ref><ref type="bibr" target="#b15">Chen et al., 2020b)</ref> recently made progress by using many negatives for dense loss signal. Beyond images, InfoNCE has been applied to NLP <ref type="bibr" target="#b16">(Chuang et al., 2020;</ref><ref type="bibr" target="#b19">Giorgi et al., 2020)</ref>, but may require supervision <ref type="bibr">(Fang &amp; Xie, 2020)</ref>.</p><p>Code representation learning Many works apply machine learning to code . We address code clone detection <ref type="bibr" target="#b71">(White et al., 2016)</ref>, variable type inference <ref type="bibr" target="#b25">(Hellendoorn et al., 2018)</ref>, and summarization <ref type="bibr" target="#b4">(Alon et al., 2019a)</ref>. Others have also explored ML for summarization <ref type="bibr" target="#b46">(Movshovitz-Attias &amp; Cohen, 2013;</ref><ref type="bibr" target="#b1">Allamanis et al., 2016;</ref><ref type="bibr" target="#b33">Iyer et al., 2016)</ref> and type inference <ref type="bibr" target="#b53">(Pradel et al., 2019;</ref><ref type="bibr" target="#b48">Pandi et al., 2020;</ref><ref type="bibr" target="#b70">Wei et al., 2020;</ref><ref type="bibr">Allamanis et al., 2020;</ref><ref type="bibr">Bielik &amp; Vechev, 2020)</ref> with various languages and datasets. The tree or graph structure of code can be exploited to encode invariances in the representation. Inst2vec <ref type="bibr" target="#b9">(Ben-Nun et al., 2018)</ref> locally embeds individual statements in LLVM IR by processing a contextual flow graph with a context prediction objective <ref type="bibr" target="#b45">(Mikolov et al., 2013)</ref>. Tree-Based CNN embeds the Abstract Syntax Tree (AST) nodes of high-level source code. Code2seq <ref type="bibr" target="#b4">(Alon et al., 2019a)</ref> embeds AST paths with an attention-based encoder and LSTM decoder for supervised sequence-to-sequence tasks. <ref type="bibr" target="#b35">Kanade et al. (2020) and</ref><ref type="bibr" target="#b20">Feng et al. (2020)</ref> pre-train a Transformer <ref type="bibr" target="#b67">(Vaswani et al., 2017)</ref> on code using variants of the masked language modeling objective <ref type="bibr">(Devlin et al., 2018)</ref>, an instance of the cloze task <ref type="bibr" target="#b65">(Taylor, 1953)</ref> for reconstructing corrupted tokens. Recurrent networks have also been pre-trained on code <ref type="bibr" target="#b31">(Hussain et al., 2020)</ref> as language models <ref type="bibr" target="#b51">(Peters et al., 2018;</ref><ref type="bibr" target="#b36">Karampatsis &amp; Sutton, 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>Understanding global program functionality is important for difficult semantic tasks. For these problems, learned representations should be similar for functionally equivalent programs and dissimilar for non-equivalent programs. The principle of contrastive learning offers a simple approach for learning such representations if data can be organized into pairs of similar positives and dissimilar negatives <ref type="bibr" target="#b6">(Arora et al., 2019)</ref>. We use these to shape representation space, drawing positives together and pushing negatives apart. A major question remains: given an unlabeled corpus of programs, how do we identify or generate similar programs for positives? We address this question in §3.1 and §3.2, then introduce our learning framework in §3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Compilation as data augmentation</head><p>Modern programming languages afford great flexibility to software developers, allowing them to implement the same desired functionality in different ways. Crowdsourced</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Code compression Identifier modification</head><p>Reformatting <ref type="formula">(</ref> datasets mined from developers, such as GitHub repositories, have many near-duplicates in terms of textual similarity <ref type="bibr" target="#b0">(Allamanis, 2019)</ref>, and are bound to contain even more functional equivalences for common tasks. Satisfiability solvers can identify these equivalent programs <ref type="bibr" target="#b34">(Joshi et al., 2002;</ref><ref type="bibr" target="#b8">Bansal &amp; Aiken, 2006)</ref>, but functional equivalence is undecidable in general <ref type="bibr" target="#b57">(Rice, 1953)</ref>. Also, formal documentation of semantics is required. Programs can instead be compared approximately using test-cases <ref type="bibr" target="#b42">(Massalin, 1987)</ref>, but this is costly and requires executing untrusted code.</p><p>Instead of searching for equivalences, we propose correct by construction data augmentation. Our insight is to apply source-to-source compiler transformations to unlabeled code to generate many variants with the same functionality. For example, dead-code elimination (DCE) is a common compiler optimization that removes operations that leave the output of a function unchanged. While DCE preserves program functionality, <ref type="bibr" target="#b68">Wang &amp; Christodorescu (2019)</ref> find that up to 12.7% of the predictions of current supervised algorithm classification models change after DCE. Supervised datasets were insufficient to acquire the domain knowledge that DCE does not change the algorithm.</p><p>We unambiguously parse a particular source code sequence, e.g. W * x + b into a tree-structured representation (+ ( * W x) b) called an Abstract Syntax Tree (AST). This tree is then transformed by automated traversal algorithms. A rich body of prior programming language work explores parsing then transforming ASTs to optimize a program prior to machine code generation. If source code is emitted by the compiler rather than machine code, this is called source-to-source transformation. Source-to-source transformations are common for optimization and obfuscation purposes in dynamic languages like JavaScript. Further, if each transformation preserves code functionality, then any composition also preserves code functionality.</p><p>We leverage the Babel and Terser compiler infrastructure tools for JavaScript <ref type="bibr">(McKenzie et al., 2020;</ref><ref type="bibr">Santos et al., 2020)</ref> to perform different transformations on method bodies. Example transformations are shown in <ref type="figure">Figure 4</ref>. <ref type="table">Table 1</ref> and the supplement list all transformations, but we broadly  group program transformations into three categories.</p><p>Code compression passes change the syntactic structure of code and perform correct-by-construction transformations such as pre-computing constant expressions at compile time.</p><p>Identifier modification transformations substitute method and variable names with random or short tokens, masking part of the human-readable information in a program but leaving its functionality unchanged. Finally, transformations for Regularization improve model generalization by reducing the number of trivial positive pairs with high text overlap. The line subsampling pass in this group potentially modifies program semantics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Transformation dropout for view diversity</head><p>Computer vision datasets are often augmented with altered images like crops. Back-translations have been used as data augmentations for natural language <ref type="bibr" target="#b62">(Sennrich et al., 2016)</ref>. Similarly, each compiler transformation is an augmentation to a program. Each transformation is a function τ : P → P, where the space of programs P is composed of both the set of valid ASTs and the set of programs in source form.</p><p>Stochastic augmentations like random crops generate many views of an image, but most of our compiler-based transformations are deterministic. To produce a diverse set of transformed programs, we randomly apply a subset of available compiler passes in a pre-specified order, applying transform τ i with probability p i . Intermediate programs are converted between AST and source form as needed for the compiler. Algorithm 1 details our transformation dropout procedure. <ref type="figure">Figure 5</ref> measures the resulting diversity in programs. We precompute up to 20 augmentations of 1.8M JavaScript methods from GitHub. Algorithm 1 deduplicates method variants before pre-training since some transforms will leave the program unchanged. 89% of the methods have more than one alternative after applying 20 random sequences of transformations. The remaining methods without syntactically distinct alternatives include one-line functions that are obfuscated. We apply subword regularization <ref type="bibr" target="#b38">(Kudo, 2018)</ref> as a final transformation to derive different tokenizations every batch, so pairs derived from the same original method will still differ. All transformations are fast; our compiler transforms 300 functions per second on a single CPU core.</p><p>Algorithm 1 Transformation dropout: Stochastic program augmentation with two encodings (AST or source).</p><p>1: Input: Program source x, transformation functions τ1, . . . τ k , transform probabilities p1, . . . p k , count N 2: Returns: N variants of x 3: V ← {x}, a set of augmented program variants 4: for SAMPLE i ← 1 . . . N − 1 do 5:</p><p>x ← x 6:</p><p>for transform t ← 1 . . . k do 7:</p><p>Sample yt ∼ Bernoulli(pt) 8:</p><p>if yt = 1 then 9:</p><p>if REQUIRESAST(τt(·)) and ¬ISAST(x ) then x ← PARSETOAST(x ) 10:</p><p>else if ¬REQUIRESAST(τt(·)) and ISAST(x ) then x ← LOWERTOSOURCE(x ) 11:</p><p>x ← τt(x ) 12: end if 13: end for 14:</p><p>if ISAST(x ) then x ← LOWERTOSOURCE(x ) 15:</p><p>V ← V ∪ {x } 16: end for 17: return V</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Learning an encoder with contrastive pre-training</head><p>While BERT pre-trains a neural program encoder by reconstructing tokens (a generative task), we apply constrative learning to code by shaping the representation at the method level. Contrastive learning is a natural framework to induce invariances into a model by attracting positives while repelling negatives. To adapt recent contrastive learning objectives for images to code representation learning, we leverage the augmentations discussed in Section 3.1-3.2 to define the positive program pairs. Dissimilar negatives are randomly sampled from other programs. We extend the Momentum Contrast method <ref type="bibr" target="#b24">(He et al., 2019)</ref> that was designed for image representation learning. In our case, we learn a program encoder f q that maps a sequence of program tokens to a single, fixed dimensional embedding. This embedding is projected with a small MLP before computing   the pre-training objective.</p><formula xml:id="formula_0">Z i T C K H W V x V p L P S K O U 8 Y z n u q w = " &gt; A A A B / X i c b V D L S s N A F J 3 4 r P E V H z s 3 g 6 U g C C W p g i 6 L b l x W s A 9 o Y p h M J + 3 Q y Y O Z G 6 G G 4 q + 4 c a G I W / / D n X / j t M 1 C W w 9 c O J x z L / f e E 6 S C K 7 D t b 2 N p e W V 1 b b 2 0 Y W 5 u b e / s W n v 7 L Z V k k r I m T U Q i O w F R T P C Y N Y G D Y J 1 U M h I F g r W D 4 f X E b z 8 w q X g S 3 8 E o Z V 5 E + j E P O S W g J d 8 6 r L g 5 d o F k 2 B 2 b l d A f m s P 7 U 9 8 q 2 1 V 7 C r x I n I K U U Y G G b 3 2 5 v Y R m E Y u B C q J U 1 7 F T 8 H I i g V P B x q a b K Z Y S O i R 9 1 t U 0 J h F T X j 6 9 f o w r W u n h M J G 6 Y s B T 9 f d E T i K l R l G g O y M C A z X v T c T / v G 4 G 4 a W X 8 z j N g M V 0 t i j M B I Y E T 6 L A P S 4 Z B T H S h F D J 9</formula><formula xml:id="formula_1">v F l Z Z f W / S t h O G V + o 9 9 m j h V x 5 Y = " &gt; A A A B + H i c b V B N S 8 N A E N 3 U r x o / G v X o Z b E U P J W k C n o s e v F Y w b Z C E 8 p m u 2 m X b j Z x d y L U 0 F / i x Y M i X v 0 p 3 v w 3 b t s c t P X B w O O 9 G W b m h a n g G l z 3 2 y q t r W 9 s b p W 3 7 Z 3 d v f 2 K c 3 D Y 0 U m m K G v T R C T q P i S a C S 5 Z G z g I d p 8 q R u J Q s G 4 4 v p 7 5 3 U e m N E / k H U x S F s R k K H n E K Q E j 9 Z 1 K z c + x D y T D / t S O + g 9 9 p + r W 3 T n w K v E K U k U F W n 3 n y x 8 k N I u Z B C q I 1 j 3 P T S H I i Q J O B Z v a f q Z Z S u i Y D F n P U E l i p o N 8 f v g U 1 4 w y w F G i T E n A c / X 3 R E 5 i r S d x a D p j A i O 9 7 M 3 E / 7 x e B t F l k H O Z Z s A k X S y K M o E h w b M U 8 I A r R k F M D C F U c X M r p i O i C A W T l W 1 C 8 J Z f X i W d R t 0 7 q z d u z 6 v N q y K O M j p G J + g U e e g C N d E N a q E 2 o i h D z + g V v V l P 1 o v 1 b n 0 s W k t W M X O E / s D 6 / A F d E p J A &lt; / l</formula><p>Pre-training objective The contrastive objective maximizes the similarity of positives without collapsing onto a single representation. Like <ref type="bibr" target="#b24">He et al. (2019)</ref>, we use In-foNCE <ref type="bibr" target="#b47">(Oord et al., 2018)</ref>, a tractable objective that frames contrastive learning as a classification task: can the positives be identified among a batch of sampled negatives? InfoNCE computes the probability of selecting the positive (transformed program) by taking the softmax of projected embedding similarities across a batch of negatives. Eq. <ref type="formula">(1)</ref> shows the InfoNCE loss for instance discrimination, a function whose value is low when q is similar to the positive key embedding k + and dissimilar to negative key embeddings k − . t is a temperature hyperparameter <ref type="bibr" target="#b73">(Wu et al., 2018)</ref>.</p><formula xml:id="formula_2">L q,k + ,k − = − log exp(q·k + /t) exp(q·k + /t)+ k − exp(q·k − /t) (1)</formula><p>The query representation q = f q (x q ) is computed by the encoder network f q , and x q is a query program. Likewise, k = f k (x k ) using the EMA key encoder f k . Views x q , x k depend on the specific domain and pretext task. In our case, the views are tokenized representations of the augmented programs, and the summation k − in the normalizing denominator is taken over the queue of pre-computed negatives as well as other non-matching keys in the batch.</p><p>To reduce memory consumption, we enqueue past batches to cache activations for negative samples. These cached samples are valid negatives if the queue is smaller than the dataset size. Following <ref type="bibr" target="#b24">He et al. (2019)</ref>, the query encoder f q is trained via gradient descent while the key encoder f k is trained slowly via an exponential moving average (EMA) of the query encoder parameters. The EMA update stabilizes the pre-computed key embeddings across training iterations. Since keys are only embedded once per epoch, we use a very large set of negatives, over 100K, with minimal additional computational cost and no explicit hard negative mining.</p><p>ContraCode is agnostic to the architecture of the program encoder f q . We evaluate contrastive pre-training of 6layer Transformer <ref type="bibr" target="#b67">(Vaswani et al., 2017)</ref> and 2-layer BiL-STM <ref type="bibr" target="#b60">(Schuster &amp; Paliwal, 1997;</ref><ref type="bibr" target="#b29">Huang et al., 2015)</ref> architectures, with specific details in Section 4.</p><p>Transfer learning After pre-training converges, the encoder f q is transferred to downstream tasks. For code clone detection, we transfer the representation f q (x) in zero-shot, without fine-tuning. For tasks where the output space differs from the encoder, we add a task-specific MLP or Transformer decoder after f q , then fine-tune the resulting network end-to-end on labeled task data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Evaluation</head><p>We evaluate whether self-supervised pre-training with Con-traCode improves JavaScript and TypeScript code analysis through (1) code clone detection <ref type="bibr" target="#b7">(Baker, 1992)</ref>, (2) extreme code summarization <ref type="bibr" target="#b1">(Allamanis et al., 2016)</ref> and <ref type="formula">(3)</ref> type inference <ref type="bibr" target="#b25">(Hellendoorn et al., 2018)</ref> tasks.</p><p>Clone detection experiments show that contrastive and hybrid representations with our compiler-based augmentations are predictive of program functionality in-the-wild, and that contrastive representations are the most robust to adversarial edits ( §4.1). Contrastive pre-training outperforms baseline supervised and self-supervised methods on all three tasks ( §4.1-4.3). Finally, ablations suggest it is better to augment unlabeled programs during pre-training rather than augmenting smaller supervised datasets ( §4.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental setup</head><p>Models are pre-trained on Code-SearchNet, a large corpus of methods extracted from popular GitHub repositories <ref type="bibr" target="#b30">(Husain et al., 2019)</ref>. CodeSearchNet contains 1,843,099 JavaScript programs. Only 81,487 methods have both a documentation string and a method name. The asymmetry between labeled and unlabeled programs stems from JavaScript coding practices where anonymous functions are widespread. The pre-training dataset described in Section 3.1 is the result of augmenting all 1.8M programs.</p><p>As our approach supports any encoder, we evaluate two architectures: a 2-layer Bidirectional LSTM with 18M parameters, similar to the supervised model used by <ref type="bibr" target="#b25">Hellendoorn et al. (2018)</ref>, and a 6-layer Transformer with 23M parameters. For a baseline self-supervised approach, we pre-train both architectures with the RoBERTa MLM objective, then transfer it to downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Evaluating Functionality and Robustness: Zero-shot Code Clone Detection</head><p>ContraCode learns to match variants of programs with similar functionality. While transformations produce highly diverse token sequences (quantified in the supplement), they are artificial and do not change the underlying algorithm. Human programmers can solve a problem with many data structures, algorithms and programming models. Are pretrained representations consistent across programs written by different people? We benchmark on code clone detection, a binary classification task to detect whether two programs solve the same problem or different ones. This is useful for deduplicating, refactoring and retrieving code, as well as checking approximate code correctness.</p><p>Datasets exist like BigCloneBench <ref type="bibr" target="#b64">(Svajlenko et al., 2014)</ref>, but to the best of our knowledge, there is no benchmark for the JavaScript programming language. We collected 274 in-the-wild JavaScript programs correctly solving 33 problems from the HackerRank interview preparation website. There are 2065 pairs solving the same problem and 70K pairs solving different problems, which we randomly subsample to 2065 to balance the classes. Since we probe zero-shot performance based on pre-trained representations, there is no training set. Instead, we threshold cosine similarity of pooled representations of the programs u and v: u T v/ u v . Many traditional code analysis methods for clone detection measure textual similarity <ref type="bibr" target="#b7">(Baker, 1992)</ref>. As a baseline heuristic, we threshold the dissimilarity score, a scaled Levenshtein edit distance between normalized and tokenized programs that excludes formatting changes. However, are these representations robust to code edits? We adversarially edit one program in each pair by applying the loss-maximizing code compression and identifier modification transformation among N samples from Algorithm 1. These transformations preserve program functionality, so ground-truth labels are unchanged. With only 4 possible edits, RoBERTa performs worse than the heuristic (-5.8% AUROC) and worse than random guessing (50% AUROC), indicating it is highly sensitive to these kinds of implementation details. ContraCode retains much of its performance (+39% AUROC over RoBERTa) as it explicitly optimizes for invariance to code edits. Surprisingly, the hybrid model is less robust than ContraCode alone, perhaps indicating that MLM learns non-robust features <ref type="bibr" target="#b32">(Ilyas et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Fine-tuning for Type Inference</head><p>JavaScript is a dynamically typed language, where variable types are determined at runtime based on the values they represent. Manually annotating code with types helps tools flag possible bugs before runtime by detecting incompatible types. Annotations also help programmers document code. However, annotations are tedious to maintain. Type inference tools automatically predict types from context.</p><p>To learn to infer types, we use the same annotated dataset of TypeScript programs from DeepTyper <ref type="bibr" target="#b25">(Hellendoorn et al., 2018)</ref>, without GitHub repositories that were made private or deleted since publication. The training set contains 15,570 TypeScript files from 187 repositories with Benefiting from unlabeled JavaScript programs is challenging because TypeScript is a different dialect. TypeScript supports a superset of JavaScript's grammar, adding type annotations and syntactic sugar that must be learned during fine-tuning. Further, the pre-training dataset contains methods, while DeepTyper's dataset includes entire modules. The RoBERTa baseline may perform poorly since the MLM objective focuses on token reconstruction that is overly sensitive to local syntactic structure, or because sufficient finetuning data is available, described as weight "ossification" by <ref type="bibr" target="#b27">Hernandez et al. (2021)</ref>. To combine the approaches, we minimized our loss in addition to MLM as a hybrid localglobal objective to pre-training a Transformer, improving accuracy by +6.31% over the RoBERTa Transformer.</p><p>Learning outperforms static analysis by a large margin.</p><p>Overall, our best model has +8.9% higher top-1 accuracy than the built-in TypeScript CheckJS type inference system,  <ref type="bibr" target="#b4">(Alon et al., 2019a)</ref> 12.17% 7.65% 9.39% RoBERTa MLM  15.13% 11.47% 12.45% Transformer <ref type="bibr">(Vaswani et al., 2017) 18.11% 15.78% 16.86%</ref> showing the promise of learned code analysis. Surfacing multiple candidate types can also be useful to users. While CheckJS only produces a single prediction, one of our top-5 predictions is correct for 85.6% of labeled tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Fine-tuning for Extreme Code Summarization</head><p>The extreme code summarization task asks a model to predict the name of a method given its body <ref type="bibr" target="#b1">(Allamanis et al., 2016)</ref>. These names often summarize the method, such as reverseString(...). Summarization models could help programmers interpret poorly documented code. We create a JavaScript summarization dataset using the 81,487 labeled methods in the CodeSearchNet dataset. The name is masked in the method declaration. A sequence-to-sequence model with an autoregressive decoder is trained to maximize log likelihood of the ground-truth name, a form of abstractive summarization. All models overfit, so stop early according to validation loss. As proposed by <ref type="bibr" target="#b1">Allamanis et al. (2016)</ref>, we evaluate model predictions by precision, recall and F1 scores over the set of method name tokens. <ref type="table" target="#tab_6">Table 4</ref> shows code summarization results in four settings:</p><p>(1) supervised training using baseline tree-structured architectures that analyze the AST (code2vec, code2seq), (2) pre-training on all 1.8M programs using MLM followed by fine-tuning on the labeled programs (RoBERTa), (3) training a supervised Transformer from scratch and (4) contrastive pre-training followed by fine-tuning with augmentations.</p><p>Contrastive pre-training outperforms code2seq by +8.2% test precision, +7.3% recall, and +7.9% F1 score. The treebased code2seq architecture is a way to encode code-specific invariances into the model, while contrastive pre-training learns invariances through data augmentation. ContraCode outperforms self-supervised pre-training with RoBERTa by +4.8% F1. ContraCode also achieves slightly higher performance than the Transformer learned from scratch with the same network architecture. While this improvement is relatively smaller, code summarization is a difficult task. Naming conventions are not consistent between programmers, and the metric measures exact token matches. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Understanding the importance of augmentation</head><p>We analyze the effect of our proposed augmentations on supervised learning from scratch. We then study the importance of individual augmentations during pre-training.</p><p>Supervised learning with data augmentation As a baseline, we re-train models from scratch with compiler transforms during supervised learning rather than pre-training. Data augmentation artificially expands labeled training sets.</p><p>For sequence-to-sequence summarization, we apply a variety of augmentations; these all preserve the method name label. For type inference, labels are aligned to input tokens, so they must be realigned after transformation. We only apply token-level transforms as we can track label locations. Ablating pre-training augmentations Some data augmentations could be more valuable than others. Empirically, pre-training converges faster with a smaller set of augmentations at the same batch size since the positives are syntactically more similar, but this hurts downstream performance. <ref type="table" target="#tab_9">Table 6</ref> shows that type inference accuracy degrades when different groups of augmentations are removed. Semantics-preserving code compression passes that require code analysis are the most important, improving top-1 accuracy by 1.95% when included. Line subsampling serves as a regularizer, but changes program semantics. LS is relatively less important, but does help accuracy. Identifier modifications preserve semantics, but change useful naming information. Removing these hurts the least. Additional results We perform additional ablations in the supplement by transferring different parts of the network to downstream tasks, computing the contrastive objective with representations taken from different encoder layers, varying architecture, and tuning the pre-training procedure. These experiments suggest that as many parameters as possible should be transferred to the downstream task. Details of the pre-training strategy are also important. For an LSTM, computing the contrastive objective using a global representation q summarizing the whole input sequence x q outperforms a more local representation based on pooling across tokens. Further, a large batch size is helpful to stabilize pre-training. The supplement also includes qualitative results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>Large-scale unannotated repositories of code like GitHub are a powerful resource for learning machine-aided programming tools. However, most current approaches to code representation learning do not leverage unannotated data, and popular self-supervised learning approaches like BERT that learn to reconstruct the text of code are not robust. Instead of reconstructing the text of code, learning what it says, we learn what programs do. We propose ContraCode, a contrastive self-supervised algorithm that learns representations invariant to code transformations. Our method optimizes for this invariance via novel compiler-based data augmentations for code. In experiments on JavaScript, ContraCode learns effective representations of code functionality, and is robust to adversarial code edits. We find that ContraCode significantly improves performance on three downstream JavaScript code understanding tasks. We use the Babel compiler infrastructure <ref type="bibr">(McKenzie et al., 2020)</ref> and the terser JavaScript library for AST-based program transformations. We perform variable renaming and dead code insertion (variable declaration insertion) using custom Babel transforms, subword regularization with sentencepiece Python tokenization library, line subsampling using JavaScript string manipulation primatives and other transformations with terser. Terser has two highlevel transformation modes, mangling and compression, each with finer grained controls such as formatting, comment and log removal, and dead code elimination. We show an example merge sort with variants in <ref type="figure">Figure 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reformatting, beautification, compression (R, B, C):</head><p>Personal coding conventions do not affect the semantics of code; auto-formatting normalizes according to a style convention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dead-code elimination (DCE):</head><p>In this pass, all unused code with no side effects are removed. Various statements can be inlined or removed as stale or unneeded functionality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Type upconversion (T):</head><p>In JavaScript, some types are polymorphic &amp; can be converted between each other. As an example, booleans can be represented as true or as 1.</p><p>Constant folding (CF): During constant folding, all expressions that can be pre-computed at compilation time can be inlined. For example, the expression (2 + 3) * 4 is replaced with 20.</p><p>Variable renaming, identifier mangling (VR, IM): Arguments can be renamed with random word sequences and identifiers can be replaced with short tokens to make the model robust to naming choices. Program behavior is preserved despite obfuscation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dead-code insertion (DCI):</head><p>Commonly used no-ops such as comments and logging are inserted.</p><p>Subword regularization (SW): From <ref type="bibr" target="#b38">Kudo (2018)</ref>, text is tokenized in several different ways, with a single word (_function) or subtokens (_func tion).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Line subsampling (LS):</head><p>We randomly sample (p = 0.9) lines from a method body. While not semantics-preserving, line subsampling serves as a regularizer.</p><p>Original merge sort program // Split the array into halves and merge them recursively function mergeSort (arr) { if (arr.length === 1) { // return once we hit an array with a single item return arr } const middle = Math.floor(arr.length / 2) // get the middle item of the array rounded down const left = arr.slice(0, middle) // items on the left side const right = arr.slice(middle) // items on the right side return merge( mergeSort(left), mergeSort(right) ) } Variable renaming, comment removal, reformatting</p><formula xml:id="formula_3">function mergeSort(e) {</formula><p>if (e.length === 1) { return e; } const t = Math.floor(e.length / 2); const l = e.slice(0, t); const n = e.slice(t); return merge(mergeSort(l), mergeSort(n)); } Combining variable declarations, inlining conditional function mergeSort(e) { if (1 === e.length) return e; const t = Math.floor(e.length / 2), r = e.slice(0, t), n = e.slice(t); return merge(mergeSort(r), mergeSort(n)); } <ref type="figure">Figure 7</ref>. Given a JavaScript code snippet implementing the merge sort algorithm, we apply semantics-preserving transformations to produce functionally-equivalent yet textually distinct code sequences. Variable renaming and identifier mangling passes change variable names. Compression passes eliminate unnecessary characters such as redundant variable declarations and brackets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. How similar are transformed programs?</head><p>To understand the diversity created by program transformations, we compute the Levenshtein minimum edit distance between positive pairs in the precomputed pre-training dataset, i.e. transformed variants of the same source method.</p><p>For comparison, we also compute the edit distance between negative pairs: transformed variants of different programs.</p><p>The edit distance D(x q , x k ) computes the minimum number of token insertions, deletions or substitutions needed to transform the tokenized query progrm x q into the key program x k . To normalize by sequence length | · |, let</p><formula xml:id="formula_4">dissimilarity D (x q , x k ) = D(x q , x k ) max(|x q |, |x k |)<label>(2)</label></formula><p>Dissimilarity ranges from 0% for programs with the same sequence of tokens, to 100% for programs without any shared tokens. Note that whitespace transformations do not affect the metric because the tokenizer collapses repeated whitespace. For the positives, we estimate dissimilarity by sampling one pair per source program in the CodeSearchNet dataset (1.6M source programs with at least one pair). We sample the same number of negative pairs. <ref type="figure" target="#fig_5">Figure 8</ref> shows a histogram of token dissimilarity. Positive pairs have 65% mean dissimilarity, while negatives have 86% mean dissimilarity. Negatives are more dissimilar on average as source sequences could have different lengths, idioms and functionality. Still, the transformations generated quite different positive sequences, with less than half of their tokens shared. The 25th, median and 75th percentile dissimilarity is 59%, 66% and 73% for positives, and 82%, 87% and 90% for negatives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experimental setup Architectures</head><p>The Transformer encoder has 6 layers (23M parameters) in all experiments. For code summarization experiments, we add 4 decoder layers with causal masking to generate the natural language summary. We leverage the default positional embedding function (sin, cos) as used in the original Transformer architecture. The network originally proposed in DeepTyper <ref type="bibr" target="#b25">(Hellendoorn et al., 2018)</ref> had 11M parameters with a 300 dimensional hidden state. We increase the hidden state size to 512 to increase model capacity, so our BiLSTM for type prediction has 17.5M parameters. During fine-tuning, across all experiments, we optimize parameters using Adam with linear learning rate warmup and decay. For the Transformer, the learning rate is linearly increased for 5,000 steps from 0 to a maximum of 10 −4 . For the bidirectional LSTM, the learning rate is increased for between 2,500 and 10,000 steps to a maximum of 10 −3 . Type inference hyperparameters are selected by validation top-1 accuracy.</p><p>ContraCode pre-training The InfoNCE objective is minimized with temperature t = 0.07 following <ref type="bibr" target="#b24">He et al. (2019)</ref>.  encoder parameters θ q . To pretrain a Transformer using the ContraCode objective, we first embed each token in the program using the Transformer. However, the InfoNCE objective is defined in terms of a single embedding for the full program. The ContraCode Transformer is pre-trained with a batch size of 96. Our model averages the 512-dimensional token embeddings across the sequence, then applies a twolayer MLP with 512 hidden units and a ReLU activation to extract a 128-dimensional program embedding for the loss.</p><p>The DeepTyper bidirectional LSTM architecture offers two choices for extracting a global program representation. We aggregate a 1024-dimensional global representation of the program by concatenating its four terminal hidden states (from two sequence processing directions and two stacked LSTM layers), then apply the same MLP architecture as before to extract a 128-dimensional program representation. Alternatively, we can average the hidden state concatenated from each direction across the tokens in the sequence before applying the MLP head. We refer to the hidden-state configuration as a global representation and the sequence averaging configuration as a local representation in <ref type="table">Table 8</ref>.</p><p>We pre-train the BiLSTM with large batch size of 512 and apply weight decay. <ref type="figure" target="#fig_6">Figure 9</ref> shows two programs sampled from the HackerRank clone detection dataset. These programs successfully solve the same problem, so they are clones. We report metrics that treat code clone detection as a binary classification task given a pair of programs. 2065 pairs of programs solving the same HackerRank problem and 2065 pairs of programs solving different problems are sampled to construct an evaluation dataset. We use the area under the Receiver Operating Characteristic (AUROC) metric and Average Precision (AP) metrics. The standard error of the AUROC is reported according to the Wilcoxon statistic <ref type="bibr" target="#b17">(Fogarty et al., 2005)</ref>. Average Precision is the area under the Precision-Recall curve. AUROC and AP are both computed using the scikit-learn library <ref type="bibr" target="#b50">(Pedregosa et al., 2011)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Code clone detection on HackerRank programs</head><p>A Transformer predicts contextual embeddings of each token in a program, but our thresholded cosine similiarity classifier requires fixed length embeddings of whole programs.</p><p>To determine if two programs that may differ in length are clones, we pool the token representations across the sequence. We evaluated both mean pooling and max pooling the representation. For the hybrid model pre-trained with both RoBERTa (MLM) and contrastive objectives, mean pooling achieved the best AUROC and AP. For other models, max pooling performed the best.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Type prediction</head><p>Following DeepTyper <ref type="bibr" target="#b25">(Hellendoorn et al., 2018)</ref>, our regenerated dataset for type prediction has 187 training projects with 15,570 TypeScript files, totaling 6,902,642 tokens. We tune hyperparameters on a validation set of 23 distinct projects with 1,803 files and 490,335 tokens, and evaluate on a held-out test set of 24 projects with 2,206 files and 958,821. The training set is smaller than originally used in DeepTyper as several projects were made private or deleted from GitHub before May 2020 when we downloaded the data, but we used the same commit hashes for available projects so our splits are a subset of the original. We have released the data with our open-source code to facilitate further work on a stable benchmark as more repositories are deleted over time. We perform early stopping to select the number of training epochs. We train each model for 100 epochs and select the checkpoint with the minimum accuracy@1 metric (all types, including any) on the validation set. Except for the model learned from scratch, the Transformer architectures are pre-trained for 240K steps.</p><p>Models with the DeepTyper architecture converge faster on the pre-training tasks and are pre-trained for 20K iterations (unless otherwise noted).</p><p>Extreme code summarization by method name prediction We train method prediction models using the labeled subset of CodeSearchNet. Neither method names nor docstrings are provided as input to the model: the docstring is deleted, and the method name is replaced with the token 'x'. Thus, the task is to predict the method name using the method body and comments alone.</p><p>To decode method names from all models except the code2vec and code2seq baselines which implement their own decoding procedures, we use a beam search with a beam of size 5 and a maximum target sequence length of 20 subword tokens. We detail the cumulative distribution of program lengths in <ref type="figure" target="#fig_7">Figure 10</ref>. The ContraCode summarization Transformer only needed to be pre-trained for 20K iterations, with substantially faster convergence than RoBERTa (240K iterations). During fine-tuning, we apply the LS,SW,VR,DCI augmentations to ContraCode.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Baselines</head><p>Baselines for code summarization and type prediction trained their models on an inconsistent set of programming languages and datasets. In order to normalize the effect of datasets, we selected several diverse state-of-the-art baselines and reimplemented them on the JavaScript dataset.</p><p>AST-based models The authors of code2vec <ref type="bibr" target="#b5">(Alon et al., 2019b)</ref> and code2seq <ref type="bibr" target="#b4">(Alon et al., 2019a)</ref>, AST-based code understanding models, made both data and code available, but train their model on the Java programming language. In order to extend the results in their paper to JavaScript for comparison with our approach, we generated an AST path dataset for the CodeSearchNet dataset. The sensitivity of path-mining embeddings to different datasets is documented in prior work, so published F1 scores are not directly comparable; F1 scores for code2vec <ref type="bibr" target="#b5">(Alon et al., 2019b)</ref> vary between 19 <ref type="bibr" target="#b4">(Alon et al., 2019a)</ref> and 43 <ref type="bibr" target="#b5">(Alon et al., 2019b)</ref> depending on the dataset used. Therefore, we use the same dataset generation code as the authors for fair comparison. We first parse the source functions using the Babel compiler infrastructure. Using the original code on these ASTs, up to 300 token-to-token (leaf-to-leaf) paths are extracted from each function's AST as a precomputed dataset. Then, we generate a token and AST node vocabulary using the same author-provided code, and train the models for 20 epochs, using early stopping for code2seq. We observed that code2vec overfits after 20 epochs, and longer training was not beneficial.</p><p>DeepTyper <ref type="bibr" target="#b25">(Hellendoorn et al., 2018)</ref> DeepTyper uses a two layer GRU with a projection over possible classes, with an embedding size of 300 and hidden dimension of 650. However, we found improved performance by replacing the GRU with a bidirectional LSTM (BiLSTM). We normalize the LSTM parameter count to match our model, and therefore use a hidden dimension size of 512. We also use subword tokenization rather than space delimited tokens according to <ref type="bibr" target="#b38">Kudo (2018)</ref>, as subwords are a key part of state-of-the-art models for NLP <ref type="bibr" target="#b61">(Sennrich et al., 2015)</ref>.</p><p>RoBERTa We pre-trained an encoder using RoBERTa's masked language modeling loss on our augmented version of CodeSearchNet, the same data used to pre-train Contra-Code. This model is then fine-tuned on downstream datasets. Unlike the original BERT paper which cuBERT <ref type="bibr" target="#b35">(Kanade et al., 2020)</ref> is based on, hyperparameters from RoBERTa have been found to produce better results during pre-training. RoBERTa pre-trains using a masked language modeling (MLM) objective, where 15% of tokens in a sentence are masked or replaced and are reconstructed by the model. We did not use the BERT Next Sentence Prediction (NSP) loss which RoBERTa finds to be unnecessary. We normalize baseline parameter count by reducing the number of Transformer layers from 24 to 6 for a total of 23M parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Additional results and ablations</head><p>Code clone detection ROC and PR curves <ref type="figure">Figure 11</ref> plots true postive rate vs false positive rate and precision vs recall for different zero-shot classifiers on the code clone detection downstream tasks. These classifiers threshold a similarity score given by token-level edit distance for the heuristic approach or cosine similarity for the neural network representations. The hybrid self-supervised model combining ContraCode's contrastive objective and masked language modeling achieves better tradeoffs than the other approaches. <ref type="figure">Figure 12</ref> shows the AUROC and Average Precision of four Transformer models on the same task under adversarial transformations of one input program. Untrained models as well as models pre-trained with RoBERTa's MLM objective are not robust to these code transformations. However, the model pre-trained with ContraCode preserves much of its performance as the adversarial attack is strengthened.</p><p>Which part of the model should be transferred? Sim-CLR <ref type="bibr" target="#b14">(Chen et al., 2020a)</ref> proposed using a small MLP head to reduce the dimensionality of the representation used in the InfoNCE loss during pre-training, and did not transfer the MLP to the downstream image-classification task. In contrast, we find it beneficial to transfer part of the contrastive MLP head to type inference, showing a 2% improvement in top-5 accuracy over transferring the encoder only <ref type="table" target="#tab_11">(Table 7)</ref>. We believe the improvement stems from fine-tuning both the encoder and MLP which allows feature adaptation, while SimCLR trained a linear model on top of frozen features. We only transferred the MLP when contrasting the mean of token embeddings during pre-training, not the terminal hidden states, as the dimensionality of the MLP head differs. These representations are compared next.</p><p>Should we pre-train global or local representations?</p><p>We compare pre-training DeepTyper with two variants of ContraCode. We either use the mean of token hidden states across the program (averaging local features), or the terminal hidden states as input to the MLP used to extract the contrastive representation q = f q (x) (global features). Token-level features might capture more syntactic details, but averaging pooling ignores order. <ref type="table">Table 8</ref> shows the accuracy of a BiLSTM pre-trained with each strategy. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Warm-started layers Acc@1 Acc@5</head><p>BiLSTM 49.32% 80.03% BiLSTM, 1 layer of MLP 49.15% 82.58%  Using the global features for pre-training yields significantly improved performance, +2.38% acc@1 after 10K iterations of pre-training (not converged for the purposes of ablation). The global pre-training strategy achieves our best results.</p><p>Do pre-trained encoders help more with shallow decoders?</p><p>For the sequence-to-sequence code summarization task, ContraCode only pre-trains the encoder of the Transformer. In <ref type="table">Table 9</ref>, we ablate the depth of the decoder to understand how much shallow decoders benefit from contrastive pre-training of the encoder. Similar experiments were performed in a vision context by <ref type="bibr">(Erhan et al., 2010)</ref>, where different numbers of layers of a classifier are pre-trained. After 45k pre-training steps, the 4-layer decoder achieves 0.50% higher precision, 0.64% higher recall and 0.77% higher F1 score than the 1-layer model, so additional decoder depth is helpful for the downstream task. The 1-layer decoder model also benefits significantly from longer pre-training, with a 6.3% increase in F1 from 10k to 45k iterations. This large of an improvement indicates that ContraCode could be more helpful for pre-training when the number of randomly initialized parameters at the start of fine-tuning is small. For larger decoders, more parameters must be optimized during-finetuning, and the value of pre-training is diminished. <ref type="figure" target="#fig_9">Figure 13</ref>, we compare two strategies of refreshing the MoCo queue of key embeddings (the dictionary of negative program representations assumed to be non-equivalent to the batch of positives). In the first strategy, we add 8 items out of the batch to the queue (1×), while in the second we add 96 items (12×). In addition, we use a larger queue (65k versus 125k keys) and a slightly larger batch size (64 versus 96). We observe that for the baseline queue fill rate, the accuracy decreases for the first 8125 iterations as the queue fills. This decrease in accuracy is expected as the task becomes more difficult due to the increasing number of negatives during queue warmup. However, it is surprising that accuracy grows so slowly once the queue is filled. We suspect this is because the key encoder changes significantly over thousands of iterations: with a momentum term m = 0.999, the original key encoder parameters are decayed by a factor of 2.9 × 10 −4 by the moving average. If the queue is rapidly refreshed, queue embeddings are predicted by recent key encoders, not old parameters. This also indicates that a large diversity of negative, non-equivalent programs are helpful for rapid convergence of ContraCode pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contrastive representation learning strategies In</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Qualitative results</head><p>t-SNE visualization of representations We qualitatively inspect the structure of the learned representation space by visualizing self-supervised representations of variants of 28 programs using t-SNE <ref type="bibr" target="#b40">(Maaten &amp; Hinton, 2008)</ref> in <ref type="figure" target="#fig_10">Figure 15</ref>. Representations of transformed variants of the same program are plotted with the same color. ContraCode (BiL-STM) clusters variants closely together. Indeed, contrastive learning learns representations that are invariant to a wide <ref type="figure">Figure 14</ref>. Our model, a variant of DeepTyper pretrained with ContraCode, generates type annotations for two programs in the held-out set. The model consistently predicts the correct return type of functions, and even predicts project-specific types imported at the top of the file. The model corresponds to the top row of <ref type="table">Table 8</ref>, though is not our best performing model.  <ref type="figure" target="#fig_4">Figure 16</ref>. A JavaScript program from the CodeSearchNet dataset not seen during training and the predicted method names from a Transformer pre-trained with ContraCode. ContraCode predicts the correct method name as its most likely decoding. class of automated compiler-based transformations. In comparison, the representations learned by masked language modeling (RoBERTa) show more overlap between different programs, and variants do not cleanly cluster. With a hybrid loss combining masked language modeling and contrastive learning, representations of variants of the same program once again cluster. <ref type="figure" target="#fig_4">Figure 16</ref> shows a qualitative example of predictions for the code summarization task. The JavaScript method is not seen during training. A Transformer pre-trained with ContraCode predicts the correct method name through beam search. The next four predictions are reasonable, capturing that the method processes an image. The 2nd and 3rd most likely decodings, getImageItem and createImage, use get and create as synonyms for load, though the final two unlikely decodings include terms not in the method body.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Code summaries</head><p>Type inferences We can also visualize outputs of the type inference model. <ref type="figure">Figure 14</ref> shows two TypeScript programs from the held-out test set. User-provided type annotations are removed from the programs, and the model is provided with a tokenized form without access to dependencies. We visualize predictions from a variant of DeepTyper pre-trained with ContraCode. This corresponds to the bestperforming model in <ref type="table">Table 8</ref>.</p><p>In the first program, our model consistently predicts the correct return and parameter type. While a tool based on static analysis could infer the void return types, the type of the message argument is ambiguous without access to the imported write method signature. Still, the model correctly predicts with high confidence that the variable message is a string. In the second program, ContraCode correctly predicts 4 of 8 types including the ViewContainerRef and ChangeDetectorRef types, each imported from the AngularJS library. As this sample is held-out from the training set, these predictions show generalization from other repositories using AngularJS.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>) { while (i &lt; n) { ... } } function (str, len) { return str.slice(0, len); } function f(n) { return n&lt;2 ? 1 : f(n-1) + f(n-2); } function (arr) { for (i of arr) { ... } } Maximize similarity with equivalent programs Minimize similarity with functionally different programs Given a program,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .Figure 5 .</head><label>45</label><figDesc>A JavaScript method from our unlabeled training set with two automatically generated semantically-equivalent programs. The method is from the StackEdit Markdown editor. Histogram of the number of unique transformed variants per JavaScript method during pre-training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " k w v F l Z Z f W / S t h O G V + o 9 9 m j h V x 5 Y = " &gt; A A A B + H i c b V B N S 8 N A E N 3 U r x o / G v X o Z b E U P J W k C n o s e v F Y w b Z C E 8 p m u 2 m X b j Z x d y L U 0 F / i x Y M i X v 0 p 3 v w 3 b t s c t P X B w O O 9 G W b m h a n g G l z 3 2 y q t r W 9 s b p W 3 7 Z 3 d v f 2 K c 3 D Y 0 U m m K G v T R C T q P i S a C S 5 Z G z g I d p 8 q R u J Q s G 4 4 v p 7 5 3 U e m N E / k H U x S F s R k K H n E K Q E j 9 Z 1 K z c + x D y T D / t S O + g 9 9 p + r W 3 T n w K v E K U k U F W n 3 n y x 8 k N I u Z B C q I 1 j 3 P T S H I i Q J O B Z v a f q Z Z S u i Y D F n P U E l i p o N 8 f v g U 1 4 w y w F G i T E n A c / X 3 R E 5 i r S d x a D p j A i O 9 7 M 3 E / 7 x e B t F l k H O Z Z s A k X S y K M o E h w b M U8 I A r R k F M D C F U c X M r p i O i C A W T l W 1 C 8 J Z f X i W d R t 0 7 q z d u z 6 v N q y K O M j p G J + g U e e g C N d E N a q E 2 o i h D z + g V v V l P 1 o v 1 b n 0 s W k t W M X O E / s D 6 / A F d E p J A &lt; / l a t e x i t &gt; f k &lt; l at e x i t s h a 1 _ b a s e 6 4 = " / t y r M S r E / Z i F Z K S p z m o b 4 g k 8 E h s = " &gt; A A A B + H i c b V D L S g N B E J y N r 7 g + E v X o Z T A E P I X d K O g x 6 M V j B P O A 7 L L M T m a T I b M P Z n q E u O R L v H h Q x K u f 4 s 2 / c Z L s Q R M L G o q q b r q 7 w k x w B Y 7 z b Z U 2 N r e 2 d 8 q 7 9 t 7 + w W G l e n T c V a m W l H V o K l L Z D 4 l i g i e s A x w E 6 2 e S k T g U r B d O b u d + 7 5 F J x d P k A a Y Z 8 2 M y S n j E K Q E j B d V K 3 c u x B 0 R j b 2 Z H w S S o 1 p y G s w B e J 2 5 B a q h A O 6 h + e c O U 6 p g l Q A V R a u A 6 G f g 5 k c C p Y D P b 0 4 p l h E 7 I i A 0 M T U j M l J 8 v D p / h u l G G O E q l q Q T w Q v 0 9 k Z N Y q W k c m s 6 Y w F i t e n P x P 2 + g I b r 2 c 5 5 k G l h C l 4 s i L T C k e J 4 C H n L J K I i p I Y R K b m 7 F d E w k o W C y s k 0 I 7 u r L 6 6 T b b L g X j e b 9 Z a 1 1 U 8 R R R q f o D J 0 j F 1 2 h F r p D b d R B F G n 0 j F 7 R m / V k v V j v 1 s e y t W Q V M y f o D 6 z P H 1 P 6 k j o = &lt; / l a t e x i t &gt; Embed q &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " H / j q p S m 0 8 d t f k S I P s z p 3 T h F A j b M = " &gt; A A A B + 3 i c b V B N S 8 N A E J 3 U r x q / Y j 1 6 W S w F T y W p g h 6 L X j x W s K 3 Q h L D Z b t q l m w 9 3 N 2 I J + S t e P C j i 1 T / i z X / j t s 1 B W x 8 M P N 6 b Y W Z e k H I m l W 1 / G 5 W 1 9 Y 3 N r e q 2 u b O 7 t 3 9 g H d Z 6 M s k E o V 2 S 8 E T c B 1 h S z m L a V U x x e p 8 K i q O A 0 3 4 w u Z 7 5 / U c q J E v i O z V N q R f h U c x C R r D S k m / V G m 6 O X I U z 5 B Z m I / Q n 5 o N v 1 e 2 m P Q d a J U 5 J 6 l C i 4 1 t f 7 j A h W U R j R T i W c u D Y q f J y L B Q j n B a m m 0 m a Y j L B I z r Q N M Y R l V 4 + v 7 1 A D a 0 M U Z g I X b F C c / X 3 R I 4 j K a d R o D s j r M Z y 2 Z u J / 3 m D T I W X X s 7 i N F M 0 J o t F Y c a R S t A s C D R k g h L F p 5 p g I p i + F Z E x F p g o H Z e p Q 3 C W X 1 4 l v V b T O W u 2 b s / r 7 a s y j i o c w w m c g g M X 0 I Y b 6 E A X C D z B M 7 z C m 1 E Y L 8 a 7 8 b F o r R j l z B H 8 g f H 5 A 8 R e k v g = &lt; / l a t e x i t &gt; k + &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " t 0 g</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>a 2 Y D o g k F H R g p g 7 B m X 9 5 k b R q V e e s W r s 9 L 9 e v i j h K 6 A g d o x P k o A t U R z e o g Z q I o k f 0 j F 7 R m / F k v B j v x s e s d c k o Z g 7 Q H x i f P + D B k 4 8 = &lt; / l a t e x i t &gt; {⌧ } &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " j 3 Q N h m h l N D y V Q 5 W e 0 b 7 L P M u L a l M = " &gt; A A A B / X i c b V D L S s N A F J 3 4 r P E V H z s 3 g 6 X g x p J U Q Z d F N y 4 r 2 A c 0 M U y m k 3 b o 5 M H M j V B D 8 V f c u F D E r f / h z r 9 x 2 m a h r Q c u H M 6 5 l 3 v v C V L B F d j 2 t 7 G 0 v L K 6 t l 7 a M D e 3 t n d 2 r b 3 9 l k o y S V m T J i K R n Y A o J n j M m s B B s E 4 q G Y k C w d r B 8 H r i t x + Y V D y J 7 2 C U M i 8 i / Z i H n B L Q k m 8 d u j l 2 g W T Y H Z u V 0 B + a l e H 9 q W + V 7 a o 9 B V 4 k T k H K q E D D t 7 7 c X k K z i M V A B V G q 6 9 g p e D m R w K l g Y 9 P N F E s J H Z I + 6 2 o a k 4 g p L 5 9 e P 8 Y V r f R w m E h d M e C p + n s i J 5 F S o y j Q n R G B g Z r 3 J u J / X j e D 8 N L L e Z x m w G I 6 W x R m A k O C J 1 H g H p e M g h h p Q q j k + l Z M B 0 Q S C j o w U 4 f g z L + 8 S F q 1 q n N W r d 2 e l + t X R R w l d I S O 0 Q l y 0 A W q o x v U Q E 1 E 0 S N 6 R q / o z X g y X o x 3 4 2 P W u m Q U M w f o D 4 z P H + Z 4 k 5 E = &lt; / l a t e x i t &gt; Enqueue as future negative Maximize Minimize k &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 3 x G i m k e O 8 C B B u + b 4 4 T + I B r 6 W E R k = " &gt; A A A B / X i c b V D L S s N A F J 3 4 r P E V H z s 3 g 6 X g x p J U Q Z d F N y 4 r 2 A c 0 M U y m k 3 b o 5 M H M j V B D 8 V f c u F D E r f / h z r 9 x 2 m a h r Q c u H M 6 5 l 3 v v C V L B F d j 2 t 7 G 0 v L K 6 t l 7 a M D e 3 t n d 2 r b 3 9 l k o y S V m T J i K R n Y A o J n j M m s B B s E 4 q G Y k C w d r B 8 H r i t x + Y V D y J 7 2 C U M i 8 i / Z i H n B L Q k m 8 d V t w c u 0 A y 7 I 7 N S u g P z e H 9 q W + V 7 a o 9 B V 4 k T k H K q E D D t 7 7 c X k K z i M V A B V G q 6 9 g p e D m R w K l g Y 9 P N F E s J H Z I + 6 2 o a k 4 g p L 5 9 e P 8 Y V r f R w m E h d M e C p + n s i J 5 F S o y j Q n R G B g Z r 3 J u J / X j e D 8 N L L e Z x m w G I 6 W x R m A k O C J 1 H g H p e M g h h p Q q j k + l Z M B 0 Q S C j o w U 4 f g z L + 8 S F q 1 q n N W r d 2 e l + t X R R w l d I S O 0 Q l y 0 A W q o x v U Q E 1 E 0 S N 6 R q / o z X g y X o x 3 4 2 P W u m Q U M w f o D 4 z P H + P J k 5 E = &lt; / l a t e x i t &gt; k &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 3 x G i m k e O 8 C B B u + b 4 4 T + I B r 6 W E R k = " &gt; A A A B / X i c b V D L S s N A F J 3 4 r P E V H z s 3 g 6 X g x p J U Q Z d F N y 4 r 2 A c 0 M U y m k 3 b o 5 M H M j V B D 8 V f c u F D E r f / h z r 9 x 2 m a h r Q c u H M 6 5 l 3 v v C V L B F d j 2 t 7 G 0 v L K 6 t l 7 a M D e 3 t n d 2 r b 3 9 l k o y S V m T J i K R n Y A o J n j M m s B B s E 4 q G Y k C w d r B 8 H r i t x + Y V D y J 7 2 C U M i 8 i / Z i H n B L Q k m 8 d V t w c u 0 A y 7 I 7 N S u g P z e H 9 q W + V 7 a o 9 B V 4 k T k H K q E D D t 7 7 c X k K z i M V A B V G q 6 9 g p e D m R w K l g Y 9 P N F E s J H Z I + 6 2 o a k 4 g p L 5 9 e P 8 Y V r f R w m E h d M e C p + n s i J 5 F S o y j Q n R G B g Z r 3 J u J / X j e D 8 N L L e Z x m w G I 6 W x R m A k O C J 1 H g H p e M g h h p Q q j k + l Z M B 0 Q S C j o w U 4 f g z L + 8 S F q 1 q n N W r d 2 e l + t X R R w l d I S O 0 Q l y 0 A W q o x v U Q E 1 E 0 S N 6 R q / o z X g y X o x 3 4 2 P W u m Q U M w f o D 4 z P H + P J k 5 E = &lt; / l a t e x i t &gt; k &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 3 x G i m k e O 8 C B B u + b 4 4 T + I B r 6 W E R k = " &gt; A A A B / X i c b V D L S s N A F J 3 4 r P E V H z s 3 g 6 X g x p J U Q Z d F N y 4 r 2 A c 0 M U y m k 3 b o 5 M H M j V B D 8 V f c u F D E r f / h z r 9 x 2 m a h r Q c u H M 6 5 l 3 v v C V L B F d j 2 t 7 G 0 v L K 6 t l 7 a M D e 3 t n d 2 r b 3 9 l k o y S V m T J i K R n Y A o J n j M m s B B s E 4 q G Y k C w d r B 8 H r i t x + Y V D y J 7 2 C U M i 8 i / Z i H n B L Q k m 8 d V t w c u 0 A y 7 I 7 N S u g P z e H 9 q W + V 7 a o 9 B V 4 k T k H K q E D D t 7 7 c X k K z i M V A B V G q 6 9 g p e D m R w K l g Y 9 P N F E s J H Z I + 6 2 o a k 4 g p L 5 9 e P 8 Y V r f R w m E h d M e C p + n s i J 5 F S o y j Q n R G B g Z r 3 J u J / X j e D 8 N L L e Z x m w G I 6 W x R m A k O C J 1 H g H p e M g h h p Q q j k + l Z M B 0 Q S C j o w U 4 f g z L + 8 S F q 1 q n N W r d 2 e l + t X R R w l d I S O 0 Q l y 0 A W q o x v U Q E 1 E 0 S N 6 R q / o z X g y X o x 3 4 2 P W u m Q U M w f o D 4 z P H + P J k 5 E = &lt; / l a t e x i t &gt; k &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 3 x G i m k e O 8 C B B u + b 4 4 T + I B r 6 W E R k = " &gt; A A A B / X i c b V D L S s N A F J 3 4 r P E V H z s 3 g 6 X g x p J U Q Z d F N y 4 r 2 A c 0 M U y m k 3 b o 5 M H M j V B D 8 V f c u F D E r f / h z r 9 x 2 m a h r Q c u H M 6 5 l 3 v v C V L B F d j 2 t 7 G 0 v L K 6 t l 7 a M D e 3 t n d 2 r b 3 9 l k o y S V m T J i K R n Y A o J n j M m s B B s E 4 q G Y k C w d r B 8 H r i t x + Y V D y J 7 2 C U M i 8 i / Z i H n B L Q k m 8 d V t w c u 0 A y 7 I 7 N S u g P z e H 9 q W + V 7 a o 9 B V 4 k T k H K q E D D t 7 7 c X k K z i M V A B V G q 6 9 g p e D m R w K l g Y 9 P N F E s J H Z I + 6 2 o a k 4 g p L 5 9 e P 8 Y V r f R w m E h d M e C p + n s i J 5 F S o y j Q n R G B g Z r 3 J u J / X j e D 8 N L L e Z x m w G I 6 W x R m A k O C J 1 H g H p e M g h h p Q q j k + l Z M B 0 Q S C j o w U 4 f g z L + 8 S F q 1 q n N W r d 2 e l + t X R R w l d I S O 0 Q l y 0 A W q o x v U Q E 1 E 0 S N 6 R q / o z X g y X o x 3 4 2 P W u m Q U M w f o D 4 z P H + P J k 5 E = &lt; / l a t e x i t &gt; k &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 3 x G i m k e O 8 C B B u + b 4 4 T + I B r 6 W E R k = " &gt; A A A B / X i c b V D L S s N A F J 3 4 r P E V H z s 3 g 6 X g x p J U Q Z d F N y 4 r 2 A c 0 M U y m k 3 b o 5 M H M j V B D 8 V f c u F D E r f / h z r 9 x 2 m a h r Q c u H M 6 5 l 3 v v C V L B F d j 2 t 7 G 0 v L K 6 t l 7 a M D e 3 t n d 2 r b 3 9 l k o y S V m T J i K R n Y A o J n j M m s B B s E 4 q G Y k C w d r B 8 H r i t x + Y V D y J 7 2 C U M i 8 i / Z i H n B L Q k m 8 d V t w c u 0 A y 7 I 7 N S u g P z e H 9 q W + V 7 a o 9 B V 4 k T k H K q E D D t 7 7 c X k K z i M V A B V G q 6 9 g p e D m R w K l g Y 9 P N F E s J H Z I + 6 2 o a k 4 g p L 5 9 e P 8 Y V r f R w m E h d M e C p + n s i J 5 F S o y j Q n R G B g Z r 3 J u J / X j e D 8 N L L e Z x m w G I 6 W x R m A k O C J 1 H g H p e M g h h p Q q j k + l Z M B 0 Q S C j o w U 4 f g z L + 8 S F q 1 q n N W r d 2 e l + t X R R w l d I S O 0 Q l y 0 A W q o x v U Q E 1 E 0 S N 6 R q / o z X g y X o x 3 4 2 P W u m Q U M w f o D 4 z P H + P J k 5 E = &lt; / l a t e x i t &gt; k &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 3 x G i m k e O 8 C B B u + b 4 4 T + I B r 6 W E R k = " &gt; A A A B / X i c b V D L S s N A F J 3 4 r P E V H z s 3 g 6 X g x p J U Q Z d F N y 4 r 2 A c 0 M U y m k 3 b o 5 M H M j V B D 8 V f c u F D E r f / h z r 9 x 2 m a h r Q c u H M 6 5 l 3 v v C V L B F d j 2 t 7 G 0 v L K 6 t l 7 a M D e 3 t n d 2 r b 3 9 l k o y S V m T J i K R n Y A o J n j M m s B B s E 4 q G Y k C w d r B 8 H r i t x + Y V D y J 7 2 C U M i 8 i / Z i H n B L Q k m 8 d V t w c u 0 A y 7 I 7 N S u g P z e H 9 q W + V 7 a o 9 B V 4 k T k H K q E D D t 7 7 c X k K z i M V A B V G q 6 9 g p e D m R w K l g Y 9 P N F E s J H Z I + 6 2 o a k 4 g p L 5 9 e P 8 Y V r f R w m E h d M e C p + n s i J 5 F S o y j Q n R G B g Z r 3 J u J / X j e D 8 N L L e Z x m w G I 6 W x R m A k O C J 1 H g H p e M g h h p Q q j k + l Z M B 0 Q S C j o w U 4 f g z L + 8 S F q 1 q n N W r d 2 e l + t X R R w l d I S O 0 Q l y 0 A W q o x v U Q E 1 E 0 S N 6 R q / o z X g y X o x 3 4 2 P W u m Q U M w f o D 4 z P H + P J k 5 E = &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4 = " k w</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>a t e x i t &gt; MLP MLP ContraCode pre-trains a neural program encoder fq and transfers it to downstream tasks. A-B. Unlabeled programs are transformed C. into augmented variants. D. We pre-train fq by maximizing similarity of projected embeddings of positive program pairs-variants of the same program-and minimizing similarity with a queue of cached negatives. E. ContraCode supports any architecture for fq that produces a global program embedding such as Transformers and LSTMs. fq is then fine-tuned on smaller labeled datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 .</head><label>8</label><figDesc>Also following<ref type="bibr" target="#b24">He et al. (2019)</ref>, the key encoder's parameters are computed with the momentum update equation θ k ← mθ k + (1 − m)θ q , equivalent to an EMA of the query Histogram of pairwise token dissimilarity for contrastive positives (transformed variants of the same method) and negatives (transformed variants of different methods). Code transformations produce positives with dissimilar token sequences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 .</head><label>9</label><figDesc>Code clone detection example. These programs solve the same HackerRank coding challenge (reading and summing two integers), but use different coding conventions. The neural code clone detector should classify this pair as a positive, i.e. a clone.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 .</head><label>10</label><figDesc>CodeSearchNet code summarization dataset statistics: (a) The majority of code sequences are under 2000 characters, but there is long tail of programs that span up to 15000 characters long, (b) JavaScript method names are relatively short compared to languages like C and Java.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 11 .Figure 12 .</head><label>1112</label><figDesc>Receiver Operating Characteristic (ROC) and Precision-Recall (PR) curves for non-adversarial classifiers on the code clone detection task. Equal F1 score curves are shown on right. Adversarial AUROC and Average Precision for four models on the code clone detection task: a randomly initialized transformer, and transformers pre-trained on code with the RoBERTa MLM objective, our contrastive objective, or both. Representations learned by the contrastive model transfer robustly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 13 .</head><label>13</label><figDesc>Pre-training quickly converges if negative programs in the queue are frequently changed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 15</head><label>15</label><figDesc>. t-SNE<ref type="bibr" target="#b40">(Maaten &amp; Hinton, 2008)</ref> plot of mean pooled program representations learned with masked language modeling (RoBERTa), contrastive learning (ContraCode), and a hybrid loss (RoBERTa + ContraCode). Transformed variants of the same program share the same color. Note that colors may be similar across different programs.function x(url, callback, error) { var img = new Image(); img.src = url; if(img.complete){ return callback(img); } img.onload = function(){ img.onload = null;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Zero-shot code clone detection with cosine similarity probe. Contrastive and hybrid representations improve clone detection AUROC on unmodified (natural) HackerRank programs by +8% and +10% AUROC over a heuristic textual similarity probe, respectively, suggesting they are predictive of functionality. Contrastive representations are also the most robust to adversarial code transformations. RoBERTa MLM 79.39±0.70 81.47 37.81±0.24 51.42 10.09±0.50 32.52</figDesc><table><row><cell></cell><cell cols="2">Natural code</cell><cell cols="4">Adversarial (N =4) Adversarial (N =16)</cell></row><row><cell></cell><cell>AUROC</cell><cell>AP</cell><cell>AUROC</cell><cell>AP</cell><cell>AUROC</cell><cell>AP</cell></row><row><cell>Edit distance heuristic</cell><cell cols="5">69.55±0.81 73.75 31.63±0.82 42.85 12.11±0.54</cell><cell>32.46</cell></row><row><cell>Randomly initialized Transformer</cell><cell cols="4">72.31±0.79 75.82 22.72±0.20 37.73</cell><cell>3.09±0.28</cell><cell>30.95</cell></row><row><cell>+ RoBERTa MLM pre-train</cell><cell cols="4">74.04±0.77 77.65 25.83±0.21 39.46</cell><cell>4.51±0.33</cell><cell>31.17</cell></row><row><cell>+ ContraCode pre-train</cell><cell cols="5">75.73±0.75 78.02 64.97±0.24 66.23 58.32±0.88</cell><cell>59.66</cell></row><row><cell>+ ContraCode +</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>reports the area under the ROC curve (AUROC) and</cell></row><row><cell>average precision (AP, area under Precision-Recall). All</cell></row><row><cell>continuous representations improve clone detection over</cell></row><row><cell>the heuristic on natural code. Self-supervision through</cell></row><row><cell>RoBERTa MLM pre-training improves over a randomly</cell></row></table><note>initialized network by +1.7% AUROC. Contrastive pre- training achieves +3.4% AUROC over the same baseline. A hybrid objective combining both the contrastive loss and MLM has the best performance with +7.0% AUROC (+5.4% over MLM alone). This indicates that ContraCode learns a a more useful representation of functionality than MLM, though both objectives are useful for natural code.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Type inference accuracy on TypeScript programs in the<ref type="bibr" target="#b25">Hellendoorn et al. (2018)</ref> dataset. ContraCode (BiLSTM) outperforms baseline top-1 accuracies by 2.28% to 13.16%. As ContraCode does not modify model architecture, contrastive pretraining can be combined with each baseline. Compared with TypeScript's built-in type inference, we improve accuracy by 8.9%.</figDesc><table><row><cell>Baseline</cell><cell>Method</cell><cell cols="2">Acc@1 Acc@5</cell></row><row><cell>Static</cell><cell cols="2">TypeScript CheckJS (Bierman et al., 2014) 45.11%</cell><cell>-</cell></row><row><cell>analysis</cell><cell>Name only (Hellendoorn et al., 2018)</cell><cell cols="2">28.94% 70.07%</cell></row><row><cell>Transformer</cell><cell>Transformer (supervised) + ContraCode pre-train</cell><cell cols="2">45.66% 80.08% 46.86% 81.85%</cell></row><row><cell>RoBERTa-6</cell><cell>Transformer (RoBERTa MLM pre-train) + ContraCode pre-train (hybrid)</cell><cell cols="2">40.85% 75.76% 47.16% 81.44%</cell></row><row><cell></cell><cell>DeepTyper (supervised)</cell><cell cols="2">51.73% 82.71%</cell></row><row><cell>DeepTyper</cell><cell cols="3">+ RoBERTa MLM pre-train (10K steps) 50.24% 82.85%</cell></row><row><cell>(BiLSTM)</cell><cell>+ ContraCode pre-train</cell><cell cols="2">52.65% 84.60%</cell></row><row><cell></cell><cell>+ ContraCode pre-train (+ SW reg ft)</cell><cell cols="2">54.01% 85.55%</cell></row></table><note>6,902,642 total tokens. Validation and test sets are from held-out repositories. For additional supervision, missing types are inferred by static analysis to augment user-defined types as targets. All types are removed from model input. A 2-layer MLP head predicts types from output token embed- dings. We perform early stopping based on validation set top-1 accuracy. For our remaining experiments, the baseline RoBERTa models are pre-trained on the same augmented dataset as ContraCode for fair comparison.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>summarizes results. Contrastive pre-training out-</cell></row><row><cell>performs all baseline learned methods, showing meaningful</cell></row><row><cell>transfer. ContraCode can be applied in a drop-in fashion</cell></row><row><cell>to each of the baselines. Simply pre-training each baseline</cell></row><row><cell>with the contrastive objective and data augmentations yields</cell></row><row><cell>absolute accuracy improvements of +1.2%, +6.3%, +2.3%</cell></row><row><cell>top-1 and +1.8%, +5.7%, +2.8% top-5 over the Transformer,</cell></row><row><cell>RoBERTa, and DeepTyper, respectively.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 .</head><label>4</label><figDesc>Results for different settings of the code summarization task: supervised training with 81K functions, masked language model pre-training, training from scratch and contrastive pre-training with fine-tuning.</figDesc><table><row><cell>Method</cell><cell>Precision Recall</cell><cell>F1</cell></row><row><cell>code2vec (Alon et al., 2019b)</cell><cell cols="2">10.78% 8.24% 9.34%</cell></row><row><cell>code2seq</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 .</head><label>5</label><figDesc>On two tasks, compiler data augmentations degrade performance when training supervised models from scratch.</figDesc><table><row><cell>Method for code summarization</cell><cell>F1</cell></row><row><cell>Transformer (Table 4)</cell><cell>16.86</cell></row><row><cell>+ LS,SW,VR,DCI augmentations</cell><cell>15.65</cell></row><row><cell>Method for type inference</cell><cell>Acc@1</cell></row><row><cell>Transformer (Table 3)</cell><cell>45.66</cell></row><row><cell>+ SW regularization</cell><cell>43.96</cell></row><row><cell>+ LS,SW augmentations</cell><cell>44.14</cell></row><row><cell>DeepTyper (Table 3)</cell><cell>51.73</cell></row><row><cell>+ SW regularization</cell><cell>49.93</cell></row><row><cell>+ LS,SW augmentations</cell><cell>50.93</cell></row><row><cell>+ stronger LS,SW augmentations</cell><cell>50.33</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>shows results. Compiler-based data augmentations</cell></row><row><cell>degrade supervised models, perhaps by creating a training</cell></row><row><cell>distribution not reflective of evaluation programs. However,</cell></row><row><cell>as shown in  §4.1-4.3, augmenting during ContraCode pre-</cell></row><row><cell>training yields a more accurate model. Our contrastive</cell></row><row><cell>learning framework also allows learning over large numbers</cell></row><row><cell>of unlabeled programs that supervised learning alone cannot</cell></row><row><cell>leverage. The ablation indicates that augmentations do not</cell></row><row><cell>suffice, and contrastive learning is important.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 .</head><label>6</label><figDesc>Ablating compiler transformations used during contrastive pre-training. The DeepTyper BiLSTM is pre-trained with constrastive learning for 20K steps, then fine-tuned for type inference. Augmentations are only used during pre-training. Each transformation contributes to accuracy.</figDesc><table><row><cell>Augmentations used for pre-training</cell><cell>Acc@1 Acc@5</cell></row><row><cell>All augmentations (Table 3)</cell><cell>52.65% 84.60%</cell></row><row><cell cols="2">w/o identifier modification (-VR, -IM) 51.94% 84.43%</cell></row><row><cell>w/o line subsampling (-LS)</cell><cell>51.05% 81.63%</cell></row><row><cell cols="2">w/o code compression (-T,C,DCE,CF) 50.69% 81.95%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 .</head><label>7</label><figDesc>If local representations are learned, transferring part of the Contrastive MLP head improves type inference. The encoder is a 2-layer BiLSTM (d=512), with a 2-layer MLP head for both pre-training purposes and type inference. The mean hidden state representation is optimized for 10K iterations for the purposes of this ablation.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 .Table 9 .</head><label>89</label><figDesc>Contrasting global, sequence-level representations outperforms contrasting local representations. We compare using the terminal (global) hidden states of the DeepTyper BiLSTM and the mean pooled token-level (local) hidden states.GlobalInfoNCE with terminal hidden state, 20K steps 52.65% 84.60% InfoNCE with terminal hidden state, 10K steps 51.70% 83.03% Training time and decoder depth ablation on the method name prediction task. Longer pre-training significantly improves downstream performance when a shallow, 1 layer decoder is used.</figDesc><table><row><cell cols="2">Representation Optimization</cell><cell></cell><cell>Acc@1 Acc@5</cell></row><row><cell>Local</cell><cell cols="3">InfoNCE with mean token rep., 10K steps</cell><cell>49.32% 80.03%</cell></row><row><cell>Decoder</cell><cell cols="3">Pre-training (1.8M programs) (81k programs) Supervision Precision Recall</cell><cell>F1</cell></row><row><cell cols="2">Transformer, 1 layer MoCo, 10k steps</cell><cell>Original set</cell><cell>11.91% 5.96% 7.49%</cell></row><row><cell cols="2">Transformer, 1 layer MoCo, 45k steps</cell><cell>Original set</cell><cell>17.71% 12.57% 13.79%</cell></row><row><cell cols="2">Transformer, 4 layers MoCo, 45k steps</cell><cell>Original set</cell><cell>18.21% 13.21% 14.56%</cell></row><row><cell>Top 5 accuracy</cell><cell></cell><cell></cell></row><row><cell cols="2">1x queue fill rate</cell><cell></cell></row><row><cell cols="2">12x queue fill rate</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Koushik Sen, Jonathan Ho, Aravind Srinivas and Rishabh Singh. In addition to NSF CISE Expeditions Award CCF-1730628, the NSF GRFP under Grant No. DGE-1752814, and ONR PECASE N000141612723, this research is supported by gifts from Amazon Web Services, Ant Financial, Ericsson, Facebook, Futurewei, Google, Intel, Microsoft, NVIDIA, Scotiabank, Splunk and VMware.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Program transformation details</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The adverse effects of code duplication in machine learning models of code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Allamanis</surname></persName>
		</author>
		<idno type="DOI">10.1145/3359591.3359735</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software, Onward! 2019</title>
		<meeting>the 2019 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software, Onward! 2019<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="143" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A convolutional attention network for extreme summarization of source code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A survey of machine learning for big code and naturalness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Devanbu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">81</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Typilus: Neural type hints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ducousso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Programming Language Design and Implementation (PLDI)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">code2seq: Generating sequences from structured representations of code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yahav</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning distributed representations of code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zilberstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yahav</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the ACM on Programming Languages</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A theoretical analysis of contrastive unsupervised representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Khandeparkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khodak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Plevrakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Saunshi</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="http://proceedings.mlr.press/v97/saunshi19a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<editor>Chaudhuri, K. and Salakhutdinov, R.</editor>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019-06-15" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="5628" to="5637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A program for identifying duplicated code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computing Science and Statistics</title>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Automatic generation of peephole superoptimizers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aiken</surname></persName>
		</author>
		<idno>1595934510. doi: 10.1145/ 1168857.1168906</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS XII</title>
		<meeting>the 12th International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS XII<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="394" to="403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural code comprehension: A learnable representation of code semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ben-Nun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Jakobovits</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hoefler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Defexts: A curated dataset of reproducible real-world bugs for modern jvm languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Benton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghanbari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Companion Proceedings (ICSE-Companion)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="47" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Adversarial robustness for code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bielik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vechev</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Understanding typescript</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bierman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Torgersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Object-Oriented Programming (ECOOP)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Signature verification using a&quot; siamese&quot; time delay neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bromley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Säckinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="737" to="744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Macnine Learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<title level="m">Improved baselines with momentum contrastive learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yen-Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Debiased contrastive learning</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Case studies in the use of roc curve analysis for sensor-based estimates in human computer interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fogarty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Hudson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Graphics Interface 2005, GI &apos;05</title>
		<meeting>Graphics Interface 2005, GI &apos;05<address><addrLine>Waterloo, CAN</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
	<note>ISBN 1568812655</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07728</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Giorgi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Nitski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Bader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>De-Clutr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.03659</idno>
		<title level="m">Deep contrastive learning for unsupervised textual representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Svyatkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tufano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Clement</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Drain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sundaresan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Graphcodebert</surname></persName>
		</author>
		<title level="m">Pre-training code representations with data flow</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Noise-contrastive estimation: A new estimation principle for unnormalized statistical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="297" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;06)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Visualizing and understanding the effectiveness of bert</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4134" to="4143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05722</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep learning type inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">J</forename><surname>Hellendoorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allamanis</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering</title>
		<meeting>the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="152" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Data-efficient image recognition with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">J</forename><surname>Hénaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Fauw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Oord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>Scaling laws for transfer</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Universal language model finetuning for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="328" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Bidirectional lstm-crf models for sequence tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01991</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">CodeSearchNet challenge: Evaluating the state of semantic code search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Husain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gazit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.09436</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep transfer learning for source code modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<idno type="DOI">10.1142/s0218194020500230</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Software Engineering and Knowledge Engineering</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">05</biblScope>
			<biblScope unit="page" from="649" to="668" />
			<date type="published" when="2020-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Adversarial examples are not bugs, they are features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Wallach, H., Larochelle, H., Beygelzimer, A., d&apos;Alché-Buc, F., Fox, E., and Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="125" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Summarizing source code using a neural attention model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2073" to="2083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Denali: A goaldirected superoptimizer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Randall</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
		<idno type="DOI">10.1145/512529.512566</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGPLAN 2002 Conference on Programming Language Design and Implementation, PLDI &apos;02</title>
		<meeting>the ACM SIGPLAN 2002 Conference on Programming Language Design and Implementation, PLDI &apos;02<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="304" to="314" />
		</imprint>
	</monogr>
	<note>Association for Computing Machinery</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Pre-trained contextual embedding of source code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Maniatis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shi</surname></persName>
		</author>
		<idno>abs/2001.00059</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R.-M</forename><surname>Karampatsis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scelmo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.13214</idno>
		<title level="m">Source code embeddings from language models</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A field study of refactoring challenges and benefits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nagappan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering</title>
		<meeting>the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Subword regularization: Improving neural network translation models with multiple subword candidates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kudo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="66" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bharambe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="181" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A look at the smallest program</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Massalin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Superoptimizer</surname></persName>
		</author>
		<idno type="DOI">10.1145/36206.36194</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second International Conference on Architectual Support for Programming Languages and Operating Systems, ASPLOS II</title>
		<meeting>the Second International Conference on Architectual Support for Programming Languages and Operating Systems, ASPLOS II<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="1987" />
			<biblScope unit="page" from="122" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Babel: compiler for writing next generation javascript</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mckenzie</surname></persName>
		</author>
		<ptr target="https://github.com/babel/babel" />
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Ithemal: Accurate, portable and fast basic block throughput estimation using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mendis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Renda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amarasinghe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Carbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4505" to="4515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Natural language models for predicting programming comments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Movshovitz-Attias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="35" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">V</forename><surname>Pandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Opttyper</surname></persName>
		</author>
		<title level="m">Probabilistic type inference by optimising logical and natural constraints</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2536" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Deepbugs: A learning approach to name-based bug detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pradel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the ACM on Programming Languages</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">OOPSLA</biblScope>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pradel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gousios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Typewriter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.03768</idno>
		<title level="m">Neural type prediction with search-based validation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Evaluation of generalizability of neural program analyzers under semanticpreserving transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R I</forename><surname>Rabin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Alipour</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ArXiv</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ArXiv</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Classes of recursively enumerable sets and their decision problems. Transactions of the</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">G</forename><surname>Rice</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1953" />
			<publisher>American Mathematical Society</publisher>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="358" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Terser: Javascript parser, mangler and compressor toolkit for es6+</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Santos</surname></persName>
		</author>
		<ptr target="https://github.com/terser/terser" />
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Paliwal</surname></persName>
		</author>
		<idno type="DOI">10.1109/78.650093</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
	<note>Signal Processing</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.07909</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Improving neural machine translation models with monolingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
		<idno>doi: 10.18653/ v1/P16-1009</idno>
		<ptr target="https://www.aclweb.org/anthology/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-08" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="16" to="1009" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Trabucco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.12345</idno>
		<title level="m">Synthetic datasets for neural program synthesis</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Towards a big data curated benchmark of inter-project code clones</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Svajlenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Keivanloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Mia</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICSME.2014.77</idno>
		<ptr target="https://doi.org/10.1109/ICSME.2014.77" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 IEEE International Conference on Software Maintenance and Evolution, ICSME &apos;14</title>
		<meeting>the 2014 IEEE International Conference on Software Maintenance and Evolution, ICSME &apos;14<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="476" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Cloze procedure&quot;: A new tool for measuring readability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journalism Quarterly</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="415" to="433" />
			<date type="published" when="1953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Contrastive multiview coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Christodorescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Coset</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11445</idno>
		<title level="m">A benchmark for evaluating neural program embeddings</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Learning blended, precise semantic program embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ArXiv</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Probabilistic type inference using graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dillig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lambdanet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Deep learning code fragments for code clone detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tufano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vendome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Poshyvanyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">31st IEEE/ACM International Conference on Automated Software Engineering (ASE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="87" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">On mutual information in contrastive learning for visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yamins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goodman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Adversarial examples for models of code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yefet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yahav</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1910.07517</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="649" to="666" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
