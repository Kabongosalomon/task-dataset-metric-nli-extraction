<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fine-grained Sentiment Classification using BERT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manish</forename><surname>Munikar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Electronics and Computer Engineering Pulchowk Campus</orgName>
								<orgName type="department" key="dep2">Institute of Engineering</orgName>
								<orgName type="institution">Tribhuvan University Lalitpur</orgName>
								<address>
									<country key="NP">Nepal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sushil</forename><surname>Shakya</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Electronics and Computer Engineering Pulchowk Campus</orgName>
								<orgName type="department" key="dep2">Institute of Engineering</orgName>
								<orgName type="institution">Tribhuvan University Lalitpur</orgName>
								<address>
									<country key="NP">Nepal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aakash</forename><surname>Shrestha</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Electronics and Computer Engineering Pulchowk Campus</orgName>
								<orgName type="department" key="dep2">Institute of Engineering</orgName>
								<orgName type="institution">Tribhuvan University Lalitpur</orgName>
								<address>
									<country key="NP">Nepal</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fine-grained Sentiment Classification using BERT</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-sentiment classification</term>
					<term>natural language pro- cessing</term>
					<term>language model</term>
					<term>pretraining</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Sentiment classification is an important process in understanding people's perception towards a product, service, or topic. Many natural language processing models have been proposed to solve the sentiment classification problem. However, most of them have focused on binary sentiment classification. In this paper, we use a promising deep learning model called BERT to solve the fine-grained sentiment classification task. Experiments show that our model outperforms other popular models for this task without sophisticated architecture. We also demonstrate the effectiveness of transfer learning in natural language processing in the process.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Sentiment classification is a form of text classification in which a piece of text has to be classified into one of the predefined sentiment classes. It is a supervised machine learning problem. In binary sentiment classification, the possible classes are positive and negative. In fine-grained sentiment classification, there are five classes (very negative, negative, neutral, positive, and very positive). Sentiment classification model, like any other machine learning model, requires its input to be a fixed-sized vector of numbers. Therefore, we need to convert a text-sequence of words represented as ASCII or Unicode-into a fixedsized vector that encodes the meaningful information of the text. Many statistical and deep learning NLP models have been proposed just for that. Recently, there has been an explosion of developments in NLP as well as other deep learning architectures.</p><p>While transfer learning (pretraining and finetuning) has become the de-facto standard in computer vision, NLP is yet to utilize this concept fully. However, neural language models such as word vectors <ref type="bibr" target="#b0">[1]</ref>, paragraph vectors <ref type="bibr" target="#b1">[2]</ref>, and GloVe <ref type="bibr" target="#b2">[3]</ref> have started the transfer learning revolution in NLP. Recently, Google researchers published BERT (Bidirectional Encoder Representations from Transformers) <ref type="bibr" target="#b3">[4]</ref>, a deep bidirectional language model based on the Transformer architecture <ref type="bibr" target="#b4">[5]</ref>, and advanced the state-of-the-art in many popular NLP tasks. In this paper, we use the pretrained BERT model and finetune it for the fine-grained sentiment classification task on the Stanford Sentiment Treebank (SST) dataset.</p><p>The rest of the paper is organized into six sections. In Section II, we mention our motivation for this work. In Section III, we discuss related works. In Section IV, we describe the dataset we performed our experiments on. We explain our model architecture and methodology in detail in Section V. Then we present and analyze our results in Section VI. Finally, we provide our concluding remarks in Section VII.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. MOTIVATION</head><p>We have been working on replicating the different research paper results for sentiment analysis, especially on the finegrained Stanford Sentiment Treebank (SST) dataset. After the popularity of BERT, researchers have tried to use it on different NLP tasks, including binary sentiment classification on SST-2 (binary) dataset, and they were able to obtain state-of-the-art results as well. But we haven't yet found any experimentation done using BERT on the SST-5 (fine-grained) dataset. Because BERT is so powerful, fast, and easy to use for downstream tasks, it is likely to give promising results in SST-5 dataset as well. This became the main motivation for pursuing this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. RELATED WORK</head><p>Sentiment classification is one of the most popular tasks in NLP, and so there has been a lot of research and progress in solving this task accurately. Most of the approaches have focused on binary sentiment classification, most probably because there are large public datasets for it such as the IMDb movie review dataset <ref type="bibr" target="#b5">[6]</ref>. In this section, we only discuss some significant deep learning NLP approaches applied to sentiment classification.</p><p>The first step in sentiment classification of a text is the embedding, where a text is converted into a fixed-size vector. Since the number of words in the vocabulary after tokenization and stemming is limited, researchers first tackled the problem of learning word embeddings. The first promising language model was proposed by Mikolov et al. <ref type="bibr" target="#b0">[1]</ref>. They trained continuous semantic representation of words from large unlabeled text that could be fine-tuned for downstream tasks. Pennington et al. <ref type="bibr" target="#b2">[3]</ref> used a co-occurrence matrix and only trained on nonzero elements to efficiently learn semantic word embeddings. Bojanowski et al. <ref type="bibr" target="#b6">[7]</ref> broke words into character n-grams for smaller vocabulary size and fast training.</p><p>The next step is to combine a variable number of word vectors into a single fixed-size document vector. The trivial way is to take the sum or the average, but they don't lose the ordering information of words and thus don't give good results. Tai et al. <ref type="bibr" target="#b7">[8]</ref> used recursive neural networks to compute vector representation of sentences by utilizing the intrinsic tree structure of natural language sentences. Socher et al. <ref type="bibr" target="#b8">[9]</ref> introduced a tensor-based compositionaity function for better interaction between child nodes in recursive networks. They also introduced the Stanford Sentiment Treebank (SST) dataset for fine-grained sentiment classification. Tai et al. <ref type="bibr" target="#b9">[10]</ref> applied various forms of long short-term memory (LSTM) networks and Kim <ref type="bibr" target="#b10">[11]</ref> applied convolutional neural networks (CNN) towards sentiment classification.</p><p>All of the approaches mentioned above are context-free, i.e., they generate single word embedding for each word in the vocabulary. For instance, "bank" would have the same representation in "bank deposit" and "river bank". Recent language model research has been trying to train contextual embeddings. Peters et al. <ref type="bibr" target="#b11">[12]</ref> extracted context-sensitive features from leftto-right and right-to-left LSTM-based language model. Devlin et al. <ref type="bibr" target="#b3">[4]</ref> proposed BERT (Bidirectional Encoder Representations from Transformers), an attention-based Transformer architecture <ref type="bibr" target="#b4">[5]</ref>, to train deep bidirectional representations from unlabeled texts. Their architecture not only obtains stateof-the-art results on many NLP tasks but also allows a high degree of parallelism since it is not based on sequential or recurrent connections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. DATASET</head><p>Stanford Sentiment Treebank (SST) <ref type="bibr" target="#b8">[9]</ref> is one of the most popular publicly available datasets for fine-grained sentiment classification task. It contains 11,855 one-sentence movie reviews extracted from Rotten Tomatoes. Not only that, each sentence is also parsed by the Stanford constituency parser <ref type="bibr" target="#b12">[13]</ref> into a tree structure with the whole sentence as the root node and the individual words as leaf nodes. Moreover, each node is labeled by at least three humans. In total, SST contains 215,154 unique manually labeled texts of varying lengths. <ref type="figure" target="#fig_1">Fig  2 shows</ref> a sample review from the SST dataset in a parsetree structure with all its nodes labeled. Therefore, this dataset can be used to train models to learn the sentiment of words, phrases, and sentences together.</p><p>There are five sentiment labels in SST: 0 (very negative), 1 (negative), 2 (neutral), 3 (positive), and 4 (very positive). If we only consider positivity and negativity, we get the binary SST-2 dataset. If we consider all five labels, we get SST-5. For this research, we evaluate the performance of various models on all nodes as well as on just the root nodes, and on both SST-2 and SST-5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. METHODOLOGY</head><p>Sentiment classification takes a natural language text as input and outputs a sentiment score ∈ {0, 1, 2, 3, 4}. Our method has three stages from input sentence to output score, which are described below. We use pretrained BERT model to build a sentiment classifier. Therefore, in this section, we briefly explain BERT and then describe our model architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. BERT</head><p>BERT (Bidirectional Encoder Representations from Transformers is an embedding layer designed to train deep bidirectional representations from unlabeled texts by jointly conditioning on both left and right context in all layers. It is pretrained from a large unsupervised text corpus (such as Wikipedia dump or BookCorpus) using the following objectives:</p><p>• Masked word prediction: In this task, 15% of the words in the input sequence are masked out, the entire sequence is fed to a deep bidirectional Transfomer <ref type="bibr" target="#b4">[5]</ref> encoder, and then the model learns to predict the masked words. • Next sentence prediction: To learn the relationship between sentences, BERT takes two sentences A and B as inputs and learns to classify whether B actually follows A or is it just a random sentence. Unlike traditional sequential or recurrent models, the attention architecture processes the whole input sequence at once, enabling all input tokens to be processed in parallel. The layers of BERT architecture are visualized in <ref type="figure" target="#fig_2">Fig 3.</ref> Pretrained BERT model can be fine-tuned with just one additional layer to obtain state-of-the-art results in a wide range of NLP tasks <ref type="bibr" target="#b3">[4]</ref>.</p><p>There are two variants for BERT models: BERT BASE and BERT LARGE . The difference between them is listed in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>OpenAI GPT Lstm Lstm</head><p>Lstm Lstm  sequence embedding that can be used for classifying the whole sequence.</p><formula xml:id="formula_0">T 1 T 2 T N ... E 1 E 2 E N ... T 1 T 2 T N ... E 1 E 2 E N ...</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Preprocessing</head><p>We perform the following preprocessing steps on the review text before we feed them into out model. 1) Canonicalization: First, we remove all the digits, punctuation symbols and accent marks, and convert everything to lowercase.</p><p>2) Tokenization: We then tokenize the text using the Word-Piece tokenizer <ref type="bibr" target="#b13">[14]</ref>. It breaks the words down to their prefix, root, and suffix to handle unseen words better. For example, playing → play + ##ing.</p><p>3) Special token addition: Finally, we add the [CLS] and [SEP] tokens at the appropriate positions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Proposed Architecture</head><p>We build a simple architecture with just a dropout regularization <ref type="bibr" target="#b14">[15]</ref> and a softmax classifier layers on top of pretrained BERT layer to demonstrate that BERT can produce great results even without any sophisticated task-specific architecture. <ref type="figure" target="#fig_3">Fig 4 shows</ref> the overall architecture of our model. There are four main stages. The first is the proprocessing step as described earlier. Then we compute the sequence embedding from BERT. We then apply dropout with a probability factor of 0.1 to regularize and prevent overfitting. Dropout is only applied in training phase and not in inference phase. Finally, the softmax classification layer will output the probabilities of the input text belonging to each of the class labels such that the sum of the probabilities is 1. The softmax layer is just a fully connected neural network layer with the softmax activation function. The softmax function σ : R K → R K is given in <ref type="bibr" target="#b0">(1)</ref>.</p><formula xml:id="formula_1">σ(z) i = e zi K j=1 e zj for i = 1, . . . , K<label>(1)</label></formula><p>where z = (z 1 , . . . , z K ) ∈ R K is the intermediate output of the softmax layer (also called logits). The output node with the highest probability is then chosen as the predicted label for the input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXPERIMENTS AND RESULTS</head><p>In this section, we discuss the results of our model and compare with it some of the popular models that solve the same problem, i.e., sentiment classification on the SST dataset.</p><p>A. Comparison Models 1) Word embeddings: In this method, the word vectors pretrained on large text corpus such as Wikipedia dump are averaged to get the document vector, which is then fed to the sentiment classifier to compute the sentiment score.</p><p>2) Recursive networks: Various types of recursive neural networks (RNN) have been applied on SST <ref type="bibr" target="#b8">[9]</ref>. We compare our results with the standard RNN and the more sophisticated RNTN. Both of them were trained on SST from scratch, without pretraining.</p><p>3) Recurrent networks: Sophisticated recurrent networks such as left-to-right and bidrectional LSTM networks have also been applied on SST <ref type="bibr" target="#b9">[10]</ref>. 4) Convolutional networks: In this approach, the input sequences were passed through a 1-dimensional convolutional neural network as feature extractors <ref type="bibr" target="#b10">[11]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation Metric</head><p>Since the dataset has roughly balanced number of samples of all classes, we directly use the accuracy measure to evaluate the performance of our model and compare it with other models. The accuracy is defined simply as follows: accuracy = number of correct predictions total number of predictions ∈ [0, 1] (2)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Results</head><p>The result and comparisons are shown in <ref type="table" target="#tab_0">Table II</ref>. It shows the accuracy of various models on SST-2 and SST-5. It includes results for all phrases as well as for just the root (whole review). We can see that our model, despite being a simple architecture, performs better in terms of accuracy than many popular and sophisticated NLP models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>In this paper, we used the pretrained BERT model and finetuned it for the fine-grained sentiment classification task on the SST dataset. Even with such a simple downstream architecture, our model was able to outperform complicated architectures like recursive, recurrent, and convolutional neural networks. Thus, we have demonstrated the transfer learning capability in NLP enabled by deep contextual language models like BERT.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig 1 shows a black-box view of a fine-grained sentiment classifier model. High-level black-box view of a sentiment classifier showing its input and output.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>A sample sentence from the SST dataset. (Source: Adapted from<ref type="bibr" target="#b8">[9]</ref>.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>BERT Architecture, where En is the n-th token in the input sequence, Trm is the Transformer block, and Tn is the corresponding output embedding. (Source: Adapted from<ref type="bibr" target="#b3">[4]</ref>.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Proposed architecture for fine-grained sentiment classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table I .</head><label>I</label><figDesc>BERT requires its input token sequence to have a certain format. First token of every sequence should be [CLS] (classification token) and there should be a [SEP] token (separation token) after every sentence. The output embedding corresponding to the [CLS] token is the</figDesc><table><row><cell>Trm</cell><cell>Trm</cell><cell>...</cell><cell>Trm</cell><cell>Trm</cell><cell>Trm</cell><cell>...</cell><cell>Trm</cell></row><row><cell>Trm</cell><cell>Trm</cell><cell>...</cell><cell>Trm</cell><cell>Trm</cell><cell>Trm</cell><cell>...</cell><cell>Trm</cell></row></table><note>1) Input format:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I BERT</head><label>I</label><figDesc>BASE VS. BERT LARGE .</figDesc><table><row><cell></cell><cell>BERT BASE</cell><cell>BERT LARGE</cell></row><row><cell>No. of layers (Transformer blocks)</cell><cell>12</cell><cell>24</cell></row><row><cell>No. of hidden units</cell><cell>768</cell><cell>1024</cell></row><row><cell>No. of self-attention heads</cell><cell>12</cell><cell>16</cell></row><row><cell>Total trainable parameters</cell><cell>110M</cell><cell>340M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II</head><label>II</label><figDesc></figDesc><table><row><cell cols="5">ACCURACY (%) OF OUR MODELS ON SST DATASET COMPARED TO</cell></row><row><cell></cell><cell>OTHER MODELS. 1</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="2">SST-2</cell><cell cols="2">SST-5</cell></row><row><cell></cell><cell>All</cell><cell>Root</cell><cell>All</cell><cell>Root</cell></row><row><cell>Avg word vectors [9]</cell><cell>85.1</cell><cell>80.1</cell><cell>73.3</cell><cell>32.7</cell></row><row><cell>RNN [8]</cell><cell>86.1</cell><cell>82.4</cell><cell>79.0</cell><cell>43.2</cell></row><row><cell>RNTN [9]</cell><cell>87.6</cell><cell>85.4</cell><cell>80.7</cell><cell>45.7</cell></row><row><cell>Paragraph vectors [2]</cell><cell>-</cell><cell>87.8</cell><cell>-</cell><cell>48.7</cell></row><row><cell>LSTM [10]</cell><cell>-</cell><cell>84.9</cell><cell>-</cell><cell>46.4</cell></row><row><cell>BiLSTM [10]</cell><cell>-</cell><cell>87.5</cell><cell>-</cell><cell>49.1</cell></row><row><cell>CNN [11]</cell><cell>-</cell><cell>87.2</cell><cell>-</cell><cell>48.0</cell></row><row><cell>BERT BASE</cell><cell>94.0</cell><cell>91.2</cell><cell>83.9</cell><cell>53.2</cell></row><row><cell>BERT LARGE</cell><cell>94.7</cell><cell>93.1</cell><cell>84.2</cell><cell>55.5</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Some values are blank in "All" columns because the original authors of those paper did not publish their result on all phrases.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>We would like to express our gratitude towards Prof. Dr. Shashidhar Ram Joshi for his invaluable advice and guidance on this paper. We also thank all the helpers and reviewers for their valuable input to this work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1301.3781</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2011-06" />
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="135" to="146" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semi-supervised recursive autoencoders for predicting sentiment distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="151" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1556" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5882</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05365</idno>
		<title level="m">Deep contextualized word representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Japanese and korean voice search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nakajima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="5149" to="5152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
