<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On Deep Learning Techniques to Boost Monocular Depth Estimation for Autonomous Navigation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raul</forename><surname>De Queiroz Mendes</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer and Electrical Engineering</orgName>
								<orgName type="institution" key="instit1">São Carlos School of Engineering</orgName>
								<orgName type="institution" key="instit2">University of São Paulo</orgName>
								<address>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduardo</forename><forename type="middle">Godinho</forename><surname>Ribeiro</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer and Electrical Engineering</orgName>
								<orgName type="institution" key="instit1">São Carlos School of Engineering</orgName>
								<orgName type="institution" key="instit2">University of São Paulo</orgName>
								<address>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Dos</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer and Electrical Engineering</orgName>
								<orgName type="institution" key="instit1">São Carlos School of Engineering</orgName>
								<orgName type="institution" key="instit2">University of São Paulo</orgName>
								<address>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santos</forename><surname>Rosa</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer and Electrical Engineering</orgName>
								<orgName type="institution" key="instit1">São Carlos School of Engineering</orgName>
								<orgName type="institution" key="instit2">University of São Paulo</orgName>
								<address>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valdir</forename><surname>Grassi</surname><genName>Jr</genName></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer and Electrical Engineering</orgName>
								<orgName type="institution" key="instit1">São Carlos School of Engineering</orgName>
								<orgName type="institution" key="instit2">University of São Paulo</orgName>
								<address>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">On Deep Learning Techniques to Boost Monocular Depth Estimation for Autonomous Navigation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>SIDE</term>
					<term>CNN</term>
					<term>Deep Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Inferring the depth of images is a fundamental inverse problem within the field of Computer Vision since depth information is obtained through 2D images, which can be generated from infinite possibilities of observed real scenes. Benefiting from the progress of Convolutional Neural Networks (CNNs) to explore structural features and spatial image information, Single Image Depth Estimation (SIDE) is often highlighted in scopes of scientific and technological innovation, as this concept provides advantages related to its low implementation cost and robustness to environmental conditions. In the context of autonomous vehicles, state-of-the-art CNNs optimize the SIDE task by producing high-quality depth maps, which are essential during the autonomous navigation process in different locations. However, such networks are usually supervised by sparse and noisy depth data, from Light Detection and Ranging (LiDAR) laser scans, and are carried out at high computational cost, requiring high-performance Graphic Processing Units (GPUs). Therefore, we propose a new lightweight and fast supervised CNN architecture combined with novel feature extraction models which are designed for real-world autonomous navigation. We also introduce an efficient surface normals module, jointly with a simple geometric 2.5D loss function, to solve SIDE problems. We also innovate by incorporating multiple Deep Learning techniques, such as the use of densification algorithms and additional semantic, surface normals and depth information to train our framework. The method introduced in this work focuses on robotic applications in indoor and outdoor environments and its results are evaluated on the competitive and publicly available NYU Depth V2 and KITTI Depth datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Autonomous navigation technologies are increasingly present in the academy, industries, agriculture and, to a lesser extent, on the streets and homes to aid in everyday tasks. However, many challenges still need to be solved to make autonomous vehicles a reality for the population. One of these challenges involves the improvement of perception mechanisms that integrate the robotic platforms of the autonomous system. These mechanisms are responsible for mapping the environment in which the vehicle is inserted, helping it to comprehend the surrounding 3D space <ref type="bibr" target="#b0">[1]</ref>.</p><p>In order to understand the 3D space, the perception system must be able to estimate the distance in which the objects are positioned in the scene. From this, the autonomous vehicle may avoid obstacles and perform safer traffic on navigable surfaces of different scenarios such as cities and roads. Therefore, it is essential that robotic platforms have the ability to explore depth cues of the environment, which can also benefit other Computer Vision tasks such as plane estimation <ref type="bibr" target="#b1">[2]</ref>, occluding contours estimation <ref type="bibr" target="#b2">[3]</ref>, object detection <ref type="bibr" target="#b3">[4]</ref>, Visual Odometry (VO) <ref type="bibr" target="#b4">[5]</ref> and Simultaneous Localization and Mapping (SLAM) <ref type="bibr" target="#b5">[6]</ref>.</p><p>RGBD sensors, Light Detection and Ranging (LiDAR), stereo cameras, RADAR and SONAR are widely commercialized technologies for depth sensing. However, there are limitations regarding the use of these technologies for depth perception applications. LiDAR sensors are costprohibitive and perform sparse and noisy depth measurements over long distances, Kinect sensors are sensitive to light and have a lower distance measurement limit when compared to LiDAR, stereo cameras fail in reflective and low texture regions, whereas RADAR and SONAR are generally used as auxiliaries.</p><p>Thus, the use of monocular cameras, in the context of robotic perception, becomes advantageous, once they demand a smaller installation space, are low cost, capture RGB images, are robust to environmental conditions, are efficient in terms of energy consumption and are a widespread technology <ref type="bibr" target="#b6">[7]</ref>. built with structured-light and LiDAR sensors, motivated the emergence of new works in the Single Image Depth Estimation (SIDE) area. However, state-of-the-art SIDE approaches use sparse and noisy ground truth to train networks and do not fully benefit from how depth, semantics and surface normals can be added to improve the performance of the method regarding speed and accuracy. Moreover, deep models applied to the SIDE task still present significant estimation errors which indicate that the area is open to new solutions.</p><p>In this work, we aim to introduce a Fully Convolutional Network (FCN), with an encoder-decoder structure, to tackle single-view depth estimation problems in robotic applications. For the training of this network, we centered on developing a simple 2.5D loss function that focuses on geometric cues of the image. Such a loss is used in conjunction with a novel module that estimates surface normals based on the depth prediction step.</p><p>At the same time, this work also shows how some Deep Learning methods may be applied to improve the results of our framework with respect to the quality and accuracy of the predictions. Among such methods, we employ: depth completion <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>, semantic segmentation <ref type="bibr" target="#b9">[10]</ref>, surface normals estimation <ref type="bibr" target="#b10">[11]</ref>, Atrous Spatial Pyramid Pooling (ASPP) <ref type="bibr" target="#b11">[12]</ref>, skip-connections <ref type="bibr" target="#b12">[13]</ref>, multi-scale models <ref type="bibr" target="#b13">[14]</ref> and multi-layered deconvolutions <ref type="bibr" target="#b14">[15]</ref>.</p><p>The developed pipeline is tested and evaluated indoors using the NYU Depth V2 dataset <ref type="bibr" target="#b15">[16]</ref> and outdoors using the KITTI Depth dataset <ref type="bibr" target="#b16">[17]</ref>. Moreover, our work provides comprehensive surveys over the monocular depth estimation and depth completion areas and we compare the proposed method with other approaches in the SIDE literature with the aid of metrics that are widely disseminated.</p><p>The main contributions of this paper are:</p><p>• Extensive and detailed surveys on monocular depth estimation and depth completion; • A simple CNN architecture, along with four feature extraction models, that can be applied for both SIDE and correlated tasks at a high frame rate; • A lightweight module that is capable of estimating surface normals with state-of-the-art accuracy; • A practical 2.5D loss function that is employed in the depth and surface normals experiments; • Ablation studies considering variations in loss functions, network structures, fine-tuning techniques and additional training information; • Application of different indoor and outdoor datasets and evaluation on the publicly available and competitive KITTI Depth <ref type="bibr" target="#b16">[17]</ref> and NYU Depth V2 <ref type="bibr" target="#b17">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Depth Estimation: A Brief Survey</head><p>There is a considerable amount of classic methods in the literature that tackle monocular depth inference problems <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref>. However, with the development of more complex and efficient deep CNNs <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref>, several recent works started to exploit such networks, besides graphic models, to extract dense feature maps from RGB images in SIDE applications.</p><p>For the presented task, typical CNN models have a fully convolutional architecture with an encoder-decoder structure. The encoder extracts feature maps from the input and the decoder retrieves full resolution images. The training process is usually performed by minimizing a regression loss function such as the Mean Squared Error (MSE) <ref type="bibr" target="#b25">[26]</ref> and the Mean Absolute Error (MAE) <ref type="bibr" target="#b26">[27]</ref>. Regarding the type of learning, SIDE models can be trained in three different manners: supervised, semi-supervised and self-supervised.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Supervised Learning</head><p>This type of SIDE approach uses sparse and noisy reference depth maps, constructed through point clouds from laser scans, to train supervised deep CNNs.</p><p>Eigen et al. <ref type="bibr" target="#b27">[28]</ref> presented a fundamental work in which the developed architecture is composed of two stacks. The first stack estimates the scene depth from a global point of view and the second stack performs local refinements. The authors also introduced a scale-invariant loss function (SILog) that highlights the depth relationships of the image pixels, instead of focusing on the general scale.</p><p>With another fundamental work, Laina et al. <ref type="bibr" target="#b28">[29]</ref> introduced an FCN composed of up-projection and residual learning mechanisms <ref type="bibr" target="#b29">[30]</ref> to predict accurate depth maps in a faster way. The authors also presented the benefits of using the Huber reverse loss function (BerHu) <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref> to train SIDE networks.</p><p>Exploring the benefits of graphic models, Liu et al. <ref type="bibr" target="#b32">[33]</ref> combined a CNN with a continuous Conditional Random Field (CRF) and proposed a new superpixel pooling method that addresses the problem of information loss after successive downsampling operations. Cao et al. <ref type="bibr" target="#b33">[34]</ref> used classification methods, as opposed to common regression techniques, through the discretization of the ground truth (depth bins). Their results show that SIDE by classification can surpass the conventional regression. However, the accuracy depends on the number of depth bins since the loss function is not able to differentiate depth values within a bin.</p><p>Inspired by this idea, Liao et al. <ref type="bibr" target="#b34">[35]</ref> employed a regression loss combined with a classification function to improve accuracy. The authors also introduced a CNN that outputs depth maps from RGB images concatenated with reference depth data from 2D laser scans.</p><p>Using different approaches, Xu et al. <ref type="bibr" target="#b13">[14]</ref> proposed a network structure that integrates a CNN with a fusion module composed of continuous CRFs. Fu et al. <ref type="bibr" target="#b25">[26]</ref> developed an ordinal regression method based on a spacingincreasing discretization technique. Exploiting transfer learning strategies, Alhashim et al. <ref type="bibr" target="#b14">[15]</ref> employed the DenseNet-169 <ref type="bibr" target="#b21">[22]</ref> model pretrained on the ILSVRC <ref type="bibr" target="#b35">[36]</ref> in a new FCN architecture.</p><p>Recently, Lee et al. <ref type="bibr" target="#b12">[13]</ref> introduced a deep network architecture that applies the DenseNet-161 <ref type="bibr" target="#b21">[22]</ref> and the ResNext-101 <ref type="bibr" target="#b23">[24]</ref> as encoder. Coupled with the encoder structure, the authors used a dense multi-scale feature learning module (Dense ASPP), followed by a decoder with convolution and upconvolution blocks, downsampling and concatenation operations and Local Planar Guidance layers (LPG).</p><p>Exploring temporal features, Mancini et al. <ref type="bibr" target="#b0">[1]</ref> introduced Fully Connected Long Short-Term Memory (FC-LSTM) <ref type="bibr" target="#b36">[37]</ref> layers in a CNN to address the monocular depth prediction task. However, to use image sequences as input to an FC-LSTM, they need to be transformed into 1D vectors, which impairs the network's learning of spatial relationships. Conv-LSTM <ref type="bibr" target="#b37">[38]</ref>, on the other hand, is a type of neural layer that allows the extraction of spatial and temporal features. This layer is employed in the works of Kumar et al. <ref type="bibr" target="#b38">[39]</ref> and Wang et al. <ref type="bibr" target="#b39">[40]</ref>.</p><p>Applying attention-based techniques, which are adaptive and allow only features that have meaningful information to be focused on during training, Xu et al. <ref type="bibr" target="#b40">[41]</ref> proposed a multi-scale CNN composed of an attention-guided CRF module to perform single-view depth prediction. Following a resembling idea, Chen et al. <ref type="bibr" target="#b41">[42]</ref> introduced an attention-based context aggregation CNN that aggregates contextual information at pixel and global levels from the feature maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1.">SIDE and Semantic Segmentation</head><p>Semantic segmentation and monocular depth estimation are fundamental and correlated tasks in Computer Vision, since, from them, it is possible to infer scene geometry, artifacts scale and location, the scene's structure and the distances at which objects are located <ref type="bibr" target="#b42">[43]</ref>. Moreover, the aforementioned tasks benefit mutually, as understanding the depth of the scene helps semantic segmentation during categorization, especially for the case when there are different artifacts with similar structures and at different depths <ref type="bibr" target="#b43">[44]</ref>. On the other hand, semantic labels provide geometric and perspective information that assists the depth estimation of each object in the scene, improving mainly the predictions at high gradient regions (borders).</p><p>Classic approaches already employ methods addressing semantic segmentation to estimate depth <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46]</ref> and RGBD images to obtain precise semantic classes <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b46">47]</ref>. Using CNNs, Eigen et al. <ref type="bibr" target="#b47">[48]</ref> introduced a deep multiscale network, which, with minor modifications, can predict depth, surface normals and semantic maps. Benefiting from graphic models, Mousavian et al. <ref type="bibr" target="#b48">[49]</ref> developed a jointly CNN and CRF to estimate depth and semantics. Wang et al. <ref type="bibr" target="#b43">[44]</ref> also tackled SIDE and semantics simultaneously using two CNNs and a Hierarchical CRF. Going further, Jiao et al. <ref type="bibr" target="#b42">[43]</ref> achieved impressive accuracy approaching both tasks with a new CNN, whose training is based on minimizing an attention-guided loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2.">SIDE and Surface Normals Estimation</head><p>The first studies on 3D understanding sought to obtain, among other information, illumination changes and color intensities <ref type="bibr" target="#b49">[50]</ref> and volumetric shapes <ref type="bibr" target="#b50">[51]</ref>. Due to their limitations, later works focused on other types of geometric cues such as segments and vanishing points <ref type="bibr" target="#b19">[20]</ref> and super-pixels <ref type="bibr" target="#b51">[52]</ref>. However, these methods depend on volumetric relationships and structural constraints. With the aid of datasets generated by RGBD sensors, later methods sought to estimate the 2.5D layout of scenes through depth and surface normals estimations, which are highly correlated. Some of these works were developed by Silberman et al. <ref type="bibr" target="#b17">[18]</ref> and Fouhay et al. <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b53">54]</ref>.</p><p>In the deep learning scenario, Wang et al. <ref type="bibr" target="#b1">[2]</ref> introduced a framework that estimates depth and surface normals through a four-stream CNN, coupled with a dense CRF, which retrieves planar surfaces and edges, as well as partial depth and surface normals. Also in a multitask context, Qi et al. <ref type="bibr" target="#b54">[55]</ref> created a network with two branches, called normal-to-depth and depth-to-normal, for depth and surface normals estimations. Based on the affinity rate between pixel pairs produced in correlated tasks, Zhang et al. <ref type="bibr" target="#b55">[56]</ref> proposed a CNN that explores similarity patterns between corresponding pixels, through an affinity matrix, to generate depth, semantics and surface normals maps simultaneously.</p><p>Rather than using multi-scale decoding structures, Yin et al. <ref type="bibr" target="#b10">[11]</ref> developed a CNN that performs SIDE and benefits from 3D geometric features of the scene through virtual normals. From another perspective, Ramamonjisoa et al. <ref type="bibr" target="#b2">[3]</ref> introduced the SharpNet, which predicts depth, surface normals and occluding contours, as well as a loss function that forces consistency between depth and occluding contours and between depth and surface normals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Self-supervised Learning</head><p>With the premise that supervised methods demand a large amount of ground truth depth data, which require significant time and effort to be produced, self-supervised learning strategies began to be developed. The emergence of self-supervised methods showed that it is possible to train SIDE models using only synchronized image pairs from stereo rigs <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b57">58]</ref> or sequences of frames <ref type="bibr" target="#b58">[59]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1.">Training with Stereo Images</head><p>With a breakthrough work, Garg et al. <ref type="bibr" target="#b56">[57]</ref> proposed a CNN that is fed with pairs of stereo images and retrieves depth maps. The model learns the nonlinear transformations necessary to recover the depth map using a loss function equivalent to the photometric difference between images. Godard et al. <ref type="bibr" target="#b26">[27]</ref> also formulated a deep CNN, called Monodepth, which receives only the left image of the stereo pair during testing. During training, their network learns to infer the disparity maps of both camera images. Going further, Godard et al. <ref type="bibr" target="#b59">[60]</ref> improved Monodepth with a loss function able to reduce the number of artifacts, to deal with occluded pixels and to disregard pixels that do not change in a sequence of images.</p><p>With a semi-supervised scope, the pipelines proposed by Kuznietsov et al. <ref type="bibr" target="#b60">[61]</ref> and Amiri et al. <ref type="bibr" target="#b61">[62]</ref> employ supervised learning from sparse data produced by LiDAR sensors, as well as self-supervised training reasoned by stereo pairs. On the other hand, the pipelines proposed in the works of Yang et al. <ref type="bibr" target="#b62">[63]</ref> and Andraghetti et al. <ref type="bibr" target="#b63">[64]</ref> use a self-supervised training strategy, based on stereo images, and also a supervised training procedure assisted by VO. Furthermore, Ramirez et al. <ref type="bibr" target="#b64">[65]</ref> was the first to consider stereo self-supervised learning along with the supervision of semantic classes.</p><p>Following the stereo matching setup, Luo et al. <ref type="bibr" target="#b65">[66]</ref> and Tosi et al. <ref type="bibr" target="#b66">[67]</ref> presented SIDE frameworks that synthesize the right view of the stereo pair from the original left image. In a multi-task approach, Li et al. <ref type="bibr" target="#b67">[68]</ref> introduced a deep network that estimates depth and 6D-camera pose with the assistance of a loss function that addresses the spatial and temporal aspects of the incoming image pairs. Also, Babu et al. <ref type="bibr" target="#b68">[69]</ref> developed a framework that predicts depth maps and camera pose with the help of a loss based on the Charbonnier penalty <ref type="bibr" target="#b69">[70]</ref>. This penalty expression is used in both spatial and temporal reconstruction errors that constitute the objective function.</p><p>Other stereo-based methods resort to trinocular training <ref type="bibr" target="#b70">[71]</ref>, synthetic training data <ref type="bibr" target="#b71">[72]</ref>, generative adversarial training <ref type="bibr" target="#b72">[73,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b74">75]</ref>, uncertainty handling <ref type="bibr" target="#b75">[76,</ref><ref type="bibr" target="#b76">77]</ref>, occlusion handling <ref type="bibr" target="#b77">[78]</ref> and knowledge distillation <ref type="bibr" target="#b78">[79]</ref>. Network architectures that are lighter and faster for real-time applications in embedded systems <ref type="bibr" target="#b79">[80,</ref><ref type="bibr" target="#b81">81,</ref><ref type="bibr" target="#b82">82]</ref> are also applied in the stereo matching self-supervised approach of SIDE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.">Training with Monocular Video</head><p>With the first efforts to perform self-supervised depth estimation from monocular video, Zhou et al. <ref type="bibr" target="#b58">[59]</ref> proposed a self-supervised model that performs SIDE and camera pose estimation. On the other hand, Yang et al. <ref type="bibr" target="#b83">[83]</ref> developed a CNN-based method that estimates depth and surface normals through edge recognition. In a later work, Yang et al. <ref type="bibr" target="#b84">[84]</ref> introduced an architecture that retrieves depth maps, surface normals and edge representations using a regularization method named 3D as-smoothas-possible.</p><p>Also with a multitask approach, Yin et al. <ref type="bibr" target="#b85">[85]</ref> proposed a self-supervised framework that predicts depth, optical flow and camera pose through an adaptive geometric consistency loss function, which is robust to occlusions and outliers. In parallel, Mahjourian et al. <ref type="bibr" target="#b86">[86]</ref> introduced a method that estimates depth and ego-motion from temporally consistent adjacent images using a 3D geometric loss function. Wang et al. <ref type="bibr" target="#b87">[87]</ref> leveraged depth normalization operations and a differentiable version of a direct VO algorithm. Showing promising results on SIDE and VO, the method from Casser et al. <ref type="bibr" target="#b88">[88]</ref> can model dynamic objects present in the scene with the support of instance segmentation masks.</p><p>Other SIDE strategies, self-supervised by monocular videos, employ semantics, flow and camera motion information <ref type="bibr" target="#b89">[89,</ref><ref type="bibr" target="#b90">90,</ref><ref type="bibr" target="#b91">91,</ref><ref type="bibr" target="#b92">92,</ref><ref type="bibr" target="#b9">10]</ref>, as well as other types of geometric constraints <ref type="bibr" target="#b92">[92,</ref><ref type="bibr" target="#b93">93,</ref><ref type="bibr" target="#b94">94]</ref> and lightweight CNNs <ref type="bibr" target="#b95">[95]</ref>.</p><p>The monocular video approach is challenging since the deep network needs to estimate the camera transformations through the sequence of input images, in addition to the depth of the scene. CNNs, trained with stereo images, do not need to estimate the camera pose, as they explore the correspondence of both images of the stereo pair. However, as a disadvantage, faults are prone to occur in areas close to occlusions <ref type="bibr" target="#b59">[60]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Depth Completion</head><p>Depth completion is distinct from the SIDE task, as the former is focused on the densification of sparse and noisy point cloud data, obtained from light-structured sensors, LIDAR sensors or SLAM/Structure from Motion (SfM) algorithms <ref type="bibr" target="#b96">[96]</ref>. In addition to filling data into depth channel spaces without information (depth inpainting) <ref type="bibr" target="#b97">[97,</ref><ref type="bibr" target="#b98">98]</ref>, the depth completion task also aims at depth denoising <ref type="bibr" target="#b99">[99,</ref><ref type="bibr" target="#b100">100]</ref> and depth super-resolution <ref type="bibr" target="#b101">[101,</ref><ref type="bibr" target="#b102">102]</ref>.</p><p>The first depth completion strategies typically resorted to handcrafted features to infer dense depth or disparity images <ref type="bibr" target="#b103">[103]</ref>. In this context, Ku et al. <ref type="bibr" target="#b104">[104]</ref> presented an unguided depth completion algorithm, based on inversion, expansion and morphological closing. On the other hand, the method of Schneider et al. <ref type="bibr" target="#b105">[105]</ref> exploits the guidance of semantic labels and edge maps to estimate dense images. However, such classic approaches cannot generalize well for different types of scenes, demanding fine adjustments of specific parameters for each circumstance. With that in mind, these classic algorithms were outperformed by novel learning-based models, which can be divided regarding the presence or absence of a guidance RGB image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1.">Depth Completion from Sparse Samples</head><p>In this category, depth completion techniques retrieve dense depth maps from sparse depth samples. Uhrig et al. <ref type="bibr" target="#b16">[17]</ref> introduced a sparse convolutional network composed of sparse convolutional layers that handle sparsity through the use of observation masks. Such masks hold the valid pixels information from the input feature maps, which are propagated by pooling operations.</p><p>Enhancing the normalized convolution approach, Eldesokey et al. <ref type="bibr" target="#b106">[106]</ref> proposed an unguided CNN that receives sparse depth and confidence maps as input. This network employs new normalized convolutions that infer confidence maps and transmit them to the adjacent layer. Chodosh et al. <ref type="bibr" target="#b107">[107]</ref> used established concepts of sparse representation learning (dictionary learning) in a neural network pipeline, based on the alternating direction neural network, to address the task of depth completion.</p><p>Huang et al. <ref type="bibr" target="#b108">[108]</ref> developed a CNN that benefits from observation masks and sparsity-invariant operations such as upsampling, average and convolution. Recently, sparsity-invariant convolutions <ref type="bibr" target="#b16">[17]</ref> are being revisited and complemented by novel mask aware operations to handle the RGB guided depth completion task <ref type="bibr" target="#b109">[109]</ref>. Other few works are focused on closing the gap between unguided and guided depth completion methods <ref type="bibr" target="#b110">[110]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2.">Guided Depth Completion</head><p>The approaches that deal with guided depth completion leverage RGB image cues to densify sparse and noisy depth maps <ref type="bibr" target="#b111">[111]</ref>. Ma et al. <ref type="bibr" target="#b6">[7]</ref> proposed a CNN that is fed with color images and sparse depth data sampled with the Bernoulli probability strategy. This deep model can predict dense depth maps from different numbers of target samples. In a later work, Ma et al. <ref type="bibr" target="#b8">[9]</ref> introduced a self-supervised pipeline that handles sparse data and also receives sequences of RGB images as input.</p><p>Addressing both SIDE and depth completion, Cheng et al. <ref type="bibr" target="#b112">[112]</ref> designed a framework that employs a Convolutional Spatial Propagation Network (CSPN). This CNN propagates through the application of recurrent convolutions in a fixed neighborhood and, due to this, it can learn the affinity matrix related to the pixels of the output depth map from the depth estimation stage. Posteriorly, Cheng et al. <ref type="bibr" target="#b96">[96]</ref> enhanced the CSPN's depth completion performance through an adaptive technique that infers some network's hyper-parameters.</p><p>Inspired by the CSPN, the network architecture proposed by Park et al. <ref type="bibr" target="#b113">[113]</ref> learns the affinities of the estimated depth map pixels, and its confidences, in a nonlocal neighborhood. In another variation, Xu et al. <ref type="bibr" target="#b114">[114]</ref> designed a deformable spatial propagation network that propagates with adaptive receptive fields and affinity matrices.</p><p>On the other hand, Chen et al. <ref type="bibr" target="#b115">[115]</ref> developed a depth completion CNN that learns 2D-3D features in a multilevel way, while Li et al. <ref type="bibr" target="#b116">[116]</ref> used blocks of hourglass networks that are fed with sparse samples and guided by the encoder features. Recently, Tang et al. <ref type="bibr" target="#b111">[111]</ref> designed a guided method that employs spatially variable convolution kernels. Other works also benefit from morphological operators <ref type="bibr" target="#b117">[117]</ref>, additional surface normals information <ref type="bibr" target="#b103">[103,</ref><ref type="bibr" target="#b118">118]</ref>, phase masks <ref type="bibr" target="#b119">[119]</ref> and cross-guidance techniques <ref type="bibr" target="#b120">[120]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Final Considerations</head><p>From <ref type="table" target="#tab_0">Table 1</ref>, we can see that state-of-the-art supervised CNNs reach a prediction speed that does not exceed 20 fps, using powerful Graphic Processing Units (GPUs) and feature extraction networks with no less than 25M trainable parameters. However, comparing the results obtained in <ref type="bibr" target="#b88">[88,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b95">95,</ref><ref type="bibr" target="#b76">77]</ref> with <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b10">11]</ref>, we have that supervised methods still obtain more accurate depth predictions. Furthermore, self-supervised methods, in some cases, require VO techniques <ref type="bibr" target="#b87">[87,</ref><ref type="bibr" target="#b90">90,</ref><ref type="bibr" target="#b63">64]</ref>, semi-supervised pipelines <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b121">121,</ref><ref type="bibr" target="#b66">67]</ref>, both images from the stereo pair during training <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b77">78]</ref>, as well as optical flow, semantic and surface normals cues <ref type="bibr" target="#b84">[84,</ref><ref type="bibr" target="#b122">122,</ref><ref type="bibr" target="#b91">91,</ref><ref type="bibr" target="#b9">10]</ref> to obtain better results.</p><p>Therefore, this work aims to explore the benefits of supervised networks, along with other deep learning techniques, to solve problems that involve SIDE. The next section shows how the current proposal is correlated to state-of-the-art FCN architectures, which employ supervised learning to perform monocular depth estimation. Other deep learning methods, which include the tasks of semantic segmentation, surface normals estimation and depth completion, are also covered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we exploit the concepts behind our novel supervised monocular depth estimation method. We aim to introduce the details from the proposed baseline CNN architecture, which is divided into the encoder and decoder well-defined phases. Along with such architecture, 4 different encoder models are presented. The developed models are used in the ablation studies step so that we may verify the one that best fits our framework. We also cover a significant variety of loss functions that are employed in the monocular depth estimation studies.</p><p>Later, the pipelines that account for semantic segmentation, surface normals estimation and depth completion are described. Through small modifications in the proposed CNN, the former approach uses some pre-trained weights for semantic segmentation to enhance the depth inference accuracy. In the second strategy, a new surface normals estimation module is developed, which leverages the retrieved depth maps, the feature maps produced by the entire baseline and edge constraints. Furthermore, a simple geometric loss function is introduced to handle both depth and surface normals predictions efficiently. We add a gaussian blur technique to the proposed module to verify its robustness to noisy inputs.</p><p>In the last pipeline, we employ the sampling strategy based on the categorical distribution to densify sparse and noisy depth maps for indoor and outdoor environments. All the variations of the baseline network can be executed at a frame rate above 20.</p><p>Posteriorly, we introduce the experimental phases that cover the implementation details, the datasets used and the evaluation criteria applied to our results. Finally, we present the ablation studies, with the quantitative and qualitative analysis, and the conclusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Network Architecture</head><p>The proposed supervised FCN, named DenseSIDENet (DSN), is inspired by the framework of Lee et al. <ref type="bibr" target="#b12">[13]</ref> since their model is state-of-the-art for the SIDE task. Compared to the most accurate method of Lee et al. <ref type="bibr" target="#b12">[13]</ref>, ours can reach 56× fewer trainable parameters. <ref type="figure" target="#fig_0">Fig. 1</ref> illustrates the baseline of the DenseSIDENet highlighting the encoder-decoder structure.  We implemented as encoder the following lightweight feature extraction CNNs: MobileNet <ref type="bibr" target="#b130">[130]</ref>, DenseNet-121 <ref type="bibr" target="#b21">[22]</ref> and new variations of the robotic grasp detection network proposed by Ribeiro et al. <ref type="bibr" target="#b131">[131]</ref>. The aforementioned networks have 0.6M (up to 4M ), 6M and 0.3M (up to 9M ) trainable parameters, respectively, which makes it possible to evaluate the framework at a high frame rate without losing accuracy.</p><p>The CNN depicted in <ref type="figure" target="#fig_1">Fig. 2</ref> refers to one of the proposed variations of Ribeiro's model <ref type="bibr" target="#b131">[131]</ref>. Due to its efficiency and size, it is designed for the DSN encoder stage. Particularly, the number of filters ({32, 164, 96, 128}) and the kernel size (3 × 3) of each convolution are maintained from the original network. However, the flatten operation, the FC layers and the output layer are removed.</p><p>We also developed a dense configuration of the network of <ref type="figure" target="#fig_1">Fig. 2</ref>. The standard structure of the DenseNet presents [131] for the task of robotic grasping detection a sequence of dense convolution blocks interspersed with transition layers which are composed of convolution and average pooling operations. Therefore, layers 1, 2, 3 and 4 of the architecture in <ref type="figure" target="#fig_1">Fig. 2</ref> are redesigned to resemble dense blocks alternated with transition layers. However, the filter ratio ({32, 164, 96, 128}), the kernel size (3 × 3) and the max-pooling operations are preserved, according to what is proposed by Ribeiro et al. <ref type="bibr" target="#b131">[131]</ref>. <ref type="table">Table 2</ref> details 3 lightweight versions of the proposed dense architecture. For each model, k refers to the growth rate of the filters, which is added at the end of the dense blocks, θ corresponds to the filter's compression and t refers to the total number of trainable parameters. Beyond the differences presented in <ref type="table">Table 2</ref>, the stages of the proposed models are initialized with the following filter distribution: 32 filters for the initial convolution, 164 filters for the dense blocks 1, 2 and 3, 96 filters for the transition layers 1, 2 and 3, 128 filters for the dense block 4. The first convolutions of the dense blocks are set up with 2× the current number of filters.</p><p>Attached to the DSN encoder, a pair of upconvolution, concatenation and convolution blocks replace the last pooling layers of the feature extraction network to prevent the feature maps resolution from decreasing. Afterwards, a Dense ASPP <ref type="bibr" target="#b132">[132]</ref> is implemented according to Lee et al. <ref type="bibr" target="#b12">[13]</ref>, who also utilize the dilated rates 3, 6, 12, 18, 24.</p><p>Since this module consists of densely connected dilated convolution layers, it does not suffer from the same degradation problems as the original ASPP. Thus, the densely connected ASPP is applied to extract multi-scale features and, at the same time, to avoid the excessive reduction of the receptive field.</p><p>Due to this, the DSN decoder stage processes larger feature maps, with multiple-sized receptive fields, and performs less upsampling operations to recover the original image resolution (D) at the output. <ref type="figure" target="#fig_0">Fig. 1</ref> shows that the decoder stage does not add highly complex neural layers that could increase the number of trainable parameters and decrease the DSN's prediction speed.</p><p>As also can be noticed in <ref type="figure" target="#fig_0">Fig. 1</ref>, the upconvolution block is formed by a convolution layer with 3 × 3 kernel, which is preceded by a nearest neighbors upsampling operation that retrieves feature maps with 2× increased resolution. In the framework, such blocks are followed by batch normalization operations and, subsequently, by skip-connections/concatenations. The first upconvolution block is initialized with only 256 filters which are decreased by a fraction of 2 until the last layer.</p><p>Throughout the network structure, ReLU <ref type="bibr" target="#b133">[133]</ref> and eLU <ref type="bibr" target="#b134">[134]</ref> activation functions are employed, with the exception of the output layer that uses the sigmoid function. The output depth map of the DSN is multiplied by the maximum depth value comprising the dataset used. The network is fed with a single RGB image with 352×704 and 416 × 544 pixels and predicts a depth map with 352 × 1216 and 480 × 640 pixels for the outdoor <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b135">135]</ref> and indoor <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b136">136,</ref><ref type="bibr" target="#b137">137,</ref><ref type="bibr" target="#b138">138]</ref> datasets respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">SIDE Loss Functions</head><p>The losses Scale-invariant Error (L SILog ) <ref type="bibr" target="#b27">[28]</ref>, modified Scale-invariant Error (L + SILog ) <ref type="bibr" target="#b12">[13]</ref>, Huber (L Huber ) <ref type="bibr" target="#b139">[139]</ref>, BerHu (L BerHu ) <ref type="bibr" target="#b30">[31]</ref>, Mean Absolute Error (L 1 ) <ref type="bibr" target="#b26">[27]</ref>, Mean Squared Error (L 2 ) <ref type="bibr" target="#b25">[26]</ref>, Charbonnier (L charbo ) <ref type="bibr" target="#b140">[140]</ref> and Log-cosh (L cosh ) <ref type="bibr" target="#b141">[141]</ref> are used in the training phase of the network depicted in <ref type="figure" target="#fig_0">Fig. 1</ref>. The pixels without depth information are disregarded by the loss functions, as they do not influence the adjustment of the network weights.</p><p>The L SILog expression (Eq. 1) was introduced in [28] and has often been used in monocular depth estimation CNNs <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b139">139]</ref> since it is an efficient function to determine the relationships between the depths of a scene without the influence of its scale <ref type="bibr" target="#b27">[28]</ref>. Such a loss function, modified in <ref type="bibr" target="#b12">[13]</ref>, can be expressed by Eq. 2, in which y * i represents the ground truth depth values, y i refers to the prediction of the network for the i − th pixel and N is equal to the total number of samples.</p><formula xml:id="formula_0">L SILog (h) = 1 N i h 2 i − λ N 2 i h i 2 ,<label>(1)</label></formula><formula xml:id="formula_1">h i = log(y i ) − log(y * i ), λ = 0.85 L + SILog = θ S(h),<label>(2)</label></formula><formula xml:id="formula_2">S(h) = 1 N N i=1 h 2 i − 1 N N i=1 h i 2 + (1 − λ) 1 N N i=1 h i 2 , h i = log(y i ) − log(y * i ), λ = 0.85, θ = 10</formula><p>.0 BerHu, the inverse function of L Huber (Eq. 3), is also applied in this work due to its advantages related to the combination of the losses L 1 and L 2 . Among these benefits, residuals (|y i − y * i |) with high values are penalized by the function L 2 which is sensitive to outliers. On the other hand, residuals with low values are penalized by the L 1 function, which returns greater depth values than those returned by the L 2 . The BerHu penalty can be expressed by Eq. 4 with the default κ = 5.</p><formula xml:id="formula_3">L Huber (h) = |h| |h| ≥ g, h 2 +g 2 2g |h| &lt; g,<label>(3)</label></formula><formula xml:id="formula_4">h = log(y) − log(y * ), g = 1 κ max i (| log(y i ) − log(y * i )|) L BerHu (h) = |h| |h| ≤ g, h 2 +g 2 2g |h| &gt; g (4) h = log(y) − log(y * ), g = 1 κ max i (| log(y i ) − log(y * i )|)</formula><p>In addition to the L SILog , L + SILog , L BerHu and L Huber expressions, the L 1 and L 2 loss functions, represented by Eqs. 5 and 6, respectively, are employed during training, <ref type="table">Table 2</ref>: Composition of the proposed models to be applied in the feature extraction stage of the DenseSIDENet baseline. For all models, k indicates the filters growth rate, θ is the filters compression and t is the total number of parameters Layers Model 1 (k = 6, θ = 1, t = 1M ) Model 2 (k = 16, θ = 1, t = 3M ) Model 3 (k = 24, θ = 1, t = 9M ) Initial Convolution 3 × 3 conv, stride 2 3 × 3 conv, stride 2 3 × 3 conv, stride 2 Initial Pooling 3 × 3 max pooling, stride 2 3 × 3 max pooling, stride 2 3 × 3 max pooling, stride 2</p><formula xml:id="formula_5">Dense Block 1 3 × 3 conv, stride 1 3 × 3 conv, stride 1 ×6 3 × 3 conv, stride 1 3 × 3 conv, stride 1 ×6 3 × 3 conv, stride 1 3 × 3 conv, stride 1 ×6</formula><p>Transition Layer 1 3 × 3 conv, stride 1 2 × 2 max pooling, stride 2 3 × 3 conv, stride 1 2 × 2 max pooling, stride 2 3 × 3 conv, stride 1 2 × 2 max pooling, stride 2</p><formula xml:id="formula_6">Dense Block 2 3 × 3 conv, stride 1 3 × 3 conv, stride 1 ×12 3 × 3 conv, stride 1 3 × 3 conv, stride 1 ×12 3 × 3 conv, stride 1 3 × 3 conv, stride 1 ×12</formula><p>Transition Layer 2 3 × 3 conv, stride 1 2 × 2 max pooling, stride 2 3 × 3 conv, stride 1 2 × 2 max pooling, stride 2 3 × 3 conv, stride 1 2 × 2 max pooling, stride 2</p><formula xml:id="formula_7">Dense Block 3 3 × 3 conv, stride 1 3 × 3 conv, stride 1 ×18 3 × 3 conv, stride 1 3 × 3 conv, stride 1 ×18 3 × 3 conv, stride 1 3 × 3 conv, stride 1 ×24</formula><p>Transition Layer 3 3 × 3 conv, stride 1 2 × 2 max pooling, stride 2 3 × 3 conv, stride 1 2 × 2 max pooling, stride 2 3 × 3 conv, stride 1 2 × 2 max pooling, stride 2</p><formula xml:id="formula_8">Dense Block 4 3 × 3 conv, stride 1 3 × 3 conv, stride 1 ×16 3 × 3 conv, stride 1 3 × 3 conv, stride 1 ×14 3 × 3 conv, stride 1 3 × 3 conv, stride 1 ×18</formula><p>as they are widely addressed by single-view depth estimation networks <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b25">26]</ref>. The Charbonnier penalty, described by Eq. 7, has been applied to problems involving regression, mainly in flow estimation approaches <ref type="bibr" target="#b140">[140]</ref> and self-supervised SIDE frameworks <ref type="bibr" target="#b68">[69]</ref>. The parameters a and α of L charbo (h) are adjusted according to what is established by the work of Babu et al. <ref type="bibr" target="#b68">[69]</ref>.</p><formula xml:id="formula_9">L 1 (h) = 1 N N i=1 |h i | , h i = log(y i ) − log(y * i ) (5) L 2 (h) = 1 N N i=1 (h i ) 2 , h i = log(y i ) − log(y * i ) (6) L charbo (h) = 1 N N i=1 h 2 i + α 2 a ,<label>(7)</label></formula><p>h i = log(y i ) − log(y * i ), a = 0.45, α = 10 −3 The Log-cosh loss (Eq. 8) presents an inverse behavior to that of BerHu, operating as L 1 for high residuals and as L 2 for low residuals. This makes its behavior similar to the Huber function, yet the L cosh (h) has the advantage of being totally differentiable.</p><formula xml:id="formula_10">L cosh (h) = N i=1 log (cosh (h i )) , h i = y i − y * i (8)</formula><p>Based on the presented losses, we aim to use the one that produces the best results in the tests of the proposed network, replacing the term L (y, y * ) of the attentionbased function at distant depths, Eq. 9, which is introduced in the work of Jiao et al. <ref type="bibr" target="#b42">[43]</ref>. According to the authors, this function allows the network to focus on regions where the availability of pixels with depth information is reduced, once they are unevenly distributed.</p><p>The constant β, the attention term γ in logarithmic scale and the function L a in summation setup are the modifications proposed over the original expression <ref type="bibr" target="#b42">[43]</ref>. In our approach, β is set to 10.0 and 1.0 for the indoor and outdoor datasets, respectively, due to scale variations.</p><formula xml:id="formula_11">L a = N i=1 (γ + ) L (y, y * ),<label>(9)</label></formula><formula xml:id="formula_12">γ = log(y * i ) max i (log(y * i ))</formula><p>, = 1.0 − min(log(βy i ), log(βy * i )) max(log(βy i ), log(βy * i )) Finally, the loss functions L + 1 and L + 2 are similar to the L 1 and L 2 expressions respectively. However, the former ones are computed only by the total sum of the equation results, instead of the average calculation. Based on experimental analysis, these variations of L 1 and L 2 are applied only when training the network with the indoor datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">SIDE with Semantic Cues Pipeline</head><p>The influence of additional semantic information on the proposed FCN is analyzed through fine-tunning. According to such a strategy, the DSN is firstly pre-trained for semantic segmentation and part of the computed weights are transferred to the monocular depth estimation framework.</p><p>The loss function that is used for the semantic segmentation task can be represented by Eq. 10, where K is equal to the total number of categories, y * i is the ground truth in the one-hot encoding format, y i is the network prediction, after the application of the softmax activation function, and t represents the elements of the input feature map.</p><formula xml:id="formula_13">L cce (y, y * ) = − K i=1 y * i log(y i ), y i = e ti K j e tj<label>(10)</label></formula><p>Only part of the weights obtained with the semantic segmentation framework is fine-tuned in the depth estimation phase due to the structural differences between the two CNNs, especially in the last layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">SIDE with Surface Normals Cues Pipeline</head><p>Differently from previous works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b55">56]</ref> and closer to the approach of Yin et al. <ref type="bibr" target="#b10">[11]</ref>, we leverage the correlation between depth and surface normals estimation in only one framework. Thus, we do not employ any extra branches or other supplementary models to exploit 3D geometric constraints from surface normals. Contrariwise, they are inferred from the output depth map and the same neural layers used for SIDE.</p><p>To train our CNN for surface normals estimation, the ground truth is obtained as in the works of Fouhey et al. <ref type="bibr" target="#b52">[53]</ref>, Qi et al. <ref type="bibr" target="#b54">[55]</ref> and Zhang et al. <ref type="bibr" target="#b55">[56]</ref>. Therefore, firstly, the point cloud in 3D space is recovered from the depth maps, assuming the pinhole camera model according to Eq. 11. In such an equation, (υ i , ν i ) refers to the pixel positions in the image, (x i , y i , z i ) represents the 3D coordinates of the point cloud, where the values of z i are equal to the depths of the scene ρ i , (o x , o y ) is the location of the optical axis and f x and f y are the focal lengths of the axes x and y respectively.</p><formula xml:id="formula_14">x i = z i (υ i − o x ) f x , y i = z i (ν i − o y ) f y , z i = ρ i<label>(11)</label></formula><p>As proposed by Qi et al. <ref type="bibr" target="#b54">[55]</ref>, after reconstructing the point cloud, we determine the tangent planes to the cloud depth data by defining a neighborhood of points of size m, which belongs to the same plan. Once computed the local planes, the surface normal vectors s = (s x , s y , s z ) can be obtained through the relationship Ds = c, where D m×3 is the coordinate matrix of the point cloud and c m×1 is a vector with constant values. In order to minimize ||Ds − c|| 2 2 , the least-squares solutionŝ 3×m is applied to the normal equations system of Eq. 12.</p><formula xml:id="formula_15">D T Dŝ = D T c<label>(12)</label></formula><p>From Eq. 12, we can get to Eq. 13 which is used to directly calculate the normalized surface normal vectors. Following what was established by Qi et al. <ref type="bibr" target="#b54">[55]</ref>, the vector c m×1 is defined with unit values.</p><formula xml:id="formula_16">s = (D T D) −1 D T c ||(D T D) −1 D T c|| 2<label>(13)</label></formula><p>The developed module shares the same feature maps produced in the SIDE step, yet they are concatenated with the predicted depth map and with the output from the Sobel edge detection algorithm <ref type="bibr" target="#b142">[142]</ref>. This algorithm is computationally efficient, it is robust to the noise present in the input grayscale image and it retrieves the high gradient features of the input which helps the proposed CNN to better reason about depth in the boundaries of the objects. Eqs. 15 and 16 describe the H x and H y masks used by the Sobel filter to calculate the gradients approximation of the image I (Eq. 14). With the achieved gradient vector, its magnitude H and direction Ψ are computed to determine the pixels in border regions, which follow the relationship H(i, j) &gt; τ wherein τ is equal to the threshold value.</p><formula xml:id="formula_17">I x = ∂I ∂x , I y = ∂I ∂y (14) I x (i, j) = (I * H x )(i, j), I y (i, j) = (I * H y )(i, j) (15) H x =   −1 0 1 −2 0 2 −1 0 1   , H y = −H T x , H = I 2 x + I 2 y , Ψ = arctan I y I x<label>(16)</label></formula><p>To validate the noise robustness of the surface normals module, a convolutional layer with Gaussian kernel <ref type="bibr" target="#b143">[143]</ref> is implemented before the Sobel edge detector. This convolutional layer is responsible for filtering the noise present in the input grayscale image I, making the output of the edge detection step less affected by them. The proposed linear Gaussian filter is free from second peaks in the frequency domain and can be expressed by Eq. 17 wherein I g is the filtered image and G refers to the m × n kernel that obeys a Gaussian distribution with σ = 1.76.</p><formula xml:id="formula_18">Ig(i, j) = (I * G)(i, j) = m n G(m, n)I(i − m, j − n), (17) G(m, n) = 1 2πσ 2 e − m 2 +n 2 2σ 2</formula><p>For the joint approach of depth and surface normals estimation, we designed the geometric loss function L 2.5D , whose expression comprises the functions L (y, y * ) and Φ(n, n * ). The former loss is replaced by the one that generates the most accurate depth predictions. The latter loss computes the cosine similarity between the valid pixels from the surface normals predictions n and the ground truth n * . Also, Φ(n, n * ) is weighted by a proportionality constant ψ.</p><formula xml:id="formula_19">L 2.5D = L (y, y * ) + ψ(Φ(n, n * )),<label>(18)</label></formula><p>Φ(n, n * ) = 1 − n, n * ||n|| 2 ||n * || 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Depth Completion Pipeline</head><p>This work also aims to analyze the behavior of the results produced by the network when it is trained and tested with additional depth data at the input, which can be obtained through low-cost 2D LiDARs and the outputs of visual odometry algorithms <ref type="bibr" target="#b6">[7]</ref>.</p><p>We simulate visual odometry algorithms or depth sensors by a sampling technique which incorporates the multinomial distribution with the number of independent experiments n = 1. Thus, only a random sampling attempt of a sparse depth data χ is performed for the ground truth pixels paired with the network input images. This special case of the multinomial distribution is called categorical distribution, in which the number of categories is fixed at k = 2. The Eq. 19 regards the mass probability function associated with the random variable χ.</p><formula xml:id="formula_20">F (χ | ϕ) = k−1 i=0 ϕ χi i , ϕ i &gt; 0, k−1 i=0 ϕ i = 1<label>(19)</label></formula><p>The probabilities ϕ 1 and ϕ 0 refer, respectively, to the success and failure of sampling a sparse depth data. Therefore, as proposed by Ma et al. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>To implement and train the proposed network, a server with 4.20GHz CPU, 2TB HD, 200GB SSD, Ubuntu 16.04 operating system, Tensorflow 1.13.1 development framework and two NVIDIA Titan X GPUs (both are used in the training phase and only one in the test phase). The Adam optimization algorithm <ref type="bibr" target="#b144">[144]</ref> is standard for all experiments, with α = 10 −3 , β 1 = 0.9, β 2 = 0.999 and = 10 −8 , as proposed by Kingma et al. <ref type="bibr" target="#b144">[144]</ref>.</p><p>Furthermore, the learning rate is set to 10 −4 with a polynomial decay at a rate of 0.9. In each experiment, the CNN is trained for 50 epochs with batch size 16. However, the batch size is set to 32 in the training steps that involve the MobileNetV2-50, the network of <ref type="figure" target="#fig_1">Fig. 2</ref> and the proposed Model 1.</p><p>Although the feature maps generated by the first CNN convolution layers present primitive visual information, the parameters of the batch normalizations and the weights of the first two convolutions of the feature extraction network are not fixed, as occurs in the methods proposed by Fu et al. <ref type="bibr" target="#b25">[26]</ref> and Lee et al. <ref type="bibr" target="#b12">[13]</ref>.</p><p>To avoid overfitting and artificially increase the number of training samples, three types of online data augmentation are employed. Among them, the random horizontal flip operations, with a probability of 50%, and rotation in the intervals of [−2.5 • , 2.5 • ] and [−1.0 • , 1.0 • ] for the indoor and outdoor datasets respectively. Brightness [0.75, 1.25], color [0.9, 1.1] and contrast [0.9, 1.1] distortions are also considered with a probability of 50%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">Outdoor</head><p>In previous works of the SIDE literature, the KITTI Raw <ref type="bibr" target="#b145">[145]</ref>, whose images are obtained directly from the sparse point clouds generated by a LiDAR sensor, is used as ground truth. However, recently, the KITTI Vision Benchmark Suite has released the official KITTI Depth dataset, which is composed of denser depth maps due to the combination of 11 laser scans <ref type="bibr" target="#b16">[17]</ref>. The KITTI Depth consists of stereo images in gray and color scales, which are captured by two pairs of stereo cameras that compose the autonomous navigation platform capable of traversing external environments to collect data.</p><p>Moreover, the dataset provides depth measurements generated by the scans of the Velodyne HDL-64E LiDAR in the form of point clouds. Another relevant aspect of the KITTI Depth is the denser ground truth, compared to the point clouds, which presents a grayscale image format, whose pixels provide the depth information of the left and right scenes from the stereo cameras.</p><p>Both the stereo images and the densified ground truth have a typical resolution of 375 × 1242 pixels and are divided into 61 scenes for training and testing, which comprise the categories "city", "residential", "road" and "campus". At first, the KITTI Raw <ref type="bibr" target="#b145">[145]</ref> was subdivided into 32 different training scenes and 29 test scenes by Eigen et al. <ref type="bibr" target="#b27">[28]</ref>. The training subdivision contains 23488 images and the testing set comprises 697 images. Bringing the Eigen Split to the KITTI Depth <ref type="bibr" target="#b16">[17]</ref>, we aim to use 23158 images randomly selected from the total 23488 training samples.</p><p>For the semantic segmentation task, the KITTI Vision Benchmark provides 200 manually annotated maps for training and 200 test samples with a typical resolution of 375 × 1242 pixels. Due to this, additional training data from the Cityscapes dataset <ref type="bibr" target="#b135">[135]</ref> are included. The Cityscapes consists of 5K semantic maps (with a resolution of 1024 × 2048 pixels), which are formed by fine annotations that sum up 30 different classes. From the 5K images, 2975, 1525 and 500 are reserved for training, testing and validation respectively. Therefore, in this work, the KITTI's 200 semantic training maps are complemented by 3475 Cityscapes samples.</p><p>Corresponding to the semantic maps, the Cityscapes dataset also provides the same number of disparity maps, which can be used to reconstruct their respective depth maps. Due to the availability of disparity maps for training, testing and validation, 5K depth maps reconstructed from the Cityscapes are used to pre-train the proposed framework.</p><p>We also apply the KITTI Odometry dataset <ref type="bibr" target="#b146">[146]</ref> in the VO studies with the proposed CNN. Such a dataset is composed of 22 sequences with stereo images of outdoor scenarios. However, only the first 11 sequences have their respective reference trajectories, while the remaining ones are used for benchmark evaluation. As proposed by Loo et al. <ref type="bibr" target="#b4">[5]</ref>, we focus on the stereo sequences with available ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">Indoor</head><p>The NYU Depth V2 <ref type="bibr" target="#b17">[18]</ref> is composed of pairs of RGB images and depth maps, with a standard resolution of 480 × 640 pixels, captured by the camera and depth sensor of a Microsoft Kinect. Altogether, the official dataset is formed by 240K pairs of RGBD training samples that integrate 464 indoor scenes. As other SIDE state-of-the-art works <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b14">15]</ref>, we apply the official subdivision of the NYU Depth V2 data which comprises 249 training scenes and 215 test scenes.</p><p>From the 249 training scenes, totaling 120K samples, 24931 images are selected to train the proposed CNN. On the other hand, from the 215 test scenes, 654 images are pre-defined to test our framework. Additionally, 8587 samples, from the SUNRGBD <ref type="bibr" target="#b136">[136]</ref>, Berkeley B3DO <ref type="bibr" target="#b137">[137]</ref> and SUN3D <ref type="bibr" target="#b138">[138]</ref> datasets are employed in pre-training phases.</p><p>To address the semantic segmentation task, the NYU Depth V2 provides 1449 images, with 35064 different objects, that present semantic annotations with 894 different categories. We also add 10336 samples, with their corresponding semantic labels, from the SUNRGBD dataset <ref type="bibr" target="#b136">[136]</ref> to the 1449 available training images.</p><p>The surface normals ground truth, for both indoor and outdoor datasets, is produced according to what is proposed in the work of Fouhey et al. <ref type="bibr" target="#b52">[53]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation Metrics</head><p>The proposed FCN is tested outdoors according to the Eigen Split. However, as this subdivision is originally proposed for the KITTI Raw dataset <ref type="bibr" target="#b145">[145]</ref>, only 652 of the 697 test images present ground truth in the KITTI Depth <ref type="bibr" target="#b16">[17]</ref>. Furthermore, the network predictions are evaluated considering the central crop established by Garg et al. <ref type="bibr" target="#b56">[57]</ref> and distance caps of 0 − 50m and 0 − 80m. For the evaluation of the framework in indoor environments, the central crop proposed in <ref type="bibr" target="#b27">[28]</ref> is used in the predictions that correspond to the 654 test images from the NYU Depth V2.</p><p>Therefore, in order to present quantitative results, the Scale-invariant Error (Eq. 20), Threshold (Eq. 21), Absolute Relative Difference (Eq. 22), Squared Relative Difference (Eq. 23), Log10 (Eq. 24), Mean Absolute Error (Eq. 25), Linear RMSE (Eq. 26) and Log RMSE (Eq. 27), are applied in the evaluation phase wherein T is equivalent to the total amount of pixels with depth information: </p><formula xml:id="formula_21">1 |T | y∈T (y − y * ) 2 − 1 2T 2 y∈T (y − y * ) 2 (20) % of y i s.t. max y i y * i , y * i y i = δ &lt; threshold<label>(21)</label></formula><formula xml:id="formula_22">1 |T | y∈T |y − y * | y *<label>(22)</label></formula><formula xml:id="formula_23">1 |T | y∈T || log y − log y * || 2<label>(26)</label></formula><p>The surface normals maps retrieved by our pipeline are evaluated according to the metrics introduced by Fouhey et al. <ref type="bibr" target="#b52">[53]</ref>, which are commonly used by recent works <ref type="bibr" target="#b147">[147,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b55">56]</ref>. Such metrics consider the angular difference for each valid pixel between the estimated surface normals and the ground truth by calculating the mean, median, root mean squared error (RMSE) and the precision of the estimated pixels. This accuracy is measured by the percentage of vectors whose angular difference does not exceed a threshold n = {11.25 • , 22.5 • , 30 • }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Outdoor Ablation Studies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1.">Preliminary Analysis</head><p>Firstly, regarding depth completion approaches to enhance the depth estimation results, we used the closing morphological operation to partially densify the reference maps of the KITTI Depth <ref type="bibr" target="#b16">[17]</ref>, generating the KITTI Morphological dataset. Along with the closing technique, we applied the RGB guide&amp;certainty method, introduced by Gansbeke et al. <ref type="bibr" target="#b7">[8]</ref>, and the depth completion strategy of Hilbert Maps <ref type="bibr" target="#b148">[148]</ref> to produce the KITTI Completed and the KITTI Continuous respectively.</p><p>Through the KITTI Benchmark metrics, the depth completion results from the application of 5 iterations of the closing morphological technique are compared to other methods in the literature in <ref type="table" target="#tab_2">Table 3</ref>. In such a table, it is possible to note that the closing operator, with kernel 3×3, is the method that best preserves the ground truth depth information. However, the learning-based techniques retrieve totally dense depth maps, whereas the closing ones can be tuned to output images with different densification levels.</p><p>In <ref type="table" target="#tab_3">Table 4</ref>, we may infer that the bigger the number of iterations (it) the denser the KITTI Morphological dataset becomes. Moreover, the proposed KITTI Completed is the densest dataset employed once it is built with the learning-based RGB guide&amp;certainty approach. The results exposed in <ref type="table" target="#tab_4">Table 5</ref> show that our pipeline achieves slightly better performance when jointly trained with the partially dense dataset and the BerHu loss function.</p><p>However, the accuracy of the method decreases when it is trained with the KITTI Completed and the KITTI Continuous datasets due to the presence of greater distortions in the reference depth maps. Furthermore, although the KITTI Continuous is less dense than the KITTI Completed, the former dataset includes a higher amount of artifacts that hinder the network learning. Therefore, the KITTI Morphological presents the best balance between densification and modification of the original ground truth values, so that our CNN can better understand the structure of the scene. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2.">Structural Studies</head><p>The following studies are conducted with the KITTI Morphological, with setup k = 3 and it = 2 since it is beneficial to the training of our CNN. To explore the response of the proposed pipeline to other encoder networks, we carried out new experiments whose results are presented in <ref type="table" target="#tab_5">Table 6</ref>. From the metrics of <ref type="table" target="#tab_5">Table 6</ref>, we can observe that the DSN with Model 3 as encoder achieves the best trade-off regarding the number of trainable parameters, prediction speed and accuracy. Moreover, analyzing the methods with comparable sizes, it is possible to state that all the proposed lightweight feature extraction networks <ref type="figure" target="#fig_0">(Fig. 2, Model 1</ref>, Model 2 and Model 3) surpass the stateof-the-art MobileNetV2 and DenseNet-121.</p><p>To improve the performance of the DSN with Model 3 as its feature extraction network (DSN 3), we impose modifications in the structure of the CNN decoder and in its training procedure. Through <ref type="table" target="#tab_6">Table 7</ref>, we may verify that the use of the attention loss, combined with the L BerHu expression, decreases the framework accuracy, whereas the application of the L + BerHu function enhances the results. This indicates that the BerHu loss function is already capable of balancing the penalties for residuals generated in regions closer and more distant to the scene, without the need for an attention term. Furthermore, the network learning is boosted when the term L 1 of the BerHu penalty acts on a wider range of residuals, considering the KITTI Morphological dataset.</p><p>The pyramid modules are simple blocks that concatenate multi-scale feature maps of the DSN 3. In the decoder, the Pyramid 1 module concatenates the upsampled feature maps from the last 3 convolution blocks with the output from the final upconvolution, while the Pyramid 2 adds the feature maps from the first convolution to this concatenation. Regarding the metrics presented in <ref type="table" target="#tab_6">Table  7</ref>, both the pyramid modules are beneficial to the proposed framework, yet they slow down the network's processing speed.</p><p>Also, the results obtained when the DSN 3 is pretrained with semantic and disparity maps show that both pre-trainings improve the metrics values. Although our method acquires important cues, like the object edges, from the semantic maps with 19 different categories, the monocular features learned from the same depth estimation task show better improvements. All the structural changes in the decoder of the proposed pipeline reduce its inference speed, yet the number of trainable parameters is not significantly increased.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3.">Surface Normals Analysis</head><p>The surface normals module is tested in the DSN 3 framework, along with the Pyramid 1 module, the edge detection phase, the 2.5D loss function and the weights pretrained on the reference depth maps of the Cityscapes <ref type="bibr" target="#b135">[135]</ref> which are produced from their respective disparity maps. Based on the Abs Rel metric in <ref type="table" target="#tab_7">Table 8</ref>, we may see that the network configuration with ψ 1 = 10 6 , ψ 2 = 10 6 and without the presence of the Gaussian filter is the one that generates the smallest errors, implying that the edge detector of the surface normals module is robust to noise in the input image.   Moreover, considering the errors and the accuracy of the methods exposed in <ref type="table" target="#tab_7">Table 8</ref>, we have that the geometric cues, provided by the surface normals ground truth and computed by the geometric loss function, support the DSN to retrieve 2.5D maps more consistent with the scene. The surface normals module has little influence on the total size of the framework and can be used in other methods that tackle SIDE problems. Only the prediction speed is negatively affected by the use of the aforementioned strategy.</p><p>An overview of the works that address the SIDE task and that evaluate its approaches in the KITTI Depth <ref type="bibr" target="#b16">[17]</ref> can be visualized in <ref type="table" target="#tab_8">Table 9</ref>. The results obtained by our pipeline are comparable to those generated by stateof-the-art methods with different types of training procedures. Furthermore, based on <ref type="table" target="#tab_0">Table 1</ref>, the proposed CNN has fewer trainable parameters and, due to its prediction speed, it may be applied to real self-driving scenarios. The depth maps retrieved by the DSN configuration of <ref type="table" target="#tab_8">Table 9</ref> are depicted in <ref type="figure">Fig. 3f</ref>. Compared to the methods of Figs. 3c, 3d and 3e, ours is able to better identify the shape of the objects that compose the scene, outputting more smoothly and well-outlined contours, even in regions close to occlusions. Also, in the depth maps of <ref type="figure">Fig. 3f</ref>, we may notice a higher amount of inferred artifacts, mainly in the more distant regions of the scene, which are more difficult to be comprehended by the other methods.</p><p>On the other hand, <ref type="figure">Fig. 4</ref> compares the predictions from our approach with the ones produced by recent supervised frameworks. As can be seen in such a figure, the DSN outputs high-definition depth maps with fewer blurred regions under the sky limit. Through <ref type="figure">Fig. 5</ref>, it is possible to verify that the 3D point clouds reconstructed from the predictions of the DSN have a denser distribution than the point clouds created with the sparse and noisy ground truth. The 3D maps from <ref type="figure">Fig. 5</ref> also show that our framework is robust to data noise and it is capable of modeling the shape of the objects with few errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.4.">Depth Completion Studies</head><p>To study the performance of the DSN on depth completion applications, we employed the sampling technique based on the categorical distribution of Eq. 19 to feed the CNN with RGBD maps. The results of such experiments, exposed in <ref type="table" target="#tab_0">Table 10</ref>, enforce the theory that the more Li-DAR points are presented to the network input, the more accurate the retrieved depth maps are. This can be visualized in the graphs of <ref type="figure">Fig. 6</ref>, in which the error metrics decrease and the precision metrics increase as more sparse depth data is added to the network input.</p><p>Based on <ref type="table" target="#tab_0">Table 10</ref> metrics, we can conclude that the 3D space cues yielded by the surface normals reference maps cause a slightly positive influence in the densification process. Moreover, the best performance of our depth completion pipeline is compared to the results from other approaches in the literature in <ref type="table" target="#tab_0">Table 11</ref>.</p><p>Although our CNN is capable of densifying depth maps at a high frame rate, it also generates dense maps with a quality that surpasses that of other state-of-the-art works. Our most accurate depth completion results are also depicted in <ref type="figure" target="#fig_5">Fig. 7</ref>, in which both the experiments regarding 200 and 500 sparse depth data, fed into the DSN, achieve high-quality dense depth maps.</p><p>From the graphs in <ref type="figure">Fig. 8</ref>, we analyze the effects caused on our depth completion CNN when it is fed with sparse data generated by the Monodepth2 pre-trained with the mono + stereo configuration. The behavior of the curve related to the Monodepth2 approach is increasing for the Abs Rel metric and decreasing for the δ &lt; 1.25 [%] since the more sparse data are sampled from the depth maps produced by Monodepth2, the more the errors of these maps influence the DSN results.</p><p>However, the graphs of <ref type="figure">Fig. 8</ref> also indicate that the performance of our network (without the surface normals module) is improved by using fewer amounts of depth data sampled from the Monodepth2 results. Therefore, the DSN may be successfully applied in real-world situations, where there is no ground truth available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.5.">Visual Odometry Results</head><p>Once depth data are estimated, they can be incorporated into VO/SLAM systems to improve the quality of visual odometry and the reconstructed map. With respect to Semi-Direct Visual Odometry (SVO) methods, the accurate depth information obtained from SIDE approaches can be added to enhance the invariance of the tracking algorithm to illumination changes in the input overexposed or underexposed images <ref type="bibr" target="#b4">[5]</ref>. Moreover, with the aid of such additional cues, the mapping points step of the SVO framework is performed with a smaller uncertainty, leading to an increase in the matching features accuracy and to a decrease in the convergence time of this feature correspondence mechanism <ref type="bibr" target="#b4">[5]</ref>.</p><p>CNN-SLAM and CNN-SVO <ref type="bibr" target="#b4">[5]</ref> are examples of works in which the incorporation of the depth maps, predicted by a deep neural network, into the processing pipeline is done successfully. More specifically, the former work used a ResNet-based FCN <ref type="bibr" target="#b28">[29]</ref>, whereas the latter one used the Monodepth <ref type="bibr" target="#b26">[27]</ref> to generate the depth estimates. Since the predictions from the DSN are better than those produced by the cited methods, we fed the CNN-SVO framework with our depth estimations to better determine corresponding features. Using different setups of our CNN, we evaluated the improvements that this modification in mapping the points of consecutive keyframes brings to the SVO task.</p><p>Particularly, we compared the results obtained by the joint application of the CNN-SVO and variations of the DSN with the CNN-SVO alone, the Direct Sparse Odometry (DSO) <ref type="bibr" target="#b179">[179]</ref>, the SVO <ref type="bibr" target="#b180">[180]</ref> and the ORB-SLAM without loop closure <ref type="bibr" target="#b181">[181]</ref> through <ref type="table" target="#tab_0">Table 12</ref>. Such experiments are conducted with the help of the 11 sequences with  <ref type="figure">Figure 3</ref>: Qualitative comparison between the predictions from our best approach (DSN †) and from recent self-supervised methods that address the SIDE task. The input images and the ground truth belong to the KITTI Depth dataset <ref type="bibr" target="#b16">[17]</ref>. The reference depth maps are interpolated for visualization purposes ground truth trajectories provided by the KITTI Odometry dataset <ref type="bibr" target="#b146">[146]</ref>. Also, we employed the Absolute Trajectory Error (ATE) and the RMSE metric to expose the aforementioned results. In <ref type="table" target="#tab_0">Table 12</ref>, the spaces that are not filled with metric values in meters are related to the techniques that are not able to compute the entire sequence due to failures in the tracking algorithm. As it is proposed in the work of Loo et al. <ref type="bibr" target="#b4">[5]</ref>, we stored the ATEs with a median of 5 runs. Analyzing the metrics from <ref type="table" target="#tab_0">Table 12</ref>, it is plausible to conclude that when the CNN-SVO technique, along with the Bundle Adjustment algorithm (BA), employs the depth maps retrieved by the DSN, it is capable of tracking most of the addressed sequences in a more precise way. This occurs due to the initialization of new map points in the CNN-SVO pipeline with lower depth uncertainties. Furthermore, the results from <ref type="table" target="#tab_0">Table 12</ref> also indicate that when the CNN-SVO is combined with the DSN configuration that leverages surface normals cues, it provides the best overall metric values.</p><p>The trajectories illustrated in <ref type="figure" target="#fig_7">Fig. 9</ref> are the aligned qualitative results of our best visual odometry approach in the KITTI Odometry sequences. Such trajectories confirm the metrics exposed in <ref type="table" target="#tab_0">Table 12</ref>. The scale drift values from <ref type="figure" target="#fig_0">Fig. 10</ref> show that our tracking results have a scale close to the ground truth. However, the small lags of scale occur due to the distortions suffered by the output depth maps of the DSN, which are resized to be applied in the CNN-SVO framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Indoor Ablation Studies 4.5.1. Structural Studies</head><p>To analyze the behavior of the DSN 3 when it is exposed to other types of scenes, we also conducted ablation studies with indoor datasets, which are composed of images with different scales than those present in the outdoor datasets. At first, we modified the loss function and the decoder structure of our framework to determine an optimized combination. Regarding the RMSE metric, Table      <ref type="figure">Figure 6</ref>: Errors and accuracy evolution of the DSN predictions according to the number of input sparse samples. We used the ground truth of the KITTI Depth dataset <ref type="bibr" target="#b16">[17]</ref> to present the results 13 indicates that both the attention term and the modification in the constant κ of the BerHu function do not bring benefits to the network predictions since, in comparison to the KITTI Depth <ref type="bibr" target="#b16">[17]</ref>, the NYU Depth V2 dataset <ref type="bibr" target="#b17">[18]</ref> is denser both in regions closer and further from the scene. Due to the denser profile of the ground truth of indoor datasets, simple loss functions, such as L + 1 and L + 2 , are capable of handling the pixel distribution of the reference maps and update the network weights in a faster and more efficient way.</p><p>As also can be inferred from <ref type="table" target="#tab_0">Table 13</ref>, the Pyramid 1 module enhances the value of most of the metrics. However, the block Pyramid 2 is not advantageous for the DSN 3 predictions once, for the NYU Depth V2 <ref type="bibr" target="#b17">[18]</ref>, the upsampled feature maps from the initial convolution of the  network decoder do not present the necessary monocular cues to support the estimation of the output depth map. Moreover, we utilized the semantic maps, with 13 fine annotated labels, of the NYU Depth V2 <ref type="bibr" target="#b17">[18]</ref> and SUN-RGBD <ref type="bibr" target="#b136">[136]</ref> datasets, as well as the depth ground truth of all indoor datasets covered in this work to pre-train the proposed framework. As in the experiments addressing outdoor data, we conclude that the pre-trained weights for semantic segmentation help our approach to reason about high-frequency regions of the image, yet the features learned from the pre-training of the DSN 3 on the same task leads to more accurate results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2.">Surface Normals Analysis</head><p>Considering indoor scenes, the results obtained from the experiments involving our approach that is trained with both depth and surface normals reference maps are presented in <ref type="table" target="#tab_0">Table 14</ref>. From the RMSE metric, it is possible to evidence that the application of the edge detector, implemented in the surface normals module, improves the results once it supports the proposed CNN in the process of predicting more accurate boundaries. The edge detection operation reduces the network's prediction speed by 1 fps and does not change its number of trainable parameters.</p><p>When the DSN 3 is pre-trained on the additional sur- Ground Truth Monodepth2 <ref type="figure">Figure 8</ref>: Comparison between the errors and precision obtained by our depth completion framework (without the surface normals module) when it is introduced to sparse data from the ground truth and the depth maps generated by the Monodepth2 <ref type="bibr" target="#b59">[60]</ref> face normals ground truth, with the help of the geometric 2.5D penalty, the results are also enhanced. This occurs since our learning-based method is introduced to a greater amount of 3D geometric features which help it to better generalize. Although the presence of the Gaussian filter leads to better values for the Sqr Rel and δ &lt; 1.25 3 met-   The sequences are taken from the KITTI Odometry dataset <ref type="bibr" target="#b146">[146]</ref> rics, its absence is related to the configuration of the proposed framework that generated the most accurate overall results. This proves the efficiency and robustness of the surface normals module, which can be successfully applied to our SIDE approach at a high frame rate.</p><p>Comparing the most accurate results of the DSN, on the NYU Depth V2 dataset <ref type="bibr" target="#b17">[18]</ref>, with the metrics obtained by different works in the state-of-the-art, <ref type="table" target="#tab_0">Table 15</ref> shows that our method reaches the top-3 of the benchmark ranking. However, among the approaches in the top-3, ours has the lowest runtime and total size, as shown in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>Also, we evaluated the predictions from the surface normals module of the DSN and compared the acquired results with the ones generated by recent works through <ref type="table" target="#tab_0">Table 16</ref>. Masking out the invalid pixels of the reference surface normals maps, we achieve the most accurate results presented in <ref type="table" target="#tab_0">Table 16</ref>.</p><p>On the other hand, considering all the pixels of the surface normals ground truth, we still get the best results of the Median and the accuracy metrics. Therefore, this demonstrates that our surface normals module is more advantageous than the other techniques since, besides not consisting of an entire branch or another model, it works mutually with the monocular depth estimation step.</p><p>Through the indoor scenes depicted in Figs. 11, 12 and 13, we visually compare the most accurate depth and surface normals predictions of the proposed CNN with those obtained by other supervised pipelines from the SIDE literature. <ref type="figure" target="#fig_0">By Fig. 11f</ref>, we can notice that our method is capable of better estimating the depth of finer objects and the geometric structure of the elements that compose the scenes, making the predictions closer to the ground truth.</p><p>Comparing the depth maps of <ref type="figure" target="#fig_0">Fig. 12b</ref> with those illustrated in <ref type="figure" target="#fig_0">Figs. 12b and 12c</ref>, it is viable to conclude that our results present greater details for both the closest and the most distant artifacts from the camera. Moreover, <ref type="figure" target="#fig_0">Fig. 13</ref> shows that the DSN completes the ground truth pixels without information and outputs surface normals vectors more consistent with the local and global layout of the scene.</p><p>The point clouds illustrated in Figs. 14c and 14e reveal that our predictions are denser than the reference point clouds and that the estimated pixels do not obey a distorted distribution of the 3D structure which portrays the environments exposed in <ref type="figure" target="#fig_0">Fig. 14a</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.3.">Depth Completion Studies</head><p>To analyze the performance of our depth completion framework in indoor scenarios, we conducted studies with the NYU Depth V2 dataset <ref type="bibr" target="#b17">[18]</ref> wherein we fed the DSN with sparse depth data, sampled from the ground truth, and RGB images to retrieve dense depth maps. The results of our CNN are presented in <ref type="table" target="#tab_0">Table 17</ref>. We may observe a similar behavior of the errors and accuracy metrics than those acquired in the outdoor studies with the same depth completion network. <ref type="figure" target="#fig_0">Fig. 15</ref> illustrates the DSNs response to the application of the increasing amount of sparse samples in its input.</p><p>We compare the results generated by the best DSN setup (DSN †) with the ones acquired from recent depth completion methods through <ref type="table" target="#tab_0">Table 18</ref>. Considering both Abs Rel and RMSE metrics, we achieve the top-2 in the benchmark ranking of <ref type="table" target="#tab_0">Table 18</ref>. However, the Non-local CSPN technique <ref type="bibr" target="#b113">[113]</ref> has a higher number of trainable parameters (25 M) and it applies a more complex feature extraction CNN (ResNet-34). <ref type="figure" target="#fig_0">Figure 16</ref> shows the predictions produced by the DSN and by state-of-the-art techniques that address the depth completion task. Through this figure, it is plausible to state that our results are the ones that come closest to the reference. We also present the surface normals maps estimated by the DSN which are visually consistent with the output depth maps. Furthermore, the metrics presented in <ref type="table" target="#tab_0">Table 19</ref> indicate that, as with depth predictions, the quality of the surface normals maps increases as more sparse depth data is added to the network's input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>Through the conducted surveys, we can imply that the monocular depth estimation and depth completion areas have significantly advanced in recent years due to the emergence of Deep Learning methods. These methods may overcome the performance of classical Computer Vision techniques and also generate better predictions for other fundamental tasks such as semantic segmentation, surface normals estimation and VO.</p><p>Particularly, considering all the experimental analysis, we designed a lightweight CNN architecture, whose number of trainable parameters can vary from 2 M up to 12 M. Our framework can also be used in real-world applications since it achieves prediction speeds from 32 fps up to 88 fps.</p><p>For the tests involving outdoor scenarios, the Dens-eSIDENet configuration that includes the proposed surface normals module, the 2.5D geometric loss function, the pyramid block and the pre-training with the cityscapes <ref type="bibr" target="#b135">[135]</ref> dataset provided results in the state-of-the-art for the SIDE (Abs Rel = 0.075), depth completion (Abs Rel = 0.019) and VO (seq00 RM SE = 16.9438) tasks, regarding the KITTI Benchmark datasets <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b146">146,</ref><ref type="bibr" target="#b156">156]</ref>.</p><p>On the other hand, for the experiments involving indoor environments, the same DSN setup aforementioned, with the exception of the pre-training step that is conducted with the aid of indoor datasets <ref type="bibr" target="#b136">[136,</ref><ref type="bibr" target="#b137">137,</ref><ref type="bibr" target="#b138">138]</ref>, produced promising results for the single-view depth prediction (RM SE = 0.429), depth completion (RM SE = 0.102) and surface normals estimation (RM SE = 12.2) tasks on the NYU Benchmark datasets <ref type="bibr" target="#b17">[18]</ref>.</p><p>Therefore, we were able to determine optimal training conditions, using different deep learning techniques, as well as optimized network structures that enabled the generation of high-quality predictions in reduced processing time. As future work, we intend to employ the developed CNNs  <ref type="bibr" target="#b25">[26]</ref> (e) SharpNet <ref type="bibr" target="#b2">[3]</ref> (f) DSN † <ref type="figure" target="#fig_0">Figure 11</ref>: Comparison between the depth predictions of indoor scenes from our SIDE pipeline and other supervised methods in the state-of-the-art. The DSN † approach is related to the DSN configuration that produces the most accurate overall results. The input images and the ground truth come from the NYU Depth V2 dataset <ref type="bibr" target="#b17">[18]</ref>. The ground truth is interpolated for visualization purposes for other tasks such as robotic grasping and visual servoing control.</p><p>(a) RGB Input (b) DORN <ref type="bibr" target="#b25">[26]</ref> (c) DenseDepth <ref type="bibr" target="#b14">[15]</ref> (d) DSN † <ref type="figure" target="#fig_0">Figure 12</ref>: Detailed evaluation of the predictions of our method and the ones retrieved by recent supervised approaches. DSN † refers to the best DSN setup and the RGB inputs compose the NYU Depth V2 dataset <ref type="bibr" target="#b17">[18]</ref> (a) RGB Input (b) Ground Truth (c) MS-CNN <ref type="bibr" target="#b47">[48]</ref> (d) SkipNet <ref type="bibr" target="#b147">[147]</ref> (e) Qi et al. <ref type="bibr" target="#b54">[55]</ref> (f) PAP <ref type="bibr" target="#b55">[56]</ref> (g) DSN † <ref type="figure" target="#fig_0">Figure 13</ref>: Qualitative comparison of the surface normals maps retrieved by our framework and by state-of-the-art techniques. DSN † corresponds to the best DSN configuration. The RGB inputs are part of the NYU Depth V2 dataset <ref type="bibr" target="#b17">[18]</ref>. We produced the ground truth images through the method of Fouhey et al. <ref type="bibr" target="#b52">[53]</ref> (a) RGB input (b) Depth GT (c) Depth prediction (d) SN GT (e) SN prediction <ref type="figure" target="#fig_0">Figure 14</ref>: Qualitative evaluation of the point clouds reconstructed from the most accurate DSN predictions. The input images and the depth ground truth compose the KITTI Depth dataset <ref type="bibr" target="#b16">[17]</ref>. However, we created the reference surface normals data through the algorithm developed by Fouhey et al. <ref type="bibr" target="#b52">[53]</ref>      <ref type="figure" target="#fig_0">Figure 15</ref>: Evolution of the depth completion metrics obtained by the DSN. We employed the reference depth maps of the NYU Depth V2 dataset <ref type="bibr" target="#b17">[18]</ref> to plot the results  <ref type="figure" target="#fig_0">Figure 16</ref>: Comparison between the dense depth maps retrieved by our best depth completion CNN (DSN †) and by other methods in the literature. We also present the surface normals maps estimated by the DSN †. The NYU Depth V2 <ref type="bibr" target="#b17">[18]</ref> is employed by all approaches. Legend: RGB -input image; 500d -sparse input data; GT -sparse ground truth</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Proposed baseline of the DenseSIDENet</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Modified structure of the CNN proposed by Ribeiro et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc><ref type="bibr" target="#b6">[7]</ref>, ϕ 1 = d d * and ϕ 0 = 1− d d * wherein d and d * represent the desired amount of samples and the amount of valid data available per depth map respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Differences between the depth maps retrieved by the best DSN configuration (DSN †) and by SIDE approaches in the state-of-the-art with a supervised training pipeline. The input RGB images are part of the KITTI Depth dataset<ref type="bibr" target="#b16">[17]</ref> (a) RGB Input(b) Ground Truth (c) DSN † (d) GroundTruth (e) DSN † Qualitative analysis of the point clouds reconstructed from the DSN predictions. DSN † indicates the DSN setup that leads to the most accurate overall metrics. The RGB input and the reference point clouds with depth data are taken from the KITTI Depth dataset<ref type="bibr" target="#b16">[17]</ref>. However, the reference surface normals information are generated by the authors with the aid of the algorithm introduced by Fouhey et al.<ref type="bibr" target="#b52">[53]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Qualitative evaluation of our depth completion framework based on the noisy and sparse ground truth of the KITTI Depth dataset<ref type="bibr" target="#b16">[17]</ref>. Legend: Input -input RGB image; GT -sparse ground truth; d -sparse input data</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Qualitative results of the camera trajectory estimations obtained on the KITTI Odometry<ref type="bibr" target="#b146">[146]</ref> sequences 00, 02, 05, 07, 08 and 10 by the CNN-SVO + BA jointly with the DSN and the surface normals module. The black dotted and solid blue trajectories represent the ground truth and the predictions respectively</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>Scale drift profile of the tracking results generated by the CNN-SVO + BA + DSN (with the surface normals module).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Specifications of SIDE state-of-the-art works. The symbol † indicates the total number of trainable parameters of the encoder, while ‡ denotes the total size of the framework</figDesc><table><row><cell>Method</cell><cell>Dataset</cell><cell>GPU</cell><cell>Network encoder</cell><cell cols="2">Parameters Prediction speed</cell></row><row><cell>BTS [13]</cell><cell>KITTI Depth / NYU Depth V2</cell><cell>NVIDIA GTX 1080 Ti</cell><cell>DenseNet-161</cell><cell>47 M ‡</cell><cell>16 f ps</cell></row><row><cell>DORN [26]</cell><cell>KITTI Depth / Make3D / NYU Depth V2</cell><cell>NVIDIA Titan X Pascal</cell><cell>ResNet-101</cell><cell>110 M ‡</cell><cell>2 f ps</cell></row><row><cell>VNL [11]</cell><cell>KITTI Depth / NYU Depth V2</cell><cell cols="2">Huawei GPU-accelerated Cloud Server ResNext-101 (32 × 4d)</cell><cell>44 M †</cell><cell>2 f ps</cell></row><row><cell>DenseDepth [15]</cell><cell>KITTI Depth / NYU Depth V2</cell><cell>NVIDIA TITAN Xp</cell><cell>DenseNet-169</cell><cell>42 M ‡</cell><cell>-</cell></row><row><cell>ACAN [42]</cell><cell>KITTI Depth / NYU Depth V2</cell><cell>NVIDIA GTX 1080 Ti</cell><cell>ResNet-101</cell><cell>44 M †</cell><cell>-</cell></row><row><cell>PAP-Depth [56]</cell><cell>KITTI Depth / SUNRGBD / NYU Depth V2</cell><cell>NVIDIA P40</cell><cell>ResNet-50</cell><cell>25 M †</cell><cell>5 f ps</cell></row><row><cell>DABC [123]</cell><cell>ScanNet / KITTI Depth</cell><cell>NVIDIA GTX 1080</cell><cell>ResNext-101 (64 × 4d)</cell><cell>-</cell><cell>1 f ps</cell></row><row><cell>APMoE [124]</cell><cell>NYU Depth V2 / BSDS500 / Stanford-2D-3D / Cityscapes</cell><cell>NVIDIA Titan X</cell><cell>ResNest-50</cell><cell>25 M †</cell><cell>5 f ps</cell></row><row><cell>CSWS [125]</cell><cell>NYU Depth V2 / KITTI Depth</cell><cell>NVIDIA Tesla Titan X</cell><cell>ResNet-152</cell><cell>60 M †</cell><cell>5 f ps</cell></row><row><cell>Cao et al. [34]</cell><cell>NYU Depth V2 / KITTI Depth / SUNRGBD</cell><cell>-</cell><cell>ResNet-152</cell><cell>82 M ‡</cell><cell>-</cell></row><row><cell>Kuznietsov et al. [61]</cell><cell>KITTI Depth</cell><cell>NVIDIA GTX 980 Ti</cell><cell>ResNet-50</cell><cell>80 M ‡</cell><cell>-</cell></row><row><cell>LSIM [126]</cell><cell>KITTI Depth</cell><cell>NVIDIA Titan X</cell><cell>ResNet-50</cell><cell>25 M †</cell><cell>12 f ps</cell></row><row><cell>DHGRL [127]</cell><cell>KITTI Depth / Make3D / NYU Depth V2</cell><cell>NVIDIA K80</cell><cell>ResNet-50</cell><cell>25 M †</cell><cell>5 f ps</cell></row><row><cell>SDNet [128]</cell><cell>KITTI Depth / Cityscapes</cell><cell>NVIDIA Titan Xp</cell><cell>ResNet-50</cell><cell>25 M †</cell><cell>5 f ps</cell></row><row><cell>monoResMatch [67]</cell><cell>KITTI Depth / Cityscapes</cell><cell>NVIDIA 2080 Ti</cell><cell>monoResMatch</cell><cell>42 M ‡</cell><cell>29 f ps</cell></row><row><cell>monoResMatch [67]</cell><cell>KITTI Depth / Cityscapes</cell><cell>Jetson TX2</cell><cell>monoResMatch</cell><cell>42 M ‡</cell><cell>1 f ps</cell></row><row><cell>3Net [71]</cell><cell>KITTI Depth / Cityscapes</cell><cell>NVIDIA 2080 Ti</cell><cell>ResNet-50</cell><cell>25 M †</cell><cell>49 f ps</cell></row><row><cell>3Net [71]</cell><cell>KITTI Depth / Cityscapes</cell><cell>Jetson TX2</cell><cell>ResNet-50</cell><cell>25 M †</cell><cell>2 f ps</cell></row><row><cell>VOMonodepth [64]</cell><cell>KITTI Depth</cell><cell>NVIDIA 2080 Ti</cell><cell>ResNet-50</cell><cell>25 M †</cell><cell>41 f ps</cell></row><row><cell>VOMonodepth [64]</cell><cell>KITTI Depth</cell><cell>Jetson TX2</cell><cell>ResNet-50</cell><cell>25 M †</cell><cell>1 f ps</cell></row><row><cell>Monodepth [27]</cell><cell>KITTI Depth / Make3D</cell><cell>NVIDIA Titan X</cell><cell>ResNet-50</cell><cell>31 M ‡</cell><cell>28 f ps</cell></row><row><cell>Monodepth2 [60]</cell><cell>KITTI Depth</cell><cell>NVIDIA Titan X</cell><cell>ResNet-18</cell><cell>14 M ‡</cell><cell>-</cell></row><row><cell>SA-Attention [129]</cell><cell>KITTI Depth / Make3D</cell><cell>-</cell><cell>ResNet-50</cell><cell>34 M ‡</cell><cell>-</cell></row><row><cell>Struct2depth [88]</cell><cell>KITTI Depth / Cityscapes / Fetch Indoor Navigation</cell><cell>GTX 1080Ti</cell><cell>Struct2depth</cell><cell>14 M ‡</cell><cell>30 f ps</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparison of state-of-the-art depth completion methods according to the KITTI Benchmark<ref type="bibr" target="#b16">[17]</ref> metrics (iM AE = 1</figDesc><table><row><cell cols="2">and iRM SE =</cell><cell>1 RM SE )</cell><cell></cell><cell>M AE</cell></row><row><cell>Method</cell><cell cols="4">MAE↓ RMSE↓ iMAE↓ iRMSE↓</cell></row><row><cell>SGDU [105]</cell><cell cols="2">605.470 2312.570</cell><cell>2.050</cell><cell>7.380</cell></row><row><cell>NN+CNN [17]</cell><cell cols="2">416.140 1419.750</cell><cell>1.290</cell><cell>3.250</cell></row><row><cell>ADNN [107]</cell><cell cols="2">439.480 1325.370</cell><cell>3.190</cell><cell>59.390</cell></row><row><cell>IP-Basic [104]</cell><cell cols="2">302.600 1288.460</cell><cell>1.290</cell><cell>3.780</cell></row><row><cell>DFuseNet [149]</cell><cell cols="2">429.930 1206.660</cell><cell>1.790</cell><cell>3.620</cell></row><row><cell>VOICED [150]</cell><cell cols="2">299.410 1169.970</cell><cell>1.200</cell><cell>3.560</cell></row><row><cell>Morph-Net [117]</cell><cell cols="2">310.490 1045.450</cell><cell>1.570</cell><cell>3.840</cell></row><row><cell>CSPN [112]</cell><cell cols="2">279.460 1019.640</cell><cell>1.150</cell><cell>2.930</cell></row><row><cell>DCrgb 80b 3coef [151]</cell><cell>215.750</cell><cell>965.870</cell><cell>0.980</cell><cell>2.430</cell></row><row><cell>Conf-Net [152]</cell><cell>257.540</cell><cell>962.280</cell><cell>1.090</cell><cell>3.100</cell></row><row><cell>DFineNet [153]</cell><cell>304.170</cell><cell>943.890</cell><cell>1.390</cell><cell>3.210</cell></row><row><cell>Spade-RGBsD [154]</cell><cell>234.810</cell><cell>917.640</cell><cell>0.950</cell><cell>2.170</cell></row><row><cell>IR L2 [110]</cell><cell>292.360</cell><cell>901.430</cell><cell>1.350</cell><cell>4.920</cell></row><row><cell>DDP [155]</cell><cell>203.960</cell><cell>832.940</cell><cell>0.850</cell><cell>2.100</cell></row><row><cell>NConv [106]</cell><cell>233.260</cell><cell>829.980</cell><cell>1.030</cell><cell>2.600</cell></row><row><cell>Sparse-to-Dense [9]</cell><cell>249.950</cell><cell>814.730</cell><cell>1.210</cell><cell>2.800</cell></row><row><cell>CrossGuidance [120]</cell><cell>253.980</cell><cell>807.420</cell><cell>1.330</cell><cell>2.730</cell></row><row><cell cols="2">Revisiting NN+CNN [109] 225.810</cell><cell>792.800</cell><cell>0.990</cell><cell>2.420</cell></row><row><cell>PwP [118]</cell><cell>235.170</cell><cell>777.050</cell><cell>1.130</cell><cell>2.420</cell></row><row><cell>RGB guide&amp;certainty [8]</cell><cell>215.020</cell><cell>772.870</cell><cell>0.930</cell><cell>2.190</cell></row><row><cell>DSPN [114]</cell><cell>220.360</cell><cell>766.740</cell><cell>1.030</cell><cell>2.470</cell></row><row><cell>MSG-CHN [116]</cell><cell>220.410</cell><cell>762.190</cell><cell>0.980</cell><cell>2.300</cell></row><row><cell>DeepLiDAR [103]</cell><cell>226.500</cell><cell>758.380</cell><cell>1.150</cell><cell>2.560</cell></row><row><cell>UberATG-FuseNet [115]</cell><cell>221.190</cell><cell>752.880</cell><cell>1.140</cell><cell>2.340</cell></row><row><cell>CSPN++ [96]</cell><cell>209.280</cell><cell>743.690</cell><cell>0.900</cell><cell>2.070</cell></row><row><cell>NLSPN [113]</cell><cell>199.590</cell><cell>741.680</cell><cell>0.840</cell><cell>1.990</cell></row><row><cell>GuideNet [111]</cell><cell>218.830</cell><cell>736.240</cell><cell>0.990</cell><cell>2.250</cell></row><row><cell>Closing (k = 3, it = 5)</cell><cell>148.963</cell><cell>519.606</cell><cell>0.597</cell><cell>1.082</cell></row><row><cell>Closing (k = 3, it = 4)</cell><cell>129.353</cell><cell>457.346</cell><cell>0.533</cell><cell>0.972</cell></row><row><cell>Closing (k = 3, it = 3)</cell><cell>106.925</cell><cell>391.138</cell><cell>0.458</cell><cell>0.867</cell></row><row><cell>Closing (k = 3, it = 2)</cell><cell>82.137</cell><cell>326.323</cell><cell>0.363</cell><cell>0.747</cell></row><row><cell>Closing (k = 3, it = 1)</cell><cell cols="2">46.724 226.214</cell><cell>0.211</cell><cell>0.541</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Analysis of the densification degrees of the datasets used according to the average number and average percentage of valid pixels. 375 × 1242 is the typical resolution for the KITTI Depth<ref type="bibr" target="#b16">[17]</ref> and 352 × 704 is the image size that is fed into our framework</figDesc><table><row><cell></cell><cell cols="2">Average number</cell><cell cols="2">Average percentage</cell></row><row><cell>Dataset</cell><cell cols="2">of valid pixels</cell><cell cols="2">of valid pixels</cell></row><row><cell></cell><cell cols="4">375 × 1242 352 × 704 375 × 1242 352 × 704</cell></row><row><cell>KITTI Depth</cell><cell>75407</cell><cell>43649</cell><cell>16.190%</cell><cell>17.614%</cell></row><row><cell>KITTI Morphological (k = 3, it = 1)</cell><cell>142309</cell><cell>82377</cell><cell>30.555%</cell><cell>33.242%</cell></row><row><cell>KITTI Morphological (k = 3, it = 2)</cell><cell>166765</cell><cell>96525</cell><cell>35.806%</cell><cell>38.951%</cell></row><row><cell>KITTI Morphological (k = 3, it = 3)</cell><cell>182949</cell><cell>105884</cell><cell>39.280%</cell><cell>42.728%</cell></row><row><cell>KITTI Morphological (k = 3, it = 4)</cell><cell>195587</cell><cell>113172</cell><cell>41.994%</cell><cell>45.669%</cell></row><row><cell>KITTI Morphological (k = 3, it = 5)</cell><cell>205478</cell><cell>118875</cell><cell>44.118%</cell><cell>47.971%</cell></row><row><cell>KITTI Continuous</cell><cell>291489</cell><cell>158255</cell><cell>62.585%</cell><cell>63.862%</cell></row><row><cell>KITTI Completed</cell><cell>428031</cell><cell>247807</cell><cell>91.901%</cell><cell>100%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Evaluation of the proposed method considering variations in the loss function and the dataset used during training. The DenseNet-121 is fixed as the feature extraction network of the DSN Sqr Rel↓ RMSE↓ RMSE (log)↓ SILog↓ log10↓ δ &lt; 1.25↑ δ &lt; 1.25 2 ↑ δ &lt; 1.25 3 ↑</figDesc><table><row><cell cols="5">Method Abs Rel↓ DenseNet-121 Loss Dataset Cap L2 KITTI Depth 0 − 80 m 0.114</cell><cell>0.663</cell><cell>4.367</cell><cell>0.173</cell><cell>15.890</cell><cell>0.050</cell><cell>0.853</cell><cell>0.963</cell><cell>0.989</cell></row><row><cell>DenseNet-121</cell><cell>L1</cell><cell>KITTI Depth</cell><cell>0 − 80 m</cell><cell>0.108</cell><cell>0.624</cell><cell>4.200</cell><cell>0.166</cell><cell>15.294</cell><cell>0.047</cell><cell>0.868</cell><cell>0.965</cell><cell>0.990</cell></row><row><cell cols="2">DenseNet-121 Lcharbo</cell><cell>KITTI Depth</cell><cell>0 − 80 m</cell><cell>0.108</cell><cell>0.639</cell><cell>4.255</cell><cell>0.167</cell><cell>15.323</cell><cell>0.047</cell><cell>0.866</cell><cell>0.966</cell><cell>0.989</cell></row><row><cell cols="2">DenseNet-121 LHuber</cell><cell>KITTI Depth</cell><cell>0 − 80 m</cell><cell>0.099</cell><cell>0.538</cell><cell>4.076</cell><cell>0.154</cell><cell>14.308</cell><cell>0.044</cell><cell>0.883</cell><cell>0.972</cell><cell>0.992</cell></row><row><cell cols="2">DenseNet-121 LSILog</cell><cell>KITTI Depth</cell><cell>0 − 80 m</cell><cell>0.097</cell><cell>0.513</cell><cell>3.939</cell><cell>0.150</cell><cell>13.569</cell><cell>0.043</cell><cell>0.890</cell><cell>0.975</cell><cell>0.993</cell></row><row><cell>DenseNet-121</cell><cell>Lcosh</cell><cell>KITTI Depth</cell><cell>0 − 80 m</cell><cell>0.096</cell><cell>0.518</cell><cell>3.855</cell><cell>0.150</cell><cell>13.800</cell><cell>0.042</cell><cell>0.892</cell><cell>0.974</cell><cell>0.993</cell></row><row><cell cols="2">DenseNet-121 L + SILog</cell><cell>KITTI Depth</cell><cell>0 − 80 m</cell><cell>0.095</cell><cell>0.494</cell><cell>3.856</cell><cell>0.146</cell><cell>13.214</cell><cell>0.042</cell><cell>0.894</cell><cell>0.976</cell><cell>0.994</cell></row><row><cell cols="2">DenseNet-121 LBerHu</cell><cell>KITTI Depth</cell><cell>0 − 80 m</cell><cell>0.089</cell><cell>0.478</cell><cell>3.788</cell><cell>0.142</cell><cell>13.004</cell><cell>0.040</cell><cell>0.903</cell><cell>0.977</cell><cell>0.994</cell></row><row><cell cols="4">DenseNet-121 LBerHu KITTI Morphological (k = 3, it = 1) 0 − 80 m</cell><cell>0.088</cell><cell>0.471</cell><cell>3.792</cell><cell>0.141</cell><cell>12.750</cell><cell>0.040</cell><cell>0.903</cell><cell>0.979</cell><cell>0.994</cell></row><row><cell cols="4">DenseNet-121 LBerHu KITTI Morphological (k = 3, it = 2) 0 − 80 m</cell><cell>0.088</cell><cell>0.461</cell><cell>3.780</cell><cell>0.140</cell><cell>12.747</cell><cell>0.040</cell><cell>0.905</cell><cell>0.978</cell><cell>0.994</cell></row><row><cell cols="4">DenseNet-121 LBerHu KITTI Morphological (k = 3, it = 3) 0 − 80 m</cell><cell>0.088</cell><cell>0.458</cell><cell>3.800</cell><cell>0.142</cell><cell>12.860</cell><cell>0.040</cell><cell>0.903</cell><cell>0.977</cell><cell>0.994</cell></row><row><cell cols="4">DenseNet-121 LBerHu KITTI Morphological (k = 3, it = 4) 0 − 80 m</cell><cell>0.088</cell><cell>0.479</cell><cell>3.855</cell><cell>0.143</cell><cell>13.024</cell><cell>0.040</cell><cell>0.904</cell><cell>0.976</cell><cell>0.993</cell></row><row><cell cols="4">DenseNet-121 LBerHu KITTI Morphological (k = 3, it = 5) 0 − 80 m</cell><cell>0.088</cell><cell>0.474</cell><cell>3.881</cell><cell>0.143</cell><cell>13.053</cell><cell>0.040</cell><cell>0.903</cell><cell>0.977</cell><cell>0.994</cell></row><row><cell cols="2">DenseNet-121 LBerHu</cell><cell>KITTI Continuous</cell><cell>0 − 80 m</cell><cell>1.640</cell><cell>36.498</cell><cell>24.577</cell><cell>0.978</cell><cell>21.627</cell><cell>0.415</cell><cell>0.013</cell><cell>0.036</cell><cell>0.072</cell></row><row><cell cols="2">DenseNet-121 LBerHu</cell><cell>KITTI Completed</cell><cell>0 − 80 m</cell><cell>0.220</cell><cell>1.741</cell><cell>6.269</cell><cell>0.249</cell><cell>20.577</cell><cell>0.084</cell><cell>0.698</cell><cell>0.924</cell><cell>0.980</cell></row><row><cell>DenseNet-121</cell><cell>L2</cell><cell>KITTI Depth</cell><cell>0 − 50 m</cell><cell>0.110</cell><cell>0.509</cell><cell>3.203</cell><cell>0.161</cell><cell>14.807</cell><cell>0.047</cell><cell>0.867</cell><cell>0.970</cell><cell>0.992</cell></row><row><cell>DenseNet-121</cell><cell>L1</cell><cell>KITTI Depth</cell><cell>0 − 50 m</cell><cell>0.104</cell><cell>0.480</cell><cell>3.102</cell><cell>0.155</cell><cell>14.293</cell><cell>0.045</cell><cell>0.881</cell><cell>0.970</cell><cell>0.992</cell></row><row><cell cols="2">DenseNet-121 Lcharbo</cell><cell>KITTI Depth</cell><cell>0 − 50 m</cell><cell>0.103</cell><cell>0.487</cell><cell>3.122</cell><cell>0.156</cell><cell>14.277</cell><cell>0.045</cell><cell>0.879</cell><cell>0.972</cell><cell>0.991</cell></row><row><cell cols="2">DenseNet-121 LHuber</cell><cell>KITTI Depth</cell><cell>0 − 50 m</cell><cell>0.094</cell><cell>0.393</cell><cell>2.892</cell><cell>0.142</cell><cell>13.165</cell><cell>0.041</cell><cell>0.897</cell><cell>0.978</cell><cell>0.994</cell></row><row><cell cols="2">DenseNet-121 LSILog</cell><cell>KITTI Depth</cell><cell>0 − 50 m</cell><cell>0.093</cell><cell>0.377</cell><cell>2.833</cell><cell>0.139</cell><cell>12.542</cell><cell>0.041</cell><cell>0.904</cell><cell>0.980</cell><cell>0.995</cell></row><row><cell>DenseNet-121</cell><cell>Lcosh</cell><cell>KITTI Depth</cell><cell>0 − 50 m</cell><cell>0.092</cell><cell>0.390</cell><cell>2.817</cell><cell>0.140</cell><cell>12.839</cell><cell>0.040</cell><cell>0.904</cell><cell>0.979</cell><cell>0.994</cell></row><row><cell cols="2">DenseNet-121 L + SILog</cell><cell>KITTI Depth</cell><cell>0 − 50 m</cell><cell>0.090</cell><cell>0.365</cell><cell>2.771</cell><cell>0.136</cell><cell>12.229</cell><cell>0.040</cell><cell>0.907</cell><cell>0.981</cell><cell>0.996</cell></row><row><cell cols="2">DenseNet-121 LBerHu</cell><cell>KITTI Depth</cell><cell>0 − 50 m</cell><cell>0.085</cell><cell>0.352</cell><cell>2.721</cell><cell>0.131</cell><cell>11.981</cell><cell>0.037</cell><cell>0.916</cell><cell>0.982</cell><cell>0.995</cell></row><row><cell cols="4">DenseNet-121 LBerHu KITTI Morphological (k = 3, it = 1) 0 − 50 m</cell><cell>0.084</cell><cell>0.342</cell><cell>2.732</cell><cell>0.130</cell><cell>11.723</cell><cell>0.037</cell><cell>0.916</cell><cell>0.983</cell><cell>0.996</cell></row><row><cell cols="4">DenseNet-121 LBerHu KITTI Morphological (k = 3, it = 2) 0 − 50 m</cell><cell>0.083</cell><cell>0.335</cell><cell>2.701</cell><cell>0.129</cell><cell>11.722</cell><cell>0.037</cell><cell>0.919</cell><cell>0.983</cell><cell>0.996</cell></row><row><cell cols="4">DenseNet-121 LBerHu KITTI Morphological (k = 3, it = 3) 0 − 50 m</cell><cell>0.083</cell><cell>0.331</cell><cell>2.688</cell><cell>0.130</cell><cell>11.784</cell><cell>0.037</cell><cell>0.916</cell><cell>0.982</cell><cell>0.995</cell></row><row><cell cols="4">DenseNet-121 LBerHu KITTI Morphological (k = 3, it = 4) 0 − 50 m</cell><cell>0.083</cell><cell>0.345</cell><cell>2.729</cell><cell>0.131</cell><cell>11.906</cell><cell>0.037</cell><cell>0.917</cell><cell>0.982</cell><cell>0.995</cell></row><row><cell cols="4">DenseNet-121 LBerHu KITTI Morphological (k = 3, it = 5) 0 − 50 m</cell><cell>0.083</cell><cell>0.338</cell><cell>2.744</cell><cell>0.131</cell><cell>11.912</cell><cell>0.037</cell><cell>0.916</cell><cell>0.983</cell><cell>0.995</cell></row><row><cell cols="2">DenseNet-121 LBerHu</cell><cell>KITTI Continuous</cell><cell>0 − 50 m</cell><cell>1.545</cell><cell>28.857</cell><cell>19.418</cell><cell>0.939</cell><cell>23.091</cell><cell>0.395</cell><cell>0.029</cell><cell>0.069</cell><cell>0.128</cell></row><row><cell cols="2">DenseNet-121 LBerHu</cell><cell>KITTI Completed</cell><cell>0 − 50 m</cell><cell>0.215</cell><cell>1.454</cell><cell>5.163</cell><cell>0.240</cell><cell>19.124</cell><cell>0.082</cell><cell>0.709</cell><cell>0.933</cell><cell>0.983</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table><row><cell>Fig. 2 CNN</cell><cell>256</cell><cell cols="2">3 M 88 f ps 0 − 80 m</cell><cell>0.102</cell><cell>0.594</cell><cell>4.094</cell><cell>0.160</cell><cell>14.797</cell><cell>0.045</cell><cell>0.876</cell><cell>0.968</cell><cell>0.991</cell></row><row><cell>Model 1</cell><cell>256</cell><cell>3 M</cell><cell>59 f ps 0 − 80 m</cell><cell>0.093</cell><cell>0.517</cell><cell>3.941</cell><cell>0.149</cell><cell>13.776</cell><cell>0.041</cell><cell>0.895</cell><cell>0.974</cell><cell>0.992</cell></row><row><cell>Model 2</cell><cell>256</cell><cell>6 M</cell><cell>57 f ps 0 − 80 m</cell><cell>0.091</cell><cell>0.501</cell><cell>3.851</cell><cell>0.146</cell><cell>13.243</cell><cell>0.041</cell><cell>0.901</cell><cell>0.977</cell><cell>0.993</cell></row><row><cell>Model 3</cell><cell>256</cell><cell cols="2">12 M 50 f ps 0 − 80 m</cell><cell>0.083</cell><cell>0.434</cell><cell>3.646</cell><cell>0.135</cell><cell>12.326</cell><cell>0.037</cell><cell>0.914</cell><cell>0.980</cell><cell>0.994</cell></row><row><cell>MobileNetV2-50</cell><cell>128</cell><cell>2 M</cell><cell>87 f ps 0 − 80 m</cell><cell>0.122</cell><cell>0.760</cell><cell>4.612</cell><cell>0.184</cell><cell>17.074</cell><cell>0.052</cell><cell>0.842</cell><cell>0.955</cell><cell>0.986</cell></row><row><cell>MobileNetV2-50</cell><cell>256</cell><cell>5 M</cell><cell>81 f ps 0 − 80 m</cell><cell>0.116</cell><cell>0.729</cell><cell>4.508</cell><cell>0.178</cell><cell>16.326</cell><cell>0.051</cell><cell>0.853</cell><cell>0.959</cell><cell>0.987</cell></row><row><cell>MobileNetV2-140</cell><cell>256</cell><cell cols="2">10 M 62 f ps 0 − 80 m</cell><cell>0.104</cell><cell>0.596</cell><cell>4.084</cell><cell>0.161</cell><cell>14.799</cell><cell>0.045</cell><cell>0.877</cell><cell>0.969</cell><cell>0.991</cell></row><row><cell>MobileNetV2-140</cell><cell>512</cell><cell cols="2">20 M 39 f ps 0 − 80 m</cell><cell>0.100</cell><cell>0.575</cell><cell>4.065</cell><cell>0.158</cell><cell>14.206</cell><cell>0.044</cell><cell>0.883</cell><cell>0.970</cell><cell>0.991</cell></row><row><cell>DenseNet-121</cell><cell>256</cell><cell cols="2">12 M 41 f ps 0 − 80 m</cell><cell>0.088</cell><cell>0.461</cell><cell>3.780</cell><cell>0.140</cell><cell>12.747</cell><cell>0.040</cell><cell>0.905</cell><cell>0.978</cell><cell>0.994</cell></row><row><cell>Fig. 2 CNN</cell><cell>256</cell><cell cols="2">3 M 88 f ps 0 − 50 m</cell><cell>0.098</cell><cell>0.456</cell><cell>3.030</cell><cell>0.150</cell><cell>13.796</cell><cell>0.043</cell><cell>0.888</cell><cell>0.974</cell><cell>0.993</cell></row><row><cell>Model 1</cell><cell>256</cell><cell>3 M</cell><cell>59 f ps 0 − 50 m</cell><cell>0.089</cell><cell>0.380</cell><cell>2.843</cell><cell>0.138</cell><cell>12.712</cell><cell>0.039</cell><cell>0.907</cell><cell>0.980</cell><cell>0.994</cell></row><row><cell>Model 2</cell><cell>256</cell><cell>6 M</cell><cell>57 f ps 0 − 50 m</cell><cell>0.086</cell><cell>0.370</cell><cell>2.789</cell><cell>0.134</cell><cell>12.186</cell><cell>0.038</cell><cell>0.914</cell><cell>0.981</cell><cell>0.994</cell></row><row><cell>Model 3</cell><cell>256</cell><cell cols="2">12 M 50 f ps 0 − 50 m</cell><cell>0.079</cell><cell>0.314</cell><cell>2.592</cell><cell>0.124</cell><cell>11.315</cell><cell>0.035</cell><cell>0.926</cell><cell>0.984</cell><cell>0.996</cell></row><row><cell>MobileNetV2-50</cell><cell>128</cell><cell>2 M</cell><cell>87 f ps 0 − 50 m</cell><cell>0.117</cell><cell>0.592</cell><cell>3.431</cell><cell>0.172</cell><cell>15.892</cell><cell>0.049</cell><cell>0.856</cell><cell>0.961</cell><cell>0.989</cell></row><row><cell>MobileNetV2-50</cell><cell>256</cell><cell>5 M</cell><cell>81 f ps 0 − 50 m</cell><cell>0.111</cell><cell>0.560</cell><cell>3.339</cell><cell>0.166</cell><cell>15.139</cell><cell>0.048</cell><cell>0.866</cell><cell>0.965</cell><cell>0.989</cell></row><row><cell>MobileNetV2-140</cell><cell>256</cell><cell cols="2">10 M 62 f ps 0 − 50 m</cell><cell>0.100</cell><cell>0.463</cell><cell>3.044</cell><cell>0.151</cell><cell>13.810</cell><cell>0.043</cell><cell>0.889</cell><cell>0.974</cell><cell>0.993</cell></row><row><cell>MobileNetV2-140</cell><cell>512</cell><cell cols="2">20 M 39 f ps 0 − 50 m</cell><cell>0.095</cell><cell>0.434</cell><cell>2.973</cell><cell>0.146</cell><cell>13.172</cell><cell>0.042</cell><cell>0.896</cell><cell>0.975</cell><cell>0.993</cell></row><row><cell>DenseNet-121</cell><cell>256</cell><cell cols="2">12 M 41 f ps 0 − 50 m</cell><cell>0.083</cell><cell>0.335</cell><cell>2.701</cell><cell>0.129</cell><cell>11.722</cell><cell>0.037</cell><cell>0.919</cell><cell>0.983</cell><cell>0.996</cell></row></table><note>Comparison of the results obtained by the proposed framework with different feature extraction networks in the KITTI Depth dataset [17]. Model 1, Model 2 and Model 3 refer to the DenseSIDENet with the encoder models of Table 2. The MobileNetV2-50 and MobileNetV2-140 are versions of the MobileNetV2 with depth multipliers 1.4 and 0.5 respectively. The BerHu loss function is used in all experiments and the Features column indicates the number of input feature maps to the decoder stage of the DSN Method Features Size Speed Cap Abs Rel↓ Sqr Rel↓ RMSE↓ RMSE (log)↓ SILog↓ log10↓ δ &lt; 1.25↑ δ &lt; 1.25 2 ↑ δ &lt; 1.25 3 ↑</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Behavior of the DSN when subjected to changes in its structure and training. The DSN 3 is the baseline CNN with the Model 3 as encoder. The Pyramid 1 and Pyramid 2 are pyramid-shaped modules implemented in the decoder. CS and Sem refer to pre-training with the disparities of the Cityscapes dataset<ref type="bibr" target="#b135">[135]</ref> and the semantic maps of the KITTI<ref type="bibr" target="#b156">[156]</ref> and Cityscapes<ref type="bibr" target="#b135">[135]</ref> respectively. The L + BerHu is equal to the L BerHu expression, yet with κ = 4</figDesc><table><row><cell>Method</cell><cell>Loss</cell><cell>Size</cell><cell>Speed</cell><cell>Cap</cell><cell cols="8">Abs Rel↓ Sqr Rel↓ RMSE↓ RMSE (log)↓ SILog↓ log10↓ δ &lt; 1.25↑ δ &lt; 1.25 2 ↑ δ &lt; 1.25 3 ↑</cell></row><row><cell>DSN 3</cell><cell>LBerHu</cell><cell cols="3">12 M 50 f ps 0 − 80 m</cell><cell>0.083</cell><cell>0.434</cell><cell>3.646</cell><cell>0.135</cell><cell>12.326</cell><cell>0.037</cell><cell>0.914</cell><cell>0.980</cell><cell>0.994</cell></row><row><cell>DSN 3</cell><cell cols="4">La + LBerHu 12 M 50 f ps 0 − 80 m</cell><cell>0.084</cell><cell>0.437</cell><cell>3.621</cell><cell>0.133</cell><cell>12.259</cell><cell>0.037</cell><cell>0.914</cell><cell>0.981</cell><cell>0.994</cell></row><row><cell>DSN 3 DSN 3 + Pyramid 1 DSN 3 + Pyramid 1 + Sem DSN 3 + Pyramid 1 + CS DSN 3 + Pyramid 2 + CS</cell><cell>L + BerHu L + BerHu L + BerHu L + BerHu L + BerHu</cell><cell cols="3">12 M 50 f ps 0 − 80 m 12 M 39 f ps 0 − 80 m 12 M 39 f ps 0 − 80 m 12 M 39 f ps 0 − 80 m 12 M 32 f ps 0 − 80 m</cell><cell>0.083 0.082 0.081 0.078 0.078</cell><cell>0.431 0.418 0.414 0.405 0.376</cell><cell>3.628 3.593 3.557 3.399 3.344</cell><cell>0.133 0.132 0.133 0.125 0.124</cell><cell>12.146 12.122 12.287 11.490 11.323</cell><cell>0.037 0.037 0.036 0.034 0.034</cell><cell>0.915 0.919 0.916 0.926 0.926</cell><cell>0.982 0.981 0.980 0.985 0.985</cell><cell>0.995 0.995 0.995 0.995 0.996</cell></row><row><cell>DSN 3</cell><cell>LBerHu</cell><cell cols="3">12 M 50 f ps 0 − 50 m</cell><cell>0.079</cell><cell>0.314</cell><cell>2.592</cell><cell>0.124</cell><cell>11.315</cell><cell>0.035</cell><cell>0.926</cell><cell>0.984</cell><cell>0.996</cell></row><row><cell>DSN 3</cell><cell cols="4">LBerHu + La 12 M 50 f ps 0 − 50 m</cell><cell>0.080</cell><cell>0.319</cell><cell>2.594</cell><cell>0.123</cell><cell>11.267</cell><cell>0.035</cell><cell>0.927</cell><cell>0.985</cell><cell>0.996</cell></row><row><cell>DSN 3 DSN 3 + Pyramid 1 DSN 3 + Pyramid 1 + Sem DSN 3 + Pyramid 1 + CS DSN 3 + Pyramid 2 + CS</cell><cell>L + BerHu L + BerHu L + BerHu L + BerHu L + BerHu</cell><cell cols="3">12 M 50 f ps 0 − 50 m 12 M 39 f ps 0 − 50 m 12 M 39 f ps 0 − 50 m 12 M 39 f ps 0 − 50 m 12 M 32 f ps 0 − 50 m</cell><cell>0.078 0.078 0.077 0.074 0.074</cell><cell>0.312 0.301 0.301 0.302 0.274</cell><cell>2.596 2.553 3.539 2.437 2.393</cell><cell>0.122 0.121 0.123 0.116 0.114</cell><cell>11.143 11.093 11.309 10.579 10.413</cell><cell>0.035 0.034 0.034 0.032 0.032</cell><cell>0.927 0.931 0.927 0.937 0.937</cell><cell>0.986 0.985 0.984 0.988 0.988</cell><cell>0.996 0.996 0.996 0.996 0.997</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Evaluation of the DSN with the surface normals module in the KITTI Depth dataset [17]. The DSN 3 + Pyramid 1 + CS + SN setup, as well as the Sobel edge detector and the L + BerHu loss, is standard for all experiments. SN means that the training is carried out with additional surface normals ground truth. The Blur column indicates the presence or absence of a Convolution with Gaussian kernel. ψ 1 and ψ 2 are the constants of the 2.5D loss function employed in the pre-training and training steps respectively Cap Abs Rel↓ Sqr Rel↓ RMSE↓ RMSE (log)↓ SILog↓ log10↓ δ &lt; 1.25↑ δ &lt; 1.25 2 ↑ δ &lt; 1.25 3 ↑ DSN 3 + Pyramid 1 + CS + SN 12 M 33 f ps 10 6 10 6 0 − 80 m</figDesc><table><row><cell>Method</cell><cell>Size</cell><cell cols="2">Speed Blur ψ1</cell><cell>ψ2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.075</cell><cell>0.363</cell><cell>3.253</cell><cell>0.119</cell><cell>10.886</cell><cell>0.033</cell><cell>0.934</cell><cell>0.986</cell><cell>0.996</cell></row><row><cell cols="3">DSN 3 + Pyramid 1 + CS + SN 12 M 33 f ps</cell><cell cols="2">10 6 10 7 0 − 80 m</cell><cell>0.076</cell><cell>0.360</cell><cell>3.259</cell><cell>0.120</cell><cell>10.950</cell><cell>0.034</cell><cell>0.932</cell><cell>0.986</cell><cell>0.996</cell></row><row><cell cols="3">DSN 3 + Pyramid 1 + CS + SN 12 M 32 f ps</cell><cell cols="2">10 6 10 6 0 − 80 m</cell><cell>0.075</cell><cell>0.365</cell><cell>3.264</cell><cell>0.120</cell><cell>10.947</cell><cell>0.033</cell><cell>0.934</cell><cell>0.986</cell><cell>0.996</cell></row><row><cell cols="3">DSN 3 + Pyramid 1 + CS + SN 12 M 32 f ps</cell><cell cols="2">10 6 10 7 0 − 80 m</cell><cell>0.081</cell><cell>0.391</cell><cell>3.347</cell><cell>0.126</cell><cell>11.591</cell><cell>0.036</cell><cell>0.924</cell><cell>0.985</cell><cell>0.996</cell></row><row><cell cols="3">DSN 3 + Pyramid 1 + CS + SN 12 M 33 f ps</cell><cell cols="2">10 6 10 6 0 − 50 m</cell><cell>0.071</cell><cell>0.267</cell><cell>2.351</cell><cell>0.111</cell><cell>10.053</cell><cell>0.031</cell><cell>0.944</cell><cell>0.989</cell><cell>0.997</cell></row><row><cell cols="3">DSN 3 + Pyramid 1 + CS + SN 12 M 33 f ps</cell><cell cols="2">10 6 10 7 0 − 50 m</cell><cell>0.072</cell><cell>0.263</cell><cell>2.365</cell><cell>0.111</cell><cell>10.129</cell><cell>0.032</cell><cell>0.942</cell><cell>0.989</cell><cell>0.997</cell></row><row><cell cols="3">DSN 3 + Pyramid 1 + CS + SN 12 M 32 f ps</cell><cell cols="2">10 6 10 6 0 − 50 m</cell><cell>0.072</cell><cell>0.268</cell><cell>2.355</cell><cell>0.111</cell><cell>10.113</cell><cell>0.031</cell><cell>0.944</cell><cell>0.989</cell><cell>0.997</cell></row><row><cell cols="3">DSN 3 + Pyramid 1 + CS + SN 12 M 32 f ps</cell><cell cols="2">10 6 10 7 0 − 50 m</cell><cell>0.077</cell><cell>0.296</cell><cell>2.473</cell><cell>0.118</cell><cell>10.827</cell><cell>0.034</cell><cell>0.934</cell><cell>0.987</cell><cell>0.996</cell></row><row><cell>(a) RGB Input</cell><cell cols="2">(b) Ground Truth</cell><cell></cell><cell cols="2">(c) GeoNet [85]</cell><cell cols="3">(d) Poggi et al. [80]</cell><cell cols="2">(e) MiniNet [95]</cell><cell>(f) DSN †</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>Performances of our framework and other works in the SIDE literature according to the KITTI Depth [17] metrics. Legend: DSN † -DSN setup that yields the most accurate overall results; MV -monocular video; SP -stereo pair; CM -camera motion; SI -single image; GT -ground truth; SL -semantic labels; SN -surface normals; DSO -direct sparse odometry; SGM -semi-global matching; DM -disparity map; Cal. Cam. -calibrated camera Method Training Input/Auxiliar Info Cap Abs Rel↓ Sqr Rel↓ RMSE↓ RMSE log ↓ δ &lt; 1.25↑ δ &lt; 1.25 2 ↑ δ &lt; 1.25 3 ↑ Saxena et al.</figDesc><table><row><cell>[157]</cell><cell>SI/GT Depth</cell><cell>0 − 80 m</cell><cell>0.280</cell><cell>3.012</cell><cell>8.734</cell><cell>0.361</cell><cell>0.601</cell><cell>0.820</cell><cell>0.926</cell></row><row><cell>Eigen et al. [28]</cell><cell>SI/GT Depth</cell><cell>0 − 80 m</cell><cell>0.203</cell><cell>1.548</cell><cell>6.307</cell><cell>0.282</cell><cell>0.702</cell><cell>0.898</cell><cell>0.967</cell></row><row><cell>Liu et al. [158]</cell><cell>SI/GT Depth</cell><cell>0 − 80 m</cell><cell>0.201</cell><cell>1.584</cell><cell>6.471</cell><cell>0.273</cell><cell>0.680</cell><cell>0.898</cell><cell>0.967</cell></row><row><cell>Zhou et al. [59]</cell><cell>MV/-</cell><cell>0 − 80 m</cell><cell>0.198</cell><cell>1.836</cell><cell>6.565</cell><cell>0.275</cell><cell>0.718</cell><cell>0.901</cell><cell>0.960</cell></row><row><cell>UnDeepVO [68]</cell><cell>SP/Cal. Cam.</cell><cell>0 − 80 m</cell><cell>0.183</cell><cell>1.730</cell><cell>6.570</cell><cell>0.268</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Yang et al. [159]</cell><cell>MV/-</cell><cell>0 − 80 m</cell><cell>0.182</cell><cell>1.481</cell><cell>6.501</cell><cell>0.267</cell><cell>0.725</cell><cell>0.906</cell><cell>0.963</cell></row><row><cell>AdaDepth [160]</cell><cell cols="2">SI and Synt. SI/Synt. GT Depth (self) 0 − 80 m</cell><cell>0.167</cell><cell>1.257</cell><cell>5.578</cell><cell>0.237</cell><cell>0.771</cell><cell>0.922</cell><cell>0.971</cell></row><row><cell>Klodt et al. [121]</cell><cell>MV/-</cell><cell>0 − 80 m</cell><cell>0.166</cell><cell>1.490</cell><cell>5.988</cell><cell>-</cell><cell>0.778</cell><cell>0.919</cell><cell>0.966</cell></row><row><cell>Mahjourian et al. [86]</cell><cell>MV/-</cell><cell>0 − 80 m</cell><cell>0.163</cell><cell>1.240</cell><cell>6.220</cell><cell>0.250</cell><cell>0.762</cell><cell>0.916</cell><cell>0.968</cell></row><row><cell>LEGO [84]</cell><cell>MV/-</cell><cell>0 − 80 m</cell><cell>0.162</cell><cell>1.352</cell><cell>6.276</cell><cell>0.252</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Poggi et al. [80]</cell><cell>SP/-</cell><cell>0 − 80 m</cell><cell>0.153</cell><cell>1.363</cell><cell>6.030</cell><cell>0.252</cell><cell>0.789</cell><cell>0.918</cell><cell>0.963</cell></row><row><cell>GeoNet [85]</cell><cell>MV/-</cell><cell>0 − 80 m</cell><cell>0.153</cell><cell>1.328</cell><cell>5.737</cell><cell>0.232</cell><cell>0.802</cell><cell>0.934</cell><cell>0.972</cell></row><row><cell>Pilzer et al. [79]</cell><cell>SP/Cal. Cam.</cell><cell>0 − 80 m</cell><cell>0.152</cell><cell>1.388</cell><cell>6.016</cell><cell>0.247</cell><cell>0.789</cell><cell>0.918</cell><cell>0.965</cell></row><row><cell>DDVO [87]</cell><cell>MV/-</cell><cell>0 − 80 m</cell><cell>0.151</cell><cell>1.257</cell><cell>5.583</cell><cell>0.228</cell><cell>0.810</cell><cell>0.936</cell><cell>0.974</cell></row><row><cell>DF-Net [122]</cell><cell>MV/-</cell><cell>0 − 80 m</cell><cell>0.150</cell><cell>1.124</cell><cell>5.507</cell><cell>0.223</cell><cell>0.806</cell><cell>0.933</cell><cell>0.973</cell></row><row><cell>Ranjan et al. [91]</cell><cell>MV/-</cell><cell>0 − 80 m</cell><cell>0.148</cell><cell>1.149</cell><cell>5.464</cell><cell>0.226</cell><cell>0.815</cell><cell>0.935</cell><cell>0.973</cell></row><row><cell>MiniNet et al. [95]</cell><cell>MV/-</cell><cell>0 − 50 m</cell><cell>0.141</cell><cell>1.080</cell><cell>5.264</cell><cell>0.216</cell><cell>0.825</cell><cell>0.941</cell><cell>0.976</cell></row><row><cell>Struct2depth [88]</cell><cell cols="2">MV/Cal. Cam. (Only intrinsics matrix) 0 − 80 m</cell><cell>0.141</cell><cell>1.026</cell><cell>5.291</cell><cell>0.215</cell><cell>0.816</cell><cell>0.945</cell><cell>0.979</cell></row><row><cell>Elkerdawy et al. [82]</cell><cell>SP/Cal. Cam.</cell><cell>0 − 80 m</cell><cell>0.136</cell><cell>-</cell><cell>5.891</cell><cell>-</cell><cell>0.827</cell><cell>-</cell><cell>-</cell></row><row><cell>PHN et al. [161]</cell><cell>SI/GT Depth</cell><cell>0 − 80 m</cell><cell>0.136</cell><cell>-</cell><cell>4.082</cell><cell>0.164</cell><cell>0.864</cell><cell>0.966</cell><cell>0.989</cell></row><row><cell>Zhan FullNYU [162]</cell><cell>SP/CM</cell><cell>0 − 80 m</cell><cell>0.135</cell><cell>1.132</cell><cell>5.585</cell><cell>0.229</cell><cell>0.820</cell><cell>0.933</cell><cell>0.971</cell></row><row><cell>Wong et al. [163]</cell><cell>SP/-(Cal. needed in test)</cell><cell>0 − 80 m</cell><cell>0.133</cell><cell>1.126</cell><cell>5.515</cell><cell>0.231</cell><cell>0.826</cell><cell>0.934</cell><cell>0.969</cell></row><row><cell>3Net (ResNet-50) [71]</cell><cell>Trinocular setup/-</cell><cell>0 − 80 m</cell><cell>0.129</cell><cell>0.996</cell><cell>5.281</cell><cell>0.223</cell><cell>0.831</cell><cell>0.939</cell><cell>0.974</cell></row><row><cell>StrAT [164]</cell><cell>SI/Cal. Cam.</cell><cell>0 − 80 m</cell><cell>0.128</cell><cell>1.019</cell><cell>5.403</cell><cell>0.227</cell><cell>0.827</cell><cell>0.935</cell><cell>0.971</cell></row><row><cell>EPC++ [165]</cell><cell cols="2">MV/Cal. Cam. (Only intrinsics matrix) 0 − 80 m</cell><cell>0.128</cell><cell>0.935</cell><cell>5.011</cell><cell>0.209</cell><cell>0.831</cell><cell>0.945</cell><cell>0.979</cell></row><row><cell>Gordon et al. [166]</cell><cell>MV/-</cell><cell>0 − 80 m</cell><cell>0.124</cell><cell>0.930</cell><cell>5.120</cell><cell>0.206</cell><cell>0.851</cell><cell>0.950</cell><cell>0.978</cell></row><row><cell>Sparse-to-Continuous [148]</cell><cell>SI/GT Depth</cell><cell>0 − 80 m</cell><cell>0.123</cell><cell>0.641</cell><cell>4.524</cell><cell>0.199</cell><cell>0.881</cell><cell>0.966</cell><cell>0.986</cell></row><row><cell>SA-Attention [129]</cell><cell cols="2">MV/Cal. Cam. (Only intrinsics matrix) 0 − 80 m</cell><cell>0.121</cell><cell>0.837</cell><cell>4.945</cell><cell>0.197</cell><cell>0.853</cell><cell>0.955</cell><cell>0.982</cell></row><row><cell>3Net (VGG) [71]</cell><cell>Trinocular setup/-</cell><cell>0 − 80 m</cell><cell>0.119</cell><cell>1.201</cell><cell>5.888</cell><cell>0.208</cell><cell>0.844</cell><cell>0.941</cell><cell>0.978</cell></row><row><cell>SceneNet [167]</cell><cell>SI and SP/SL</cell><cell>0 − 80 m</cell><cell>0.118</cell><cell>0.905</cell><cell>5.096</cell><cell>0.211</cell><cell>0.839</cell><cell>0.945</cell><cell>0.977</cell></row><row><cell>Su et al. [168]</cell><cell>SI/GT Depth</cell><cell>0 − 80 m</cell><cell>0.117</cell><cell>-</cell><cell>4.251</cell><cell>0.174</cell><cell>0.894</cell><cell>0.971</cell><cell>0.984</cell></row><row><cell>SDNet [128]</cell><cell>SI/GT Depth and SL</cell><cell>0 − 80 m</cell><cell>0.116</cell><cell>0.945</cell><cell>4.916</cell><cell>0.208</cell><cell>0.861</cell><cell>0.952</cell><cell>0.968</cell></row><row><cell>Cao et al. [34]</cell><cell>SI/GT Depth</cell><cell>0 − 80 m</cell><cell>0.115</cell><cell>-</cell><cell>4.712</cell><cell>0.198</cell><cell>0.887</cell><cell>0.963</cell><cell>0.982</cell></row><row><cell>Monodepth [27]</cell><cell>SP/Cal. Cam.</cell><cell>0 − 80 m</cell><cell>0.114</cell><cell>0.898</cell><cell>4.935</cell><cell>0.206</cell><cell>0.861</cell><cell>0.949</cell><cell>0.976</cell></row><row><cell>LSIM [126]</cell><cell>SP/Cal. Cam.</cell><cell>0 − 80 m</cell><cell>0.113</cell><cell>0.898</cell><cell>5.048</cell><cell>0.208</cell><cell>0.853</cell><cell>0.948</cell><cell>0.976</cell></row><row><cell>Kuznietsov et al. [61]</cell><cell>SP/Cal. Cam. and GT Depth</cell><cell>0 − 80 m</cell><cell>0.113</cell><cell>0.741</cell><cell>4.621</cell><cell>0.189</cell><cell>0.862</cell><cell>0.960</cell><cell>0.986</cell></row><row><cell>SuperDepth [58]</cell><cell>SP/Cal. Cam.</cell><cell>0 − 80 m</cell><cell>0.112</cell><cell>0.875</cell><cell>4.958</cell><cell>0.207</cell><cell>0.852</cell><cell>0.947</cell><cell>0.977</cell></row><row><cell>DeepLabV3+ (F10) [169]</cell><cell>SI/GT Depth and Cal. Cam.</cell><cell>0 − 80 m</cell><cell>0.110</cell><cell>0.666</cell><cell>4.186</cell><cell>0.168</cell><cell>0.880</cell><cell>0.966</cell><cell>0.988</cell></row><row><cell>Monodepth2 [60]</cell><cell>SP or MV or both/Cal. Cam.</cell><cell>0 − 80 m</cell><cell>0.106</cell><cell>0.806</cell><cell>4.630</cell><cell>0.193</cell><cell>0.876</cell><cell>0.958</cell><cell>0.980</cell></row><row><cell>PackNet-SfM [93]</cell><cell>MV/Velocity term (optional)</cell><cell>0 − 80 m</cell><cell>0.104</cell><cell>0.758</cell><cell>4.386</cell><cell>0.182</cell><cell>0.895</cell><cell>0.964</cell><cell>0.982</cell></row><row><cell>Guizilini et al. [10]</cell><cell>MV/Pre-tained network with SL</cell><cell>0 − 80 m</cell><cell>0.100</cell><cell>0.761</cell><cell>4.270</cell><cell>0.175</cell><cell>0.902</cell><cell>0.965</cell><cell>0.982</cell></row><row><cell>CFA [170]</cell><cell>SI/GT Depth and SL</cell><cell>0 − 80 m</cell><cell>0.100</cell><cell>0.601</cell><cell>4.298</cell><cell>0.174</cell><cell>0.874</cell><cell>0.966</cell><cell>0.989</cell></row><row><cell>Refine&amp;Distill [171]</cell><cell>SP/Cal. Cam.</cell><cell>0 − 80 m</cell><cell>0.098</cell><cell>0.831</cell><cell>4.656</cell><cell>0.202</cell><cell>0.882</cell><cell>0.948</cell><cell>0.973</cell></row><row><cell>DVSO [63]</cell><cell>SP/GT Stereo DSO Depth</cell><cell>0 − 80 m</cell><cell>0.097</cell><cell>0.734</cell><cell>4.442</cell><cell>0.187</cell><cell>0.888</cell><cell>0.958</cell><cell>0.980</cell></row><row><cell>SOM [172]</cell><cell>SI/GT Depth</cell><cell>0 − 80 m</cell><cell>0.097</cell><cell>0.398</cell><cell>3.007</cell><cell>0.133</cell><cell>0.913</cell><cell>0.985</cell><cell>0.997</cell></row><row><cell>Depth Hints [173]</cell><cell>SP/Cal. Cam.</cell><cell>0 − 80 m</cell><cell>0.096</cell><cell>0.710</cell><cell>4.393</cell><cell>0.185</cell><cell>0.890</cell><cell>0.962</cell><cell>0.981</cell></row><row><cell>monoResMatch [67]</cell><cell>SP/Proxy GT with SGM</cell><cell>0 − 80 m</cell><cell>0.096</cell><cell>0.673</cell><cell>4.351</cell><cell>0.184</cell><cell>0.890</cell><cell>0.961</cell><cell>0.981</cell></row><row><cell>VGG16-UNet [72]</cell><cell>SI/Synthetic SP-DM and Cal. Cam.</cell><cell>0 − 80 m</cell><cell>0.096</cell><cell>0.641</cell><cell>4.095</cell><cell>0.168</cell><cell>0.892</cell><cell>0.967</cell><cell>0.986</cell></row><row><cell>SemiDepth [62]</cell><cell>SP/Cal. Cam. and GT Depth</cell><cell>0 − 80 m</cell><cell>0.096</cell><cell>0.552</cell><cell>3.995</cell><cell>0.152</cell><cell>0.892</cell><cell>0.972</cell><cell>0.992</cell></row><row><cell>SVS [66]</cell><cell>SP/Cal. Cam.</cell><cell>0 − 80 m</cell><cell>0.094</cell><cell>0.626</cell><cell>4.252</cell><cell>0.177</cell><cell>0.891</cell><cell>0.965</cell><cell>0.984</cell></row><row><cell>DenseDepth [15]</cell><cell>SI/GT Depth</cell><cell>0 − 80 m</cell><cell>0.093</cell><cell>0.589</cell><cell>4.170</cell><cell>0.171</cell><cell>0.886</cell><cell>0.965</cell><cell>0.986</cell></row><row><cell>VOMonodepth [64]</cell><cell>SP/Sparse Depths and Cal. Cam.</cell><cell>0 − 80 m</cell><cell>0.091</cell><cell>0.548</cell><cell>3.690</cell><cell>0.181</cell><cell>0.892</cell><cell>0.956</cell><cell>0.979</cell></row><row><cell>FAL-netB49 [78]</cell><cell>SP/Cal. Cam.</cell><cell>0 − 80 m</cell><cell>0.088</cell><cell>0.547</cell><cell>4.004</cell><cell>0.175</cell><cell>0.898</cell><cell>0.966</cell><cell>0.984</cell></row><row><cell>Monodepth2-Self [77]</cell><cell>SP or MV or both/Cal. Cam.</cell><cell>0 − 80 m</cell><cell>0.083</cell><cell>-</cell><cell>3.682</cell><cell>-</cell><cell>0.919</cell><cell>-</cell><cell>-</cell></row><row><cell>BA-Net [174]</cell><cell>MV/Cal. Cam.</cell><cell>0 − 80 m</cell><cell>0.083</cell><cell>-</cell><cell>3.640</cell><cell>0.134</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DSN †</cell><cell>SI/GT and SN</cell><cell>0 − 80 m</cell><cell>0.075</cell><cell>0.363</cell><cell>3.253</cell><cell>0.119</cell><cell>0.934</cell><cell>0.986</cell><cell>0.996</cell></row><row><cell>VNL [11]</cell><cell>SI/GT Depth and VN</cell><cell>0 − 80 m</cell><cell>0.072</cell><cell>-</cell><cell>3.258</cell><cell>0.117</cell><cell>0.938</cell><cell>0.990</cell><cell>0.998</cell></row><row><cell>DORN [26]</cell><cell>SI/GT Depth</cell><cell>0 − 80 m</cell><cell>0.072</cell><cell>0.307</cell><cell>2.727</cell><cell>0.120</cell><cell>0.932</cell><cell>0.984</cell><cell>0.994</cell></row><row><cell>BTS [13]</cell><cell>SI/GT Depth</cell><cell>0 − 80 m</cell><cell>0.059</cell><cell>0.245</cell><cell>2.756</cell><cell>0.096</cell><cell>0.956</cell><cell>0.993</cell><cell>0.998</cell></row><row><cell>Zhou et al. [59]</cell><cell>MV/-</cell><cell>0 − 50 m</cell><cell>0.190</cell><cell>1.436</cell><cell>4.975</cell><cell>0.258</cell><cell>0.735</cell><cell>0.915</cell><cell>0.968</cell></row><row><cell>Kim et al. [175]</cell><cell>SI/GT Depth</cell><cell>0 − 50 m</cell><cell>0.177</cell><cell>-</cell><cell>6.570</cell><cell>0.254</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Garg et al. [57]</cell><cell>SP/CM</cell><cell>1 − 50 m</cell><cell>0.169</cell><cell>1.080</cell><cell>5.104</cell><cell>0.273</cell><cell>0.740</cell><cell>0.904</cell><cell>0.962</cell></row><row><cell>Mahjourian et al. [86]</cell><cell>MV/-</cell><cell>0 − 50 m</cell><cell>0.155</cell><cell>0.927</cell><cell>4.549</cell><cell>0.231</cell><cell>0.781</cell><cell>0.931</cell><cell>0.975</cell></row><row><cell>GeoNet [85]</cell><cell>MV/-</cell><cell>0 − 50 m</cell><cell>0.147</cell><cell>0.936</cell><cell>4.348</cell><cell>0.218</cell><cell>0.810</cell><cell>0.941</cell><cell>0.977</cell></row><row><cell>Poggi et al. [80]</cell><cell>SP/-</cell><cell>0 − 50 m</cell><cell>0.145</cell><cell>1.014</cell><cell>4.608</cell><cell>0.227</cell><cell>0.813</cell><cell>0.934</cell><cell>0.972</cell></row><row><cell>Pilzer et al. [79]</cell><cell>SP/Cal. Cam.</cell><cell>0 − 50 m</cell><cell>0.144</cell><cell>1.007</cell><cell>4.660</cell><cell>0.240</cell><cell>0.793</cell><cell>0.923</cell><cell>0.968</cell></row><row><cell>MiniNet et al. [95]</cell><cell>MV/-</cell><cell>0 − 50 m</cell><cell>0.135</cell><cell>0.839</cell><cell>4.067</cell><cell>0.205</cell><cell>0.838</cell><cell>0.947</cell><cell>0.978</cell></row><row><cell>Wong et al. [163]</cell><cell>SP/-(Cal. needed in test)</cell><cell>0 − 50 m</cell><cell>0.126</cell><cell>1.832</cell><cell>4.172</cell><cell>0.217</cell><cell>0.840</cell><cell>0.941</cell><cell>0.973</cell></row><row><cell>Sparse-to-Continuous [148]</cell><cell>SI/GT Depth</cell><cell>0 − 50 m</cell><cell>0.119</cell><cell>0.444</cell><cell>3.097</cell><cell>0.180</cell><cell>0.893</cell><cell>0.974</cell><cell>0.990</cell></row><row><cell>Monodepth [27]</cell><cell>SP/Cal. Cam.</cell><cell>0 − 50 m</cell><cell>0.108</cell><cell>0.657</cell><cell>3.729</cell><cell>0.194</cell><cell>0.873</cell><cell>0.954</cell><cell>0.979</cell></row><row><cell>Kuznietsov et al. [61]</cell><cell>SP/Cal. Cam. and GT Depth</cell><cell>1 − 50 m</cell><cell>0.108</cell><cell>0.595</cell><cell>3.518</cell><cell>0.179</cell><cell>0.875</cell><cell>0.964</cell><cell>0.988</cell></row><row><cell>Cao et al. [34]</cell><cell>SI/GT Depth</cell><cell>0 − 50 m</cell><cell>0.107</cell><cell>-</cell><cell>3.605</cell><cell>0.187</cell><cell>0.898</cell><cell>0.966</cell><cell>0.984</cell></row><row><cell>Su et al. [168]</cell><cell>SI/GT Depth</cell><cell>0 − 50 m</cell><cell>0.107</cell><cell>-</cell><cell>3.440</cell><cell>0.172</cell><cell>0.900</cell><cell>0.976</cell><cell>0.990</cell></row><row><cell>LSIM et al. [126]</cell><cell>SP/Cal. Cam.</cell><cell>0 − 50 m</cell><cell>0.106</cell><cell>0.653</cell><cell>3.790</cell><cell>0.195</cell><cell>0.867</cell><cell>0.954</cell><cell>0.979</cell></row><row><cell>CFA [170]</cell><cell>SI/GT Depth and SL</cell><cell>0 − 50 m</cell><cell>0.096</cell><cell>0.482</cell><cell>3.338</cell><cell>0.166</cell><cell>0.886</cell><cell>0.980</cell><cell>0.995</cell></row><row><cell>Gan et al. [176]</cell><cell>SI/GT Depth</cell><cell>0 − 50 m</cell><cell>0.094</cell><cell>0.552</cell><cell>3.133</cell><cell>0.165</cell><cell>0.898</cell><cell>0.967</cell><cell>0.986</cell></row><row><cell>DSN †</cell><cell>SI/GT and SN</cell><cell>0 − 50 m</cell><cell>0.071</cell><cell>0.267</cell><cell>2.351</cell><cell>0.111</cell><cell>0.944</cell><cell>0.989</cell><cell>0.997</cell></row><row><cell>DORN [26]</cell><cell>SI/GT Depth</cell><cell>0 − 50 m</cell><cell>0.071</cell><cell>0.268</cell><cell>2.271</cell><cell>0.116</cell><cell>0.936</cell><cell>0.985</cell><cell>0.995</cell></row><row><cell>BTS [13]</cell><cell>SI/GT Depth</cell><cell>0 − 50 m</cell><cell>0.056</cell><cell>0.169</cell><cell>1.925</cell><cell>0.087</cell><cell>0.964</cell><cell>0.994</cell><cell>0.999</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 :</head><label>10</label><figDesc>Evaluation of our depth completion pipeline according to Eigen Split applied to the KITTI Depth dataset [17]. The DSN 3 + Pyramid 1 + CS configuration, besides the L + BerHu function, is taken as default for the first experiments. The DSN 3 + Pyramid 1 + CS + SN setup, along with the edge detector, is analyzed afterward</figDesc><table><row><cell>Method</cell><cell cols="8">Cap Samples Abs Rel↓ RMSE↓ log10↓ δ &lt; 1.25↑ δ &lt; 1.25 2 ↑ δ &lt; 1.25 3 ↑</cell></row><row><cell>DSN 3 + Pyramid 1 + CS</cell><cell>80</cell><cell>20</cell><cell>0.049</cell><cell>2.829</cell><cell>0.021</cell><cell>0.961</cell><cell>0.991</cell><cell>0.997</cell></row><row><cell>DSN 3 + Pyramid 1 + CS</cell><cell>80</cell><cell>50</cell><cell>0.035</cell><cell>2.397</cell><cell>0.015</cell><cell>0.976</cell><cell>0.994</cell><cell>0.998</cell></row><row><cell>DSN 3 + Pyramid 1 + CS</cell><cell>80</cell><cell>100</cell><cell>0.029</cell><cell>2.184</cell><cell>0.013</cell><cell>0.981</cell><cell>0.995</cell><cell>0.998</cell></row><row><cell>DSN 3 + Pyramid 1 + CS</cell><cell>80</cell><cell>200</cell><cell>0.024</cell><cell>1.926</cell><cell>0.010</cell><cell>0.987</cell><cell>0.996</cell><cell>0.999</cell></row><row><cell>DSN 3 + Pyramid 1 + CS</cell><cell>80</cell><cell>500</cell><cell>0.019</cell><cell>1.672</cell><cell>0.008</cell><cell>0.991</cell><cell>0.997</cell><cell>0.999</cell></row><row><cell>DSN 3 + Pyramid 1 + CS + SN</cell><cell>80</cell><cell>200</cell><cell>0.024</cell><cell>1.863</cell><cell>0.010</cell><cell>0.987</cell><cell>0.996</cell><cell>0.999</cell></row><row><cell>DSN 3 + Pyramid 1 + CS + SN</cell><cell>80</cell><cell>500</cell><cell>0.019</cell><cell>1.588</cell><cell>0.008</cell><cell>0.991</cell><cell>0.997</cell><cell>0.999</cell></row><row><cell>DSN 3 + Pyramid 1 + CS</cell><cell>50</cell><cell>20</cell><cell>0.045</cell><cell>1.922</cell><cell>0.019</cell><cell>0.970</cell><cell>0.993</cell><cell>0.998</cell></row><row><cell>DSN 3 + Pyramid 1 + CS</cell><cell>50</cell><cell>50</cell><cell>0.032</cell><cell>1.595</cell><cell>0.014</cell><cell>0.982</cell><cell>0.995</cell><cell>0.999</cell></row><row><cell>DSN 3 + Pyramid 1 + CS</cell><cell>50</cell><cell>100</cell><cell>0.026</cell><cell>1.432</cell><cell>0.011</cell><cell>0.986</cell><cell>0.996</cell><cell>0.999</cell></row><row><cell>DSN 3 + Pyramid 1 + CS</cell><cell>50</cell><cell>200</cell><cell>0.021</cell><cell>1.252</cell><cell>0.009</cell><cell>0.990</cell><cell>0.997</cell><cell>0.999</cell></row><row><cell>DSN 3 + Pyramid 1 + CS</cell><cell>50</cell><cell>500</cell><cell>0.017</cell><cell>1.075</cell><cell>0.007</cell><cell>0.993</cell><cell>0.998</cell><cell>0.999</cell></row><row><cell>DSN 3 + Pyramid 1 + CS + SN</cell><cell>50</cell><cell>200</cell><cell>0.022</cell><cell>1.247</cell><cell>0.09</cell><cell>0.990</cell><cell>0.997</cell><cell>0.999</cell></row><row><cell>DSN 3 + Pyramid 1 + CS + SN</cell><cell>50</cell><cell>500</cell><cell>0.018</cell><cell>1.051</cell><cell>0.008</cell><cell>0.993</cell><cell>0.998</cell><cell>0.999</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 11 :</head><label>11</label><figDesc>Quantitative comparison of the results from our framework and the results from other depth completion techniques.The metrics are computed considering the Eigen Split and the KITTI Depth dataset<ref type="bibr" target="#b16">[17]</ref>. We apply cap 80 in all the analysis</figDesc><table><row><cell>Method</cell><cell cols="6">Samples Abs Rel↓ RMSE↓ δ &lt; 1.25↑ δ &lt; 1.25 2 ↑ δ &lt; 1.25 3 ↑</cell></row><row><cell>full-MAE [177]</cell><cell>∼650</cell><cell>0.179</cell><cell>7.14</cell><cell>0.709</cell><cell>0.888</cell><cell>0.956</cell></row><row><cell>Liao et al. [35]</cell><cell>225</cell><cell>0.113</cell><cell>4.50</cell><cell>0.874</cell><cell>0.960</cell><cell>0.984</cell></row><row><cell>Ma et al. [7]</cell><cell>200</cell><cell>0.083</cell><cell>3.851</cell><cell>0.919</cell><cell>0.970</cell><cell>0.986</cell></row><row><cell>Ma et al. [7]</cell><cell>500</cell><cell>0.073</cell><cell>3.378</cell><cell>0.935</cell><cell>0.976</cell><cell>0.989</cell></row><row><cell>MC + SPN [112]</cell><cell>500</cell><cell>0.059</cell><cell>3.248</cell><cell>0.944</cell><cell>0.977</cell><cell>0.989</cell></row><row><cell>SPN [178]</cell><cell>500</cell><cell>0.063</cell><cell>3.243</cell><cell>0.943</cell><cell>0.978</cell><cell>0.991</cell></row><row><cell>Mirror connection (MC) [112]</cell><cell>500</cell><cell>0.051</cell><cell>3.049</cell><cell>0.953</cell><cell>0.979</cell><cell>0.990</cell></row><row><cell>CSPN [112]</cell><cell>500</cell><cell>0.049</cell><cell>3.029</cell><cell>0.955</cell><cell>0.980</cell><cell>0.990</cell></row><row><cell>MC + CSPN [112]</cell><cell>500</cell><cell>0.044</cell><cell>2.977</cell><cell>0.957</cell><cell>0.980</cell><cell>0.991</cell></row><row><cell>MC + CSPN + ACSPF [112]</cell><cell>500</cell><cell>0.042</cell><cell>2.843</cell><cell>0.961</cell><cell>0.982</cell><cell>0.992</cell></row><row><cell>DSN 3 + Pyramid 1 + CS + SN</cell><cell>200</cell><cell>0.024</cell><cell>1.863</cell><cell>0.987</cell><cell>0.996</cell><cell>0.999</cell></row><row><cell>DSN 3 + Pyramid 1 + CS + SN</cell><cell>500</cell><cell>0.019</cell><cell>1.588</cell><cell>0.991</cell><cell>0.997</cell><cell>0.999</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 12 :</head><label>12</label><figDesc>Evaluation of DSN predictions applied to the CNN-SVO framework according to the RMSE metric. We used the first 11 sequences of the KITTI Odometry dataset<ref type="bibr" target="#b146">[146]</ref> to generate the results. Legend: SVO -Semi-Direct Visual Odometry; DSO -Direct Sparse Odometry; DSN † -DSN setup, without the surface normals module, that produces the best results; BA -Bundle Adjustment; DSN ‡ -DSN setup, with the surface normals module, that produces the most accurate results</figDesc><table><row><cell cols="8">Sequence SVO [180] CNN-SVO + BA [5]</cell><cell>CNN-SVO [5] + BA + DSN ‡</cell><cell>CNN-SVO [5] + BA + DSN †</cell><cell>CNN-SVO [5] + DSN †</cell><cell>DSO [179]</cell><cell>ORB-SLAM [181] (without loop closure)</cell></row><row><cell cols="2">00</cell><cell></cell><cell></cell><cell>-</cell><cell cols="2">17.5269</cell><cell></cell><cell>16.9438</cell><cell>19.7020</cell><cell>42.9432</cell><cell>113.1838</cell><cell>77.9502</cell></row><row><cell cols="2">01</cell><cell></cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">02</cell><cell></cell><cell></cell><cell>-</cell><cell cols="2">50.5119</cell><cell></cell><cell>14.9513</cell><cell>15.7180</cell><cell>45.0136</cell><cell>116.8108</cell><cell>41.0064</cell></row><row><cell cols="2">03</cell><cell></cell><cell></cell><cell>-</cell><cell cols="2">3.4588</cell><cell></cell><cell>1.9309</cell><cell>3.4340</cell><cell>12.4293</cell><cell>1.3943</cell><cell>1.0182</cell></row><row><cell cols="2">04</cell><cell></cell><cell></cell><cell>58.3970</cell><cell cols="2">2.4414</cell><cell></cell><cell>2.3090</cell><cell>4.1912</cell><cell>5.6721</cell><cell>0.422</cell><cell>0.9302</cell></row><row><cell cols="2">05</cell><cell></cell><cell></cell><cell>-</cell><cell cols="2">8.1513</cell><cell></cell><cell>7.1224</cell><cell>9.4541</cell><cell>30.2511</cell><cell>47.4605</cell><cell>40.3542</cell></row><row><cell cols="2">06</cell><cell></cell><cell></cell><cell>-</cell><cell cols="2">11.5091</cell><cell></cell><cell>8.7421</cell><cell>11.4059</cell><cell>22.5910</cell><cell>55.6173</cell><cell>52.2282</cell></row><row><cell cols="2">07</cell><cell></cell><cell></cell><cell>-</cell><cell cols="2">6.5141</cell><cell></cell><cell>1.9292</cell><cell>2.3099</cell><cell>8.0324</cell><cell>16.7192</cell><cell>16.546</cell></row><row><cell cols="2">08</cell><cell></cell><cell></cell><cell>-</cell><cell cols="2">10.9755</cell><cell></cell><cell>8.3792</cell><cell>9.4699</cell><cell>13.3337</cell><cell>111.0832</cell><cell>51.6215</cell></row><row><cell cols="2">09</cell><cell></cell><cell></cell><cell>-</cell><cell cols="2">10.6873</cell><cell></cell><cell>10.9941</cell><cell>15.0341</cell><cell>28.6044</cell><cell>52.2251</cell><cell>58.1742</cell></row><row><cell cols="2">10</cell><cell></cell><cell></cell><cell>-</cell><cell cols="2">4.8354</cell><cell></cell><cell>2.3163</cell><cell>2.8371</cell><cell>4.1040</cell><cell>11.090</cell><cell>18.4765</cell></row><row><cell></cell><cell></cell><cell>5</cell><cell></cell><cell>·10 −2 Abs Rel [m]</cell><cell></cell><cell>100</cell><cell cols="2">δ &lt; 1.25 [%]</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Cap 80</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Error</cell><cell>2 3 4</cell><cell></cell><cell>Cap 50</cell><cell>Accuracy</cell><cell>97 98 99</cell><cell></cell><cell>Cap 80</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Cap 50</cell></row><row><cell></cell><cell></cell><cell>1</cell><cell>0</cell><cell>200 400</cell><cell></cell><cell>96</cell><cell>0</cell><cell>200 400</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Samples</cell><cell></cell><cell></cell><cell></cell><cell>Samples</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>RMSE [m]</cell><cell></cell><cell></cell><cell cols="2">δ &lt; 1.25 2 [%]</cell></row><row><cell></cell><cell></cell><cell>3</cell><cell></cell><cell></cell><cell></cell><cell>100</cell><cell></cell></row><row><cell>Error</cell><cell cols="2">1.5 2 2.5</cell><cell></cell><cell>Cap 80 Cap 50</cell><cell>Accuracy</cell><cell>99.4 99.6 99.8 99.2</cell><cell></cell><cell>Cap 80 Cap 50</cell></row><row><cell></cell><cell></cell><cell>1</cell><cell>0</cell><cell>200 400</cell><cell></cell><cell>99</cell><cell>0</cell><cell>200 400</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Samples</cell><cell></cell><cell></cell><cell></cell><cell>Samples</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 13 :</head><label>13</label><figDesc>Performance of the proposed SIDE method for different architecture configurations and training strategies. The feature extraction network of the DSN 3 is the developed Model 3. Both pyramid modules, besides the loss function L + BerHu , are implemented as in the analyzes involving the outdoor datasets. The terms Sem and Indoor indicate the pre-training of the DSN 3 with semantic maps and extra depth maps, respectively, provided by the indoor datasets Method Loss Size Speed Abs Rel↓ Sqr Rel↓ RMSE↓ RMSE (log)↓ SILog↓ log10↓ δ &lt; 1.25↑ δ &lt; 1.25 2 ↑ δ &lt; 1.25 3 ↑</figDesc><table><row><cell>DSN 3</cell><cell></cell><cell></cell><cell></cell><cell>LBerHu</cell><cell>12 M 51 f ps</cell><cell>0.169</cell><cell>0.136</cell><cell>0.523</cell><cell>0.208</cell><cell>17.143</cell><cell>0.070</cell><cell>0.768</cell><cell>0.934</cell><cell>0.978</cell></row><row><cell>DSN 3</cell><cell></cell><cell></cell><cell cols="3">La + LBerHu 12 M 51 f ps</cell><cell>0.179</cell><cell>0.144</cell><cell>0.540</cell><cell>0.216</cell><cell>17.315</cell><cell>0.073</cell><cell>0.746</cell><cell>0.926</cell><cell>0.977</cell></row><row><cell>DSN 3</cell><cell></cell><cell></cell><cell></cell><cell>L + BerHu</cell><cell>12 M 51 f ps</cell><cell>0.179</cell><cell>0.151</cell><cell>0.538</cell><cell>0.215</cell><cell>17.277</cell><cell>0.073</cell><cell>0.750</cell><cell>0.924</cell><cell>0.976</cell></row><row><cell>DSN 3</cell><cell></cell><cell></cell><cell></cell><cell>L1</cell><cell>12 M 51 f ps</cell><cell>0.180</cell><cell>0.151</cell><cell>0.548</cell><cell>0.217</cell><cell>17.588</cell><cell>0.073</cell><cell>0.746</cell><cell>0.927</cell><cell>0.978</cell></row><row><cell cols="3">DSN 3 + Pyramid 1</cell><cell></cell><cell>LBerHu</cell><cell>12 M 41 f ps</cell><cell>0.171</cell><cell>0.133</cell><cell>0.521</cell><cell>0.207</cell><cell>16.605</cell><cell>0.070</cell><cell>0.758</cell><cell>0.936</cell><cell>0.982</cell></row><row><cell cols="4">DSN 3 + Pyramid 1 DSN 3 + Pyramid 1 DSN 3 + Pyramid 2 DSN 3 + Pyramid 2 + Sem DSN 3 + Pyramid 1 + Indoor</cell><cell>L + 2 L + 1 L + 1 L + 1 L + 1</cell><cell>12 M 41 f ps 12 M 41 f ps 12 M 32 f ps 12 M 32 f ps 12 M 41 f ps</cell><cell>0.175 0.167 0.174 0.174 0.142</cell><cell>0.138 0.130 0.143 0.139 0.098</cell><cell>0.520 0.519 0.529 0.522 0.445</cell><cell>0.207 0.207 0.210 0.211 0.177</cell><cell>16.462 16.859 16.896 17.231 14.197</cell><cell>0.071 0.069 0.071 0.071 0.059</cell><cell>0.757 0.763 0.756 0.760 0.823</cell><cell>0.934 0.933 0.933 0.932 0.954</cell><cell>0.981 0.981 0.979 0.977 0.987</cell></row><row><cell></cell><cell></cell><cell>9</cell><cell>·10 −2</cell><cell cols="2">Abs Rel [m]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Error</cell><cell>4 5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">20 50 100 200 500</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Samples</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">δ &lt; 1.25 [%]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">100</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">99</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">98</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Accuracy</cell><cell cols="2">95 96 97</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">94</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">93</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">20 50 100 200 500</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Samples</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 14 :</head><label>14</label><figDesc>Analysis of our pipeline that leverages surface normals cues. The configuration DSN 3 + Pyramid 1 + Indoor + SN, along with the L + 1 loss, is maintained in all experiments. Legend: DSN 3 -DSN with Model 3 as encoder; Pyramid 1 -decoder's pyramid-shaped module; Indoor -pre-training with indoor datasets; SN -surface normals ground truth; Edge -Sobel edge detector; Blur -convolution with Gaussian kernel; ψ 1 and ψ 2 -constants of the 2.5D loss function used in the pre-training and training steps respectively Method Size Speed Edge Blur ψ1 ψ2 Abs Rel Sqr Rel RMSE RMSE (log) SILog log10 δ &lt; 1.25 δ &lt; 1.25 2 δ &lt; 1.25 3 DSN 3 + Pyramid 1 + Indoor + SN 12 M 35 f ps</figDesc><table><row><cell>10 0</cell><cell>0.138</cell><cell>0.096</cell><cell>0.444</cell><cell>0.177</cell><cell>14.227</cell><cell>0.058</cell><cell>0.825</cell><cell>0.954</cell><cell>0.986</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 15 :</head><label>15</label><figDesc>Quantitative comparison between our method and others that address the single-view depth estimation task. The results are evaluated on the NYU Depth V2 dataset<ref type="bibr" target="#b17">[18]</ref>. DSN † refers to the DSN setup that provides the most accurate overall resultsMethodAbs Rel↓ RMSE↓ log10↓ δ &lt; 1.25↑ δ &lt; 1.25 2 ↑ δ &lt; 1.25 3 ↑ Saxena et al.</figDesc><table><row><cell>[157]</cell><cell>0.349</cell><cell>1.214</cell><cell>-</cell><cell>0.447</cell><cell>0.745</cell><cell>0.897</cell></row><row><cell>Karsch et al. [182]</cell><cell>0.349</cell><cell>1.210</cell><cell>1.131</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Liu et al. [183]</cell><cell>0.335</cell><cell>1.060</cell><cell>1.127</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Ladicky et al. [184]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.542</cell><cell>0.829</cell><cell>0.941</cell></row><row><cell>Eigen et al. [28]</cell><cell>0.214</cell><cell>0.877</cell><cell>0.285</cell><cell>0.611</cell><cell>0.887</cell><cell>0.971</cell></row><row><cell>Wang et al. [44]</cell><cell>0.220</cell><cell>0.824</cell><cell>-</cell><cell>0.605</cell><cell>0.890</cell><cell>0.970</cell></row><row><cell>HCRF [185]</cell><cell>0.232</cell><cell>0.821</cell><cell>0.094</cell><cell>0.621</cell><cell>0.886</cell><cell>0.968</cell></row><row><cell>DCNF [158]</cell><cell>0.213</cell><cell>0.759</cell><cell>0.087</cell><cell>0.650</cell><cell>0.906</cell><cell>0.976</cell></row><row><cell>NR forest [186]</cell><cell>0.187</cell><cell>0.744</cell><cell>0.078</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MS-CNN [48]</cell><cell>0.158</cell><cell>0.641</cell><cell>-</cell><cell>0.769</cell><cell>0.950</cell><cell>0.988</cell></row><row><cell>Chakrabarti et al. [187]</cell><cell>0.149</cell><cell>0.620</cell><cell>-</cell><cell>0.806</cell><cell>0.958</cell><cell>0.987</cell></row><row><cell>Li et al. [188]</cell><cell>0.152</cell><cell>0.611</cell><cell>0.064</cell><cell>0.789</cell><cell>0.955</cell><cell>988</cell></row><row><cell>SOM [172]</cell><cell>0.136</cell><cell>0.604</cell><cell>0.067</cell><cell>0.814</cell><cell>0.959</cell><cell>0.990</cell></row><row><cell>Xu et al. [41]</cell><cell>0.125</cell><cell>0.593</cell><cell>-</cell><cell>0.806</cell><cell>0.952</cell><cell>0.986</cell></row><row><cell>MS-CRF [14]</cell><cell>0.121</cell><cell>0.586</cell><cell>0.052</cell><cell>0.811</cell><cell>0.954</cell><cell>0.987</cell></row><row><cell>PAD-Net [189]</cell><cell>0.120</cell><cell>0.582</cell><cell>-</cell><cell>0.817</cell><cell>0.954</cell><cell>0.987</cell></row><row><cell>DeepLabV3+ (F10) [169]</cell><cell>0.162</cell><cell>0.575</cell><cell>-</cell><cell>0.772</cell><cell>0.942</cell><cell>0.984</cell></row><row><cell>FCRN [29]</cell><cell>0.127</cell><cell>0.573</cell><cell>0.055</cell><cell>0.811</cell><cell>0.953</cell><cell>0.988</cell></row><row><cell>Lee et al. [190]</cell><cell>0.139</cell><cell>0.572</cell><cell>-</cell><cell>0.815</cell><cell>0.963</cell><cell>0.991</cell></row><row><cell>Qi et al. [55]</cell><cell>0.128</cell><cell>0.569</cell><cell>0.057</cell><cell>0.834</cell><cell>0.960</cell><cell>0.990</cell></row><row><cell>Index Network [191]</cell><cell>-</cell><cell>0.565</cell><cell>-</cell><cell>0.786</cell><cell>-</cell><cell>-</cell></row><row><cell>RF-LW [192]</cell><cell>0.149</cell><cell>0.565</cell><cell>0.105</cell><cell>0.790</cell><cell>0.955</cell><cell>0.990</cell></row><row><cell>Cao et al. [34]</cell><cell>0.141</cell><cell>0.540</cell><cell>0.060</cell><cell>0.790</cell><cell>0.955</cell><cell>0.990</cell></row><row><cell>RelativeDepth [193]</cell><cell>0.131</cell><cell>0.538</cell><cell>0.087</cell><cell>0.837</cell><cell>0.971</cell><cell>0.994</cell></row><row><cell>SC-SfMLearner-ResNet18 [194]</cell><cell>0.147</cell><cell>0.536</cell><cell>0.062</cell><cell>0.804</cell><cell>0.950</cell><cell>0.986</cell></row><row><cell>SENet-154 [195]</cell><cell>0.115</cell><cell>0.530</cell><cell>0.050</cell><cell>0.866</cell><cell>0.975</cell><cell>0.993</cell></row><row><cell>Kim et al. [175]</cell><cell>0.117</cell><cell>0.525</cell><cell>-</cell><cell>0.825</cell><cell>0.976</cell><cell>0.993</cell></row><row><cell>SARPN [196]</cell><cell>0.111</cell><cell>0.514</cell><cell>0.048</cell><cell>0.846</cell><cell>0.968</cell><cell>0.994</cell></row><row><cell>DORN [26]</cell><cell>0.115</cell><cell>0.509</cell><cell>0.051</cell><cell>0.828</cell><cell>0.965</cell><cell>0.992</cell></row><row><cell>AdaDepth [160]</cell><cell>0.114</cell><cell>0.506</cell><cell>-</cell><cell>0.856</cell><cell>0.966</cell><cell>0.991</cell></row><row><cell>TRL [197]</cell><cell>0.144</cell><cell>0.501</cell><cell>0.181</cell><cell>0.815</cell><cell>0.962</cell><cell>0.992</cell></row><row><cell>Zhang et al. [197]</cell><cell>0.144</cell><cell>0.501</cell><cell>0.181</cell><cell>0.815</cell><cell>0.962</cell><cell>0.992</cell></row><row><cell>PHN et al. [161]</cell><cell>0.144</cell><cell>0.501</cell><cell>-</cell><cell>0.835</cell><cell>0.962</cell><cell>0.992</cell></row><row><cell>Su et al. [168]</cell><cell>0.137</cell><cell>0.498</cell><cell>0.058</cell><cell>0.826</cell><cell>0.967</cell><cell>0.995</cell></row><row><cell>SDC-Depth [198]</cell><cell>0.128</cell><cell>0.497</cell><cell>0.174</cell><cell>0.845</cell><cell>0.966</cell><cell>0.990</cell></row><row><cell>PAP [56]</cell><cell>0.121</cell><cell>0.497</cell><cell>0.175</cell><cell>0.846</cell><cell>0.968</cell><cell>0.994</cell></row><row><cell>ACAN [42]</cell><cell>0.138</cell><cell>0.496</cell><cell>0.101</cell><cell>0.826</cell><cell>0.964</cell><cell>0.990</cell></row><row><cell>SharpNet [3]</cell><cell>0.139</cell><cell>0.495</cell><cell>0.047</cell><cell>0.888</cell><cell>0.979</cell><cell>0.995</cell></row><row><cell>DenseDepth [15]</cell><cell>0.123</cell><cell>0.465</cell><cell>0.053</cell><cell>0.846</cell><cell>0.974</cell><cell>0.994</cell></row><row><cell>DSN †</cell><cell>0.132</cell><cell>0.429</cell><cell>0.056</cell><cell>0.834</cell><cell>0.959</cell><cell>0.987</cell></row><row><cell>VNL [11]</cell><cell>0.108</cell><cell>0.416</cell><cell>0.048</cell><cell>0.875</cell><cell>0.976</cell><cell>0.994</cell></row><row><cell>BTS [13]</cell><cell>0.110</cell><cell>0.392</cell><cell>0.047</cell><cell>0.885</cell><cell>0.978</cell><cell>0.994</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 16 :</head><label>16</label><figDesc>Performance of the proposed CNN and other techniques on the surface normals ground truth of the NYU Depth V2 [18]. Legend: DSN † -DSN configuration that provides the most accurate overall results; DSN ‡ -same DSN configuration as the DSN †, yet with the Gaussian filter; Mask -mask out the invalid pixels related to the surface normals ground truth Method Mean↓ Median↓ RMSE↓ 11.25 • ↑ 22.50 • ↑ 30 • ↑ Fouhey et al.</figDesc><table><row><cell>[53]</cell><cell>36.3</cell><cell>19.2</cell><cell>-</cell><cell>16.4</cell><cell>36.6</cell><cell>48.2</cell></row><row><cell>UIOW [54]</cell><cell>35.2</cell><cell>17.9</cell><cell>-</cell><cell>40.5</cell><cell>54.1</cell><cell>58.9</cell></row><row><cell>Ladicky et al. [199]</cell><cell>33.5</cell><cell>23.1</cell><cell>-</cell><cell>27.7</cell><cell>49.0</cell><cell>58.7</cell></row><row><cell>MS-CNN [48]</cell><cell>23.7</cell><cell>15.5</cell><cell>-</cell><cell>39.2</cell><cell>62.0</cell><cell>71.1</cell></row><row><cell>Deep3D [200]</cell><cell>26.9</cell><cell>14.8</cell><cell>-</cell><cell>42.0</cell><cell>61.2</cell><cell>68.2</cell></row><row><cell>SkipNet [147]</cell><cell>19.8</cell><cell>12.0</cell><cell>28.2</cell><cell>47.9</cell><cell>70.0</cell><cell>77.8</cell></row><row><cell>SURGE [2]</cell><cell>20.6</cell><cell>12.2</cell><cell>-</cell><cell>47.3</cell><cell>68.9</cell><cell>76.6</cell></row><row><cell>Qi et al. [55]</cell><cell>19.0</cell><cell>11.8</cell><cell>26.9</cell><cell>48.4</cell><cell>71.5</cell><cell>79.5</cell></row><row><cell>PAP [56]</cell><cell>18.6</cell><cell>11.7</cell><cell>25.5</cell><cell>48.8</cell><cell>72.2</cell><cell>79.8</cell></row><row><cell>DSN † + Mask</cell><cell>8.6</cell><cell>5.5</cell><cell>12.2</cell><cell>72.2</cell><cell>91.0</cell><cell>96.8</cell></row><row><cell>DSN †</cell><cell>18.7</cell><cell>6.9</cell><cell>33.7</cell><cell>63.2</cell><cell>79.7</cell><cell>84.8</cell></row><row><cell>DSN ‡ + Mask</cell><cell>8.6</cell><cell>5.5</cell><cell>12.2</cell><cell>72.3</cell><cell>91.0</cell><cell>96.9</cell></row><row><cell>DSN ‡</cell><cell>18.7</cell><cell>6.8</cell><cell>33.7</cell><cell>63.3</cell><cell>79.7</cell><cell>84.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 17 :</head><label>17</label><figDesc>NYU Depth V2 [18] metrics obtained by our depth completion pipeline. The pattern DSN 3 + Pyramid 1 + Indoor, jointly with the L + 1 function, is selected for all the experiments that do not involve the surface normals ground truth. DSN 3 + Pyramid 1 + Indoor + SN † is the DSN setup with the surface normals module which yields the most accurate results Method Cap Samples Abs Rel↓ RMSE↓ log10↓ δ &lt; 1.25↑ δ &lt; 1.25 2 ↑ δ &lt; 1.25 3 ↑</figDesc><table><row><cell>DSN 3 + Pyramid 1 + Indoor</cell><cell>10</cell><cell>20</cell><cell>0.046</cell><cell>0.221</cell><cell>0.020</cell><cell>0.964</cell><cell>0.992</cell><cell>0.998</cell></row><row><cell>DSN 3 + Pyramid 1 + Indoor</cell><cell>10</cell><cell>50</cell><cell>0.032</cell><cell>0.179</cell><cell>0.014</cell><cell>0.979</cell><cell>0.995</cell><cell>0.998</cell></row><row><cell>DSN 3 + Pyramid 1 + Indoor</cell><cell>10</cell><cell>100</cell><cell>0.024</cell><cell>0.151</cell><cell>0.010</cell><cell>0.986</cell><cell>0.996</cell><cell>0.999</cell></row><row><cell>DSN 3 + Pyramid 1 + Indoor</cell><cell>10</cell><cell>200</cell><cell>0.018</cell><cell>0.132</cell><cell>0.008</cell><cell>0.990</cell><cell>0.997</cell><cell>0.999</cell></row><row><cell>DSN 3 + Pyramid 1 + Indoor</cell><cell>10</cell><cell>500</cell><cell>0.013</cell><cell>0.106</cell><cell>0.006</cell><cell>0.994</cell><cell>0.998</cell><cell>0.999</cell></row><row><cell>DSN 3 + Pyramid 1 + Indoor + SN †</cell><cell>10</cell><cell>200</cell><cell>0.017</cell><cell>0.128</cell><cell>0.007</cell><cell>0.991</cell><cell>0.998</cell><cell>0.999</cell></row><row><cell>DSN 3 + Pyramid 1 + Indoor + SN †</cell><cell>10</cell><cell>500</cell><cell>0.012</cell><cell>0.102</cell><cell>0.005</cell><cell>0.994</cell><cell>0.999</cell><cell>1.000</cell></row><row><cell>DSN 3 + Pyramid 1 + Indoor</cell><cell>5</cell><cell>20</cell><cell>0.045</cell><cell>0.191</cell><cell>0.019</cell><cell>0.965</cell><cell>0.992</cell><cell>0.998</cell></row><row><cell>DSN 3 + Pyramid 1 + Indoor</cell><cell>5</cell><cell>50</cell><cell>0.031</cell><cell>0.153</cell><cell>0.013</cell><cell>0.980</cell><cell>0.995</cell><cell>0.999</cell></row><row><cell>DSN 3 + Pyramid 1 + Indoor</cell><cell>5</cell><cell>100</cell><cell>0.023</cell><cell>0.129</cell><cell>0.010</cell><cell>0.987</cell><cell>0.997</cell><cell>0.999</cell></row><row><cell>DSN 3 + Pyramid 1 + Indoor</cell><cell>5</cell><cell>200</cell><cell>0.018</cell><cell>0.111</cell><cell>0.008</cell><cell>0.990</cell><cell>0.998</cell><cell>0.999</cell></row><row><cell>DSN 3 + Pyramid 1 + Indoor</cell><cell>5</cell><cell>500</cell><cell>0.013</cell><cell>0.090</cell><cell>0.005</cell><cell>0.994</cell><cell>0.998</cell><cell>1.000</cell></row><row><cell>DSN 3 + Pyramid 1 + Indoor + SN †</cell><cell>5</cell><cell>200</cell><cell>0.017</cell><cell>0.107</cell><cell>0.007</cell><cell>0.991</cell><cell>0.998</cell><cell>0.999</cell></row><row><cell>DSN 3 + Pyramid 1 + Indoor + SN †</cell><cell>5</cell><cell>500</cell><cell>0.012</cell><cell>0.086</cell><cell>0.005</cell><cell>0.994</cell><cell>0.999</cell><cell>1.000</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 18 :</head><label>18</label><figDesc>Quantitative evaluation of our best depth completion framework (DSN †) and other techniques in the state-of-the-art. The NYU Depth V2 [18] is used to compute all the metrics Method Samples Abs Rel↓ RMSE↓ δ &lt; 1.25↑ δ &lt; 1.25 2 ↑ δ &lt; 1.25 3 ↑ Ma et al.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>[7]</cell><cell></cell><cell>200</cell><cell>0.044</cell><cell>0.230</cell><cell>0.971</cell><cell>0.994</cell><cell>0.998</cell></row><row><cell></cell><cell cols="3">NConv [106]</cell><cell></cell><cell>200</cell><cell>0.027</cell><cell>0.173</cell><cell>0.982</cell><cell>0.996</cell><cell>0.999</cell></row><row><cell></cell><cell cols="3">GuideNet [111]</cell><cell></cell><cell>200</cell><cell>0.024</cell><cell>0.142</cell><cell>0.988</cell><cell>0.998</cell><cell>1.000</cell></row><row><cell></cell><cell cols="3">DSN †</cell><cell></cell><cell>200</cell><cell>0.017</cell><cell>0.128</cell><cell>0.991</cell><cell>0.998</cell><cell>0.999</cell></row><row><cell cols="4">Silberman et al. [18]</cell><cell></cell><cell>500</cell><cell>0.084</cell><cell>0.479</cell><cell>0.924</cell><cell>0.976</cell><cell>0.989</cell></row><row><cell></cell><cell cols="3">TGV [201]</cell><cell></cell><cell>500</cell><cell>0.123</cell><cell>0.635</cell><cell>0.819</cell><cell>0.930</cell><cell>0.968</cell></row><row><cell cols="4">Zhang et al. [202]</cell><cell></cell><cell>500</cell><cell>0.042</cell><cell>0.228</cell><cell>0.971</cell><cell>0.993</cell><cell>0.997</cell></row><row><cell></cell><cell cols="3">Ma et al. [7]</cell><cell></cell><cell>500</cell><cell>0.043</cell><cell>0.204</cell><cell>0.978</cell><cell>0.996</cell><cell>0.999</cell></row><row><cell cols="5">Ma et al. [7] + Bilateral [203]</cell><cell>500</cell><cell>0.084</cell><cell>0.479</cell><cell>0.924</cell><cell>0.976</cell><cell>0.989</cell></row><row><cell cols="4">Ma et al. [7] + SPN [178]</cell><cell></cell><cell>500</cell><cell>0.031</cell><cell>0.172</cell><cell>0.983</cell><cell>0.997</cell><cell>0.999</cell></row><row><cell></cell><cell cols="3">NConv [106]</cell><cell></cell><cell>500</cell><cell>0.018</cell><cell>0.129</cell><cell>0.990</cell><cell>0.998</cell><cell>1.000</cell></row><row><cell></cell><cell cols="3">CSPN [112]</cell><cell></cell><cell>500</cell><cell>0.016</cell><cell>0.117</cell><cell>0.992</cell><cell>0.999</cell><cell>1.000</cell></row><row><cell cols="4">Imran et al. [151]</cell><cell></cell><cell>500</cell><cell>0.013</cell><cell>0.118</cell><cell>0.994</cell><cell>0.999</cell><cell>-</cell></row><row><cell></cell><cell cols="3">CSPN++ [96]</cell><cell></cell><cell>500</cell><cell>-</cell><cell>0.116</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="4">DeepLiDAR [103]</cell><cell></cell><cell>500</cell><cell>0.022</cell><cell>0.115</cell><cell>0.993</cell><cell>0.999</cell><cell>1.000</cell></row><row><cell></cell><cell cols="3">Xu et al. [118]</cell><cell></cell><cell>500</cell><cell>0.018</cell><cell>0.112</cell><cell>0.995</cell><cell>0.999</cell><cell>1.000</cell></row><row><cell></cell><cell cols="3">GuideNet [111]</cell><cell></cell><cell>500</cell><cell>0.015</cell><cell>0.101</cell><cell>0.995</cell><cell>0.999</cell><cell>1.000</cell></row><row><cell></cell><cell cols="3">DSN †</cell><cell></cell><cell>500</cell><cell>0.012</cell><cell>0.102</cell><cell>0.994</cell><cell>0.999</cell><cell>1.000</cell></row><row><cell></cell><cell cols="3">NLSPN [113]</cell><cell></cell><cell>500</cell><cell>0.012</cell><cell>0.092</cell><cell>0.996</cell><cell>0.999</cell><cell>1.000</cell></row><row><cell></cell><cell></cell><cell>5</cell><cell cols="3">·10 −2 Abs Rel [m]</cell><cell></cell><cell></cell><cell>100</cell><cell>δ &lt; 1.25 [%]</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Cap 10</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Error</cell><cell>2 3 4</cell><cell></cell><cell></cell><cell>Cap 5</cell><cell></cell><cell>Accuracy</cell><cell>97 98 99</cell><cell>Cap 10</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Cap 5</cell></row><row><cell></cell><cell></cell><cell>1</cell><cell>0</cell><cell cols="2">200 400</cell><cell></cell><cell></cell><cell>96</cell><cell>0</cell><cell>200 400</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Samples</cell><cell></cell><cell></cell><cell>Samples</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">RMSE [m]</cell><cell></cell><cell></cell><cell>δ &lt; 1.25 2 [%]</cell></row><row><cell></cell><cell cols="2">0.25</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>100</cell></row><row><cell>Error</cell><cell cols="2">0.1 0.15 0.2</cell><cell>0</cell><cell cols="2">200 400 Cap 10 Cap 5</cell><cell></cell><cell>Accuracy</cell><cell>99 99.4 99.6 99.8 99.2</cell><cell>0</cell><cell>200 400 Cap 10 Cap 5</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Samples</cell><cell></cell><cell></cell><cell>Samples</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 19 :</head><label>19</label><figDesc>Analysis of the surface normals predictions from our best depth completion CNN (DSN †) when it receives different amounts of sparse samples as input. Mask stands for the operation of masking out the invalid pixels corresponding to the surface normals ground truth Method Samples Mean↓ Median↓ RMSE↓ 11.25 • ↑ 22.50 • ↑ 30 • ↑</figDesc><table><row><cell>DSN † + Mask</cell><cell>200</cell><cell>7.6</cell><cell>3.5</cell><cell>12.2</cell><cell>77.1</cell><cell>91.0</cell><cell>95.6</cell></row><row><cell>DSN †</cell><cell>200</cell><cell>17.8</cell><cell>4.6</cell><cell>33.7</cell><cell>67.6</cell><cell>79.7</cell><cell>83.7</cell></row><row><cell>DSN † + Mask</cell><cell>500</cell><cell>6.1</cell><cell>3.0</cell><cell>9.6</cell><cell>82.9</cell><cell>94.9</cell><cell>98.1</cell></row><row><cell>DSN †</cell><cell>500</cell><cell>16.5</cell><cell>3.9</cell><cell>33.0</cell><cell>72.6</cell><cell>83.2</cell><cell>86</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Toward domain independence for learning-based monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mancini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Costante</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Valigi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Ciarfuglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Delmerico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1778" to="1785" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Surge: Surface regularized geometry estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="172" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sharpnet: Fast and accurate recovery of occluding contours in monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ramamonjisoa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep optics for monocular depth estimation and 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wetzstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10193" to="10202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cnn-svo: Improving the mapping in semi-direct visual odometry using single-image depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename><surname>Loo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Amiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mashohor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5218" to="5223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cnn-slam: Realtime dense monocular slam with learned depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tateno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6243" to="6252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sparse-to-dense: Depth prediction from sparse depth samples and a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karaman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sparse and noisy lidar completion with rgb guidance and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th International Conference on Machine Vision Applications (MVA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Self-supervised sparseto-dense: Self-supervised depth completion from lidar and monocular camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">V</forename><surname>Cavalheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karaman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3288" to="3295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Semantically-guided representation learning for self-supervised monocular depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Guizilini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ambrus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.12319</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Enforcing geometric constraints of virtual normal for depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5684" to="5693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">H</forename><surname>Suh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.10326</idno>
		<title level="m">From big to small: Multi-scale local planar guidance for monocular depth estimation</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-scale continuous crfs as sequential deep networks for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5354" to="5362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Alhashim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wonka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.11941</idno>
		<title level="m">High quality monocular depth estimation via transfer learning</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Indoor scene segmentation using a structured light sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE international conference on computer vision workshops (ICCV workshops)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="601" to="608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="746" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Closing the loop in scene interpretation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Recovering surface layout from an image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="151" to="172" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Recovering the spatial layout of cluttered rooms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hedau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th international conference on computer vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1849" to="1856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep ordinal regression network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Batmanghelich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2002" to="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unsupervised monocular depth estimation with left-right consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="270" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2366" to="2374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deeper depth prediction with fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth international conference on 3D vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="239" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition<address><addrLine>Las Vegas, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zwald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lambert-Lacroix</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.6868</idno>
		<title level="m">The berhu penalty and the grouped effect</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep convolutional neural fields for depth estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5162" to="5170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Estimating depth from monocular images as classification using deep fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3174" to="3182" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Parse geometry from a line: Monocular depth estimation with partial laser observation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kodagoda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5059" to="5066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Convolutional lstm network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xingjian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="802" to="810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Depthnet: A recurrent neural network architecture for monocular depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Bhandarkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Prasad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="283" to="291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Recurrent neural network for (un-) supervised learning of monocular video visual odometry and depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Pizer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5555" to="5564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Structured attention guided convolutional neural fields for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3917" to="3925" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Attention-based context aggregation network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.10137</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Look deeper into depth: Monocular depth estimation with semantic booster and attention-driven loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="53" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Towards unified depth and semantic prediction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2800" to="2809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Recovering occlusion boundaries from an image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="328" to="346" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Single image depth estimation from predicted semantic labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1253" to="1260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning rich features from rgb-d images for object detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="345" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2650" to="2658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Joint semantic segmentation and depth estimation with deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Košecká</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="611" to="619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Shape-fromshading: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-S</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Cryer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="690" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Binford</surname></persName>
		</author>
		<title level="m">Visual perception by computer. leee cont. on systems and control</title>
		<imprint>
			<date type="published" when="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning 3-d scene structure from a single still image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 11th International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Data-driven 3d primitives for single image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3392" to="3399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Unfolding an indoor origami world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="687" to="702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Geonet: Geometric neural network for joint depth and surface normal estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="283" to="291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Patternaffinitive propagation across depth, surface normal and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4106" to="4115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Unsupervised cnn for single view depth estimation: Geometry to the rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Bg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="740" to="756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Superdepth: Self-supervised, super-resolved monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ambruş</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9250" to="9256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1851" to="1858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Digging into self-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3828" to="3838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Semi-supervised deep learning for monocular depth map prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kuznietsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stuckler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6647" to="6655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Semi-supervised monocular depth estimation with left-right consistency using deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Amiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename><surname>Loo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Robotics and Biomimetics (ROBIO)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="602" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Deep virtual stereo odometry: Leveraging deep depth prediction for monocular direct sparse odometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stuckler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="817" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Enhancing selfsupervised monocular depth estimation with traditional visual odometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Andraghetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Myriokefalitakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Dovesi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Luque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pieropan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mattoccia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="424" to="433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Geometry meets semantics for semi-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Z</forename><surname>Ramirez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mattoccia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Di</forename><surname>Stefano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="298" to="313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Single view stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="155" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Learning monocular depth estimation infusing traditional stereo knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Aleotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mattoccia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9799" to="9809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Undeepvo: Monocular visual odometry through unsupervised deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE international conference on robotics and automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7286" to="7291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Undemon: Unsupervised deep network for depth and ego-motion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Majumdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1082" to="1088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Two deterministic half-quadratic regularization algorithms for computed imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Charbonnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Blanc-Feraud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Aubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Barlaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 1st International Conference on Image Processing</title>
		<meeting>1st International Conference on Image Processing</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1994" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="168" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Learning monocular depth estimation with unsupervised trinocular assumptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mattoccia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="324" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Learning monocular depth by distilling cross-domain stereo networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="484" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Monocular depth prediction using generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Bhandarkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Prasad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="300" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Generative adversarial networks for unsupervised monocular depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Aleotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mattoccia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Unsupervised single image underwater depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="624" to="628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Self-supervised monocular image depth learning and confidence estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">R</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">W</forename><surname>John</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">381</biblScope>
			<biblScope unit="page" from="272" to="281" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">On the uncertainty of self-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Aleotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mattoccia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3227" to="3237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Forget about the lidar: Selfsupervised depth estimators with med probability volumes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Gonzalezbello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="page">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Unsupervised adversarial depth estimation using cycled generative networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pilzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Puscas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="587" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Towards real-time unsupervised monocular depth estimation on cpu</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Aleotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mattoccia</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2018</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
				<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5848" to="5854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Enabling energy-efficient unsupervised monocular depth estimation on armv7-based platforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Peluso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cipolletta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Calimera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mattoccia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automation &amp; Test in Europe Conference &amp; Exhibition (DATE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1703" to="1708" />
		</imprint>
	</monogr>
	<note>2019 Design</note>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Lightweight monocular depth estimation model by joint end-to-end filter pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Elkerdawy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4290" to="4294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Unsupervised learning of geometry from videos with edge-aware depthnormal consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Lego: Learning edge with geometry all at once by watching videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="225" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Geonet: Unsupervised learning of dense depth, optical flow and camera pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1983" to="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from monocular video using 3d geometric constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mahjourian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5667" to="5675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Learning depth from monocular videos using direct methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Buenaposada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2022" to="2030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Depth prediction without the sensors: Leveraging structure for unsupervised learning from monocular videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Casser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mahjourian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8001" to="8008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Teed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.04605</idno>
		<title level="m">Deepv2d: Video to depth with differentiable structure from motion</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Unsupervised monocular depth and ego-motion learning with structure and semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Casser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mahjourian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Competitive collaboration: Joint unsupervised learning of depth, camera motion, optical flow and motion segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Balles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12240" to="12249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Self-supervised learning with geometric constraints in monocular video: Connecting flow, depth, and camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7063" to="7072" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">Packnet-sfm: 3d packing for self-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Guizilini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ambrus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.02693</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<title level="m" type="main">Unsupervised learning of depth and ego-motion from cylindrical panoramic video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ventura</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.00979</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Mininet: An extremely lightweight convolutional neural network for real-time unsupervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">166</biblScope>
			<biblScope unit="page" from="255" to="267" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05377</idno>
		<title level="m">Cspn++: Learning context and resource aware convolutional spatial propagation networks for depth completion</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Depth map inpainting under a second-order smoothness prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heikkilä</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Scandinavian Conference on Image Analysis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="555" to="566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Probability contour guided depth map inpainting and superresolution using non-local total generalized variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-F</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="9003" to="9020" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Fast depth image denoising and enhancement using a deep convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2499" to="2503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Layer depth denoising and completion for structured-light rgb-d cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><forename type="middle">S</forename><surname>Cheung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1187" to="1194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Shading-based shape refinement of rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-K</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1415" to="1422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Sparse depth super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2245" to="2253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Deeplidar: Deep surface normal guided depth prediction for outdoor scene from sparse lidar data and single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3313" to="3322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">In defense of classical image processing: Fast depth completion on the cpu</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th Conference on Computer and Robot Vision (CRV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="16" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pinggera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<title level="m">German conference on pattern recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="37" to="48" />
		</imprint>
	</monogr>
	<note>Semantically guided depth upsampling</note>
</biblStruct>

<biblStruct xml:id="b106">
	<monogr>
		<title level="m" type="main">Confidence propagation through cnns for guided sparse depth regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eldesokey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Deep convolutional compressed sensing for lidar depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="499" to="513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Hms-net: Hierarchical multi-scale sparsity-invariant network for sparse depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<monogr>
		<title level="m" type="main">Revisiting sparsity invariant convolution: A network for image guided depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Belyaev</surname></persName>
		</author>
		<imprint>
			<publisher>IEEE Access</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">From depth what can you see? depth completion via auxiliary image reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11306" to="11315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-P</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.01238</idno>
		<title level="m">Learning guided convolutional network for depth completion</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Depth estimation via affinity learned with convolutional spatial propagation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="103" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<monogr>
		<title level="m" type="main">Non-local spatial propagation network for depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.10042</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b114">
	<monogr>
		<title level="m" type="main">Deformable spatial propagation network for depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.04251</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Learning joint 2d-3d representations for depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10023" to="10032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">A multiscale guided cascade hourglass network for depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="32" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Learning morphological operators for depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dimitrievski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veelaert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Philips</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Advanced Concepts for Intelligent Vision Systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="450" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Depth completion from sparse lidar data with depth-normal constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2811" to="2820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Phasecam3d-learning phase masks for passive single view depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Boominathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veeraraghavan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Computational Photography (ICCP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Deep architecture with cross guidance between single image and sparse lidar data for depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="79801" to="79810" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Supervising the new with the old: learning sfm from sfm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Klodt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="698" to="713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Df-net: Unsupervised joint learning of depth and flow using cross-task consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="36" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Deep attention-based classification network for robust depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="663" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Pixel-wise attentional gating for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1024" to="1033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Monocular depth estimation with hierarchical fusion of dilated cnns and soft-weighted-sum inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page" from="328" to="339" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Learn stereo, infer mono: Siamese networks for self-supervised, monocular, depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Deep hierarchical guidance and regularization learning for end-to-end depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page" from="430" to="442" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Sdnet: Semantically guided depth estimation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ochs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kretz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="288" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Unsupervised highresolution depth learning from videos with dual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6872" to="6881" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
	<note>Mobilenetv2: Inverted residuals and linear bottlenecks</note>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Fast convolutional neural network for real-time robotic grasp detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">G</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Grassi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">19th International Conference on Advanced Robotics (ICAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="49" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Denseaspp for semantic segmentation in street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3684" to="3692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Agarap</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.08375</idno>
		<title level="m">Deep learning using rectified linear units (relu)</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b134">
	<monogr>
		<title level="m" type="main">Fast and accurate deep network learning by exponential linear units (elus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-A</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07289</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">Sun rgb-d: A rgb-d scene understanding benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="567" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Janoch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<title level="m">A category-level 3d object dataset: Putting the kinect to work</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="141" to="165" />
		</imprint>
	</monogr>
	<note>Consumer depth cameras for computer vision</note>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Sun3d: A database of big spaces reconstructed using sfm and object labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1625" to="1632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">On regression losses for deep depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">Le</forename><surname>Saux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Trouvé-Peloux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Almansa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Champagnat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">25th IEEE International Conference on Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2915" to="2919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">A quantitative analysis of current practices in optical flow estimation and the principles behind them</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="115" to="137" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<monogr>
		<title level="m" type="main">Log hyperbolic cosine loss improves variational auto-encoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<monogr>
		<title level="m" type="main">Feature extraction and image processing for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nixon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aguado</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>Academic press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Gaussian filters for parameter and state estimation: A general review of theory and recent trends</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">H</forename><surname>Afshari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Gadsden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Habibi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">135</biblScope>
			<biblScope unit="page" from="218" to="238" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Vision meets robotics: The kitti dataset</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1231" to="1237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">Marr revisited: 2d-3d alignment via surface normal prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5965" to="5974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">Sparse-tocontinuous: Enhancing monocular depth estimation using occupancy maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N. Dos Santos</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Guizilini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Grassi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">19th International Conference on Advanced Robotics (ICAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="793" to="800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">Dfusenet: Deep fusion of rgb and sparse depth information for image guided dense depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Shivakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Transportation Systems Conference (ITSC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="13" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">Unsupervised depth completion from visual inertial odometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tsuei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1899" to="1906" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">Depth coefficients for depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Imran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Morris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12438" to="12447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hekmatian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Al-Stouhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.10148</idno>
		<title level="m">Conf-net: Predicting depth completion error-map forhigh-confidence dense 3d point-cloud</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">Dfinenet: Ego-motion estimation and depth refinement from sparse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Shivakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.06397</idno>
	</analytic>
	<monogr>
		<title level="m">noisy depth input with rgb guidance</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">Sparse and dense data with cnns: Depth completion and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">De</forename><surname>Charette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wirbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Perrotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nashashibi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="52" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main">Dense depth posterior (ddp) from single image and sparse range</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3353" to="3362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<analytic>
		<title level="a" type="main">Augmented reality meets computer vision: Efficient data generation for urban driving scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Alhaija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mustikovela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint/>
	</monogr>
	<note>IJCV</note>
</biblStruct>

<biblStruct xml:id="b157">
	<monogr>
		<title level="m" type="main">Make3d: Depth perception from a single still image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>AAAI</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1571" to="1576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">Learning depth from single monocular images using deep convolutional neural fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="2024" to="2039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<monogr>
		<title level="m" type="main">Unsupervised learning of geometry with edge-aware depth-normal consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.03665</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b160">
	<analytic>
		<title level="a" type="main">Adadepth: Unsupervised content congruent adaptation for depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nath Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Krishna</forename><surname>Uppala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2656" to="2665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<analytic>
		<title level="a" type="main">Progressive hardmining network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3691" to="3702" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b162">
	<analytic>
		<title level="a" type="main">Unsupervised learning of monocular depth estimation and visual odometry with deep feature reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Weerasekera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="340" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b163">
	<analytic>
		<title level="a" type="main">Bilateral cyclic constraint and adaptive regularization for unsupervised monocular depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5644" to="5653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b164">
	<analytic>
		<title level="a" type="main">Structured adversarial training for unsupervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sakurikar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="314" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b165">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.06125</idno>
		<title level="m">Every pixel counts++: Joint learning of geometry and motion with 3d holistic understanding</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b166">
	<analytic>
		<title level="a" type="main">Depth from videos in the wild: Unsupervised monocular depth learning from unknown cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jonschkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8977" to="8986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b167">
	<analytic>
		<title level="a" type="main">Towards scene understanding: Unsupervised monocular depth estimation with semantic-aware representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><forename type="middle">F</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2624" to="2632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b168">
	<analytic>
		<title level="a" type="main">Monocular depth estimation using information exchange network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b169">
	<analytic>
		<title level="a" type="main">Single image depth estimation trained via depth from defocus cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7683" to="7692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b170">
	<analytic>
		<title level="a" type="main">Monocular depth estimation by learning from heterogeneous datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gurram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Urfalioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Halfaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bouzaraa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>López</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2176" to="2181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b171">
	<analytic>
		<title level="a" type="main">Refine and distill: Exploiting cycle-inconsistency and knowledge distillation for unsupervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pilzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lathuiliere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9768" to="9777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b172">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-C</forename><surname>Lien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.04594</idno>
		<title level="m">Structureattentioned memory network for monocular depth estimation</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b173">
	<analytic>
		<title level="a" type="main">Self-supervised monocular depth hints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Turmukhambetov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2162" to="2171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b174">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.04807</idno>
		<title level="m">Ba-net: Dense bundle adjustment network</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b175">
	<analytic>
		<title level="a" type="main">Deep monocular depth estimation via integration of global and local predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="4131" to="4144" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b176">
	<analytic>
		<title level="a" type="main">Monocular depth estimation with affinity, vertical pooling, and label enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="224" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b177">
	<analytic>
		<title level="a" type="main">Multi-modal auto-encoders as joint estimators for robotics scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cadena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b178">
	<analytic>
		<title level="a" type="main">Learning affinity via spatial propagation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">De</forename><surname>Mello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1520" to="1530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b179">
	<monogr>
		<title level="m" type="main">Direct sparse odometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="611" to="625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b180">
	<analytic>
		<title level="a" type="main">Fast semi-direct monocular visual odometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Forster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pizzoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scaramuzza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Svo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international conference on robotics and automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="15" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b181">
	<analytic>
		<title level="a" type="main">Orb-slam2: An open-source slam system for monocular, stereo, and rgb-d cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mur-Artal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Tardós</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1255" to="1262" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b182">
	<monogr>
		<title level="m" type="main">Depth transfer: Depth extraction from video using non-parametric sampling, IEEE transactions on pattern analysis and machine intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Karsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Kang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="2144" to="2158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b183">
	<analytic>
		<title level="a" type="main">Discrete-continuous depth estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="716" to="723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b184">
	<analytic>
		<title level="a" type="main">Pulling things out of perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="89" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b185">
	<analytic>
		<title level="a" type="main">Van Den Hengel, M. He, Depth and surface normal estimation from monocular images using regression on deep features and hierarchical crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1119" to="1127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b186">
	<analytic>
		<title level="a" type="main">Monocular depth estimation using neural regression forest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5506" to="5514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b187">
	<analytic>
		<title level="a" type="main">Depth from a single image by harmonizing overcomplete local network predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2658" to="2666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b188">
	<analytic>
		<title level="a" type="main">A two-streamed network for estimating fine-scaled depth maps from single rgb images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3372" to="3380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b189">
	<analytic>
		<title level="a" type="main">Pad-net: Multitasks guided prediction-and-distillation network for simultaneous depth estimation and scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="675" to="684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b190">
	<analytic>
		<title level="a" type="main">Single-image depth estimation based on fourier domain analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="330" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b191">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.09895</idno>
		<title level="m">Index network</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b192">
	<analytic>
		<title level="a" type="main">Real-time joint semantic segmentation and depth estimation using asymmetric annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nekrasov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dharmasiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Spek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Drummond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7101" to="7107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b193">
	<analytic>
		<title level="a" type="main">Monocular depth estimation using relative depth maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b194">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-J</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.02708</idno>
		<title level="m">Unsupervised depth learning in challenging indoor video: Weak rectification to rescue</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b195">
	<analytic>
		<title level="a" type="main">Revisiting single image depth estimation: Toward higher resolution maps with accurate object boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ozay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Okatani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1043" to="1051" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b196">
	<monogr>
		<title level="m" type="main">Structure-aware residual pyramid network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.06023</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b197">
	<analytic>
		<title level="a" type="main">Joint taskrecursive learning for semantic segmentation and depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="235" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b198">
	<analytic>
		<title level="a" type="main">Sdc-depth: Semantic divide-and-conquer network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="541" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b199">
	<monogr>
		<title level="m" type="main">Discriminatively trained dense surface normal estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladickỳ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zeisl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ECCV</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b200">
	<analytic>
		<title level="a" type="main">Designing deep networks for surface normal estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="539" to="547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b201">
	<analytic>
		<title level="a" type="main">Image guided depth upsampling using anisotropic total generalized variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ferstl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Reinbacher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rüther</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="993" to="1000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b202">
	<analytic>
		<title level="a" type="main">Deep depth completion of a single rgb-d image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="175" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b203">
	<analytic>
		<title level="a" type="main">The fast bilateral solver</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="617" to="632" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
