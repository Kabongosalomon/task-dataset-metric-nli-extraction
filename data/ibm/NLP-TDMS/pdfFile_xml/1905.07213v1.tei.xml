<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adaptation of Deep Bidirectional Multilingual Transformers for Russian Language</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuri</forename><surname>Kuratov</surname></persName>
							<email>yurii.kuratov@phystech.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Neural Networks and Deep Learning Lab</orgName>
								<orgName type="institution">Moscow Institute of Physics and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Arkhipov</surname></persName>
							<email>arkhipov.mu@mipt.ru</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Neural Networks and Deep Learning Lab</orgName>
								<orgName type="institution">Moscow Institute of Physics and Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Adaptation of Deep Bidirectional Multilingual Transformers for Russian Language</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note type="submission">Submitted to Dialogue 2019 conference</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The paper introduces methods of adaptation of multilingual masked language models for a specific language. Pre-trained bidirectional language models show state-of-the-art performance on a wide range of tasks including reading comprehension, natural language inference, and sentiment analysis. At the moment there are two alternative approaches to train such models: monolingual and multilingual. While language specific models show superior performance, multilingual models allow to perform a transfer from one language to another and solve tasks for different languages simultaneously. This work shows that transfer learning from a multilingual model to monolingual model results in significant growth of performance on such tasks as reading comprehension, paraphrase detection, and sentiment analysis. Furthermore, multilingual initialization of monolingual model substantially reduces training time. Pre-trained models for the Russian language are open sourced.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A large amount of work is devoted to unsupervised pre-training of neural networks on a variety of Natural Language Processing (NLP) tasks. Unsupervised pre-training shows significant improvements in almost every NLP task <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10]</ref>.</p><p>At the moment one of the best performing models for unsupervised pretraining is BERT <ref type="bibr" target="#b3">[4]</ref>. This model is based on Transformer <ref type="bibr" target="#b15">[16]</ref> architecture and trained on a large number of unlabeled texts from Wikipedia to solve Masked Language Modelling task. It shows state-of-the-art results on a wide range of NLP tasks in English.</p><p>Currently, there are publicly available two monolingual English and Chinese models and a single multilingual model. It was previously reported that monolingual models performance is significantly better than multilingual ones 1 .</p><p>In the present work, we consider the possibility of multilingual to monolingual transfer. We use Russian as a target language for transfer. We show that it is possible to train the monolingual model using multilingual initialization.</p><p>To show this, we evaluated the multilingual model on a number of common NLP tasks from the target language. The model trained in a monolingual setting achieves substantially better performance compared to the multilingual model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model Architecture</head><p>In the present work, we use BERT <ref type="bibr" target="#b3">[4]</ref> model for all our experiments. The model is a Transformer <ref type="bibr" target="#b15">[16]</ref> encoder. The basic building blocks of the model is Self-Attention. The model was trained on the Masked Language Modelling and next sentence prediction tasks. We refer readers to check BERT original paper for details about the model <ref type="bibr" target="#b3">[4]</ref>.</p><p>We used 12 layers (Transformer blocks) version of BERT with self-attention hidden size 768, feed-forward hidden size 3072, and 12 self-attention heads. This setting corresponds to BERT BASE model from <ref type="bibr" target="#b3">[4]</ref>. Task-specific layers were trained according to the BERT paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Language transfer</head><p>In this work, we consider the transfer of multilingual BERT model to monolingual. Authors of <ref type="bibr" target="#b3">[4]</ref> showed that monolingual models show superior performance compared to multilingual one. Furthermore, the BERT model uses the subword segmentation algorithm <ref type="bibr" target="#b13">[14]</ref> to cope with large vocabulary problem. Multilingual models use only a small part of the entire vocabulary for a single language. It results in much longer sequences after tokenization compared to the monolingual model. Since the Transformer model has quadratic computational complexity in terms of input sequence length, it is highly undesirable.</p><p>We investigated the possibility of using the multilingual model as initialization for the monolingual model. The basic idea is to use knowledge about target language that already captured during multilingual training. It also is known that training model using data from multiple languages can significantly improve the performance of the model <ref type="bibr" target="#b8">[9]</ref>. We used the multilingual model from BERT repository 2 . This model was trained on one hundred languages with largest Wikipedias. The target language is Russian. All parameters of the model except word embeddings were initialized from the multilingual model <ref type="bibr" target="#b8">[9]</ref>.</p><p>The new subword vocabulary was obtained using subword-nmt 3 . Training of the subword vocabulary was performed on the Russian part of Wikipedia and news data. The part of Wikipedia data was around 80%. The result of this step is a new monolingual Russian subword vocabulary. This vocabulary contains longer Russian words and subwords compared to multilingual one.</p><p>New word embeddings matrix was obtained by assembling monolingual embeddings from multilingual. Namely, embeddings of all tokens from the intersection of multilingual and monolingual vocabulary were left without any changes. The same for special tokens like <ref type="bibr">[UNK]</ref> or <ref type="bibr">[CLS]</ref>. We replaced all tokens from outside the intersection with tokens from the monolingual vocabulary. These tokens are mostly longer subword units which are combinations of shorter units present in the intersection. New tokens are initialized with the mean value of embeddings from the intersection. For example, there are tokens 'bi' and '##rd' in the intersection of vocabularies, where '##' stands for the continuation of the word. There is also a token 'bird' present in the monolingual vocabulary and absent in the multilingual vocabulary. The embedding of 'bird' is initialized as the mean value of the embeddings 'bi' and '##rd'.</p><p>The model with reassembled vocabulary and embeddings matrix was trained on the same data that was used for building of the monolingual vocabulary. The following hyperparameters were used for training:</p><p>• batch size: 256</p><p>• learning rate: 2 · 10 −5</p><p>• optimizer: Adam</p><formula xml:id="formula_0">• L2 regularization: 10 −2</formula><p>The monolingual Russian model 4 is available as a part of the DeepPavlov library 5 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Tasks description</head><p>We have chosen three tasks to evaluate our approach: paraphrase identification, sentiment analysis, and question answering. We briefly describe them in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Paraphrase Identification with ParaPhraser</head><p>ParaPhraser <ref type="bibr" target="#b10">[11]</ref> is a dataset for paraphrase detection in Russian language. Two sentences are paraphrases if they have the same meaning. This dataset consists of 7227/1924 train/test pairs of sentences which are labeled as precise paraphrases, near paraphrases or non-paraphrases. One approach for paraphrase identification is a binary classification: first class is precise and near paraphrases, second class -non-paraphrases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Sentiment Analysis with RuSentiment</head><p>RuSentiment <ref type="bibr" target="#b12">[13]</ref> is a dataset for sentiment analysis of posts from VKontakte (VK), the most popular social network in Russia. Realised in 2018, it became one of the largest sentiment datasets for Russian language with 30521 posts. Each post is labeled with one of the five classes. The informal language presented in RuSentiment dataset makes it more challenging for our model, trained on Wikipedia and news articles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Question answering with SDSJ Task B</head><p>As part of Sberbank Data Science Journey 6 2017 was held a competition with two tasks. Task B was inspired by Stanford Question Answering Dataset (SQuAD) <ref type="bibr" target="#b11">[12]</ref>. Organizers collected about 50,000 (train and development set) questions and contexts, where the answer is always a span from corresponding context. SQuAD dataset encouraged community to develop sophisticated and effective neural architectures such as BiDAF <ref type="bibr" target="#b14">[15]</ref>, R-NET <ref type="bibr" target="#b16">[17]</ref>, Mnemonic Reader <ref type="bibr" target="#b4">[5]</ref>. All this models are based on attention mechanisms <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7]</ref> and building joint context-question representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>We evaluated BERT multilingual model and BERT trained with our approach (RuBERT) on three tasks: paraphrase identification, sentiment analysis, and question answering. All reported results were obtained by averaging across 5 runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>model</head><p>F-1 Accuracy Neural networks <ref type="bibr" target="#b10">[11]</ref> 79.82 76.65 Classifier + linguistic features <ref type="bibr" target="#b10">[11]</ref> 81.10 77.39 Machine Translation + Semantic similarity <ref type="bibr" target="#b5">[6]</ref>  SDSJ Task B and ParaPhraser datasets share the same domain with data, which we used for training RuBERT. The RuSentiment dataset is based on posts from a social network and shows to be more challenging for RuBERT. As result, we can see only 1 F-1 point improvement from previous state of the art for RuSentiment in <ref type="table" target="#tab_2">Table 2</ref>, comparing to 4-6 F-1 points improvement on SDSJ Task B and ParaPhraser (results in <ref type="table" target="#tab_0">Table 1</ref> and <ref type="table" target="#tab_3">Table 3</ref>).   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Vocabulary comparison</head><p>BERT multilingual and RuBERT have the same size of vocabulary (about 120k subtokens), but RuBERT vocabulary was built especially for Russian language. <ref type="figure" target="#fig_1">Figure 1</ref> shows that RuBERT model allows to reduce mean sequence length in 1.6 times in subtokens, what makes possible to increase batch size or feed longer texts to the model, comparing to BERT multilingual.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Training dynamics</head><p>In this section we compare training BERT model for Russian language from scratch (random initialization) and initialized with BERT multilingual. <ref type="figure" target="#fig_2">Figure  2</ref> shows that BERT multilingual initialization helps to converge faster: about 800 thousand steps is required for random initialized model to get the same loss as at 250 thousand step of multilingual initialization. It takes about two days to train for 250 thousand steps (on Tesla P100 x 8), so it helped us to save six days of computational time. Proposed averaging of new subtokens in vocabulary also has positive effect on the rate of convergence (instead of averaging we could take random initialization for new subtokens). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we have shown that Transformer network pre-trained on the multilingual Masked Language Modelling task significantly improves performance on a number of Russian NLP tasks compared to existing solutions. Furthermore, language-specific unsupervised training with multilingual initialization results in even better improvements. Pre-trained models for the Russian language are open sourced, as well as code to reproduce our results as part of DeepPavlov library.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgments</head><p>This work was supported by National Technology Initiative and PAO Sberbank project ID 0000000007417F630002.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Distribution of lengths in subtokens of contexts with their questions (SDSJ Task B dataset). Red vertical lines represent mean values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Models training dynamics to get to the same value of loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>ParaPhraser. We compare BERT based models with models in nonstandard run setting, when all resources were allowed.</figDesc><table><row><cell>78.51</cell><cell>81.41</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell cols="3">: RuSentiment. We used only randomly selected posts (21,268) subset</cell></row><row><cell>for training.</cell><cell></cell><cell></cell></row><row><cell>model</cell><cell>F-1 (dev)</cell><cell>EM (dev)</cell></row><row><cell cols="2">R-Net from DeepPavlov [2] 80.04</cell><cell>60.62</cell></row><row><cell>BERT multilingual</cell><cell>83.39 ± 0.08</cell><cell>64.35 ± 0.39</cell></row><row><cell>RuBERT</cell><cell cols="2">84.60 ± 0.11 66.30 ± 0.24</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Results on question answering with SDSJ Task B. Models performance was evaluated on development set (public leaderboard subset).</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/google-research/bert/blob/master/multilingual.md#results arXiv:1905.07213v1 [cs.CL] 17 May 2019</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/google-research/bert 3 https://github.com/rsennrich/subword-nmt</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">http://files.deeppavlov.ai/deeppavlov_data/bert/rubert_cased_L-12_H-768_A-12_ v1.tar.gz 5 https://github.com/deepmipt/deeppavlov/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://sdsj.sberbank.ai/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deeppavlov: Open-source library for dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burtsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Seliverstov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Airapetyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arkhipov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Baymurzina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bushkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Gureenkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Khakhulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kuratov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kuznetsov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2018, System Demonstrations</title>
		<meeting>ACL 2018, System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="122" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semi-supervised sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3079" to="3087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pretraining of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Reinforced mnemonic reader for machine reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02798</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Paraphrase detection using machine translation and textual similarity algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kravchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Artificial Intelligence and Natural Language</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="277" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.04025</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mulcaire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.11598</idno>
		<title level="m">Polyglot semantic role labeling</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Paraphraser: Russian paraphrase corpus and shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pivovarova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Pronoza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yagunova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pronoza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Artificial Intelligence and Natural Language</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="211" to="225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05250</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Rusentiment: An enriched sentiment analysis dataset for social media in russian</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rumshisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Volkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gronas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gribov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="755" to="763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.07909</idno>
		<title level="m">Neural machine translation of rare words with subword units</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Bidirectional attention flow for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01603</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Gated selfmatching networks for reading comprehension and question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="189" to="198" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
