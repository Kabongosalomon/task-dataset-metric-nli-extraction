<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Comprehensive Attention Self-Distillation for Weakly-Supervised Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyi</forename><surname>Huang</surname></persName>
							<email>zeyih@andrew</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijayakumar</forename><surname>Bhagavatula</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Huang</surname></persName>
							<email>donghuang@cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Comprehensive Attention Self-Distillation for Weakly-Supervised Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Weakly Supervised Object Detection (WSOD) has emerged as an effective tool to train object detectors using only the image-level category labels. However, without object-level labels, WSOD detectors are prone to detect bounding boxes on salient objects, clustered objects and discriminative object parts. Moreover, the imagelevel category labels do not enforce consistent object detection across different transformations of the same images. To address the above issues, we propose a Comprehensive Attention Self-Distillation (CASD) training approach 2 for WSOD. To balance feature learning among all object instances, CASD computes the comprehensive attention aggregated from multiple transformations and feature layers of the same images. To enforce consistent spatial supervision on objects, CASD conducts self-distillation on the WSOD networks, such that the comprehensive attention is approximated simultaneously by multiple transformations and feature layers of the same images. CASD produces new state-of-the-art WSOD results on standard benchmarks such as PASCAL VOC 2007/2012 and MS-COCO.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Visual object detection has achieved remarkable progress in the last decade thanks to the advances of Convolutional Neural Networks (CNNs) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. An integral part of the achievement is the availability of large-scale training data with precise bounding-box annotations (PASCAL VOC <ref type="bibr" target="#b2">[3]</ref>, MS-COCO <ref type="bibr" target="#b3">[4]</ref>, etc). However, obtaining such fine-grained annotations at a large scale is labor-intensive and time-consuming, which drove many researchers to explore the weakly-supervised setting. Weakly-Supervised Object Detection (WSOD) <ref type="bibr" target="#b4">[5]</ref> aims to learn object detectors with only the image-level category labels indicating whether an image contains an object or not.</p><p>Most previous methods for WSOD are based on the Multiple Instance Learning (MIL) <ref type="bibr" target="#b5">[6]</ref>. These methods regard images as bags and object proposals as instances. A positive bag contains at least one positive instance while all instances being negative in a negative bag. WSOD instance classifiers (object detectors) are trained over these bags. Recently, leveraging the powerful representation learning capacity of CNNs, several researchers proposed end-to-end MIL networks (OICR <ref type="bibr" target="#b6">[7]</ref>, PCL <ref type="bibr" target="#b6">[7]</ref>, MIST <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>) with promising WSOD performances. These CNN methods regard the instance classification (object detection) problem as a latent model learning within a bag classification (image classification) problem, where the final image scores are the aggregation of the instance scores. However, due to the under-determined and ill-posed nature of WSOD, there is still a large performance gap between the weakly-supervised detectors and fully-supervised detectors.</p><p>The existing methods have two main sets of issues as demonstrated in <ref type="figure" target="#fig_0">Fig. 1</ref>. First, in the "Biased WSOD" column of <ref type="figure" target="#fig_0">Fig. 1 (a)</ref> , there are three typical problems. Missing instance: Salient objects are easily detected while inconspicuous instances tend to be ignored. Clustered instances: multiple adjacent instances of the same category may be detected in a single bounding box. Part domination:</p><p>The bounding boxes are prone to focus on the most discriminative object parts instead of the entire objects. Second, in the "Inconsistent WSOD" column of <ref type="figure" target="#fig_0">Fig. 1 (b)</ref>, the same image and its different image transformations, i.e., "Original Image", "Flipped Image" and "Scaled Image", do not produce the same object bounding boxes.</p><p>WSOD conducts classification on object proposals (e.g., bounding boxes generated by selective search <ref type="bibr" target="#b10">[11]</ref>) with image-level class labels. The object proposals receive high classification scores are considered as objects detected by WSOD. As we dive deep into the above issues from a feature learning perspective, we overlay the attention maps of object proposals that get high confidences in WSOD ( <ref type="figure" target="#fig_0">Fig. 1</ref>). High intensity in attention maps corresponds to highly discrimiative and biased features learned by the WSOD networks. We observe the drawbacks of WSOD detection are closely associated with the issues in feature learning. For "Biased WSOD", it is clear that salient objects, clustered objects, and certain object parts contain spatial features that dominate the WSOD classification. From a statistical machine learning point-of-view, feature domination is typically established by the biased feature distribution in training data. For "Inconsistent WSOD", the different transformations of the same image are typically generated by data augmentation and are used to train the WSOD networks in different training iterations. The same class image-level labels of transformed images do not enforce spatially consistent feature learning and may lead to part domination and missing instances. Note that the inconsistency on feature localization was not an issue for fullsupervised setting where augmented training data with precise bounding box labels can naturally encourage consistency.</p><p>The above observations inspire us to address WSOD issues using an attention-based feature learning method. We propose a Comprehensive Attention Self-Distillation (CASD) approach for WSOD training. To balance feature learning among objects, CASD computes the comprehensive attention aggregated from multiple transformations and feature layers of the same images. The "CASD (ours)" column of <ref type="figure" target="#fig_0">Fig. 1</ref> (a) demonstrates that CASD generates balanced attention on less salient objects, individual objects, and entire objects, which enables WSOD detection on these objects. To enforce consistent spatial supervision on objects, CASD conducts self-distillation on the WSOD network itself, such that the comprehensive attention is approximated simultaneously by multiple transformations and layers of the same images. The "CASD (ours)" column of <ref type="figure" target="#fig_0">Fig. 1</ref> (b) demonstrates that CASD generates consistent attention on different transformed variants of the same image, leading to consistent WSOD detection in different transformations.</p><p>By computing the comprehensive attention maps, CASD aggregates "free" resources of spatial supervision for WSOD, including image transformations and low-to-high feature layers. By conducting self-distillation on the WSOD network with the comprehensive attention maps, CASD enforces instance-balanced and spatially-consistent supervision, therefore robust bounding box localization for WSOD. CASD achieves the state-of-the-art on several standard benchmarks, e.g. PASCAL VOC 2007/2012 and MS-COCO, outperforming other methods by clear margins. Systematic ablation studies are also conducted on the effects of transformations and feature layers on CASD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Weakly Supervised Object Detection</head><p>Recent WSOD performance are significantly boosted by incorporating Multiple Instance Learning (MIL) in Convolutional Neural Networks (CNN). <ref type="bibr" target="#b4">[5]</ref> introduces the first end-to-end Weakly Supervised Deep Detection Network (WSDDN) with MIL, which inspired many following works. <ref type="bibr" target="#b11">[12]</ref> combines WSDDN and multi-stage instance classifiers into an Online Instance Classifier Refinement (OICR) framework. <ref type="bibr" target="#b6">[7]</ref> further improves OICR with a robust proposal generation module based on proposal clustering, namely Proposal Cluster Learning (PCL). <ref type="bibr" target="#b12">[13]</ref> introduces Continuation Multiple Instance Learning (C-MIL) by relaxing the original MIL loss function with a set of smoothed loss functions preventing detectors to be part dominating. <ref type="bibr" target="#b7">[8]</ref> proposes a multiple instance self-training framework with an online regression branch. <ref type="bibr" target="#b13">[14]</ref> and <ref type="bibr" target="#b14">[15]</ref> leverage segmentation maps to generate instance proposals with rich contextual information. <ref type="bibr" target="#b15">[16]</ref> and <ref type="bibr" target="#b16">[17]</ref> introduce detection-segmentation cyclic collaborative frameworks. Different from the above methods that regularize the WSOD outputs, CASD directly enforces comprehensive and consistent WSOD feature learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Attention Mechanism in Computer Vision DNNs</head><p>The attention mechanism provides a fine-grained view of the features learned in CNNs. <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref> introduce the connection between class-wise attention maps and image-level class labels. Due to its explicit spatial clues, attention maps have been used to improve computer vision tasks in two ways: (1) Re-weighting features. <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref> re-weight features with spatial-wise attention maps. <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28]</ref> improve supervised tasks by inverting gradient-based spatial-wise and channel-wise attention. (2) Loss regularization. <ref type="bibr" target="#b28">[29]</ref> introduces a consistency loss between attention maps under different input transformations for image classification. <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref> propose cross-layer consistency losses over attention maps for image classification and lane detection. To the best of our knowledge, CASD is the first attempt to explore attention regularization for WSOD. Moreover, existing methods for other tasks only encourage consistency of features, rather than the completeness of features. Specifically, the features tend to focus on object parts but fail to localize less salient objects in WSOD. CASD encourages both consistent and spatially complete feature learning guided by the comprehensive attention maps, which explicitly addresses WSOD issues above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Knowledge Distillation</head><p>Knowledge distillation <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref> is a CNN training process to transfer knowledge from teacher networks to student networks. It has found wide applications in model compression, incremental learning, and continual learning. The student networks mimic the the teacher networks on predictive probabilities <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b33">34]</ref>, intermediate features <ref type="bibr" target="#b34">[35]</ref>, or attention maps of intermediate neural activations <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref>. In contrast, CASD is a knowledge self-distillation process that transfers the comprehensive attention knowledge within the WSOD model itself, rather than another teacher model, across multiple views of the same data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Background</head><p>We first review a basic WSOD framework called Online Instance Classifier Refinement (OICR) <ref type="bibr" target="#b11">[12]</ref>, then introduce CASD as a general training module on top of OICR. Formally, we denote I ∈ R h×w×3 as an RGB image and y = [y 1 , ..., y C ] ∈ [0, 1] C as the labels associated with I. C is the total number of object categories. y is a binary vector where y c = 1 indicates the presence of at least one object of the c th category and y c = 0 indicates absence. The OICR framework consists of two main components (See <ref type="figure">Fig. 2</ref>):</p><formula xml:id="formula_0">Figure 2:</formula><p>The OICR WSOD network <ref type="bibr" target="#b11">[12]</ref> with our Comprehensive Attention Self-Distillation (CASD) training module. OICR includes two parts: Proposal Feature Extractor extracts feature vectors for proposals. Multiple Instance Learning Head aggregates the proposal scores for imagelevel classification. K branches of instance classifiers are sequentially refined for better localization. CASD computes proposal attention maps from each proposal feature maps, aggregates attentions into comprehensive attention maps, and self-distills the comprehensive attention maps to improve WSOD (described in Sec. 3.2).</p><p>Proposal Feature Extractor. From an input image I, the object proposals R = {R 1 , R 2 , ..., R N } are generated by Selective Search <ref type="bibr" target="#b10">[11]</ref> where N is total number of proposals (bounding boxes). Then, a CNN backbone is used to extract the image feature maps for I, from which the proposal feature maps are extracted respectively. Lastly, these proposal feature maps are fed to a Region-of-Interest (RoI) Pooling layer <ref type="bibr" target="#b0">[1]</ref> and two Fully-Connected (FC) layers to obtain proposal feature vectors.</p><p>Multiple Instance Learning (MIL) Head. The MIL head learns the latent instance classifiers (detectors). This module takes the above-obtained proposal feature vectors as input and conducts the image-level MIL classification, regarding detection as a latent model learning problem.</p><p>Following WSDDN <ref type="bibr" target="#b4">[5]</ref>, the proposal feature vectors are forked into two parallel classification and detection branches to generate two matrices x cls , x det ∈ R C×N . Each column of x cls and x det corresponds to a score vector of an object proposal (e.g., R i ). Then the two matrices are normalized by the softmax layers σ(·) along the category direction (column-wise) and proposal direction (row-wise) respectively, leading to σ(x cls ) and σ(x det ). Finally, the instance-level classification score for object proposals are computed by the element-wise product x = σ(x cls ) σ(x det ). The image-level classification score for the c th class is computed as</p><formula xml:id="formula_1">p c = N i=1 x i,c . The MIL multiple-label classification loss is used to supervise MIL classification: L mlc = − C c=1 {y c log p c + (1 − y c ) log(1 − p c )}.</formula><p>The proposal scores x can be used to select proposals as detected objects. However, this step is prone to selecting proposals corresponding to the most discriminative object instances and object parts.</p><p>To address this general WSOD issue, <ref type="bibr" target="#b11">[12]</ref> introduces multi-stage OICR branches to refine the MIL head. The proposal feature vectors from Proposal Feature Extractor are fed to K refinement stages of OICR instance classifiers. Each k th stage is supervised by the instance-level pseudo-labels selected from the N k top-scoring proposals in previous stage. Each instance classifier consists of an FC layer and a softmax function, and outputs a proposal score matrix x k ∈ R (C+1)×N . (The background class is the (C + 1) th category in x k .). The loss for the k th classifier is defined as</p><formula xml:id="formula_2">L k ref = − 1 N k N k r=1 C+1 c=1ŷ</formula><p>k r,c log x k r,c . (r corresponds to the region R r .) During the inference, the proposal score matrices of all K classifiers are summed to predict bounding boxes with Non-Maximum Suppression (NMS). Besides the refinement loss, <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b7">8]</ref> also incorporate a regression loss to further improve the bounding box localization capability. Following Fast-RCNN, we define</p><formula xml:id="formula_3">L k reg = 1 G k G k r=1 smooth L1 (t r ,t r )</formula><p>where G k is the total number of positive proposals in k-th branch. t r andt r are tuples including location offsets and sizes of r-th predicted and ground truth bounding-box. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Comprehensive Attention Distillation</head><p>Building upon <ref type="bibr" target="#b11">[12]</ref>, Comprehensive Attention Self-Distillation (CASD) encourages consistent and balanced representation learning in two folds: Input-wise CASD and Layer-wise CASD.</p><p>Input-wise (IW) CASD. IW-CASD conducts CASD over input images under multiple transformations ( <ref type="figure" target="#fig_1">Fig. 3 (a)</ref>). Consider a set of inputs including the original image I, its horizontally flipped image I f lip = T f lip (I) and its scaled image I scale = T scale (I). These input images are fed into the same WSOD feature extractor to get the image feature maps outputted by the last convolution layer: F, F f lip and F scale , respectively. At each refinement branch, N K positive and negative object proposals are selected by the same way as <ref type="bibr" target="#b11">[12]</ref>. For the r th object proposal R r (r = 1, · · · , N K ), the proposal feature P r ∈ R H×W ×L is cropped from F, and is used to compute the proposal attention map A r ∈ R H×W by channel-wise average pooling and element-wise Sigmoid activation.</p><formula xml:id="formula_4">A r (m, n) = S( 1 L L i=1 P i r (m, n)),<label>(1)</label></formula><p>where A r (m, n) denotes the proposal attention magnitude at spatial location (m, n) of proposal R r . S(·) is the element-wise Sigmoid operator. The proposal attention maps A f lip r and A scale r from F f lip and F scale can be computed in the same way as Eq. 1.</p><p>The example in <ref type="figure" target="#fig_1">Fig. 3 (a)</ref> shows that attention maps A r , A f lip r and A scale r focus on different parts of proposal R r . This is the common case in WSOD: for input image under different transformations, the feature distribution within class does not correlate with the feature distribution between classes. Unless regularized, the image-level classification of WSOD always relies on the most discriminative features, which only correspond to the proposals on salient objects or object parts.</p><p>The union of these attention maps (denoted by A IW r ) always covers more comprehensive parts of the object than individual attention maps,</p><formula xml:id="formula_5">A IW r = max(A r , T f lip (A f lip r ), A scale r ),<label>(2)</label></formula><p>where max(·) is the element-wise max operator. We define the input-wise CASD based on A IW r as a refinement problem on the WSOD network: updating the WSOD feature extractor such that any transformed variants of the same image should generate comprehensive attention close to A IW r . To this end, A IW r are simultaneously approximated from individual attention maps. For each k th refinement branch , the IW-CASD loss function is defined as</p><formula xml:id="formula_6">L k IW = 1 N K N K r=1 A IW r − A r 2 + A IW r − T f lip (A f lip r ) 2 + A IW r − A scale r ) 2 .<label>(3)</label></formula><p>N K is the total number of selected proposals in the k th branch. The computation of A IW r and updating of the WSOD feature extractor are alternative steps in training. We consider this is a knowledge distillation process from A IW r to the WSOD network itself, and hereby name it as self-distillation. However, the alternating process above may also lead to local optimum. In particular, the individual attention map containing more high-intensity elements may dominate attention maps associated with other transformations. We balance the distillation among different transformations by independently applying Inverted Attention (IA) <ref type="bibr" target="#b25">[26]</ref> on each individual attention. For each attention, IA randomly masks out top features highlighted by attention maps and force more features to be activated. This training technique regularizes CASD and produces better results (see ablation study in Section 4).</p><p>Additionally, the aggregated proposal score matrices are used in the WSOD network training for each transformed variant of the same image. Specifically, in the MIL head, IW-CASD takes several transformed variants of the same image to generate groups of proposal instance score matrices for each branch (K + 1 in total). Within each k th group, each score matrix provides a transformed view of the original image for detection. Thus it is natural for WSOD to aggregate the score matrices within each k th group to form a robust proposal score matrix asx k = 1</p><formula xml:id="formula_7">3 (x k + x k f lip + x k scale )</formula><p>. Layer-wise(LW) CASD (LW-CASD) operates on proposal feature maps of image I produced at multiple layers of WSOD feature extractor (see <ref type="figure" target="#fig_1">Fig. 3 (b)</ref>). The feature extractor network consists of Q convolutional blocks B 1 , ..., B Q , each of which outputs a feature map F B1 , ..., F B Q , respectively. F B Q is used as the feature map for generating proposal feature vectors in the MIL head. As illustrated by the example in <ref type="figure" target="#fig_1">Fig. 3 (b)</ref>, at the early layers of a CNN, the network focuses on local low-level features, while at deeper layers, the network tends to focus on global semantic features. Conducting CASD over layers enriches the proposal features with more granularity.</p><p>In LW-CASD, we use RoI pooling at different feature blocks to generate proposal feature maps, such that they share the same spatial size. From these proposal attention maps, the layer-wise attention maps A Bq s are generated in a similar way as Eq. 1, and aggregated to obtain the comprehensive attention maps,</p><formula xml:id="formula_8">A LW r = max(A B1 r , · · · , A B Q ),<label>(4)</label></formula><p>where max(·) is the element-wise max operator. For the k th refinement branch, the IW-CASD loss is</p><formula xml:id="formula_9">L k LW = 1 N K Q q=1 N K r=1 A LW r − A Bq r 2 .<label>(5)</label></formula><p>The choice of layers in IW-CASD are studied in Section 4.</p><p>Remarks. 1) Channel-wise average pooling+sigmoid in Eq. 1 is not the only choice for attention estimation. We also explored other mechanisms such as Grad-CAM <ref type="bibr" target="#b18">[19]</ref>, but there is only a negligible performance difference. Eq. 1 is used due to its simplicity and computation efficiency. </p><formula xml:id="formula_10">L CASD−W SOD = L mlc + K k=1 (αL k ref + βL k reg + γL k IW + γL k LW ),<label>(6)</label></formula><p>where α, β and γ balance the weights of different losses. K is the number of refinement branches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head><p>Datasets and Metrics. Three standard WSOD benchmarks, PASCAL VOC 2007, VOC 2012 <ref type="bibr" target="#b40">[41]</ref> and MS-COCO <ref type="bibr" target="#b41">[42]</ref>  <ref type="bibr" target="#b42">[43]</ref> are used as WSOD backbones. Batch size is set to be T that is the number of input transformations. The maximum iteration numbers are set to be 80K, 160K and 200K for VOC 2007, VOC 2012, and MS-COCO respectively. The whole WSOD network is optimized in an end-to-end way by stochastic gradient descent (SGD) with a momentum of 0.9, an initial learning rate of 0.001 and a weight decay of 0.0005. The learning rate will decay with a factor of 10 at the 40k th , 80k th , and 120k th iteration for VOC 2007, VOC 2012 and MS-COCO, respectively. The total number of refinement branches K is set to be 2. The confidence threshold for Non-Maximum Suppression (NMS) is 0.3. For all experiments, we set α = 0.1, β = 0.05 and γ = 0.1 in the total loss. Note that we reimplemented the OICR loss in Pytorch and found that using a fixed α = 0.1 is slightlty better than the adaptive weighting policy in vanilla OICR (48.9% mAP 0.5 vs. 48.3% mAP 0.5 on VOC 2007). Thus we keep the fixed weight α for the OICR loss. Selective Search <ref type="bibr" target="#b10">[11]</ref> is used to generate about 2, 000 object proposals for each image.</p><p>To have a fair comparison with other methods, following <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b6">7]</ref>, multi-level scaling and horizontal flipping data augmentation are conducted in training. Multi-level scaling is also used in testing. Specifically, in the data augmentation, the short edges of input images are randomly re-scaled to a scale in {480, 576, 688, 864, 1200}, and the longest image edges are capped to 2, 000, then a random horizontal flipping is randomly conducted on the scaled images. In evaluation, input images are augmented with all five scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Ablation Studies</head><p>We conducted three sets of ablation studies on the VOC 2007 with metric mAP 0.5 (%) in <ref type="table" target="#tab_4">Table 1-3.</ref> All results are based on the VGG16 backbone.</p><p>CASD main configurations. We conducted ablation studies on the main components of CASD in <ref type="table">Table 1</ref> under the mAP 0.5 metric. Our baseline achieves 48.9% mAP 0.5 . With the proposed Input-wise CASD ("+IW"), the baseline is boosted to 54.1% mAP 0.5 while the proposed Layerwise CASD (+LW) improves the baseline to 52.3% mAP 0.5 . Both IW-CASD and LW-CASD can consistently improve the baseline with a clear 5.2% and 3.4% gain respectively. Combining IW with LW ("+IW+LW") can further boost the performance to 55.3% mAP 0.5 that is 6.4% better than the baseline. In addition, IW-CASD without Inverted Attention (IA) and proposal score aggregation (PSA), "+IW w/o IA+PSA", could achieve 51.4% mAP 0.5 which is 2.5% superior to the baseline. Proposal score aggregation brings a 1.2% mAP 0.5 performance gain, leading to 52.6% mAP 0.5 in IW-CASD without Inverted Attention ("+IW w/o IA"). And Inverted Attention (+IW) has a 1.5% mAP 0.5 performance boost, justifying the effectiveness of IA.</p><p>Then we further demonstrate the validity of regression branch and stronger augmentation. With the regression branch, "+IW+LW+Reg" achieves 56.1% mAP 0.5 that is 0.8% better than "+IW+LW". Moreover, with stronger data augmentation of "flip+scale+color", we can achieve the best CASD performance ("+IW+LW+Reg * ") 56.8% mAP 0.5 . Here the "Color" augmentation applies some photo-metric distortions to the images which is the same as those described in <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45]</ref>.</p><p>CASD layer configurations. Built upon the baseline, additional ablation study on layer configurations of LW-CASD are shown in <ref type="table" target="#tab_3">Table 2</ref>. The B i is the i th convolutional block of VGG16 and B 4 denotes the last block before the FC layer for image classification. As shown in the table, the best result 52.3% mAP 0.5 is obtained with LW-CASD using B 4 +B 3 +B 2 blocks. As demonstrated in <ref type="figure" target="#fig_1">Fig.  3 (b)</ref>, these results indicate that middle-level feature attention maps (e.g. B 2 and B 3 ) encode balanced discriminative clues (more spatially distributed than B 1 ) for WSOD, while the low-level feature attention maps (e.g. B 1 ) contain noisy spatial information which may deteriorate the performance.</p><p>Besides <ref type="table" target="#tab_3">Table 2</ref>, all other experiments in this work use the B 4 + B 3 + B 2 configuration in CASD.</p><p>Attention regularization strategies. Besides our attention self-distillation, there are other regularization strategies emerged for semi-supervised object detection and multi-label classification such as Prediction Consistency <ref type="bibr" target="#b45">[46]</ref> and Attention Consistency <ref type="bibr" target="#b28">[29]</ref>. For a thorough demonstration of the advantage of our attention distillation strategy, we implement these strategies under the WSOD setting and compare them with the CASD. Similar to <ref type="bibr" target="#b45">[46]</ref>, prediction consistency in WSOD is implemented by minimizing the JS-Divergence between predictions of differently transformed data. Attention consistency <ref type="bibr" target="#b28">[29]</ref> in WSOD force the attention of the last proposal feature maps to be consistent under different input transformations based on the MSE loss.  <ref type="table">Table 1</ref>: Ablation study of CASD main configurations. "IW" as Input-Wise CASD, "LW" as Layer-Wise CASD, "IA" as Inverted Attention, "PSA": proposal score aggregation, "Reg" as regression branch.    <ref type="bibr" target="#b4">[5]</ref> 34.8 -OICR <ref type="bibr" target="#b11">[12]</ref> 41.2 37.9 PCL <ref type="bibr" target="#b6">[7]</ref> 43.5 40.6 C-MIL <ref type="bibr" target="#b12">[13]</ref> 50.5 46.7 WSOD2(+Reg.) <ref type="bibr" target="#b39">[40]</ref> 53.6 47.2 Pred Net <ref type="bibr" target="#b46">[47]</ref> 52.9 48.4 C-MIDN <ref type="bibr" target="#b14">[15]</ref> 52.6 50.2 MIST(+Reg.) <ref type="bibr" target="#b7">[8]</ref> 54.9 52.1 CASD(ours) 56.8 53.6  We compared IW-CASD ("+IW w/o IA") with the other two methods in <ref type="table" target="#tab_4">Table 3</ref>. IW-CASD is superior to both prediction and attention consistency strategies by a 2.4% mAP 0.5 and 1.6% mAP 0.5 performance gain respectively. This validates the superiority of CASD over other attention consistency methods.</p><p>Sensitivity analysis on loss weight γ: For the loss weight γ in Eq. 6, we evaluated CASD on VOC 2007 with γ = 0.05, 0.075, 0.1, 0.15, 0.2, and got mAP 54.1%, 54.7%, 55.3%, 55.0%, 55.0% respectively, which demonstrates that CASD is robust to γ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison with the State-of-the-arts</head><p>Here we experimentally compare the full version of CASD with other state-of-the-art methods. In <ref type="table" target="#tab_5">Table 4</ref>, with the same VGG16 backbone, CASD reaches the new state-of-the-art mAP 0.5 of 56.8% and 53.6% on VOC 2007 and VOC 2012, which are 1.9% and 1.5% higher than the latest state-ofthe-art (MIST(+Reg.) <ref type="bibr" target="#b7">[8]</ref>). In the results on MS-COCO (  <ref type="figure">Fig. 4</ref> and 5 in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we proposed the Comprehensive Attention Self-Distillation (CASD) algorithm to regularize the WSOD training. CASD aggregates "free" resources of spatial supervision within the WSOD network, such as the different attentions produced under multiple image transformations and low-to-high feature layers. Through self-distillation on the WSOD network, CASD enforces instance-balanced and spatially-consistent supervision over objects and achieves the new state-ofthe-art WSOD results. As a training module, we believe CASD can be generalized and benefit other weakly-supervised and semi-supervised tasks, such as instance segmentation, pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>This paper pushes the frontier of the weakly supervised object detection and reduce its performance gap with the supervised detection. This work is also a general regularization approach that may benefit semi-supervised learning, weakly-supervised learning, and self-supervised representation learning. The ultimate research vision is to potentially relieve the burden of human annotations on training data. This effort may reduce the cost, balance human bias, accelerate the evolution of machine perception technology, and help us to understand how to enable learning with minimal supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A More Experimental Results</head><p>A.1 Visualization <ref type="figure">Figure 4</ref>: Visualization of CASD-WSOD results. Top: success cases; Bottom: failure cases. Only the attention maps of high confidence proposals by WSOD detection are overlaid on input images. Green bounding boxes denote the ground truth.</p><p>We first present qualitative results of CASD-WSOD in <ref type="figure">Fig. 4</ref>. Recall that WSOD conducts classification on object proposals (e.g., bounding boxes generated by Selective Search <ref type="bibr" target="#b10">[11]</ref>) with image-level class labels. The object proposals receive high classification scores are considered as objects detected by WSOD. In <ref type="figure">Fig. 4</ref>, only the attention maps of high confidence proposals by WSOD detection are overlaid on input images. <ref type="figure">Fig. 4</ref> shows both the success and the failure cases of CASD. When the objects are relatively big and are completely visible, CASD tends to succeed. The failure cases mainly occur on objects that are small or under heavy occlusion. We deduce that there are two main factors contributing to the failure:</p><p>(1) The Selective Search (SS) <ref type="bibr" target="#b10">[11]</ref> algorithm may not generate good proposals for heavily occluded objects. This could be improved by using better objectness proposal generators such as the RPN of Faster-RCNN. (2) The incomplete appearance of the occluded objects make CASD difficult to learn the long-range dependency among the object parts. This could be improved by hard-sample mining in CASD training. <ref type="figure" target="#fig_3">Fig. 5</ref> compares results of MIST <ref type="bibr" target="#b7">[8]</ref> and CASD. CASD has better success in detecting high-quality bounding boxes than MIST. This localization advantages of CASD benefit from its learning of comprehensive attention (see the bottom row of <ref type="figure" target="#fig_3">Fig. 5</ref>). We further demonstrate the localization quality of CASD in <ref type="table" target="#tab_8">Table 6</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 CorLoc on Trainval Sets</head><p>Correct Localization (CorLoc) was used in some previous works to evaluate performance on the VOC 2007 and VOC 2012 trainval sets. CorLoc only evaluates the localization accuracy of detectors. In all those works, CorLoc was reported when WSOD is trained on both the training and validation sets, and tested on both sets. This is probably why CorLoc was mostly used for ablation, not for the main comparison between algorithms.</p><p>For completeness, we provide this additional metric in <ref type="table" target="#tab_8">Table 6</ref>. CASD has the best overall localization accuracy among all compared methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method VOC 2007 VOC 2012</head><p>WSOD State-of-the-Art WSDDN <ref type="bibr" target="#b4">[5]</ref> 53.5 -OICR <ref type="bibr" target="#b11">[12]</ref> 60.6 62.1 PCL <ref type="bibr" target="#b6">[7]</ref> 62.7 63.2 C-MIL <ref type="bibr" target="#b12">[13]</ref> 65.0 67.4 WSOD2(+Reg.) <ref type="bibr" target="#b39">[40]</ref> 69.5 71.9 Pred Net <ref type="bibr" target="#b46">[47]</ref> 70.9 69.5 C-MIDN <ref type="bibr" target="#b14">[15]</ref> 68.7 71.2 MIST(+Reg.) <ref type="bibr" target="#b7">[8]</ref> 68.8 70.9 Ours CASD 70.4 72.3 </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Typical WSOD issues and our CASD solution. Only the attention maps of high confidence proposals by WSOD detection are overlaid on input images. (a) Biased WSOD detects salient objects, clustered instances and object parts. (b) Inconsistent WSOD detects different objects on different transformations of the same image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Comprehensive Attention Self-Distillation (CASD). Input-wise (IW) CASD in (a) distills the comprehensive attention maps that are aggregated from the proposal feature maps of the same image under different transformations (horizontal flipping and scaling). Layer-wise (LW) CASD in (b) computes the comprehensive attention maps over different network layers. Both IW-CASD and LW-CASD encourage the WSOD network to learn balanced and consistent representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>.</head><label></label><figDesc>2) Besides flipping and scaling, any other image transformations could also be used in IW-CASD. 3) In Eq. 3 and Eq. 5, gradients are not back-propagated to the comprehensive attention maps A IW r or A LW r Overall Loss Function. The overall loss for training a CASD-based WSOD network is composed of the losses from Sec. 3.1 and Sec. 3.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Comparison of MIST [8] and CASD. Top: MIST detection; Middle: CASD detection; Bottom: CASD overlaid with attentions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>, are used in our experiments. Both VOC 2007 and VOC 2012 contain 20 object classes with an additional background class. In VOC 2007, the total of 9, 962 images are split into three subsets: 2, 501 for training, 2, 510 for validation and 4, 951 testing. In VOC 2012, all the 22, 531 images are split into the 5, 717 training images, 5, 823 validation images, the rest 10, 991 test images. For both datasets, we followed the standard routine in WSOD<ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7]</ref> to train on the details. All experiments were implemented in PyTorch. The VGG16 and ResNet50 pre-trained on ImageNet</figDesc><table><row><cell>Implementation</cell></row></table><note>train+val set and evaluate on the test set. For MS-COCO trainval set, the train set (82, 783 images) is used for training and the val set (40K images) is used for testing. Only image-level labels are utilized in training. For evaluation, a predicted bounding box is considered to be positive if it has an IoU&gt; 0.5 with the ground-truth. For VOC, mean Average Precision mAP 0.5 (IoU threshold at 0.5) is reported. For MS-COCO, we report mAP 0.5 and mAP (averaged over IoU thresholds in [.5 : 0.05 : .95]).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>+ B 3 + B 2 52.3 B 4 + B 3 + B 2 + B 1 51.4</figDesc><table><row><cell>Method</cell><cell>VOC 2007</cell></row><row><cell>B 4 + B 3</cell><cell>51.8</cell></row><row><cell>B 4</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Ablation study of CASD layer configurations based on the Baseline+LW.</figDesc><table><row><cell>Method</cell><cell>VOC 2007</cell></row><row><cell>Prediction Consistency [46]</cell><cell>50.2</cell></row><row><cell>Attention Consistency [29]</cell><cell>51.0</cell></row><row><cell>Attention Distillation (Ours)</cell><cell>52.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: Ablation study of strategies for attention</cell></row><row><cell>regularization based on Baseline+IW w/o IA.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Comparison with SOTA WSOD results (VGG16 backbone, mAP 0.5 ) on the PASCAL VOC 2007 and VOC 2012.</figDesc><table><row><cell>Method</cell><cell cols="3">Backbone mAP mAP 0.5</cell></row><row><cell>PCL [7]</cell><cell>VGG16</cell><cell>8.5</cell><cell>19.4</cell></row><row><cell>C-MIDN [13]</cell><cell>VGG16</cell><cell>9.6</cell><cell>21.4</cell></row><row><cell>WSOD2 [40]</cell><cell cols="3">VGG16 10.8 22.7</cell></row><row><cell cols="4">MIST(+Reg.) [8] VGG16 11.4 24.3</cell></row><row><cell cols="4">MIST(+Reg.) [8] ResNet50 12.6 26.1</cell></row><row><cell>CASD(ours)</cell><cell cols="3">VGG16 12.8 26.4</cell></row><row><cell>CASD(ours)</cell><cell cols="3">ResNet50 13.9 27.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Comparison with SOTA WSOD results on the MS-COCO dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 )</head><label>5</label><figDesc>. With the VGG16 backbone, CASD produces 12.8% mAP and 26.4% mAP 0.5 , outperforming the VGG16 version of MIST(+Reg.) by clear margins of 1.4% and 2.1%. With the ResNet50 backbone, CASD achieves the state-of-the-art 13.9% mAP and 27.8% mAP 0.5 outperforming MIST(+Reg.) by clear margins of 1.3% mAP and 1.7% mAP 0.5 . Qualitative visualization of detection results can be found in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Comparison of the localization performance (CorLoc) on the VOC 2007 and VOC 2012 trainval sets.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Disclosure of Funding</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Microsoft coco: Common objects in context. ArXiv, abs/1405.0312</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Weakly supervised deep detection networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2846" to="2854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A framework for multiple-instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oded</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomás</forename><surname>Lozano-Pérez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 10</title>
		<editor>M. I. Jordan, M. J. Kearns, and S. A. Solla</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="570" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pcl: Proposal cluster learning for weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="176" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Instance-aware, context-focused, and memory-efficient weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongzheng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.04725</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Slv: Spatial likelihood voting for weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaowu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian-Sheng</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12995" to="13004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Boosting weakly supervised object detection with progressive knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanyi</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.07986</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jasper Rr Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Koen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theo</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold Wm</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="171" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multiple instance detection network with online instance classifier refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2843" to="2851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Jianbin Jiao, and Qixiang Ye. C-mil: Continuation multiple instance learning for weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2199" to="2208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Weakly supervised cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Diba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Pazandeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="914" to="922" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fang Wan, Haihang You, and Dongrui Fan. C-midn: Coupled multiple instance detection network with segmentation guidance for weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochun</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9834" to="9843" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cyclic guidance for weakly supervised joint detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liujuan</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="697" to="707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Weakly supervised object detection with segmentation collaboration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meina</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9735" to="9744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Joon-Young Lee, and In So Kweon. Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongchan</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijie</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.02983</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Weakly supervised region proposal network and object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angtian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongluan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="352" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Incorporating network built-in priors in weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fatemeh Sadat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Sadegh Aliakbarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Petersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1382" to="1396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Attention-based dropout layer for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjung</forename><surname>Shim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2219" to="2228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Improving object detection with inverted attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1294" to="1302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multiple anchor learning for visual object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Self-challenging improves cross-domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haohan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Visual attention consistency under image transforms for multi-label image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochuan</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="729" to="739" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning lightweight lane detection cnns by self attention distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuenan</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1013" to="1021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Sharpen focus: Learning with attention separability and consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lezi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srikrishna</forename><surname>Karanam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kuan-Chuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajat Vikram</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="512" to="521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning without memorizing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prithviraj</forename><surname>Dhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajat Vikram</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan-Chuan</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5138" to="5146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="2935" to="2947" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6550</idno>
		<title level="m">Fitnets: Hints for thin deep nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.03928</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Residual attention network for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengqing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xialei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinash</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Bhotika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05347</idno>
		<title level="m">Continual universal object detection</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning efficient object detection models with knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guobin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wongun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="742" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Wsod2: Learning bottom-up and top-down objectness distillation for weakly-supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8292" to="8300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Howard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.5402</idno>
		<title level="m">Some improvements on deep convolutional neural network based image classification</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Consistency-based semi-supervised learning for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jisoo</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungeui</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeesoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nojun</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10758" to="10767" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Dissimilarity coefficient based weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Arun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M Pawan</forename><surname>Jawahar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9432" to="9441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Another policy is to set different loss weights for the two losses respectively</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Iw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eq. 6, we have a single loss weight γ for both the IW-CASD loss L k IW and LW-CASD loss L k LW</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>Which way is better? So we conduct the following ablation study on VOC. Fixing γ IW = 0.1, CASD gets 55.0%, 55.5%, 55.3%, 54.8%, 54.8% mAP 0.5 when γ LW = 0.05, 0.075, 0.1, 0.15, 0.2 respectively. Fixing γ LW = 0.1, CASD gets 54.3%, 54.6%, 55.3%, 55.1%, 54.9% mAP 0.5 when γ IW = 0.05, 0.075, 0.1, 0.15, 0.2 respectively</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<idno>At L IW = 0.1 and L LW = 0.075</idno>
		<title level="m">CASD achieves</title>
		<imprint>
			<biblScope unit="page">55</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Evidence for consistency and completeness in CASD: First, the attention maps and predicted bounding boxes in Fig. 1 and Fig. 5 compare the results of OICR/MIST and CASD, qualitatively demonstrating CASD gets more consistent and complete object features. Second, on the horizontal flipped VOC 2007 test set</title>
	</analytic>
	<monogr>
		<title level="m">CASD achieves</title>
		<imprint>
			<biblScope unit="page">56</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">We conduct an ablation study on layer-wise CASD with Grad-CAM that gets 52.0% mAP 0.5 on VOC</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Casd With Grad-Cam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CASD, we utilize channel-wise average pooling+sigmoid to get the attention map</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>This is slightly worse than 52.6% mAP 0.5 of layer-wise CASD with channelwise average pooling+sigmoid. Thus the latter is computationally more efficient and adopted in our paper</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Clarification on the branch number of k in OICR: The vanilla OICR [12] has 3 OICR branches (k = 3), and suggests the larger k the better results. We only use k = 2 in all our experiments due to our GPU limitations</title>
	</analytic>
	<monogr>
		<title level="m">Thus CASD may achieve better results by setting k = 3 on GPU with sufficient memory</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
