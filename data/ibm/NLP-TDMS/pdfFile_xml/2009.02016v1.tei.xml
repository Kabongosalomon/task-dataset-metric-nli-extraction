<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dynamic Context-guided Capsule Network for Multimodal Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>ACM</publisher>
				<availability status="unknown"><p>Copyright ACM</p>
				</availability>
				<date>October 12-16, 2020. October 12-16, 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Lin</surname></persName>
							<email>huanlin@stu.xmu.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fandong</forename><surname>Meng</surname></persName>
							<email>fandongmeng@tencent.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Su</surname></persName>
							<email>jssu@xmu.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjing</forename><surname>Yin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyuan</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubin</forename><surname>Ge</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tencent</forename><surname>Wechat</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">China</forename><forename type="middle">Jiebo</forename><surname>Luo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fandong</forename><surname>Meng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Su</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjing</forename><surname>Yin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyuan</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubin</forename><surname>Ge</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Tencent WeChat AI</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">University of Rochester</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution">University of Rochester</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Dynamic Context-guided Capsule Network for Multimodal Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 28th ACM International Conference on Multimedia (MM &apos;20)</title>
						<meeting>the 28th ACM International Conference on Multimedia (MM &apos;20) <address><addrLine>Seattle, WA, USA 2020; Seattle, WA, USA; New York, NY, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>ACM</publisher>
							<biblScope unit="volume">10</biblScope>
							<date type="published">October 12-16, 2020. October 12-16, 2020</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3394171.3413715</idno>
					<note>ACM ISBN 978-1-4503-7988-5/20/10. . . $15.00</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS • Computing methodologies → Natural language generation</term>
					<term>Computer vision</term>
					<term>Machine translation * Corresponding author</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multimodal machine translation (MMT), which mainly focuses on enhancing text-only translation with visual features, has attracted considerable attention from both computer vision and natural language processing communities. Most current MMT models resort to attention mechanism, global context modeling or multimodal joint representation learning to utilize visual features. However, the attention mechanism lacks sufficient semantic interactions between modalities while the other two provide fixed visual context, which is unsuitable for modeling the observed variability when generating translation. To address the above issues, in this paper, we propose a novel Dynamic Context-guided Capsule Network (DCCN) for MMT. Specifically, at each timestep of decoding, we first employ the conventional source-target attention to produce a timestep-specific source-side context vector. Next, DCCN takes this vector as input and uses it to guide the iterative extraction of related visual features via a context-guided dynamic routing mechanism. Particularly, we represent the input image with global and regional visual features, we introduce two parallel DCCNs to model multimodal context vectors with visual features at different granularities. Finally, we obtain two multimodal context vectors, which are fused and incorporated into the decoder for the prediction of the target word. Experimental results on the Multi30K dataset of English-to-German and English-to-French translation demonstrate the superiority of DCCN. Our code is available on https://github.com/DeepLearnXMU/MM-DCCN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>MMT significantly extends the conventional text-based machine translation by taking corresponding images as additional inputs <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b41">42]</ref>. The assumption behind this is that the translation is expected to be more accurate compared to purely text-based translation, since the visual context helps to solve data sparsity and ambiguity problems <ref type="bibr" target="#b27">[28]</ref>. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, with the help of the image, MMT model is able to correctly translate "bank" to "erdwall". Overall, the research on MMT is of great significance. On the one hand, similar to other multimodal tasks such as image captioning <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b40">41]</ref> and visual question answering <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b63">64]</ref>, MMT involves computer vision and natural language processing (NLP) and proves the effectiveness of visual features in translation tasks. In other words, it not only requires an algorithm with in-depth understanding of visual contexts, but also connects its interpretation with a language model to create a natural sentence. On the other hand, MMT has wide applications, such as translating multimedia news, product information and movie subtitles <ref type="bibr" target="#b62">[63]</ref>. Therefore, MMT has become an attractive but challenging multimodal task.</p><p>Very importantly, one of the key issues in MMT is how to effectively utilize visual features during the process of translation. To achieve this goal, three categories of methods have been investigated: (1) exploiting visual features as global visual context <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">27]</ref>; <ref type="bibr" target="#b1">(2)</ref> applying attention mechanism to extract visual context, where one common approach is to employ a timestep-specific attention mechanism to extract visual context <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b25">26]</ref> and another way is to use source hidden states to consider visual features and then use the obtained invariant visual context as a complement to source hidden states <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b15">16]</ref>; <ref type="bibr" target="#b2">(3)</ref> learning multimodal joint representations <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b62">63]</ref>. Despite their successes, these approaches still have various shortcomings. First, global visual context and learning multimodal joint representations can not encode the observed variability when generating translation. Second, according to previous studies <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b51">52]</ref>, extracting visual context is beyond the capacity of a single-step attention due to the complexity of multimodal tasks. Although multiple attention layers can refine the extraction of visual context, the improvement is still limited. One possible reason is that too many parameters of multiple attention layers make the model vulnerable to over-fitting, especially when limited training examples are given in MMT. Moreover, the above methods only use visual features at global or regional level, which is unable to offer sufficient visual guidance. As a result, visual features are not fully utilized, limiting the potential of MMT models.</p><p>To overcome these issues, in this paper, we propose a novel Dynamic Context-guided Capsule Network (DCCN) for MMT. At each timestep of decoding, we first employ the standard sourcetarget attention to produce a timestep-specific source-side context vector. Next, DCCN takes this context vector as input and uses it to guide the iterative extraction of related visual context during the dynamic routing process, where a multimodal context vector is updated simultaneously. In particular, to fully exploit image information, we employ DCCN to extract visual features at two complementary granularities: global visual features and regional visual features, respectively. In this way, we can obtain two multimodal context vectors that are then fused for the prediction of the current target word. Compared with previous studies, DCCN is able to dynamically extract visual context without introducing a large number of parameters, which is suitable to model such kind of variability observed in machine translation. Potentially, DCCN learns a better multimodal joint representation for MMT. Therefore, it is also applicable to other related tasks that require a joint representation of two different modalities, such as visual question answering. In summary, the major contributions of our work are three-fold:</p><p>• We introduce a capsule network to effectively capture visual features at different granularities for MMT, which has advantage of effectively capturing visual features without explosive parameters. To the best of our knowledge, our work is the first attempt to explore a capsule network to extract visual features for MMT. • We propose a novel context-guided dynamic routing for the capsule network, which uses the timestep-specific sourceside context vector as the guiding signal to dynamically produce a multimodal context vector for MMT.</p><p>• We conduct experiments on the Multi30K English-German and English-French datasets. Experimental results show that our model significantly outperforms several competitive MMT models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>The related work mainly includes the studies of multimodal context modeling in MMT and capsule networks.</p><p>Multimodal context modeling in MMT. How to fully exploit context for neural machine translation has always been a hot research topic <ref type="bibr">[44-47, 49, 58-61]</ref>, which is the same for MMT. The commonly-used approaches to extract multimodal context for MMT can be classified into three categories: (1) Learning global visual features for MMT. For instance, Huang et al. <ref type="bibr" target="#b26">[27]</ref> concatenated global and regional visual features with source sequences. Calixto and Liu <ref type="bibr" target="#b7">[8]</ref> utilized global visual features as additional tokens in the source sequence, to initialize the encoder hidden states or initialize the first decoder hidden state. (2) Leveraging attention mechanism to exploit visual features. In this aspect, Caglayan et al. <ref type="bibr" target="#b6">[7]</ref> and Calixto et al. <ref type="bibr" target="#b8">[9]</ref> incorporated spatial visual features into the MMT model via an independent attention mechanism. Furthermore, Delbrouck and Dupont <ref type="bibr" target="#b16">[17]</ref> employed Compact Bilinear Pooling to fuse the attention-based context vectors of two modalities. Meanwhile, Libovický and Helcl <ref type="bibr" target="#b31">[32]</ref> explored flat and hierarchical combinations to fuse the attention-based context vectors of two modalities. Instead of using attention mechanism, Grönroos et al. <ref type="bibr" target="#b22">[23]</ref> introduced a gating layer to modify the prediction distribution on both visual features and decoder states. Unlike previous studies, Delbrouck and Dupont <ref type="bibr" target="#b15">[16]</ref> utilized the attention mechanism on visual inputs for the source hidden states. Along this line, Arslan et al. <ref type="bibr" target="#b2">[3]</ref> extended this approach into Transformer, and Helcl et al. <ref type="bibr" target="#b25">[26]</ref> used timestep-specific source-side context vector as attention query to dynamically produce the visual context vectors. (3) Applying multitask learning to jointly model translation task with other visual related tasks. For example, Elliott and Kádár <ref type="bibr" target="#b20">[21]</ref> decomposed multimodal translation into two sub-tasks: learning to translate and learning visually grounded representations. Zhou et al. <ref type="bibr" target="#b62">[63]</ref> optimized the learning of a shared visual language embedding and a multimodal attention-based translator. Calixto et al. <ref type="bibr" target="#b9">[10]</ref> introduced a continuous latent variable for MMT, which contains the underlying semantic information extracted from texts and images. Recently, Yin et al. <ref type="bibr" target="#b56">[57]</ref> uses a unified multi-modal graph to capture various semantic relationships between multi-modal semantic units.</p><p>Capsule Network. Recently, capsule network has been widely used in computer vision <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b53">54]</ref> and NLP <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b54">55]</ref> tasks. Specific to machine translation, Wang et al. <ref type="bibr" target="#b49">[50]</ref> employed dynamic routing algorithm to model child-parent relationships between lower and higher encoder layers. Yang et al. <ref type="bibr" target="#b55">[56]</ref> proposed a query-guided capsule networks to cluster context information into different perspectives from which the target translation may concern. Zheng et al. <ref type="bibr" target="#b61">[62]</ref> separated translated and untranslated source words into different groups of capsules.</p><p>To the best of our knowledge, our work is the first attempt to introduce the capsule network into MMT. Furthermore, we use the timestep-specific source-side context vector rather than static   .,t as guidance, these two DCCNs iteratively extract global and regional visual features to form two multimodal context vectors: m д and m r , respectively.</p><p>source hidden states to guide the routing procedure. Therefore, we can dynamically produce visual context for MMT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OUR MODEL</head><p>As shown in <ref type="figure" target="#fig_2">Figure 2</ref>, our model is based on Transformer <ref type="bibr" target="#b47">[48]</ref>. The most important feature of our model is that two DCCNs are introduced to dynamically learn multimodal context vectors for generating translations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Encoder</head><p>Given the source sentence X , we represent each source word as the sum of its word embedding and positional encoding. Next, we follow Vaswani et al. <ref type="bibr" target="#b47">[48]</ref> to use a stack of L e identical layers to encode X , where each layer consists of two sub-layers. Note that we also introduce residual connection and layer normalization to each sub-layer, of which the descriptions are omitted.</p><p>Specifically, at the l-th layer (1≤l ≤L e ), the first sub-layer is a multi-head self-attention:</p><formula xml:id="formula_0">H (l ) e = MultiHead(S (l −1) , S (l −1) , S (l −1) ),<label>(1)</label></formula><p>where H (l ) e is the temporary encoder hidden states, MultiHead( * ) is a multi-head self-attention function, and S (l −1) ∈R d w ×|X | is the representation of the source sentence at the (l-1)-th layer, d w is the model dimension. Particularly, S (0) is the concatenation of all source word embeddings. Taking a query matrix Q, a key matrix K and a value matrix V as inputs, MultiHead( * ) is defined as follows:</p><formula xml:id="formula_1">MultiHead(Q, K, V) = Concat(head 1 , ..., head N h )W C (2) where head k = Attention(QW Q k , KW K k , VW V k ) = Softmax( QW Q k (KW K k ) T √ d w )VW V k ,</formula><p>where W C and W * k are learnable parameter matrices. The second sub-layer is a position-wise fully connected feedforward network. It is applied to each position separately and identically, forming the representation S (l ) of the source sentence as</p><formula xml:id="formula_2">S (l ) = FFN(H (l ) e ),<label>(3)</label></formula><p>where FFN( * ) is a position-wise feed-forward function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Decoder</head><p>As shown in <ref type="figure" target="#fig_2">Figure 2</ref>, our decoder is an extension of the Transformer decoder <ref type="bibr" target="#b47">[48]</ref>. It takes the already generated sequence as inputs and uses a stack of L d identical layers to produce target-side hidden states. Similar to the standard Transformer decoder, each layer of our decoder contains three sub-layers. The only difference is that at the last decoder layer, two DCCNs are equipped to produce timestep-specific multimodal context vectors for MMT. Specifically, the first sub-layer is also a multi-head self-attention:</p><formula xml:id="formula_3">H (l ) d = MultiHead(T (l −1) , T (l −1) , T (l −1) ), 1 ≤ l ≤ L d ,<label>(4)</label></formula><p>where H (l ) d denotes the temporary decoder hidden states, produced by a multi-head self-attention mechanism fed with the target-side hidden states T (l −1) at the previous layer.</p><p>Typically, at the second sub-layer, a multi-head source-target attention mechanism is used to dynamically produce the timestepspecific source-side context vectors C (l ) :</p><formula xml:id="formula_4">C (l ) = MultiHead(H (l ) d , S (L e ) , S (L e ) ), 1 ≤ l ≤ L d .<label>(5)</label></formula><p>The third sub-layer is a position-wise fully connected feed-forward neural network, of which the definition depends on the decoder layer. At the first (L d − 1) layers, this sub-layer produces the targetside hidden states T (l ) as follows:</p><formula xml:id="formula_5">T (l ) = FFN(C (l ) ), 1 ≤ l ≤ L d − 1.<label>(6)</label></formula><p>Very importantly, at the last (L d -th) decoder layer, we introduce two DCCNs between the second and third sub-layers to learn multimodal context representations at the t-th timestep as follows:</p><formula xml:id="formula_6">m д = CapsuleNet(C (L d )</formula><p>.,t , I д ),</p><formula xml:id="formula_7">m r = CapsuleNet(C (L d )<label>(7)</label></formula><p>.,t , I r ),</p><p>where CapsuleNet( * ) is a context-guided dynamic routing function, C</p><formula xml:id="formula_9">(L d )</formula><p>.,t is the t-th column vector of C (L d ) , representing the sourceside context vectors at the t-th timestep, I д and I r represent global visual features and local visual features respectively, as depicted in subsection 3.2.1. By using DCCN, we can iteratively extract the related visual features to dynamically produce better multimodal context representations for MMT. We will describe this procedure in subsection 3.2.2. Next, we fuse m д and m r via the following gating mechanism:</p><formula xml:id="formula_10">M (L d ) .,t = αm д + (1 − α)m r (9) α = Sigmoid(W д m д + W r m r ),<label>(10)</label></formula><p>where W д and W r are learnable parameters. Correspondingly, the Eq. 6 at the last decoder layer becomes</p><formula xml:id="formula_11">T (L d ) = FFN(M (L d ) ).<label>(11)</label></formula><p>Finally, with the target-side hidden states generated by Eq. 11, our decoder adopts a Softmax layer to generate the probability distribution of the current target word y t :</p><formula xml:id="formula_12">P(y t |X , Y &lt;t ; θ ) ∝ exp(WT (L d ) .,t ),<label>(12)</label></formula><p>where Y &lt;t is the previously generated target words y 1 y 2 . . . y t −1 , W ∈R |V y |×d w is a model parameter, V y is the target vocabulary, and</p><formula xml:id="formula_13">T (L d )</formula><p>.,t is the t-th column vector of T (l ) for predicting y t .   <ref type="formula">(2)</ref> regional visual features that illustrate class annotations of each region (e.g. cat, arms, peak). Following Anderson et al. <ref type="bibr" target="#b1">[2]</ref>, we employ the R-CNN based bottom-up attention to identify the regions with class annotations. For each region, we generate the corresponding prediction probability distribution over 1,600 classes from Visual Genome. To represent each region as a vector, we project its class annotations into word embeddings and define the region vector as the weighted sum of its class annotation embeddings. Finally, all region vectors are concatenated to represent the semantics of input image. In practice, we keep the number of predicted regions up to 10 so as to reduce negative effects of abundant regions, therefore the regional visual features can be represented as a 10ÃŮ256 matrix I r , where each of the 10 rows consists of a 256D feature vector.</p><formula xml:id="formula_14">… { } { } { } … … … … multimodal context { } =1 { } =1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Dynamic</head><p>Context-guided Capsule Network. Our DCCN is a significant extension of the conventional capsule network. Thus, it retains the advantages on iterative feature extraction of capsule network, which has shown effective in many computer vision <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b53">54]</ref> and NLP <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b53">[54]</ref><ref type="bibr" target="#b54">[55]</ref><ref type="bibr" target="#b55">[56]</ref><ref type="bibr" target="#b61">62]</ref> tasks. More importantly, unlike the conventional capsule network that only captures static visual context, DCCN introduces the timestep-specific source-side context vector to guide the extraction of multimodal context, which can model the observed variability during translation. <ref type="figure" target="#fig_3">Figure 3</ref> shows the architecture of DCCN. Similar to the conventional capsule network, DCCN consists of (1) low-level capsules {u i } N u i=1 (the first column from the right) encoding the visual features of the input image and (2) high-level capsules {v j } N v j=1 (the second column from the left) encoding the related visual features. Besides, it includes multimodal context capsules {m j } N v j=1 (the first column from the left), where m j indicates a temporary multimodal context vector iteratively updated with v j and is used as query to guide the extraction of related visual features.</p><p>We summarize the training procedure in Algorithm 1. Given a visual features matrix I, we use low-level capsules .,t , the input image I, the low-level capsule number N u , the highlevel capsule number N v , the iteration number N it r 2: Output: the multimodal context vector m 3: for i = 1 . . . N u do <ref type="bibr">4:</ref> Initialize the low-level capsules u i with I i 5: end for 6: for j = 1 . . . N v do 7:</p><formula xml:id="formula_15">{u i } N u i=1 (u i ∈<label>R</label></formula><p>Initialize the multimodal context capsules m j with C (L d ) .,t 8: end for 9: for each low-level capsule u i do <ref type="bibr">10:</ref> for each high-level capsule v j do</p><formula xml:id="formula_16">11: b i j ← 0 12:û j |i = W i j u i 13: ρ i j ← tanh(PCC(u i , W m m j )) 14:</formula><p>end for 15: end for 16: for itr = 1 . . . N it r do <ref type="bibr">17:</ref> for each low-level capsule u i do <ref type="bibr">18:</ref> for each high-level capsule v j do <ref type="bibr" target="#b18">19</ref>:</p><formula xml:id="formula_17">{c i j } N v j=1 ← Softmax({b i j } N v j=1 ) 20:</formula><p>end for <ref type="bibr">21:</ref> end for <ref type="bibr">22:</ref> for each high-level capsule v j do</p><formula xml:id="formula_18">23: v j ← i (c i j + ρ i j )û j |i 24: m j ← m j ⊙ W v v j 25:</formula><p>end for <ref type="bibr">26:</ref> for each low-level capsule u i do <ref type="bibr">27:</ref> for each high-level capsule v j do 28:</p><formula xml:id="formula_19">ρ i j ← tanh(PCC(u i , W m m j )) 29: b i j ← b i j + ρ i j (û j |i · v j ) 30:</formula><p>end for <ref type="bibr">31:</ref> end for 32: end for <ref type="bibr">33:</ref> </p><formula xml:id="formula_20">m = FuseMultimodalContext(m 1 , . . . , m N v )</formula><p>with the timestep-specific source-side context vector C</p><formula xml:id="formula_21">(L d )</formula><p>.,t (Line 7). Following the conventional capsule network, we introduce the matrix W ij to transform the i-th low-level capsule u i intoû j|i (Line 12), which will be used to generate the j-th high-level capsule v j .</p><p>Next, we introduce the coefficient ρ i j to measure the cross-modal correlation between u i and the multimodal context vector m j (Line 13), which can be subsequently used to generate high-level capsules v j and update b i j , benefiting the extraction of related visual features. Formally, the correlation function is defined as</p><formula xml:id="formula_22">ρ i j = tanh(PCC(u i , W m m j )) = tanh( Cov(u i , W m m j ) σ (u i ) σ (W m m j ) ),<label>(13)</label></formula><p>where PCC( * ) indicates the Pearson Correlation Coefficients, W m is a parameter matrix that maps m j to the same semantic space of u i , Cov( * ) is the covariance and σ ( * ) is the standard deviation. When ρ i j is close to +1, the visual features encoded by u i is closely related to m j , otherwise it indicates negative correlation.</p><p>Then, we conduct N it r iterations of routing to capture related visual features at current timestep (Line 16 to Line 32). At each iteration, we generate high-level capsules from low-level capsules. To do this, we employ a Softmax function along columns with the logits b i j initialized as 0 to calculate the coupling coefficient c i j (Line 19). Afterwards, we generate the high-level capsule v j to represent visual context as the weighted sum ofû j |i according to their corresponding c i j and the cross-modal correlation coefficient ρ i j (Line 23). Note that unlike the conventional dynamic routing algorithm <ref type="bibr" target="#b37">[38]</ref> where v j only depends on c i j andû j|i , we further introduce ρ i j that enables the most relevant visual features to be iteratively clustered into high-level capsules. Note that the conventional capsule network <ref type="bibr" target="#b37">[38]</ref> uses the norm of high-level capsule to represent prediction probability, thus the norm of v j is adjusted to [0,1] using the "squashing" function. Different from that, v j represents visual context in DCCN. Therefore we do not apply "squashing" function in our algorithm. Further, we introduce a transformation matrix W v to map visual context v j into the semantic space of multimodal context and follow Wu and Mooney <ref type="bibr" target="#b50">[51]</ref> to update m j with the captured visual context v j (Line 24). By doing so, we expect the updated multimodal context capsule can be better exploited to guide the routing procedure at the next iteration.</p><p>Finally, we update ρ i j (Line 28), and then use it to guide the updating of b i j (Line 29). Different from conventional dynamic routing algorithm, where b i j only depends on the cumulative "agreement" between u i and v j , we control the updating range of b i j according to the cross-modal correlation ρ i j .</p><p>Through N it r iterations of routing, we obtain N v multimodal context capsules {m j } N v j=1 , fused by a linear transformation to produce the final multimodal context vector m (Line 33).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS 4.1 Dataset</head><p>To investigate the effectiveness of our proposed model, we conduct experiments on the Multi30K dataset <ref type="bibr" target="#b19">[20]</ref>, which is an extended version of the Flickr30K Entities and has been widely used in MMT <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b41">42]</ref>. For each image, one of the English (EN) descriptions was selected and manually translated into German (DE) and French (FR) by professional translators <ref type="bibr" target="#b41">[42]</ref>. The dataset contains 29,000 instances for training, 1,024 for validation and 1,000 for testing. We also report results on the WMT2017 test set with 1,000 instances and the MSCOCO test set containing 461 out-of-domain instances with ambiguous verbs. Besides, as mentioned in subsection 3.2.1, we represent the input image with visual features in two granularities.</p><p>We apply the MOSES scripts 1 to preprocess datasets. We then employ the Byte Pair Encoding <ref type="bibr" target="#b38">[39]</ref> with 10,000 merging operations to convert tokens into subwords.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Setup</head><p>We develop our proposed model based on OpenNMT Transformer <ref type="bibr" target="#b29">[30]</ref>. Since the size of training corpus is small and the trained model tends to be over-fitting, we first perform a small grid search to obtain a set of hyper-parameters on the EN⇒DE validation set. Specifically, the layer numbers of both encoder and decoder are set to 4 and the number of attention heads is set to 8. Both hidden size and embedding size are set to 256. As implemented in <ref type="bibr" target="#b47">[48]</ref>, we use the Adam optimizer with β 2 = 0.998 and scheduled learning rate to optimize various models. The learning rate is initialized as 1. During training, each batch consists of approximately 3,700 source and target tokens. Besides, we employ the dropout strategy <ref type="bibr" target="#b42">[43]</ref> with rate 0.5 to enhance the robustness of our model. Finally, we adopt the MultEval scripts <ref type="bibr" target="#b13">[14]</ref> to evaluate the translation quality in terms of BLEU <ref type="bibr" target="#b35">[36]</ref> and METEOR <ref type="bibr" target="#b17">[18]</ref>. Particularly, we run all models three times for each experiment and report the average results.</p><p>The context-guided dynamic routing is important for the generation of multimodal context. Therefore, we investigate the impacts of its hyper parameters on the routing mechanism: high-level capsule number N v and routing iteration number N it r . To this end, we try different numbers of high-level capsules and routing iteration numbers to train our model: N v from 1 to 3, N it r from 1 to 4 on the validation set. We observe that N v larger than 1 and N it r larger than 3 do not lead to significant improvements and increase the GPU memory requirement. Hence, we use N v =1 and N it r =3 in all subsequent experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baselines</head><p>We directly refer to our MMT model as DCCN and compare it with the following commonly-used MMT baselines:</p><p>• Transformer <ref type="bibr" target="#b47">[48]</ref> A text-only machine translation model.</p><p>• Encoder-attention <ref type="bibr" target="#b15">[16]</ref>. It incorporates an encoder-based visual attention mechanism into Transformer, which uses source hidden states to consider visual features and then augment each source hidden state with its corresponding visual context. Please note that <ref type="bibr" target="#b15">[16]</ref> is based on RNN and we implement it on Transformer for comparability. • Doubly-attention <ref type="bibr" target="#b25">[26]</ref>. A doubly attentive Transformer that introduces an additional visual attention sub-layer to exploit visual features. Specifically, this sub-layer is inserted between the source-target attention and feed-forward sublayer. For visual attention, the context vectors from the source-target attention are used as queries, and the context vectors of visual attention are fed into the feed-forward sub-layer.</p><p>We also display the performance of several dominant MMT models on the same datasets. Stochastic attention <ref type="bibr" target="#b14">[15]</ref> is a stochastic and sampling-based attention mechanism, which focuses on only one spatial location of the image at every timestep. It is also the model of best performance in <ref type="bibr" target="#b14">[15]</ref>. Imagination <ref type="bibr" target="#b20">[21]</ref> employs multitask learning to jointly two sub-tasks: translating and visually grounded representation prediction. Fusion-conv [6] employs a single feed-forward network to establish the attention alignment between visual features and target-side hidden states at each timestep, where all spatial locations of image are considered to derive the context vector. Trg-mul <ref type="bibr" target="#b5">[6]</ref> modulates each target word embedding with visual features using element-wise multiplication. Latent Variable MMT <ref type="bibr" target="#b9">[10]</ref> exploits the interactions between visual and textual features for MMT through a latent variable, which can be seen as a multimodal stochastic embedding of an image and its target language description. Deliberation Network <ref type="bibr" target="#b27">[28]</ref> is based on a translate-and-refine strategy, where visual features are only used by the decoder at the second stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results on the EN⇒DE Translation Task.</head><p>Parameters Introducing visual feature features into Transformer model brings more parameters. As shown in <ref type="table">Table 1</ref>, Transformer (Row 7) has 16.1M parameters. Encoder-attention (Row 8) adds a layer normalization, a fully connected layer and two multi-head attention layers, introducing 1.1M parameters, and Doubly attention (Row 9) increases 4.0M parameters by adding two layer normalization and two multi-head attention layers. By contrast, DCCN (Row 10) only introduces 1.0M extra parameters. Thus, DCCN introduces a small number of extra parameters compared to Transformer, and requires smaller parameters than two multimodal baselines.</p><p>Model Performance <ref type="table">Table 1</ref> shows the translation quality on EN⇒DE translation task. It is obvious that DCCN outperforms most of the existing models and all baselines, except Fusion-conv (Row 3) and Trg-mul (Row 4) on METEOR. Note that these two systems are the state-of-the-arts on WMT 2017, with parameter  <ref type="figure">Figure 4</ref>: BLEU scores on different translation groups divided according to source sentence lengths. Since MSCOCO only contains two sentences longer than 20, we divide all sentences longer than 20 into one group for testing.  selection based on METEOR. Moreover, we draw two interesting conclusions: First, DCCN model outperforms Encoder-attention, which uses static source hidden states to attend to visual features. The underlying reasons consist of two aspects: (1) encoder-attention depends on static source representations to extract visual context. By contrast, DCCN utilizes the timestep-specific source-side context vector to extract visual context; and (2) the context-guided dynamic routing mechanism exploits the interactions between different modalities to produce better multimodal context vector.</p><p>Second, although Doubly-attention also uses timestep-specific source-side context vector, DCCN model still achieves a significant improvement over it. This demonstrates again the advantage of modeling the semantic interactions between different modalities on learning multimodal context vectors.</p><p>Finally, following Bahdanau et al. <ref type="bibr" target="#b3">[4]</ref>, we divide our test sets into different groups based on the lengths of source sentences, and then compare different models in each group. <ref type="figure">Figure 4</ref> reports the BLEU scores of two test sets on different groups. Overall, our model still consistently achieves the best performance in most groups. Thus, we confirm again the effectiveness and generality of DCCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Study</head><p>To explore the effectiveness of different components in DCCN, we further compare our model with the following variants in <ref type="table" target="#tab_4">Table 2</ref>:</p><p>(1) dynamic routing (global). To build this variant, we only use global visual features in our model. The result in Row 2 indicates that removing the regional visual features lead to performance drop. This result suggests that regional visual features are indeed useful for multimodal representation learning.</p><p>(2) dynamic routing (regional). Unlike the above variant, we only use regional visual features to represent the input image in this variant. According to the result shown in Row 3, we observe this change results in a significant performance decline, demonstrating that global visual features also bring useful visual information to our model.</p><p>(3) dynamic routing ⇒ attention. Apparently, one advantage of our model lies in leveraging context-guided dynamic routing to exploit the semantic interactions between different modalities for learning multimodal representation. Here we separately replace the context-guided dynamic routing with the conventional attention mechanism to exploit global visual features, regional visual features, and both of them. Then we investigate the change of model performance. To facilitate the following descriptions, we refer to these three variants as dynamic routing (global) + attention (regional), dynamic routing (regional) + attention (global), and attention (global) + attention (regional), respectively. From Row 4 to Row 6 of <ref type="table" target="#tab_4">Table  2</ref>, we observe that dynamic routing (global) + attention (regional) slightly outperforms dynamic routing (global) while is inferior to DCCN. Similarly, the performance of dynamic routing (regional) + attention (global) is between dynamic routing (regional) and DCCN. Moreover, when we use attention mechanism rather than dynamic routing to extract two kinds of visual features, the performance of our model degrades most. Based on these experimental results, we can draw the conclusion that context-guided dynamic routing is able to better extract two types of visual features than conventional attention mechanism.</p><p>(4)w/o context guidance in dynamic routing. By removing the context guidance from DCCN, we adopt the standard capsule network to extract visual features. As shown in Row 7, the model performance drops drastically in this case. This result is consistent with our intuition that the ideal visual features required for translation should be dynamically captured at different timesteps.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Case Study</head><p>When encountering ambiguous source words or complicated sentences, it is difficult for MMT models to translate correctly without corresponding visual features. To further demonstrate the effectiveness of DCCN, we display the 1-best translations of the four cases generated by different models, as shown in <ref type="figure" target="#fig_6">Figure 5</ref>. When encountering ambiguous nouns, the regional visual features are more helpful. For example, in case (a), both Transformer and Encoder-attention miss the translation of source word "cliff ", while Doubly-attention and DCCN translate it correctly according to the detected object "rock". In case (b), we can find that the ambiguous source word "student" is translated to "Ãľtudiants (college students)" by all baselines, while only DCCN correctly translates it with "ÃľlÃĺves (generally refers to all students)" with the help of the detected object "kid".</p><p>When the model has difficulty in translating words out of the predicted objects such as verbs and adjectives, the global visual features are more helpful. In case (c), the source word "rides" is not associated with any object and thus all baselines choose "fÃďhrt (drive)", while only DCCN translates it correctly. In case (d), three baselines translate "harvest rice" to "circulant (flow)", "travaillent (work)" and "pagaient (paddle)", respectively. By contrast, only DCCN can produce the correct translation with the help of image information.</p><p>These cases reveal that DCCN can fully utilize complementary visual information to learn more accurate representations and disambiguate during translation in different cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Results on the EN⇒FR Translation Task</head><p>To investigate the generality of our proposed model, we also conduct experiments on the EN⇒FR translation task. <ref type="table" target="#tab_7">Table 3</ref> reports the final experimental results. Likewise, no matter which evaluation metric is used, our model still achieves better performance than all baselines. This result strongly demonstrates again that DCCN is effective and general to different language pairs in MMT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we have proposed a novel context-guided capsule network (DCCN) for MMT. As a significant extension of the conventional capsule network, DCCN utilizes the timestep-specific source-side context vector to dynamically guide the extraction of visual features at different timesteps, where the semantic interactions between modalities can be fully exploited for MMT via context-guided dynamic routing mechanism. Moreover, we employ DCCN to extract visual features in two complementary granularities: global visual features and regional visual features, respectively. Experimental results on English-to-German and English-to-French MMT tasks strongly demonstrate the effectiveness of our model. In the future, we plan to apply DCCN to other multimodal tasks such as visual question answering and multimodal text summarization.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An example of English (EN)-to-German (DE) MMT, according to the image, "bank" in the source sentence indicates sloping raised land rather than financial organization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Feed</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>The architecture of our model. Note that we only equip the last decoder layer with two DCCNs. Using the timestepspecific source-side context vector C(L d )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>The routing procedure of DCCN each of the 196 rows consists of a 256D feature vector; and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>1 :</head><label>1</label><figDesc>256 ) to individually encode the semantic representation of each row of I (Line 4), and initialize each multimodal context capsule m j Algorithm 1 Context-guided Dynamic Routing Mechanism. Input: the timestep-specific source-side context vector C (L d )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Translation examples of different MMT models. We show the object with the highest probability of each region.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>3.2.1 Visual Features.To fully exploit visual information for MMT, we investigate two kinds of visual features to enhance text-based translation: (1) global visual features, which represent an input image with high-level concepts. Here we use the res4f layer activations of pre-trained 50-layer Residual Network (ResNet-50)<ref type="bibr" target="#b24">[25]</ref> as global visual features. These spatial features encode an image in a 14ÃŮ14 grid, where each grid is represented by a 1,024D feature vector, only encoding the information about the specific part of the image. Before fed into DCCN, we first follow Calixto et al.<ref type="bibr" target="#b8">[9]</ref> to transform global visual features into a 196ÃŮ256 matrix I д where</figDesc><table><row><cell>high-level capsules (visual context)</cell><cell>� |</cell><cell>low-level capsules (visual features)</cell></row><row><cell></cell><cell></cell><cell>=1</cell></row><row><cell>…</cell><cell>…</cell><cell></cell></row><row><cell></cell><cell>…</cell><cell></cell></row><row><cell>PCC</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Ablation study of our model on the EN⇒DE translation task.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Objects: [kid, floor, woman, window] EN: A group of students sit and listen to the speaker. Ref (FR): Un groupe de élèves sont assis et écoutent la intervenante. Transformer: Un groupe de étudiants sont assis et écoutent la orateur. Encoder-attention: Un groupe de étudiants sont assis et écoutent le orateur. Un groupe de élèves sont assis et écoutent le orateur. Objects: [leg, water, rope, man, rock] EN: A man is abseiling down a cliff over the ocean . Ref (DE): Ein Mann seilt sich an einer Klippe über dem Ozean ab. Transformer: Ein Mann wird von einem Aussichtspunkt über dem Meer . Encoder-attention: Ein Mann stößt schnell über den Ozean. Doubly-attention: Ein Mann wird von einer Klippe über das Meer geworfen. ... reitet auf den Schultern eines Mannes ... Transformer: ... fährt auf den Schultern eines Mannes ... Encoder-attention: ... fährt auf den Schultern eines Mannes ... Doubly-attention : ... fährt auf den Schultern eines Mannes ... DCCN: ... reitet auf den Schultern eines Mannes ... Trois agriculteurs récoltent du riz dans un champ de riz . Transformer: Trois agriculteurs circulant dans un champ de riz . Trois agriculteurs pagaient dans un champ de riz . DCCN: Trois agriculteurs récoltent du riz dans un champ de riz .</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Doubly-attention: Un groupe de étudiants sont assis et écoutent le</cell></row><row><cell>(a)</cell><cell cols="3">orateur. DCCN: DCCN: Ein Mann stößt sich eine Klippe über das Meer. (b)</cell></row><row><cell></cell><cell>Objects: [shorts, man, woman, sign, sidewalk, umbrella, dress, glasses, girl]</cell><cell></cell><cell>Objects : [grass, man, shirt, hat, tree, field, jacket]</cell></row><row><cell></cell><cell>EN: A girl wearing a mask rides on a man's shoulders through a crowded</cell><cell></cell><cell>EN: Three farmers harvest rice out in a rice field .</cell></row><row><cell cols="2">sidewalk. Ref (DE): (c)</cell><cell>(d)</cell><cell>Ref (FR): Encoder-attention: Trois agriculteurs travaillent dans un champ de riz . Doubly-attention :</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Experimental results on the EN⇒FR translation task.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://www.statmt.org/moses/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">ACKNOWLEDGMENTS</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Hierarchical Multi-label Classification of Text with Capsule Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Aly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Remus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Biemann</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p19-2045</idno>
		<ptr target="https://doi.org/10.18653/v1/p19-2045" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics</title>
		<meeting>the 57th Conference of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="323" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00636</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2018.00636" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6077" to="6086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Doubly Attentive Transformer Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hasan Sait Arslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Fishel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anbarjafari</surname></persName>
		</author>
		<idno>abs/1807.11605</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural Machine Translation by Jointly Learning to Align and Translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="https://iclr.cc/archive/www/doku.php%3Fid=iclr2015:accepted-main.html" />
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="432" to="439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Findings of the Third Shared Task on Multimodal Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loïc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiraag</forename><surname>Lala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Frank</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/w18-6402</idno>
		<ptr target="https://doi.org/10.18653/v1/w18-6402" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation: Shared Task Papers</title>
		<meeting>the Third Conference on Machine Translation: Shared Task Papers<address><addrLine>Belgium, Brussels</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="304" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">LIUM-CVC Submissions for WMT17 Multimodal Translation Task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Caglayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walid</forename><surname>Aransa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Bardet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mercedes</forename><surname>García-Martínez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loïc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Masana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Herranz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><surname>Van De Weijer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/w17-4746</idno>
		<ptr target="https://doi.org/10.18653/v1/w17-4746" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Conference on Machine Translation</title>
		<meeting>the Second Conference on Machine Translation<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="432" to="439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Multimodal attention for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Caglayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loïc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<idno>abs/1609.03976</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Incorporating Global Visual Features into Attention-based Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacer</forename><surname>Calixto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d17-1105</idno>
		<ptr target="https://doi.org/10.18653/v1/d17-1105" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="992" to="1003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Doubly-Attentive Decoder for Multi-modal Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacer</forename><surname>Calixto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Campbell</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1175</idno>
		<ptr target="https://doi.org/10.18653/v1/P17-1175" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1913" to="1924" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Latent Variable Model for Multimodal Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacer</forename><surname>Calixto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Rios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilker</forename><surname>Aziz</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p19-1642</idno>
		<ptr target="https://doi.org/10.18653/v1/p19-1642" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics</title>
		<meeting>the 57th Conference of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6392" to="6405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">StructCap: Structured Semantic Embedding for Image Captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuhai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunsheng</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3123266.3123275</idno>
		<ptr target="https://doi.org/10.1145/3123266.3123275" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Multimedia Conference</title>
		<meeting>the 2017 ACM on Multimedia Conference<address><addrLine>CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="46" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">StructCap: Structured Semantic Embedding for Image Captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuhai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunsheng</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3123266.3123275</idno>
		<ptr target="https://doi.org/10.1145/3123266.3123275" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM International Conference on Multimedia</title>
		<meeting>the 25th ACM International Conference on Multimedia<address><addrLine>CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="46" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Transfer Capsule Network for Aspect Level Sentiment Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieyun</forename><surname>Qian</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p19-1052</idno>
		<ptr target="https://doi.org/10.18653/v1/p19-1052" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics</title>
		<meeting>the 57th Conference of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="547" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Better Hypothesis Testing for Statistical Machine Translation: Controlling for Optimizer Instability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">H</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/P11-2031/" />
	</analytic>
	<monogr>
		<title level="m">The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference. Association for Computer Linguistics</title>
		<meeting><address><addrLine>Portland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="176" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An empirical study on the effectiveness of images in Multimodal Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Benoit</forename><surname>Delbrouck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stéphane</forename><surname>Dupont</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d17-1095</idno>
		<ptr target="https://doi.org/10.18653/v1/d17-1095" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="910" to="919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Modulating and attending the source image during encoding improves Multimodal Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Benoit</forename><surname>Delbrouck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stéphane</forename><surname>Dupont</surname></persName>
		</author>
		<idno>abs/1712.03449</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Multimodal compact bilinear pooling for multimodal neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Benoit</forename><surname>Delbrouck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephane</forename><surname>Dupont</surname></persName>
		</author>
		<idno>abs/1703.08084</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Meteor 1.3: Automatic Metric for Reliable Optimization and Evaluation of Machine Translation Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lavie</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/W11-2107/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Workshop on Statistical Machine Translation,. Association for Computer Linguistics</title>
		<meeting>the Sixth Workshop on Statistical Machine Translation,. Association for Computer Linguistics<address><addrLine>Edinburgh, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="85" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Findings of the Second Shared Task on Multimodal Machine Translation and Multilingual Image Description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loïc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-4718</idno>
		<ptr target="https://doi.org/10.18653/v1/W17-4718" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Conference on Machine Translation</title>
		<meeting>the Second Conference on Machine Translation<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="215" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi30K: Multilingual English-German Image Descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalil</forename><surname>Sima&amp;apos;an</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/w16-3210</idno>
		<ptr target="https://doi.org/10.18653/v1/w16-3210" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th Workshop on Vision and Language, hosted by the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 5th Workshop on Vision and Language, hosted by the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imagination Improves Multimodal Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ákos</forename><surname>Kádár</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/I17-1014/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Joint Conference on Natural Language Processing. Asian Federation of Natural Language Processing</title>
		<meeting>the Eighth International Joint Conference on Natural Language Processing. Asian Federation of Natural Language Processing<address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="130" to="141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Enhancing Visual Question Answering Using Dropout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyuan</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3240508.3240662</idno>
		<ptr target="https://doi.org/10.1145/3240508.3240662" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM International Conference on Multimedia</title>
		<meeting>the 26th ACM International Conference on Multimedia<address><addrLine>Seoul, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1002" to="1010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The MeMAD Submission to the WMT18 Multimodal Translation Task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stig-Arne</forename><surname>Grönroos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Huet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikko</forename><surname>Kurimo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorma</forename><surname>Laaksonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Mérialdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mats</forename><surname>Sjöberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umut</forename><surname>Sulubacak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphaël</forename><surname>Troncy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raúl</forename><surname>Vázquez</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/w18-6439</idno>
		<ptr target="https://doi.org/10.18653/v1/w18-6439" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation: Shared Task Papers</title>
		<meeting>the Third Conference on Machine Translation: Shared Task Papers<address><addrLine>Belgium, Brussel</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="603" to="611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Aligning Linguistic Words and Visual Semantic Units for Image Captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longteng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3343031.3350943</idno>
		<ptr target="https://doi.org/10.1145/3343031.3350943" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia</title>
		<meeting>the 27th ACM International Conference on Multimedia<address><addrLine>Nice, France</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="765" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.90</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2016.90" />
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Las Vegas, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">CUNI System for the WMT18 Multimodal Translation Task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindrich</forename><surname>Helcl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindrich</forename><surname>Libovický</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dusan</forename><surname>Varis</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/w18-6441</idno>
		<ptr target="https://doi.org/10.18653/v1/w18-6441" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation: Shared Task Papers</title>
		<meeting>the Third Conference on Machine Translation: Shared Task Papers<address><addrLine>Belgium, Brussels</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="616" to="623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Attention-based Multimodal Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Yao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederick</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sz-Rung</forename><surname>Shiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/w16-2360</idno>
		<ptr target="https://doi.org/10.18653/v1/w16-2360" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation. Association for Computational Linguistics</title>
		<meeting>the First Conference on Machine Translation. Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="639" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Distilling Translations with Visual Awareness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Ive</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranava</forename><surname>Madhyastha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p19-1653</idno>
		<ptr target="https://doi.org/10.18653/v1/p19-1653" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics</title>
		<meeting>the 57th Conference of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6525" to="6538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">CapsuleGAN: Generative Adversarial Capsule Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayush</forename><surname>Jaiswal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wael</forename><surname>Abdalmageed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Premkumar</forename><surname>Natarajan</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-11015-4_38</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-11015-4_38" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision<address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="526" to="535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">OpenNMT: Open-Source Toolkit for Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Senellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-4012</idno>
		<ptr target="https://doi.org/10.18653/v1/P17-4012" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Association for Computer Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics. Association for Computer Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="67" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Neural Network Encapsulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01252-6_16</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-01252-6_16" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision<address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="266" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Attention Strategies for Multi-Source Sequence-to-Sequence Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindrich</forename><surname>Libovický</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindrich</forename><surname>Helcl</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-2031</idno>
		<ptr target="https://doi.org/10.18653/v1/P17-2031" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="196" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Erasing-based Attention Learning for Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3343031.3350993</idno>
		<ptr target="https://doi.org/10.1145/3343031.3350993" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia, MM 2019</title>
		<meeting>the 27th ACM International Conference on Multimedia, MM 2019<address><addrLine>Nice, France; Nice, France</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019-10-21" />
			<biblScope unit="page" from="1175" to="1183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Hierarchical Question-Image Co-Attention for Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/6202-hierarchical-question-image-co-attention-for-visual-question-answering" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="289" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Multi-modal Capsule Routing for Actor and Action Video Segmentation Conditioned on Natural Language Queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruce</forename><surname>Mcintosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duarte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yogesh</forename><surname>Singh Rawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
		<idno>abs/1812.00303</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Bleu: a Method for Automatic Evaluation of Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.3115/1073083.1073135</idno>
		<ptr target="https://doi.org/10.3115/1073083.1073135" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. Association for Computer Linguistics</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics. Association for Computer Linguistics<address><addrLine>Philadelphia, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">CRA-Net: Composed Relation Attention Network for Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3343031.3350925</idno>
		<ptr target="https://doi.org/10.1145/3343031.3350925" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia</title>
		<meeting>the 27th ACM International Conference on Multimedia<address><addrLine>Nice, France</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1202" to="1210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Dynamic Routing Between Capsules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/6975-dynamic-routing-between-capsules" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3856" to="3866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Neural Machine Translation of Rare Words with Subword Units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p16-1162</idno>
		<ptr target="https://doi.org/10.18653/v1/p16-1162" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. Association for Computer Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics. Association for Computer Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Dual Directed Capsule Network for Very Low Resolution Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maneet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shruti</forename><surname>Nagpal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richa</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayank</forename><surname>Vatsa</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2019.00043</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2019.00043" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<meeting><address><addrLine>Seoul, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="340" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unpaired Cross-lingual Image Caption Generation with Self-Supervised Rewards</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhe</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yida</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Jin</surname></persName>
		</author>
		<idno type="DOI">10.1145/3343031.3350996</idno>
		<ptr target="https://doi.org/10.1145/3343031.3350996" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia</title>
		<meeting>the 27th ACM International Conference on Multimedia<address><addrLine>Nice, France</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="784" to="792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A Shared Task on Multimodal Machine Translation and Crosslingual Image Description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalil</forename><surname>Sima&amp;apos;an</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W16-2346</idno>
		<ptr target="https://doi.org/10.18653/v1/W16-2346" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation</title>
		<meeting>the First Conference on Machine Translation<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="543" to="553" />
		</imprint>
	</monogr>
	<note>Shared Task Papers</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=2670313" />
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Variational Recurrent Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaojie</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianpei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biao</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16791" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence<address><addrLine>New Orleans, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5488" to="5495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Exploring Discriminative Word-Level Domain Contexts for Multi-domain Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiali</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huating</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjing</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2019.2954406</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2019.2954406" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A Hierarchy-to-Sequence Attentional Neural Machine Translation Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiali</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xie</surname></persName>
		</author>
		<idno type="DOI">10.1109/TASLP.2018.2789721</idno>
		<ptr target="https://doi.org/10.1109/TASLP.2018.2789721" />
	</analytic>
	<monogr>
		<title level="j">IEEE ACM Trans. Audio Speech Lang. Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="623" to="632" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Exploiting reverse target-side contexts for neural machine translation via asynchronous bidirectional decoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junfeng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.artint.2019.103168</idno>
		<ptr target="https://doi.org/10.1016/j.artint.2019.103168" />
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">277</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Attention is All you Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/7181-attention-is-all-you-need" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Exploiting Cross-Sentence Context for Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Way</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d17-1301</idno>
		<ptr target="https://doi.org/10.18653/v1/d17-1301" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<editor>Martha Palmer, Rebecca Hwa, and Sebastian Riedel</editor>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2826" to="2831" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Towards Linear Time Neural Machine Translation with Capsule Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1074</idno>
		<ptr target="https://doi.org/10.18653/v1/D19-1074" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="803" to="812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Faithful Multimodal Explanation for Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-4812</idno>
		<ptr target="https://doi.org/10.18653/v1/W19-4812" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP. Association for Computational Linguistics</title>
		<meeting>the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP. Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="103" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Are You Talking to Me? Reasoned Visual Dialog Generation Through Adversarial Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00639</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2018.00639" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6106" to="6115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">MS-CapsNet: A Novel Multi-Scale Capsule Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canqun</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbin</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.1109/LSP.2018.2873892</idno>
		<ptr target="https://doi.org/10.1109/LSP.2018.2873892" />
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1850" to="1854" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Capsule Graph Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Xinyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihui</forename><surname>Chen</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Byl8BnRcYm" />
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Investigating Capsule Networks with Dynamic Routing for Text Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyang</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soufei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d18-1350</idno>
		<ptr target="https://doi.org/10.18653/v1/d18-1350" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3110" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Enhancing Context Modeling with a Query-Guided Capsule Network for Document-level Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinchao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fandong</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1164</idno>
		<ptr target="https://doi.org/10.18653/v1/D19-1164" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1527" to="1537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A Novel Graph-based Multi-modal Fusion Encoder for Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjing</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fandong</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chulun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/2020.acl-main.273/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3025" to="3035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Multi-Domain Neural Machine Translation with Word-Level Domain Context Discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiali</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huating</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjing</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiang</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d18-1041</idno>
		<ptr target="https://doi.org/10.18653/v1/d18-1041" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="447" to="457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Variational Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d16-1050</idno>
		<ptr target="https://doi.org/10.18653/v1/d16-1050" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="521" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Improving the Transformer Translation Model with Document-Level Context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feifei</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d18-1049</idno>
		<ptr target="https://doi.org/10.18653/v1/d18-1049" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="533" to="542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Asynchronous Bidirectional Decoding for Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongji</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16784" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<editor>Sheila A. McIlraith and Kilian Q. Weinberger</editor>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence<address><addrLine>New Orleans, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5698" to="5705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Dynamic Past and Future for Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaixiang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin-Yu</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1086</idno>
		<ptr target="https://doi.org/10.18653/v1/D19-1086" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="931" to="941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">A Visual Attention Grounding Neural Model for Multimodal Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runxiang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d18-1400</idno>
		<ptr target="https://doi.org/10.18653/v1/d18-1400" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3643" to="3653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">More Than An Answer: Neural Pivot Network for Visual Qestion Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunsheng</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3123266.3123335</idno>
		<ptr target="https://doi.org/10.1145/3123266.3123335" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Multimedia Conference</title>
		<meeting>the 2017 ACM on Multimedia Conference<address><addrLine>CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="681" to="689" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
