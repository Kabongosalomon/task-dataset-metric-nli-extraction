<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Wider or Deeper: Revisiting the ResNet Model for Visual Recognition *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zifeng</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<postCode>5005</postCode>
									<settlement>Adelaide</settlement>
									<region>SA</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<postCode>5005</postCode>
									<settlement>Adelaide</settlement>
									<region>SA</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den Hengel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<postCode>5005</postCode>
									<settlement>Adelaide</settlement>
									<region>SA</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Wider or Deeper: Revisiting the ResNet Model for Visual Recognition *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The trend towards increasingly deep neural networks has been driven by a general observation that increasing depth increases the performance of a network. Recently, however, evidence has been amassing that simply increasing depth may not be the best way to increase performance, particularly given other limitations. Investigations into deep residual networks have also suggested that they may not in fact be operating as a single deep network, but rather as an ensemble of many relatively shallow networks. We examine these issues, and in doing so arrive at a new interpretation of the unravelled view of deep residual networks which explains some of the behaviours that have been observed experimentally. As a result, we are able to derive a new, shallower, architecture of residual networks which significantly outperforms much deeper models such as ResNet-200 on the ImageNet classification dataset. We also show that this performance is transferable to other problem domains by developing a semantic segmentation approach which outperforms the state-of-the-art by a remarkable margin on datasets including PASCAL VOC, PASCAL Context, and Cityscapes. The architecture that we propose thus outperforms its comparators, including very deep ResNets, and yet is more efficient in memory use and sometimes also in training time. The code and models are available at https://github.com/itijyou/ademxapp.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The convolutional networks used by the computer vision community have been growing deeper and deeper each year since Krizhevsky et al. <ref type="bibr" target="#b15">[16]</ref> proposed AlexNet in 2012. The deepest network <ref type="bibr" target="#b11">[12]</ref> in the literature is a residual network (ResNet) with 1,202 trainable layers, which was trained using the tiny images in the CIFAR-10 dataset <ref type="bibr" target="#b14">[15]</ref>. The image size here is important, because it means that the size of corresponding feature maps is relatively small, which is critical in training extremely deep models. Most networks operating on more practically interesting image sizes tend to have the order of one, to two, hundred layers, e.g. the 200-layer ResNet <ref type="bibr" target="#b12">[13]</ref> and 96-layer Inception-ResNet <ref type="bibr" target="#b29">[30]</ref>. The progression to deeper networks continues, however, with Zhao et al. <ref type="bibr" target="#b36">[37]</ref> having trained a 269-layer network for semantic image segmentation. These networks were trained using the ImageNet classification dataset <ref type="bibr" target="#b26">[27]</ref>, where the images are of much higher resolution. Each additional layer requires not only additional memory, but also additional training. The marginal gains achieved by each additional layer diminish with depth, however, to the point where Zhao et al. <ref type="bibr" target="#b36">[37]</ref> achieved only an improvement of 1.1% (from 42.2% to 43.3% by mean intersectionover-union scores) after almost doubling the number of layers (from 152 to 269). On the other hand, Zagoruyko and Komodakis showed that it is possible to train much shallower but wider networks on CIFAR-10, which outperform a ResNet <ref type="bibr" target="#b11">[12]</ref> with its more than one thousand layers. The question thus naturally arises as to whether deep, or wide, is the right strategy.</p><p>In order to examine the issue we first need to understand the mechanism behind ResNets. Veit et al. <ref type="bibr" target="#b30">[31]</ref> have claimed that they actually behave as exponential ensembles of relatively shallow networks. However, there is a gap between their proposed unravelled view of a ResNet, and a real exponential ensemble of sub-networks, as illustrated in the top row of <ref type="figure" target="#fig_0">Fig. 1</ref>. Since the residual units are nonlinear, we cannot further split the bottom path into two subnetworks, i.e., M c and M d . It turns out that ResNets are only assembling linearly growing numbers of sub-networks. Besides, the key characteristic of our introduced view is that it depends on the effective depth l of a network. This l amounts to the number of residual units which backward gradients during training can go through. When l ≥ 2, the two-unit ResNet in <ref type="figure" target="#fig_0">Fig. 1</ref> can be seen as an ensemble of three sub-networks, i.e., M a , M b , and M 2 e , as shown in the bottom left. When l = 1, nothing changes except that we replace the third sub-network with a shallower one M 1 e , as shown in the bottom right example. The superscripts in M 1 e and M 2 e denote their actual depths. About the unravelled view, the effective depth of a ResNet, and the actual depth of a sub-network, more details will be provided in the sequence. It is also worth noting that Veit et al. <ref type="bibr" target="#b30">[31]</ref> empir-ically found that most gradients in a 110-layer ResNet can only go through up to seventeen residual units, which supports our above hypothesis that the effective depth l exists for a specific network.</p><p>In this paper, our contributions include:</p><p>• We introduce a further developed intuitive view of ResNets, which helps us understand their behaviours, and find possible directions to further improvements.</p><p>• We propose a group of relatively shallow convolutional networks based on our new understanding. Some of them achieve the state-of-the-art results on the Ima-geNet classification dataset <ref type="bibr" target="#b26">[27]</ref>.</p><p>• We evaluate the impact of using different networks on the performance of semantic image segmentation, and show these networks, as pre-trained features, can boost existing algorithms a lot. We achieve the best results on PASCAL VOC <ref type="bibr" target="#b7">[8]</ref>, PASCAL Context <ref type="bibr" target="#b23">[24]</ref>, and Cityscapes <ref type="bibr" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Our work here is closely related to two topics, residual network (ResNet) based image classification and semantic image segmentation using fully convolutional networks .</p><p>As we have noted above, He et al. <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref> recently proposed the ResNets to combat the vanishing gradient problem during training very deep convolutional networks. ResNets have outperformed previous models at a variety of tasks, such as object detection <ref type="bibr" target="#b6">[7]</ref> and semantic image segmentation <ref type="bibr" target="#b2">[3]</ref>. They are gradually replacing VGGNets <ref type="bibr" target="#b27">[28]</ref> in the computer vision community, as the standard feature extractors. Nevertheless, the real mechanism underpinning the effectiveness of ResNets is not yet clear. Veit et al. <ref type="bibr" target="#b30">[31]</ref> claimed that they behave like exponential ensembles of relatively shallow networks, yet the 'exponential' nature of the ensembles has yet to be theoretically verified. Residual units are usually non-linear, which prevents a ResNet from exponentially expanding into separated sub-networks, as illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>. It is also unclear as to whether a residual structure is required to train very deep networks. For example, Szegedy et al. <ref type="bibr" target="#b29">[30]</ref> showed that it is 'not very difficult' to train competitively deep networks, even without residual shortcuts. Currently, the most clear advantage of ResNets is in their fast convergence <ref type="bibr" target="#b11">[12]</ref>. Szegedy et al. <ref type="bibr" target="#b29">[30]</ref> observed similar empirically results to support that. On the other hand, Zagoruyko and Komodakis <ref type="bibr" target="#b35">[36]</ref> found that a wide sixteen-layer ResNet outperformed the original thin thousand-layer ResNet <ref type="bibr" target="#b12">[13]</ref> on datasets composed of tiny images such as CIFAR-10 <ref type="bibr" target="#b14">[15]</ref>. The analysis we present here is motivated by their empirical testing, but aims at a more theoretical approach, and the observation that a grid . This shows that f2(·) never operates independently of the result of f1(·), and thus that the number of independent classifiers increases linearly with the number of residual units. Analysing the interactions between residual units at a given effective depth, here labelled l, illuminates the paths taken by gradients during training.</p><formula xml:id="formula_0">f 1 f 2 Shortcut Residual unit f 1 f 2 f 1 f 2 f 2 f 1 f 1 = 2 = 1 f 1 f 2 f 1 f 1 f 2 f 1 a b c d a</formula><p>search of configuration space is impractical on large scale datasets such as the ImageNet classification dataset <ref type="bibr" target="#b26">[27]</ref>.</p><p>Semantic image segmentation amounts to predicting the categories for each pixel in an image. Long et al. <ref type="bibr" target="#b21">[22]</ref> proposed the fully convolutional networks (FCN) to this end. FCNs soon became the mainstream approach to dense prediction based tasks, especially due to its efficiency. Besides, empirical results in the literature <ref type="bibr" target="#b2">[3]</ref> showed that stronger pre-trained features can yet further improve their performance. We thus here base our semantic image segmentation approach on fully convolutional networks, and will show the impact of different pre-trained features on final segmentation results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Residual networks revisited</head><p>We are concerned here with the full pre-activation version of residual networks (ResNet) <ref type="bibr" target="#b12">[13]</ref>. For shortcut connections, we consider identity mappings <ref type="bibr" target="#b12">[13]</ref> only. We omit the raw input and the top-most linear classifier for clarity. Usually, there may be a stem block <ref type="bibr" target="#b29">[30]</ref> or several traditional convolution layers <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref> directly after the raw input. We omit these also, for the purpose of simplicity.</p><p>For the residual Unit i, let y i−1 be the input, and let f i (·) be its trainable non-linear mappings, also named Block i. The output of Unit i is recursively defined as:</p><formula xml:id="formula_1">y i ≡ f i (y i−1 , w i ) + y i−1 ,<label>(1)</label></formula><p>where w i denotes the trainable parameters, and f i (·) is often two or three stacked convolution stages. In the full preactivation version, the components of a stage are in turn a batch normalization <ref type="bibr" target="#b13">[14]</ref>, a rectified linear unit <ref type="bibr" target="#b24">[25]</ref> (ReLU) non-linearity, and a convolution layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Residual networks unravelled online</head><p>Applying Eqn.(1) in one substitution step, we expand the forward pass into:</p><formula xml:id="formula_2">y 2 = y 1 +f 2 (y 1 ,w 2 )</formula><p>(2) = y 0 +f 1 (y 0 ,w 1 )+f 2 (y 0 +f 1 (y 0 ,w 1 ),w 2 )</p><p>(3) = y 0 +f 1 (y 0 ,w 1 )+f 2 (y 0 ,w 2 )+f 2 (f 1 (y 0 ,w 1 ),w 2 ), <ref type="bibr" target="#b3">(4)</ref> which describes the unravelled view by Veit et al. <ref type="bibr" target="#b30">[31]</ref>, as shown in the top row of <ref type="figure" target="#fig_0">Fig. 1</ref>. Since f 2 (·) is non-linear, we cannot derive Eqn.(4) from Eqn. <ref type="bibr" target="#b2">(3)</ref>. So the whole network is not equivalent to an exponentially growing ensemble of sub-networks. It is rather, more accurately, described as a linearly growing ensemble of sub-networks. For the twounit ResNet as illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, there are three, e.g., M a , M b , and M 2 e , sub-networks respectively corresponding to the three terms in Eqn.(3), i.e., y 0 , f 1 (y 0 , w 1 ), and</p><formula xml:id="formula_3">f 2 (y 0 + f 1 (y 0 , w 1 ), w 2 ).</formula><p>Veit et al. in <ref type="bibr" target="#b30">[31]</ref> showed that the paths which gradients take through a ResNet are typically far shorter than the total depth of that network. They thus introduced the idea of effective depth as a measure for the true length of these paths. By characterising the units of a ResNet given its effective depth, we illuminate the impact of varying paths that gradients actually take, as in <ref type="figure" target="#fig_0">Fig. 1</ref>. We here illustrate this impact in terms of small effective depths, because to do so for larger ones would require diagrams of enormous networks. The impact is the same, however.</p><p>Take the ResNet in <ref type="figure" target="#fig_0">Fig. 1</ref> for example again. In an SGD iteration, the backward gradients are:</p><formula xml:id="formula_4">∆w 2 = df 2 dw 2 · ∆y 2 (5) ∆y 1 = ∆y 2 + f 2 · ∆y 2 (6) ∆w 1 = df 1 dw 1 · ∆y 2 + df 1 dw 1 · f 2 · ∆y 2 ,<label>(7)</label></formula><p>where f 2 denotes the derivative of f 2 (·) to its input y 1 . When effective depth l ≥ 2, both terms in Eqn. <ref type="bibr" target="#b6">(7)</ref> are nonzeros, which corresponds to the bottom-left case in <ref type="figure" target="#fig_0">Fig. 1</ref>. Namely, Block 1 receives gradients from both M b and M 2 e . However, when effective depth l = 1, the gradient ∆y 2 vanishes after passing through Block 2. Namely, f 2 · ∆y 2 → 0. So, the second term in Eqn. <ref type="bibr" target="#b6">(7)</ref> also goes to zeros, which is illustrated by the bottom-right case in <ref type="figure" target="#fig_0">Fig. 1</ref>. The weights in Block 1 indeed vary across different iterations, but they are updated only by M b . To M 1 e , Block 1 is no more than an additional input providing preprocessed representations, because Block 1 is not end-to-end trained, from the point of view of M 1 e . In this case, we name M 1 e to have an actual depth of one. We say that the ResNet is over-deepened, and that it cannot be trained in a fully end-to-end manner, even with those shortcut connections.</p><p>Let d be the total number of residual units. We can see a ResNet as an ensemble of different sub-networks, i.e.,</p><formula xml:id="formula_5">M i , i = {0, 1, · · · , d}. The actual depth of M i is min(i, l).</formula><p>We show an unravelled three-unit ResNet with different effective depths in <ref type="figure" target="#fig_1">Fig. 2</ref>. By way of example, note that M 1 in <ref type="figure" target="#fig_1">Fig. 2</ref> contains only Block 1, whereas M 2 contains both Block 1 and Block 2. Among the cases illustrated, the bottom left example is more complicated, where d = 3 and l = 2. From the point of view of M 2 3 , the gradient of Block 1 is f 3 · ∆y 3 + f 2 · f 3 · ∆y 3 , where the first term is non-zero. M 2 3 will thus update Block 1 at each iteration. Considering the non-linearity in Block 3, it is non-trivial to tell if this is as good as the fully end-to-end training case, as illustrated by M 3 in the top right example. An investigation of this issue remains future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Residual networks behaviours revisited</head><p>Very deep ResNets. Conventionally, it is not easy to train very deep networks due to the vanishing gradient problem <ref type="bibr" target="#b0">[1]</ref>. To understand how a very deep ResNet is trained, the observation by Veit et al. <ref type="bibr" target="#b30">[31]</ref> is important, i.e., gradients vanish exponentially as the length of paths increases. Now refer to the top-right example in <ref type="figure" target="#fig_1">Fig. 2</ref>. This is somewhat similar to the case of a shallow or reasonably deep ResNet, when d ≤ l. At the beginning, the shallowest subnetwork, i.e., M 1 , converges fast, because it gives Block 1 the largest gradients. From the point of view of M 2 , Block 2 may also receive large gradients due to the path with a length of one. However, the input of Block 2 partly depends on Block 1. It would not be easy for Block 2 to converge before the output of Block 1 stabilises. Similarly, Block 3 will need to wait for Blocks 1 and 2, and so forth. In this way, a ResNet seems like an ensemble with a growing number of sub-networks. Besides, each newly added sub-network will have a larger actual depth than all the previous ones. Note that Littwin and Wolf <ref type="bibr" target="#b19">[20]</ref>, in a concurrent work, have theoretically showed that ResNets are virtual ensembles whose depth grows as training progresses. Their result to some extent coincides with the above described process.</p><formula xml:id="formula_6">f 3 f 1 f 2 f 1 f 2 f 1 f 3 f 1 f 2 f 1 f 1 f 2 f 1 f 3 f 1 f 2 f 1 f 1 f 2 f 1 f 3 f 1 f 2 f 1 ≥ 3 = 2 = 3 =</formula><p>The story will however be different when the actual depth becomes as large as the effective depth. Refer to the bottom-right example in <ref type="figure" target="#fig_1">Fig. 2</ref>. This is somewhat similar to the case of an over-deepened ResNet, when d is much larger than l. Again, Block 1 in M 1 gets trained and stabilises first. However, this time M 1 2 is not fully end-to-end trained any more. Since M <ref type="bibr">1 2</ref> gives no gradients to Block 1, it becomes a one-block sub-network trained on top of some preprocessed representations, which are obtained by adding the output of Block 1 up to the original input. In this way, the newly added sub-network M 1 2 still has an actual depth of one, which is no deeper than the previous one, i.e., M 1 , and so forth for M <ref type="bibr">1 3</ref> . ResNets thus avoid the vanishing gradient problem by reshaping themselves into multiple shallower sub-networks. This is just another view of delivering gradients to bottom layers through shortcut connections. Researchers <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b29">30]</ref> have claimed that the residual shortcut connections are not necessary even in very deep networks. However, there are usually short paths in their proposed networks as well. For example, the 76-layer Inception-v4 network <ref type="bibr" target="#b29">[30]</ref> has a much shorter twenty-layer route from its input to the output. There might be differences in the details <ref type="bibr" target="#b31">[32]</ref> between fusion by concatenation (Inception-v4) and fusion by summation (ResNets). However, the manner of avoiding the vanishing gradient problem is probably similar, i.e., using shortcuts, either with trainable weights or not. We are thus not yet in a position to be able to claim that the vanishing gradient problem has been solved.</p><p>Wide ResNets. Conventionally, wide layers are more prone to over-fitting, and sometimes require extra regularization such as dropout <ref type="bibr" target="#b28">[29]</ref>. However, Zagoruyko and Komodakis <ref type="bibr" target="#b35">[36]</ref> showed the possibility to effectively train times wider ResNets, even without any extra regularization. To understand how a wide ResNet is trained, refer to the top right example in <ref type="figure" target="#fig_1">Fig. 2</ref>. This simulates the case of a rather shallow network, when d is smaller than l. We reuse the weights of Block 1 for four times. Among these, Block 1 is located in three different kinds of circumstances. In the bottom-most path of the sub-network M 3 , it is supposed to learn some low-level features; in M 2 , it should learn both low-level and mid-level features; and in M 1 , it has to learn everything. This format of weight sharing may suppress over-fitting, especially for those units far from the top-most linear classifier. Hence ResNets inherently introduce regularization by weight sharing among multiple very different sub-networks.</p><p>Residual unit choices. For better performance, we hope that a ResNet should expand into a sufficiently large number of sub-networks, some of which should have large model capacity. So, given our previous observations, the requirements for an ideal mapping function in a residual unit are, 1) being strong enough to converge even if it is reused in many sub-networks, and 2) being shallow enough to enable an large effective depth. Since it is very hard to build a model with large capacity using a single trainable layer <ref type="bibr" target="#b12">[13]</ref>, the most natural choice would be a residual unit with two wide convolution stages. This coincides with empirical results reported by Zagoruyko and Komodakis <ref type="bibr" target="#b35">[36]</ref>. They found that, among the most trivial structure choices, the best one is to stack two 3 × 3 convolution stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Wider or deeper?</head><p>To summarize the previous subsections, shortcut connections enable us to train wider and deeper networks. As they growing to some point, we will face the dilemma between width and depth. From that point, going deep, we will actually get a wider network, with extra features which are not completely end-to-end trained; going wider, we will literally get a wider network, without changing its end-to-end characteristic. We have learned the strength of depth from the previous plain deep networks without any shortcuts, e.g., the AlexNet <ref type="bibr" target="#b15">[16]</ref> and VGGNets <ref type="bibr" target="#b27">[28]</ref>. However, it is not clear whether those extra features in very deep residual networks can perform as well as conventional fully end-to-end trained features. So in this paper, we only favour a deeper model, when it can be completely end-to-end trained.</p><p>In practice, algorithms are often limited by their spatial costs. One way is to use more devices, which will however increase communication costs among them. With similar memory costs, a shallower but wider network can have times more number of trainable parameters. There-fore, given the following observations in the literature,</p><p>• Zagoruyko and Komodakis <ref type="bibr" target="#b35">[36]</ref> found that the performance of a ResNet was related to the number of trainable parameters. Szegedy et al. <ref type="bibr" target="#b29">[30]</ref> came to a similar conclusion, according to the comparison between their proposed Inception networks.</p><p>• Veit et al. <ref type="bibr" target="#b30">[31]</ref> found that there is a relatively small effective depth for a very deep ResNet, e.g., seventeen residual units for a 110-layer ResNet.</p><p>most of the current state-of-the-art models on the ImageNet classification dataset <ref type="bibr" target="#b26">[27]</ref> seem over-deepened, e.g., the 200-layer ResNet <ref type="bibr" target="#b12">[13]</ref> and 96-layer Inception-ResNet <ref type="bibr" target="#b29">[30]</ref>.</p><p>The reason is that, to effectively utilize GPU memories, we should make a model shallow. According to our previous analysis, paths longer than the effective depth in ResNets are not trained in a fully end-to-end manner. Thus, we can remove most of these paths by directly reducing the number of residual units. For example, in our best performing network, there are exactly seventeen residual units.</p><p>With empirical results, we will show that our fully endto-end networks can perform much better than the previous much deeper ResNets, especially as feature extractors. However, even if a rather shallow network (eight-unit, or twenty-layer) can outperform ResNet-152 on the ImageNet classification dataset, we will not go that shallow, because an appropriate depth is vital to train good features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Approach to image classification</head><p>We show the proposed networks in <ref type="figure" target="#fig_2">Fig. 3</ref>. There are three architectures, with different input sizes. Dashed blue rectangles to denote convolution stages, which are respectively composed of a batch normalization, an ReLU nonlinearity and a convolution layer, following the second version of ResNets <ref type="bibr" target="#b12">[13]</ref>. The closely stacked two or three convolution stages denote different kinds of residual units (B1-B7), with inner shortcut connections <ref type="bibr" target="#b12">[13]</ref>. Each kind corresponds to a level, where all units share the same kernel sizes and numbers of channels, as given in the dashed black rectangles in the left-most column of <ref type="figure" target="#fig_2">Fig. 3</ref>. As mentioned before, there are two 3×3 convolution layers in most residual units (B1-B5). However, in B6 and B7, we use bottleneck structures as in ResNets <ref type="bibr" target="#b11">[12]</ref>, except that we adjust the numbers of channels to avoid drastic changes in width. Each of our networks usually consists of one B6, one B7, and different numbers of B1-B5. For those with a 224×224 input, we do not use B1 due to limited GPU memories. Each of the green triangles denotes a down-sampling operation with a rate of two, which is clear given the feature map sizes of different convolution stages (in dashed blue rectangles). To this end, we can let the first convolution layer at according levels have a stride of two. Or, we can use an extra spatial pooling layer, whose kernel size is three and stride is two. In a network whose classification results are reported in this paper, we always use pooling layers for down-sampling. We average the top-most feature maps into 4,096-dimensional final features, which matches the cases of AlexNet <ref type="bibr" target="#b15">[16]</ref> and VGGNets <ref type="bibr" target="#b27">[28]</ref>. We will show more details about network structures in Subsection 6.1. Implementation details. We run all experiments using the MXNet framework <ref type="bibr" target="#b3">[4]</ref>, with four devices (two K80 or four Maxwell Titan X cards) on a single node. We follow settings in the re-implementation of ResNets by Gross and Wilber <ref type="bibr" target="#b9">[10]</ref> as possible. But, we use a linear learning rate schedule, which was reported as a better choice by Mishkin et al. <ref type="bibr" target="#b22">[23]</ref>. Take Model A in <ref type="table">Table 1</ref> for example. We start from 0.1, and linearly reduce the learning rate to 10 −6 within 450k iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Approach to semantic image segmentation</head><p>Our approach is similar to the fully convolutional networks (FCN) <ref type="bibr" target="#b21">[22]</ref> implemented in the first version of DeepLab <ref type="bibr" target="#b1">[2]</ref>. However, without getting too many factors entangled, we in this paper do not introduce any multiscale structures <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b2">3]</ref>, deep supervision signals <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b36">37]</ref>, or global context features <ref type="bibr" target="#b36">[37]</ref>. Besides, we do not apply any multi-scale testing, model averaging or CRF based post-processing, except for the test set of ADE20K <ref type="bibr" target="#b39">[40]</ref>.</p><p>Given a pre-trained network, there are three steps to reshape it into a network suitable for semantic image segmentation, as stated below. 1) Resolution. To generate score maps at 1/8 resolution, we remove down-sampling operations and increase dilation rates accordingly in some convolution layers. For clarity, first suppose that we always down-sample features maps using a convolution layer with a stride of two. Take networks with 224×224 inputs for example. We set stride of the first convolution layer in B5 to one, and increase the dilation rate from one to two for the following layers; We do the same thing to the first convolution layer in B6 too, and increase the dilation rate from two to four for the following layers. In the case of down-sampling using a pooling layer, everything is the same except that we set stride of that pooling layer to one. Sometimes, we will have to apply a pooling layer with dilation <ref type="bibr" target="#b32">[33]</ref>. On the other hand, we do not make any change for networks with 56×56 inputs, since there are only three down-sampling operations in each of them.</p><p>It is notable that all down-sampling operations are implemented using spatial pooling layers in our originally pretrained networks. We find it harmful for FCNs in our preliminary experiments, probably due to too strong spatial invariance. To this end, we replace several top-most downsampling operations in a network, and then tune it for some additional iterations. Take Model A in <ref type="table">Table 1</ref> for example again. We remove the top-most three pooling layers (before B4, B5 and B6), increase the strides of according convolution layers up to two, and tune it for 45k iterations using the ImageNet dataset <ref type="bibr" target="#b26">[27]</ref>, starting from a learning rate of 0.01.</p><p>2) Classifier. We remove the top-most linear classifier and the global pooling layer, and then consider two cases. For one thing, we follow a basic large field-of-view setting in DeepLab-v2 <ref type="bibr" target="#b2">[3]</ref>, called '1 convolution'. Namely, we just add back a single linear layer as the new classifier. For anther, we insert an additional non-linear convolution stage (without batch normalization) below the linear classifier. This case is called '2 convolutions'. Both of the added layers have 3×3 kernels, with a dilation rate of twelve. The top-most two-layer classifier thus has a receptive field of 392×392 on the final feature maps. By default, we let the number of channels in the hidden layer be 512.</p><p>3) Dropout. To alleviate over-fitting, we also apply the traditional dropout <ref type="bibr" target="#b28">[29]</ref> to very wide residual units. The dropout rate is 0.3 for those with 2,048 channels, e.g., the last three units in ResNets and the second last units (B6) in our networks; while 0.5 for those with 4,096 channels, e.g., the top-most units (B7) in our networks.</p><p>Implementation details. We fix the moving means and variations in batch normalization layers during finetuning <ref type="bibr" target="#b11">[12]</ref>. We use four devices on a single node. The batch size is sixteen, so there are four examples per device. We first tune each network for a number of iterations, keeping the learning rate unchanged at 0.0016. And then, we reduce the learning rate gradually during another number of iterations, following a linear schedule <ref type="bibr" target="#b22">[23]</ref>. For datasets with available testing sets, we evaluate these numbers of iterations on validation sets. During training, we first re-size an image by a ratio randomly sampled from [0.7, 1.3], and then generate a sample by cropping one 500×500 subwindow at a randomly selected location.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experimental results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Image classification results</head><p>We evaluate our proposed networks 1 on the ILSVRC 2012 classification dataset <ref type="bibr" target="#b26">[27]</ref>, with 1.28 million images for training, respectively belonging to 1,000 categories. We report top-1 and top-5 error rates on the validation set. We compare various networks in <ref type="table">Table 1</ref>, where we obtain all the results by testing on a single crop. However, we list the ten-crop result for VGG16 <ref type="bibr" target="#b27">[28]</ref> since it is not inherently a fully convolutional network. For networks trained with 224×224 inputs, the testing crop size is 320×320, following the setting used by He et al. <ref type="bibr" target="#b12">[13]</ref>. For those with 112×112 and 56×56 inputs, we use 160×160 and 80×80 crops respectively. For Inception networks <ref type="bibr" target="#b29">[30]</ref>, the testing crop size is 299×299 <ref type="bibr" target="#b12">[13]</ref>. The names of our proposed networks are composed of training crop sizes and the numbers of residual units on different levels. Take 56-1-1-1-1-9-1-1 for example. Its input size is 56, and there are only one unit on all levels except for Level 5 (B5 in <ref type="figure" target="#fig_2">Fig. 3)</ref>.</p><p>Notable points about the results are as follows. 1) Relatively shallow networks can outperform very deep ones, which is probably due to large model capacity, coinciding with the results reported by Zagoruyko and Komodakis <ref type="bibr" target="#b35">[36]</ref>. For example, the much shallower Model B achieves similar error rates as ResNet-152, and even runs slightly faster. And particularly, Model A performs the best among all the networks.</p><p>2) We can trade performance for efficiency by using a small input size. For example, Model D performs slightly worse than ResNet-152, but is almost two times faster. This may be useful when efficiency is strictly required. Mishkin et al. <ref type="bibr" target="#b22">[23]</ref> also reduced the input size for efficiency. However, they did not remove down-sampling operations accordingly to preserve the size of final feature maps, which resulted in much degraded performance.</p><p>3) Models C, D and E perform comparably, even though Model C has larger depth and more parameters. This comparison shows the importance of designing a network properly. In these models, we put too many layers on low resolution levels (7×7, B5 in <ref type="figure" target="#fig_2">Fig. 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Semantic image segmentation results</head><p>We evaluate our proposed networks on four widely used datasets. When available, we report, 1) the pixel accuracy, which is the percentage of correctly labelled pixels on a whole test set, 2) the mean pixel accuracy, which is the mean of class-wise pixel accuracies, and 3) the mean IoU <ref type="table">Table 1</ref>. Comparison of networks by top-1 (%) and top-5 (%) errors on the ILSVRC 2012 validation set <ref type="bibr" target="#b26">[27]</ref> with 50k images, obtained using a single crop. Testing speeds (images/second) are evaluated with ten images/mini-batch using cuDNN 4 on a GTX 980 card. Input sizes during training are also listed. Note that a smaller size often leads to faster training speed. score, which is the mean of class-wise intersection-overunion scores.</p><p>PASCAL VOC 2012 <ref type="bibr" target="#b7">[8]</ref>. This dataset consists of daily life photos. There are 1,464 labelled images for training and another 1,449 for validation. Pixels either belong to the background or twenty object categories, including bus, car, cat, sofa, monitor, etc. Following the common criteria in the literature <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b1">2]</ref>, we augment the dataset with extra labelled images from the semantic boundaries dataset <ref type="bibr" target="#b10">[11]</ref>. So in total, there are 10,582 images for training.</p><p>We first compare different networks in <ref type="table">Table 2</ref>. Notable points about the results are as follows.</p><p>1) We cannot make statistically significant improvement by using ResNet-152 instead of ResNet-101. However, Model A performs better than ResNet-152 by 3.4%. Using one hidden layer leads to a further improvement by 2.1%.</p><p>2) The very deep ResNet-152 uses too many memories due to intentionally enlarged depth. With our settings, it even cannot be tuned using many mainstream GPUs with only 12GB memories.</p><p>3) Model B performs worse than ResNet-101, even if it performs better on the classification task as shown in Table 1. This shows that it is not reliable to tell a good feature extractor only depending on its classification performance. And it again shows why we should favour deeper models. 4) Model A2 performs worse than Model A on this dataset. We initialize it using weights from Model A, and tune it with the Places 365 data <ref type="bibr" target="#b38">[39]</ref> for 45k iterations. This is reasonable since there are only object categories in this dataset, while Places 365 is for scene classification tasks.</p><p>We then compare our method with previous ones on the test set in <ref type="table">Table 3</ref>. Only using the augmented PASCAL VOC data for training, we achieve a mean IoU score of 82.5% 2 , which is better than the previous best one by 3.4%. This is a significant margin, considering that the gap between ResNet-based and VGGNet-based methods is 3.8%. Our method wins for seventeen out of the twenty object categories, which was the official criteria used in the PASCAL VOC challenges <ref type="bibr" target="#b7">[8]</ref>. In some works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b2">3]</ref>, models were further pre-trained using the Microsoft COCO <ref type="bibr" target="#b18">[19]</ref> data, which consists of 120k labelled images. In this case, the current best mean IoU is 79.7% reported by Chen et al. <ref type="bibr" target="#b2">[3]</ref>.</p><p>They also used multi-scale structure and CRF-based postprocessing in their submission, which we do not consider here. Nevertheless, our method outperforms theirs by 2.8%, which further shows the effectiveness of our features pretrained only using the ImageNet classification data <ref type="bibr" target="#b26">[27]</ref>.</p><p>Cityscapes <ref type="bibr" target="#b4">[5]</ref>. This dataset consists of street scene photos taken by car-carried cameras. There are 2975 labelled images for training and another 500 for validation. Besides, there is also an extended set with 19,998 coarsely labelled images. Pixels belong to nineteen semantic classes, including road, car, pedestrian, bicycle, etc. These classes further belong to seven categories, i.e., flat, nature, object, sky, construction, human, and vehicle.</p><p>We first compare different networks in <ref type="table" target="#tab_2">Table 4</ref>. On this dataset, ResNet-152 again shows no advantage against ResNet-101. However, Model A1 outperforms ResNet-101 by 4.2% in terms of mean IoU scores, which again is a significant margin. Because there are many scene classes, models pre-trained using Places 365 <ref type="bibr" target="#b38">[39]</ref> are supposed to perform better, which coincides with our results.</p><p>We then compare our method with previous ones on the test set in <ref type="table">Table 5</ref>. The official criteria on this dataset includes two levels, i.e., class and category. Besides, there is also an instance-weighted IoU score for each of the two, which assigns high scores to those pixels of small instances.  <ref type="bibr" target="#b12">[13]</ref> uses ResNet-101 <ref type="bibr" target="#b11">[12]</ref>, while others use VGG16 <ref type="bibr" target="#b27">[28]</ref>. LRR <ref type="bibr" target="#b8">[9]</ref> also uses the coarse set for training.</p><p>Namely, this score penalizes methods ignoring small instances, which may cause fatal problems in vehicle-centric scenarios. Our method achieves a class-level IoU score of 78.4% 3 , and outperforms the previous best one by 6.6%. Furthermore, in the case of instance-weighted IoU score, our method also performs better than the previous best one by 6.4%. It is notable that these significant improvements show the strength of our pre-trained features, considering that DeepLab-v2 <ref type="bibr" target="#b2">[3]</ref> uses ResNet-101, and LRR <ref type="bibr" target="#b8">[9]</ref> uses much more data for training. ADE20K <ref type="bibr" target="#b39">[40]</ref>. This dataset consists of both indoor and outdoor images with large variations. There are 20,210 labelled images for training and another 2k for validation. Pixels belong to 150 semantic categories, including sky, house, bottle, food, toy, etc.</p><p>We first compare different networks in tion task on the ImageNet dataset. This shows that large model capacity may become more critical in complicated tasks, since there are more parameters in Model C. We then compare our method with others on the test set in <ref type="table">Table 6</ref>. The official criteria on this dataset is the average of pixel accuracies and mean IoU scores. For better performance, we apply multi-scale testing, model averaging and post-processing with CRFs. Our Model A2 performs the best among all methods using only a single pre-trained model. However, in this submission, we only managed to include two kinds of pre-trained features, i.e., Models A and C. Nevertheless, our method only performs slightly worse than the winner by a margin of 0.47%.</p><p>PASCAL Context <ref type="bibr" target="#b23">[24]</ref>. This dataset consists of images from PASCAL VOC 2010 <ref type="bibr" target="#b7">[8]</ref> with extra object and stuff labels. There are 4,998 images for training and another 5,105 for validation. Pixels either belong to the background category or 59 semantic categories, including bag, food, sign, ceiling, ground and snow. All images in this dataset are no larger than 500×500. Since the test set is not available, here we directly apply the hyper-parameters which are used on the PASCAL VOC dataset. Our method again performs the best with a clear margin by all the three kinds of scores, as shown in <ref type="table" target="#tab_3">Table 7</ref>. In particular, we improve the IoU score by 2.4% compared to the previous best method <ref type="bibr" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We have analysed the ResNet architecture, in terms of the ensemble classifiers therein and the effective depths of the residual units. On the basis of that analysis we calculated a new, more spatially efficient, and better performing architecture which actually achieves fully end-to-end training for large networks. Using this new architecture we designed a group of correspondingly shallow networks, and showed that they outperform the previous very deep residual networks not only on the ImageNet classification dataset, but also when applied to semantic image segmentation. These results show that the proposed architecture delivers better feature extraction performance than the current state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Network structures</head><p>The graph structures of Model A for the ImageNet (ILSVRC 2012) <ref type="bibr" target="#b26">[27]</ref> classification can be accessed at: https://cdn.rawgit.com/itijyou/ademxapp/master/misc/ilsvrc_model_a.pdf Model A2 for the PASCAL VOC 2012 <ref type="bibr" target="#b7">[8]</ref> segmentation can be accessed at: https://cdn.rawgit.com/itijyou/ademxapp/master/misc/voc_model_a2.pdf.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Gradients in residual networks</head><p>We show results of the experiment on gradients proposed by Veit et al. <ref type="bibr" target="#b30">[31]</ref>, with various residual networks. Namely, for a trained network with n units, we sample individual paths of a certain length k, and measure the norm of gradients that arrive at the input. Each time, we first feed a batch forward through the whole network; then during the backward pass, we randomly sample k units. For them, we only propagate gradients through their trainable mapping functions, but without their shortcut connections. For the remaining n − k units, we do the opposite, namely, only propagating gradients through their shortcut connections. We record the norm of those gradients that reach the input for varying path length k, and show the results in <ref type="figure" target="#fig_4">Fig. 4</ref>. Note the varying magnitude and maximum path length in individual figures. These are compared to the middle part of <ref type="figure">Fig. 6</ref> in <ref type="bibr" target="#b30">[31]</ref>. However, differently we further divide the computed norm of a batch by its number of examples. According to the results in <ref type="figure" target="#fig_4">Fig. 4</ref>, ResNet-110 trained on CIFAR-10, as well as ResNet-101 and ResNet-152 trained on ILSVRC 2012, generate much smaller gradients from their long paths than from their short paths. In contrast, our Model A trained on ILSVRC 2012, generates more comparable gradients from its paths with different lengths.   <ref type="figure">Figure 5</ref>. Qualitative results on the PASCAL VOC 2012 <ref type="bibr" target="#b7">[8]</ref> val set. The model was trained using the train set augmented using SBD <ref type="bibr" target="#b10">[11]</ref>. In each example, from top to bottom, there are in turn the original image, the ground-truth, the predicted label, and the difference map between the ground-truth and the predicted label. <ref type="figure">Figure 6</ref>. Qualitative results on the Cityscapes <ref type="bibr" target="#b4">[5]</ref> val set. The model was trained using the train set. In each example, from top to bottom, there are in turn the original image, the ground-truth, the predicted label, and the difference map between the ground-truth and the predicted label. <ref type="figure">Figure 7</ref>. Failure cases on the PASCAL VOC 2012 <ref type="bibr" target="#b7">[8]</ref> val set. The model was trained using the train set augmented using SBD <ref type="bibr" target="#b10">[11]</ref>. In each example, from top to bottom, there are in turn the original image, the ground-truth, the predicted label, and the difference map between the ground-truth and the predicted label. <ref type="figure">Figure 8</ref>. Failure cases on the Cityscapes <ref type="bibr" target="#b4">[5]</ref> val set. The model was trained using the train set. In each example, from top to bottom, there are in turn the original image, the ground-truth, the predicted label, and the difference map between the ground-truth and the predicted label. <ref type="figure">Figure 9</ref>. Qualitative results on the ADE20K <ref type="bibr" target="#b39">[40]</ref> val set. The model was trained using the train set. In each example, from top to bottom, there are in turn the original image, the ground-truth, the predicted label, and the difference map between the ground-truth and the predicted label. <ref type="figure" target="#fig_0">Figure 10</ref>. More qualitative results on the ADE20K <ref type="bibr" target="#b39">[40]</ref> val set. The model was trained using the train set. In each example, from top to bottom, there are in turn the original image, the ground-truth, the predicted label, and the difference map between the ground-truth and the predicted label. <ref type="figure" target="#fig_0">Figure 11</ref>. Qualitative results on the PASCAL Context <ref type="bibr" target="#b23">[24]</ref> val set. The model was trained using the train set. In each example, from top to bottom, there are in turn the original image, the ground-truth, the predicted label, and the difference map between the ground-truth and the predicted label.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>The unravelled view of a simple ResNet. The fact that f2(·) is non-linear gives rise to the inequality in the top row, as f2(a + b) = f2(a) + f2(b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>The impact of inserting an extra residual unit into a twounit ResNet, which depends on the effective depth l.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Overview of our proposed networks with different input sizes. Note that B1-B7 are respectively a residual unit.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Gradient magnitude at input given a path length k in various residual networks. See the text for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>http://host.robots.ox.ac.uk:8080/anonymous/H0KLZK.html</figDesc><table><row><cell>method</cell><cell cols="2">pixel acc.</cell><cell cols="2">mean acc.</cell><cell>mean IoU</cell></row><row><cell cols="5">results on the Cityscapes val set</cell></row><row><cell>ResNet-101, 1 conv.</cell><cell>95.49</cell><cell></cell><cell></cell><cell>81.76</cell><cell>73.63</cell></row><row><cell>ResNet-152, 1 conv.</cell><cell>95.53</cell><cell></cell><cell></cell><cell>81.61</cell><cell>73.50</cell></row><row><cell>Model A, 1 conv.</cell><cell>95.80</cell><cell></cell><cell></cell><cell>83.81</cell><cell>76.57</cell></row><row><cell>Model A2, 1 conv.</cell><cell>95.91</cell><cell></cell><cell></cell><cell>84.48</cell><cell>77.18</cell></row><row><cell>Model A2, 2 conv.</cell><cell>96.05</cell><cell></cell><cell></cell><cell>84.96</cell><cell>77.86</cell></row><row><cell cols="5">results on the ADE20K val set</cell></row><row><cell>ResNet-101, 2 conv.</cell><cell>79.07</cell><cell></cell><cell></cell><cell>48.73</cell><cell>39.40</cell></row><row><cell>ResNet-152, 2 conv.</cell><cell>79.33</cell><cell></cell><cell></cell><cell>49.55</cell><cell>39.77</cell></row><row><cell>Model E, 2 conv.</cell><cell>79.61</cell><cell></cell><cell></cell><cell>50.46</cell><cell>41.00</cell></row><row><cell>Model D, 2 conv.</cell><cell>79.87</cell><cell></cell><cell></cell><cell>51.34</cell><cell>41.91</cell></row><row><cell>Model C, 2 conv.</cell><cell>80.53</cell><cell></cell><cell></cell><cell>52.32</cell><cell>43.06</cell></row><row><cell>Model A, 2 conv.</cell><cell>80.41</cell><cell></cell><cell></cell><cell>52.86</cell><cell>42.71</cell></row><row><cell>Model A2, 2 conv.</cell><cell>81.17</cell><cell></cell><cell></cell><cell>53.84</cell><cell>43.73</cell></row><row><cell cols="6">Table 4. Comparison by semantic image segmentation scores (%)</cell></row><row><cell cols="6">on the Cityscapes val set [5] with 500 images, and the ADE20K</cell></row><row><cell cols="2">val set [40] with 2k images.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>method</cell><cell cols="5">cla. IoU cla. iIoU cat. IoU cat. iIoU</cell></row><row><cell>Dilation10 [35]</cell><cell>67.1</cell><cell cols="2">42.0</cell><cell>86.5</cell><cell>71.1</cell></row><row><cell>DeepLab-v2  *  [13]</cell><cell>70.4</cell><cell cols="2">42.6</cell><cell>86.4</cell><cell>67.7</cell></row><row><cell>Context [18]</cell><cell>71.6</cell><cell cols="2">51.7</cell><cell>87.3</cell><cell>74.1</cell></row><row><cell>LRR + [9]</cell><cell>71.8</cell><cell cols="2">47.9</cell><cell>88.4</cell><cell>73.9</cell></row><row><cell>Model A2, 2 conv.</cell><cell>78.4</cell><cell cols="2">59.1</cell><cell>90.9</cell><cell>81.1</cell></row><row><cell cols="6">Table 5. Comparison by semantic image segmentation scores (%)</cell></row><row><cell cols="6">on the Cityscapes test set [5] with 1,525 images. DeepLab-v2</cell></row></table><note>2</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>On this dataset, ResNet-152 performs slightly better than ResNet-101. However, Model A2 outperforms ResNet-152 by 4.0% in terms of mean IoU scores. Being similar with Cityscapes, this dataset has many scene categories. So, Model A2 performs slightly better than Model A. Another notable point is that, Model C takes the second place on this dataset, even if it performs worse than Model A in the image classifica-34.2 68.9 49.4 60.3 75.3 74.7 77.6 21.4 62.5 46.8 71.8 63.9 76.5 73.9 45.2 72.4 37.4 70.9 55.1 62.2 CRFasRNN [38] 87.5 39.0 79.7 64.2 68.3 87.6 80.8 84.4 30.4 78.2 60.4 80.5 77.8 83.1 80.6 59.5 82.8 47.8 78.3 67.1 72.0 DeconvNet [26] 89.9 39.3 79.7 63.9 68.2 87.4 81.2 86.1 28.5 77.0 62.0 79.0 80.3 83.6 80.2 58.8 83.4 54.3 80.[34] 91.9 48.1 93.4 69.3 75.5 94.2 87.5 92.8 36.7 86.9 65.2 89.1 90.2 86.5 87.2 64.6 90.1 59.7 85.5 72.7 79.1 Model A, 2 conv. 94.4 72.9 94.9 68.8 78.4 90.6 90.0 92.1 40.1 90.4 71.7 89.9 93.7 91.0 89.1 71.3 90.7 61.3 87.7 78.1 82.5</figDesc><table><row><cell>method</cell><cell>aero.</cell><cell>bicy.</cell><cell>bird</cell><cell>boat</cell><cell>bott.</cell><cell>bus</cell><cell>car</cell><cell>cat</cell><cell>chai.</cell><cell>cow</cell><cell>dini.</cell><cell>dog</cell><cell>hors.</cell><cell>moto.</cell><cell>pers.</cell><cell>pott.</cell><cell>shee.</cell><cell>sofa</cell><cell>trai.</cell><cell>tvmo.</cell><cell>mean</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="8">using augmented PASCAL VOC data only</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FCN-8s [22]</cell><cell cols="21">76.8 7 65.0 72.5</cell></row><row><cell>DPN [21]</cell><cell cols="21">87.7 59.4 78.4 64.9 70.3 89.3 83.5 86.1 31.7 79.9 62.6 81.9 80.0 83.5 82.3 60.5 83.2 53.4 77.9 65.0 74.1</cell></row><row><cell>Context [18]</cell><cell cols="21">90.6 37.6 80.0 67.8 74.4 92.0 85.2 86.2 39.1 81.2 58.9 83.8 83.9 84.3 84.8 62.1 83.2 58.2 80.8 72.3 75.3</cell></row><row><cell cols="14">VeryDeep  using augmented PASCAL VOC &amp; COCO data</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Context [18]</cell><cell cols="21">94.1 40.4 83.6 67.3 75.6 93.4 84.4 88.7 41.6 86.4 63.3 85.5 89.3 85.6 86.0 67.4 90.1 62.6 80.9 72.5 77.8</cell></row><row><cell cols="22">DeepLab-v2  *  [3] 92.6 60.4 91.6 63.4 76.3 95.0 88.4 92.6 32.7 88.5 67.6 89.6 92.1 87.0 87.4 63.3 88.3 60.0 86.8 74.5 79.7</cell></row><row><cell cols="22">Table 3. Comparison with previous results by mean intersection-over-union scores (%) on the PASCAL VOC test set [8] with 1,456 images.</cell></row><row><cell cols="11">Asterisked methods use ResNet-101 [12], while others use VGG16 [28].</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>method</cell><cell></cell><cell></cell><cell cols="6">ave. of pixel acc. &amp; mean IoU</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SegModel</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">53.23</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CASIA IVA</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">54.33</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">360+MCG-ICT-CAS SP</cell><cell></cell><cell></cell><cell></cell><cell cols="2">54.68</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">SenseCUSceneParsing</cell><cell></cell><cell></cell><cell></cell><cell cols="2">55.38</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ours</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">56.41</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>method</cell><cell></cell><cell cols="7">models ave. of pixel acc. &amp; mean IoU</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>NTU-SP</cell><cell></cell><cell>2</cell><cell></cell><cell></cell><cell cols="2">53.57</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SegModel</cell><cell></cell><cell>5</cell><cell></cell><cell></cell><cell cols="2">54.65</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">360+MCG-ICT-CAS SP</cell><cell>-</cell><cell></cell><cell></cell><cell cols="2">55.57</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">SenseCUSceneParsing</cell><cell>-</cell><cell></cell><cell></cell><cell cols="2">57.21</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ours</cell><cell></cell><cell>2</cell><cell></cell><cell></cell><cell cols="2">56.74</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="9">Table 6. Comparison by semantic image segmentation scores (%)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">on the ADE20K test set [40] with 3,352 images.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 7 .</head><label>7</label><figDesc>Comparison by semantic image segmentation scores (%) on the PASCAL Context val set [24] with 5,105 images.</figDesc><table><row><cell>method</cell><cell>feature</cell><cell cols="3">pixel acc. mean acc. mean IoU</cell></row><row><cell>FCN-8s [22]</cell><cell>VGG16</cell><cell>65.9</cell><cell>46.5</cell><cell>35.1</cell></row><row><cell>BoxSup [6]</cell><cell>VGG16</cell><cell>-</cell><cell>-</cell><cell>40.5</cell></row><row><cell>Context [18]</cell><cell>VGG16</cell><cell>71.5</cell><cell>53.9</cell><cell>43.3</cell></row><row><cell cols="2">VeryDeep [34] ResNet-101</cell><cell>72.9</cell><cell>54.8</cell><cell>44.5</cell></row><row><cell cols="2">DeepLab-v2 [3] ResNet-101</cell><cell>-</cell><cell>-</cell><cell>45.7</cell></row><row><cell cols="2">Model A2, 2 conv.</cell><cell>75.0</cell><cell>58.1</cell><cell>48.1</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We will release these networks soon.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://www.cityscapes-dataset.com/benchmarks</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Qualitative results</head><p>We show qualitative results of semantic image segmentation on PASCAL VOC <ref type="bibr" target="#b7">[8]</ref>, Cityscapes <ref type="bibr" target="#b4">[5]</ref>, ADE20K <ref type="bibr" target="#b39">[40]</ref>, and PASCAL Context <ref type="bibr" target="#b23">[24]</ref>, respectively in Figs. 5, 6, 9, 10 and 11, and show some failure cases in Figs 7 and 8. In a difference map, grey and black respectively denotes correctly and wrongly labelled pixels, while white denotes the officially ignored pixels during evaluation. Note that we do not apply post-processing with CRFs, which can smooth the output but is too slow in practice, especially for large images.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Representations</title>
		<meeting>Int. Conf. Learn. Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deeplab</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00915</idno>
		<title level="m">Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">MXNet: A flexible and efficient machine learning library for heterogeneous distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.01274</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The Cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">BoxSup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Instance-aware semantic segmentation via multi-task network cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The PASCAL visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Laplacian pyramid reconstruction and refinement for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.02264</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Training and investigating residual nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wilber</surname></persName>
		</author>
		<ptr target="http://torch.ch/blog/2016/02/04/resnets.html" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.05027</idno>
		<title level="m">Identity mappings in deep residual networks</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inf. Process. Syst</title>
		<meeting>Advances in Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07648</idno>
		<title level="m">Fractalnet: Ultra-deep neural networks without residuals</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Exploring context with deep structured models for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.03183</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Littwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02525</idno>
		<title level="m">The loss surface of residual networks: Ensembles and the role of batch normalization</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semantic image segmentation via deep parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sergievskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02228</idno>
		<title level="m">Systematic evaluation of CNN advances on the ImageNet</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The role of context for object detection and semantic segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
		<meeting>Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Inception-v4, Inception-Resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alemi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07261</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wilber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.06431</idno>
		<title level="m">Residual networks behave like ensembles of relatively shallow networks</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07716</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Deeply-fused nets</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07251</idno>
		<title level="m">Dense CNN learning with equivalent mappings</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Bridging category-level and instance-level semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.06885</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<title level="m">Multi-scale context aggregation by dilated convolutions</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<title level="m">Wide residual networks</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Understanding scene in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<ptr target="http://image-net.org/challenges/talks/2016/SenseCUSceneParsing.pdf" />
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Conditional random fields as recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Places: An image database for deep scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02055</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.05442</idno>
		<title level="m">Semantic understanding of scenes through ADE20K dataset</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
