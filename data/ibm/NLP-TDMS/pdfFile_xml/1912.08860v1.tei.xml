<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Lower Dimensional Kernels for Video Discriminators Lower-Dimensional Video Discriminators for Generative Adversarial Networks A R T I C L E I N F O</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Kahembwe</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">The School of Informatics</orgName>
								<orgName type="laboratory">Robust Autonomy and Decisions Group</orgName>
								<orgName type="institution">The University of Edinburgh</orgName>
								<address>
									<addrLine>10 Crichton St</addrLine>
									<postCode>EH8 9AB</postCode>
									<settlement>Edinburgh</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">The Edinburgh Centre of Robotics</orgName>
								<orgName type="institution" key="instit2">The University of Edinburgh&apos;s Bayes Centre</orgName>
								<address>
									<addrLine>47 Potterrow</addrLine>
									<postCode>EH8 9BT</postCode>
									<settlement>Edinburgh</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">The School of Engineering and Physical Sciences</orgName>
								<orgName type="institution">Heriot-Watt University</orgName>
								<address>
									<addrLine>The Robotarium</addrLine>
									<postCode>EH14 4AS</postCode>
									<settlement>Edinburgh</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subramanian</forename><surname>Ramamoorthy</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">The School of Informatics</orgName>
								<orgName type="laboratory">Robust Autonomy and Decisions Group</orgName>
								<orgName type="institution">The University of Edinburgh</orgName>
								<address>
									<addrLine>10 Crichton St</addrLine>
									<postCode>EH8 9AB</postCode>
									<settlement>Edinburgh</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">The Edinburgh Centre of Robotics</orgName>
								<orgName type="institution" key="instit2">The University of Edinburgh&apos;s Bayes Centre</orgName>
								<address>
									<addrLine>47 Potterrow</addrLine>
									<postCode>EH8 9BT</postCode>
									<settlement>Edinburgh</settlement>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">FiveAI</orgName>
								<address>
									<addrLine>5th Floor, 12 Blenheim Place</addrLine>
									<postCode>EH7 5JH</postCode>
									<settlement>Greenside, Edinburgh</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Lower Dimensional Kernels for Video Discriminators Lower-Dimensional Video Discriminators for Generative Adversarial Networks A R T I C L E I N F O</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Generative Adversarial Networks Discriminator Analysis Video Generation</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A B S T R A C T</head><p>This work presents an analysis of the discriminators used in Generative Adversarial Networks (GANs) for Video. We show that unconstrained video discriminator architectures induce a loss surface with high curvature which make optimisation difficult. We also show that this curvature becomes more extreme as the maximal kernel dimension of video discriminators increases. With these observations in hand, we propose a family of efficient Lower-Dimensional Video Discriminators for GANs (LDVD GANs). The proposed family of discriminators improve the performance of video GAN models they are applied to and demonstrate good performance on complex and diverse datasets such as UCF-101. In particular, we show that they can double the performance of Temporal-GANs and provide for state-of-the-art performance on a single GPU.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Motivation</head><p>Goodfellow et al. <ref type="bibr" target="#b9">[10]</ref> introduced generative adversarial training of neural networks as a way to model complex data distributions. They demonstrated the efficacy of this training regime on the task of image generation, subsequently establishing the field of Generative Adversarial Networks (GANs). GAN-based image generation has since observed significant advances; from architectural contributions <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b13">14]</ref> to novel forms of losses <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13]</ref> and stabilization methods <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b10">11]</ref>. Current state-of-the-art models for image generation produce high-resolution visual results that are sometimes difficult for humans to distinguish from those derived from the true data distribution <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b14">15]</ref>.</p><p>In comparison, video generation has not enjoyed the same level of progress as image generation. Video GAN (VGAN) models have benefited from methods such as progressive growing <ref type="bibr" target="#b0">[1]</ref> and Lipschitz regulation <ref type="bibr" target="#b26">[27]</ref>. However, VGAN models are still insufficient when it comes to modelling the true video distribution and produce results that are easily identified as lying outside its support.</p><p>There are many possible reasons for the comparatively limited performance of GAN models on the task of video modelling when compared to image modelling. The most obvious reason being that video modelling is a higher dimensional and more complex task due to the addition of a temporal dimension. The additional dimension significantly increases the number of parameters required by a GAN model to sufficiently capture the true data distribution, resulting in higher memory and compute costs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Summary</head><p>We analyse the discriminators used in VGAN models and reveal a more nuanced contributing factor to the limited performance of these models. We find that the dimensionality of the 3D kernels used in video discriminators induces higher curvature in the loss landscape and that this is detrimental to first order optimization methods such as stochastic gradient decent. In light of this observation, we target the maximum kernel dimensionality of video GAN models as an area of optimisation for the purposes of stabilizing training and improving performance. We also explore computation and memory efficiency from this perspective. To conclude, we demonstrate that in lowering the maximal kernel dimensionality of a video GAN model, we can reduce pathologies in the loss landscape, improve overall model performance while significantly increasing memory and computation efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.">Layout</head><p>This work is organized as follows; in the following section we review the relevant literature in both the image and video GAN domains. Section 3 details an analysis of the MoCoGAN and TGAN video discriminators. Section 4 introduces a family of lower dimensional video discriminators. Secion 5 compares the performance of the different discriminators against prior art. Section 6 concludes this study with a discussion of the results and its implications for the design of video discriminators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Image Generation</head><p>Adversarial training, within the context of neural networks <ref type="bibr" target="#b9">[10]</ref>, pits two networks against each other in a zero-sum non-cooperative game which is solved, in the game-theoretic sense, by the application of the minimax theorem <ref type="bibr" target="#b20">[21]</ref>. A network, called the generator ( ), learns to model the true data distribution by fooling an adversary, termed the discriminator ( ), whose job is to learn a classifier that tells apart the generated data from the real data. The standard formulation of this game is given by the value function ( , );</p><formula xml:id="formula_0">min max ( , ) = E ∼ ( ) ( ) + E ∼ ( ) ( )<label>(1)</label></formula><p>where (⋅) is a real-valued function whose exact form depends on the choice of loss function <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b21">22]</ref>. Goodfellow et al. <ref type="bibr" target="#b9">[10]</ref> initially presented models trained with this approach, termed Generative Adversarial Networks (GANs), on the task of image generation. In the GAN research community, significant effort has been dedicated to improving performance on image generation tasks as a benchmark and to stabilizing the adversarial training regime <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b6">7]</ref>. To facilitate evaluation of GAN research, metrics such as the Inception Score (IS) <ref type="bibr" target="#b28">[29]</ref> and the Fréchet Inception Distance (FID) <ref type="bibr" target="#b11">[12]</ref> have become the standard for benchmarking image quality and diversity respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Video Generation</head><p>Generative adversarial networks for video (VGAN) by Vondrick et al. <ref type="bibr" target="#b33">[34]</ref> was the first model to extend GANs to the video domain. It is composed of a 3D discriminator and a two-stream generator; one stream generating the static background content using 2D kernels and another 3D stream generating the foreground dynamic content.</p><p>Subsequent models such as Temporal GAN (TGAN) <ref type="bibr" target="#b26">[27]</ref> and Motion and Content decomposed GAN (MoCoGAN) <ref type="bibr" target="#b32">[33]</ref> use a two-stage generator. In TGAN, the first stage samples a random latent ∈ ℝ and generates a set of latent variables 1.. conditioned on that define a video trajectory across time-steps. The second stage takes the concatenated latents [ ; ], 1 and generates a single image for each time-step ∈ . The first stage of TGAN's generator uses 1D kernels, followed by 2D kernels for image generation in the second stage. It utilises 3D kernels in its discriminator.</p><p>MoCoGAN <ref type="bibr" target="#b32">[33]</ref> explicitly models the static and dynamic attributes of video separately. This model assumes a factorized latent space, where the content in video is embedded on a subspace, ∈ ℝ and its associated motion is embedded on another subspace, ∈ ℝ . The generator models the joint space ∈ ℝ , where ℝ = ℝ + ℝ , using a combination of a Gated Recurrent Unit (GRU) <ref type="bibr" target="#b7">[8]</ref> to generate the temporal dynamics of video and a 2D convolutional upscaling network to generate the associated spatial information. The video generation process involves sampling a latent variable at each time-step ∈ from the motion subspace, processing it with a GRU and concatenating the resulting output with to form = [ ;</p><p>( 1− )], where ∈ . The upscaling network is then used to project to the image space to form video frames. The MoCoGAN discriminator architecture consists of two components, a 2D image and 3D video discriminator. It is the current state-of-the-art model for low-resolution, 64x64 video generation <ref type="bibr" target="#b29">[30]</ref>. <ref type="bibr" target="#b1">2</ref> In the high-resolution video GAN literature, Acharya et al. <ref type="bibr" target="#b0">[1]</ref> propose Progressive Video GAN (ProVGAN), a model that combines progressive growing with 3D kernels and a sliced-wasserstein loss to generate video at 256x256 resolution. Saito and Saito <ref type="bibr" target="#b27">[28]</ref> explore sub-sampling as a way to scale video GAN models to 192x192 video generation and achieve state-of-the-art unconditional generation results when combined with a large batch training regime. Their proposed model, TGANv2, reduces memory and computational costs by sub-sampling across time, space and batch size as the resolution of feature maps increases. The TGANv2 video generator is comprised of a convolutional LSTM and a 2D image generator. It also uses multiple rendering layers at different resolutions within the generation pipeline, in conjunction with a hierarchy of 3D discriminators to critic the generated video at different spatial and temporal resolutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Discriminator Architectures</head><p>Although there is some variety in the architectures of video GAN generators, the associated discriminator architectures have remained fairly consistent. There are currently two primary choices for video GAN discriminators; a dual (2D image + 3D video) discriminator architecture as in MoCoGAN, or a 3D video discriminator architecture as in VGAN, TGAN and ProVGAN. <ref type="bibr" target="#b2">3</ref> In this section, we take a closer look at the current choice of video GAN discriminators and study the impact that architectural decisions for this component have on model performance. In particular, we explore the question;</p><p>• "What makes a good discriminator for video GANs?".</p><p>To answer this question, we analyse the properties of seminal video GAN discriminators for both dual and single discriminator architectures. For the dual discriminator architecture, our analysis focuses on the MoCoGAN model since it is the first video generation model to incorporate multiple architectural components in its discriminator. TGAN is used as the representative model for single component discriminators due to the architectural similarity of its video discriminator to that of MoCoGAN. <ref type="bibr" target="#b0">1</ref> [ ; ] denotes the concatenation operation ; between and <ref type="bibr" target="#b1">2</ref> We refer to the state-of-the-art as benchmarked on the UCF-101 dataset <ref type="bibr" target="#b2">3</ref> The dimensionality of the discriminator is always in reference to the maximum convolution kernel dimension</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Experimental Setup</head><p>The original experimental code for the MoCoGAN 4 and TGAN 5 model is publicly available. As such, all experiments use the original experimental code and settings to allow for accurate analysis and aid reproducibility. Training and performance benchmarking is carried out on a single 12GB Titan-X GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Quantitative Experiments</head><p>We use the UCF-101 dataset for our quantitative experiments <ref type="bibr" target="#b29">[30]</ref>. This dataset was initially introduced for action recognition but was co-opted by the GAN research field in order to benchmark video GAN models. The UCF-101 dataset <ref type="bibr" target="#b29">[30]</ref> is a video dataset consisting of 13,320 video clips divided across 101 different action categories, at a spatial resolution of 320 by 240 pixels.</p><p>Our preprocessing pipeline center crops all videos to 240x240 pixels. For the MoCoGAN experiments, the video is temporally subsampled by a factor of 2 and 16 consecutive frames are randomly extracted. For the TGAN experiments, the video is not subsampled in order to match the original experimental conditions. <ref type="bibr" target="#b5">6</ref> We then resize the resulting video to the model resolution (i.e. 16x64x64 or 16x128x128). We use the "trainlist01" training split containing 9537 videos to train our models as in previous works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Metrics</head><p>We benchmark performance against the video extensions of the Inception Score (IS) and Fréchet Inception Distance (FID) where a higher IS implies that the generated video is of a higher visual quality and a lower FID implies a better fit to the modes of the data distribution (i.e. good diversity). <ref type="bibr" target="#b6">7</ref> These metrics are highly sensitive to implementation so we use their exact implementation from the original TGAN experiments <ref type="bibr" target="#b26">[27]</ref>. We calculate the IS using the Sport1M pre-trained C3D classification model fine-tuned on UCF-101 from the original TGAN experiments. We calculate the FID using the activations from the second to last linear layer, denoted fc7, of the same C3D model.</p><p>As in previous works, we generate 10000 samples from the model to evaluate each metric and derive a rough standard deviation by repeating this procedure four times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Qualitative Results</head><p>We use the MUG Facial Expression Database (MUG-FED) for our qualitative experiments <ref type="bibr" target="#b1">[2]</ref>. This dataset is composed of 1462 video sequences of 86 people demonstrating 7 categories of facial expression; anger, fear, disgust, happiness, sadness, surprise and neutral. The videos are recorded at a resolution of 896x896 pixels and range between 40-180 frames.</p><p>Evaluation Metrics For each model, 10000 video samples are generated and randomly sub-divided into 100 batches.</p><p>All videos in each batch are tiled and aggregated into a single larger video. The tiled videos are presented, two at a time, to human participants for a side-by-side visual evaluation of sample quality and batch diversity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">MoCoGAN Discriminator</head><p>Although the seminal video GAN models used a single 3D video discriminator, later works such as MoCoGAN achieve better results with a dual (2D image + 3D video) discriminator model. In theory, a high-capacity discriminator with kernels whose dimensionality matches that of the input data distribution should allow for better criticism. In practice, this is not observed and the MoCoGAN authors attribute the performance boost gained with the addition of an image level discriminator to its ability to "focus on static appearances". In the following section, we investigate this claim and provide a more thorough explanation grounded with empirical observations. <ref type="table" target="#tab_0">Table 1</ref> details the architectures for the MoCoGAN 2D image and 3D video discriminators. These are identical patch-level discriminators that operate on 46 × 46 patches of the input video frames with the distinction that the image discriminator is comprised of 2D convolution kernels and the video discriminator utilizes 3D kernels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Ablation Study</head><p>We replicate the results for this model on the UCF-101 dataset and ablate its discriminator components to ascertain how much each component contributes to the final model performance. Results are presented in <ref type="table" target="#tab_1">Table 2</ref>. <ref type="bibr" target="#b3">4</ref> MoCoGAN Code: https://github.com/sergeytulyakov/mocogan 5 TGAN Code: https://github.com/pfnet-research/tgan <ref type="bibr" target="#b5">6</ref> Results for TGAN trained with temporal subsampling and MoCoGAN trained without temporal subsampling are explicitly labelled <ref type="bibr" target="#b6">7</ref> These metrics are defined relatively and should be approached with caution <ref type="bibr" target="#b4">[5]</ref>  </p><formula xml:id="formula_1">Input height × width × 3 16 × height × width × 3 c0</formula><p>Conv2D-(N64, K4, S2, P1), LReLU Conv3D-(N64, K4, S(1,2,2), P(0,1,1)), LReLU c1</p><p>Conv2D-(N128, K4, S2, P1), BN, LReLU Conv3D-(N128, K4, S(1,2,2), P(0,1,1)), BN, LReLU c2</p><p>Conv2D-(N256, K4, S2, P1), BN, LReLU Conv3D-(N256, K4, S(1,2,2), P(0,1,1)), BN, LReLU c3</p><p>Conv2D-(N1, K4, S2, P1) Conv3D-(N1, K4, S(1,2,2), P(0,1,1))  In <ref type="table" target="#tab_1">Table 2</ref>, we observe that there is a degradation in performance without an image-level discriminator (row 1 vs row 3) and we also observe that image-level statistics can account for most of the model performance as demonstrated by the the model trained with an image-only discriminator (row 4).</p><p>We were not able to replicate the published results and suspect that this could be due to the MoCoGANs saturating loss function. The loss function used in the original GAN formulation set</p><formula xml:id="formula_2">( ) = log( ) and ( ) = log(1 − ) in Equation 1.</formula><p>saturates when the discriminator overpowers the generator and learns a perfect classifier between the real and fake data distribution. As a result, the gradient signal propagated back to the generator via the discriminator vanishes, stalling training for the generator. Goodfellow et al. <ref type="bibr" target="#b9">[10]</ref> proposed a solution for this by training the generator to maximise , i.e. = − . This provides for a non-saturating version of the GAN loss that allows for a nonvanishing gradient signal through out training ( <ref type="figure" target="#fig_1">Figure 2</ref>). The non-saturating loss did not improve performance but we maintain it for all further experiments to avoid potential saturation issues. We were only able to to replicate the published performance of the MoCoGAN model, by doubling the number of channels in every component of the model (row 5). Doubling just the channels for the video discriminator was not sufficient even though it accounts for 80% of the discriminator parameters (row 6). Finally, we also measure the impact of temporal subsampling on model performance as it was not used in previous models such as TGAN (row 2). We observe that temporal subsampling accounts for a significant portion of model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Hessian Analysis</head><p>In this section, we analyse the loss surface induced by the image and video discriminators via analysis of the Hessian of the GAN objective with respect to the discriminator parameters. Hessian analysis of neural networks is computationally expensive, especially for large models such as those used in the video GAN domain. As an approximation, we employ the -operator from Pearlmutter <ref type="bibr" target="#b22">[23]</ref> to calculate the exact Hessian vector product for neural networks and combine it with the Lanczos algorithm to calculate the eigen spectra of the Hessian. The gradient and Hessian are taken with respect to the parameters of the discriminator and are given bẏ = ∇ ( , ) and̈ = ∇ 2 ( , ) respectively. We track the leading eigenvalues of the Hessian throughout training and present these results in <ref type="figure" target="#fig_3">Figure 3</ref>.   <ref type="table" target="#tab_1">Table 2</ref>). + denotes the largest positive eigenvalue encountered during training and − the largest negative eigenvalue. <ref type="figure" target="#fig_3">Figure 3</ref> provides an interesting perspective as to what is going on with the MoCoGAN discriminator. The primary observation is that for identical discriminator architectures, an increase in kernel dimensionality results in a loss surface with significantly more pathological curvature. Each discriminator component individually encounters eigenvalues during training that are an order of magnitude larger than the majority of leading eigenvalues. But there is also a further order of magnitude difference between the maximum eigenvalue and majority leading eigenvalues for the 2D image discriminator, when compared against the 3D video discriminator (see <ref type="figure" target="#fig_3">Figure 3a</ref> vs 3b and <ref type="figure" target="#fig_3">Figure 3c vs 3d</ref> ). An increase in parameters results in a generally smoother loss landscape with possibly more saddle points (see <ref type="figure" target="#fig_3">Figure 3a</ref> vs 3c and Figure 3b vs 3d). A consistent observation is that the magnitude of the largest eigenvalue tends to reduce throughout training and that this effect is less observable with an increase in discriminator kernel dimensionality.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Kernel Complexity vs Kernel Dimensionality:</head><p>It could be said that the observations in <ref type="figure" target="#fig_3">Figure 3</ref> are possibly an artifact of kernel complexity rather than dimensionality. But increasing the parameter complexity of the image discriminator such that it matches that of the video discriminator leads to little deterioration in the loss landscape of the image discriminator ( <ref type="figure" target="#fig_5">Figure 4a</ref> vs <ref type="figure" target="#fig_3">Figure 3b</ref>). Instead we observe that even with an increase in kernel complexity, the loss landscapes induced by the image discriminators are more similar to each other in terms of curvature than to those induced by the video discriminator <ref type="figure" target="#fig_3">(Figure 3a</ref>, 3c, and 4a vs <ref type="figure" target="#fig_3">Figure 3b</ref>). Furthermore, <ref type="figure" target="#fig_5">Figure 4b</ref> shows that before collapse, a video discriminator with half the kernel complexity of an image discriminator induces a similar loss landscape to that of the original MoCoGAN video discriminator <ref type="figure" target="#fig_3">( Figure 3b</ref> vs <ref type="figure" target="#fig_5">Figure 4b</ref>) .</p><p>Dataset Complexity vs Kernel Dimensionality: An argument could be made that the dataset complexity affects the curvature of the loss landscape. We analyse the loss surface of the MoCoGAN model trained on two different datasets with very different mode characteristics and scene dynamics, UCF-101 and MUG-FED. UCF-101 includes 101 different classes with different inter and intra class variability for each video when compared to MUG-FED. MUG-FED is a dataset with a fixed background and different faces under controlled lighting making different facial expressions. UCF-101 is a significantly more complex dataset than MUG-FED and our analysis shows that indeed, the loss landscape is affected by the dataset, resulting in landscapes with different characteristics (see <ref type="figure" target="#fig_3">Figure 3a</ref> vs 4c and <ref type="figure" target="#fig_3">Figure 3b vs 4d</ref>). But we still observe that the curvature of this landscape is primarily dictated by the kernel dimensionality resulting in similar curvature magnitude profiles for models trained on either dataset (see <ref type="figure" target="#fig_3">Figure 3a</ref>, 3b vs <ref type="figure" target="#fig_5">Figure 4c, 4d)</ref>. In particular, we observe the same order of magnitude difference between the eigenvalues of the loss Hessian for 2D image discriminators when compared against their 3D video counterparts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">Explaining the Dual Video Discriminator</head><p>Stochastic gradient decent (SGD) requires a smooth loss landscape for stable optimisation. An ill-conditioned Hessian alludes to directions of high curvature in this landscape that lead to instabilities in the optimization of typical neural networks <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b16">17]</ref>. The GAN optimisation process is itself also highly unstable, exhibiting rotational mechanics and cyclical dynamics that are detrimental to convergence <ref type="bibr" target="#b3">[4]</ref>, more so if the true data distribution is concentrated on a lower dimensional manifold as is likely in video <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b17">18]</ref>. <ref type="figure" target="#fig_3">Figure 3</ref> shows that the optimisation landscape induced by video GAN discriminators has significant pathologies. For the MoCoGAN discriminators, the highest Hessian eigenvalue observed is at times up to an order of magnitude larger than the next leading eigenvalue. This behaviour is observed at the early stages of training for the 2D image discriminator but these extreme outliers exist throughout training for the 3D video discriminator. More importantly, <ref type="figure" target="#fig_3">Figure 3</ref> shows that these pathologies are made worse as the kernel dimensionality of the discriminator increases. The eigenvalues of the loss Hessian induced by the video discriminator are altogether almost an order of magnitude larger than those of the image discriminator. <ref type="figure" target="#fig_5">Figure 4</ref> shows that this is irrespective of dataset and kernel parameter complexity. Altogether, these observations help to explain the emergence of dual 2D image and 3D video discriminators in models such as MoCoGAN <ref type="bibr" target="#b32">[33]</ref>. SGD and its derivatives face a bigger challenge optimising the loss landscape induced by a 3D discriminator when compared to that of a 2D discriminator. As a result, the 2D discriminator likely improves performance by providing a better image-level gradient signal for the generator. The observations in <ref type="table" target="#tab_1">Table 2</ref>, row 6, where increasing the number of parameters for the 3D video discriminator did not lead to a significant boost in performance lend further support to this hypothesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">TGAN Discriminator</head><p>In the previous section, we established that naive application of 3D discriminators induces loss landscapes with high curvature when compared to that induced by 2D discriminators. TGAN <ref type="bibr" target="#b26">[27]</ref> utilizes a single 3D video discriminator and manages to match the performance of dual discriminator models like MoCoGAN. In our replication study (see <ref type="table" target="#tab_6">Table 8</ref>); TGAN consistently outperforms MoCoGAN. Inspired by Wasserstein GAN <ref type="bibr" target="#b2">[3]</ref>, TGAN proposes clamping of the spectral norm of the discriminator to a maximum of one in order to stabilize training. This is achieved in practice via Singular Value Clipping (SVC) of the weight matrices such that all singular values are equal to or less than one, enforcing a 1-Lipschitz constraint on the video discriminator. This clipping is applied every iterations during training and the TGAN authors demonstrate that applying SVC leads to stable training and significantly better performance. We analyse the loss landscape induced by SVC and observe how it impacts model performance. The results are presented in <ref type="table" target="#tab_2">Table 3</ref> and <ref type="figure" target="#fig_6">Figure 5</ref>. <ref type="figure" target="#fig_6">Figure 5</ref> provides an interesting insight into the TGAN discriminator, especially when contrasted against the MoCo-GAN video discriminator (see <ref type="figure" target="#fig_3">Figure 3b</ref>). The TGAN discriminator induces a loss landscape filled with saddle points, characterised by the symmetry between the positive and negative eigenvalues of the loss Hessian. Without SVC, we observe that the curvature of the loss landscape becomes more extreme throughout training <ref type="figure" target="#fig_6">(Figure 5b</ref>). With SVC applied, we observe a comparatively smooth loss landscape with a better conditioned Hessian. This leads to more stable training dynamics and better performance as shown in <ref type="table" target="#tab_2">Table 3a</ref>.</p><p>We also observe from results presented in row 2 of <ref type="table" target="#tab_1">Table 2 and the last row of Table 3a</ref>, that temporal subsampling of video frames has a significant impact on model performance. When temporal subsampling of frames is experimentally controlled for, it appears that TGAN significantly outperforms MoCoGAN according to both FID and IS. Conv3D-(N64, K4, S2, P1), LReLU c1</p><p>Conv3D-(N128, K4, S2, P1), BN, LReLU c2</p><p>Conv3D-(N256, K4, S2, P1), BN, LReLU c3</p><p>Conv3D-(N512, K4, S2, P1), BN, LReLU c4</p><p>Conv2D-(N1, K4, S1, P0,) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Lower-Dimensional Video Discriminators</head><p>We have established that good discriminator performance is promoted by a smooth loss landscape, which is well understood for neural networks. We have shown how enforcing a 1-Lipschitz constraint on the discriminator can smooth the loss landscape. We have also demonstrated that there is a strong correlation between the conditioning number of the Hessian and the kernel dimensionality of the discriminator. This opens up an interesting direction in terms of discriminator architecture design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• "Do video discriminators require 3D kernels?"</head><p>In addition to the higher curvature optimisation landscapes induced by higher dimensional discriminators, there are other disadvantages to using higher dimensional kernels such as an increase in computation and memory costs. In this section, we propose solutions to these issues by exploiting the insights gained from Section 3.</p><p>Most generators for video GANs utilize kernels with a maximum dimensionality of two <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b27">28]</ref>, but all discriminators currently incorporate 3D kernels. Thus, we explore the possibility that it may be possible to capture temporal dynamics using a more compressed kernel representation since locally, most information useful for video discrimination may lie on a lower dimensional manifold. This hypothesis is supported by results from related domains such as video recognition, where <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b15">16]</ref> have successfully removed 3D kernels from classification models without compromising model performance. In most cases, performance has improved and our observations from Section 3 provide a possible explanation for this phenomenon, a better conditioned loss Hessian. Similarly, we seek to replace 3D video GAN discriminators with lower dimensional approximations, resulting in memory and computational efficiency gains as well as better performance due to more stable training dynamics.</p><p>We now introduce a family of Lower Dimensional Video Discriminators for Generative Adversarial Networks (LDVD-GANs). These discriminators are characterised by having a maximal kernel dimension that is lower than the ambient dimension of the data modality they are applied to.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Factorized Convolutions</head><p>Decomposing 3D convolution kernels into 2D and/or 1D is an area of active research interest in video recognition and understanding <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b34">35]</ref>. This process can formally be defined as:</p><formula xml:id="formula_3">K ℎ, , = A ℎ, ⊗ b . where K ∈ ℝ ℎ × × , A ∈ ℝ ℎ × , b ∈ ℝ ,<label>(2)</label></formula><p>are convolution kernels and ⊗ denotes the Kronecker product. K is from the subset of kernels that can be factorized into A and b as shown in Eq. 2. A convolution over a feature map F ∈ ℝ ℎ × × can then be defined as:</p><formula xml:id="formula_4">F +1 = (F ℎ , * ○ A) * ○ b<label>(3)</label></formula><p>where * ○ denotes the convolution operation 8 and the subscripts denote the dimensions over which it is applied. The R(2+1)D model <ref type="bibr" target="#b31">[32]</ref> applies the factorization sequentially, as shown in Eq. 3, while the S3D-G <ref type="bibr" target="#b34">[35]</ref> model combines it with a feature gating mechanism. The Pseudo-3D (P3D) family of models <ref type="bibr" target="#b23">[24]</ref> explores different orderings of spatial and temporal convolutions within the bottleneck block of a 2D residual network. The model <ref type="bibr" target="#b30">[31]</ref> splits the network in half, applying spatial convolutions to the first half and temporal convolutions to the down-stream features. In the two-stream literature, ST-ResNet <ref type="bibr" target="#b8">[9]</ref> applies a temporal-spatial-temporal factorization within its bottleneck blocks combined with inter-stream residual paths. All of these models present impressive results when compared to their 3D counterparts within their application domains.</p><p>The factorized convolution applied in our discriminators is similar to the one shown in Eq. 3, with the addition of an activation layer between the spatial and temporal convolutions. That is, our factorised convolution is of the form: <ref type="table">Table 4</ref> Factorized discriminators</p><formula xml:id="formula_5">F +1 = LReLU(F ℎ , * ○ A) * ○ b (4)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(a) MoCoGAN -Factorized</head><p>Layer Configuration Block Operations c0 ℎ, K(1,4,4),S(1,2,2),P(0,1,1),ch64 c0 ℎ, , LReLU c0 K(4,1,1),S1,P(1,0,0),ch64 c0 , LReLU c1 ℎ, K(1,4,4),S(1,2,2),P(0,1,1),ch128 c1 ℎ, , LReLU c1 K(4,1,1),S1,P(1,0,0),ch128 c1 , BN, LReLU c2 ℎ, K(1,4,4),S(1,2,2),P(0,1,1),ch256 c2 ℎ, , LReLU c2 K(4,1,1),S1,P(1,0,0),ch256 c2 , BN, LReLU c3 ℎ, <ref type="figure" target="#fig_0">K(1,4,4)</ref>,S(1,2,2),P(0,1,1),ch256 c4 ℎ, , LReLU c3 K(4,1,1),S1,P(1,0,0),ch1 c4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(b) TGAN -Factorized</head><p>Layer Configuration Block Operations c0 ℎ, K(1,4,4),S(1,2,2),P(0,1,1),ch64 c0 ℎ, , LReLU c0 K(4,1,1),S(2,1,1),P(1,0,0),ch64 c0 , LReLU c1 ℎ, K(1,4,4),S(1,2,2),P(0,1,1),ch128 c1 ℎ, , LReLU c1 K(4,1,1),S(2,1,1),P(1,0,0),ch128 c1 , BN, LReLU c2 ℎ, K(1,4,4),S(1,2,2),P(0,1,1),ch256 c2 ℎ, , LReLU c2 K(4,1,1),S(2,1,1),P(1,0,0),ch256 c2 , BN, LReLU c3 ℎ, K(1,4,4),S(1,2,2),P(0,1,1),ch512 c3 ℎ, , LReLU c3 K(4,1,1),S(2,1,1),P(1,0,0),ch512 c3 , BN, LReLU c4 ℎ, K(4,4),S1,P0,ch1 c4 ℎ, , LReLU</p><p>The factorized MoCoGAN and TGAN discriminator architectures are presented in <ref type="table">Table 4</ref>. They are identical to their original counterparts with the exception that all the 3D convolution kernels are factorized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Temporal Shift Module</head><p>The Temporal Shift Module (TSM) <ref type="bibr" target="#b15">[16]</ref>, entirely forgoes 3D and/or 1D kernels. Instead, it adapts a purely 2D network for video processing by shifting a portion of channels temporally. This allows for an increase in the temporal receptive field of each layer controlled by the temporal shift distance and the layer depth.</p><p>For MoCoGAN, we apply the TSM to the MoCoGAN image discriminator architecture and use that as the sole discriminitive function during training. For TGAN, we first replace all 3D convolution layers with their 2D counterparts and then interleave convolution operations with shifting operations via the use of TSMs.</p><p>Temporal shifting is only applied between intermediate layers and the temporal shift distance in either direction is a single time-step applied to a quarter of the channels for each direction as in <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>It is important to note that we do not do any hyper-parameter tuning, nor do we deviate from the experimental setups of the original TGAN and MoCoGAN experiments. Our sole focus in these experiments is to apply the insights gained from Section 3 and demonstrate the efficacy of using Lower Dimensional Video Discriminators (LDVDs) for video generation. In doing so we also improve on the state-of-the-art for video generation and provide a significantly more efficient architecture for high-resolution video generation, competitive with state-of-the-art multi-gpu models. Crucially, we achieve all this by showing that our proposed lower dimensional discriminators can double the performance of previously published models and set state-of-the-art results using only a single GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Factorized Convolutions</head><p>We explore the space of lower dimensional video architectures induced by our formulation in Section 4. We aim to gauge how much dimension factorization affects baseline performance when using a lower dimensional video discriminator. As such, we benchmark lower dimensional discriminators that are factorized to different degrees. Our proposed LDVD architectures in <ref type="table">Table 4</ref> apply factorized convolutions for all 3D convolution layers in their respective video discriminator architectures. For the MoCoGAN discriminator <ref type="table" target="#tab_0">(Table 1)</ref>, factorizing layer c0 to c3 corresponds to the factorised discriminator architecture shown in <ref type="table">Table 4a</ref> whose performance is presented in the final row of <ref type="table">Table 5b</ref>. The same applies for the TGAN discriminator <ref type="table" target="#tab_2">(Table 3b)</ref>, where factorizing layer c0 to c3 corresponds to the factorised discriminator architecture shown in <ref type="table">Table 4b</ref> whose performance is presented in the final row of <ref type="table">Table 5a</ref>. We denote the different discriminators by the layers at which factorization is applied. c0 denotes factorization of the first 3D convolution layer, c0-c1 denotes factorization of the first and second convolution layers, so on and so forth. All other non-factorized convolution layers are restricted to using 2D convolution kernels. The performance of models trained with these discriminators is presented in <ref type="table">Table 5</ref>. The TGAN discriminator has an additional 2D layer appended to it. We explore how temporal aggregation methods applied to its inputs affect model performance. The results are presented in <ref type="figure" target="#fig_8">Figure 6</ref>. <ref type="table">Table 5</ref> Performance on the UCF-101 dataset for discriminators with factorization applied to a varying number of convolution layers</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(a) TGAN -Factorized Convolutions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Inception Score↑ FID↓ Params TGAN <ref type="bibr" target="#b26">[27]</ref> 11  <ref type="table">Table 5</ref> and <ref type="figure" target="#fig_8">Figure 6</ref> provide for several interesting observations; the first being that factorized LDVDs can outperform their 3D counterparts using a fraction of their parameters. In particular for the TGAN discriminator, performance improves in every case. The best performing factorized TGAN discriminator boosts the IS by around 15% and consequently the state-of-the-art for 64x64 video generation by around 10%. In comparison, we only observe moderate performance improvements for the MoCoGAN model and suspect that this is likely due to a bottlenecked discriminator. We denote the best performing factorized TGAN and MoCoGAN models as TGAN-F and MoCoGAN-F respectively. These are the c0-c1 discriminators from <ref type="table">Table 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Improving MoCoGAN Performance:</head><p>In Section 3 we demonstrate three ways of encouraging smooth loss landscapes; increasing the number of discriminator parameters, using an LDVD and directly enforcing that all layers in the discriminator are 1-Lipschitz continuous. The TGAN model applies Singular Value Clipping (SVC) to stabilize training and its factorized discriminators benefit from it. We explore enforcing a 1-Lipschitz constraint on the MoCoGAN-F discriminator via spectral normalization <ref type="bibr" target="#b18">[19]</ref> and observe that it improves performance to an IS of 12.33 ± .09 and an FID of 9069 ± 4.  Temporal Resolution: An interesting observation about the MoCoGAN-F and TGAN-F discriminators is that they only process temporal information in their initial couple of layers. Factorised discriminators with more capacity to process temporal information further downstream and over a longer temporal receptive field do not perform as well. <ref type="table" target="#tab_4">Table 6</ref> summarizes experiments from our exploration of the temporal shifting strategy. As in the original TSM models from Lin et al. <ref type="bibr" target="#b15">[16]</ref>, single-step temporal shifting in each direction is applied to a quarter of feature maps. Our discriminator naming convention is similar to that in the previous section, whereby ci-ck denotes a discriminator with temporal shifting applied after layers i through k. All discriminators are entirely 2D in nature with temporal shifting used to capture and merge temporal information between intermediate layers. The temporal shift module (TSM) allows for video discriminators with the same number of parameters as traditional image discriminators. Additionally, the parameter cost of the discriminator is constant regardless of the size of the temporal receptive field. This can be seen in <ref type="table" target="#tab_4">Table 6a</ref>, where a 2D discriminator improves the performance of the TGAN model by around 10%, through depth-wise regulation of the Temporal Receptive Field (TRV). We observe a drop in performance for the MoCoGAN model <ref type="table" target="#tab_4">(Table 6b</ref>) and attribute it to the discriminator not being 1-Lipschitz continuous. We denote the best performing discriminators for TGAN and MoCoGAN from <ref type="table" target="#tab_4">Table 6</ref>, TGAN-TSM and MoCoGAN-TSM respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Temporal Shift Module</head><p>When comparing temporal shifting to factorized convolutions, we observe that the temporal processing carried out by the 1D kernels in factorized convolutions result in significant improvements in performance for negligible cost in terms of memory and computation. The shifting strategy is comparatively expensive, often increasing training times by 10-40% depending on the number of shifting operations carried out.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Comparison with State-of-the-Art</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1.">A note on the IS of UCF-101 Videos</head><p>An accurate comparison against previous work requires that a distinction be made between low and high resolution video generation. This is because metrics such as the inception score (IS) are known to be sensitive to image resolution <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b5">6]</ref>. In video, the additional temporal dimension is unconstrained and requires some procedure for sub-sequence selection. Additionally, data augmentation methods such as frame sub-subsampling (i.e. skip every frames) or cropping are applied to some published models but not others. We benchmark the IS of the UCF-101 'trainlist01' dataset used in the video GAN literature under different conditions. Since dataset videos have more frames than the evaluation networks 16 frame temporal resolution, we can derive a rough standard deviation by repeating the evaluation process four times with randomly sampled 16 frame sub-sequences. These results are shown in <ref type="table" target="#tab_5">Table 7</ref>. Dataset Normalization: We observed that the IS is highly sensitive to the mean normalisation file used during evaluation and as a result maintain the use of the mean normalisation file provided by the TGAN authors <ref type="bibr" target="#b26">[27]</ref>. We note that our 128 resolution results in <ref type="table" target="#tab_5">Table 7</ref> are close to the true data IS of 83.18 published in <ref type="bibr" target="#b0">[1]</ref>.  <ref type="table" target="#tab_6">Table 8</ref> presents results for the best performing TSM and factorized discriminators against the state-of-the-art models for 64x64 video generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2.">Lower Resolution Generation</head><p>Both TGAN-TSM and TGAN-F outperform the original TGAN architecture using a quarter of the the parameters of the original discriminator. Furthermore, TGAN-F sets a new state-of-the-art result for 64x64 video generation and outperforms complex higher resolution models such as ProVGAN (see <ref type="table">Table.</ref> 9) without exploiting temporal subsampling, a technique which we have shown to significantly boost performance (see the second row of <ref type="table" target="#tab_1">Table 2 and  the last row of Table 3a</ref>). Next, we explore higher resolution video generation with this architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3.">Higher Resolution Generation</head><p>The computation and memory gains achieved by reducing the maximum kernel dimension of the discriminator allows for the TGAN model to be scaled up to higher resolutions without issue, even on a single GPU system. Our highresolution video generation model is based on our best performing low-resolution model, TGAN-F, with appropriate modifications made to support higher resolutions.  <ref type="table" target="#tab_7">Table 9</ref> presents results for TGAN-F benchmarked on the task of 128x128 video generation. TGAN-F (+ 4xTem-poralCh) corresponds to quadrupling the number of channels in the temporal frame generator, TGAN-F (+ 2xImageCh) corresponds to doubling the number of channels in the TGAN-F image generator, TGAN-F (+ 2xTime) corresponds to doubling the temporal receptive field by subsampling a longer video and TGAN-F (+ All) corresponds to applying all the above modifications to the TGAN-F architecture.</p><p>The first observation is that TGAN-F trained on a single GPU outperforms all single-GPU models by at least 15%. The second observation is that TGAN-F is significantly more parameter efficient than previous video GAN architectures. This consequently enables it to be more memory efficient, enabling training with batch sizes of up to 32 and at resolutions as high as 16x128x128 on a single GPU. Another observation is that TGAN-F (+ All) almost doubles the performance of the original TGAN model while using the same generator architecture, hyper-parameters and hardware constraints. The last observation is that TGAN-F (+ All) on a single gpu provides for state-of-the-art results while using a fraction of the parameters and compute of multi-GPU VGAN models. This demonstrates the efficacy and efficiency of LDVD-GANs like TGAN-F, and shows its superior performance when compared to the original TGAN model and many other video GAN models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TGAN-F Loss Landscape:</head><p>Hessian analysis of the loss landscape induced by the TGAN-F discriminator during optimisation shows that the curvature of this space is more than halved when compared to that of the original TGAN discriminator (see <ref type="figure">Figure 7b</ref> vs 7a). The smoother loss landscape induced by the lower dimensional discriminator helps to explain the improved performance of the TGAN-F model when compared to its original higher dimensional counterpart, TGAN. <ref type="figure" target="#fig_0">Figure 1</ref> shows frames from TGAN-F models trained on MUG-FED at different resolutions. <ref type="figure" target="#fig_11">Figure 9</ref> shows selected high-resolution samples from the same model trained on the UCF-101 dataset. We observe learned camera zooming and panning motions. Full resolution random samples and other qualitative results are available in the supplementary material. 9</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.4.">Qualitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 10</head><p>Human evaluation of the quality and diversity of samples generated by different models trained on the MUG-FED dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>The field of image generation has enjoyed significant advances in recent years, and our work aims at taking a step towards doing the same for video generation. Specifically, we study the properties of video discriminator architectures and find that higher dimensional video discriminators induce a loss landscape with relatively higher curvature. As a result, we question the utility of 3D kernels in video GAN models and empirically demonstrate that they are not required for the video generation problem as it is currently framed. Our design proceeds by replacing 3D kernels with lower dimensional approximations, and our proposed lower dimensional discriminators, improve the performance of video GAN generators they are applied to. As a result, we demonstrate performance that is competitive with the stateof-the-art for both single and multi-gpu video generation; in both low-resolution and high-resolution video generation settings.</p><p>We carried out a wide range of experiments across two generator models and many more discriminator architectures. We summarise the successful results of this investigation in Section 3 and Section 5. These experiments demonstrate that the curvature of the loss landscape for video GAN discriminators increases with kernel dimensionality. We also uncover guiding principles to limit this behaviour; mainly avoiding 3D kernels, but also enforcing a 1-Lipschitz discriminator and increasing the number of parameters in a model (Section 4). Based on these principles, we propose a family of lower dimensional video discriminator architectures that provide for efficient but powerful video GAN models. Subsequently, we explore one such lower dimensional discriminator architecture, TGAN-F, resulting in state-of-the-art performance for a single-gpu model (Section 5.3).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Bibliography</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Frames from videos generated by our TGAN-F model trained on MUG-FED at 256×256, 128×128, and 64×64 resolutions (top to bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Norm of the gradient at each node in the computation graph for batches of real and fake data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) MoCoGAN-Image Discriminator (b) MoCoGAN-Video Discriminator (c) MoCoGAN 2xChannels -Image Discriminator (d) MoCoGAN 2xChannels -Video Discriminator</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>The 10 largest eigenvalues of the loss Hessian with respect to the image and video discriminators of the MoCoGAN architecture. (a) and (b) show these values for the original MoCoGAN architecture. (c) and (d) show these for MoCoGAN with the number of channels doubled (see MocoGAN -2xChannels in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(a) MoCoGAN ksize8 -Image Discriminator (b) MoCoGAN ksize2 -Video Discriminator (c) MoCoGAN MUG -Image Discriminator (d) MoCoGAN MUG -Video Discriminator</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>The 10 largest eigenvalues of the loss Hessian with respect to the image and video discriminators of the MoCoGAN architecture. (a) shows these values for an image discriminator with kernels that have four times the parameters of the original image discriminator. (b) shows these values for a video discriminator with an eighth the parameters per kernel of the original video discriminator. (c) and (d) show these values for the MoCoGAN discriminators trained on the MUG-FED dataset. + denotes the largest positive eigenvalue encountered during training and − the largest negative eigenvalue.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>The 10 leading eigenvalues of the loss Hessian with respect to the TGAN discriminator, with and without SVC applied. + denotes the largest positive eigenvalue encountered during training and − the largest negative eigenvalue.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>(a) Pooling strategy vs Performance (b) Parameter Efficiency vs Performance</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Performance of factorized TGAN discriminator architectures on 64x64 UCF-101 video generation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>4 9Figure 7 :</head><label>47</label><figDesc>33.0/67.0 50.0/50.0 TGAN/TGAN-F 41.6/58.4 50.0/50.0 MoCoGAN/TGAN-F 25.0/75.0 16.6/83.Supplementary Material: https://drive.google.com/drive/folders/1J9gjS2HRTwoADQVqVoMbs5pBtO7rOGF6 (a) TGAN -SVC (b) TGAN -F The 10 leading eigenvalues of the Hessian with respect to the TGAN and TGAN-F discriminator. + denotes the largest positive eigenvalue encountered during training and − the largest negative eigenvalue.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>The first frame of video samples for each of the models on 64x64 UCF-101. Full samples available in the supplementary material</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 :</head><label>9</label><figDesc>Frames from videos generated by TGAN-F trained on UCF-101 at 128×128 resolutions. Top: Zoom into scene, Middle: Pan left to right, Bottom: Rotate around center of focus.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>MoCoGAN Image and Video Discriminators</figDesc><table><row><cell>Layer</cell><cell>Image Configuration</cell><cell>Video Configuration</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell cols="2">MoCoGAN Ablation</cell><cell></cell><cell></cell></row><row><cell cols="2">Row Model</cell><cell>Disc Params</cell><cell>IS ↑</cell><cell>FID ↓</cell></row><row><cell>1</cell><cell>MoCoGAN</cell><cell cols="3">3.3M 11.58 ± .04 9485.34 ± 14.61</cell></row><row><cell>2</cell><cell>MoCoGAN -NoTemporalSubsampling</cell><cell cols="2">3.3M 10.35 ± .06</cell><cell>9657.44 ± 3.90</cell></row></table><note>3 MoCoGAN -Video Discriminator Only 2.7M 11.09 ± .03 9565.65 ± 21.03 4 MoCoGAN -Image Discriminator Only .7M 8.26 ± .04 10494.09 ± 12.52 5 MoCoGAN -2xChannels 13.2M 12.61 ± .08 9166.81 ± 9.92 6 MoCoGAN -2xChannels -Video Discriminator Only 10.5M 11.73 ± .05 9461.10 ± 1.85 7 MoCoGAN -Video Discriminator Only -ksize2 .3M 3.98 ± .02 13664.81 ± 7.64 8 MoCoGAN -Image Discriminator Only -ksize8 2.7M 7.62 ± .04 10337.71 ± 0.47 9 MoCoGAN-Original published in [33] 3.3M 12.42 ± .03</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 TGAN</head><label>3</label><figDesc></figDesc><table><row><cell>Model</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">(a) TGAN Reproduction</cell><cell></cell><cell>(b) TGAN Discriminator</cell></row><row><cell>Model</cell><cell>IS ↑</cell><cell>FID ↓</cell><cell>Layer Block Configuration</cell></row><row><cell>TGAN -SVC</cell><cell>11.93 ± .08</cell><cell>9127.80 ± 13.77</cell><cell>Input 16 × height × width × 3</cell></row><row><cell>TGAN -SVC -Original [27]</cell><cell>11.85 ± .07</cell><cell></cell><cell>c0</cell></row><row><cell>TGAN -Normal</cell><cell>8.98 ± .06</cell><cell>10093.02 ± 11.06</cell><cell></cell></row><row><cell>TGAN -Normal -Original [27]</cell><cell>9.18 ± .11</cell><cell></cell><cell></cell></row><row><cell>TGAN -SVC -2xTime</cell><cell>13.28 ± .09</cell><cell>8797.95 ± 10.39</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6</head><label>6</label><figDesc>Performance on the UCF-101 dataset for discriminators using the Temporal Shift Modules at different layer depths</figDesc><table><row><cell></cell><cell cols="2">(a) TGAN -Temporal Shifting</cell><cell></cell><cell cols="3">(b) MoCoGAN -Temporal Shifting</cell><cell></cell></row><row><cell>Model</cell><cell cols="3">Inception Score↑ TRV Params</cell><cell>Model</cell><cell cols="3">Inception Score↑ TRV Params</cell></row><row><cell>TGAN [27]</cell><cell>11.85 ± .07</cell><cell>22</cell><cell>11M</cell><cell>MoCoGAN [33]</cell><cell>12.42 ± .03</cell><cell>22</cell><cell>3.3M</cell></row><row><cell>TGAN Ours</cell><cell>11.93 ± .08</cell><cell>22</cell><cell>11M</cell><cell>MoCoGAN Ours</cell><cell>11.58 ± .04</cell><cell>22</cell><cell>3.3M</cell></row><row><cell>Layer: c0</cell><cell>11.76 ± .06</cell><cell>3</cell><cell>2.8M</cell><cell>Layer: c0</cell><cell>8.90 ± .05</cell><cell>3</cell><cell>0.7M</cell></row><row><cell>Layer: c0-c1</cell><cell>12.11 ± .12</cell><cell>5</cell><cell>2.8M</cell><cell>Layer: c0-c1</cell><cell>9.82 ± .07</cell><cell>5</cell><cell>0.7M</cell></row><row><cell>Layer: c0-c2</cell><cell>12.32 ± .07</cell><cell>7</cell><cell>2.8M</cell><cell>Layer: c0-c2</cell><cell>9.60 ± .02</cell><cell>7</cell><cell>0.7M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7</head><label>7</label><figDesc>Inception Score of the UCF-101 training dataset under different conditions</figDesc><table><row><cell>Inception Score</cell><cell>Resolution</cell><cell cols="2">2x Subsampling Random Video Reversal</cell><cell>Crop</cell></row><row><cell>59.61 ± .21</cell><cell>16 × 64 × 64 × 3</cell><cell></cell><cell></cell><cell>center</cell></row><row><cell>63.69 ± .32</cell><cell>16 × 64 × 64 × 3</cell><cell>✓</cell><cell></cell><cell>center</cell></row><row><cell>63.58 ± .20</cell><cell>16 × 64 × 64 × 3</cell><cell>✓</cell><cell>✓</cell><cell>center</cell></row><row><cell>61.83 ± .21</cell><cell>16 × 64 × 64 × 3</cell><cell>✓</cell><cell>✓</cell><cell>random</cell></row><row><cell>90.78 ± .21</cell><cell>16 × 128 × 128 × 3</cell><cell></cell><cell></cell><cell>center</cell></row><row><cell>91.20 ± .14</cell><cell>16 × 128 × 128 × 3</cell><cell>✓</cell><cell></cell><cell>center</cell></row><row><cell>90.55 ± .10</cell><cell>16 × 128 × 128 × 3</cell><cell>✓</cell><cell>✓</cell><cell>center</cell></row><row><cell>89.73 ± .20</cell><cell>16 × 128 × 128 × 3</cell><cell>✓</cell><cell>✓</cell><cell>random</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8</head><label>8</label><figDesc>Performance on the 64x64 UCF-101 video generation benchmark</figDesc><table><row><cell>Model</cell><cell>Inception Score ↑</cell><cell>FID ↓</cell><cell>Parameter Reduction ↑</cell></row><row><cell>VGAN [34] (2016)</cell><cell>8.31 ± .09</cell><cell></cell><cell></cell></row><row><cell>TGAN [27] (2017)</cell><cell>11.85 ± .07</cell><cell></cell><cell></cell></row><row><cell>MoCoGAN [33] (2018)</cell><cell>12.42 ± .03</cell><cell></cell><cell></cell></row><row><cell>TGAN (our reproduction)</cell><cell>11.93 ± .08</cell><cell>9127.80 ± 13.77</cell><cell></cell></row><row><cell>TGAN-TSM</cell><cell>12.32 ± .07</cell><cell>9796.68 ± 2.29</cell><cell>74.93%</cell></row><row><cell>TGAN-F</cell><cell>13.62 ± .06</cell><cell>8942.63 ± 3.72</cell><cell>74.19%</cell></row><row><cell>MoCoGAN (our reproduction)</cell><cell>11.58 ± .04</cell><cell>9485.34 ± 14.61</cell><cell></cell></row><row><cell>MoCoGAN-TSM</cell><cell>9.82 ± .07</cell><cell>10608.66 ± 15.67</cell><cell>79.99%</cell></row><row><cell>MoCoGAN-F</cell><cell>11.60 ± .05</cell><cell>9424.01 ± 12.16</cell><cell>69.61%</cell></row><row><cell>MoCoGAN-F + SN</cell><cell>12.33 ± .09</cell><cell>9069.11 ± 3.97</cell><cell>69.61%</cell></row><row><cell>MoCoGAN + TGAN-F Discriminator</cell><cell>12.53 ± .01</cell><cell>9038.42 ± 9.21</cell><cell>15.16%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 9</head><label>9</label><figDesc>Performance on the UCF-101 benchmark for high-resolution video generation</figDesc><table><row><cell>Model</cell><cell>Resolution</cell><cell>Batch Size</cell><cell>Compute</cell><cell>IS↑</cell><cell>FID↓</cell><cell>Params</cell></row><row><cell>ProVGAN [1] (2018)</cell><cell>32×256×256</cell><cell></cell><cell>Multi-GPU</cell><cell>13.59</cell><cell></cell><cell></cell></row><row><cell cols="2">ProVGAN + SWGAN [1] (2018) 32×256×256</cell><cell></cell><cell>Multi-GPU</cell><cell>14.56</cell><cell></cell><cell></cell></row><row><cell>TGANv2 [28] (2018)</cell><cell>16×192×192</cell><cell>64</cell><cell>4 GPUs</cell><cell>22.70 ± .19</cell><cell></cell><cell>200M</cell></row><row><cell>TGAN-F</cell><cell>16×128×128</cell><cell>32</cell><cell>1 GPU</cell><cell>16.85 ± .04</cell><cell>8797 ± 7</cell><cell>16M</cell></row><row><cell>TGAN-F + 4xTemporalCh</cell><cell>16×128×128</cell><cell>32</cell><cell>1 GPU</cell><cell cols="2">17.72 ± .20 8361 ± 11</cell><cell>27M</cell></row><row><cell>TGAN-F + 2xImageCh</cell><cell>16×128×128</cell><cell>32</cell><cell>1 GPU</cell><cell cols="2">20.35 ± .23 7817 ± 10</cell><cell>25M</cell></row><row><cell>TGAN-F + 2xTime</cell><cell>16×128×128</cell><cell>32</cell><cell>1 GPU</cell><cell cols="2">17.23 ± .15 8444 ± 18</cell><cell>16M</cell></row><row><cell>TGAN-F + All</cell><cell>16×128×128</cell><cell>32</cell><cell>1 GPU</cell><cell cols="2">22.91 ± .19 8016 ± 17</cell><cell>70M</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">This includes all operations associated with deep learning convolutions; e.g. padding, dilation, etc</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Towards high resolution video generation with progressive growing of sliced wasserstein gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Acharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Paudel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.02419</idno>
		<ptr target="http://arxiv.org/abs/1810.02419" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The MUG facial expression database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Aifanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Papachristou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Delopoulos</surname></persName>
		</author>
		<ptr target="http://ieeexplore.ieee.org/document/5617662/" />
	</analytic>
	<monogr>
		<title level="m">11th International Workshop on Image Analysis for Multimedia Interactive Services, WIAMIS 2010</title>
		<meeting><address><addrLine>Desenzano del Garda, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-04-12" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v70/arjovsky17a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08-11" />
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The mechanics of n-player differentiable games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Racaniere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Foerster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tuyls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Graepel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pmlr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stockholm</forename><surname>Stockholmsmãďssan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sweden</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v80/balduzzi18a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<editor>Dy, J., Krause, A.</editor>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="354" to="363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A note on the inception score</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Barratt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sharma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.01973</idno>
		<ptr target="http://arxiv.org/abs/1801.01973" />
	</analytic>
	<monogr>
		<title level="m">ICML Workshop on Theoretical Foundations and Applications of Deep Generative Models</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Pros and cons of gan evaluation measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cviu.2018.10.009</idno>
		<ptr target="https://doi.org/10.1016/j.cviu.2018.10.009" />
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">179</biblScope>
			<biblScope unit="page" from="41" to="65" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Large scale GAN training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.11096</idno>
		<ptr target="https://openreview.net/forum?id=B1xsqj09Fm" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1179</idno>
		<ptr target="https://www.aclweb.org/anthology/D14-1179" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Spatiotemporal residual networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Wildes</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/6433-spatiotemporal-residual-networks-for-video-action-recognition" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12-05" />
			<biblScope unit="page" from="3468" to="3476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5423-generative-adversarial-nets" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27</title>
		<editor>Weinberger, K.Q.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=3295222.3295327" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<publisher>Curran Associates Inc., USA</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5769" to="5779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1706.08500" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-09" />
			<biblScope unit="page" from="6629" to="6640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The relativistic discriminator: a key element missing from standard GAN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jolicoeur-Martineau</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=S1erHoR5t7" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Progressive growing of GANs for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Hk99zCeAb" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1812.04948" />
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08383</idno>
		<ptr target="http://arxiv.org/abs/1811.08383" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep learning via hessian-free optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=3104322.3104416" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on International Conference on Machine Learning</title>
		<meeting>the 27th International Conference on International Conference on Machine Learning<address><addrLine>Omnipress, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="735" to="742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Which training methods for GANs do actually converge?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pmlr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stockholm</forename><surname>Stockholmsmãďssan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sweden</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v80/mescheder18a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<editor>Dy, J., Krause, A.</editor>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3481" to="3490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoshida</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=B1QRgziT-" />
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Gradient descent gan optimization is locally stable</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/7142-gradient-descent-gan-optimization-is-locally-stable.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5585" to="5595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Zur theorie der gesellschaftsspiele</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Neumann</surname></persName>
		</author>
		<idno type="DOI">10.1007/BF01448847</idno>
		<idno>doi:10.1007/BF01448847</idno>
		<ptr target="https://doi.org/10.1007/BF01448847" />
	</analytic>
	<monogr>
		<title level="j">Mathematische Annalen</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page" from="295" to="320" />
			<date type="published" when="1928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">f-gan: Training generative neural samplers using variational divergence minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cseke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tomioka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/6066-f-gan-training-generative-neural-samplers-using-variational-divergence-minimization.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="271" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fast exact multiplication by the hessian</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Pearlmutter</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1994.6.1.147</idno>
		<idno>doi:10.1162/neco.1994.6.1.147</idno>
		<ptr target="http://dx.doi.org/10.1162/neco.1994.6.1.147" />
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="147" to="160" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal representation with pseudo-3d residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2017.590</idno>
		<idno>doi:10.1109/ICCV.2017.590</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2017.590" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-10-22" />
			<biblScope unit="page" from="5534" to="5542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1511.06434" />
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Learning Representations, ICLR 2016</title>
		<meeting><address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-05-02" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Ill-conditioning in neural network training problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saarinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bramley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cybenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Scientific Computing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="693" to="714" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Temporal generative adversarial nets with singular value clipping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matsumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saito</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2017.308</idno>
		<idno>doi:10.1109/ICCV.2017.308</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2017.308" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-10-22" />
			<biblScope unit="page" from="2849" to="2858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Tganv2: Efficient training of large models for video generation with multiple subsampling layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saito</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.09245</idno>
		<ptr target="http://arxiv.org/abs/1811.09245" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/6125-improved-techniques-for-training-gans" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12-05" />
			<biblScope unit="page" from="2226" to="2234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">UCF101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<ptr target="http://arxiv.org/abs/1212.0402" />
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Human action recognition using factorized spatio-temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Shi</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2015.522</idno>
		<idno>doi:10.1109/ICCV.2015.522</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2015.522" />
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision, ICCV 2015</title>
		<meeting><address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-07" />
			<biblScope unit="page" from="4597" to="4605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00675</idno>
		<ptr target="http://openaccess.thecvf.com/content_cvpr_2018/html/Tran_A_Closer_Look_CVPR_2018_paper.html" />
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-18" />
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Mocogan: Decomposing motion and content for video generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00165</idno>
		<ptr target="http://openaccess.thecvf.com/content_cvpr_2018/html/Tulyakov_MoCoGAN_Decomposing_Motion_CVPR_2018_paper.html" />
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-18" />
			<biblScope unit="page" from="1526" to="1535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Generating videos with scene dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/6194-generating-videos-with-scene-dynamics" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12-05" />
			<biblScope unit="page" from="613" to="621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01267-0_19</idno>
		<idno>doi:10.1007/978-3-030-01267-0\_19</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-01267-0_19" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018 -15th European Conference</title>
		<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-09-08" />
			<biblScope unit="page" from="318" to="335" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XV</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Self-attention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1805.08318" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<editor>Chaudhuri, K., Salakhutdino, R.</editor>
		<meeting>the 36th International Conference on Machine Learning<address><addrLine>PMLR, Long Beach, California USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
