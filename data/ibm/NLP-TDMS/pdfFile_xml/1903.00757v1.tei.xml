<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GraphVite: A High-Performance CPU-GPU Hybrid System for Node Embedding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaocheng</forename><surname>Zhu</surname></persName>
							<email>zhaocheng.zhu@umontreal.ca</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhen</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Qu</surname></persName>
							<email>meng.qu@umontreal.ca</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
							<email>jian.tang@hec.ca</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Mila -Québec AI Institute Université de Montréal</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Mila -Québec AI Institute</orgName>
								<orgName type="institution">Université de Montréal</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Mila -Québec AI Institute HEC Montréal CIFAR AI Research Chair</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<address>
									<settlement>San Francisco</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">GraphVite: A High-Performance CPU-GPU Hybrid System for Node Embedding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3308558.3313508</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>CCS CONCEPTS • Computing methodologies → Machine learning; Parallel al- gorithms</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning continuous representations of nodes is attracting growing interest in both academia and industry recently, due to their simplicity and effectiveness in a variety of applications. Most of existing node embedding algorithms and systems are capable of processing networks with hundreds of thousands or a few millions of nodes. However, how to scale them to networks that have tens of millions or even hundreds of millions of nodes remains a challenging problem. In this paper, we propose GraphVite, a high-performance CPU-GPU hybrid system for training node embeddings, by co-optimizing the algorithm and the system. On the CPU end, augmented edge samples are parallelly generated by random walks in an online fashion on the network, and serve as the training data. On the GPU end, a novel parallel negative sampling is proposed to leverage multiple GPUs to train node embeddings simultaneously, without much data transfer and synchronization. Moreover, an efficient collaboration strategy is proposed to further reduce the synchronization cost between CPUs and GPUs. Experiments on multiple real-world networks show that GraphVite is super efficient. It takes only about one minute for a network with 1 million nodes and 5 million edges on a single machine with 4 GPUs, and takes around 20 hours for a network with 66 million nodes and 1.8 billion edges. Compared to the current fastest system, GraphVite is about 50 times faster without any sacrifice on performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Networks are ubiquitous in the real world. Examples like social networks <ref type="bibr" target="#b18">[19]</ref>, citation networks <ref type="bibr" target="#b26">[27]</ref>, protein-protein interaction networks <ref type="bibr" target="#b29">[30]</ref> and many more cover a wide range of applications. In network analysis, it is critical to have effective representations for nodes, as these representations largely determine the performance of many downstream tasks. Recently, there is a growing interest in unsupervised learning of continuous node representations, which is aimed at preserving the structure of networks in a low-dimensional space. This kind of approaches has been proven successful in various applications, such as node classification <ref type="bibr" target="#b22">[23]</ref>, link prediction <ref type="bibr" target="#b13">[14]</ref>, and network visualization <ref type="bibr" target="#b30">[31]</ref>.</p><p>Many works have been proposed on this stream, including Deep-Walk <ref type="bibr" target="#b22">[23]</ref>, LINE <ref type="bibr" target="#b31">[32]</ref>, and node2vec <ref type="bibr" target="#b7">[8]</ref>. These methods learn effective node embeddings by predicting the neighbors of each node and can be efficiently optimized by asynchronous stochastic gradient descent (ASGD) <ref type="bibr" target="#b24">[25]</ref>. On a single machine with multi-core CPUs, they are capable of processing networks with one or a few millions of nodes. Given that real-world networks easily go to tens of millions nodes and nearly billions of edges, how to adapt node embedding methods to networks of such large scales remains very challenging. One may think of exploiting computer clusters for training large-scale networks. However, it is a non-trivial task to extend existing methods to distributed settings. Even if distributed algorithms are available, the cost of large CPU clusters is still prohibitive for many users. Therefore, we are wondering whether it is possible to scale node embedding methods to very large networks on a single machine, which should be particularly valuable for common users. Inspired by the recent success of training deep neural networks with GPUs <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b12">13]</ref>, we would like to utilize such highly parallel hardware to accelerate the training of node embeddings. However, directly adopting GPUs for node embedding could be inefficient, since the sampling procedure in node embedding requires excessive arXiv:1903.00757v1 <ref type="bibr">[cs.</ref>LG] 2 Mar 2019 random memory access on the network structure, which is at the disadvantage of GPUs. Compared to GPUs, CPUs are much more capable of performing random memory access. Therefore, it would be wise to use both CPUs and GPUs for training node embeddings. Along this direction, a straightforward solution is to follow the mini-batch stochastic gradient descent (mini-batch SGD) paradigm utilized in existing deep learning frameworks (e.g. TensorFlow <ref type="bibr" target="#b0">[1]</ref> and PyTorch <ref type="bibr" target="#b21">[22]</ref>). Different from deep neural networks, the training of node embeddings involves much more memory access per computation. As a result, mini-batch SGD would suffer from severe memory latency on the bus before it benefits from fast GPU computation. Therefore, other than mini-batch SGD, we need to design a system that leverages distinct advantages of CPUs and GPUs and uses them collaboratively to train node embeddings efficiently.</p><p>Overall, the main challenges of building an efficient node embedding system with GPUs are:</p><p>(1) Limited GPU Memory The parameter matrices of node embeddings are quite large while the memory of a single GPU is very small. Modern GPUs usually have a capacity of 12GB or 16GB. (2) Limited Bus Bandwidth The bandwidth of the bus is much slower than the computation speed of GPUs. There will be severe latency if GPUs exchange data with the main memory frequently. (3) Large Synchronization Cost A lot of data are transferred between CPUs and GPUs. Both the CPU-GPU or inter-GPU synchronizations are very costly.</p><p>In this paper, we propose a high-performance CPU-GPU hybrid system called GraphVite for training node embeddings on largescale networks. GraphVite takes full advantages of CPUs and GPUs by co-optimizing the node embedding algorithm and the system. Specifically, we observe that existing node embedding methods typically consist of two stages, i.e. network augmentation and embedding training. In the first stage, an augmented network is constructed by random walks on the original network, and positive edges are sampled on the augmented network. In the second stage, node embeddings are trained according to the samples generated in the previous stage. Since the augmentation stage involves excessive random access, we resort to CPUs for this part. The training stage is assigned to GPUs as it is mainly composed of matrix computation.</p><p>In GraphVite, the above challenges are addressed by three components, namely parallel online augmentation, parallel negative sampling and collaboration strategy. In parallel online augmentation, CPUs augment the network with random walks and generate edge samples in an online fashion. In parallel negative sampling, the edge samples are organized into a grid sample pool, where each block corresponds to a subset of the network. Then GPUs iteratively fetch orthogonal blocks and their corresponding embeddings in each episode. Because GPUs do not share any embeddings, multiple GPUs can perform gradient updates with negative sampling in its own subset simultaneously. With such a design, the problem of limited GPU memory is solved as each GPU only stores the subset of node embeddings corresponding to the current sample block. The problem of limited bus bandwidth is mitigated since model parameters are transferred only when GPUs change their blocks. No inter-GPU synchronization is needed and CPU-GPU synchronization is only needed at the end of each episode. The collaboration strategy further reduces the synchronization cost between CPUs and GPUs on the sample pool.</p><p>We evaluate GraphVite on 4 real-world networks of different scales. On a single machine with 4 Tesla P100 GPUs, our system only takes one minute to train a network with 1 million nodes and 5 million edges. Compared to the current fastest system <ref type="bibr" target="#b31">[32]</ref>, GraphVite is 51 times faster and does not sacrifice any performance. On a network with 66 million nodes and 1.8 billion edges, GraphVite takes only around 20 hours to finish training. We also investigate the speed of GraphVite under different hardware configurations. Even on economic GPUs like GeForce GTX 1080, GraphVite is able to achieve a speedup of 29 times compared to the current fastest system. Organization Section 2 reviews existing state-of-the-art node embedding methods and points out the challenges of extending these methods to GPUs. Section 3 introduces our proposed system in details. We present our experiments in Section 4, followed by extensive ablation studies in Section 5. Section 6 summarizes the related work, and we conclude this paper in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES</head><p>In this section, some preliminary knowledge is introduced. We first review existing state-of-the-art node embedding methods, followed by a discussion on the main challenges of extending these methods to GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Node Embedding Methods Review</head><p>Given a network G = (V , E), the goal of node embedding is to learn a low-dimensional representation for each node. The learned embeddings are expected to capture the structure of the network. Towards this goal, most existing methods train node embeddings to distinguish the edges in E (i.e. positive edges) from some randomly sampled node pairs (i.e. negative edges). In other words, edges are essentially utilized as training data. Since many real-world networks are extremely sparse, most existing embedding methods conduct random walks on the original network to introduce more connectivity. Specifically, they connect nodes within a specified distance on a random walk path as additional positive edges. For example, LINE <ref type="bibr" target="#b31">[32]</ref> uses a breadth-first search strategy on low-degree nodes, while DeepWalk <ref type="bibr" target="#b22">[23]</ref> uses a depth-first search strategy for all nodes. Node2vec <ref type="bibr" target="#b7">[8]</ref> developed a mixture of the above two strategies.</p><p>Once the network is augmented, node embeddings are trained on samples from the augmented network. Typically, the embeddings are encoded in two sets, namely vertex embedding matrix and context embedding matrix. For an edge sample (u, v), the dot product of vertex[u] and context[v] is computed to predict whether the sample is a positive edge. This encourages neighbor nodes to have close embeddings, whereas distant nodes will have very different embeddings.</p><p>Overall, the computation procedures of these node embedding methods can be divided into two stages: network augmentation and embedding training. Algorithm 1 summarizes the general framework of existing node embedding methods. Note that the first stage can be easily parallelized, and the second stage can be parallelized via asynchronous SGD. In most existing node embedding systems, these two stages are executed in a sequential order, with each stage parallelized by a bunch of CPU threads.</p><p>Algorithm 1 General framework of node embedding </p><formula xml:id="formula_0">1: E ′ ← E 2: for v ∈ V do ▷ parallelizable 3: for u ∈ Walk(v) do 4: E ′ ← E ′ ∪ {(v, u, Weight(v,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Challenges for Hybrid Node Embedding System</head><p>Inspired by the recent success of training neural networks with GPUs, we are interested in building a node embedding system by leveraging the power of GPUs, which can benefit the embedding training stage. Since the first stage of network augmentation involves extensive memory random access, CPUs are more suitable for this stage. As a result, we desire to develop a hybrid CPU-GPU system for training node embeddings. A common approach for a hybrid machine learning system is the mini-batch SGD paradigm, which is widely adopted in existing deep learning frameworks such as TensorFlow <ref type="bibr" target="#b0">[1]</ref> and PyTorch <ref type="bibr" target="#b21">[22]</ref>. In mini-batch SGD, model parameters are stored on GPUs and training data is iteratively passed to GPUs in batches. However, mini-batch SGD cannot be applied directly to node embedding on large networks. Take a scale-free network with 50 million nodes and 1 billion edges as an example. (1) The size of the augmented network goes to 373 GB large, which may overwhelm the memory of most servers. We need to figure out a way to generate the augmented network and edge samples on the fly. (2) The embedding matrices are much larger than the parameter matrices in deep neural networks. Either vertex or context matrix consumes 23.8 GB memory, which is beyond the memory limit of any single GPU. As a result, both the vertex and context embedding matrices have to be stored in the main memory, and transferred to the GPUs in small parts during training. See <ref type="table">Table 1</ref>   <ref type="table">Table 1</ref>: Memory cost of node embedding on a scale-free network with 50 million nodes and 1 billion edges.</p><p>system will be bounded by the speed of parameter transfer from CPUs to GPUs severely. Indeed, such a system is even worse than its CPU parallel counterpart, which is verified in our experiments (see <ref type="table" target="#tab_7">Table 3</ref>). Another challenge in a hybrid system is the large synchronization cost. Since the system is distributed on multiple CPUs and GPUs, there is necessary data (e.g. parameters and edge samples) shared across sub tasks. A trivial but safe solution is to synchronize the shared data frequently, which will result in huge synchronization cost. To achieve high speed performance, the system should reduce shared data as much as possible, and use a collaboration strategy to minimize synchronization cost between devices.  The two stages are executed asynchronously with our collaboration strategy.</p><p>In this section, we introduce a high-performance hybrid CPU-GPU system called GraphVite for training node embeddings. Our system leverages distinct advantages of CPUs and GPUs and addresses the above three challenges. Specifically, we propose a parallel online augmentation for efficient network augmentation on CPUs. We introduce a parallel negative sampling to cooperate multiple GPUs for embedding training. A collaboration strategy is also proposed to reduce the synchronization cost between CPUs and GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Parallel Online Augmentation</head><p>As discussed in Section 2.1, the first stage of node embedding methods is to augment the original network with random walks. Since the augmented network is usually one or two magnitude larger than the original one, it is impossible to load it into the main memory if the original network is already very large. Therefore, we introduce a parallel online augmentation, which generates augmented edge samples on the fly without explicit network augmentation. Our method can be viewed as an online extension of the augmentation and edge sampling method used in LINE <ref type="bibr" target="#b31">[32]</ref>. First, we draw a departure node with the probability proportional to the degree of each node. Then we perform a random walk from the departure node, and pick node pairs within a specific augmentation distance s as edge samples. Note that edge samples generated in the same random walk are correlated and may degrade the performance of optimization. Inspired by the experience replay technique widely used in reinforcement learning <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b19">20]</ref>, we collect edge samples into a sample pool, and shuffle the sample pool before transferring it to GPUs for embedding training. The proposed edge sampling method can be parallelized when each thread is allocated with an independent sample pool in advance. Algorithm 2 gives the process of parallel online augmentation in details. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 Parallel Online Augmentation</head><formula xml:id="formula_1">for i ← 0 to num_CPU − 1 do ▷ paralleled 3: pool[i] ← 4:</formula><p>while pool is not full do 5:</p><p>x ← DepartureSampling(G) <ref type="bibr">6:</ref> for u, v ∈ RandomWalkSampling(x) do <ref type="bibr">7:</ref> if Distance(u, v) &lt;= s then <ref type="bibr">8:</ref> pool .append((u, v)) return Concatenate(pool[·]) 15: end function Pseudo Shuffle While shuffling the sample pool is important to optimization, it slows down the network augmentation stage (see <ref type="table">Table 7</ref>). The reason is that a general shuffle consists of lots of random memory access and cannot be accelerated by the CPU cache. The loss in speed will be even worse if the server has more than one CPU socket. To mitigate this issue, we propose a pseudo shuffle technique that shuffles correlated samples in a much more cachefriendly way and improves the speed of the system significantly.</p><p>Note that most correlation comes from edge samples that share the source node or the target node in the same random walk. As such correlation occurs in a group of s samples for an augmentation distance s, we divide the sample pool into s continuous blocks, and scatter correlated samples into different blocks. For each block, we always append samples sequentially at the end, which can benefit a lot from CPU cache. The s blocks are concatenated to form the final sample pool.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Parallel Negative Sampling</head><p>In the embedding training stage, we divide the training task into small fragments and distribute them to multiple GPUs. The sub tasks are necessarily designed with little shared data to minimize the synchronization cost among GPUs. To see how model parameters can be distributed to multiple GPUs without overlap, we first introduce a definition of ϵ-gradient exchangeable.</p><formula xml:id="formula_2">Definition 1. ϵ-gradient exchangeable. A loss function L(X ; θ )</formula><p>is ϵ-gradient exchangeable on two sets of training data X 1 , X 2 if for small ϵ ≥ 0, ∀θ 0 ∈ Θ and ∀α ∈ R + , exchanging the order of two gradient descent steps results in a vector difference with norm no more than ϵ.</p><formula xml:id="formula_3">θ 1 ← θ 0 − α ∇L(X 1 ; θ 0 ) θ 2 ← θ 1 − α ∇L(X 2 ; θ 1 ) (1) θ ′ 1 ← θ 0 − α ∇L(X 2 ; θ 0 ) θ ′ 2 ← θ ′ 1 − α ∇L(X 1 ; θ ′ 1 )<label>(2)</label></formula><p>i.e. ∥θ 2 − θ ′ 2 ∥ ≤ ϵ is true for the above equations. Particularly, we abbreviate 0-gradient exchangeable to gradient exchangeable. Due to the sparse nature of node embedding training, there are many sets that form gradient exchangeable pairs in the network. For example, for two edge sample sets X 1 , X 2 ⊆ E, if they do not share any source nodes or target nodes, X 1 and X 2 are gradient exchangeable. Even if X 1 and X 2 share some nodes, they can still be ϵ-gradient exchangeable if the learning rate α and the number of iterations are bounded.</p><p>Based on the gradient exchangeability observed in node embedding, we propose a parallel negative sampling algorithm for the embedding training stage. For n GPUs, we partition rows of vertex and context into n partitions respectively (see the top-left corner of <ref type="figure" target="#fig_3">Figure 2</ref>). This results in an n × n partition grid for the sample pool, where each edge belongs to one of the blocks. In this way, any pair of blocks that does not share row or column is gradient exchangeable. Blocks in the same row or column are ϵ-gradient exchangeable, as long as we restrict the number of iterations on each block.</p><p>We define episode as the block-level step used in parallel negative sampling. During each episode, we send n orthogonal blocks and their corresponding vertex and context partitions to n GPUs respectively. Each GPU then updates its own embedding partitions with ASGD. Because these blocks are mutually gradient exchangeable and do not share any row in the parameter matrices, multiple GPUs can perform ASGD concurrently without any synchronization. At the end of each episode, we gather the updated parameters from all GPUs and assign another n orthogonal blocks. Here ϵgradient exchangeable is controlled by the number of total samples in n orthogonal blocks, which we define as episode size. The smaller episode size, the better ϵ-gradient exchangeable we will have for embedding training. However, smaller episode size will also induce more frequent synchronization. Hence the episode size is tuned so that there is a good trade off between the speed and ϵ-gradient exchangeable (see Section 5.3). <ref type="figure" target="#fig_3">Figure 2</ref> gives an example of parallel negative sampling with 4 partitions. Typically, node embedding methods sample negative edges from all possible nodes. However, it could be very time-consuming if GPUs have to communicate with each other to get the embeddings of their negative samples. To avoid this cost, we restrict that negative samples can only be drawn from the context rows on the current GPU. Though this seems a little problematic, we find it works well in practice. An intuitive explanation is that with parallel online augmentation, every node is likely to have positive samples with nodes from all context partitions. As a result, every node can potentially form negative samples with all possible nodes.</p><p>Note that although we demonstrate with the number of partitions equal to n, the parallel negative sampling can be easily generalized to cases with any number of partitions greater than n, simply by processing the orthogonal blocks in subgroups of n during each episode. Algorithm 3 illustrates the hybrid system for multiple GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Collaboration Strategy</head><p>Our parallel negative sampling enables different GPUs to train node embeddings concurrently, with only synchronization required between episodes. However, it should be noticed that the sample pool is also shared between CPUs and GPUs. If they synchronize on the sample pool, then only workers of the same stage can access the pool at the same time, which means hardware is idle for half of the time. To eliminate this problem, we propose a collaboration strategy to reduce the synchronization cost. We allocate two sample pools in the main memory, and let CPUs and GPUs always work on different vertex_partitions ← Partition(vertex) <ref type="bibr">3:</ref> context_partitions ← Partition(context) <ref type="bibr">4:</ref> while not converge do <ref type="bibr">5:</ref> pool ← ParallelOnlineAugmentation(num_CPU ) <ref type="bibr">6:</ref> block[·][·] ← Redistribute(pool) <ref type="bibr">7:</ref> for offset ← 0 to num_GPU − 1 do <ref type="bibr">8:</ref> for i ← 0 to num_GPU − 1 do ▷ paralleled <ref type="bibr">9:</ref> vid ← i 10:</p><p>cid ← (i + offset) mod num_GPU <ref type="bibr">11:</ref> send vertex_partitions[vid] to GPU i <ref type="bibr">12:</ref> send context_partitions[cid] to GPU i end while 19: end function pools. CPUs first fill up a sample pool and pass it to GPUs. After that, parallel online augmentation and parallel negative sampling are performed concurrently on CPUs and GPUs respectively. The two pools are swapped when CPUs fill up a new pool. <ref type="figure" target="#fig_1">Figure 1</ref> illustrates this procedure. With the collaboration strategy, the synchronization cost between CPUs and GPUs is reduced and the speed of our hybrid system is almost doubled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Discussion</head><p>Here we further discuss some practical details of our hybrid system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Batched Transfer</head><p>In parallel negative sampling, the sample pool is assigned to GPUs by block, which is sometimes very large for the  memory of a GPU. Instead of copying the whole sample block to a GPU, we transfer the sample block by a small granularity. In this way, the memory cost of edge samples on GPUs becomes negligible.</p><p>Bus Usage Optimization When the number of partitions equals the number of GPUs, we can further optimize the bus usage by fixing the context partition for each GPU. In this way, we save the transfer of context matrix and further reduce the synchronization cost between CPUs and GPUs.</p><p>Single GPU Case Although parallel negative sampling is proposed for multiple GPUs, our hybrid system is compatible with a single GPU. Typically a GPU can hold at most 12 million node embeddings. So a single GPU is sufficient for training node embeddings on networks that contain no more than 12 million nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we verify the effectiveness and efficiency of GraphVite. We first evaluate our system on Youtube, which is a large network widely used in the literature of node embeddings. Then we evaluate GraphVite on three larger datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We use the following datasets in our experiments. Statistics of these networks are summarized in <ref type="table" target="#tab_5">Table 2</ref>.</p><p>• Youtube <ref type="bibr" target="#b18">[19]</ref> is a large-scale social network in the Youtube website. It contains 1 million nodes and 5 million edges. For some of the nodes, they have labels that represent the type of videos users enjoy. • Friendster-small <ref type="bibr" target="#b36">[37]</ref> is a sub-graph induced by all the labeled nodes in Friendster. It has 8 million nodes and 447 million edges. The node labels in this network are the same as those in Friendster. • Hyperlink-PLD <ref type="bibr" target="#b15">[16]</ref> is a hyperlink network extracted from the Web corpus 1 . We use the pay-level-domain aggregated version of the network. It has 43 million nodes and 623 million edges. This dataset does not contain any label. • Friendster <ref type="bibr" target="#b36">[37]</ref> is a very large social network in an online gaming site. It has 65 million nodes and 1.8 billion edges. Some nodes have labels that represent the group users join.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Compared Systems</head><p>We compare GraphVite with the following node embedding systems.</p><p>• LINE [32] 2 is a CPU parallel system based on C++. We parallel its network augmentation stage for fair comparison with other methods. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementation Details</head><p>Our implementation generally follows the open source codes of LINE 2 and DeepWalk 3 . We adopt the asynchronous SGD <ref type="bibr" target="#b24">[25]</ref> in GPU training, and leverage the on-chip shared memory of GPU for fast forward and backward propagation. We also utilize the alias table trick <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b31">32]</ref> to boost parallel online augmentation and parallel negative sampling.</p><p>Our hyperparameters are set according to the settings in LINE <ref type="bibr" target="#b31">[32]</ref> and DeepWalk <ref type="bibr" target="#b22">[23]</ref>. We treat networks as undirected graphs. During the network augmentation stage, we sample random walks with a length of 40 edges. We use a degree-guided strategy to partition both vertex and context matrices. More specifically, we first sort nodes by their degrees and then assign them into different partitions in a zig-zag fashion, as illustrated in <ref type="figure" target="#fig_4">Figure 3</ref>. We tune the episode size to maximize the speed of our hybrid system. During the embedding training stage, negative samples are sampled with a probability proportional to the 3/4 power of the node degrees. For each positive sample, we draw 1 negative sample and scale the gradient of the negative sample by 5 to match the gradient scale in LINE. We follow the initial learning rate of 0.025 and the linear learning rate decay mechanism in LINE and DeepWalk. We only adopt the O3 optimization in g++ and nvcc. We do not use any non-standard optimizations or low precision training <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b38">39]</ref>, though they may further improve the speed of our system.   <ref type="table" target="#tab_10">Table 4</ref>: Results of node classification on Youtube number of training epochs is set to 4,000. For the other 3 datasets, the length of random walks is set to 2, and the total number of training epochs is set to 2,000 since they are denser. The dimension of node embeddings is set to 128 except on Friendster, where we use 96. For other hyperparameters, we follow their default values in previous works. For fair comparison, we report the training time of all methods with the same number of training epochs. We parallel the network augmentation in LINE. For DeepWalk, we store the random walks in memory, which is the fastest setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results on Youtube</head><p>We first evaluate our hybrid system on the widely-used Youtube dataset. We compare the speed and performance of GraphVite with existing systems of node embedding. For existing systems, we replicate their parallel implementations and report their training time under the same number of training epochs. <ref type="table" target="#tab_7">Table 3</ref> presents the speed of different systems. Among all existing systems, LINE <ref type="bibr" target="#b31">[32]</ref> takes the minimal total time to run. However, the GPU implementation of LINE in OpenNE is even worse than its CPU counterpart, possibly due to the mini-batch SGD paradigm it uses. Compared to the current fastest system, LINE, GraphVite is much more efficient. With 4 GPUs, our system finishes training node emebdding on a million-scale network in only one and a half minutes. Even on a single GPU, GraphVite takes no more than 4 minutes and is still 19 times faster than LINE.</p><p>One may be curious about the performance of node embeddings learned by GraphVite. Therefore, we compare the performance of GraphVite with existing systems on the standard task of multi-label node classification. Note that normalizing the embeddings or not yields different trade off between Micro-F1 and Macro-F1 metrics. For fair comparison, we follow the practice in <ref type="bibr" target="#b31">[32]</ref> and train one-vsrest linear classifiers over the normalized node embeddings. <ref type="table" target="#tab_10">Table 4</ref> summarizes the performance over different percentages of training data. It is observed that GraphVite achieves the best or competitive results in most settings, showing that GraphVite does not sacrifice any performance. In some small percentage cases, GraphVite falls a little behind DeepWalk. This is because GraphVite uses negative sampling for optimization, while DeepWalk uses both hierarchical softmax and negative sampling, which could be more robust to few labeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Results on Larger Datasets</head><p>GraphVite Friendster-small Hyperlink-PLD Friendster     <ref type="table">Table 6</ref>: Ablation of main components in GraphVite. Note that the baseline has the same GPU implementation with GraphVite and parallel edge sampling on CPU. The baseline should be regarded as a very strong one.</p><p>To demonstrate the scalability of GraphVite, we further test GraphVite on three larger networks. We learn the node embeddings of Friendster-small with 1 GPU and 4 GPUs. For Hyperlink-PLD and Friendster, since their embedding matrices cannot fit into the memory of a single GPU, we only evaluate them with 4 GPUs. <ref type="table" target="#tab_9">Table 5</ref> gives the training time of GraphVite on these datasets. The training time of baseline systems is not reported here, as all existing systems cannot solve such large networks in a week, except LINE <ref type="bibr" target="#b31">[32]</ref> on Friendster-small. Compared to them, GraphVite takes less than 1 day to train node embeddings on the largest dataset Friendster with 1.8 billion edges, showing that GraphVite can be an efficient tool for analyzing billion-scale networks.</p><p>We also evaluate the performance of the node embeddings on these datasets. For Friendster-small and Friendster, we test their node embeddings on multi-label node classification. The test set is built on the top-100 communities of Friendster and has a total of 39,679 nodes. We do not normalize the learned embeddings during evaluation, and report Macro-F1 and Micro-F1 based on 2% labeled data. For Hyperlink-PLD, we adopt link prediction as the evaluation task since node labels are not available. We randomly exclude 0.01% edges from the training set, and combine them with the same number of uniformly sampled negative edges to form a test set. Each edge sample is scored by the cosine similarity of two node embeddings. We report the AUC metric for link prediction. <ref type="figure">Figure 4</ref> presents the performance of GraphVite over different training epochs on these datasets. On Friendster-small, we also plot the performance of LINE for reference. Due to the long training time, we only report the performance of LINE by the end of all training epochs. It is observed that GraphVite converges on all these datasets. On the Friendster-small dataset, GraphVite significantly outperforms LINE. On the Hyperlink-PLD, we get an AUC of 0.943. On Friendster, the Micro-F1 reaches about 81.0%. All the above observations verify the performance of our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">ABLATION STUDY</head><p>To have a more comprehensive understanding of different components in GraphVite, we conduct several ablation experiments. For intuitive comparison, we evaluate these experiments on the standard Youtube dataset. We only report performance results based on 2% labeled data due to space limitation. All the speedup ratios are computed with respect to LINE <ref type="bibr" target="#b31">[32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">What is the contribution of each main component?</head><p>In the GraphVite, parallel online augmentation, parallel negative sampling, and the collaboration strategy are the main components in the sytem. Here we study how these components contribute to the performance of our system. We compare GraphVite with a strong baseline system with single GPU. Specifically, the baseline has the same GPU implementation as GraphVite, while it uses the standard parallel edge sampling instead of parallel online augmentation, and executes two stages sequentially. <ref type="table">Table 6</ref> shows the results of this ablation. Compared to the baseline, we notice that parallel online augmentation helps improve the quality of node embeddings, since it introduces more connectivity to the sparse network. Besides, parallel online augmentation also accelerates the system a little, as it reuses nodes and reduces the amortized cost of each sample. With parallel negative sampling, we are able to employ multiple GPUs for training, and the speed is boosted by about 3 times. Moreover, the collaboration strategy even improves the speed and does not impact the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Is it necessary to perform pseudo shuffle?</head><p>In parallel online augmentation, GraphVite performs pseudo shuffle to decorrelate the augmented edge samples, while some existing systems <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b22">23]</ref> do not shuffle their samples. We compare the proposed pseudo shuffle with three baselines, including no shuffle, a full random shuffle and an index mapping algorithm. The index mapping algorithm preprocesses a random mapping on the indexes of samples and saves the time of computing random variables. <ref type="table">Table 7</ref> gives the results of different shuffle algorithms on a single GPU. It is observed that all shuffle algorithms are about 1 percent better than the no shuffle baseline. However, different shuffle algorithms vary largely in their speed. Compared to the no shuffle baseline, the random shuffle and index mapping algorithms slow down the system by several times, while our pseudo shuffle has only a little overhead. Therefore, we conclude that pseudo shuffle is the best practice considering both speed and performance.  <ref type="table">Table 7</ref>: Results of performance and speed by different shuffle algorithms. The proposed pseudo shuffle algorithm achieves the best trade off between performance and speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">What is a practical choice for episode size?</head><p>In parallel negative sampling, GraphVite relies on the property of gradient exchangeability to ensure its approximation to standard SGD. While the smaller episode size provides better exchangebility, it will increase the frequency of synchronization over rows of the embedding matrices, and thus slows down embedding training. To quantify such influence in speed and performance, we examine our system on 4 GPUs with different episode sizes. <ref type="figure" target="#fig_7">Figure 5</ref> plots the curves of speed and performance with respect to different episode sizes. On the performance side, we notice that the performance of GraphVite is insensitive to the choice of the episode size. Compared to the single GPU baseline, parallel negative sampling achieves competitive or slightly better results, probably due to the regularization effect introduced by partition. On the speed side, larger episode size achieves more speedup since it reduces the amortized burden of the bus. The speed drops at very large episode size, as there becomes only a few episodes in training. Therefore, we choose an episode size of 2 * 10 8 edge samples for Youtube. Generally, the best episode size is proportional to |V |, so one can set the episode size for other networks accordingly. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">What is the speedup w.r.t the numebr of CPUs and GPUs?</head><p>In GraphVite, both online augmentation and negative sampling can be parallelized on multiple CPUs or GPUs, and synchronization is only required between episodes. Therefore, our system should have great scalability. To verify that point, we investigate our system with different number of CPU and GPU. We change the number of GPU from 1 to 4, and vary the number of sampler per GPU from 1 to 5. The effective number of CPU threads is #GPU * (#sampler per GPU + 1) as there is one scheduler thread for each GPU. <ref type="figure" target="#fig_8">Figure 6</ref> plots the speedup ratio with respect to different number of CPUs and GPUs. The speedup ratio almost forms a plane over both variables, showing that our system scales almost linearly to the hardware. Quantitatively, GraphVite achieves a relative speedup of 11× when the hardware is scaled to 20×. The speedup is about half of its theoretical maximum. We believe this is mainly due to the increased synchronization cost, as well as increased load on shared main memory and bus when we use more CPUs and GPUs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Does the hardware configuration matter?</head><p>Up to now, all experiments are conducted on a server with Xeon E5 CPUs and Tesla P100 GPUs. One might wonder whether such a high performance depends on the specific hardware configuration. Therefore, we further test our system on an economic server with Core i7 CPUs and GTX 1080 GPUs. <ref type="table" target="#tab_13">Table 8</ref> compares the results from two configurations. Different hardware does have difference in speed, but the gap is marginal. The time only increases to 1.6× when we move to the economic server. Note that this two configurations are almost the best and the worst in current machine learning servers, so one could expect a running time between these two configurations on his own hardware.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RELATED WORK</head><p>Node embedding has been proven effective in a wide range of applications, such as node classification <ref type="bibr" target="#b9">[10]</ref>, link prediction <ref type="bibr" target="#b13">[14]</ref>, and network visualization <ref type="bibr" target="#b30">[31]</ref>. Many different methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36]</ref> have been proposed to learn node embeddings that preserve the structure of networks from different aspects. Among them, DeepWalk <ref type="bibr" target="#b22">[23]</ref>, LINE <ref type="bibr" target="#b31">[32]</ref> node2vec <ref type="bibr" target="#b7">[8]</ref> and VERSE <ref type="bibr" target="#b32">[33]</ref> are built on either edge or path samples of networks, which makes them the most scalable methods of all. Our work follows this stream and is related to these methods.</p><p>Node Embedding Algorithm Generally, node embedding algorithms consist of two stages, namely network augmentation and embedding training. The network augmentation stage is widely adopted in existing methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b31">32]</ref> to improve the performance of learned embeddings on sparse networks. DeepWalk <ref type="bibr" target="#b22">[23]</ref> and node2vec <ref type="bibr" target="#b7">[8]</ref> augment networks by generating random paths according to different distributions. The edge samples derived by path are correlated in those methods. LINE <ref type="bibr" target="#b31">[32]</ref> directly adds edges to networks and generates independent edge samples. GraRep <ref type="bibr" target="#b3">[4]</ref> and NetMF <ref type="bibr" target="#b23">[24]</ref> take different powers of the adjacency matrix as augmentation. Our parallel online augmentation generates decorrelated edge samples using pseudo shuffle, and thus is close to the augmentation in LINE. However, our augmentation does not need to store the whole augmented network, which saves a lot of disk and memory usage compared to existing methods. In the embedding training stage, most existing node embedding algorithms <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b31">32]</ref> train node embedding with standard negative sampling <ref type="bibr" target="#b17">[18]</ref> in a shared memory space. While there is a parallel word embedding algorithm <ref type="bibr" target="#b28">[29]</ref> that restricts negative sampling within the context partition of each worker, it still needs to transfer rows of embedding matrices between each worker for positive samples. By contrast, our parallel negative sampling trains on orthogonal sample blocks and does not need any transfer between worker during an episode. The most related method is the distributed SGD used in large-scale matrix factorization algorithms <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b39">40]</ref>. These methods divide the input matrix into n × n blocks and factorize orthogonal blocks simultaneously. Different from these methods, our system mainly focuses on the negative sampling technique and is designed for the node embedding task.</p><p>Node Embedding System From the perspective of system, our work belongs to the parallel implementation of node embedding. There are many CPU parallel systems, including DeepWalk 3 , LINE 2 , node2vec 4 and VERSE <ref type="bibr" target="#b5">6</ref> . These systems use asynchronous SGD <ref type="bibr" target="#b24">[25]</ref> in embedding training and exploit multiple CPU threads for acceleration. Due to the limited computation speed of CPUs, such systems cannot scale to ten-million-scale networks without a large CPU cluster. Recently, there are some GPU parallel systems <ref type="bibr" target="#b1">[2]</ref> built on deep learning frameworks like TensorFlow <ref type="bibr" target="#b0">[1]</ref> or PyTorch <ref type="bibr" target="#b21">[22]</ref>. Since existing frameworks are based on mini-batch SGD paradigm, these systems severely suffer the problem of limited bus bandwidth, and are even worse than their CPU counterparts. Compared to them, GraphVite is a hybrid CPU-GPU system that leverages distinct advantages of CPUs and GPUs, and uses them collaboratively to train node embedding, which makes it much faster than either pure CPU or mini-batch-SGD based systems.</p><p>In addition, parallel word embedding systems <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b27">28]</ref> are also very related to our work, since they share similar embedding training and negative sampling steps with node embedding. Among these methods, Wombat <ref type="bibr" target="#b27">[28]</ref> and BlazingText <ref type="bibr" target="#b8">[9]</ref> accelerate training with GPUs. Wombat only supports single GPU. BlazingText can scale to multiple GPUs, but it simply makes a copy of the parameter matrices on each GPU. Our system is more efficient than BlazingText in two aspects. First, our system partition the parameter matrices and consumes less memory on each GPU. Second, our system requires less synchronization cost, as GPUs do not share any rows in the parameter matrices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>In this paper, we present a high-performance CPU-GPU hybrid system for node embedding. Our system extends existing node embedding methods to GPUs and significantly accelerates training node embeddings on a single machine. With parallel online augmentation, GraphVite efficiently utilizes CPU threads to generate augmented edge samples for node embedding training. With parallel negative sampling, GraphVite enables training node embeddings on multiple GPUs without much synchronization. A collaboration strategy is also developed to reduce the synchronization cost between CPUs and GPUs. Experiments on 4 large networks prove that GraphVite significantly outperforms existing systems in speed without sacrifice on performance. In the future, we plan to generalize our system to semi-supervised settings and graph neural networks, such as graph convolutional networks <ref type="bibr" target="#b11">[12]</ref>, graph attention networks <ref type="bibr" target="#b33">[34]</ref>, and neural message passing networks <ref type="bibr" target="#b6">[7]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Overview of our hybrid system. The gray and yellow boxes correspond to the stages of network augmentation and embedding training respectively. The former is performed by parallel online augmentation on CPUs, while the latter is performed by parallel negative sampling on GPUs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1 :</head><label>1</label><figDesc>function ParallelOnlineAugmentation(num_CPU ) 2:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of parallel negative sampling on 4 GPUs. During each episode, GPUs take orthogonal blocks from the sample pool. Each GPU trains embeddings with negative samples drawn from its own context nodes. Synchronization is only needed between episodes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Algorithm 3</head><label>3</label><figDesc>Parallel Negative Sampling 1: function ParallelNegativeSampling(num_GPU ) 2:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Degree-guided node and context partition strategy in the case of 4 partitions.We define a training epoch as training |E| positive edge samples. For Youtube, the length of random walk is set to 5, and the total</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Speed and performance of GraphVite with respect to different episode sizes. The dashed line represents the single GPU baseline without parallel negative sampling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Results of speedup under different number of hardware. It is observed that the speedup is almost linear to the number of CPUs and GPUs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Evaluation Task 47-class node classification 100-class node classification link prediction 100-class node classification</figDesc><table><row><cell>Dataset</cell><cell>Youtube</cell><cell>Friendster-small</cell><cell>Hyperlink-PLD</cell><cell>Friendster</cell></row><row><cell>|V |</cell><cell>1,138,499</cell><cell>7,944,949</cell><cell>39,497,204</cell><cell>65,608,376</cell></row><row><cell>|E|</cell><cell>4,945,382</cell><cell>447,219,610</cell><cell>623,056,313</cell><cell>1,806,067,142</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Statistics of the datasets used in experiments</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Results of time of different systems on Youtube. The preprocessing time refers to all the overhead before training, including network input and offline network augmentation. Note the preprocessing time of OpenNE is not comparable since it does not have the network augmentation stage. The speedup ratio of GraphVite is computed with regard to LINE, which is the current fastest system. 36.70 38.93 40.26 41.08 41.79 42.28 42.70 43.04 43.34 LINE[32]+augmentation 36.78 40.37 42.10 43.25 43.90 44.44 44.83 45.18 45.50 45.67 DeepWalk[32] 39.68 41.78 42.78 43.55 43.96 44.31 44.61 44.89 45.06 45.23 GraphVite 39.19 41.89 43.06 43.96 44.53 44.93 45.26 45.54 45.70 45.86 21.73 25.28 27.36 28.50 29.59 30.43 31.14 31.81 32.32 LINE[32]+augmentation 22.18 27.25 29.87 31.88 32.86 33.73 34.50 35.15 35.76 36.19 DeepWalk[32] 28.39 30.96 32.28 33.43 33.92 34.32 34.83 35.27 35.54 35.86 GraphVite 25.61 29.46 31.32 32.70 33.81 34.59 35.27 35.82 36.14 36.49</figDesc><table><row><cell>Method</cell><cell cols="3">CPU threads GPU</cell><cell cols="2">Training time</cell><cell cols="3">Preprocessing time</cell><cell></cell><cell></cell></row><row><cell>LINE [32]</cell><cell>20</cell><cell>-</cell><cell></cell><cell cols="2">1.24 hrs</cell><cell></cell><cell>17.4 mins</cell><cell></cell><cell></cell><cell></cell></row><row><cell>DeepWalk [23]</cell><cell>20</cell><cell>-</cell><cell></cell><cell cols="2">1.56 hrs</cell><cell></cell><cell>14.2 mins</cell><cell></cell><cell></cell><cell></cell></row><row><cell>node2vec [8]</cell><cell>20</cell><cell>-</cell><cell></cell><cell cols="2">47.7 mins</cell><cell></cell><cell>25.9 hrs</cell><cell></cell><cell></cell><cell></cell></row><row><cell>LINE in OpenNE [2]</cell><cell>1</cell><cell>1</cell><cell></cell><cell cols="2">&gt; 1 day</cell><cell></cell><cell>2.14 mins</cell><cell></cell><cell></cell><cell></cell></row><row><cell>GraphVite</cell><cell>6</cell><cell>1</cell><cell></cell><cell cols="2">3.98 mins(18.7×)</cell><cell></cell><cell>7.37 s</cell><cell></cell><cell></cell><cell></cell></row><row><cell>GraphVite</cell><cell>24</cell><cell>4</cell><cell></cell><cell cols="2">1.46 mins(50.9×)</cell><cell></cell><cell>16.0 s</cell><cell></cell><cell></cell><cell></cell></row><row><cell>% Labeled Nodes</cell><cell>1%</cell><cell>2%</cell><cell>3%</cell><cell>4%</cell><cell>5%</cell><cell>6%</cell><cell>7%</cell><cell>8%</cell><cell>9%</cell><cell>10%</cell></row><row><cell cols="2">Micro-F1(%) 32.98 Macro-F1(%) LINE[32] LINE[32] 17.06</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Results of time on larger datasets. We only evaluate Hyperlink-PLD and Friendster with 4 GPUs since their embedding matrices cannot fit into the memory of a single GPU.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Figure 4 :</head><label>4</label><figDesc>Performance curves of GraphVite on larger datasets. For Friendster, we plot the results of LINE for reference. The other systems cannot solve any of these datasets within a week.</figDesc><table><row><cell></cell><cell cols="6">Parallel Online Parallel Negative Collaboration Micro-F1 Macro-F1 Training time Augmentation Sampling (4 GPUs) Strategy</cell></row><row><cell>Single GPU baseline</cell><cell></cell><cell></cell><cell></cell><cell>35.26</cell><cell>20.38</cell><cell>8.61 mins</cell></row><row><cell></cell><cell>✓</cell><cell></cell><cell></cell><cell>41.48</cell><cell>29.80</cell><cell>6.35 mins</cell></row><row><cell></cell><cell></cell><cell>✓</cell><cell></cell><cell>34.38</cell><cell>19.81</cell><cell>2.66 mins</cell></row><row><cell></cell><cell>✓</cell><cell>✓</cell><cell></cell><cell>41.75</cell><cell>29.30</cell><cell>2.24 mins</cell></row><row><cell>GraphVite</cell><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell>41.89</cell><cell>29.46</cell><cell>1.46 mins</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc>Training time of GraphVite under different hardware configurations. Generally GraphVite may take a time between these two configurations on most hardware.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/phanein/deepwalk 4 https://github.com/aditya-grover/node2vec 5 https://github.com/thunlp/OpenNE</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://github.com/xgfs/verse 7 https://www.computecanada.ca</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>We would like to thank Compute Canada 7 for supporting GPU servers. Jian Tang is supported by the Natural Sciences and Engineering Research Council of Canada and the Canada CIFAR AI Chair Program. We specially thank Wenbin Hou for useful discussions on C++ and GPU programming techniques, and Sahith Dambekodi for proofreading this paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: a system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">OpenNE: An open source toolkit for Network Embedding</title>
		<ptr target="https://github.com/thunlp/OpenNE" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
		<respStmt>
			<orgName>Natural Language Processing Lab at Tsinghua University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">BMF: Block matrix approach to factorization of large scale data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bhavana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nair</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.00444</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Grarep: Learning graph representations with global structural information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaosheng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiongkai</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM International on Conference on Information and Knowledge Management</title>
		<meeting>the 24th ACM International on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="891" to="900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Flexible, high performance convolutional neural networks for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ueli</forename><surname>Dan C Ciresan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Luca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Gambardella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI Proceedings-International Joint Conference on Artificial Intelligence</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">1237</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Largescale matrix factorization with distributed stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Gemulla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Nijkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Haas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sismanis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 17th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="69" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01212</idno>
		<title level="m">Neural message passing for quantum chemistry</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">BlazingText: Scaling and Accelerating Word2Vec using Multiple GPUs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineet</forename><surname>Khare</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Machine Learning on HPC Environments</title>
		<meeting>the Machine Learning on HPC Environments</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.05584</idno>
		<title level="m">Representation learning on graphs: Methods and applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shihao</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadathur</forename><surname>Satish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Dubey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.06172</idno>
		<title level="m">Parallelizing word2vec in multi-core and many-core architectures</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The link-prediction problem for social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Liben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Nowell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American society for information science and technology</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="1019" to="1031" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Reinforcement learning for robots using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long-Ji</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
		<respStmt>
			<orgName>Carnegie-Mellon Univ Pittsburgh PA School of Computer Science</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The graph structure in the web: Analyzed on different aggregation levels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Meusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastiano</forename><surname>Vigna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Lehmberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Bizer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Web Science</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="33" to="47" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paulius</forename><surname>Micikevicius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonah</forename><surname>Alben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Houston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksii</forename><surname>Kuchaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganesh</forename><surname>Venkatesh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.03740</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Mixed precision training. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Measurement and analysis of online social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Mislove</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Marcon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gummadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bobby</forename><surname>Druschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bhattacharjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th ACM SIGCOMM conference on Internet measurement</title>
		<meeting>the 7th ACM SIGCOMM conference on Internet measurement</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="29" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Playing atari with deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.5602</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Distributed inference for latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Padhraic</forename><surname>Smyth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur U</forename><surname>Asuncion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1081" to="1088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Network embedding as matrix factorization: Unifying deepwalk, line, pte, and node2vec</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Eleventh ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="459" to="467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hogwild: A lock-free approach to parallelizing stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="693" to="701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Software framework for topic modelling with large corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radim</forename><surname>Rehurek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Sojka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks</title>
		<meeting>the LREC 2010 Workshop on New Challenges for NLP Frameworks</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="93" to="93" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Efficient and accurate Word2Vec implementations in GPU and shared-memory multicore architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Trevor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gita</forename><surname>Simonton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alaghband</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">High Performance Extreme Computing Conference (HPEC)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Distributed Negative Sampling for Word Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stergios</forename><surname>Stergiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zygimantas</forename><surname>Straznickas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rolina</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Tsioutsiouliklis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2569" to="2575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The STRING database in 2017: quality-controlled proteinprotein association networks, made broadly accessible</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damian</forename><surname>Szklarczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milan</forename><surname>Wyder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Simonovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nadezhda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Doncheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peer</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="page">937</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Visualizing largescale and high-dimensional data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingzhou</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on World Wide Web. International World Wide Web Conferences Steering Committee</title>
		<meeting>the 25th International Conference on World Wide Web. International World Wide Web Conferences Steering Committee</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="287" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web. International World Wide Web Conferences Steering Committee</title>
		<meeting>the 24th International Conference on World Wide Web. International World Wide Web Conferences Steering Committee</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">VERSE: Versatile Graph Embeddings from Similarity Measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Tsitsulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Mottin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panagiotis</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 World Wide Web Conference on World Wide Web. International World Wide Web Conferences Steering Committee</title>
		<meeting>the 2018 World Wide Web Conference on World Wide Web. International World Wide Web Conferences Steering Committee</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="539" to="548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Graph attention networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R Devon</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hjelm</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.10341</idno>
		<title level="m">Deep graph infomax</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Structural deep network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1225" to="1234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Defining and evaluating network communities based on ground-truth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaewon</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge and Information Systems</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="181" to="213" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">NOMAD: Non-locking, stOchastic Multi-machine algorithm for Asynchronous and Decentralized matrix completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyokun</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiang-Fu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inderjit</forename><surname>Svn Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="975" to="986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuchang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuheng</forename><surname>Zou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.06160</idno>
		<title level="m">Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A fast parallel SGD for matrix factorization in shared memory systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chin</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th ACM conference on Recommender systems</title>
		<meeting>the 7th ACM conference on Recommender systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
