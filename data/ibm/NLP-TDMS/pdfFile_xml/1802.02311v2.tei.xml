<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Empirical Evaluation of Deep Learning for ICD-9 Code Assignment using MIMIC-III Clinical Notes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="20198-06-11">June 11, 2019 8 Jun 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinmiao</forename><surname>Huang</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cesar</forename><surname>Osorio</surname></persName>
							<email>cesar.osorio@gatech.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><forename type="middle">Wicent</forename><surname>Sy</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><forename type="middle">Wicent</forename><surname>Sy</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
								<address>
									<settlement>Atlanta</settlement>
									<region>Georgia</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Georgia Institute of Technology</orgName>
								<address>
									<settlement>Atlanta</settlement>
									<region>Georgia</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Georgia Institute of Technology</orgName>
								<address>
									<settlement>Atlanta</settlement>
									<region>Georgia</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">An Empirical Evaluation of Deep Learning for ICD-9 Code Assignment using MIMIC-III Clinical Notes</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="20198-06-11">June 11, 2019 8 Jun 2019</date>
						</imprint>
					</monogr>
					<note type="submission">Preprint submitted to Computer Methods and Programs in Biomedicine</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep Learning</term>
					<term>Clinical Notes</term>
					<term>Machine Learning</term>
					<term>ICD-9</term>
					<term>Medical Codes</term>
					<term>RNNs</term>
					<term>CNNs</term>
					<term>MIMIC-III</term>
					<term>Code Assignment * corresponding author</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Background and Objective: Code assignment is of paramount importance in many levels in modern hospitals, from ensuring accurate billing process to creating a valid record of patient care history. However, the coding process is tedious and subjective, and it requires medical coders with extensive training. This study aims to evaluate the performance of deep-learning-based systems to automatically map clinical notes to ICD-9 medical codes. Methods: The evaluations of this research are focused on end-to-end learning methods without manually defined rules. Traditional machine learning algorithms, as well as state-of-the-art deep learning methods such as Recurrent Neural Networks and Convolution Neural Networks, were applied to the Medical Information Mart for Intensive Care (MIMIC-III) dataset. An extensive number of experiments was applied to different settings of the tested algorithm. Results: Findings showed that the deep learning-based methods outperformed other conventional machine learning methods. From our assessment, the best models could predict the top 10 ICD-9 codes with 0.6957 F 1 and 0.8967 accuracy and could estimate the top 10 ICD-9 categories with 0.7233 F 1 and 0.8588 accuracy. Our implementation also outperformed existing work under certain evaluation metrics. Conclusion: A set of standard metrics was utilized in assessing the performance of ICD-9 code assignment on MIMIC-III dataset. All the developed evaluation tools and resources are available online, which can be used as a baseline for further research.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Electronic health record (EHR) data include a variety of patient clinical information such as medical history, vital signs, lab test results, and clinical notes. Such data can help in building a continuous flow of information between doctors and patients. More so, systematic reviews have shown that clinical care quality can be improved considerably using predictive analysis based on EHR data <ref type="bibr" target="#b0">[1]</ref>.</p><p>EHR data contain both structured (e.g., blood pressure) and unstructured data (e.g., doctor's observation). While many medical systems focus on structured biosignal features in EHRs to build the clinical decision making systems <ref type="bibr" target="#b1">[2]</ref>, more than 80% of health record data are unstructured text <ref type="bibr" target="#b2">[3]</ref>. For example, clinical notes contain information about patients' medical history and doctors' observations and comments regarding their interactions with patients.</p><p>The systems evaluated in this paper assign ICD-9 codes from a patient's free-text EHR. These codes can be subsequently used in billing or creating a valid record of patient care history. Currently, the task of assigning diagnosis codes is carried out manually by medical coders. Also, the volume of medical records generated makes the manual classification of diagnoses a laborintensive process, thus resulting in a significant backlog of work. Automating ICD-9 code assignment will not only make the clinical process more efficient, but it will also take note of all EHRs and provide support to expedite some levels of semantic analysis which can help clinicians diagnose and improve the medical care systems effectively. Over the past two decades, researchers have explored machine learning methods to assign ICD-9 codes based on clinical notes, such as Support Vector Machine (SVM) <ref type="bibr" target="#b3">[4]</ref>, Naive Bayes <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>, and Boosting <ref type="bibr" target="#b6">[7]</ref>. Despite their research efforts, it is believed that the accuracy of this problem can be further improved, especially with the recent breakthrough in deep learning approaches. Deep learning techniques have shown a significant improvement in many Natural Language Processing (NLP) tasks such as language translation <ref type="bibr" target="#b7">[8]</ref>, natural language understanding <ref type="bibr" target="#b8">[9]</ref>, and sentiment analysis <ref type="bibr" target="#b9">[10]</ref>. What's more, deep learning models can often be trained end-to-end without any domain-specific and handdesigned feature engineering, a process that is often tedious.</p><p>Furthermore, there is a lack of a baseline for the community to reliably assess different algorithms on benchmark datasets. Because of this challenge, this paper will focus on evaluating the performance of state-of-the-art deep neural networks to diagnose learning systems on a widely-used and publicly available dataset. Our results will be compared with several traditional classification systems, including Logistic Regression, Random Forests and Feed-forward Neural Networks (FNNs), each of which aims to predict the code from the clinical notes. In addition, an extensive number of experiments will be applied to different settings of the tested classification algorithm. Apart from using word embedding to transform a patient's free-text EHR into information that could be used to predict ICD-9 codes, this research will evaluate the impact of word embedding trained from MIMIC-III <ref type="bibr" target="#b10">[11]</ref> dataset and medical domain word embedding. In short, this paper aims to provide a baseline for the learning-based ICD-9 code assignment on MIMIC-III dataset.</p><p>The contributions of this study are three-fold. The first contribution is the development of deep learning-based algorithms to map ICD-9 codes to clinical discharge summaries. The implementation of this research outperformed existing works under certain evaluation metrics. The second is the comparison of the performance of a wide variety of state-of-the-art machine learning and deep learning algorithms on MIMIC-III dataset. The third is the utilization of a set of standard metrics to assess the performance of ICD-9 code assignment on MIMIC-III dataset, which can be used as a baseline for further research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The task of automatic ICD-9 coding has been attempted for decades. In 1995, Larkey and Croft <ref type="bibr" target="#b11">[12]</ref> designed classifiers for the automatic assignment of ICD-9 codes to discharge summaries. Automated ICD-9 coding for radiology reports was one of the first challenges in informatics community <ref type="bibr" target="#b12">[13]</ref> in 2007. There are two major categories of approaches for automatically assigning ICD-9 codes using text-free clinical notes. One category is rule-based and the other category is learning-based. Rule-based systems are designed by human experts. This approach has outperformed other methods in many cases <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b6">7]</ref>. However, this kind of system relies heavily on the manual intervention of medical professionals, thus making it difficult to maintain and scale up to more general cases. Learning-based systems, on the other hand, do not require any domain knowledge from medical experts and rely only on learning algorithms to find the underlying distribution of the provided datasets <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b13">14]</ref>. A detailed review of extracting information from textual documents in the EHR can be found in <ref type="bibr" target="#b14">[15]</ref> and <ref type="bibr" target="#b15">[16]</ref>.</p><p>End-to-End data-driven approaches have gained popularity in the last few years. Recent methods based on deep learning have also demonstrated state-of-the-art performance in a wide variety of tasks, including computer vision <ref type="bibr" target="#b16">[17]</ref>, speech recognition <ref type="bibr" target="#b17">[18]</ref>, and NLP <ref type="bibr" target="#b8">[9]</ref>. In the clinical domain, Choi el. al. <ref type="bibr" target="#b18">[19]</ref> used Recurrent Neural Networks (RNNs) to predict heart failures. Lipton el. al. <ref type="bibr" target="#b19">[20]</ref> utilized Long Short-Term Memory (LSTM) to classify 128 diagnoses from 13 frequently but irregularly sampled clinical measurements extracted from structured EHR data. Similarly, DoctorAI <ref type="bibr" target="#b1">[2]</ref> and RETAIN <ref type="bibr" target="#b20">[21]</ref> utilized RNNs on structured EHR data for diagnostic classification. Many researchers also used deep learning on unstructured free-text to predict the diagnosis. Bai, for instance, proposed a deep transfer learning framework for ICD-9 coding by making use of a large number of MeSH domain knowledge <ref type="bibr" target="#b21">[22]</ref>. Prakash et. al. <ref type="bibr" target="#b22">[23]</ref> exploited raw text from Wikipedia as a source of knowledge and introduced condensed memory neural networks to learn the diagnosis on MIMIC-III data. Given that Prakash et.al. <ref type="bibr" target="#b22">[23]</ref> tackled a problem similar to ours, our results were compared with their findings in Section 4.2.3. A survey of recent deep learning techniques for EHR can also be found in <ref type="bibr" target="#b23">[24]</ref>. <ref type="figure" target="#fig_0">Figure 1</ref> depicts an overview of the methodology pipeline of this research. Our methodology involves the following steps: data preprocessing, feature extraction, and model training and testing. Specifically, the libraries used were: Spark for data preprocessing; Spark, Sklearn, and Gensim for feature extraction; and Spark ML, Keras for model training and testing. Azure virtual machines (NC24 with K80 GPU) were used to run our experiments. Sections 3.1 to 3.3 describe each step in more detail. Each model was evaluated under a set of metrics, as described in Section 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Data Preprocessing</head><p>The MIMIC-III dataset is a large dataset relating to patients admitted to critical care units at a large tertiary care hospital. It contains de-identified medical records of patients who stayed from 2001 to 2012 within the intensive care units at Beth Israel Deaconess Medical Center <ref type="bibr" target="#b10">[11]</ref>. The goal of this study is to explore useful semantic information using unstructured data. Therefore, only the free-text clinic note section from the dataset was used, specifically the noteevents table. Furthermore, the focus was on the discharge summaries category as it contained actual ground truth and free-text compared to other categories. Because discharge summaries were written 3 after carrying out the diagnosis, the notes were sanitized by removing any mention of classlabels (ICD-9 codes). This approach is similar to the one utilized by Prakash et al. <ref type="bibr" target="#b22">[23]</ref>. <ref type="table" target="#tab_0">Table 1</ref> describes the number of unique patients, hospital admissions, ICD-9 codes and ICD-9 categories involved in MIMIC-III dataset. All MIMIC-III describes the whole dataset, while noteevents and discharge summaries explain the corresponding subsets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Coverage</head><p>Patients Hospital Admissions ICD-9 Codes ICD-9 Categories All MIMIC-III 46520 58976 6984 943 noteevents 46146 58361 6967 943 discharge summaries 41127 52726 6918 942 The data were preprocessed to produce separate datasets using two approaches. The first approach is to treat the ICD-9 code independently from each other, find the admissions (unique HADM ID) for each ICD-9 classification, and consider only records related to the top 10 and top 50 common ICD-9 codes. The top 10 and top 50 were chosen because they covered a majority of the dataset (76.9% and 93.6% as illustrated in <ref type="table" target="#tab_3">Table 3</ref>). The second approach is to group ICD-9 codes into categories based on their hierarchical nature, with categories for larger sets of similar health conditions (For instance, "cholera due to vibrio cholerae" has the ICD-9 code 001.0, and is categorized as a type of cholera, which is also a type of intestinal infectious disease). The next step is to find the patients for top 10 and top 50 common categories. Evaluations would be separately performed on the four datasets, which will hereby be referred to as top-10-code, top-50-code, top-10-cat and top-50-cat, respectively. <ref type="table" target="#tab_2">Table 2</ref> shows the top 10 ICD-9 codes and top 10 ICD-9 categories. <ref type="table" target="#tab_3">Table 3</ref> also describes the number of unique hospital admissions related to the four datasets mentioned in the previous paragraph.</p><p>The filtered datasets will be split into 50-25-25 for training, validation and testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Feature Extraction</head><p>Two approaches will be used for feature extraction: They include Term Frequency -Inverse Document Frequency (tfidf ) and word2vec <ref type="bibr" target="#b24">[25]</ref>. The tfidf serves as a baseline of comparison with word2vec. 4   The tfidf aims to evaluate the level of importance of a word to a document in a collection of documents or corpus. It is the product of two statistics: tf and idf. While tf is the number of times a word appears in a given document, and idf measures whether a word is common or rare across the corpus. The following definition of idf will be used for our calculations:</p><formula xml:id="formula_0">id f (w) = log n d d f (d, w) + 1</formula><p>where n d is the total number of documents, and d f (d, w) represents the number of documents that contain the word w.</p><p>To calculate tfidf, all the notes in the filtered training data set were first tokenized. Next, a document-word matrix with the count of each word in each note (tf ) was created. Finally, each word was multiplied by the corresponding idf. Two tfidf configurations were also used: (1) one with top 40,000 words with highest tfidf scores as the bag of word features; (2) the other one with a minimum document frequency of 10 and a maximum document frequency of 0.8 of the total number of documents, which reduced the total number of words to around 20,000 words.</p><p>The model word2vec takes a tokenized text corpus as an input and produces word vectors as an output. The Continuous Bag of Words (CBOW) architecture was then used to predict the target word based on the context: words that precede and follow the target word. The CBOW is basically a Neural Network model that consists of inputs, projection and output layers where the traditional non-linear hidden layer is removed to reduce the time complexity and the projection layer is shared by all the words. The inputs are words in the context. We used text notes from MIMIC-III as corpus to train our word2vec model. Pre-trained word vectors induced from PubMed were also utilized. PubMed is a database of biomedical literature and the word vectors can be found at https://github.com/cambridgeltl/BioNLP-2016 <ref type="bibr" target="#b25">[26]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Model Training and Testing</head><p>One fundamental assumption adopted by traditional supervised learning algorithms is that each sample has only one label assigned to it. In our problem, each sample has multiple (one or more) ICD-9 codes attached to it. Generally, there are two main methods for tackling the multi-label classification problem <ref type="bibr" target="#b26">[27]</ref> One is the problem transformation methods and the other is algorithm adaptation methods. Problem transformation methods transform the multi-label problem into a set of binary classification or regression problems, and multiple binary classifiers are trained separately for each label. Algorithm adaptation methods, on the other hand, adapt the algorithms to perform multi-label classifications in its full form, and only one classifier is trained for all the labels.</p><p>In our study, three baseline approaches were first created: Linear Regression, Random Forests and Feed-forward Neural Networks. Then problem transformation methods were used to obtain the multi-label output for Linear Regression and Random Forests classifiers. Specifically, in order to assign each sample a set of target labels, n different models for n different labels were trained. Each model independently predicts a mutual exclusive output (0 or 1) for each sample data. For Feed-Forward Neural Networks, algorithm adaptation based methods were utilized, given that the neural network could be easily adapted to multi-label problem by setting up multiple neurons in the network output layer and each neuron represents a target label correspondingly. Similar to FNNs, algorithm adaptation-based methods were used in our deep learning models. In the following sub-sections, our implemented models will be described in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1.">Baseline Models</head><p>Logistic Regression (LR): Our first baseline model is a binomial logistic regression model implemented using Spark ML. For each label (ICD-9 code or category), a separate logistic regression model was trained, and each model independently predicted the said label (0 or 1 for the corresponding ICD-9 code or category). Different configurations were tried; specifically, "the number of iterations" was tuned between 5 to 100. Because only notes under discharge summaries category were used, there was one note per admission. Features extracted from this note were used as inputs for this classifier. For tfidf, the features were directly used as input features. For word2vec, the input features were the average of all the feature vectors of the words in the notes. This simple yet popular method was successfully applied in other studies to obtain sentence or document embedding <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref>.</p><p>Random Forests (RFs): Our second baseline model is a random forest model implemented using Spark ML. The same approach and input for the logistic regression were used here (one model for each label). Different configurations were also evaluated; specifically the "tree depth" was tuned between 5 to 30.</p><p>Feed-forward Neural Networks (FNNs): One advantage of Neural Networks is that it can be fitted to multi-label problems in just one model with the proper activation function. The FNNs were implemented as the baseline for algorithm adaptation based multi-label classification problem (see Section 3.3). The same input features and train-test data split as previously described were used. The ReLU activation function was utilized for all the hidden layers and sigmoid function was used for the output layer, binary cross entropy as the loss function, and stochastic gradient descent as the optimizer. Several neural network models with one to four different hidden layers were also tried. For each hidden layer, a total of seven models was employed with the following combination of neuron sizes: 50, 100, 300, 500 and 1000. Among our model architectures, the best performed model pipeline is shown in  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2.">Deep Neural Network Models</head><p>In this study, the problem of ICD-9 code assignment from clinical notes was treated as multilabel classification problem on sequential observations x 1 , x 2 , · · · , x n , where x i is the word2vec features calculated for word i in the discharge summary. Unlike the features used for the baseline models in which the sequential information was not preserved, each word was taken sequentially from the discharge summary. The input features for this classifier are N most recent word sequences taken from the notes. If there were insufficient feature events, zero vectors were padded at the beginning. The word sequence was then converted into vectors using an embedding matrix based on a word2vec model (See Section 3.2).</p><p>Convolutional Neural Networks (CNNs) have achieved remarkable results in image processing related problems. In recent years, CNN models have shown excellent results for NLP such as in semantic parsing <ref type="bibr" target="#b29">[30]</ref>, search query retrieval <ref type="bibr" target="#b30">[31]</ref>, and sentence classification <ref type="bibr" target="#b31">[32]</ref>. Thus, a series of experiments with CNNs were carried out. In general, the same architecture described in <ref type="bibr" target="#b31">[32]</ref> was applied. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, the features were first concatenated into n × k feature vector, where n is the number of words, and k is the number of dimensions extracted from word2vec. A set of convolution filters with dimension h × k was then applied to a window of h words to produce new features. The filters were then applied to each possible window of words in the sentences to produce a feature map. Finally, a max-overtime pooling operation over the feature map was applied to generate the fully connected layer. A sigmoid activation function was also applied to generate the multi-label output. Various number of layers for our CNNs were tried, including three to ten convolutional (conv) layers with size 64, 128, or 256 for each layer. Then, each layer was followed with a max pooling layer, and one to three fully connected (fc) dense layers were attached to the last convolutional layer with a size of 4096, 1024 or 128. Among our model architecture setting, the best performed model pipeline is shown in <ref type="table" target="#tab_7">Table 5</ref>. The same architecture was used on both top-10 and top-50 codes. Based on the hardware setting described, the training time for CNNs was less than 30 minutes with 500 maximum epochs and early stop if the validation loss did not improve for consecutive 10 epochs.  Recurrent Neural Networks (RNNs): RNNs are a type of neural network architecture designed to handle sequential inputs. They have shown promising results in many machine learning tasks <ref type="bibr" target="#b32">[33]</ref>. Several RNN architectures were explored in this study. All the architectures follow the same pattern shown in <ref type="figure" target="#fig_2">Figure 3</ref>, where blue circles represent the text feature vectors. The green rectangles and the yellow rectangle represent recurrent hidden layers and the multi-label code assignment, respectively. Basically, the RNN cells went through the input sentences. Each word x t in the sentences generated a hidden layer h t . Each hidden layer was connected with a directed connection weights w h to its successive layer h t+1 . The weights w h were shared among all the hidden layers (shared over time). The hidden layers of the RNN generated the outputŝ y when the RNN cells reached the last word. Sigmoid cross-entropy was then used as the loss function and RMSprop as the optimizer. Although RNNs can handling input sequences of variable sizes, in practice, they face difficulties when modeling long-term dependencies <ref type="bibr" target="#b33">[34]</ref>. To address this issue, various recurrent units were developed. Among those sophisticated recurrent units, in this study, two popular ones were evaluated: LSTMs <ref type="bibr" target="#b34">[35]</ref> and Gated Recurrent Units (GRUs) <ref type="bibr" target="#b35">[36]</ref>. Both of them could capture sequence-based inputs with long-term dependencies by utilizing a memory mechanism. Generally, LSTM contains three gates: the input gate, forget gate and output gate. The forget gate decides what information from previous and current input should be preserved or ignored. The input gate decides the values that will be updated to the cell states. The output gate decides what the next hidden state should be. A GRU is an LSTM without an output gate, and it uses an update gate to decide what past information should be kept and a reset gate decides how much past information should be discarded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8</head><p>The input features to RNNs are the same as those used in CNNs. Three stacked recurrent layers with a combination 64, 128 and 256 units for each layer in our RNNs were tried. To predict the ICD-9 classification, only the output nodes of the last time step were considered, and the same activation function and loss function were applied as with the NNs. The best performed mode architecture for LSTMs and GRUs are shown in <ref type="table" target="#tab_9">Table 6</ref>. Both architectures that performed best have two stacked recurrent layers with the same unit numbers for each layer. Based on the specified hardware setting, the training time was about 6 hours for GRUs and 18 hours for LSTMs with 200 maximum epochs and early stop if the validation loss did not improve for consecutive 5 epochs.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Metrics</head><p>The combinations of our dataset, feature extraction methods, and models are evaluated under different performance metrics, including precision, accuracy, F-score and recall metrics for multilabel classification. Specifically, the following metrics are used <ref type="bibr" target="#b36">[37]</ref>:</p><formula xml:id="formula_1">Precision = 1 n n i=1 |Y i ∩ Z i | |Z i | Recall = 1 n n i=1 |Y i ∩ Z i | |Y i | F 1 = 1 n n i=1 2|Y i ∩ Z i | |Y i | + |Z i | Accuracy = 1 n n i=1 |Y i ∩ Z i | |Y i ∪ Z i |</formula><p>where Y i is the set of predicted labels, Z i is the set of ground truth labels, and n is the number of samples. Basically, precision calculates the proportion of predicted labels that are correct. Recall calculates the proportion of the actual labels that are correctly predicted. F 1 is the harmonic mean of precision and recall. Accuracy is the average proportion of the predicted correct labels to the total number of labels for all instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>This section illustrates the performance in three different aspects: (1) the baseline results, (2) the performance under different configurations, and (3) the best model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Model Performance under Different Configurations</head><p>Different model configurations have been tried to give us insight into the most appropriate model configuration. <ref type="table">Table 7</ref> describes the different methods of feature extraction used and the parameters tweaked. The features extracted are divided into two categories: non-sequential and sequential features. The non-sequential features include tfidf and word2vec, both of which were used in Logistic Regression, Random Forests, and NNs. The sequential features includes word2seq (word sequences) used in conjunction with an embedding matrix based on word2vec, which were used in CNNs, RNNs, LSTMs, and GRUs. It is pertinent to note that we experimented on (1) using our custom word2vec model created from the MIMIC-III dataset and (2) utilizing pre-trained word vectors obtained from PubMed <ref type="bibr" target="#b25">[26]</ref>. The vectors for stop words in the embedding matrix are all zeros.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature Extraction Configuration</head><p>Value tfidf feature size 20301 -40000 minDocFreq 3 -10 max df 0.8 -1.0 word2vec database self trained from MIMIC-III (m3) or pre-trained from Pubmed (pm) <ref type="bibr" target="#b25">[26]</ref> feature size 100 -600 pre-trained config context window size 2 (win 2) or 30 (win30) <ref type="bibr" target="#b25">[26]</ref> wordseq sequence length 1500-2000 stopwords removed from sequence or not removed embedding matrix derived from the word2vec under different configurations <ref type="table">Table 7</ref>: Feature Extraction Methods <ref type="figure" target="#fig_3">Figure 4</ref> indicates the model performance of each model using different feature extraction methods on the top-10-code dataset. For each model, the configuration that provided the best performance here is used on the top-50-code, top-10-cat, and top-50-cat datasets. The results are further explained in the next section (see Section 4.2). Our three non-sequential models (Logistic Regression, Random Forests and NNs) had different but comparable performance depending on the features used. For example, for top-10 code classification, tfidf with 20k features produced the best F 1 results of 0.532 and 0.322 for Logistic Regression and Random Forests respectively. However, word2vec m3 with 600 features produced the best results for FNNs with F 1 of 0.528 (although tfidf also gave a fairly good result for FNNs with F 1 of 0.488). Thus, tfidf configurations generated better results than those of word2vec. It is possible that the word2vec features lost information as the average of the word vectors was used to obtain document embedding for the non-sequential models. However, note that NNs gave fairly good results (F 1 0.528) using the word2vec feature which was at most 600 10 dimensions compared to tfidf which was 20k or above. With a larger feature size, it is reasonable to say that tfidf kept a better global representation of document embedding. This ability could be used to explain the code classification. However, word2vec did retain the satisfactory representation of word embedding despite the substantial reduction in feature dimension (i.e., 20k down to 600). Four sequential models (CNNs, simple RNNs, LTSMs and GRUs) were run under different configuration with two different types of embedding matrices (our self-trained word2vec from MIMIC-III corpus and pre-trained word2vec from PubMed <ref type="bibr" target="#b25">[26]</ref>) and two sentence sequence lengths <ref type="figure" target="#fig_0">(1500 and 2000)</ref>. In the top-10 code classification, seq. length 2000 + word2vec m3 w/ 600 features generated the best F 1 result for CNNs, seq. length 1500 + word2vec pm (win30) for LSTMs, and seq. length 2000 + word2vec m3 w/ 300 features for GRUs. In short, all feature extraction methods generated good and comparable results for CNNs, LSTMs, and GRUs. Our self-trained word2vec also performed fairly well compared with the pre-trained word2vec models from PubMed. The best performed models with self-trained word2vec have F 1 of 0.637 (CNNs), 0.696 (GRUs) and 0.683 (LSTMs) while the best performed models from PubMed word2vec are 0.589 (CNNs), 0.677 (GRUs), and 0.687 (LSTMs). Under different configurations, our self-trained word2vec outperformed PubMed word2vec in most cases. Experiments were also performed for simple RNNs. The results, however, were poor (0.0 -0.08 F 1 at best). More information on this is explained in detail in Section 5. In addition, our top F 1 scores are linked to GRUs and LSTMs with GRUs providing slightly better results. <ref type="figure" target="#fig_4">Figures 5 and 6</ref> show the model performance, which is ordered from the best to worse, for the top-10-code, top-10-cat, top-50-code, and top-50-cat dataset. <ref type="figure">Figure 7</ref> indicates the model performance for top-50-code and top-50-cat considering only the first 10 labels. Raw data are also shown in Tables 10 to 15 in the Appendix. For top-10-code and top-10-cat, GRUs generated the best F 1 results (of 0.6957 and 0.7233, respectively). Hence, top-10-cat generated slightly better results than did top-10-code. This makes sense because there are more samples per label in top-10-cat and because the labels are less specific (the differences between labels are larger). Logistic Regression and Random Forests overfit the data with about 80% to 95% training F 1 but about 40% to 50% less on testing data). Even though FNNs are not overfitting, the results on testing data are slightly better than other 11 baseline models. CNNs produces better results than FNNs, but there are more significant improvements with LSTMs and GRUs (about 70% F 1 ). This result signifies that our LSTMs and GRUs model can extract information from the sequence of words, thereby improving the F 1 and the result accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Best Model Performance 4.2.1. Overview</head><p>For top-50-code, Logistic Regression generated the best F 1 result of 0.3662. However, if only the first 10 labels are considered, GRUs generated the best F 1 result at 0.6328. For both all the label results and the first 10 label results, GRUs generated the best precision and accuracy results of 0.7520 and 0.8871, respectively. For top-50-cat, Logistic Regression also generated the best F 1 result of 0.4301. However, if only the first 10 labels are considered, LSTMs generated the best F 1 result of 0.6738. For both all the label results and the first 10 label results, GRUs generated the best precision and accuracy results of 0.7515 and 0.8345, respectively. Hence, top-50-cat generated slightly better results than did top-50-code. The baseline models (Logistic Regression and Random Forests) also overfit here. <ref type="table" target="#tab_10">Table 8</ref> presents the average overall precision performance of our selected best performance models for GRUs, LSTMs and CNNs. Average Precision (AP) summarizes the precision-recall curve as the mean of precisions achieved at different recall values and it is calculated as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">Precision-Recall Curve</head><formula xml:id="formula_2">AP = n (R n − R n−1 )P n</formula><p>where P n and R n are the precision and recall at the nth threshold. As shown in the table below, GRUs generated the best precision results for top-10-code and top-50-code. <ref type="figure" target="#fig_7">Figure 8</ref> shows precision-recall curve for the best-performed models for each label in top-10-code and the best-performing 10 labels for top-50-code. In this picture, it can be seen that for top-10code, the top five common codes performed better than did the later ones. For example, the third common label (label 2: 42731), atrial fibrillation, has the highest AP of 0.90. followed by coronary atherosclerosis (label 3: 41401) with AP of 0.89, hypertension with AP of 0.83 (label 0: 4019) and congestive heart failure (label 1: 4280) with AP of 0.81. However, in the performance of top-50-code, it was observed that less common codes could also achieve high AP scores. For example, label 44: 7742, neonatal jaundice associated with preterm delivery, which have 2183 samples in the training dataset has the highest AP score of 0.92, and label 46: V053 with 2119 samples reached fifth. Other class-wise precision-recall curve for our tested models can be found in the Appendix.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3.">Results Comparison</head><p>Prakash and Zhao <ref type="bibr" target="#b22">[23]</ref> used bag-of-words from discharge notes and Condensed Memory Neural Networks (C-MemNN) to tackle the same problem in this research. They tested their algorithm with top 50 and top 100 labels under metrics such as the macro average of Area Under the Curve (AUC), average precision over the top five predictions (Precision @5), and hamming loss.</p><formula xml:id="formula_3">AUC macro = 1 q q j=1 AUC j Hamming Loss = 1 nq n i=1 q j=1 xor(Z i, j , Y i, j )</formula><p>where AUC j is the AUC for each label, q is the number of labels, and n is the number of samples. Macro AUC is used to calculate the unweighted mean of the AUC values for each label. Hamming loss represents the fraction of labels that are incorrectly predicted. To compare our work with theirs, the same metrics were used for our best performed models for top 50 codes (top 100 labels are not compared), and the results are presented in <ref type="table" target="#tab_12">Table 9</ref>.</p><p>Based on the results, it can be seen that while their hamming loss is better than ours, our work outperforms theirs in terms of macro AUC on GRU models and significantly performs better on top five precision for all of our models.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>Although some studies have used deep learning for automatic ICD-9 code assignment, a few of them have focused on getting meaningful information directly from unstructured clinical notes. Existing research on such tasks are difficult to compare with each other because there is a lack of standard comparison metrics. This study is significant in that it conducted extensive experiments to examine the performance of popular machine learning and deep learning algorithms under a set of standard metrics, precision, recall, F 1 and accuracy. As the output label vector for our task was sparse (for each sample, only a few labels were generally active), the influence of sparsity was considered when choosing the measurements. For instance, both hamming loss and accuracy are in favor of sparse labels. An all-zero classifier would have the hamming loss close to 0 and the accuracy close to 1. Precision, recall, and F 1 are more reasonable choices compared to hamming loss and accuracy. If the developed system is aiming for recommendation, Precision@k has often been used for such purpose <ref type="bibr" target="#b37">[38]</ref>. Other optimized measures for sparse labels are also worth exploring in future research <ref type="bibr" target="#b38">[39]</ref>.</p><p>Our findings support the use of RNNs for code assignment on patient notes, as they show better performance than other machine learning models as well as previously published systems <ref type="bibr" target="#b22">[23]</ref> under certain metrics. Except from the sequential RNN models, the other models only consider the current input and they have no notion of order in time. They simply can not remember anything about what happened in the past. In a FNN, the information moves straight through the network in one direction. Because of that, the information hardly touches a node twice. In contrast, RNNs try to derive relations from the current word and what it has learned from previous words in the same sentence. In RNNs, each node at a time step takes an input from the previous node using a feedback loop.</p><p>To further understand what form of deep neural network architecture works better on automatic ICD-9 coding, the performance of CNNs, simple RNNs, and RNNs with long-term dependencies capability were also compared. Among all the compared methods (including the baseline models), simple RNNs without memory mechanism produced the worst performance. The F 1 is 0.08 at best for various configurations. Although it is well known that RNNs can only use information from near past, the unsatisfactory results may be due to the learning ability of the network itself. As explained in <ref type="bibr" target="#b33">[34]</ref>, RNNs for tasks with long-term dependencies would not be robust to input noise or would not be efficiently trainable by gradient descent, and the gradient of the loss function decays exponentially with time. Worth noting is that in order to know if representations from future time steps would help the network to gain extra information, bidirectional LSTMs and GRUs were also implemented in our experiments. Bidirectional RNNs connects two hidden layers of opposite directions to the same output. Such mechanisms make the network receive information from past and future states simultaneously. Bidirectional RNNs are especially useful when the later context of the input is needed. It was observed that bidirectional RNNs do not outperform (comparable or slightly worse) the RNNs for the top-10 code assignment task. Considering the extra computation cost of the bidirectional RNNs, this network architecture was not further explored with the belief that peeking future information may not beneficial to our task.</p><p>Despite the fact that CNNs did not perform as good as LSTMs and GRUs, the training time for CNNs (30 minutes) was significantly less than that of GRUs (6 hours) and LSTMs (18 hours). The computation for CNNs can happen in parallel, while RNNs need to be processed sequentially, After all, the subsequent steps depend on previous ones. As part of future work, the results may be improved with newly designed network architectures, such as Temporal Convolutional Networks (TCNs) <ref type="bibr" target="#b39">[40]</ref>. Bai et al. compared a series of benchmark competitions of TCNs versus RNNs, LSTMs, and GRUs across eleven different industry standard RNNs problems. They found that TCN models substantially outperformed generic recurrent architectures such as LSTMs and GRUs. They also showed that TCNs exhibited longer memory than did recurrent architectures with the same capacity <ref type="bibr" target="#b40">[41]</ref>.</p><p>The paper also evaluates the impact of word embedding on the performance of our tested methods. For baseline models, a simple yet popular method to generate the non-sequential word2vec feature vector was used. Specifically, the word vectors from a discharge summary were averaged. Models using the averaged word vector features performed worse than those using tfidf features. While for deep learning, because the sequential information can be retained, almost all the networks with word2vec could generate better results than our baseline models. In addition, compared to our self-trained word2vec from MIMIC-III and word2vec downloaded from PubMed, they showed comparable performance and the word2vec trained from MIMIC-III generally produced slightly better results.</p><p>Although LSTMs and GRUs could capture long-term dependencies, the length of our input sequence could still be too long for LSTMs and GRUs to retain useful information. A different representation may be used to shorten the sequence, e.g. sentence2vec or paragraph2vec <ref type="bibr" target="#b41">[42]</ref>. In addition to that, based on the comparison in Section 4.2.3, memory networks provide better results for certain metrics. word2vec representation and memory network can help address this problem.</p><p>Our current models for top 50 ICD-9 codes and categories were not successful. This failure may be because our current model design could not effectively distinguish between 50 different labels. To improve our model capability, we could run five top-10 models in parallel (each model predicting 10 labels), thereby making our top-50 models have the same model capability 15</p><p>as our top-10 models. It was also observed that the samples for labels 11 to 50 were greatly imbalanced (i.e., the positive samples where very few compared to negative samples). Hence, the data might not be sufficient for the deep neural network to learn adequate useful representations. As part of future work, the sample size of the clinical notes that contains those labels needs to be balanced or increased. Zeng et al. <ref type="bibr" target="#b21">[22]</ref> also showed that deep transfer learning could improve the performance of automatic ICD-9 coding on labels with limited samples. Our custom word2vec model used CBOW. Skip-gram (even though the pre-trained word2vec induced from PubMed are skip-gram based) was not used. Some previous studies have noted that skip-gram outperforms CBOW in biomedical domain tasks <ref type="bibr" target="#b25">[26]</ref>. Therefore, in future work, the effect of different word2vec parameters on our ICD-9 code or category classifier will be further explored.</p><p>Further research on what words affect the probability of a prediction could improve our understanding of the relationship between symptoms and diagnosis. The probability observation could also change our preprocessing and feature extraction methods and ultimately improve our deep learning models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>This study evaluates different NLP deep learning based models and feature extraction methods. It also establishes an empirical evaluation for learning-based automatic code assignment from the MIMIC-III discharge summary. The models are based on deep learning NLP frameworks that automatically assign clinical ICD-9 codes from free-text clinical notes. The deep learning models for predicting the top 10 ICD-9 codes and categories performed better than our baseline models that used traditional learning algorithms (best F 1 results: 0.6957 GRUs to 0.5320 Logistic Regression, and 0.7233 GRUs to 0.6313 FNNs, respectively). It was observed that the top 50 ICD-9 codes and categories results did not outperform our baseline (F 1 results: 0.3263 GRUs compared to 0.3662 Logistic Regression, and 0.3367 GRUs compare to 0.3651 FNNs). We believe that with more descriptive record collection and modern deep learning strategies, the predictive ability will likely increase. We also hope that our implementation and evaluation of the current state-of-the-art algorithms will serve as a baseline for further research on this topic.       </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Methodology Pipeline Overview</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>CNN Architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>log(ŷ n ) + (1 − y n ) · log(1 −ŷ n ) RNNs architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Model Performance under Different Configurations</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Model Performance Top 10</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Model Performance Top 50 (a) top-50-code (b) top-50-cat Model Performance Top 50 (first 10 labels only)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Model top-10-code top-10-cat top-50-code top-50-cat top-50-code(first10) top-50-cat(first10) LSTMs 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>(a) top-10-code (b) top-50-code Class-wise Precision-recall Curve for the Top 10 and the Best-performed 10 Labels for Top 50 Code Classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>MIMIC-III Descriptive Statistics</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell cols="3">: Admission number for Top 10 ICD-9 codes and top 10 ICD-9 categories</cell></row><row><cell>Data Set</cell><cell cols="2">Hospital Admissions discharge summaries Coverage (%)</cell></row><row><cell>top-10-code</cell><cell>40562</cell><cell>76.93%</cell></row><row><cell>top-50-code</cell><cell>49354</cell><cell>93.60%</cell></row><row><cell>top-10-cat</cell><cell>44419</cell><cell>84.24%</cell></row><row><cell>top-50-cat</cell><cell>51034</cell><cell>96.79%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Dataset Descriptive Statistics</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc></figDesc><table><row><cell>6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Configuration Details of the Best Performed FNN Architecture.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table /><note>Configuration Details of the Best Performed CNN Architecture.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table /><note>Configuration Details of the Best Performed LSTMs and GRUs Architectures.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table /><note>Average Precision Performance</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 :</head><label>9</label><figDesc></figDesc><table /><note>Performance Comparison with Reference [23]</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>A. Model Performance for Top 10 Label Codes</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Training</cell><cell></cell><cell></cell><cell>Test</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="3">Precision Recall Accuracy</cell><cell>F 1</cell><cell cols="3">Precision Recall Accuracy</cell><cell>F 1</cell></row><row><cell>Logistic Regression</cell><cell>0.9564</cell><cell>0.9440</cell><cell>0.9786</cell><cell>0.9501</cell><cell>0.5801</cell><cell>0.4934</cell><cell>0.8392</cell><cell>0.5320</cell></row><row><cell>Random Forests</cell><cell>0.9989</cell><cell>0.6988</cell><cell>0.9501</cell><cell>0.8086</cell><cell>0.7573</cell><cell>0.2340</cell><cell>0.8432</cell><cell>0.3219</cell></row><row><cell>Feed-forward NN</cell><cell>0.7933</cell><cell>0.5742</cell><cell>0.8998</cell><cell>0.6457</cell><cell>0.6810</cell><cell>0.4634</cell><cell>0.8622</cell><cell>0.5323</cell></row><row><cell>CNNs</cell><cell>0.8312</cell><cell>0.6713</cell><cell>0.9165</cell><cell>0.7371</cell><cell>0.7408</cell><cell>0.5687</cell><cell>0.8832</cell><cell>0.6373</cell></row><row><cell>LSTM RNNs</cell><cell>0.8106</cell><cell>0.6971</cell><cell>0.9154</cell><cell>0.7445</cell><cell>0.7574</cell><cell>0.6380</cell><cell>0.8950</cell><cell>0.6874</cell></row><row><cell>GRU RNNs</cell><cell>0.7936</cell><cell>0.6971</cell><cell>0.9126</cell><cell>0.7397</cell><cell>0.7502</cell><cell>0.6519</cell><cell>0.8967</cell><cell>0.6957</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 10 :</head><label>10</label><figDesc>Model Performance for top-10-code B. Model Performance for Top 50 Codes</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Training</cell><cell></cell><cell></cell><cell></cell><cell>Test</cell><cell></cell></row><row><cell>Model</cell><cell cols="3">Precision Recall Accuracy</cell><cell>F 1</cell><cell cols="3">Precision Recall Accuracy</cell><cell>F 1</cell></row><row><cell>Logistic Regression</cell><cell>0.9863</cell><cell>0.9768</cell><cell>0.9945</cell><cell>0.9815</cell><cell>0.4372</cell><cell>0.3213</cell><cell>0.9148</cell><cell>0.3662</cell></row><row><cell>Random Forests</cell><cell>0.9985</cell><cell>0.2852</cell><cell>0.9451</cell><cell>0.3866</cell><cell>0.5377</cell><cell>0.0953</cell><cell>0.9220</cell><cell>0.1155</cell></row><row><cell>Feed-forward NN</cell><cell>0.2490</cell><cell>0.1138</cell><cell>0.9224</cell><cell>0.1268</cell><cell>0.2251</cell><cell>0.1090</cell><cell>0.9212</cell><cell>0.1215</cell></row><row><cell>CNNs</cell><cell cols="2">0.6085 a 0.2663</cell><cell>0.9365</cell><cell cols="3">0.3200 a 0.4792 a 0.2169</cell><cell>0.9286</cell><cell>0.2609 a</cell></row><row><cell>LSTM RNNs</cell><cell cols="2">0.3526 a 0.1642</cell><cell>0.9325</cell><cell cols="3">0.1891 a 0.4022 a 0.1445</cell><cell>0.9286</cell><cell>0.1659 a</cell></row><row><cell>GRU RNNs</cell><cell cols="2">0.6539 a 0.3433</cell><cell>0.9460</cell><cell cols="3">0.3947 a 0.5592 a 0.2782</cell><cell>0.9354</cell><cell>0.3263 a</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 11 :</head><label>11</label><figDesc>Model Performance for top-50-code</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Training</cell><cell></cell><cell></cell><cell></cell><cell>Test</cell><cell></cell></row><row><cell>Model</cell><cell cols="3">Precision Recall Accuracy</cell><cell>F 1</cell><cell cols="3">Precision Recall Accuracy</cell><cell>F 1</cell></row><row><cell>Logistic Regression</cell><cell>0.9564</cell><cell>0.9440</cell><cell>0.9786</cell><cell>0.9501</cell><cell>0.5801</cell><cell>0.4934</cell><cell>0.8392</cell><cell>0.5320</cell></row><row><cell>Random Forests</cell><cell>0.9946</cell><cell>0.4937</cell><cell>0.9110</cell><cell>0.6305</cell><cell>0.7869</cell><cell>0.2009</cell><cell>0.8395</cell><cell>0.2822</cell></row><row><cell>Feed-forward NN</cell><cell>0.5266 a</cell><cell>0.2783</cell><cell>0.8408</cell><cell cols="3">0.3380 a 0.5143 a 0.2676</cell><cell>0.8370</cell><cell>0.3276 a</cell></row><row><cell>CNNs</cell><cell>0.7708</cell><cell>0.4673</cell><cell>0.8858</cell><cell>0.5377</cell><cell>0.6784</cell><cell>0.4109</cell><cell>0.8650</cell><cell>0.4739</cell></row><row><cell>LSTM RNNs</cell><cell>0.6204 a</cell><cell>0.3829</cell><cell>0.8805</cell><cell>0.4348 a</cell><cell>0.5748</cell><cell>0.3526</cell><cell>0.8688</cell><cell>0.4025 a</cell></row><row><cell>GRU RNNs</cell><cell>0.8351</cell><cell>0.6474</cell><cell>0.9168</cell><cell>0.7181</cell><cell>0.7520</cell><cell>0.5618</cell><cell>0.8871</cell><cell>0.6328</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 12 :</head><label>12</label><figDesc>Model Performance for top-50-code (first 10) a result contained nan. Computed by replacing nan with zero. 19 C. Model Performance for Top 10 Label Categories</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Training</cell><cell></cell><cell></cell><cell>Test</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="3">Precision Recall Accuracy</cell><cell>F 1</cell><cell cols="3">Precision Recall Accuracy</cell><cell>F 1</cell></row><row><cell>Logistic Regression</cell><cell>0.9437</cell><cell>0.9309</cell><cell>0.9652</cell><cell>0.9372</cell><cell>0.6458</cell><cell>0.5856</cell><cell>0.7994</cell><cell>0.6141</cell></row><row><cell>Random Forests</cell><cell>0.9983</cell><cell>0.8134</cell><cell>0.9500</cell><cell>0.8954</cell><cell>0.7653</cell><cell>0.3801</cell><cell>0.8019</cell><cell>0.4966</cell></row><row><cell>Feed-forward NN</cell><cell>0.7989</cell><cell>0.6456</cell><cell>0.8632</cell><cell>0.7083</cell><cell>0.7334</cell><cell>0.5633</cell><cell>0.8272</cell><cell>0.6314</cell></row><row><cell>CNNs</cell><cell>0.8039</cell><cell>0.6637</cell><cell>0.8681</cell><cell>0.7128</cell><cell>0.7613</cell><cell>0.6126</cell><cell>0.8446</cell><cell>0.6657</cell></row><row><cell>LSTM RNNs</cell><cell>0.8146</cell><cell>0.6807</cell><cell>0.8749</cell><cell>0.7343</cell><cell>0.7926</cell><cell>0.6536</cell><cell>0.8622</cell><cell>0.7090</cell></row><row><cell>GRU RNNs</cell><cell>0.8150</cell><cell>0.7613</cell><cell>0.8909</cell><cell>0.7861</cell><cell>0.7580</cell><cell>0.6941</cell><cell>0.8588</cell><cell>0.7233</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 13 :</head><label>13</label><figDesc>Model Performance for top-10-cat D. Model Performance for Top 50 Label Categories</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Training</cell><cell></cell><cell></cell><cell></cell><cell>Test</cell><cell></cell></row><row><cell>Model</cell><cell cols="3">Precision Recall Accuracy</cell><cell>F 1</cell><cell cols="3">Precision Recall Accuracy</cell><cell>F 1</cell></row><row><cell>Logistic Regression</cell><cell>0.9750</cell><cell>0.9572</cell><cell>0.9887</cell><cell>0.9659</cell><cell>0.4858</cell><cell>0.3894</cell><cell>0.8841</cell><cell>0.4301</cell></row><row><cell>Random Forests</cell><cell>0.9986</cell><cell>0.3294</cell><cell>0.9277</cell><cell>0.4465</cell><cell>0.6568</cell><cell>0.1142</cell><cell>0.8906</cell><cell>0.1576</cell></row><row><cell>Feed-forward NN</cell><cell>0.3522</cell><cell>0.1654</cell><cell>0.8940</cell><cell>0.2007</cell><cell>0.3600</cell><cell>0.1557</cell><cell>0.8909</cell><cell>0.1901</cell></row><row><cell>CNNs</cell><cell>0.7428</cell><cell>0.3262</cell><cell>0.9163</cell><cell>0.3870</cell><cell cols="2">0.5635 a 0.2770</cell><cell>0.9035</cell><cell>0.3301 a</cell></row><row><cell>LSTM RNNs</cell><cell cols="2">0.7117 a 0.3363</cell><cell>0.9194</cell><cell>0.3804 a</cell><cell>0.5869</cell><cell>0.2945</cell><cell>0.9087</cell><cell>0.3367 a</cell></row><row><cell>GRU RNNs</cell><cell cols="2">0.6695 a 0.3227</cell><cell>0.9179</cell><cell cols="3">0.3726 a 0.5611 a 0.2809</cell><cell>0.9067</cell><cell>0.3266 a</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 14 :</head><label>14</label><figDesc>Model Performance for top-50-cat</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Training</cell><cell></cell><cell></cell><cell>Test</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="3">Precision Recall Accuracy</cell><cell>F 1</cell><cell cols="3">Precision Recall Accuracy</cell><cell>F 1</cell></row><row><cell>Logistic Regression</cell><cell>0.9437</cell><cell>0.9309</cell><cell>0.9652</cell><cell>0.9372</cell><cell>0.6458</cell><cell>0.5856</cell><cell>0.7994</cell><cell>0.6141</cell></row><row><cell>Random Forests</cell><cell>0.9937</cell><cell>0.6321</cell><cell>0.8999</cell><cell>0.7687</cell><cell>0.7877</cell><cell>0.3282</cell><cell>0.7944</cell><cell>0.4512</cell></row><row><cell>Feed-forward NN</cell><cell>0.6905</cell><cell>0.4762</cell><cell>0.8043</cell><cell>0.5535</cell><cell>0.6795</cell><cell>0.4562</cell><cell>0.7955</cell><cell>0.5347</cell></row><row><cell>CNNs</cell><cell>0.7945</cell><cell>0.6652</cell><cell>0.8670</cell><cell>0.7142</cell><cell>0.7296</cell><cell>0.5979</cell><cell>0.8345</cell><cell>0.6481</cell></row><row><cell>LSTM RNNs</cell><cell>0.7963</cell><cell>0.6863</cell><cell>0.8768</cell><cell>0.7213</cell><cell>0.7515</cell><cell>0.6362</cell><cell>0.8514</cell><cell>0.6738</cell></row><row><cell>GRU RNNs</cell><cell>0.7901</cell><cell>0.6803</cell><cell>0.8729</cell><cell>0.7213</cell><cell>0.7382</cell><cell>0.6196</cell><cell>0.8442</cell><cell>0.6641</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 15 :</head><label>15</label><figDesc>Model Performance for top-50-cat (first 10) a result contained nan. Computed by replacing nan with zero. 20 E. Best Performance Models (LSTMs, GRUs, CNNs) Precision-Recall Curve</figDesc><table><row><cell>(a) LSTMs Top 10 Codes</cell><cell>(b) LSTMs Top 10 Categories</cell></row><row><cell>(c) LSTMs Top 50 Codes</cell><cell>(d) LSTMs Top 50 Categories</cell></row><row><cell>(e) GRUs Top 10 Codes</cell><cell>(f) GRUs Top 10 Categories</cell></row><row><cell>21</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>The authors would like to thank the anonymous reviewers for their valuable comments and feedback. This research did not receive any specific grant from funding agencies in the public, commercial, or not-for-profit sectors. The dataset used in this work is openly available developed by the MIT Lab for Computational Physiology.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflict of Interest Statement</head><p>The authors declare that there is no conflict of interest. The authors do not have financial and personal relationships with other people or organizations that could inappropriately influence (bias) their work.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The impact of ehealth on the quality and safety of health care: a systematic overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ashly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josip</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudia</forename><surname>Car</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chantelle</forename><surname>Pagliari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathrin</forename><surname>Anandan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomislav</forename><surname>Cresswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Bokun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Mckinstry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Azeem</forename><surname>Procter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aziz</forename><surname>Majeed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Med</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1000387</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Doctor ai: Predicting clinical events via recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Taha</forename><surname>Bahadori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Schuetz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimeng</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning for Healthcare Conference</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="301" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Big data in medicine is driving big changes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Martin-Sanchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Verspoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Yearb Med Inform</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="14" to="20" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Using structured ehr data and svm to support icd-9-cm coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filipe</forename><surname>Ferrao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Monica</forename><forename type="middle">D</forename><surname>Janela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henrique Mg</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Martins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Healthcare Informatics (ICHI), 2013 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="511" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Automating the assignment of diagnosis codes to patient encounters using example-based and machine learning techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Serguei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">D</forename><surname>Pakhomov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">G</forename><surname>Buntrock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chute</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="516" to="525" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Machine learning and features selection for semi-automatic icd-9-cm encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Medori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cédrick</forename><surname>Fairon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL HLT 2010 Second Louhi Workshop on Text and Data Mining of Health Documents</title>
		<meeting>the NAACL HLT 2010 Second Louhi Workshop on Text and Data Mining of Health Documents</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="84" to="89" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Three approaches to automatic assignment of icd-9-cm codes to radiology reports</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ira</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Arzumtsyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uzuner</forename><surname>Andözlem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AMIA Annual Symposium Proceedings</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">279</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Semi-supervised recursive autoencoders for predicting sentiment distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on empirical methods in natural language processing</title>
		<meeting>the conference on empirical methods in natural language processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="151" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mimic-iii, a freely accessible critical care database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">W</forename><surname>Alistair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H Lehman</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengling</forename><surname>Li-Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Ghassemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><forename type="middle">Anthony</forename><surname>Szolovits</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger G</forename><surname>Celi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific data</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">160035</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Automatic assignment of icd9 codes to discharge summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W Bruce</forename><surname>Larkey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Croft</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<pubPlace>Amherst, MA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts at Amherst</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A shared task involving multi-label classification of clinical free text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>John P Pestian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paweł</forename><surname>Brew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matykiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Hovermale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Włodzisław</forename><surname>Bretonnel Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on BioNLP 2007: Biological, Translational, and Clinical Language Processing</title>
		<meeting>the Workshop on BioNLP 2007: Biological, Translational, and Clinical Language Processing</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="97" to="104" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An experimental study in automatically categorizing medical documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Berthier</forename><surname>Ribeiro-Neto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><forename type="middle">Hf</forename><surname>Laender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luciano Rs De</forename><surname>Lima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Association for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="391" to="401" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Extracting information from textual documents in the electronic health record: a review of recent research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stéphane M Meystre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Guergana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karin</forename><forename type="middle">C</forename><surname>Savova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">F</forename><surname>Kipper-Schuler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hurdle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Yearb Med Inform</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">128</biblScope>
			<biblScope unit="page">44</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Methods and Techniques for Clinical Text Modeling and Analytics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Ling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
		<respStmt>
			<orgName>Drexel University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Abdel-Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, speech and signal processing (icassp), 2013 ieee international conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Using recurrent neural network models for early detection of heart failure onset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Schuetz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimeng</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="page">112</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning to diagnose with lstm recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zachary C Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Randall</forename><surname>Elkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wetzell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.03677</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Retain: An interpretable predictive model for healthcare using reverse time attention mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Taha</forename><surname>Bahadori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimeng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kulas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Schuetz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walter</forename><surname>Stewart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3504" to="3512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Automatic icd-9 coding via deep transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihui</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">324</biblScope>
			<biblScope unit="page" from="43" to="50" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Condensed memory networks for clinical diagnostic inferencing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaditya</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vivek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathy</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashequl</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joey</forename><surname>Qadir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oladimeji</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Farri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3274" to="3280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep ehr: a survey of recent advances in deep learning techniques for electronic health record (ehr) analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Shickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><forename type="middle">James</forename><surname>Tighe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Azra</forename><surname>Bihorac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parisa</forename><surname>Rashidi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE journal of biomedical and health informatics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1589" to="1604" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">How to train good word embeddings for biomedical nlp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Billy</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gamal</forename><surname>Crichton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sampo</forename><surname>Pyysalo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Workshop on Biomedical Natural Language Processing</title>
		<meeting>the 15th Workshop on Biomedical Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="166" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multi-label classification: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grigorios</forename><surname>Tsoumakas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Katakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Data Warehousing and Mining (IJDWM)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Retrofitting word vectors to semantic lexicons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujay</forename><surname>Kumar Jauhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1606" to="1615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Ad hoc monitoring of vocabulary shifts over time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kenter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Wevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pim</forename><surname>Huijnen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM International on Conference on Information and Knowledge Management</title>
		<meeting>the 24th ACM International on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1191" to="1200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning discriminative projections for text similarity measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Fifteenth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="247" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning semantic representations using convolutional neural networks for web search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grégoire</forename><surname>Mesnil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on World Wide Web</title>
		<meeting>the 23rd International Conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="373" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Supervised sequence labelling with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer</publisher>
			<biblScope unit="volume">385</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrice</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">On the properties of neural machine translation: Encoder-decoder approaches. Syntax, Semantics and Structure in Statistical Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">103</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A review on multi-label learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Ling</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1819" to="1837" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multi-label learning with millions of labels: Recommending advertiser bid phrases for web pages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Archit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yashoteja</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manik</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd international conference on World Wide Web</title>
		<meeting>the 22nd international conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="13" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">On label dependence and loss minimization in multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Dembczyński</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willem</forename><surname>Waegeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eyke</forename><surname>Hüllermeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="5" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Temporal convolutional networks for action segmentation and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rene</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory D</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="156" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">An empirical evaluation of generic convolutional and recurrent networks for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Zico</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01271</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
