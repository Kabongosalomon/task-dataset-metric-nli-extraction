<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Learning for Large-Scale Traffic-Sign Detection and Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Domen</forename><surname>Tabernik</surname></persName>
							<email>domen.tabernik@fri.uni-lj.si</email>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Computer and Information Science</orgName>
								<orgName type="institution">University of Ljubljana Večna pot 113</orgName>
								<address>
									<postCode>1000</postCode>
									<settlement>Ljubljana</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danijel</forename><surname>Skočaj</surname></persName>
							<email>danijel.skocaj@fri.uni-lj.si</email>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Computer and Information Science</orgName>
								<orgName type="institution">University of Ljubljana Večna pot 113</orgName>
								<address>
									<postCode>1000</postCode>
									<settlement>Ljubljana</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Learning for Large-Scale Traffic-Sign Detection and Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automatic detection and recognition of traffic signs plays a crucial role in management of the traffic-sign inventory. It provides accurate and timely way to manage traffic-sign inventory with a minimal human effort. In the computer vision community the recognition and detection of traffic signs is a well-researched problem. A vast majority of existing approaches perform well on traffic signs needed for advanced driversassistance and autonomous systems. However, this represents a relatively small number of all traffic signs (around 50 categories out of several hundred) and performance on the remaining set of traffic signs, which are required to eliminate the manual labor in traffic-sign inventory management, remains an open question. In this paper, we address the issue of detecting and recognizing a large number of traffic-sign categories suitable for automating traffic-sign inventory management. We adopt a convolutional neural network (CNN) approach, the Mask R-CNN, to address the full pipeline of detection and recognition with automatic end-to-end learning. We propose several improvements that are evaluated on the detection of traffic signs and result in an improved overall performance. This approach is applied to detection of 200 traffic-sign categories represented in our novel dataset. Results are reported on highly challenging trafficsign categories that have not yet been considered in previous works. We provide comprehensive analysis of the deep learning method for the detection of traffic signs with large intra-category appearance variation and show below 3% error rates with the proposed approach, which is sufficient for deployment in practical applications of traffic-sign inventory management.</p><p>Index Terms-Deep learning, Traffic-sign detection and recognition, Traffic-sign dataset, Mask R-CNN, Traffic-sign inventory management.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract-Automatic detection and recognition of traffic signs plays a crucial role in management of the traffic-sign inventory. It provides accurate and timely way to manage traffic-sign inventory with a minimal human effort. In the computer vision community the recognition and detection of traffic signs is a well-researched problem. A vast majority of existing approaches perform well on traffic signs needed for advanced driversassistance and autonomous systems. However, this represents a relatively small number of all traffic signs (around 50 categories out of several hundred) and performance on the remaining set of traffic signs, which are required to eliminate the manual labor in traffic-sign inventory management, remains an open question. In this paper, we address the issue of detecting and recognizing a large number of traffic-sign categories suitable for automating traffic-sign inventory management. We adopt a convolutional neural network (CNN) approach, the Mask R-CNN, to address the full pipeline of detection and recognition with automatic end-to-end learning. We propose several improvements that are evaluated on the detection of traffic signs and result in an improved overall performance. This approach is applied to detection of 200 traffic-sign categories represented in our novel dataset. Results are reported on highly challenging trafficsign categories that have not yet been considered in previous works. We provide comprehensive analysis of the deep learning method for the detection of traffic signs with large intra-category appearance variation and show below 3% error rates with the proposed approach, which is sufficient for deployment in practical applications of traffic-sign inventory management.</p><p>Index Terms-Deep learning, Traffic-sign detection and recognition, Traffic-sign dataset, Mask R-CNN, Traffic-sign inventory management.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>P ROPER management of traffic-sign inventory is an important task in ensuring safety and efficiency of the traffic flow <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>. Most often this task is performed manually. Traffic signs are captured using a vehicle-mounted camera and manual localization and recognition is performed off-line by a human operator to check for consistency with the existing database. However, such manual work can be extremely timeconsuming when applied to thousands of kilometers of roads. Automating this task would significantly reduce the amount of manual work and improve safety through quicker detection of damaged or missing traffic signs <ref type="bibr" target="#b2">[3]</ref>.</p><p>A crucial step towards the automation of this task is replacing manual localization and recognition of traffic signs with an automatic detection. In the computer-vision community the problem of traffic-sign recognition has already received a considerable attention <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, and excellent detection and recognition algorithms have already been proposed. But these solutions have been designed only for a small number of categories, mostly for traffic signs associated with advanced driver-assistance systems (ADAS) <ref type="bibr" target="#b6">[7]</ref> and autonomous vehicles <ref type="bibr" target="#b7">[8]</ref>.</p><p>Detection and recognition of a large number of traffic-sign categories remains an open question. Various previous benchmarks have addressed the traffic-sign recognition and detection task <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>. However, several of them focused only on traffic-sign recognition (TSR) and ignored the much more complex problem of traffic-sign detection (TSD) where finding accurate location of traffic sign is needed. Other benchmarks that do address TSD mostly cover only a subset of traffic-sign categories, most often ones important for ADAS and autonomous vehicles applications. Most categories appearing in such benchmarks have a distinct appearance with low inter-category variance and can be detected using handcrafted detectors and classifiers. Such examples include round mandatory signs or triangular prohibitory signs. However, many other traffic-sign classes that are not included in the existing benchmarks can be much more difficult to detect as they have a high-degree of variation in appearance. Instances of these categories may have a different real-world size, aspect ratio, color, and may contain various text and symbols (e.g., arrows) that significantly differ between instances of the same class. This often leads to a large degree of intra-category (i.e. within-category) appearance variation and at the same time leads to a low degree of inter-category (i.e. betweencategories) variations due to similar appearance of objects from different categories.</p><p>Modifying existing methods with hand-crafted features and classifiers to handle such categories would be one option; however, that would be a time-consuming task, particularly when considering that many traffic-sign appearances are not consistent between countries. A much more sensible way is to use feature learning based on real examples. This can easily adapt and capture high degree of variability in appearance over a large number of traffic signs. Recent advances in deep learning have shown promising results on detection and recognition of general objects. Previous works already employed deep learning approaches for traffic-sign detection and recognition to some extent <ref type="bibr" target="#b5">[6]</ref>; however, their evaluation focused only on a highly limited subset of traffic-sign categories <ref type="bibr" target="#b12">[13]</ref>. One of the main limitations preventing deep learning from being applied to a large set of traffic-sign categories is a lack of extensive dataset with several hundred different categories and a sufficient number of instances for each category. This issue is particularly important in deep learning where models have tens of millions of learnable parameters and large numbers of samples are needed to prevent overfitting.</p><p>In this paper, we address the issue of learning and detecting a large number of traffic-sign categories for road-based trafficsign inventory management. As our main contribution, we propose a deep-learning-based system for training a large number of traffic-sign categories using convolutional neural networks. We base our system on the state-of-the-art detector Mask R-CNN <ref type="bibr" target="#b13">[14]</ref>, which demonstrated great accuracy and speed in the field of object detection. The same network architecture is used not only for the TSR but also for accurate localization using a region proposal network, resulting in efficient end-toend learning. In contrast to traditional approaches with handcrafted features, the convolutional approach is applied to a broad set of categories, where individual traffic-sign instances are not only subject to change in lighting conditions, scale, viewing angle, blur, and occlusions, but also to significant intra-category appearance variations as well as low intercategory variations. Furthermore, we propose improvements to Mask R-CNN that are crucial for the domain of traffic signs. We propose adaptations that increase the recall rate, particularly for small traffic signs, and introduce a novel augmentation technique suitable for traffic-sign categories.</p><p>As our secondary contribution, we present a novel challenging dataset with 200 traffic-sign categories spread over 13,000 traffic-sign instances and 7000 high-resolution images. The dataset represents a novel benchmark for complex traffic signs with a large number of classes having high intra-category appearance variability. Additionally, the dataset contains enough instances to ensure appropriate learning of deep features. We achieve this by providing annotations of 200 traffic-sign categories with at least 20 instances per category (see <ref type="figure" target="#fig_0">Figure 1</ref>). Furthermore, our qualitative analysis serves as an important study for appropriateness of deep learning for the detection of large number of traffic-sign categories.</p><p>The remainder of the paper is organized as follows. Section II provides the related work overview, Section III describes the employed method, Section V presents the experimental results and discussion on qualitative analysis is provided in Section VI. The paper concludes with the discussion in Section VII.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>An enormous amount of literature exists on the topics of TSR and TSD, and several review papers are available <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b14">[15]</ref>. In general, it is very difficult to decide which approach gives better overall results, mainly due to the lack of a standard publicly available benchmark dataset that would contain an extensive set of various traffic-sign categories, as emphasized in several recent studies <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>. Most authors evaluate their approaches on one of the many public datasets with a relatively limited number of traffic-sign categories:</p><p>• The German Traffic-Sign Detection Benchmark (GTSDB) <ref type="bibr">[</ref> large dataset with 10,000 images containing at least one traffic sign and 90,000 background images. To enrich the set of considered traffic signs, some approaches sample images from multiple datasets to perform the evaluation <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>. On the other hand, a vast number of authors use their own private datasets <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>. To the best of our knowledge, the largest set of categories was considered in the private dataset of <ref type="bibr" target="#b23">[24]</ref>, distinguishing between 131 categories of non-text traffic signs from the roads of United Kingdom.</p><p>Despite a large number of traffic-sign datasets, a comparison of traffic-sign detectors for large numbers of categories remains a challenging problem. In contrast to existing benchmarks that focus mostly on small numbers of supercategories (GTSDB <ref type="bibr" target="#b9">[10]</ref>), or on small numbers of simple traffic signs (BTS <ref type="bibr" target="#b16">[17]</ref>, MASTIF <ref type="bibr" target="#b17">[18]</ref>, STSD <ref type="bibr" target="#b19">[20]</ref>, LISA <ref type="bibr" target="#b10">[11]</ref>), our comprehensive dataset contains 200 traffic-sign categories, including a large number of categories with significant intracategory variability. The closest large-scale dataset is the Tsinghua-Tencent 100K dataset; however, their evaluation still focuses only on 45 simple traffic signs. On the other hand, our dataset enables a comprehensive analysis of detectors in the context of traffic-sign inventory management.</p><p>Various methods have been employed in TSR and TSD. Traditionally hand-crafted features have been used, like histogram of oriented gradients (HOG) <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b9">[10]</ref>, scale invariant feature transform (SIFT) <ref type="bibr" target="#b4">[5]</ref>, local binary patterns (LBP) <ref type="bibr" target="#b15">[16]</ref> or integral channel features <ref type="bibr" target="#b25">[26]</ref>. A wide range of machine learning methods have also been employed, ranging from support vector machine (SVM) <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b26">[27]</ref>, logistic regression <ref type="bibr" target="#b27">[28]</ref>, and random forests <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b26">[27]</ref>, to artificial neural networks in the form of an extreme learning machine (ELM) <ref type="bibr" target="#b18">[19]</ref>.</p><p>Recently, like the entire computer vision field, TSR and TSD has also been subject to CNN renaissance. A modern CNN approach that automatically extracts multi-scale features for TSD has been applied in <ref type="bibr" target="#b28">[29]</ref>. In TSR, CNNs have been used to automatically learn feature representations as well as to perform the final classification <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>. In order to further improve the recognition accuracy, a combination of CNN and Multilayer Perceptron was applied in <ref type="bibr" target="#b33">[34]</ref>, while an ensemble classifier consisting of several CNNs was proposed in <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b31">[32]</ref>. A method that uses CNN to learn features and then applies ELM as a classifier has been applied in <ref type="bibr" target="#b34">[35]</ref>, while <ref type="bibr" target="#b35">[36]</ref> employed a deep network consisting of spatial transformer layers and a modified version of inception module. It has been shown in <ref type="bibr" target="#b36">[37]</ref> that the performance of CNN on recognition outperforms the human performance on GTSRB. A combined problems of TSR and TSD were addressed using CNNs in recent works of <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b12">[13]</ref>. In the latter, they use a heavily modified OverFeat <ref type="bibr" target="#b37">[38]</ref> network, while in the former they applied a fully convolutional network to obtain a heat map of the image, on which a region proposal algorithm was employed for detection. Finally, a separate CNN was then employed to classify the obtained regions.</p><p>Our proposed deep-learning-based approach differs from previous related works. In contrast to traditional approaches with hand-crafted features and machine learning <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b23">[24]</ref>, we propose full feature learning with end-to-end learning. Our approach also differs from other deep-learning-based trafficsign detection methods. Our method, which is based on Mask R-CNN, uses region proposal network instead of using a separate method for generating region proposals as in <ref type="bibr" target="#b5">[6]</ref>, and in contrast to <ref type="bibr" target="#b12">[13]</ref>, we employ deeper networks based on the VGG16 <ref type="bibr" target="#b38">[39]</ref> and ResNet-50 <ref type="bibr" target="#b39">[40]</ref> architectures. As opposed to both <ref type="bibr" target="#b5">[6]</ref> and <ref type="bibr" target="#b12">[13]</ref>, we also employ network pre-trained on ImageNet, which significantly reduces the need for training samples. In addition, we have implemented several extensions leading to superior performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. TRAFFIC-SIGN DETECTION WITH MASK R-CNN</head><p>In this section, we present our system for traffic-sign detection using the Mask R-CNN detector extended with several improvements. First, we present the original Mask R-CNN detector, then we present our adaptation for learning trafficsign categories, and finally, we present our data augmentation technique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Mask R-CNN</head><p>Here we briefly describe Mask R-CNN and refer the reader to <ref type="bibr" target="#b13">[14]</ref> for a more detailed description. The Mask R-CNN network <ref type="bibr" target="#b13">[14]</ref> is an extension of Faster R-CNN <ref type="bibr" target="#b40">[41]</ref>, both of which are composed of two modules. The first module is deep fully convolutional network, a so-called Region Proposal Network (RPN), that takes an input image and produces a set of rectangular object proposals, each with an objectness score. The second module is a region-based CNN, called Fast R-CNN, that classifies the proposed regions into a set of predefined categories. Fast R-CNN is highly efficient, since it shares convolutions across individual proposals. It also performs bounding box regression to further refine the quality of the proposed regions. The entire system is a single unified network, in which RPN and Fast R-CNN are merged by sharing their convolutional features. Following the recently popular terminology of neural networks with the "attention" mechanism, the RPN module tells the Fast R-CNN module where to look. Mask R-CNN then improves this system by combining the underlying network architecture with a Feature Pyramid Network (FPN) <ref type="bibr" target="#b41">[42]</ref>. With the FPN, the detector is able to improve the performance on small objects, since FPN extracts features from lower layers of the network, before the down-sampling removes important details in small objects. The underlaying network architecture, which is VGG16 <ref type="bibr" target="#b38">[39]</ref> in Faster R-CNN, is replaced with a residual network (ResNet) <ref type="bibr" target="#b39">[40]</ref> in Mask R-CNN.</p><p>Faster and Mask R-CNN are trained for the region proposal task as well as for the classification task. This is performed with a stochastic gradient descent. Mask R-CNN learns both networks simultaneously using end-to-end learning. The original Faster R-CNN implementation performed this with a 4-step optimization process that alternated between the two tasks. However, the newer end-to-end learning scheme from Mask R-CNN is also applicable to Faster R-CNN. Commonly, both networks are initialized with the ImageNet pre-trained model before they are trained on the specific domain.</p><p>Both methods enable fast detection and recognition in the test-phase. For each input image the trained model outputs a set of object bounding boxes, where each box is associated with a category label and a softmax score in the interval of [0, 1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Adaptation to traffic-sign detection</head><p>Mask R-CNN is a general method developed for the detection and recognition of general objects. In order to adapt it to the particular domain of TSD, we developed and implemented several domain specific improvements. a) Online hard-example mining: We first incorporate online hard-example mining (OHEM) into the classification learning module (Fast R-CNN module). Following the work of Shrivastava et al. <ref type="bibr" target="#b42">[43]</ref>, that introduced OHEM for Faster R-CNN, we replace the method for selecting regions of interest (ROIs) that are passed to the classification learning module. Normally, 256 ROIs per image are selected randomly, some as foreground (traffic signs) and some as background (nontraffic signs). In our approach, we replace random selection of ROIs with the selection based on their classification loss value. Regions are sorted based on their loss value and only ones with high enough loss are passed to the classification learning module. This ensures learning on samples on which the network was mistaken the most, i.e., on hard examples. We perform selection separately for the background and the foreground objects to ensure sufficient positive and negative samples during each gradient descent step.</p><p>We implement OHEM as an end-to-end learning by utilizing the existing classification module to obtain the classification losses for ROIs. Note that classification loss, which represents a criteria for selecting ROIs, is not computed for all possible ROIs generated by the RPN but only for the top ROIs based on their objectness score. We take 2000 regions and perform a non-maxima suppression (NMS) to eliminate duplicated ROIs. This is a standard approach to reduce the number of ROIs in Mask R-CNN before they are selected for learning. We experimented with using more than 2000 regions before the NMS but this significantly increased the learning time due to slower NMS without contributing to any performance gain.</p><p>b) Distribution of selected training samples: The mechanism for selecting the training samples for the region proposal network is also improved in the proposed approach. Originally, the Mask R-CNN selects ROIs randomly. This is done separately for foreground and background. However, when many small and large objects are present in the image at the same time the random selection introduces imbalance into the learning process. The imbalance arises due to large objects having a large number of ROIs that cover it, while small objects having only a small number of ROIs. Selecting samples based on this distribution will skew the learning process, since larger objects will be observed more often and favored much more than the smaller ones. To alleviate this issue we change the distribution of the selected training samples to evenly cover all sizes of the training objects. We achieve this by selecting the same number of ROIs for each object present in the image. c) Sample weighting: We incorporate additional weighting of samples during the learning process. Our evaluation showed that Mask R-CNN cannot achieve 100% recall due to missing region proposals in certain cases. We address this issue with different weighting of the training regions. During the learning, both foreground and background regions are selected; however, there are often many more background regions, since most traffic signs in images are small and only a few region proposals exists for those traffic signs. Without any weighting the learning process will observe background objects more often and will focus on learning the background instead of on the foreground. We address this problem with smaller weights for the background regions, which forces the network to learn foreground objects first. This is implemented for the training process of the region proposal network as well as for the classification network, weighting backgrounds with 0.01 for the RPN and 0.1 for the classification network. This improvement is particularly important for the RPN, since regions missed at this point in the pipeline cannot be recovered later by the classification module and would lead to poor overall recall if not addressed. d) Adjusting region pass-through during detection: Lastly, we also change the number of ROIs passed from the RPN to the classification network during the detection stage. The number of regions passed through need to be adjusted due to a large number of small objects that are commonly present in the traffic-sign domain. We increase this number from 1000 to 10,000 regions per one FPN level before the NMS. After merging ROIs from all FPN levels and performing the NMS 2000 regions are retained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Data augmentation</head><p>An important factor to consider when learning deep models is the size of the training set. Due to millions of learnable parameters the system becomes undetermined without a sufficient number of training samples. We partially address this issue with a pre-trained model, one learned on 1.2 million images of ImageNet, but we also propose an additional data augmentation. The nature of the traffic-sign domain allows us to construct a large number of new samples using artificial distortions of existing traffic-sign instances.</p><p>An additional synthetic traffic-sign instances are created by modifying segmented, real-world training samples. The traffic signs in the proposed dataset are annotated with tight bounding boxes (see <ref type="figure" target="#fig_6">Figure 5</ref>), allowing to be segmented from the training images. Two classes of distortions were performed: (i) geometric/shape distortions (perspective change, changes in scale), and (ii) appearance distortions (variations in brightness and contrast).</p><p>Before applying geometric and appearance distortions we first normalized each traffic-sign instance. For the appearance normalization, we normalized contrast of the intensity channel in the L*a*b domain, while for the geometric normalization, we calculated the homography between instance annotation points and a geometric template for a specific traffic-sign class. We manually created templates for most of the classes with the exception of several classes where this was not possible (e.g. the train crossing sign, direction signs with the shape of an arrow, etc.). We generated new synthetic instances for those classes as well but without performing geometry normalization and without applying geometric distortions to synthetic instances.</p><p>In order to generate synthetic training samples that are as realistic as possible, we followed the distribution of the training set's geometry and appearance variability. For the geometry change we estimated the distribution of Euler rotation angles (in X,Y and Z axis) of trainings examples, while for the appearance change, we estimated the distribution of averaged intensity values. We additionally estimated the distribution of scales using the size of geometry normalized (rectified) instances. We modeled all changes with a Gaussian mixture model, but used a single mixture component, K=1, for the geometry and appearance, and two mixture components, K=2, for the scale. Several examples of original, normalized and synthetically generated samples are shown in <ref type="figure" target="#fig_2">Figure 2</ref>, while  a histogram and its corresponding distributions for different distortions are depicted in <ref type="figure" target="#fig_3">Figure 3</ref>.</p><p>When generating synthetic distortions we sampled random values from the corresponding distributions. However, variance that is twice as large as the variance in the observed distribution was used to increase the likelihood of generating larger distortions. In the appearance distortion the distributions were not generic for all classes, but instead, we used different distribution for each classes. We used class specific mean instead of mean over all categories but we still applied common variance calculated from all the categories. This guarded us from generating invalid contrast values for very dark/bright categories, such as gray or white direction signs.</p><p>To emulate the real-world settings, the newly generated traffic-sign instances were inserted into the street-environmentlike background images. Background images were acquired from the subset of the BTS dataset <ref type="bibr" target="#b16">[17]</ref>, which contains no other traffic signs. At least two, and at most five, traffic signs were placed in a non-overlapping manner in random locations of each background image, avoiding the bottom central part where only the road is usually seen. With the whole augmentation process we generated enough new instances to ensured each category has at least 200 instances. This resulted in around 30,000 new traffic-sign instances spread over 8775 new training images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. THE DFG TRAFFIC-SIGN DATASET</head><p>Our dataset was acquired by the DFG Consulting d.o.o. company for the purpose of maintaining inventory of traffic signs on Slovenian roads. The RGB images were acquired with a camera mounted on a vehicle that was driven through several different Slovenian municipalities. The image data was acquired in rural as well as in urban areas. Only images containing at least one traffic sign were selected from the vast corpus of collected data. Moreover, the selection was performed in such a way that there is usually a significant scene change between any pair of selected consecutive images. Since images were acquired for the purpose of maintaining traffic-sign inventory, this allowed the image acquisition to be performed in the day-time avoiding bad weather conditions such as rain, snow and fog. Nevertheless, the dataset does include other difficult variations in the weather and the environment that are present in the real-world environment such as: rural and city/urban landscape, different levels of natural occlusions and shadows, and various ranges of a cloudy sky and direct sunlight. Images taken under winter conditions with snow cover were also included.</p><p>The dataset, termed the DFG traffic-sign dataset 1 , contains a total of 6957 images with 13,239 tightly annotated traffic-sign instances corresponding to 200 categories. The total number of instances is different for each category (see <ref type="figure" target="#fig_5">Figure 4</ref>). Each image contains annotations of all traffic signs larger than 25 pixels for any of the 200 categories in a tightly annotated polygon (see <ref type="figure" target="#fig_6">Figure 5</ref>). Categories in the dataset represent a subset of all categories from the corpus of raw images provided by the company; however, some categories in the corpus did not meet the necessary criteria to create a quality dataset. In particular, all categories in the public dataset now meet the following three criteria: (a) each category has a sufficient number of instances (at least 20 instances with a minimal bounding box size of 30 pixels), (b) each category represents a planar object and (c) each category contains traffic signs that have at least some visual consistency. Among all categories in the DFG traffic-sign dataset roughly 70 % of them correspond to traffic signs with low appearance changes, while a significantly larger appearance variability is present in the remaining 30 %. Latter signs can be of variable aspect ratio or color and can contain various text and numbers. See 200 categories of traffic signs depicted in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>Note that the dataset contains annotations as small as 25 pixels. However, annotations smaller than 30 pixels are flagged as difficult and are not considered neither for the training nor for the testing. We selected 30 pixels as a minimal size based on down-sampling of features in Faster and Mask R-CNN,  which is performed 5-times and results in 32x32 pixels being represented by 1x1 feature pixel.</p><p>A suitable train-test split was generated to provide a sufficient number of samples for both the training and the test set. A restriction was set that 25 % of traffic-sign instances for each category have to appear in the test set. For the smallest categories with only 20 instances, this ensured a minimum number of 15 samples for the training set and a minimum number of 5 samples for the test set. Images were assigned randomly to either the training or the test set. However, additional constraint mechanism was employed to ensure all images of the same physical object are always present either in the test set or in the training set but never in both of them at the same time. This was ensured by clustering images within 50 meter distance and assigning whole clusters to the training or the test set. In this way, we generated a training set with 5254 images and a test set with 1703 images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTAL EVALUATION</head><p>In this section, we perform extensive evaluation of deep learning methods that are appropriate for the traffic-sign detection and recognition. We focus on evaluating two state-of-theart, region-proposal-based methods: Faster R-CNN and Mask R-CNN. We first perform evaluation on the existing public traffic-sign dataset to establish a baseline comparison with the related work. Swedish traffic-sign dataset (STSD) is used for this purpose. Then, an extensive evaluation on newly proposed DFG traffic-sign dataset is performed with a comprehensive analysis of the proposed improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation details</head><p>A publicly available Caffe2-based, Python implementation of the Detectron <ref type="bibr" target="#b43">[44]</ref> is used for both Faster and Mask R-CNN 2 . For the Faster R-CNN, we employ the VGG16 <ref type="bibr" target="#b38">[39]</ref> network with 13 convolutional layers and 3 fully-connected layers, while for the Mask R-CNN, we employ a residual network <ref type="bibr" target="#b39">[40]</ref> with 50 convolutional layers (ResNet-50). The ResNet-50 architecture consists of 16 convolutional filters with kernel sizes of 3 × 3 or larger. Mask R-CNN also implements Feature Pyramid Network (FPN) <ref type="bibr" target="#b41">[42]</ref>, which collects features from different layers of the network to capture the information from small objects, which may be removed in higher layers due to down-sampling. Both networks are initialized with a model pre-trained on ImageNet as provided by <ref type="bibr" target="#b43">[44]</ref>. We also experimented with larger variant of the residual network using 101 layers (ResNet-101), but performance did not improve compared to ResNet-50. We therefore focused only on the ResNet-50, which at the same time is faster with half the layers of ResNet-101.</p><p>Both methods use similar learning hyper-parameters. A learning rate of 0.001 is used for Faster R-CNN with a weight decay of 0.0005, while a learning rate of 0.0025 and a weight decay of 0.0001 is used for Mask R-CNN. Both approaches also use momentum of 0.9. The same hyperparameters are used in all experiments. Note that the same hyper-parameters are used in <ref type="bibr" target="#b43">[44]</ref> to pre-train the model on ImageNet dataset. Both methods are trained end-to-end with simultaneous learning of both the region proposal network and the classification network. We learn both methods for 95 epochs and reduce the learning rate by a factor of 10 at the 50th and 75th epoch. We use two images per batch per GPU and train on STSD with 2 GPUs and on DFG dataset with 4 GPUs. This resulted in effectively using 4 images per batch on the STSD and 8 images per batch on the DFG dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Performance metrics</head><p>Several different metrics are used in this study to evaluate the proposed approach. As a primary metric, we report mean average precision (mAP), which is commonly used in the evaluation of visual object detectors. We use two variants of the mAP: (i) mAP 50 , based on the PASCAL visual object challenge <ref type="bibr" target="#b44">[45]</ref>, and (ii) mAP 50:95 , based on the COCO challenge <ref type="bibr">[46]</ref>. Both metrics define a minimal intersectionover-union (IoU) overlap with the groundtruth region for a detection to be considered as a true positive, and both compute average precision (AP) as the area under the precision-recall curve to accurately capture the trade-off between the miss rate and the false-positive rate. AP is calculated for each category independently and the final metric consists of AP For comparison with the state-of-the-art, we also report precision and recall values at best F-measure and their corresponding error rates, i.e. false-positive rate as 1 − precision and miss rate as 1 − recall, respectively. The false-positive rate shows how many detections are false, while the miss rate reveals how many traffic signs were not detected at all.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison to the state-of-the-art</head><p>Although many previously proposed approaches exist, it is quite difficult to perform a reliable comparison with those approaches, since they are mostly evaluated on non-public datasets or, only on the TSR task. To this end, we evaluated the proposed method on the Swedish traffic-sign dataset (STSD), comparing the results to the previously best performing methods published in <ref type="bibr" target="#b5">[6]</ref>, and indirectly to other methods reported therein.</p><p>The STSD benchmark contains around 20 categories with simple traffic signs in over 19,236 images separated equally into the training (denoted Set1 in STSD) and the test set (denoted Set2). However, only a subset of 3777 images from both sets contain annotations (denoted as Part0 in each set). We follow the evaluation protocol of <ref type="bibr" target="#b5">[6]</ref> and use only ten categories with images from Set1Part0 for the training and images from Set2Part0 for the testing. For fair evaluation with <ref type="bibr" target="#b5">[6]</ref>, we consider only annotations with bounding box sizes of at least 50 pixels. The remaining annotations are ignored in both the train and the test stage. Due to the GPU memory limitations, we resized images to have image size of at least 918 pixels (i.e., both width and height are at least 918 pixels). For fair comparison between different architectures, the same image size was used in all variants of Faster/Mask R-CNN. We did not use data augmentation in this experiment.</p><p>Detailed results on STSD are reported in <ref type="table" target="#tab_2">Tables I and II</ref>, with the corresponding error rates in <ref type="figure" target="#fig_7">Figure 6</ref>. When focusing on the related work and Faster/Mask R-CNN without our adaptations it is clear that pre-computed region proposals from R-CNN (as reported in <ref type="bibr" target="#b5">[6]</ref>) perform worse than the newer R-CNN variants with the region proposal network. Error rates for R-CNN are twice as large as for the Faster/Mask R-CNN. On the other hand, the fully convolutional method (FCN)  proposed by <ref type="bibr" target="#b5">[6]</ref> achieves a significantly lower false-positive rate of 2.3 % than both Faster and Mask R-CNN, but has a slightly worse miss rate of 7.1 %. Faster and Mask R-CNN have a lower miss rate by 1 percentage point (pp.). The standard mAP 50 metric in <ref type="table" target="#tab_2">Table II</ref>  Results also show that the best performance is obtained when our adaptations are applied to the Mask R-CNN. Our proposed approach, in this case, achieves mAP 50 of 95.2 %, with average false-positive rate of 2.5 % and average miss rate of 3.3 %. Compared to the related work, the FCN [6] achieves a similar false-positive rate but has at least twice as large miss rate at 7.1 %. Improvements in our approach are better reflected in F-measure, which is defined as a harmonic mean between precision and recall. Our approach clearly outperforms the state-of-the-art with 2 pp higher F-measure. Those improvements directly stem from our proposed adaptations and not from the Faster/Mask R-CNN as average miss and false-positive rates without our adaptations are still 6.6 % and 4.7 %, respectively, while they are reduced to only 3.3 % and 2.5 % with the proposed improvements. This is reflected in an improved F-measure and in mAP 50 as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Evaluation on DFG traffic-sign dataset</head><p>Next, the proposed method is evaluated on the DFG trafficsign dataset. We use the train-test split as presented in Section IV with 200 categories in 5254 training and 1703 testing images, and using only annotations with at least 30 pixels in size. Annotations below 30 pixels are ignored during training and during evaluation we ignore detections of those objects to prevent penalizing the detector when it correctly detects small objects. We further resize images for both the training and the testing due to memory limitations. We resize images in all variants of Faster/Mask R-CNN to have image sizes of at least 840 pixels in both width and height. This was made for fair comparison under the same hardware limitations for all network models. Considering images are Full-HD with the image hight of 1080 pixels, this change represents slightly less than a 25 % reduction in size. Region proposal evaluation: We first evaluate the region proposal network separately from the classification network. This allows us to assess the quality of region proposals as generated by the RPN before they are passed to the classification module. We take top N regions from the RPN and observe miss rate and recall rate of all annotated traffic signs. To ensure correct balance between categories with either small or large number of instances, we calculate metric for individual categories and then report the average over all categories.</p><p>Results are reported in <ref type="figure" target="#fig_8">Figure 7</ref>, with (a) -(b) showing results when all annotations are considered and with (c) -(d), for smaller traffic signs only, i.e., when considering only groundtruth traffic signs that are 30 − 50 pixels in size. In both cases, we report miss rate over the top-n regions using an IoU overlap of 0.50 in (a) and (c), and recall over different IoU overlaps using the top 5000 region proposals in (b) and (d). <ref type="figure" target="#fig_8">Figure 7b</ref> first reveals that Faster R-CNN performs worse than the other methods. This is particularly evident at higher IoU overlaps where Faster R-CNN performs more than 5 pp worse.</p><p>The miss rates of various top-n regions, shown in <ref type="figure" target="#fig_8">Figure 7a</ref>, demonstrate that all methods perform extremely well with over 99 % of all traffic signs found. However, only our proposed method achieves close to zero miss rate, and as indicated by the recall over IoU overlaps in <ref type="figure" target="#fig_8">Figure 7b</ref>, the proposed method is able to retain higher recall at higher overlap values. This suggests that our adaptations decrease the miss rate of the RPN and higher quality regions can be produced, i.e., regions with high overlap with the groundtruth. Moreover, improvements are more significant in smaller regions, as shown in <ref type="figure" target="#fig_8">Figure 7c</ref>  and 7d. In this case, our adaptation achieves a significantly better miss rate than Faster/Mask R-CNN that did not use our adaptation. Even at a more liberal IoU overlap of 0.50, the standard approach achieves a 3 % miss rate, while our adaptation achieves a miss rate close to zero. This difference is well observed in <ref type="figure" target="#fig_8">Figure 7d</ref>, showing our proposed method achieving higher recall rates at larger IoU. Improvements in the miss rate at this level are important for the whole pipeline, since objects missed by the region proposals at this stage cannot be recovered later by the classification network. Results show that Mask R-CNN is unable to achieve full detection of all objects, particularly for small objects; however, our adaptations overcome this issue and achieve a miss rate near zero.</p><p>Full pipeline evaluation: Next, we evaluate the whole detection pipeline with the RPN and classification networks combined. We report our results in terms of mean average precision (mAP) over all 200 categories as well as in terms of maximal possible recall that can be attained with the final detections when thresholding the score at 0.01 This value is directly related to the miss rate and the recall rate of region proposals in the previous section, and when both values are compared, we can deduce how many traffic signs were missed due to poor performance of the classification network only.</p><p>Results are reported in <ref type="table" target="#tab_2">Table III</ref> and clearly show that Faster R-CNN performs the worst among all methods, while the best results are achieved with our adaptations for Mask R-CNN. Nevertheless, all methods achieve mAP 50 of over 90 %. Compared to the original Mask R-CNN, our proposed adaptations already improve results when measured in mAP 50 and maximal recall/miss rate metrics, even without data augmentation. The performance in mAP 50 metric is improved from 93 % to over 95 %, and the miss rate error is almost halved from 5.4 % to 3.5 %. Slightly worse results are achieved in the mAP 50:95 metric but this is improved when augmentation is enabled. With augmentation we slightly improve mAP 50 , and significantly improve mAP 50:95 from 82 − 83% with the original Mask R-CNN to 84.4 % for when our adaptations and data augmentation is used. Data augmentation has contributed mostly to improving the precision of bounding boxes. Results also reveal that while overall miss rate has been reduced by half compared to the original Mask R-CNN, there still remain 3.5 % missed objects despite, as shown in the previous section, having near zero miss rate in the region proposals. This points to traffic-sign detections being lost by the classification network. Different traffic-sign sizes: We also perform evaluation considering different traffic-sign sizes with the results reported in <ref type="table" target="#tab_3">Table IV</ref>. This analysis reveals poor performance with smaller objects when using original Faster and Mask R-CNN. The difference in both mAP 50 and the maximal recall rate between small and large objects is around 2 pp . However, with our adaptations, the detection of smaller objects is improved significantly and completely eliminates the performance gap between detection of smaller and larger objects. Moreover, this is achieved on top of the improved detection for larger objects.</p><p>Deeper Residual Network: We also show results with ResNet-101 architecture in <ref type="table" target="#tab_3">Table IV</ref>. ResNet-101 performs similarly to the smaller ResNet-50 in most cases. When our improvements are not included, ResNet-101 performs less than 0.2 pp better; however, this reverses when our improvements are included. The difference between both of them still remains minimal at below 0.4 pp . Since ResNet-101 is larger with twice as many number of layers with more computational resources required, the ResNet-50 represents a significantly better choice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. QUALITATIVE ANALYSIS</head><p>In this section, we demonstrate the performance of our approach on traffic-sign detection with additional qualitative analysis. We focus only on the best performing model, namely Mask R-CNN using ResNet-50 with our adaptations and data A per-class distribution of AP 50 is depicted in <ref type="figure" target="#fig_9">Figure 8</ref>. This graph clearly shows that a large number of traffic-sign classes (108) are detected and recognized with average precision of 100 %, i.e. with no errors. For the remaining categories our approach still achieved AP of above 90 % on 60 of them, and above 80 % on 23 of them. <ref type="figure" target="#fig_10">Figure 9</ref> further shows the traffic-sign classes with their corresponding AP 50 sorted by their AP 50 in descending order.</p><p>The best performing categories at the top of the list are mostly traffic signs with low intra-category variations, i.e. with fixed sizes and fixed appearance. This includes various triangular danger signs, circular prohibitory signs, speed limit signs, rectangular information signs, etc. On the other hand, the worst performing signs at the bottom are traffic signs with a large variation of their sizes/aspect ratios as well as with a large intra-category variations, i.e., their content significantly varies from instance to instance. This includes particularly complex class of mirrors (both rectangular and round mirrors), speed  feedback signs, various direction signs and signs marking the start or the end of the towns.</p><p>Traffic signs with high intra-category variations and good performance: <ref type="figure" target="#fig_10">Figure 9</ref> reveals several traffic signs with extremely good detection rate despite having large intra-category variations in their appearance. Samples for three such trafficsign categories are depicted in <ref type="figure" target="#fig_0">Figure 10</ref>, namely they are: (i) large-direction-with-separate-lanes, (ii) left-arrow-shapeddirection and (iii) right-gray-direction. Each row in this figure depicts one category with eight instances. For clarity we display only the relevant part of the image. True detections are shown in green, false detections in red and missing detections in magenta. Examples are also sorted by their descending detection score from left-to-right. Therefore if true (green) and false (red) positive detections can be successfully separated with a threshold then false detections can be trivially eliminated by setting an appropriate detection threshold. Note that this is important when looking at false detections as many of them are not problematic at all.</p><p>When focusing on the large-direction-with-separate-lanes traffic-sign category in the first row in <ref type="figure" target="#fig_0">Figure 10</ref>, an extremely good performance is clearly shown for the traffic signs that have quite significant variation in their content as well as large variation in their sizes and aspect ratios. The first image in the top row depicts a good example of this as the traffic sign was detected with a high score despite having completely different color combination than other instances of the same class. Several detected instances are also quite small, yet our approach successfully detects them. Moreover, the last image in the first row shows a false detection of a small instance; however, a close inspection reveals that it is a correct detection. This instance was not annotated in the dataset due to small size and high occlusion of the tree.</p><p>The second row in <ref type="figure" target="#fig_0">Figure 10</ref> depicts detections of a leftarrow-shaped-direction traffic sign. This category is fairly difficult to detect as aspect ratios vary quite significantly from instance to instance, mostly due to wide viewing angles, yet the detector did not have significant issues finding them. The second-to-last example in the second row is also significantly cropped; however, the detector is still able to correctly find it.</p><p>Finally, detections for the right-gray-direction traffic sign are shown in the last row in <ref type="figure" target="#fig_0">Figure 10</ref>. Detection of this category is difficult mostly due to significant variation of the content. Those traffic signs also often appear side-byside in multiple rows which makes it difficult to generate the correct region proposal. Nevertheless, most instances have been correctly found.</p><p>Traffic signs with poor performance and low intracategory variations: Next, we focus on three worst performing traffic signs despite having low appearance variation within a category, namely: (i) left-into-right-lane-merger, (ii) traincrossing and (iii) work-in-progress. Samples are depicted in <ref type="figure" target="#fig_0">Figure 11</ref> and are organized in a similar manner as in <ref type="figure" target="#fig_0">Figure 10</ref>, with eight examples per category in a row, sorted by their descending detection score.</p><p>The worst results are achieved for the left-into-right-lanemerger traffic sign with the AP 50 of 57 %. Mask R-CNN correctly detects four out of five test instances, but appears to detect four false traffic signs as well, as can be seen in the top row. However, those false detections should not be considered problematic as the traffic sign is identical to the left-intoright-lane-merger sign with the only difference in the distance value printed below the sign. Since the correct category is also detected (shown with the dashed green line), those false detections would be eliminated by the across-category nonmaxima suppression, meaning that even in this case the issue is not as bad as it might seem. Still, such extremely minor differences between those two categories appear to pose a challenge for deep learning and point to a existing limitations of deep learning methods. The detector is also exhibiting inferior performance for the train-crossing traffic sign as seen in the second row in <ref type="figure" target="#fig_0">Figure 11</ref>. The reason in this case can be found in two missed detections out of total six traffic signs. Both missed objects are very small, with one having fairly wide viewing angle, making the detection also extremely difficult. A few detections on false objects are also visible, most likely due to the presence of cross-like shape. However, they do not contribute to poor performance due to their low detection score.</p><p>The primary issue for the work-in-progress sign, depicted in the third row of <ref type="figure" target="#fig_0">Figure 11</ref>, is high miss rate. Three out of eleven traffic signs are not detected. Most objects missed are also fairly small. The exception is the instance depicted in the last column where a significant occlusion would pose difficulty even for humans-its category was deduced from its inside color and the context.</p><p>Overall detection: Despite some missed detections shown in <ref type="figure" target="#fig_0">Figure 11</ref>, the detector still preforms extremely well even for several difficult cases. For instance, the second example in the first row of <ref type="figure" target="#fig_0">Figure 11</ref> is extremely difficult to detect due to a large viewing angle, but the detector still managed to find it-even with a large score. The detector was also able to find some fairly small instances, such as ones in the first and the last row.</p><p>Good performance is also reflected in <ref type="figure" target="#fig_0">Figure 12</ref> where all traffic-sign detections are displayed for a couple of fullresolution images. This figure shows detections of several complex instances with occlusions and small traffic sign sizes; however, the detector still performs extremely well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. DISCUSSION AND CONCLUSION</head><p>In this work, we have addressed the problem of detecting and recognizing a large number of traffic-sign categories for the main purpose of automating traffic-sign inventory management. Due to a large number of categories with small interclass but high intra-class variability, we proposed detection and recognition utilizing an approach based on the Mask R-CNN <ref type="bibr" target="#b13">[14]</ref> detector. The system provides an efficient deep network for learning a large number of categories with an efficient and fast detection. We proposed several adaptations to Mask R-CNN that improve the learning capability on the domain of traffic signs. Furthermore, we proposed a novel data augmentation technique based on the distribution of geometric and appearance distortions. As an important contribution, we also present a novel dataset, termed the DFG traffic-sign dataset, with a large number of traffic-sign categories that have low inter-class and high intra-class variability. This dataset has been made publicly available together with the code for our improvements, allowing the research community to make further progress on this problem and enabling reliable and fair comparison of different methods on a large-scale traffic-sign detection problem. We also extensively evaluated our proposed improvements and compared them against the original Faster and Mask R-CNN. Our evaluation on the DFG and the Swedish traffic-sign datasets showed that the proposed adaptations improve the performance of Mask R-CNN in several metrics. This includes improvement in the miss rate of the RPN network for smaller objects, improvement in the overall recall of the full pipeline for both small and large objects, as well as improvement in the overall performance in the mean average precision.</p><p>Our qualitative analysis further revealed how a 2 − 3% average error rate is reflected in actual detections. This is well demonstrated in <ref type="figure" target="#fig_0">Figure 12</ref> where detections of several complex traffic-sign categories are depicted. Overall, we showed that the deep learning based approach is able to achieve extremely good performance for many traffic-sign categories, including several complex ones with large intra-class variability. Large error rates for problematic traffic-sign categories are mostly due to similarity to other categories, wide viewing angles and large occlusions. However, those issues do not pose a problem for the application of maintaining an accurate record of traffic-sign inventory. They can be mitigated by the detection over several video frames or matching 3D locations from stereo cameras. In particular, this system is already being deployed for traffic-sign inventory management on Slovenian roads. However, the proposed solution is also applicable to other problems requiring the capability of traffic-sign detection such as autonomous driving and advanced driver-assistance systems.</p><p>Despite excellent performance of the proposed approach there is still room for improvement. Our analysis revealed that the ideal performance is still not achieved, mostly due to several missed detections that are being lost by the classification network. Future improvements should focus on improving this part of the system.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>The DFG traffic-sign dataset consists of 200 categories including large number of traffic signs with high intra-category appearance variations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>Several examples of traffic-sign instances as generated during the process of data augmentation: (a) original image on the left, (b) normalized geometry and appearance in the middle, and (c) generated samples with synthetic distortions on the right.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>Distributions of traffic-sign distortions computed for rotation in the top row, appearance (i.e. brightness) in the bottom left side and scale in the bottom right side. Red lines represent the Gaussian distributions, which are sampled when generating new examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 :</head><label>4</label><figDesc>Distribution of number of instances over categories in the DFG traffic-sign dataset. Horizontal red dashed line represents 20 instances per category, which we use as a cutoff point. Note, the distribution is shown in the logarithmic scale.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 :</head><label>5</label><figDesc>Several examples of traffic signs in the DFG traffic-sign dataset with their corresponding annotation masks showing the precision of the annotation mask.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 :</head><label>6</label><figDesc>Miss rates (1−recall) and false positive (1−precision) rates on Swedish traffic-sign dataset averaged over ten categories. Values are calculated at ideal F-measure. Note, smaller values are better.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 :</head><label>7</label><figDesc>Miss rate and recall for region proposals generated by the RPN. Graphs (a) and (b) show results when considering all valid annotations, while graphs (c) and (d), when considering only groundtruth traffic signs in sizes of 30 − 50 pixels. We show in (a) and (c) miss rate over top-n regions using IoU overlap of 0.50, and in (b) and (d), recall rate over different IoU overlaps using the top 5000 region proposals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 :</head><label>8</label><figDesc>Sorted per-class AP 50 distribution on the test set of the DFG traffic-sign dataset. The blue bars depict Mask R-CNN (ResNet-50) with our improvements and data augmentation, while green and red bars show change in performance (increased for green and decreased for red) compared to the base Mask R-CNN (ResNet-50) without our improvements.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 9 :</head><label>9</label><figDesc>DFG traffic-sign categories sorted by average precision (AP 50 ) calculated when using Mask R-CNN ResNet-50 with our adaptations and data augmentation. augmentation. All results in this section are reported on the test set of the DFG traffic-sign dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 10 :</head><label>10</label><figDesc>Examples of complex traffic signs with variable content and good detection on the test set of the DFG traffic-sign dataset. True positives are depicted in green, false positives in red, and missing detections (false negatives) in magenta. (*) Note, the last detection in the first row is not false since actual traffic sign was not annotated due to high occlusion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 11 :</head><label>11</label><figDesc>Examples of traffic signs with fixed content but poor detection on the test set of the DFG traffic-sign dataset. Truepositive detections are marked in green, false positives in red and missing detections (false negatives) in magenta. (*) Note that false detections in the first row occur due to two almost identical traffic-sign categories in the dataset (one with distance label below and one without). True detections with the other category detector are shown in dashed green line.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 12 :</head><label>12</label><figDesc>Examples of detections on the test set of the DFG traffic-sign dataset. True detections shown in green and missing, in magenta.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Deep Learning for Large-Scale Traffic-Sign Detection and Recognition Domen Tabernik and Danijel Skočaj Faculty of Computer and Information Science, University of Ljubljana Večna pot 113, 1000 Ljubljana {domen.tabernik,danijel.skocaj}@fri.uni-lj.si</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>Evaluation on Swedish traffic-sign dataset (STSD) with reported averaged values over ten categories.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Mask R-CNN</cell></row><row><cell>Average</cell><cell>[6] R-CNN</cell><cell>[6] FCN</cell><cell>R-CNN Faster</cell><cell>(ResNet-50)</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>No adapt.</cell><cell>Adapt. (ours)</cell></row><row><cell>Precision</cell><cell>91.2</cell><cell>97.7</cell><cell>95.4</cell><cell>95.3</cell><cell>97.5</cell></row><row><cell>Recall</cell><cell>87.2</cell><cell>92.9</cell><cell>94.0</cell><cell>93.6</cell><cell>96.7</cell></row><row><cell>F-measure</cell><cell>88.8</cell><cell>95.0</cell><cell>94.6</cell><cell>93.8</cell><cell>97.0</cell></row><row><cell>mAP 50</cell><cell>/</cell><cell>/</cell><cell>94.3</cell><cell>94.9</cell><cell>95.2</cell></row><row><cell cols="6">values averaged over all categories. A fixed IoU overlap is</cell></row><row><cell cols="6">used in the mAP 50 -using the PASCAL-based IoU overlap of</cell></row><row><cell cols="6">0.50-however, in mAP 50:95 , the reported value is an average</cell></row><row><cell cols="6">of mAP values calculated at a range of IoU overlap values.</cell></row><row><cell cols="6">The reported values are averaged over the IoU overlap range</cell></row></table><note>of [0.50, 0.95] with 0.05 increments, the same range as used in the COCO detection challenge [46]. Thus, the COCO-based mAP gives more emphasis on the quality of region overlaps, while the PASCAL-based mAP ignores that aspect.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE I :</head><label>I</label><figDesc>Detailed results on Swedish traffic-sign dataset (STDS) for different categories.</figDesc><table><row><cell></cell><cell>FCN [6]</cell><cell></cell><cell cols="2">Faster R-CNN</cell><cell></cell><cell cols="3">Mask R-CNN (ResNet-50)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Traffic Sign</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">No adaptations</cell><cell cols="3">With adaptations (our)</cell></row><row><cell></cell><cell>Prec.</cell><cell>Rec.</cell><cell>Prec.</cell><cell cols="2">Rec. AP 50</cell><cell>Prec.</cell><cell cols="2">Rec. AP 50</cell><cell>Prec.</cell><cell>Rec.</cell><cell>AP 50</cell></row><row><cell>PED. CROS.</cell><cell>100.0</cell><cell>95.2</cell><cell>92.6</cell><cell>92.6</cell><cell>94.1</cell><cell>100.0</cell><cell>97.5</cell><cell>98.2</cell><cell>99.2</cell><cell>97.6</cell><cell>97.6</cell></row><row><cell>PASS RIGHT SIDE</cell><cell>95.3</cell><cell>93.8</cell><cell>98.1</cell><cell>98.1</cell><cell>99.5</cell><cell>94.8</cell><cell>98.2</cell><cell>98.6</cell><cell>100.0</cell><cell>98.2</cell><cell>99.8</cell></row><row><cell>NO STOP/STAN</cell><cell>100.0</cell><cell>75.0</cell><cell>92.3</cell><cell>92.3</cell><cell>86.5</cell><cell cols="2">81.2 100.0</cell><cell>95.4</cell><cell cols="2">86.7 100.0</cell><cell>83.9</cell></row><row><cell>50 SIGN</cell><cell cols="2">100.0 100.0</cell><cell>81.2</cell><cell>92.9</cell><cell>90.3</cell><cell cols="2">87.5 100.0</cell><cell>97.5</cell><cell>90.0</cell><cell>96.4</cell><cell>96.9</cell></row><row><cell>PRIORITY ROAD</cell><cell>100.0</cell><cell>98.9</cell><cell>98.7</cell><cell>95.1</cell><cell>92.1</cell><cell>97.5</cell><cell>97.5</cell><cell>96.9</cell><cell>98.7</cell><cell>92.9</cell><cell>89.8</cell></row><row><cell>GIVE WAY</cell><cell>96.7</cell><cell>96.7</cell><cell>100.0</cell><cell>94.1</cell><cell>94.1</cell><cell>100.0</cell><cell>91.4</cell><cell>91.4</cell><cell>100.0</cell><cell>94.1</cell><cell>94.1</cell></row><row><cell>70 SIGN</cell><cell cols="2">100.0 100.0</cell><cell cols="3">100.0 100.0 100.0</cell><cell cols="3">100.0 100.0 100.0</cell><cell cols="3">100.0 100.0 100.0</cell></row><row><cell>80 SIGN</cell><cell>94.4</cell><cell>77.3</cell><cell>100.0</cell><cell>95.2</cell><cell>95.2</cell><cell cols="2">95.2 100.0</cell><cell>99.8</cell><cell cols="3">100.0 100.0 100.0</cell></row><row><cell>100 SIGN</cell><cell cols="2">90.5 100.0</cell><cell>94.1</cell><cell>88.9</cell><cell>92.5</cell><cell>100.0</cell><cell>61.1</cell><cell>74.8</cell><cell>100.0</cell><cell>93.8</cell><cell>93.8</cell></row><row><cell>NO PARKING</cell><cell>100.0</cell><cell>92.1</cell><cell>96.8</cell><cell>90.9</cell><cell>98.5</cell><cell>96.7</cell><cell>90.6</cell><cell>95.9</cell><cell>100.0</cell><cell>93.9</cell><cell>96.5</cell></row><row><cell>Averaged</cell><cell>97.7</cell><cell>92.9</cell><cell>95.4</cell><cell>94.0</cell><cell>94.3</cell><cell>95.3</cell><cell>93.6</cell><cell>94.9</cell><cell>97.5</cell><cell>96.7</cell><cell>95.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>also shows Faster R-CNN and Mask R-CNN with ResNet-50 achieving mAP 50 of 94.3 % and 94.9 %, respectively.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE III :</head><label>III</label><figDesc>Results on DFG traffic-sign dataset.</figDesc><table><row><cell></cell><cell>Faster</cell><cell cols="3">Mask R-CNN (ResNet-50)</cell></row><row><cell></cell><cell>R-CNN</cell><cell>No adapt.</cell><cell>With adapt.</cell><cell>With adapt. and data augment.</cell></row><row><cell>mAP 50</cell><cell>92.4</cell><cell>93.0</cell><cell>95.2</cell><cell>95.5</cell></row><row><cell>mAP 50:95</cell><cell>80.4</cell><cell>82.3</cell><cell>82.0</cell><cell>84.4</cell></row><row><cell>Max recall</cell><cell>93.8</cell><cell>94.6</cell><cell>96.5</cell><cell>96.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE IV :</head><label>IV</label><figDesc>Results on DFG traffic-sign dataset when considering different sizes of traffic signs.</figDesc><table><row><cell></cell><cell cols="2">Faster R-CNN</cell><cell cols="2">Mask R-CNN</cell><cell></cell><cell></cell><cell cols="3">Mask R-CNN with adapt. and data augmentation (ours)</cell><cell></cell></row><row><cell>Traffic-sign size</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(% signs retained)</cell><cell></cell><cell></cell><cell cols="2">ResNet-50</cell><cell cols="2">ResNet-101</cell><cell cols="2">ResNet-50</cell><cell cols="2">ResNet-101</cell></row><row><cell></cell><cell>Max recall</cell><cell>mAP 50</cell><cell>Max recall</cell><cell>mAP 50</cell><cell>Max recall</cell><cell>mAP 50</cell><cell>Max recall</cell><cell>mAP 50</cell><cell>Max recall</cell><cell>mAP 50</cell></row><row><cell cols="2">min 30 px (100%) 93.8</cell><cell>92.4</cell><cell>94.6</cell><cell>93.0</cell><cell>94.8</cell><cell>93.2</cell><cell>96.5</cell><cell>95.5</cell><cell>96.1</cell><cell>95.2</cell></row><row><cell>min 40 px (89%)</cell><cell>96.1</cell><cell>95.0</cell><cell>96.8</cell><cell>95.3</cell><cell>96.8</cell><cell>95.3</cell><cell>97.4</cell><cell>96.7</cell><cell>97.0</cell><cell>96.4</cell></row><row><cell>min 50 px (80%)</cell><cell>96.6</cell><cell>95.0</cell><cell>96.7</cell><cell>94.9</cell><cell>96.8</cell><cell>95.2</cell><cell>97.2</cell><cell>96.0</cell><cell>96.8</cell><cell>95.5</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The dataset, termed DFG traffic-sign dataset, is publicly available at http://www.vicos.si/Downloads/DFGTSD</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Our proposed improvements have been implemented in the Detectron framework and are publicly available in the GitHub repository: https://github.com/skokec/detectron-traffic-signs</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>This work was in part supported by the ARRS research project L2-6765 (ViLLarD) and ARRS research programme P2-0214. We would also like to thank the company DFG Consulting d.o.o., in particular Domen Smole, Simon Jud and mag. Tomaž Gvozdanović, for capturing and annotating images and for their help in creating the dataset.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Detection, classification, and mapping of U.S. traffic signs using google street view images for roadway inventory management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Balali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Golparvar-Fard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Visualization in Engineering</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Automated road sign inventory system based on stereo vision and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer-Aided Civil and Infrastructure Engineering</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Evaluation of Multiclass Traffic Sign Detection and Classification Methods for U.S. Roadway Asset Inventory Management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Balali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Golparvar-Fard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computing in Civil Engineering</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">4015022</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Traffic sign segmentation and classification using statistical learning methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Lillo-Castellano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mora-Jimenez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Figuera-Pozuelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Rojo-Alvarez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">153</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="286" to="299" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A novel pLSA based Traffic Signs Classification System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haloi</surname></persName>
		</author>
		<idno>abs/1503.0</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Traffic sign detection and recognition using fully convolutional network guided proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">214</biblScope>
			<biblScope unit="page" from="758" to="766" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Combining Traffic Sign Detection with 3D Tracking Towards Better Driver Assistance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Prisacariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Emerging Topics in Computer Vision and its Applications</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="425" to="446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Visual Analysis in Traffic &amp; Re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mogelmose</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
		<respStmt>
			<orgName>Faculty of Engineering and Science, Aalborg University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Man vs. computer: Benchmarking machine learning algorithms for traffic sign recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stallkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schlipsing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Igel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="323" to="332" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Detection of traffic signs in real-world images: The German traffic sign detection benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Houben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stallkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schlipsing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Igel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013-08" />
			<publisher>IJCNN. Ieee</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Vision-Based Traffic Sign Detection and Analysis for Intelligent Driver Assistance Systems: Perspectives and Survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mogelmose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Moeslund</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1484" to="1497" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Real-time traffic-sign recognition using tree classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zaklouta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Stanciulescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Traffic-Sign Detection and Classification in the Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kaiming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Wali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Hannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Samad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comparative Survey on Traffic Sign Detection and Recognition: a Review</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="40" to="44" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Przeglad Elektrotechniczny</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Traffic Sign Detection and Recognition using Features Combination and Random Forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ellahyani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Aansari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">E</forename><surname>Jaafari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJACSA</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Multi-view traffic sign detection, recognition, and 3D localisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>WACV</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A computer vision assisted geoinformation inventory for traffic infrastructure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Segvic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Brkic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th International IEEE Conference on Intelligent Transportation Systems (ITSC)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="66" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An Efficient Method for Traffic Sign Recognition Based on Extreme Learning Machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Using fourier descriptors and spatial models for traffic sign recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Analysis</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="238" to="249" />
			<date type="published" when="2011-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A novel traffic sign detection method via color segmentation and robust shape matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">169</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="77" to="88" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Color Fused Multiple Features for Traffic Sign Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIMCS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="84" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Traffic sign detection via interest region extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Salti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Petrelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Fioraio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Stefano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1039" to="1049" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Real-Time Detection and Recognition of Road Traffic Signs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Greenhalgh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirmehdi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Large scale sign detection using HOG feature variants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Overett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Petersson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Vehicles Symposium</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="326" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Traffic sign recognition -How far are we from the solution?&quot; in IJCNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013-08" />
			<publisher>Ieee</publisher>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Real-time traffic sign recognition in three stages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zaklouta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Stanciulescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics and Autonomous Systems</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="16" to="24" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Supervised Low-Rank Matrix Recovery for Traffic Sign Recognition in Image Sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE SPL</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="241" to="244" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Traffic sign detection based on convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNN</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multi-column deep neural network for traffic sign classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="333" to="338" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Traffic sign recognition with multi-scale Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNN</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2809" to="2813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Traffic Sign Recognition With Hinge Loss Trained Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Convolutional Neural Networks for Croatian Traffic Signs Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vukotić</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krapac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Šegvić</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CCVW</publisher>
			<biblScope unit="page" from="15" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A committee of neural networks for traffic sign classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNN</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1918" to="1921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Traffic Sign Recognition Using Deep Convolutional Networks and Extreme Learning Machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in IScIDE</title>
		<imprint>
			<biblScope unit="volume">9242</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="272" to="280" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Traffic Sign Classification Using Deep Inception Based Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haloi</surname></persName>
		</author>
		<idno>abs/1511.0</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">The German Traffic Sign Recognition Benchmark: A multi-class classification competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stallkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schlipsing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Igel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011-07" />
			<publisher>IJCNN. Ieee</publisher>
			<biblScope unit="page" from="1453" to="1460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">OverFea : Integrated Recognition, Localization and Detection using Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Very Deep Convolutional Networks for Large-Scale Image Recoginition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Feature Pyramid Networks for Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Training Region-Based Object Detectors with Online Hard Example Mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="761" to="769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Detectron</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron6" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://www.pascal-network.org/challenges/VOC/voc2008/workshop/index.html" />
		<title level="m">The PASCAL Visual Object Classes Challenge 2008 (VOC2008) Results</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">LNCS</title>
		<imprint>
			<biblScope unit="volume">8693</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>LNCS</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Since 2015 he is enrolled in the doctoral program of Faculty of Computer and</title>
	</analytic>
	<monogr>
		<title level="m">Domen Tabernik received a bachelors degree in Computer and Information Science in 2010</title>
		<imprint/>
		<respStmt>
			<orgName>Cognitive System Laboratory of Faculty of Computer and Information Science in University of Ljubljana</orgName>
		</respStmt>
	</monogr>
	<note>From 2010 he has been working as a computer vision researcher in the Visual. Information Science in University of Ljubljana where he is working on the topics of compositional hierarchies and deep learning</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">He is the head of the Visual Cognitive Systems Laboratory. He obtained the Ph.D. in computer and information science from the University of Ljubljana in 2003. His main research interests lie in the fields of computer vision, pattern recognition, machine learning</title>
		<imprint/>
		<respStmt>
			<orgName>Danijel Skočaj is an associate professor at the University of Ljubljana, Faculty of Computer and Information Science</orgName>
		</respStmt>
	</monogr>
	<note>and cognitive robotics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
