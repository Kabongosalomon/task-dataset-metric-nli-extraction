<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Geometry-Aware Instance Segmentation with Disparity Maps</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Ying</forename><surname>Wu</surname></persName>
							<email>choyingw@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern</orgName>
								<address>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Hu</surname></persName>
							<email>xiaoyan@argo.ai</email>
							<affiliation key="aff1">
								<address>
									<settlement>Argo</settlement>
									<region>AI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Happold</surname></persName>
							<email>mhappold@argo.ai</email>
							<affiliation key="aff1">
								<address>
									<settlement>Argo</settlement>
									<region>AI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiangeng</forename><surname>Xu</surname></persName>
							<email>qiangenx@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern</orgName>
								<address>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Neumann</surname></persName>
							<email>uneumann@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern</orgName>
								<address>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Geometry-Aware Instance Segmentation with Disparity Maps</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>page for codes and data. The full paper is available here.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>. GAIS-Net results on HQDS dataset. Left column shows stereo left images with histogram equalization to enhance contrast for better visualization. Middle and right column show Mask-RCNN and GAIS-Net results, respectively. Each instance has different colors.</p><p>With the aid of geometric information, GAIS-Net can segment out the person from the overlapping area in the first row example. In the second row scenario, Mask-RCNN generates distorted mask for the smoking motorcyclist because of cigarette plume and in contrast GAIS-Net displays a more robust shape control capability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Most previous works of outdoor instance segmentation for images only use color information. We explore a novel direction of sensor fusion to exploit stereo cameras. Geometric information from disparities helps separate overlapping objects of the same or different classes. Moreover, geometric information penalizes region proposals with unlikely 3D shapes thus suppressing false positive detections. Mask regression is based on 2D, 2.5D, and 3D ROI using the pseudo-lidar and image-based representations. These mask predictions are fused by a mask scoring process. However, public datasets only adopt stereo systems with shorter baseline and focal legnth, which limit measuring ranges of stereo cameras. We collect and utilize High-Quality Driving Stereo (HQDS) dataset, using much longer baseline and focal length with higher resolution. Our performance attains state of the art. Please refer to our project page for codes and data. The full paper is available here.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Instance segmentation, which segments out every object of interest, is an elemental task for computer vision. It is crucial for autonomous driving because it is vital to know positions for every object instance on roads. In the context of instance segmentation on images, previous approaches only operate on RGB imagery, such as Mask-RCNN <ref type="bibr" target="#b2">[3]</ref>.</p><p>However, image data could be affected by illumination, color change, shadows, or optical defects. These factors can degrade the performance of image-based instance segmentation. By utilizing another modality that provides geometric cues of scenes, and since object shapes are independent of object texture and color change, these strong priors add more robust information of the scenes. A prior work <ref type="bibr" target="#b6">[7]</ref> that goes beyond the dominant paradigm to incorporate depth information only uses it for naive ordering rather than directly regressing masks or building an end-to-end trainable model to propagate depth information. Besides, their depth maps are predicted from monocular images, making the depth ordering unreliable.</p><p>In outdoor scenes, stereo cameras or lidar sensors are commonly used for depth acquisition. Stereo cameras are low-cost and their adjustable parameters, such as longer baselines (b) and focal lengths (f ), favor stereo matching at far fields. Relationship of depth and disparity is given by</p><formula xml:id="formula_0">depth = f × b disparity .<label>(1)</label></formula><p>1-disparity (the minimal pixel difference showing the ideal longest range a stereo system could detect) represents farther distance if using longer f and b. Next, longer baselines and focal lengths favor more precise geometric estimations <ref type="bibr" target="#b4">[5]</ref>, since longer baselines produce smaller triangulation error, and longer focal lengths project objects on images with more pixels and thus enhance the robustness of stereo matching and show more complete shapes. In this paper, we propose Geometry-Aware Instance Segmentation Network (GAIS-Net) that takes the advantages of both the semantic information from image domain and geometric information from disparity maps. Our contributions are summarized as follows:</p><p>1. To our knowledge, we are the first to perform instance segmentation on imagery by fusing images and disparity information to regress object masks.</p><p>2. We collect High-Quality Driving Stereo (HQDS) dataset, with a total of 8.8K stereo pairs and with f × b 4 times larger than the current best dataset, Cityscapes.</p><p>3. We present GAIS-Net, an aggregation of representation design for instance segmentation using images, imagebased, and point cloud-based networks. We train GAIS-Net with different losses, and fuse these predictions using the mask scoring. GAIS-Net achieves the state of the art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Method</head><p>Our goal is to construct an end-to-end trainable network to perform instance segmentation for autonomous driving. Our system segments out each instance and outputs confidence scores for bounding boxes and masks for each instance. To exploit geometric information, we adopt PSM-Net <ref type="bibr" target="#b0">[1]</ref>, the state-of-the-art stereo matching network, and introduce disparity information at ROI heads. The whole network design is in <ref type="figure">Fig. 2</ref>.</p><p>We build a two-stage detector with a backbone network, such as ResNet50-FPN, and a region proposal network (RPN) with non-maximum suppression. Object proposals are collected by feeding a stereo left image into the backbone network and RPN. The same as Mask-RCNN, we perform bounding box regression, class prediction for proposals, and mask prediction based on image domain features. Corresponding losses are denoted as L box , L cls , and L 2Dmask , and are identified in <ref type="bibr" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Geometry-Aware Mask Prediction</head><p>2.5D ROI and 3D ROI. We use PSMNet <ref type="bibr" target="#b0">[1]</ref> and stereo pairs to predict dense disparity maps, projected onto the left stereo frame. Next, RPN outputs region proposals. We collect proposals and crop out these areas from the disparity map. We call these cropped out disparity areas as 2.5D ROI.</p><p>Based on the observations from pseudo-lidar work <ref type="bibr" target="#b5">[6]</ref>, which describes the advantage of back-projecting 2D grid structured data into 3D point cloud and processing with point cloud networks, we back-project the disparity map into R 3 space, where for each point, the first and second components describe its 2D grid coordinates, and the third component stores its disparity value. We name this representation as 3D ROI.</p><p>Instance Segmentation Networks. Each 3D ROI contains different number of points. To facilitate training, we uniformly sample the 3D ROI to 1024 points, and collect all the 3D ROI into a tensor. We develop a PointNet structured instance segmentation network to extract point features and perform per-point mask probability prediction. We re-project the 3D feature onto the 2D grid to calculate the mask prediction and its loss L 3Dmask . The re-projection is efficient because we do not break the point order in the point cloud-based instance segmentation. L 3Dmask , same as L 2Dmask , is a cross-entropy loss between a predicted probability mask and its matched groundtruth.</p><p>To fully utilize advantages of different representations, we further do 2.5D ROI instance segmentation with an image-based CNN. Similar to instance segmentation on 2D ROI, this network extracts local features of 2.5D ROI, and later performs per-pixel mask probability prediction. The mask prediction loss is denoted as L 2.5Dmask .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Mask Continuity</head><p>We sample 3D ROI to 1024 points uniformly. However, predicted masks, denoted as M 3D , and their outlines are sensitive to pseudo-lidar sampling strategies. An undesirable sampling is illustrated in <ref type="figure" target="#fig_0">Fig. 3</ref>. To compensate the undesirable effect, we introduce a mask continuity loss. Since objects are structured and continuous, we calculate a mask Laplacian as</p><formula xml:id="formula_1">∇ 2 M = ∂ 2 M ∂x 2 + ∂ 2 M ∂y 2</formula><p>, where x and y denote the dimensions of M . Mask Laplacian computes continuity of M . Further, the mask continuity loss is calculated as L cont = ∇ 2 M 2 for penalizing discontinuities of M .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Representation Correspondence</head><p>We use the point cloud-based network and the imagebased network to extract features and regress M 3D and M 2.5D . These two masks should be similar because they are from the same disparity map. To evaluate the similarity, cross-entropy is calculated between M 3D and M 2.5D , and serves as a self-supervised correspondence loss L corr . Minimizing this term lets the networks of different representations supervise each other to extract more descriptive features for mask regressing, resulting in similar probability distribution between M 2.5D and M 3D . Mask-RCNN uses a 14 × 14 feature grid after ROI pooling to regress masks. We also use this size at the mask heads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Mask Scores and Mask Fusion</head><p>MS-RCNN <ref type="bibr" target="#b3">[4]</ref> introduces mask scoring to directly regresses MaskIoU score based on a predicted mask and its associated matched groundtruth, showing quality of the mask prediction. However, their scores are not adopted at inference time to help manipulate mask shapes.</p><p>We adopt mask scoring and further exploit MaskIoU scores to fuse mask predictions from different represen- <ref type="figure">Figure 2</ref>. Network design of our GAIS-Net. Bbox is for bounding box. We color modules in blue and outputs or loss parts in orange. In the MaskIoU module, the 2D features and 2D predicted mask are from the 2D mask head. They are fed into MaskIoU head to regress MaskIoU scores. We draw the MaskIoU head separately for clear visualization. ⊕ stands for concatenation. tations at the inference time. The mask fusion process is illustrated in <ref type="figure">Fig. 4</ref>. During the inference time, we concatenate features and predicted masks of different representations respectively as inputs to the MaskIoU head. Scores of s 2D , s 2.5D , and s 3D are outputs from the Mask-IoU head. We fuse mask predictions using their corresponding mask scores. We first linearly combine (M 2.5D , s 2.5D ) and (M 3D , s 3D ) to obtain (M D , s D ) for the disparity. The formulation is as follows.</p><formula xml:id="formula_2">M D = M 2.5D × s 2.5D s 2.5D + s 3D +M 3D × s 3D s 2.5D + s 3D ,<label>(2)</label></formula><formula xml:id="formula_3">s D = s 2.5D × s 2.5D s 2.5D + s 3D + s 3D × s 3D s 2.5D + s 3D .<label>(3)</label></formula><p>Later, we linearly fuse (M 2D , s 2D ) and (M D , s D ) likewise to obtain the final probability mask M f and its corresponding final mask score. The inferred mask is created by binarizing M f . The mask scoring process should not be different for each representation. We only use 2D image features and M 2D to train a single MaskIoU head instead of constructing 3 MaskIoU heads for each representation. In this way, the MaskIoU module would not add much more memory use and the training is also effective. The MaskIoU loss is denoted as L miou . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">HQDS Dataset</head><p>Outdoor RGBD scene understanding is still less explored since much longer range sensing is required to align information from images and depth. Such as vehicles at distances showing in images but undetected by a depth sensor would bring ambiguity into RGBD methods. To conduct exploration of outdoor RGBD methods, and provide high quality data to reveal advantages of sensor fusion, we collect High-Quality Driving Stereo (HQDS) dataset in urban environments. <ref type="table" target="#tab_2">Table 3</ref>.1 shows a comparison with other public datasets for instance segmentation. Image resolution of HQDS is 1024 × 3072. From the table and Eq. 1. HQDS has the largest f × b. Measuring range by the configuration is up to 1650 meters with 1-pixel disparity, which is only 440 and 350 for Cityscapes and KITTI. Note that produced disparity maps are computed by stereo matching methods so actual working distances are associated with methods' robustness and image noise. However, longer baselines and focal lengths still favor far-field stereo matching since the former could show better geometry and more complete shapes for objects at distances <ref type="bibr" target="#b4">[5]</ref>.</p><p>HQDS contains 6K/1.2K stereo pairs for training/testing. We follow a half-automation process to annotate data with <ref type="bibr">Figure 4</ref>. Inference time mask fusion of predictions from different representations. We fuse the 2.5D mask and 3D mask first because they are from the same source. We then fuse the mask predictions from the image domain and disparity. ⊕ represents concatenation. a group of supervised annotators. Our internal large-scale labeling system produces preliminaries, and the annotators adjust yielded bounding boxes and mask shapes or filter out false predictions to produce HQDS groundtruth.</p><p>There are 60K instances in the training set and 11K in the testing set. We adopt 3 instance classes: human, bicycle/motorcycle, and vehicle. Although other datasets on driving adopt more, such as Cityscapes use 8 classes, from MaskRCNN's study <ref type="bibr" target="#b2">[3]</ref> they suffer from much inter-class ambiguity which leads to biased results.</p><p>Associated number of instances in the training and testing sets are (5.5K, 1.5K, 52.8K) and (2.4K, 1K, 8.4K) respectively. Most of non-synthetic datasets encounter classimbalanced issue. To remedy the imbalance, we adopt COCO dataset (instance segmentation for common objects) pretrained weights with class pruning in our implementations and comparison methods.</p><p>Evaluation and Metrics. We fairly compare with recent state-of-the-art methods validated on large-scale COCO dataset, including Mask-RCNN <ref type="bibr" target="#b2">[3]</ref>, MS-RCNN <ref type="bibr" target="#b3">[4]</ref>, Cascade Mask RCNN <ref type="bibr" target="#b1">[2]</ref>, and HTC <ref type="bibr" target="#b1">[2]</ref> (w/o semantics), by using their publicly released codes and their COCO pretrained weights. We follow their training procedures to conduct comparison experiments.</p><p>We report numerical results in the standard COCOstyle. Average precision (AP) averages across different IoU levels, from 0.5 to 0.95 with 0.05 as an interval. AP 50 and AP 75 are 2 typical IoU levels. The units are %. <ref type="table" target="#tab_1">Table 2</ref> shows the comparison with others. The proposed GAIS-Net attains the state of the art. We exceed Mask-RCNN using the same backbone by 9.7% and 6.8% for bounding box and mask AP, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Cityscapes Dataset</head><p>We also conduct experiments on Cityscapes dataset. However, its baseline and focal length are shorter than HQDS, and the maximal measuring distance is only 1/4 of HQDS. Much shorter focal length and baseline limit the working distance of stereo matching and produce disparity maps only focusing at near fields with poor shapes and geometry <ref type="bibr" target="#b4">[5]</ref>. From <ref type="table" target="#tab_2">Table 3</ref>, performance of GAIS-Net is still better than Mask-RCNN. The improvement gap between HQDS and Cityscapes is mainly caused by the latter's shorter baseline and focal length.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Undesirable sampling example. The blue areas represent foreground. Suppose we uniformly sample every grid center point in the left figure, resulting in the point cloud showing in the occupancy grid on the right. Red crosses are undesirable sampling points, which just lie outside the foreground object, making the shape after sampling different from the original one.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison between collected HQDS and other public datasets for instance segmentation with stereo data. Stereo pairs # means number of training stereo pairs. Stereo camera baseline (b) is in meters. fx is for horizontal focal length.</figDesc><table><row><cell>Dataset</cell><cell>Resolution</cell><cell>Stereo</cell><cell>b</cell><cell>f x</cell></row><row><cell></cell><cell>(megapixels)</cell><cell>Pairs #</cell><cell>(m)</cell><cell>(pixels)</cell></row><row><cell>Cityscapes</cell><cell>2.09</cell><cell>2.7K</cell><cell>0.2</cell><cell>2.2K</cell></row><row><cell>KITTI</cell><cell>0.71</cell><cell>0.2K</cell><cell>0.5</cell><cell>0.7K</cell></row><row><cell>HQDS</cell><cell>3.15</cell><cell>6K</cell><cell>0.5</cell><cell>3.3K</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Quantitative comparison on HQDS testing set. The first table is for bounding box evaluation. The second table is for mask evaluation. Bbox Evaluation AP AP 50 AP 75 AP S AP L Mask Evaluation AP AP 50 AP 75 AP S AP L</figDesc><table><row><cell>Mask-RCNN</cell><cell>36.3 57.4 38.8 19.1 51.9</cell></row><row><cell>MS-RCNN</cell><cell>42.2 65.1 46.6 20.8 59.6</cell></row><row><cell cols="2">Cas. Mask-RCNN 37.4 55.8 38.9 18.0 54.7</cell></row><row><cell>HTC</cell><cell>39.4 58.3 43.1 18.5 57.9</cell></row><row><cell>GAIS-Net</cell><cell>46.0 67.7 53.3 23.6 66.2</cell></row><row><cell>Mask-RCNN</cell><cell>33.9 53.2 35.5 14.4 49.7</cell></row><row><cell>MS-RCNN</cell><cell>39.2 61.3 40.4 18.8 56.4</cell></row><row><cell cols="2">Cas. Mask-RCNN 33.4 54.4 34.8 11.7 49.5</cell></row><row><cell>HTC</cell><cell>34.5 56.9 36.7 11.6 52.0</cell></row><row><cell>GAIS-Net</cell><cell>40.7 65.9 43.5 18.3 59.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Instance segmentation results on Cityscapes datset.</figDesc><table><row><cell>Evaluation</cell><cell>Training data</cell><cell>Mask AP</cell></row><row><cell>Mask-RCNN [3]</cell><cell>fine only</cell><cell>31.5</cell></row><row><cell>Our GAIS-Net</cell><cell>fine only</cell><cell>32.5</cell></row><row><cell>Mask-RCNN [3]</cell><cell>fine + COCO</cell><cell>36.4</cell></row><row><cell>Our GAIS-Net</cell><cell>fine + COCO</cell><cell>37.1</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pyramid stereo matching network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Ren</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Sheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Hybrid task cascade for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Piotr Dollár, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mask scoring r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaojin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongchao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A multiple-baseline stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masatoshi</forename><surname>Okutomi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Pseudo-lidar from visual depth estimation: Bridging the gap in 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Divyansh</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Depth-aware object instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linwei</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
