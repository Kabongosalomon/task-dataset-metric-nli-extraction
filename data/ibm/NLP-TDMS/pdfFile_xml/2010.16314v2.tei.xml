<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Critical Assessment of State-of-the-Art in Entity Alignment</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-03-17">17 Mar 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Berrendorf</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Ludwig-Maximilians-Universität München</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">]</forename></persName>
							<affiliation key="aff0">
								<orgName type="institution">Ludwig-Maximilians-Universität München</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Wacker</surname></persName>
							<email>l.wacker@campus.lmu.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Ludwig-Maximilians-Universität München</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Faerman</surname></persName>
							<email>faerman@dbs.ifi.lmu.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Ludwig-Maximilians-Universität München</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Critical Assessment of State-of-the-Art in Entity Alignment</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-03-17">17 Mar 2021</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Knowledge Graph · Entity Alignment · Word Embeddings</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work, we perform an extensive investigation of two state-of-the-art (SotA) methods for the task of Entity Alignment in Knowledge Graphs. Therefore, we first carefully examine the benchmarking process and identify several shortcomings, making the results reported in the original works not always comparable. Furthermore, we suspect that it is a common practice in the community to make the hyperparameter optimization directly on a test set, reducing the informative value of reported performance. Thus, we select a representative sample of benchmarking datasets and describe their properties. We also examine different initializations for entity representations since they are a decisive factor for model performance. Furthermore, we use a shared train/validation/test split for an appropriate evaluation setting to evaluate all methods on all datasets. In our evaluation, we make several interesting findings. While we observe that most of the time SotA approaches perform better than baselines, they have difficulties when the dataset contains noise, which is the case in most real-life applications. Moreover, in our ablation study, we find out that often different features of SotA method are crucial for good performance than previously assumed. The code is available at https://github.com/mberr/ea-sota-comparison.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The quality of information retrieval crucially depends on the accessible storage of information. Knowledge Graphs (KGs) often serve as such data structure <ref type="bibr" target="#b5">[6]</ref>. Moreover, to satisfy diverse information needs, a combination of multiple data sources is often inevitable. Entity Alignment (EA) <ref type="bibr" target="#b1">[2]</ref> is the discipline of aligning entities from different KGs. Once aligned, these entities facilitate information transfer between knowledge bases, or even fusing multiple KGs to a single knowledge base.</p><p>In this work, our goal is to analyze a SotA approach for the task of EA and identify which factors are essential for its performance. Although papers often use the same dataset in the evaluation and report the same evaluation metrics, the selection of SotA is not a trivial task: as we found out in our analysis, the usage of different types of external information for the initialization or train/test splits of different sizes 1 makes the results in different works incomparable. Therefore, while still guided by the reported evaluation metrics, we identified these common factors among strongly performing methods in multiple works:</p><p>-They are based on Graph Neural Networks (GNNs). GNNs build the basis of the most recent works <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref>. -They utilize entity names in the model. Supported by recent advances in word embeddings, these attributes provide distinctive features. -They consider different types of relations existing in KGs. Most GNNs ignore different relationship types and aggregate them in the preprocessing step.</p><p>Given these criteria, we selected Relation-aware Dual-Graph Convolutional Network (RDGCN) <ref type="bibr" target="#b16">[17]</ref>, as it also has demonstrated impressive performance in recent benchmarking studies <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b23">24]</ref>. Additionally, we include the recently published Deep Graph Matching Consensus (DGMC) <ref type="bibr" target="#b6">[7]</ref> method in our analysis for two reasons: the studies mentioned above did not include it, and the authors reported surprisingly good performance, considering that this method does not make use of relation type information. We start our study by reviewing the used datasets and discussing the initializations based on entity names. Although both methods utilize entity names, the actual usage differs. For comparison, we thus evaluate both methods on all datasets with all available initializations. We also report the zero-shot performance, i.e., when only using initial representations alone, as well as a simple GNN model baseline. Furthermore, we address the problem of hyperparameter optimization. Related works often do not discuss how they chose hyperparameters and, e.g., rarely report validation splits. So far, this problem was not addressed in the community. In the recent comprehensive survey <ref type="bibr" target="#b14">[15]</ref>, the authors use cross-validation for the estimation of the test performance. The models are either evaluated with hyperparameters recommended for other datasets or selected by not reported procedure. Also, in the published code of the investigated approaches, we could not find any trace of train-validation splits, raising questions about reproducibility and fairness of their comparisons. We thus create a shared split with a test, train, and validation part and extensively tune the model's hyperparameters for each of the dataset/initialization combinations to ensure that they are sufficiently optimized. Finally, we provide an ablation study for many of the parameters of a SotA approach (RDGCN), giving insight into the individual components' contributions to the final performance. <ref type="table" target="#tab_0">Table 1</ref> provides a summary of a representative sample of datasets used for benchmarking of EA approaches. In the following, we first discuss each dataset's properties and, in the second part, the initialization of entity name attributes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Datasets &amp; Initialization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Datasets</head><p>DBP15k The DBP15k dataset is the most popular dataset for the evaluation of EA approaches. It has three subsets, all of which base upon DBpedia. Each subset comprises a pair of graphs from different languages. As noted by <ref type="bibr" target="#b1">[2]</ref>, there exist multiple variations of the dataset, sharing the same entity alignment but differing in the number of exclusive entities in each graph. The alignments in the datasets are always 1:1 alignments, and due to the construction method for the datasets, exclusive entities do not have relations between them, but only to shared entities. Exclusive entities complicate the matching process, and in real-life applications, they are not easy to identify. Therefore, we believe that this dataset describes a realistic use-case only to a certain extent. We found another different variant of DBP15k as part of the PyTorch Geometric repository 2 , having a different set of aligned entities. This is likely due to extraction of alignments from data provided by <ref type="bibr" target="#b19">[20]</ref> via Google Drive 3 as described in their GitHub repository. <ref type="bibr" target="#b3">4</ref> As a result, the evaluation results published in <ref type="bibr" target="#b6">[7]</ref> are not directly comparable to other published results. In our experiments, we use the (smaller) JAPE variant with approximately 19-20k entities in each graph since it is the predominantly used variant.</p><p>OpenEA The OpenEA datasets published by <ref type="bibr" target="#b14">[15]</ref> comprise graph pairs from DBPedia, YAGO, and Wikidata obtained by iterative degree-based sampling to match the degree distribution between the source KG and the extracted subset. The alignments are exclusively 1:1 matchings, and there are no exclusive entities, i.e., every entity occurs in both graphs. We believe that this is a relatively unrealistic scenario. In our experiments, we use all graph pairs with 15k entities (15K) in the dense variant (V2), i.e., en-de-15k-v2, en-fr-15k-v2, d-y-15k-v2, d-w-15k-v2.</p><p>WK3l15k The Wk3l datasets are multi-lingual KG pairs extracted from Wikipedia. As in <ref type="bibr" target="#b1">[2]</ref>, we extract additional entity alignments from the triple alignments. The graphs contain additional exclusive entities, and there are m:n matchings. We only use the 15k variants, where each graph has approximately 15k entities. There are two graph pairs, en-de and en-fr. Moreover, the alignments in the dataset are relatively noisy: for example, en-de contains besides valid alignments such as ("trieste", "triest"), or ("frederick i, holy roman emperor", "friedrich i. (hrr)"), also ambiguous ones such as ("1", "1. fc saarbrücken"), ("1", "1. fc schweinfurt 05"), and errors such as ("1", "157"), and ("101", "100"). While the noise aggravates alignment, it also reflects a realistic setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Label-Based Initializations</head><p>Prepared translations (DBP15k) For DBP15k, we investigate label-based initializations based on prepared translations to English from <ref type="bibr" target="#b16">[17]</ref> and <ref type="bibr" target="#b6">[7]</ref> (which, in turn, originate from <ref type="bibr" target="#b19">[20]</ref>). Afterwards, they use Glove <ref type="bibr" target="#b10">[11]</ref> embeddings to obtain an entity representation. While <ref type="bibr" target="#b16">[17]</ref> only provides the final entity representation vectors without further describing the aggregation, <ref type="bibr" target="#b6">[7]</ref> splits the label into words (by white-space) and uses the sum over the words' embeddings as entity representation. <ref type="bibr" target="#b16">[17]</ref> additionally normalizes the norm of the representations to unit length.</p><p>Prepared RDGCN Embeddings (OpenEA) OpenEA <ref type="bibr" target="#b14">[15]</ref> benchmarks a large variety of contemporary entity alignment methods in a unified setting, also including RDGCN <ref type="bibr" target="#b16">[17]</ref>. Since the graphs DBPedia and YAGO collect data from similar sources, the labels are usually equal. For those graph pairs, the authors propose to delete the labels. However, RDGCN requires a label based initialization.</p><p>Thus, the authors obtain labels via attribute triples of a pre-defined set of "nameattributes" 5 : skos:prefLabel, http://dbpedia.org/ontology/birthName for DBPedia-YAGO, and http://www.wikidata.org/entity/P373, http://www.wikidata.org/entity/P1476 for DBPedia-Wikidata. However, when investigating the published code, we noticed that if the label is not found via attribute, the last part of the entity URI is used instead. For DBPedia/YAGO, this effectively leaks ground truth since they share the same label. For DBPedia/Wikidata, this results in useless labels for the Wikidata side since their labels are the Wikidata IDs, e.g., Q3391163. <ref type="table" target="#tab_1">Table 2</ref> summarizes the frequency of both cases. For d-w, DPBedia entities always use the ground truth label. For 49% of the Wikidata entities, useless labels are used for initialization. For d-y, YAGO entity representations are always initialized via an attribute triple. For DBPedia, in 81% of all cases, the ground truth label is used. We store these initial entity representations produced by the OpenEA codebase into a file and refer in the following to them as Sun initialization (since they are taken from the implementation of <ref type="bibr" target="#b14">[15]</ref>).</p><p>Multi-lingual BERT (WK3l15k) Since we did not find related work with entity embedding initialization from labels on WK3l15k, we generated those using a pre-trained multi-lingual BERT model <ref type="bibr" target="#b4">[5]</ref>, BERT-Base, Multilingual Cased 6 . Following <ref type="bibr" target="#b4">[5]</ref>, we use the sum of the last four layers as token representation since it has comparable performance to the concatenation at a quarter of its size. To summarize the token representations of a single entity label, we explore sum, mean, and max aggregation as hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>We evaluate two SotA EA methods, RDGCN <ref type="bibr" target="#b16">[17]</ref> which we reimplemented and DGMC <ref type="bibr" target="#b6">[7]</ref> for which we used the original method implementation with adapted evaluation. In the following, we revisit their architectures and highlight differences between the architecture described in the paper and what we found in the published code.</p><p>Similarly to all GNN-based approaches, both models employ a Siamese architecture. Therefore, the same model with the same weights is applied to both graphs yielding representations of entities from both KGs. Given these entity representations, the EA approaches compute an affinity matrix that describes the similarity of entity representations from both graphs. Since the main difference between methods is the GNN model in the Siamese architecture, for brevity we only describe how it is applied on a single KG G = (E, R, T ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Relation-aware Dual-Graph Convolutional Network (RDGCN)</head><p>Architecture The RDGCN <ref type="bibr" target="#b16">[17]</ref> model comprises two parts performing messagepassing processes applied sequentially. The message passing process performed by the first part can be seen as relation-aware. The model tries to learn the importance of relations and weights the messages from the entities connected by these relations correspondingly. The message passing performed by the second component utilizes a simple adjacency matrix indicating the existence of any relations between entities, which we call standard message passing. Both components employ a form of skip connections: (weighted) residual connections <ref type="bibr" target="#b7">[8]</ref> in the first part and highway layers <ref type="bibr" target="#b12">[13]</ref> in the second part.</p><p>Relation-Aware Message Passing The entity embeddings from the first component are computed by several interaction rounds comprising four steps</p><formula xml:id="formula_0">X c = RC(X e ), X c ∈ R |R|×2d</formula><p>(1)</p><formula xml:id="formula_1">X r = DA(X r , X c ), X r ∈ R |R|×2d (2) X e = P A(X e , X r ) (3) X e = X 0 e + β i · X e<label>(4)</label></formula><p>The first step, in <ref type="formula">(1)</ref>, obtains a relation context (RC) X c from the entity representations. For relation r ∈ R, we extract its relation context as a concatenation of the mean entity representations for the head and the tail entities. By denoting the set of head and tail entities for relation r with H r and T r , we can thus express its computation as (X c ) i = 1 /|Hi| j∈Hi (X e ) j 1 /|Ti| j∈Ti (X e ) j where denotes the concatenation operation. An entity occurring multiple times as the head is weighted equally to an entity occurring only once.</p><p>The second step, in <ref type="formula">(2)</ref>, is the dual graph attention (DA). The attention scores on the dual graph α D ij are computed by dot product attention with leaky ReLU activation:</p><formula xml:id="formula_2">α D ij = J ij · LeakyReLU (W L (X c ) i + W R (X c ) j ). Notice that W L (X c ) i + W R (X c ) j = (W L W R ) T ((X c ) i (X c ) j ),</formula><p>where denotes the concatenation operation. In the published code, we further found a weight sharing mechanism for W L and W R implemented, decomposing the projection weight matrices as</p><formula xml:id="formula_3">W L = W ′ L W C and W R = W ′ R W C with W ′ L , W ′ R ∈ R 1×h , W C ∈ R h×2d</formula><p>being trainable parameters, and W C shared between both projections. J ij denotes a fixed triple-based relation similarity score computed as the sum of the Jaccard similarities of the head and tail entity set for relation r i and r j : J ij := |Hi∩Hj | /|Hi∪Hj| + |Ti∩Tj | /|Ti∪Tj|. The softmax is then computed only over those relations, where J ij &gt; 0, i.e., pairs sharing at least one head or tail entity. In the implementation, this is implemented as dense attention with masking, i.e. setting α D ij = −∞ (or a very small value) for J ij = 0. While this increases the required memory consumption to O(|R| 2 ), the number of relations is usually small compared to the number of entities, cf. <ref type="table" target="#tab_0">Table 1</ref>, and thus this poses no serious computational problem. Withα D ij denoting the softmax output, the new relation representation finally is (X r ) i = ReLU jα D ij (X r ) j . In the third step, in (3), the entity representations are updated. To this end, a relation-specific scalar score is computed as α r i = LeakyReLU (WX r + b) with trainable parameters W and b. Based upon the relation-specific scores, an attention score between two entities e i , e j with at least one relation between them is given as α P ij = r∈Tij α r i . These scores are normalized with a sparse softmax over all {j | ∃r ∈ R : (e i , r, e j ) ∈ T }:</p><formula xml:id="formula_4">α P ij = softmax j ′ (α P ij ′ ) j . The final output of the primal attention is (X e ) j = ReLU ( iα ij (X e ) j ).</formula><p>The fourth step, in (4), applies a skip connection from the initial representations to the current entity representation. The weight β i is pre-defined (β 1 = 0.1, β 2 = 0.3) and not trained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Standard Message Passing</head><p>The second part of the RDGCN consists of a sequence of GCN layers with highway layers. Each layer computes</p><formula xml:id="formula_5">X ′ e = ReLU (AX e W) (5) β = σ(W g X e + b g ) (6) X e = β · X ′ e + (1 − β) · X e<label>(7)</label></formula><p>A ∈ R |E L |×|E L | denotes the adjacency matrix of the primal graph. It is constructed by first creating an undirected, unweighted adjacency matrix where there is a connection between e i , e j ∈ E L if there exists at least one triple (e i , r, e j ) ∈ T L for some relation r ∈ R L . Next, self-loops (e, e) are added for every entity e ∈ E L . Finally, the matrix is normalized by setting A = D −1/2 AD −1/2 with D denoting the diagonal matrix of node degrees. When investigating the published code, we further found out that the weight matrix W is constrained to be a diagonal matrix and initialized as an identity matrix.</p><p>Training Let x L i denote the final entity representation for e L i ∈ E L and anologously x R j for e R j ∈ E R . RDGCN is trained with a margin-based loss formulation. It adopts a hard negative mining strategy, i.e., the set of negative examples for one pair is the top k most similar entities of one of the entities according to the similarity measure used for scoring. The negative l 1 distance is used as similarity, the margin is 1, k = 10, and the negative examples are updated every 10 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Deep Graph Matching Consensus (DGMC)</head><p>DGMC <ref type="bibr" target="#b6">[7]</ref> also comprises two parts, which we name enrichment and correspondence refinement. The enrichment part is a sequence of GNN layers enriching the entity representations with information from their neighborhood. Each layer computes φ(X) = ReLU (norm(A)XW 1 +norm(A T )XW 2 +XW 3 ), where A ∈ R |E L |×|E L | denotes the symmetrically normalized adjacency matrix (as for second part of RDGCN), norm the row-wise normalization operation, X ∈ R E L ×din the layer's input, and W 1 , W 2 , W 3 ∈ R din×dout trainable parameters of the layer. An optional batch normalization and dropout follow this layer. For the enrichment phase's final output, all individual layers' outputs are concatenated before a learned final linear projection layer reduces the dimension to d out .</p><p>The second phase, the correspondence refinement, first calculates the k = 10 most likely matches in the other graph for each entity as a sparse correspondence matrix S ∈ R |E L |×|E R | , normalized using softmax. Next, it generates random vectors for each entity R ∈ R |E L |×d rnd and sends these vectors to the probable matches via the softmax normalized sparse correspondence matrix, S T R ∈ R |E R |×d rnd . A GNN layer ψ as in phase one distributes these vectors in the neighborhood of the nodes: Y R = ψ(S T R). A two-layer MLP predicts an update for the correspondence matrix, given the difference between the representations Y L and Y R . This procedure is repeated for a fixed number of refinement steps L = 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Experimental Setup For the general evaluation setting and description of metrics, we refer to <ref type="bibr" target="#b2">[3]</ref>. Here, we primarily use Hits@1 (H@1), which measures the correct entity's relative frequency of being ranked in the first position. When investigating the published code of both, RDGCN [17] 7 and DGMC [7] 8 , we did not find any code for tuning the parameters, nor a train-validation split. Also, the papers themselves do not mention a train-validation split. Thus, it is unclear how they choose the hyperparameters without a test-leakage by directly optimizing the test set's performance. We thus decided to create a shared test-train-validation split used by all our experiments to enable a fair comparison. Since DGMC already uses PyTorch, we could use their published code and extend it with HPO code. RDGCN was re-implemented in PyTorch in our codebase. We use the official train-test split for all datasets, which reserves 70% of the alignments for testing. We split the remaining part into 80% train alignments and 20% validation alignments.</p><p>We continued by tuning numerous model parameters (cf. <ref type="table">Table 3</ref>) of all models on each of the datasets in <ref type="table" target="#tab_0">Table 1</ref> and each of the available initializations described in Section 2.2 to obtain sufficiently well-tuned configurations. We used random search due to its higher sample efficiency than grid search <ref type="bibr" target="#b0">[1]</ref>. We additionally evaluate a baseline, which uses the GNN variant from DGMC without the neighborhood consensus refinement, coined GCN-Align* due to its close correspondence to <ref type="bibr" target="#b15">[16]</ref>, and also evaluate the zero-shot performance of the initial node features.</p><p>For each tested configuration, we perform early stopping on validation H@1, i.e., select the epoch according to the best validation H@1. Across all tested configurations for a model-dataset-initialization combination, we then choose the best configuration according to validation H@1 and report the test performance in <ref type="table" target="#tab_3">Table 4</ref>. We do not report performance for training on train+validation with the final configuration due to space restrictions. We decided to report performance when trained only on the train set to ensure that other works have performance numbers for comparison when tuning their own models. <ref type="table">Table 3</ref>. Investigated hyperparameters for all methods. * denotes that these parameters share the same value range but were tuned independently.  <ref type="table" target="#tab_3">Table 4</ref> presents the overall results. We can observe several points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Results</head><p>Zero-Shot Performance Generally, there is an impressive Zero-Shot performance, ranging from 39.15% for OpenEA d-w to 83.85% WK3l15k en-de. Thus, even in the weakest setting, approximately 40% of the entities can be aligned solely from their label, without any sophisticated method. Consequently, this highlights that comparison against methods not using this information is unfair. For DBP15k, we can compare the initialization from Wu et al. <ref type="bibr" target="#b16">[17]</ref>, used, e.g., by RDGCN to the performance of the initialization by Xu et al. <ref type="bibr" target="#b6">[7]</ref>, used, e.g., by DGMC. We observe that Wu's initialization is 7-9% points stronger than Xu's initialization. For OpenEA d-w we obtain 39.15% zero-shot performance, despite the original labels of the w side being meaningless identifiers. This is only due to using attribute triples with a pre-defined set of "name" attributes, cf. <ref type="table" target="#tab_1">Table 2</ref>.</p><p>Model Performance When comparing the performance of both analyzed models, we can observe that they have a clear advantage over both baselines in two of three datasets. However, we cannot identify a single winner among them. Although the performance of DGMC dropped compared to the results reported originally 9 , it still leads by about 3-4 points on almost all DBP15k subsets. Therefore, it confirms our observation that a smaller test set automatically leads to better results. Furthermore, we can see that different initialization with entity name also affects model performance, which especially applies to the ja-en subset for DGMC or fr-en for GCN-Align*. RDGCN has a clear advantage on the OpenEA subsets extracted from DBPedia with a margin of between 10 and 13 points on both subsets. Note that we significantly improved results of RDGCN on the OpenEA dataset through our extensive hyperparameter search compared to the original evaluation <ref type="bibr" target="#b14">[15]</ref>. Interestingly, as can be seen in the next section, the main reason is not the exploiting of information about different relations. The WK3L15k dataset constitutes an interesting exception. The performance of the DGMC method, which is supposed to be robust against noise due to its correspondence refinement, is not better than the zero-shot results. While RDGCN <ref type="table">Table 5</ref>. Ablation results for RDGCN on OpenEA datasets. The setting used by <ref type="bibr" target="#b16">[17]</ref> is underlined. The first number is validation H@1, the second number test H@1. Bold highlights the best configuration. Please notice that due to the specialties of EA evaluation, the test and validation performance are not directly comparable <ref type="bibr" target="#b2">[3]</ref>. and GCN-Align* can improve the results, the improvement by 1-2 points does not look very convincing. From these results, we conclude that there exists no silver bullet for the task of EA, and the method itself is still a hyperparameter. At the same time, we see that the most realistic dataset poses a real challenge for SotA methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation: RDGCN</head><p>We additionally present the results of an ablation study for some model parameters of RDGCN on the OpenEA datasets in <ref type="table">Table 5</ref>. For each presented parameter and each possible value, we fix this one parameter and select the best configuration among all configurations with the chosen parameter setting according to validation H@1. The cell then shows the validation and test performance of this configuration. We highlight the best setting on the respective graph pair in bold font. Note that the test performance numbers also coincide with the performance reported in <ref type="table" target="#tab_3">Table 4</ref> for OpenEA. We make the following interesting observations: for all but one graph pair, always normalizing the entity representations before passing them into the layers is beneficial. For d-y, where this is not the case, the difference in performance is small. For the number of GCN layers, we observe an increase in performance from 0 to 2 layers, and on some datasets (d-w, d-y) even beyond. Thus, aggregating the entities' neighbor-hood seems beneficial, highlighting the importance of the graph structure. For the number of interaction layers, which perform relation-aware message passing, we observe that for two of the four subsets (d-y, en-de) the best configuration does not use any interaction layer. However, the difference is small. None of the best configurations uses trainable node embeddings. The negative l 1 similarity is superior on all datasets, with most of the others being close to it. Using the dot product seems to be sub-optimal, maybe due to its unbound value range. Regarding hard negative mining, there is no clear tendency, but considering the hard negatives' expensive calculation (all-to-all kNN), its use might not be worthwhile. Another observation is that sometimes there is a huge gap between the test performance for the best configuration according to validation performance and the best configuration according to test performance. For instance, if we had selected the hyperparameters according to test performance for en-de, we had obtained 93.53 H@1, while choosing them according to validation performance results in only 80.03 H@1 -a difference of 13.5% points. This difference emphasizes the need for a fair hyperparameter selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we investigated state-of-the-art in Entity Alignment. Since we identified shortcomings in the commonly employed evaluation procedure, including the lack of validation sets for hyperparameter tuning and different initializations, we provided a fair and sound evaluation over a wide range of configurations. We additionally gave insight into the importance of individual components. Our results provide a strong, fair, and reproducible baseline for future works to compare against and offer deep insights into the inner workings of a GNN-based model. We plan to investigate the identified weakness against noisy labelings in future work and increase the robustness. Moreover, we aim to improve the usage of relation type information in the message passing phase of models like RDGCN, which only use them in an initial entity representation refinement stage. For some datasets such as OpenEA d-y and en-de, optimal configurations did not consider the relational information. However, intuitively, this information should help to improve the structural description of entities. Potential improvements include establishing a relation matching between the two graphs or modifying the mechanism used to integrate relational information.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Summary of the used EA datasets. We denote the entity set as E , the relation set as R, the triple set as T , the aligned entities as A and the exclusive entities as X .</figDesc><table><row><cell>dataset</cell><cell>subset</cell><cell>graph</cell><cell>|E|</cell><cell>|R|</cell><cell>|T |</cell><cell>|A|</cell><cell>|X |</cell></row><row><cell>DBP15k</cell><cell>zh-en</cell><cell>zh</cell><cell>19,388</cell><cell>1,701</cell><cell>70,414</cell><cell>15,000</cell><cell>4,388</cell></row><row><cell></cell><cell></cell><cell>en</cell><cell>19,572</cell><cell>1,323</cell><cell>95,142</cell><cell>15,000</cell><cell>4,572</cell></row><row><cell></cell><cell>ja-en</cell><cell>ja</cell><cell>19,814</cell><cell>1,299</cell><cell>77,214</cell><cell>15,000</cell><cell>4,814</cell></row><row><cell></cell><cell></cell><cell>en</cell><cell>19,780</cell><cell>1,153</cell><cell>93,484</cell><cell>15,000</cell><cell>4,780</cell></row><row><cell></cell><cell>fr-en</cell><cell>fr</cell><cell>19,661</cell><cell>903</cell><cell>105,998</cell><cell>15,000</cell><cell>4,661</cell></row><row><cell></cell><cell></cell><cell>en</cell><cell>19,993</cell><cell>1,208</cell><cell>115,722</cell><cell>15,000</cell><cell>4,993</cell></row><row><cell>WK3l15k</cell><cell>en-de</cell><cell>en</cell><cell>15,126</cell><cell>1,841</cell><cell>209,041</cell><cell>9,783</cell><cell>5,343</cell></row><row><cell></cell><cell></cell><cell>de</cell><cell>14,603</cell><cell>596</cell><cell>144,244</cell><cell>10,021</cell><cell>4,582</cell></row><row><cell></cell><cell>en-fr</cell><cell>en</cell><cell>15,169</cell><cell>2,228</cell><cell>203,356</cell><cell>7,375</cell><cell>7,794</cell></row><row><cell></cell><cell></cell><cell>fr</cell><cell>15,393</cell><cell>2,422</cell><cell>169,329</cell><cell>7,284</cell><cell>8,109</cell></row><row><cell>OpenEA</cell><cell>en-de</cell><cell>en</cell><cell>15,000</cell><cell>169</cell><cell>84,867</cell><cell>15,000</cell><cell>0</cell></row><row><cell></cell><cell></cell><cell>de</cell><cell>15,000</cell><cell>96</cell><cell>92,632</cell><cell>15,000</cell><cell>0</cell></row><row><cell></cell><cell>en-fr</cell><cell>en</cell><cell>15,000</cell><cell>193</cell><cell>96,318</cell><cell>15,000</cell><cell>0</cell></row><row><cell></cell><cell></cell><cell>fr</cell><cell>15,000</cell><cell>166</cell><cell>80,112</cell><cell>15,000</cell><cell>0</cell></row><row><cell></cell><cell>d-y</cell><cell>d</cell><cell>15,000</cell><cell>72</cell><cell>68,063</cell><cell>15,000</cell><cell>0</cell></row><row><cell></cell><cell></cell><cell>y</cell><cell>15,000</cell><cell>21</cell><cell>60,970</cell><cell>15,000</cell><cell>0</cell></row><row><cell></cell><cell>d-w</cell><cell>d</cell><cell>15,000</cell><cell>167</cell><cell>73,983</cell><cell>15,000</cell><cell>0</cell></row><row><cell></cell><cell></cell><cell>w</cell><cell>15,000</cell><cell>121</cell><cell>83,365</cell><cell>15,000</cell><cell>0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>The statistics about label-based initialization in the OpenEA codebase: attribute denotes initialization via attribute values for a predefined set of "name attributes". id denotes initialization with the last part of the entity URI. For d-y this basically leaks ground truth, whereas, for Wikidata, the URI contains only a numeric identifier, thus rendering the initialization "label" useless.</figDesc><table><row><cell>subset</cell><cell>side</cell><cell>via attribute</cell><cell>via id</cell><cell>via id (%)</cell></row><row><cell>d-w</cell><cell>d w</cell><cell>0 8,391</cell><cell>15,000 7,301</cell><cell>100.00% 48.67%</cell></row><row><cell>d-y</cell><cell>d y</cell><cell>2,883 15,000</cell><cell>12,122 0</cell><cell>80.81% 0.00%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Results in terms of H@1 for all investigated combinations of datasets, models, and initializations. Each cell represents the test performance of the best configuration of hyperparameters chosen according to validation performance.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">DBP15k (JAPE)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>init</cell><cell></cell><cell>Wu [18]</cell><cell></cell><cell></cell><cell>Xu [20]</cell><cell></cell></row><row><cell>subset</cell><cell>fr-en</cell><cell>ja-en</cell><cell>zh-en</cell><cell>fr-en</cell><cell>ja-en</cell><cell>zh-en</cell></row><row><cell>Zero Shot</cell><cell>79.47</cell><cell>63.48</cell><cell>56.07</cell><cell>83.70</cell><cell>65.64</cell><cell>59.40</cell></row><row><cell>GCN-Align*</cell><cell>81.81</cell><cell>67.45</cell><cell>57.94</cell><cell>86.74</cell><cell>67.65</cell><cell>60.32</cell></row><row><cell>RDGCN</cell><cell>86.91</cell><cell>72.90</cell><cell>66.44</cell><cell>86.82</cell><cell>74.35</cell><cell>69.54</cell></row><row><cell>DGMC</cell><cell>89.35</cell><cell>72.17</cell><cell>69.98</cell><cell>90.12</cell><cell>76.60</cell><cell>68.76</cell></row><row><cell></cell><cell></cell><cell></cell><cell>OpenEA</cell><cell></cell><cell></cell><cell></cell></row><row><cell>init</cell><cell></cell><cell></cell><cell></cell><cell>Sun [15]</cell><cell></cell><cell></cell></row><row><cell>subset</cell><cell>d-w</cell><cell></cell><cell>d-y</cell><cell>en-de</cell><cell></cell><cell>en-fr</cell></row><row><cell>Zero Shot</cell><cell>46.53</cell><cell></cell><cell>81.90</cell><cell>75.99</cell><cell></cell><cell>79.90</cell></row><row><cell>GCN-Align*</cell><cell>45.76</cell><cell></cell><cell>84.65</cell><cell>85.34</cell><cell></cell><cell>89.41</cell></row><row><cell>RDGCN</cell><cell>64.28</cell><cell></cell><cell>98.41</cell><cell>80.03</cell><cell></cell><cell>91.52</cell></row><row><cell>DGMC</cell><cell>51.29</cell><cell></cell><cell>88.60</cell><cell>88.10</cell><cell></cell><cell>89.40</cell></row><row><cell></cell><cell></cell><cell></cell><cell>WK3l15k</cell><cell></cell><cell></cell><cell></cell></row><row><cell>init</cell><cell></cell><cell></cell><cell></cell><cell>BERT</cell><cell></cell><cell></cell></row><row><cell>subset</cell><cell></cell><cell></cell><cell>en-de</cell><cell></cell><cell></cell><cell>en-fr</cell></row><row><cell>Zero Shot</cell><cell></cell><cell></cell><cell>85.55</cell><cell></cell><cell></cell><cell>77.27</cell></row><row><cell>GCN-Align*</cell><cell></cell><cell></cell><cell>85.92</cell><cell></cell><cell></cell><cell>78.22</cell></row><row><cell>RDGCN</cell><cell></cell><cell></cell><cell>86.76</cell><cell></cell><cell></cell><cell>78.05</cell></row><row><cell>DGMC</cell><cell></cell><cell></cell><cell>84.08</cell><cell></cell><cell></cell><cell>73.92</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Commonly used evaluation metrics in EA automatically become better with a smaller size of test set<ref type="bibr" target="#b2">[3]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/rusty1s/pytorch geometric/blob/d42a690fba68005f5738008a04f375ffd39bbb76/torch geometric/d 3 https://drive.google.com/open?id=1dYJtj1 J4nYJdrDY95ucGLCuZXDXI7PL 4 https://github.com/syxu828/Crosslingula-KG-Matching/blob/56710f8131ae072f00de97eb737315e4ac9510f2/READM</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://github.com/nju-websoft/OpenEA/tree/2a6e0b03ec8cdcad4920704d1c38547a3ad72abe 6 https://github.com/google-research/bert/blob/cc7051dc592802f501e8a6f71f8fb3cf9de95dc9/multilingual.md</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">https://github.com/StephanieWyt/RDGCN 8 https://github.com/rusty1s/deep-graph-matching-consensus/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">As a general rule, the results improve by 1-2 points when trained on train+validation, and it is not going to change the picture.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work has been funded by the German Federal Ministry of Education and Research (BMBF) under Grant No. 01IS18036A. The authors of this work take full responsibilities for its content.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Random search for hyper-parameter optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="281" to="305" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Knowledge graph entity alignment with graph convolutional networks: Lessons learned</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Berrendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Faerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentyn</forename><surname>Melnychuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><forename type="middle">M</forename><surname>Seidl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emine</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">João</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Magalhães</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Castells</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mário</forename><forename type="middle">J</forename><surname>Ferro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flávio</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Martins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Information Retrieval -42nd European Conference on IR Research</title>
		<meeting><address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="3" to="11" />
		</imprint>
	</monogr>
	<note>Proceedings, Part II</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Interpretable and fair comparison of link prediction or entity alignment methods with adjusted mean rank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Berrendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Faerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Vermue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Tresp</surname></persName>
		</author>
		<idno>abs/2002.06914</idno>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-channel graph neural network for entity alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1452" to="1461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT (1)</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Special issue on knowledge graphs and semantics in text analysis and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Dietz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dalton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Meij</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Retr. J</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="231" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep graph matching consensus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Eric</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
	<note>Open-Review.net</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Semi-supervised entity alignment via joint knowledge embedding model and crossgraph model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2723" to="2732" />
		</imprint>
	</monogr>
	<note>EMNLP/IJCNLP (1)</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">MRAEA: an efficient and robust entity alignment approach for cross-lingual knowledge graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenting</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Man</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanbin</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="420" to="428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<editor>Alessandro Moschitti, Bo Pang, and Walter Daelemans</editor>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2014-10-25" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Modeling multi-mapping relations for precise crosslingual entity alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghua</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP/IJCNLP (1)</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="813" to="822" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Highway networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Rupesh Kumar Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
		<idno>abs/1505.00387</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Knowledge graph alignment network with gated multi-hop neighborhood aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuzhong</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="222" to="229" />
		</imprint>
	</monogr>
	<note>The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A benchmarking study of embedding-based entity alignment for knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farahnaz</forename><surname>Akrami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengkai</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB Endow</title>
		<meeting>VLDB Endow</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="2326" to="2340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cross-lingual knowledge graph alignment via graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingsong</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="349" to="357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Relation-aware entity alignment for heterogeneous knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuting</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence</title>
		<editor>Sarit Kraus</editor>
		<meeting>the Twenty-Eighth International Joint Conference on Artificial Intelligence<address><addrLine>Macao, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="page" from="5278" to="5284" />
		</imprint>
	</monogr>
	<note>ijcai.org</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Jointly learning entity and relation representations for entity alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuting</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP/IJCNLP (1)</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="240" to="249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">High-order relation construction and mining for graph matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youmin</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoying</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuting</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luoyi</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinbing</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/2010.04348</idno>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Cross-lingual knowledge graph alignment via graph matching neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019</title>
		<editor>Anna Korhonen, David R. Traum, and Lluís Màrquez</editor>
		<meeting>the 57th Conference of the Association for Computational Linguistics, ACL 2019<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-08-02" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3156" to="3161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Aligning cross-lingual entities with multi-aspect information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiu-Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP/IJCNLP (1)</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4430" to="4440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A vectorized relational graph convolutional network for multi-relational network alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujie</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingzhong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4135" to="4141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-view knowledge graph embedding for entity alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingbing</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuzhong</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5429" to="5435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An experimental study of state-of-the-art entity alignment approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Suchanek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge &amp; Data Engineering</title>
		<imprint>
			<biblScope unit="issue">01</biblScope>
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Neighborhoodaware attentional representation for multilingual knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiannan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1943" to="1949" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
