<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RWR-GAE: Random Walk Regularization for Graph Auto Encoders</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vaibhav</surname></persName>
							<email>vvaibhav@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Yao</forename><surname>Huang</surname></persName>
							<email>poyaoh@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Frederking</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">RWR-GAE: Random Walk Regularization for Graph Auto Encoders</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Code available at https:// github.com/MysteryVaibhav/DW-GAE.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Node embeddings have become an ubiquitous technique for representing graph data in a low dimensional space. Graph autoencoders, as one of the widely adapted deep models, have been proposed to learn graph embeddings in an unsupervised way by minimizing the reconstruction error for the graph data. However, its reconstruction loss ignores the distribution of the latent representation, and thus leading to inferior embeddings. To mitigate this problem, we propose a random walk based method to regularize the representations learnt by the encoder. We show that the proposed novel enhancement beats the existing state-of-the-art models by a large margin (upto 7.5%) for node clustering task, and achieves state-of-the-art accuracy on the link prediction task for three standard datasets, cora, citeseer and pubmed.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Analysis of graph data plays an important role in various data mining tasks including node classification <ref type="bibr" target="#b1">[Kipf and Welling, 2016a]</ref>, link prediction <ref type="bibr" target="#b8">[Wang et al., 2017b]</ref>, and node clustering <ref type="bibr" target="#b8">[Wang et al., 2017a]</ref>. These tasks are useful for various kinds of graph data including protein-protein interaction networks, social media, and citation networks. However, it is known that working with graph data is a challenging task because of its high computational cost and low parallelizability. Further, the inapplicability of machine learning methods <ref type="bibr" target="#b0">[Cui et al., 2018]</ref> to such data aggravates the problem.</p><p>Recent developments in graph embeddings have emerged as a boon for dealing with complex graph data. The general idea behind learning a graph embedding is to learn a latent, low-dimensional representation of network vertices, while preserving network topology structure, vertex content, and other information. <ref type="bibr" target="#b6">[Perozzi et al., 2014]</ref> proposed a DeepWalk model to learn node embeddings by reducing the problem to a skipgram formulation <ref type="bibr" target="#b3">[Mikolov et al., 2013b]</ref> used to learn word embeddings. Recent works [Kipf and * Contact Author Left: Generated by an autoencoder model Right: Generated by the proposed model. We see that the embeddings on the left have dense representations for nodes belonging to the same cluster whereas the embeddings on the right have an even intra-cluster spread which reduces the potential loss in clustering accuracy across boundaries. See § 8.1 for a detailed discussion. <ref type="bibr" target="#b1">Welling, 2016b]</ref> show that graph autoencoder in conjunction with Graph Convolutional networks <ref type="bibr" target="#b1">[Kipf and Welling, 2016a]</ref> are even more effective in learning low dimensional representations of the nodes. However, there are a few shortcomings in using autoencoders for learning graph embeddings. First, there is no restriction on the distribution of the latent representation learnt by the encoder which might result in inefficient embeddings <ref type="bibr" target="#b5">[Pan et al., 2018]</ref>. Second, the reconstruction loss might not be a strong signal to capture the local graph topology <ref type="bibr" target="#b0">[Goyal et al., 2018]</ref>. <ref type="figure" target="#fig_0">Figure 1</ref> shows the effect of these problems on Cora. <ref type="bibr" target="#b5">[Pan et al., 2018]</ref> tried to address the first shortcoming by applying a Gaussian prior on the distribution of node representations. We argue that enforcing Gaussian prior on the latent code of node embeddings might not be the best option and propose a random walk based regularization technique which tries to enforce a restriction on the representation such that the embeddings learn to predict their context nodes. This is achieved by adding an additional training objective. This serves two purpose at once, first, instead of adding a prior on the latent representation of the nodes, we provide additional supervision for improving the quality of each node embedding. Second, the node embeddings are forced to capture the local network topology since the added objective is maximized when the node embeddings correctly predict their context embeddings. The proposed model allows for a natural graph regularization on the embeddings, whilst providing additional training signals to improve individual embeddings.</p><p>Through our experiments, we show that the proposed random walk regularization is superior to all other methods at unsupervised clustering task. The contributions of this paper are two fold,</p><p>• We propose a novel technique of using random walks for regularizing the node representations learned by a Graph autoencoder. • We show that the proposed regularization technique is effective at unsupervised clustering and outperforms all the other methods. Further, we show that the resulting embeddings are general in nature and achieve state of the art accuracy on the link prediction task as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Learning node embeddings for networks has been a long standing problem. Conventionally, learning node embeddings was seen as either a feature engineering task or a dimentionality reduction task. <ref type="bibr" target="#b7">[Tang and Liu, 2011]</ref> and <ref type="bibr">[Henderson et al., 2011]</ref> proposed to use hand-crafted features based on the network properties. On the other hand, <ref type="bibr" target="#b0">[Belkin and Niyogi, 2002]</ref> and <ref type="bibr" target="#b6">[Roweis and Saul, 2000]</ref> used linear algebra tools to reduce the adjaceny matrix of a graph to a lower dimension. The advancement of feature learning in other domains, particularly the SkipGram model <ref type="bibr" target="#b3">[Mikolov et al., 2013b]</ref>, proposed to learn word embeddings opened ways to learn node features as well. <ref type="bibr" target="#b6">[Perozzi et al., 2014]</ref> proposed a DeepWalk model which used random walk <ref type="bibr" target="#b4">[Pan et al., 2004]</ref> for learning graph embeddings. Their proposed objective was similar to the SkipGram <ref type="bibr" target="#b3">[Mikolov et al., 2013b]</ref> model. They used nodes obtained from a random walk as the context nodes and tried to predict the context nodes using the node on which the walk was performed. This work exploited the graph structure to learn the embeddings. <ref type="bibr" target="#b8">[Yang et al., 2015]</ref> proposed an extension to the DeepWalk model which enhanced the node representations by additionally incorporating the node features available from other sources, like the text features for each node. Since then, a number of probabilistic models have been proposed including <ref type="bibr" target="#b0">[Grover and Leskovec, 2016]</ref> and <ref type="bibr" target="#b7">[Tang et al., 2015]</ref>, which try to map the nodes to a low-dimensional space of features that maximizes the likelihood of preserving network neighborhoods of nodes.</p><p>In the current research where deep learning is taking control over everything, Graph autoencoders have emerged as the go-to method for embedding graphs, mostly because of its good performance, efficiency and ease of use. The idea of integrating graph with neural models was first introduced by <ref type="bibr" target="#b1">[Kipf and Welling, 2016a]</ref>, who proposed Graph Convolution Networks (GCN) which could effectively encode graphs. GCNs can naturally incorporate node features, which significantly improves predictive performance on various tasks. Inspired by the autoencoder frameworks <ref type="bibr" target="#b0">[Kingma and Welling, 2013]</ref>, <ref type="bibr">Kipf et al. proposed</ref> Graph autoencoder framework <ref type="bibr" target="#b1">[Kipf and Welling, 2016b]</ref> which used GCNs as encoder and simple inner product as decoder. <ref type="bibr" target="#b5">[Pan et al., 2018]</ref> identified that the graph autoencoders don't put any restriction on the distribution of latent representation which could possibly lead to inferior embeddings. To address this problem, they proposed an adversarially regularized graph auto encoder which puts a Gaussian prior on the latent distribution. Our work is motivated from this work, and we argue that Gaussian prior might not be the most natural distribution for a node's latent representation. We instead propose a random walk based regularization method which doesn't enforce any prior on the latent representation but regularizes the representations in such a way that they learn the network's local topology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Problem Definition</head><p>We consider a general problem of learning unsupervised graph embeddings for any graph G. A graph G = (V, E) can be represented in terms of its vertices (V = {v 1 , v 1 , . . . , v n }) and edges (E = {e ij }∀i, j s.t ∃ an edge between the nodes v i and v j . To efficiently represent the graph topology for computational use, we represent the edges using an adjacency ma-</p><formula xml:id="formula_0">trix A ∈ R n×n , where A ij = 1 if e ij ∈ E else A ij = 0.</formula><p>Depending on the nature of the graph, we might have an additional node feature matrix X ∈ R n×h , where each row of the matrix represents a h-dimensional content vector for each node in the graph.</p><p>Given a graph G, we want to learn a d-dimension vector for each node v i such that d &lt;&lt; n. Putting everything together, we want to learn a function F such that F (A, X) −→ Z, where Z is an embedding matrix in R n×d . We want Z to capture the node content as well as the topological structure in a continuous low dimensional space. <ref type="bibr" target="#b1">[Kipf and Welling, 2016a]</ref> introduced GCN to directly embed the graph structure in a low dimensional space using neural networks. Given a graph G = (A, X) where A is the adjacency matrix and X is the feature matrix, the graph convolutional network is a spectral convolutional operation denoted by f (Z l , A|W l ),</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Proposed Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Graph Convolutional Networks</head><formula xml:id="formula_1">Z l+1 = f (Z l , A|W l )<label>(1)</label></formula><p>Here, Z l is the output feature corresponding to the nodes after l th convolution. W l is the parameter associated with the l th layer. Based on the above operation, we can define arbitrarily deep networks. However, one caveat to constructing deep graph convolutional networks is that, after each layer W l is multiplied with A and since A is not normalized, it changes the scale of the feature vectors. To address this issue, we refine the convolutional function to be,</p><formula xml:id="formula_2">f (Z l , A|W l ) = σ(D −1/2ÂD−1/2 Z l W l )<label>(2)</label></formula><p>Here,Â = A+I, where I is the identity matrix,D is the diagonal node degree matrix ofÂ and σ is the activation function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Graph Autoencoder</head><p>Graph autoencoders are an extension to the autoencoder framework consisting of an encoder and a decoder network. We use a 2-layer GCN as the encoder and inner product as the decoder. The encoder output is given by Z 2 = Z = q(Z|X, A), The obtained node embeddings are then used in the decoder to reconstruct the graph (Â),</p><formula xml:id="formula_3">Z 1 = f relu (Z 0 , A|W 0 )<label>(3)</label></formula><formula xml:id="formula_4">Z 2 = f linear (Z 1 , A|W 1 )<label>(4)</label></formula><formula xml:id="formula_5">A = σ(ZZ T )<label>(5)</label></formula><p>Note that we can reconstruct both A and X. However for our method, we just reconstruct the adjacency matrix as it is more flexible for graphs which don't have content information. The network is trained using a reconstruction loss L R ,</p><formula xml:id="formula_6">L R = E q(Z|X,A) [log p(A|Z)]<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Variational Graph Autoencoder</head><p>Variational Graph autoencoders are defined by an inference model,</p><formula xml:id="formula_7">q(Z|X, A) = n i=1 q(z i |X, A) (7) q(z i |X, A) = N (z i |µ i , diag(σ 2 ))<label>(8)</label></formula><p>Here, µ = Z 2 is a matrix of mean vectors z i , σ = f linear (Z 1 , A|W ) is the covariance matrix. The decoder model remains roughly the same and the adjacency matrix can be reconstructed using the mean vectors,</p><formula xml:id="formula_8">A ij = σ(z T i z j )<label>(9)</label></formula><p>For training the variational graph autoencoder, we optimize the variational lower bound as follows,</p><formula xml:id="formula_9">L R = E q(Z|X,A) [log p(A|Z)] + KL(q(Z|X, A)||p(Z))<label>(10)</label></formula><p>Here, KL(q(·)||p(·)) denotes the Kullback-Leibler divergance and p(Z) = i N (z i |0, I) denotes the Guassian prior for the latent data distribution. We perform the reparameterization trick <ref type="bibr" target="#b0">[Kingma and Welling, 2013]</ref> to train the variational model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Random Walk Regularization</head><p>The main contribution of our model is the proposed regularization technique which forces the latent representation of the nodes to inherently capture the information of their immediate context nodes. We argue that using a Graph autoencoder with reconstruction loss for learning the node embeddings doesn't force the latent representations of the nodes to necessarily capture the local context information present at various locations in the network. Thus, we add an extra objective while training to enforce this restriction. Inspired from Deep-Walk <ref type="bibr" target="#b6">[Perozzi et al., 2014]</ref>, we leverage local information obtained from truncated random walks to learn latent representations by treating walks as the equivalent of sentences. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Random Walk with Restarts</head><p>We leverage the idea of Random Walk with Restarts (RWR) <ref type="bibr" target="#b4">[Pan et al., 2004</ref>] to obtain context nodes from any given node. W vi denotes a set of context nodes obtained using RWR from the start node v i . Algorithm 1 defines a procedure to obtain W vi .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SkipGram</head><p>Once we obtain a set of context nodes, we use a Skip-Gram <ref type="bibr" target="#b2">[Mikolov et al., 2013a]</ref> type model which has two embedding layers corresponding to the nodes and context nodes.</p><p>Originally, SkipGram was designed as a language model that maximizes the co-occurrence probability among the words that appear within a window in a sentence. For this graph setting, we borrow the idea from <ref type="bibr" target="#b6">[Perozzi et al., 2014]</ref>, and use the set of nodes obtained from the random walk as our sentence and maximize the co-occurrence probability of the nodes. The objective function used to train this model is given </p><formula xml:id="formula_10">L S = log p(µ i |Z(v i ))<label>(11)</label></formula><p>Here, µ i ∈ W vi and Z(v i ) denotes the latent representation for the node v i generated by the encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our model</head><p>Algorithm 2 is our overall proposed framework. We train the entire network in an end-to-end fashion. An important consideration while training the network was choosing the order of the back-propagated gradients. We experimented with all possibilities and picked the one which gave best performance as per <ref type="figure" target="#fig_3">Figure 3</ref>. Based on the type of encoder-decoder framework used, we present two kinds of regularized network,</p><p>• Random Walk Regularized Graph Autoencoder (RWR-GAE), for this model we use Eq. 6 to update the decoder and encoder parameters.</p><p>• Random Walk Regularized Variational Graph Eutoencoder (RWR-VGAE), the encoder in this model is based on the variational inference model, and we use Eq. 10 to update the decoder and encoder weights.</p><p>For both the models, we additionally use Eq. 11 for updating the skipgram model and encoder parameters.</p><p>Algorithm 2 Regularization through Random Walk for Graph Autoencoders Input: Graph G(V, X, A), window size w, walks per epoch γ, walk length t, restart probability α</p><formula xml:id="formula_11">Z ← Encoder(G) V = shuffle(V ) O = sample γ vertices from V for each v i ∈ O do W vi = Random Walk(A, v i , t, alpha) for each v j ∈ W vi do for each µ k ∈ W vi [j − w : j + w] do L vj = − log P r(µ k |Z(v j ))</formula><p>Update SkipGram and Encoder using ∇L vĵ A ← Decoder(Z) Update Decoder and Encoder using Eq.6 or Eq.10</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Baselines</head><p>A rich line of work has been done for learning graph embeddings in an unsupervised setting. We briefly summarize some of the recent approaches used as our baseline,</p><p>• DeepWalk <ref type="bibr" target="#b6">[Perozzi et al., 2014]</ref>: is a network representation approach which encodes social relations into a continuous vector space by learning structural regularities present within short random walks. • Spectral Clustering <ref type="bibr" target="#b7">[Tang and Liu, 2011]</ref>: is an effective approach for learning social embedding. This method generates a representation in R d from the d-smallest eigenvectors of L, the normalized graph Laplacian of G. • GAE <ref type="bibr" target="#b1">[Kipf and Welling, 2016b]</ref>: is the most recent autoencoder-based unsupervised framework for graph data, which naturally leverages both topological and content information. • VGAE <ref type="bibr" target="#b1">[Kipf and Welling, 2016b]</ref>: is a variational graph autoencoder approach for graph embedding with both topological and content information. • ARGA <ref type="bibr" target="#b5">[Pan et al., 2018]</ref>: is an adversarially regularized autoencoder algorithm which uses graph autoencoder. • ARVGA <ref type="bibr" target="#b5">[Pan et al., 2018]</ref>: is also an adversarially regularized autoencoder, which uses a variational graph autoencoder.    arated into a training, testing set and validation set. The validation set consists of 5% citation edges for hyper-parameter tuning, the test set contains 10% citation edges for reporting the final performance, and the rest are used for training. <ref type="table">Table 1</ref> contains the training data statistics for each of the datatset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Tasks and Evaluation metric</head><p>We evaluate the quality of the learned embeddings by analyzing the performance on two downstream tasks, Node clustering and Link Prediction.</p><p>• Node Clustering, unsupervised clustering based on the  node embeddings. After learning the embeddings, we do K-means clustering to get the final clusters. Following <ref type="bibr">[Xia et al., 2014]</ref>, we use five metrics to validate the clustering results: accuracy (Acc), normalized mutual information (NMI), precision, F-score (F1) and average rand index (ARI).</p><p>• Link Prediction, predict the edges and non-edges among the test set nodes. For doing such a prediction, we simply use Eq. 5 to get the reconstructed graph from the node embeddings. Following <ref type="bibr" target="#b1">[Kipf and Welling, 2016b]</ref>, we report the AUC score (the area under a receiver operating characteristic curve) and average precision (AP) score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Hyper-parameters</head><p>We use the hyper-parameters provided by <ref type="bibr" target="#b1">[Kipf and Welling, 2016b]</ref> for the autoencoder related hyper-parameters of our model. For the hyper-parameters related to the Random Walk Regularization network, we set the number of walks to 50, window size to {30, 20} and walk length to {30, 20}. In our experiments, we find that the best performing model uses 50 walks with a window size and walk length of either 30 or 20 depending on the dataset and the kind of autoencoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Results</head><p>We now present quantitative results of our model on the node clustering task. <ref type="table" target="#tab_3">Tables 3, 4</ref> and 5 contain the results for the datasets, cora, citeseer and pubmed respectively. We see that our proposed random walk regularization consistently outperforms all other baselines for all evaluation metrics. For the Cora dataset, we find that RWR-based methods improve the accuracy by 41.5% when compared to DeepWalk and by 12.4% when compared to Variational Graph Autoencoder. On the Citeseer dataset, we find that RWR-GAE beats the adversarially regularized autoencoder method by 7.5% on accuracy and by 7.1% on F1 score. For the PubMed dataset which has the largest number of nodes but the smallest number of clusters, we find that our method improves upon the GAE by 18.3% on ARI and by 7.5% on NMI.</p><p>Result for the link prediction task can be found in <ref type="table" target="#tab_2">Table 2</ref>. We find that our proposed method performs at par with the existing baselines. It is interesting to note that the random walk regularized autoencoder convincingly outperforms the DeepWalk method, indicating that the random walk is very well complemented by the autoencoder methods. <ref type="figure" target="#fig_0">Figure 1</ref> shows the quality of the embeddings using 2-d tsne <ref type="bibr" target="#b8">[Van Der Maaten, 2014]</ref> plots. The left plot is obtained by using the node embeddings learned by GAE and the right plot shows the graph embeddings learned by our RWR-GAE model. We observe that both the methods do a good job of identifying clusters based on the node embeddings. However, if we look closely at the embeddings generated by the GAE model, we find that the representation of the intra-cluster nodes are quite similar in nature but are not equally distant from the cluster centroid. Where as the embeddings generated by our RWR-GAE model have a more even spread within the cluster i.e the embeddings within a cluster are similar to each other. To measure this property, we define intra-cluster distance as the sum of euclidean distance of each node in the cluster to its centroid, averaged across all the clusters. We find that the embeddings generated by our model have less intra-cluster distance (0.64) compared to embeddings generated by GAE (0.99). We argue that this property is induced by the random walk regularization as the individual node embeddings need to predict the context nodes within a neighborhood, thus during training phase, the node embedding will prefer to converge to a representation such that it's informative of its context nodes. This results in improved embeddings for the clustering task, as a slight overlap among nodes of different clusters will now have relatively less impact on the clustering accuracy compared to the case when the intracluster embeddings are too close to each other and not evenly spread.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Graph Visualization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Study using different hyper-parameters</head><p>In this study, we try to understand how random walk helps in regularizing the embeddings. The left most embeddings in <ref type="figure" target="#fig_4">Figure 4</ref> are generated by our model with window size, number of walks and walk length set to 5. We observe that for this case, some of the clusters are subsumed into other clusters and thus achieve a very low clustering accuracy. We believe that this happens because of the extremely low values of walk length and window size. An intuitive explanation for this is that a low window size limits a nodes capability to look at enough nodes to decide its cluster candidacy. As we increase the window size, we observe that the clusters start to get more distinct from each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">Side-effects of random walk regularization</head><p>We list down two critical observations about our model. First, from <ref type="table" target="#tab_2">Table 2</ref>, we observe that our proposed model has a considerably higher variance in scores. We attribute this behaviour to the introduced randomness while selecting nodes for random walk during the training phase. Second, from Algorithm 2, we see that the number of updates made to the encoder weights are considerably higher than the GAE model, as a result our proposed model converges to the best accuracy in fewer pass over the entire data. We consistently see the best results at around 100 epochs as opposed to 200 epochs for GAE.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Node embeddings learned by two different architectures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Random Walk Regularized Graph Autoencoder. Top half of the network corresponds to the Graph Auto-Encoder. Bottom half shows the proposed Random Walk Regularization network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure 2 shows the overall architecture of our proposed network. The lower half of the figure represents the regularization network. There are two main components of the regularization network, (a) Random Walk with Restarts and (b) SkipGram model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Forward and backward propagation in order for training the model. Green arrow denotes the forward propagation and red denotes backward.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Visualization using node embeddings generated by different hyperparameters using RWR-GAE on Cora. Left-to-right: number of walks = walk length = window size = {5, 20, 30}, and accuracy = {0.34, 0.56, 0.64}</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Algorithm 1 Random Walk with RestartsInputs: adjacency matrix A, start node v 0 , path length t, restart probability α procedure RANDOM WALK(A, v 0 , l, α) path = [v 0 ] path stores the nodes in the walk</figDesc><table><row><cell>while path length ≤ t -1 do</cell><cell></cell></row><row><cell>curr ← last node in path</cell><cell></cell></row><row><cell>rand = Random(0, 1)</cell><cell>random number ∈ (0, 1)</cell></row><row><cell>if rand ≥ α then</cell><cell></cell></row><row><cell cols="2">add random neighbor(curr, A) to path</cell></row><row><cell>else</cell><cell></cell></row><row><cell>add v 0 to path</cell><cell>in the case of restart</cell></row><row><cell>return path</cell><cell></cell></row><row><cell>by the equation below,</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison of different models on the Link Prediction task across various datasets. We conduct each experiment 10 times and report the mean values with the standard deviation.</figDesc><table><row><cell>Model</cell><cell>Acc</cell><cell>NMI</cell><cell>F1</cell><cell cols="2">Precision ARI</cell></row><row><cell>SC</cell><cell cols="3">0.367 0.127 0.318</cell><cell>0.193</cell><cell>0.031</cell></row><row><cell>DW</cell><cell cols="3">0.484 0.327 0.392</cell><cell>0.361</cell><cell>0.243</cell></row><row><cell>GAE</cell><cell cols="3">0.596 0.429 0.595</cell><cell>0.596</cell><cell>0.347</cell></row><row><cell>VGAE</cell><cell cols="3">0.609 0.436 0.609</cell><cell>0.609</cell><cell>0.346</cell></row><row><cell>ARGE</cell><cell cols="3">0.640 0.449 0.619</cell><cell>0.646</cell><cell>0.352</cell></row><row><cell>ARVGE</cell><cell cols="3">0.638 0.450 0.627</cell><cell>0.624</cell><cell>0.374</cell></row><row><cell>RWR-GAE</cell><cell cols="3">0.669 0.481 0.618</cell><cell>0.629</cell><cell>0.417</cell></row><row><cell cols="4">RWR-VGAE 0.685 0.455 0.668</cell><cell>0.685</cell><cell>0.417</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Performance comparison of different models for the Clustering task on Cora.</figDesc><table><row><cell>Model</cell><cell>Acc</cell><cell>NMI</cell><cell>F1</cell><cell cols="2">Precision ARI</cell></row><row><cell>SC</cell><cell cols="3">0.239 0.056 0.299</cell><cell>0.179</cell><cell>0.010</cell></row><row><cell>DW</cell><cell cols="3">0.337 0.088 0.270</cell><cell>0.248</cell><cell>0.092</cell></row><row><cell>GAE</cell><cell cols="3">0.408 0.176 0.372</cell><cell>0.418</cell><cell>0.124</cell></row><row><cell>VGAE</cell><cell cols="3">0.344 0.156 0.308</cell><cell>0.349</cell><cell>0.093</cell></row><row><cell>ARGE</cell><cell cols="3">0.573 0.350 0.546</cell><cell>0.573</cell><cell>0.341</cell></row><row><cell>ARVGE</cell><cell cols="3">0.544 0.261 0.529</cell><cell>0.549</cell><cell>0.245</cell></row><row><cell>RWR-GAE</cell><cell cols="3">0.616 0.354 0.585</cell><cell>0.605</cell><cell>0.343</cell></row><row><cell cols="4">RWR-VGAE 0.613 0.338 0.582</cell><cell>0.595</cell><cell>0.336</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Performance comparison of different models for the Clustering task on Citeseer.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Performance comparison of different models for the Clustering task on PubMed.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>We began by inspecting the graph embeddings learnt by an autoencoder model. We identified that this model doesn't enforce any restriction on the latent distribution and just uses a reconstruction loss for training which might result in suboptimal embeddings. Further, we observed how this translated in terms of intra-cluster distances. We proposed a random walk based regularization technique for graph autoencoders which addressed both the shortcomings by adding a skipgram objective, which enforces the latent representations to capture network's local topology as well as provide additional training signal. We validated the effectiveness of our method by evaluating the performance of the learned embeddings on two different tasks, node clustering and link prediction for three standard datasets, cora, citeseer and pubmed.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps and spectral techniques for embedding and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niyogi ; Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Cui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.03280</idno>
		<idno>arXiv:1312.6114</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<editor>Keith Henderson, Brian Gallagher, Lei Li, Leman Akoglu, Tina Eliassi-Rad, Hanghang Tong, and Christos Faloutsos</editor>
		<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="663" to="671" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Auto-encoding variational bayes</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Welling ;</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<idno>arXiv:1611.07308</idno>
	</analytic>
	<monogr>
		<title level="m">Variational graph auto-encoders</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Kipf and Welling</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Automatic multimedia cross-modal correlation discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the tenth ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="653" to="658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Pan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04407</idno>
		<title level="m">Adversarially regularized graph autoencoder</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Nonlinear dimensionality reduction by locally linear embedding. science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Perozzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2000" />
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="page">93</biblScope>
		</imprint>
	</monogr>
	<note>Collective classification in network data</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Leveraging social media networks for classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">; Lei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on world wide web</title>
		<meeting>the 24th international conference on world wide web</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
	<note>Line: Largescale information network embedding. International World Wide Web Conferences Steering Committee</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Robust multi-view spectral clustering via low-rank and sparse decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">; Laurens</forename><surname>Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<editor>Rongkai Xia, Yan Pan, Lei Du, and Jian Yin</editor>
		<meeting>the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="2149" to="2155" />
		</imprint>
	</monogr>
	<note>Twenty-Fourth International Joint Conference on Artificial Intelligence</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
