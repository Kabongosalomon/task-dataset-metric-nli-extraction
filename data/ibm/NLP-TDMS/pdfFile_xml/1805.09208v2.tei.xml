<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PUSHING THE BOUNDS OF DROPOUT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gábor</forename><surname>Melis</surname></persName>
							<email>melisgl@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">DeepMind</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
							<email>cblundell@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">DeepMind</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Kočiský</surname></persName>
							<email>tkocisky@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">DeepMind</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DeepMind</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
							<email>cdyer@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">DeepMind</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
							<email>pblunsom@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">DeepMind</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">PUSHING THE BOUNDS OF DROPOUT</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Under review as a conference paper at ICLR 2019</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We show that dropout training is best understood as performing MAP estimation concurrently for a family of conditional models whose objectives are themselves lower bounded by the original dropout objective. This discovery allows us to pick any model from this family after training, which leads to a substantial improvement on regularisation-heavy language modelling. The family includes models that compute a power mean over the sampled dropout masks, and their less stochastic subvariants with tighter and higher lower bounds than the fully stochastic dropout objective. We argue that since the deterministic subvariant's bound is equal to its objective, and the highest amongst these models, the predominant view of it as a good approximation to MC averaging is misleading. Rather, deterministic dropout is the best available approximation to the true objective.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The regularisation technique known as dropout underpins numerous state-of-the-art results in deep learning <ref type="bibr" target="#b7">(Hinton et al. 2012;</ref><ref type="bibr" target="#b19">Srivastava et al. 2014)</ref>, and its application has received much attention in the form of optimisation <ref type="bibr" target="#b21">(Wang &amp; Manning 2013</ref>) and attempts at explaining or improving its approximation properties <ref type="bibr" target="#b0">(Baldi &amp; Sadowski 2013;</ref><ref type="bibr" target="#b27">Zolna et al. 2017;</ref><ref type="bibr" target="#b11">Ma et al. 2016)</ref>. The dominant perspective today views dropout as either an implicit ensemble method <ref type="bibr" target="#b22">(Warde-Farley et al. 2013)</ref> or averaging over an approximate Bayesian posterior <ref type="bibr" target="#b2">(Gal &amp; Ghahramani 2016a)</ref>. Regardless of which view we take, dropout training is carried out the same way, by minimising the expectation of the loss over randomly sampled dropout masks. However, at test time these views naturally lead to different algorithms: the Bayesian approach computes an arithmetic average as it marginalises out the weight uncertainty, while the ensemble approach typically uses the geometric average due to its close relationship to the loss. Collectively they are called MC dropout and neither is clearly better than the other <ref type="bibr" target="#b22">(Warde-Farley et al. 2013)</ref>. A third way to make predictions is to "turn dropout off", that is, propagate expected values through the network in a single, deterministic pass. This deterministic (also known as standard) dropout in considered to be an excellent approximation to MC dropout. This situation is unsatisfactory as it does not provide theoretical grounding for dropout, without which the choice of dropout variant remains arbitrary. In this paper, we provide such theoretical foundations. First, we prove the dropout objective to be a common lower bound on the objectives of a family of infinitely many models. This family includes models corresponding to the three aforementioned methods of evaluation: the arithmetic averaging, the geometric averaging, and the deterministic. Thus by maximising the dropout objective we get a single set of parameters and many models that all have the same parameters but differ in how they make predictions. This allows us to train once and perform model selection at validation time by evaluating the different methods of making predictions corresponding to individual models in the family. Second, we turn the conventional perspective on its head by showing that while dropout training performs stochastic regularisation, the trained model is best viewed as deterministic, not as a stochastic model with a deterministic approximation. This paper is structured as follows. In §2, we revisit variational dropout <ref type="bibr" target="#b2">(Gal &amp; Ghahramani 2016a)</ref> and demonstrate that, despite common perception, sharing of masks is not necessary, neither in theory nor in practice. Then, by recasting dropout in a simple conditional form, we highlight the counterintuitive role played by the variational posterior. §3 contains our main contributions. Here we construct a family of conditional models whose MAP objectives are all lower bounded by the usual dropout objective, and identify a member of this family as best in terms of model fit. In §4, we select the best of this family in terms of generalisation to improve language modelling. Finally, creating a cheap approximation to the bias of this model allows us to get better results from model tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">VARIATIONAL DROPOUT</head><p>Since its original publication <ref type="bibr" target="#b7">(Hinton et al. 2012)</ref>, dropout had been considered a stochastic regularisation method, implemented as a tweak to the loss function. That was until <ref type="bibr" target="#b2">Gal &amp; Ghahramani (2016a)</ref> grounded dropout in much-needed theory. Their subsequent work <ref type="bibr" target="#b3">(Gal &amp; Ghahramani 2016b)</ref> focused on RNNs, showing that if dropout masks are shared between time steps, the objective for their proposed variational model is the same as the commonly used dropout objective with an 2 penalty. Their method became known as variational dropout, not to be confused with <ref type="bibr" target="#b9">Kingma et al. (2015)</ref>, and is used in state-of-the-art sequential models <ref type="bibr" target="#b14">(Merity et al. 2017;</ref><ref type="bibr" target="#b13">Melis et al. 2017</ref>). Before we move on to a more general formulation we revisit it to better understand its critical features.</p><p>First, we recall the derivation of variational dropout. Consider an RNN that takes input x and maps it to output y and is trained on a set of N data points in paired sets X, Y . A variational lower bound on the log likelihood is obtained as follows:</p><formula xml:id="formula_0">ln p(Y |X) = ln E ω∼q(ω) p(Y |X, ω)p(ω) q(ω) E ω∼q(ω) ln p(Y |X, ω) − KL(q(ω)||p(ω)) = q(ω) ln p(Y |X, ω)dω − KL(q(ω)||p(ω)) = N i=1 q(ω) ln p(y i |x i , ω)dω − KL(q(ω)||p(ω)),<label>(1)</label></formula><p>where p(y|x, ω) is defined by the RNN with weights ω. Variational Bayesian methods then maximise this lower bound with respect to the variational distribution q(ω). For variational dropout, q(ω) takes the form of a mixture of two gaussians with small variances: one with zero mean that represents the dropped out rows of weights, and another with mean Θ:</p><formula xml:id="formula_1">q(ω r ) = pN (ω r |0, σ 2 I) + (1 − p)N (ω r |Θ r , σ 2 I)</formula><p>In the above, r is the index of a row of a weight matrix. Dropping whole rows of weights is equivalent to the more familiar view of dropout over units. The prior over the weights is a zero mean gaussian:</p><formula xml:id="formula_2">p(ω) = N (ω|0, σ 2 p I)</formula><p>The loss is defined based on Eq. 1. The integrals are approximated using a single sampleω ∼ q(ω), and the KL term is approximated with weight decay on Θ:</p><formula xml:id="formula_3">L = − N i=1 ln p(y i |x i ,ω i ) + KL(q(ω)||p(ω))<label>(2)</label></formula><p>The same dropout mask (and consequently the same ω) is employed at every time step. This sharing of masks is considered the defining characteristic of variational dropout, but we note in passing that the theory for the non-shared masks case is very similar and there is little between them in practice with LSTMs (see Appendix A). With this we conclude the recap of variational dropout, and describe our contributions in the rest of the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">DROPOUT AS A CONDITIONAL MODEL</head><p>In variational inference the idea is to approximate the intractable and complicated posterior with a simple, parameterised distribution q. Crucially, this approximation affects our inferences and predictions. If we are serious about it being an approximation to the posterior and want to reduce its distortion of the model p, then q can be made more flexible. But making q more flexible in variational dropout can potentially ruin the regularisation effect. So the particular choice of q plays an important, active role: it effectively performs posterior regularisation and acts as an integral part of the model.</p><p>Coming from another angle, <ref type="bibr" target="#b15">Osband (2016)</ref> makes the point that in variational dropout the posterior over weights does not concentrate with more data, unlike for example in <ref type="bibr" target="#b6">Graves (2011)</ref>, which is unexpected behaviour from a Bayesian model. This conundrum is caused by encoding dropout with a fixed rate mixture of fixed variance components in q, which also necessitates expensive tuning of the dropout rate. <ref type="bibr" target="#b4">Gal et al. (2017)</ref> proposes a way to address these shortcomings.</p><p>To avoid getting bogged down in the issues surrounding the suitability of variational inference and ease interpretation, we construct a straightforward conditional model and lower bound its MAP objective in the same form as the variational objective. Suppose we want to do MAP estimation for the model parameters (the means of the distribution of weights, Θ): arg max Θ p(Θ|X, Y ). Consider a conditional model p(Y |X, Θ) as a crippled generative model with p(x i ) constant, x i and Θ independent. Place a normal prior on the means Θ and otherwise make the weights ω conditional on Θ the same way as they were in the variational posterior q(ω):</p><formula xml:id="formula_4">p(Θ) = N (Θ|0, σ 2 p I) p(ω r |Θ) = pN (ω r |0, σ 2 I) + (1 − p)N (ω r |Θ r , σ 2 I) p(y, ω|x, Θ) = p(y|x, ω)p(ω|Θ)<label>(3)</label></formula><p>The log posterior of this model has a similar lower bound to the variational objective (Eq. 1):</p><formula xml:id="formula_5">ln p(Θ|X, Y ) N i=1 p(ω|Θ) ln p(y i |x i , ω)dω + ln p(Θ) − C MAP<label>(4)</label></formula><p>See Appendix C for detailed derivation. Dropping the normalisation constant C MAP that doesn't depend on Θ, and approximating the above integrals with a single sample, the loss corresponding to the MAP objective becomes:</p><formula xml:id="formula_6">L MAP = − N i=1 ln p(y i |x i ,ω i ) − ln p(Θ)<label>(5)</label></formula><p>The first term of this loss is identical to that of the loss for variational dropout (Eq 2). If the prior on Θ is a zero mean gaussian, then the second term is equivalent to a weight decay penalty just like the KL term in the variational setup. With the two losses being effectively the same, in the following we focus on MAP estimation for the conditional model to sidestep any questions about whether variational inference makes sense in this case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">THE DROPOUT FAMILY OF MODELS</head><p>Having developed a conditional model for dropout that leads to the same objective as variational dropout, we now derive a family of models whose objectives are all lower bounded by the usual dropout objective. We draw inspiration from the different evaluation methods employed for dropout:</p><p>• Deterministic dropout propagates the expectation of each unit through the network in single pass. This is very efficient and is viewed as a good approximation to the next option.</p><p>• MC dropout mimicks the training procedure, and averages the predicted probabilities over randomly sampled dropout masks. With one forward pass per sample, this can be rather expensive. There is some ambiguity as to what kind of averaging shall be applied: oftentimes the geometric average (GMC) is used, because of its close relationship to the loss, but the arithmetic average (AMC) is also widespread.</p><p>Our goal in this section is to demonstrate the consequences of optimising a lower bound instead of the true objective. While it is easy to argue in general that objectives of more than one model may share any given lower bound, for dropout a particularly simple explicit construction of such a family of models is possible. As we will see, this allows for post-training model selection based on validation results given a trained set of parameters. In the absence of validation results to guide model selection, inspection of the tightness of the lower bound indicates the deterministic model as the most reasonable choice from the family.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">GEOMETRIC MODEL</head><p>First, we investigate whether the geometric or the arithmetic mean is the correct choice for making predictions in the context of classification. Recall the predictive term of the MAP loss in Eq. 5: ln p(y i |x i ,ω i ). Notice how with SGD and multiple epochs, for each data point several dropout masks are encountered, and the approximating quantity becomes the geometric mean of the predicted probabilities p(y i |x i , ω) over the masked weights. For this reason, the posterior predictive distribution p(y * |x * , X, Y ) is often computed as the renormalised geometric mean. This is in apparent conflict with the conditional model that prescribes the arithmetic mean (integrating ω out of Eq. 3). However, we can define another model where the conditional distribution is directly defined to be the renormalised geometric mean</p><formula xml:id="formula_7">p(y|x, Θ) = exp Eω ∼p(ω|Θ) ln p(y|x,ω) Z(x, Θ) , Z(x, Θ) = C c=1 exp Ê ω∼p(ω|Θ) ln p(c|x,ω)<label>(6)</label></formula><p>with a slight abuse of notation, due to using the symbol p in p(y|x, Θ) although p(y|x, Θ) = E ω p(ω|Θ)p(y|x, ω). It can be shown that the arithmetic model's (Eq. 3) lower bound (Eq. 4) is a lower bound for this renormalised geometric model (Eq. 6), as well. See Appendix D for the derivation. The answer to the question whether we should use GMC or AMC is that it depends: they correspond to different models, but the dropout objective is a lower bound on the objectives of both models. So one can freely choose between GMC and AMC at evaluation time, doing model selection retrospectively after training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">THE POWER MEAN MODEL FAMILY</head><p>Having two models to choose from, it is natural to ask whether these are just instantiations of a larger class of models. We propose the power mean family of models to extend the set of models to a continuum between the geometric and arithmetic models described in §3.1 and §2.1, respectively, and show that they have the same lower bound. The power mean is defined as:</p><formula xml:id="formula_8">M α (x 1 , . . . , x n ) = 1 n n i=1 x α i 1/α</formula><p>For α = 1 we arrive at the arithmetic mean while the natural extension to α = 0 is the geometric mean as it is the limit of M α at α → 0, which can be proven with L'Hôpital's rule. Similarly to the construction of the geometric model, we define the power mean model by directly conditioning on Θ:</p><formula xml:id="formula_9">p(y|x, Θ) = α Eω ∼p(ω|Θ) p(y|x,ω) α Z(x, Θ) , Z(x, Θ) = C c=1 α Ê ω∼p(ω|Θ) p(c|x,ω) α<label>(7)</label></formula><p>where Z(x, Θ) is at most 1 if α ∈ (−∞, 1] because M α is monotonically increasing in α and Z is 1 for α = 1. Here we provide a concise derivation of a lower bound on the log posterior (the full derivation can be found in Appendix E):</p><formula xml:id="formula_10">ln p(Θ|X, Y ) = N i=1 ln α Ê ω∼p(ω|Θ) p(y i |x i ,ω) α − ln(Z(x i , Θ)) + ln p(Θ) − C MAP N i=1 ln α Ê ω∼p(ω|Θ) p(y i |x i ,ω) α + ln p(Θ) − C MAP (8) N i=1 1 α Ê ω∼p(ω|Θ) ln p(y i |x i ,ω) α + ln p(Θ) − C MAP (9) = N i=1 p(ω|Θ) ln p(y i |x i , ω)dω + ln p(Θ) − C MAP</formula><p>The first inequality above follows from Z(x, Θ) 1 for all x, Θ, while the second is an application of Jensen's rule assuming α &gt; 0. We arrived at the same lower bound on the objective as we had for the geometric (Eq. 6) and arithmetic models (Eq. 3), thus defining the power mean family with parameter α ∈ [0, 1] of models from which we can choose at evaluation time. For α &gt; 1, the normalising constant Z would be greater than 1, and this would not be a lower bound in general.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">TIGHTNESS OF THE LOWER BOUND</head><p>To better understand the quality of fit for models in the power mean family we examine the tightness of their lower bounds. There are two steps involving inequalities in the derivation of the bound: one where the normalisation constant Z is dropped (Eq. 8) and another where the logarithm is moved inside the expectation (Eq. 9). We show that the gaps introduced by these steps can be made arbitrarily small by reducing the variance of p(y|x, ω) with respect to ω.</p><p>Notice that the Jensen gap with the logarithm function is scale invariant:</p><formula xml:id="formula_11">ln(E[λL]) − E ln(λL) = ln(E L) − E ln(L)</formula><p>Intuitively, this suggests that var(L)/(E L) 2 is closely related to the size of the gap. Indeed, Maddison et al. <ref type="formula" target="#formula_0">(2017)</ref> show that if the first inverse moment of L is finite, then</p><formula xml:id="formula_12">ln(E L) − E ln(L)) = var(L) 2(E L) 2 + O( E[(L − E L) 6 ])<label>(10)</label></formula><p>Here we go a bit further and show that if there is a positive lower and upper bound on L, then there are non-trivial lower and upper bounds on its Jensen gap and these are bounds are multiplicative in var(L). Let L be a random variable such that P</p><formula xml:id="formula_13">(L ∈ (a, b)) = 1 where −∞ a &lt; b ∞. Furthermore, let ϕ(l) be a convex function. Jensen's inequality states that E[ϕ(L)] ϕ(E[L]). Liao &amp; Berg (2017) show that the Jensen gap E[ϕ(L)] − ϕ(E[L]</formula><p>) can be bounded from below and above:</p><formula xml:id="formula_14">inf{h(l; µ) | l ∈ (a, b)} var(L) E[ϕ(L)] − ϕ(E[L]) sup{h(l; µ) | l ∈ (a, b)} var(L) h(l; µ) = ϕ(l) − ϕ(µ) (l − µ) 2 − ϕ (µ) l − µ where h(l; µ)</formula><p>does not depend on the distribution of L, only on its expected value µ and on the function ϕ. Substituting L = p(c|x, ω) (a random variable on [0, 1] due to the randomness of the dropout masks) and ϕ(l) = − ln(l), we can see that the gap introduced by Eq. 9 can be made smaller by decreasing the variance of the predictions while maintaining the expected value of L (i.e. the expected probability), assuming that there is a positive lower and upper bound on them (so that the supremum is finite and the infimum is positive, respectively). A similar argument based on C c=1 M α (Eω p(c|x,ω)) = 1 shows that Z approximately monotonically approaches 1 as the variance decreases, so the gap of Eq. 8 can also be reduced.</p><p>Suppose we pick a base model from the power mean family and have a continuum of subvariants with gradually reduced variance in their predictions but the same expectation. Clearly, for each of them we can derive a lower bound the same way as we did for the power mean family. And as we showed above, the lower bounds will tend to increase as the variance of the predictions decreases (see <ref type="figure" target="#fig_1">Fig. 1a</ref>). They do not strictly increase, only tend to, due to how the Jensen gap is bounded from above and below and also due to the O term of Eq. 10. Nonetheless, as we approach determinism the lower bound is forced into increasingly tighter ranges with strictly monotonically increasing bounds around it, thus we can always reduce the variance such that there is no overlap between the ranges and we get a guaranteed improvement on the lower bound. This effect reaches its apex at the deterministic model whose lower bound is both exact and higher than any other model's. <ref type="figure" target="#fig_1">Fig. 1b</ref> illustrates that regardless of the choice of base model, reducing the prediction variance will eventually transform it into the same deterministic model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">THE EXTENDED POWER MEAN FAMILY: CONTROLLING THE TIGHTNESS OF THE BOUND</head><p>Intuitively, in the absence of other sources of stochasticity the dropout rate controls the variance of the predictions and if it is low, the lower bound can be pretty snug. However, there are two problems.</p><p>First, decreasing the dropout rate does not necessarily keep the expectation of the predictions the same. We offer no solution to this bias issue, but refer the reader to previous studies of dropout's approximation properties such as <ref type="bibr" target="#b0">(Baldi &amp; Sadowski 2013</ref>) and our subsequent empirical results.</p><p>Second, reducing the dropout rate would trade off generalisation for tighter bounds. But doing so only at evaluation time leaves the training time regularisation effect intact, and can be seen as picking another model whose lower bound tends to be higher than that of the base model. Having thus extended the dropout family further, we can now tweak both α and dropout rates at evaluation time.   Depending on the severity of the introduced bias compared to the benefits of having a tighter lower bound, the optimal variance may lie anywhere between the deterministic and the base model. We show experimentally that across a number of datasets the benefits of tighter bounds matter more, and observe monotonic improvement in model fit as evaluation time dropout rates are decreased all the way to full determinism. The experiment was conducted as follows. On an already trained model, the dropout rate was multiplied by λ ∈ [0, 1]. As <ref type="table" target="#tab_0">Table 1</ref> shows, the model fit as measured by cross entropy (XE) on the training set improves monotonically when reducing λ. Results on other datasets and with other power mean models are very similar. We call the union of the reduced dropout rate subvariants of all power mean family models the extended dropout family paramaterised by α, λ.</p><p>Therefore, we can say that dropout training optimises a deterministic model subject to regularisation constraints, and deterministic evaluation, widely believed to approximate MC evaluation, is the closest match to the true objective at our disposal. It is not that dropout evaluation has a deterministic approximation: dropout trains a deterministic model first and foremost and a continuum of stochastic ones to various extents.</p><p>In summary, we described dropout training as optimising a common lower bound for a family of models. Since this lower bound is the same for all models in the family, we can nominate any of them at evaluation time. However, the tightness of the bound varies, which affects model fit. Having trained a model with dropout, the best fit is achieved by the deterministic model with no dropout. This result isolates the regularisation effects from the biases of the lower bound and the dropout family.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">APPLYING DROPOUT</head><p>We investigate how members of the extended dropout model family perform in terms of generalisation. We follow the experimental setup of <ref type="bibr" target="#b13">Melis et al. (2017)</ref> and base our work on their best performing model variant for each dataset. Unless explicitly stated, no retraining was performed and their model weights reused. In the experiments with the tuning objective, we follow their experimental setup, using Google Vizier <ref type="bibr" target="#b5">(Golovin et al. 2017</ref>), a black-box hyperparameter tuner based on batched Gaussian Process Bandits.</p><p>See <ref type="table" target="#tab_1">Table 2</ref> for results of image classification on MNIST, character based language modelling on Enwik8, word based language modelling on PTB and Wikitext-2. On MNIST, deterministic dropout is the best in terms of cross entropy, which matches our theoretical predictions. In contrast, on language modelling arithmetic averaging produces the best results, which necessitates further analysis.  We suspected that the particularly severe form of class imbalance exhibited by the power-law word distribution (Zipf 1935) might play a role. To verify this, we contrasted training and validation XEs on PTB for words grouped by frequency (see <ref type="table" target="#tab_2">Table 3</ref>). On the training set, the gap between deterministic dropout and AMC is wider for low frequency words. On the validation set, AMC is worse for frequent words but better for rare words. The ×0.8 dropout multiplier just finds a reasonable compromise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">SOFTMAX TEMPERATURE</head><p>The observed effect is consistent with smoothing, thus we posit that the reason MNIST results are worse with AMC is that the marginal distributions of labels in the training and test set are identical by construction and further smoothing is unnecessary. On the other hand, PTB and Wikitext-2 benefit from AMC's smoothing because the penalty for underestimating low probabilities is harsh, hence the large improvement on rare words. The character based Enwik8 dataset lies somewhere in between: the training and test distributions are better matched and there are no very low probability characters.</p><p>To test the hypothesis that AMC's advantage lies in smoothing, we tested how performing smoothing by other means affects the results. In this experiment, on a trained model the temperature of the final <ref type="table">Table 4</ref>: Validation and test perplexities on PTB and Wikitext-2 with various evaluation strategies and default or optimal validation softmax temperatures. Our baseline results correspond to DET at temperature 1. Note that AMC does not benefit from setting the optimal softmax temperature ("opt"), while DET is improved by it almost to the point of matching AMC which supports the smoothing hypothesis.</p><p>Geometric (α = 0) Power α = 0.5 Arithmetic (α = 1)</p><p>Dataset Temp DET ×0.8 ×0.9 ×1.0 ×0.8 ×0.9 ×1.0 ×0.8 ×0.9 ×1.0 softmax was optimised on the validation set and the model was applied with the optimal temperature to the validation and test sets. Our experimental results in <ref type="table">Table 4</ref> support the hypotheses that AMC smooths the predicted distribution as increasing the temperature improves DET and GMC considerably but not AMC. In fact, the optimal temperature for AMC with λ = 1 was slightly lower than 1, which corresponds to sharpening, not smoothing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Validation</head><p>Tuning the evaluation time softmax temperature is similar to label smoothing <ref type="bibr" target="#b17">(Pereyra et al. 2017</ref>), the main difference being that our method does not affect training. While this is convenient, for tuning model hyperparameters, ideally we would determine the optimal evaluation parameters α, λ and the temperature for the calculation of the validation score for each set of hyperparameters tried, but this would be prohibitively expensive. Since deterministic evaluation coupled with the optimal temperature is very close to the best performing AMC model, it serves as a good proxy for the ideal tuning objective. The optimal temperature can be approximately determinined using a linear search on a subset of the validation data which is orders of magnitude faster than MC dropout. In our experiments, hyperparameter tuning with validation scores computed at the optimal softmax temperature did improve results, albeit very slightly (about half a perplexity point). Thus we can conclude that deterministic dropout is already a reasonable proxy for which to optimise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">RESULTS</head><p>We have improved the best test result of <ref type="bibr" target="#b13">Melis et al. (2017)</ref> from 58.3 to 55.7 on PTB, and from 65.9 to 63.7 on Wikitext-2 using their model weights, only tuning the evaluation parameters α, λ and the softmax temperature on the validation set. By retuning the hyperparameters of the PTB model with optimal temperature deterministic evaluation, we improved to 55.3 on PTB. For lack of resources, we did not retune for Wikitext-2. For comparison, the state of the art in language modelling without resorting to dynamic evaluation or a continuous cache pointer is Mixture of Softmaxes <ref type="bibr" target="#b24">(Yang et al. 2017</ref>) with 54.44 and 61.45 on PTB and Wikitext-2, respectively. At present, it is unclear whether the benefits of their approach and ours combine.</p><p>In summary, we looked at how different models and evaluation methods rank in terms of generalisation. Across a number of tasks and datasets the ranking differed from what was observed on the training set. We found that AMC smooths the distribution of the prediction probabilities and we achieved a similar effect without resorting to expensive sampling simply by adjusting the temperature of the final softmax. Finally, we brought the tuning objective more in line with the improved evaluation by automatically determining the optimal softmax temperature when evaluating on the validation set which further improved results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">IMPLICATIONS</head><p>The construction of a conditional model family with a common lower bound on their objectives is applicable to other latent variable models with similar structure and inference method. This lower bound admits ambiguity as to what model is being fit to the data, which in turn allows for picking any such model at evaluation time. However, the tightness of the bound and the quality of the fit varies. For dropout, the deterministic model has the best fit even though the training objective is highly stochastic, but this result hinges on the approximation properties of deterministic dropout and will not carry over to other probabilistic models in general. In particular, standard VAEs (Kingma &amp; Welling 2013) with their lower bound being very similar in construction to Eq. 1 cannot quite collapse to a deterministic model else they suffer an infinite KL penalty. Still, the lower bound being looser on the tails of q is related to problem of underestimating posterior uncertainty <ref type="bibr" target="#b20">(Turner &amp; Sahani 2011)</ref>.</p><p>In related works, expectation-linear dropout <ref type="bibr" target="#b11">(Ma et al. 2016</ref>) and fraternal dropout <ref type="bibr" target="#b27">(Zolna et al. 2017</ref>) both try to reduce the "inference gap": the mismatch between the training objective and deterministic evaluation. The gains reported in those works might be explained by reducing the bias of deterministic evaluation and also by encouraging small variance in the predictions and thus getting tighter bounds. Another recent work, activation regularisation <ref type="bibr" target="#b14">(Merity et al. 2017)</ref>, could be thought of as a mechanism to reduce the variance of predictions to a similar effect. In the context of language modelling, the connection between noise and smoothing was established by <ref type="bibr" target="#b23">Xie et al. (2017)</ref>. Our improved understanding further emphasises that connection, and at the same time challenges the way we think about dropout.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A VARIATIONAL DROPOUT WITH NON-SHARED MASKS</head><p>If q and p are redefined for the non-shared setting to be products of identical and independent, per time step factors, neither term of the variational objective requires rethinking: the MC approximation still works since q is easy to sample from, while the KL term becomes a sum of componentwise KL divergences and can still be implemented as weight decay. Consequently, both shared and non-shared masks fit into the variational framework. For a detailed derivation see Appendix B.</p><p>In related works, <ref type="bibr" target="#b16">Pachitariu &amp; Sahani (2013)</ref> in their investigation of regularisation of standard RNN based language models dismiss applying dropout to recurrent connections "to avoid introducing instabilities into the recurrent part of the LMs". <ref type="bibr" target="#b1">Bayer et al. (2013)</ref> echo this claim about RNNs, which is then cited by <ref type="bibr" target="#b25">Zaremba et al. (2014)</ref>, but their work is based on LSTMs not standard RNNs. Finally, <ref type="bibr" target="#b3">Gal &amp; Ghahramani (2016b)</ref> cite all of the above but also work with LSTMs. Their results indicate a large, about 15 perplexity point advantage to shared mask dropout for language modelling on the Penn Treebank (PTB) corpus (see <ref type="figure">Fig. 2</ref> in their paper).</p><p>Our experimental results obtained with careful and extensive hyperparameter tuning, listed in <ref type="table" target="#tab_4">Table 5</ref>, indicate only a small difference between the two which is in agreement with the empirical study of <ref type="bibr" target="#b18">Semeniuta et al. (2016)</ref>. In any case, non-shared masks, in addition to being variational, are also surprisingly competitive with shared masks for LSTMs (we make no claims about standard RNNs). We also tested whether embedding dropout (in which dropout is applied to entire vectors in the input embedding lookup table) proposed by <ref type="bibr" target="#b3">Gal &amp; Ghahramani (2016b)</ref> improves results, and find that embedding dropout does not offer any improvement on top of input dropout.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX B DERIVATION OF VARIATIONAL DROPOUT WITH NON-SHARED</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MASKS</head><p>In this section, we formulate naive (i.e. non-shared mask) dropout in the variational setting. In contrast to the shared mask case, where ω was a single set of weights, here ω 1:T (or ω, for short) has a set of weights for each time step that differ in their dropout masks. The variational posterior q(ω) and the prior p(ω) are both products of identical distributions over time:</p><formula xml:id="formula_15">q(ω 1:T ) = T t=1 q (ω t ) = T t=1 pN (ω t |0, σ 2 ) + (1 − p)N (ω t |Θ, σ 2 ) p(ω 1:T ) = T t=1 p (ω t ) = T t=1 N (ω t |0, σ 2 p )</formula><p>An unbiased approximation to the integrals in Eq. 1 is based on a single, easy to obtain samplê ω ∼ q(ω):</p><formula xml:id="formula_16">q(ω) ln p(y|x, ω)dω ≈ ln p(y|x,ω)</formula><p>Showing that the KL term can still be approximated with weight decay with non-shared masks is not much more involved. Both distributions are products of densities over independent random variables, so the componentwise KL divergencies sum. In particular:</p><formula xml:id="formula_17">KL(q(ω)||p(ω)) = T i=1 q (ω i ) ln T t=1 q (ω t ) T t=1 p (ω t ) dω = T i=1 q (ω i ) T t=1 ln q (ω t ) p (ω t ) dω = T t=1 T i=1 q (ω i ) ln q (ω t ) p (ω t ) dω = T t=1 T i=1,i =t q (ω i ) q (ω t ) ln q (ω t ) p (ω t ) dω = T t=1 T i=1,i =t q (ω i )dω \t q (ω) ln q (ω) p (ω) dω = T · KL(q (ω)||p (ω))</formula><p>We partitioned the variables into two mutually exclusive sets w t and its complement w \t , and split the multiple integral using Fubini's theorem (or, equivalently, using the expectation of independent random variables rule). After the split, the first integral is trivially 1 and the second has no dependence on T .</p><p>What we end up with is a sum of identical KL terms of the same distributions as in the shared mask case, so the full KL can be approximated with weight decay.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX C DERIVATION OF THE MAP LOWER BOUND FOR THE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ARITHMETIC MODEL</head><p>We can rewrite the posterior as: </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Lower and upper bounds on the lower bound of the model objective as a function of prediction variance for any model in the power mean family. With reduced variance, all members of the power mean family (brown shading) converge to the deterministic model while their lower bounds tighten.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Tightness of lower bounds vs evaluation time prediction variance in the extended dropout family.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>p(Θ|X, Y ) = p(X, Y |Θ)p(Θ) p(X, Y ) ∝ p(X, Y |Θ)p(Θ) = p(Y |X, ω, Θ)p(ω|Θ, X)p(Θ|X)p(X)dω ∝ p(Y |X, ω)p(ω|Θ)p(Θ)dωMoving to the log domain and using Jensen's inequality allows us to construct a lower bound that is a sum of per data point terms (i.e. something that can be conveniently optimised):ln p(Θ|X, Y ) = ln p(Y |X, ω)p(ω|Θ)p(Θ)dω − C MAP = ln p(ω|Θ) N i=1 p(y i |x i , ω)dω + ln p(Θ) − C MAP p(ω|Θ) ln N i=1 p(y i |x i , ω)dω + ln p(Θ) − C MAP = N i=1 p(ω|Θ) ln p(y i |x i , ω)dω + ln p(Θ) − C MAPAPPENDIX D DERIVATION OF THE MAP LOWER BOUND FOR THE GEOMETRICMODELFrom Eq. 6 recall that:p(y|x, Θ) = exp Eω ∼p(ω|Θ) ln p(y|x,ω) Z(x, Θ)The normalisation constant Z is at most 1, due to the geometric mean being bounded from above by the arithmetic mean on a per class c basis:Since this a conditional model, we can rewrite the posterior as:p(Θ|X, Y ) = p(X, Y |Θ)p(Θ) p(X, Y ) ∝ p(X, Y |Θ)p(Θ) = p(Y |X, Θ)p(X|Θ)p(Θ) ∝ p(Y |X, Θ)p(Θ) p(X|Θ)is dropped in the last step as it is constant. Moving to the log domain once again:ln p(Θ|X, Y ) = ln p(Y |X, Θ) + ln p(Θ) − C M AP = ln N i=1 p(y i |x i , Θ) + ln p(Θ) − C M AP = N i=1 Ê ω∼p(ω|Θ) ln p(y i |x i ,ω) − ln(Z(x i , Θ)) + ln p(Θ) − C M AP N i=1 Ê ω∼p(ω|Θ) ln p(y i |x i ,ω) + ln p(Θ) − C M AP = N i=1 p(ω|Θ) ln p(y i |x i , ω)dω + ln p(Θ) − C M APwhere the lower bound arises due to ∀i : Z(x i , Θ) 1.APPENDIX E DERIVATION OF THE MAP LOWER BOUND FOR THE POWERMEAN FAMILYIn §3.2 we proved that ∀i : Z(x i , Θ) 1. Starting from p(Θ|X, Y ) ∝ p(Y |X, Θ)p(Θ) just like in the geometric case, we derive a lower bound in the log domain:ln p(Θ|X, Y ) = ln p(Y |X, Θ) + ln p(Θ) − C MAP = ln N i=1 p(y i |x i , Θ) + ln p(Θ) − C MAP = N i=1 ln α Ê ω∼p(ω|Θ) p(y i |x i ,ω) α − ln(Z(x i , Θ)) + ln p(Θ) − C MAP i |x i ,ω) α + ln p(Θ) − C MAP i |x i ,ω) α + ln p(Θ) − C MAP y i |x i ,ω) α + ln p(Θ) − C MAP y i |x i ,ω) + ln p(Θ) − C MAP = N i=1 p(ω|Θ) ln p(y i |x i , ω)dω + ln p(Θ) − C MAP</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>PTB training XEs with various dropout rate multipliers between deterministic and GMC. Observe the monotonic improvement in training fit when reducing the dropout rate at evaluation only.</figDesc><table><row><cell>×0.0</cell><cell>×0.1</cell><cell>×0.2</cell><cell>×0.3</cell><cell>×0.4</cell><cell>×0.5</cell><cell>×0.6</cell><cell>×0.7</cell><cell>×0.8</cell><cell>×0.9</cell><cell>×1.0</cell></row><row><cell cols="11">2.731 2.738 2.746 2.755 2.766 2.777 2.791 2.807 2.826 2.849 2.878</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Validation XEs on some datasets varying the power α and the dropout rate mulitiplier λ. Deterministic dropout is not the best evaluation method for the language modelling datasets due to a simple smoothing effect.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Geometric (α = 0)</cell><cell cols="3">Power α = 0.5</cell><cell cols="3">Arithmetic (α = 1)</cell></row><row><cell>Dataset</cell><cell>DET</cell><cell>×0.8</cell><cell>×0.9</cell><cell>×1.0</cell><cell>×0.8</cell><cell>×0.9</cell><cell>×1.0</cell><cell>×0.8</cell><cell>×0.9</cell><cell>×1.0</cell></row><row><cell>MNIST</cell><cell cols="4">0.070 0.087 0.087 0.088</cell><cell>0.92</cell><cell>0.93</cell><cell cols="4">0.93 0.100 0.100 0.100</cell></row><row><cell>Enwik8</cell><cell cols="10">0.886 0.879 0.878 0.881 0.877 0.877 0.877 0.875 0.875 0.875</cell></row><row><cell>PTB</cell><cell cols="10">4.110 4.090 4.090 4.093 4.072 4.070 4.073 4.061 4.064 4.080</cell></row><row><cell cols="11">Wikitext-2 4.236 4.229 4.231 4.235 4.025 4.026 4.208 4.203 4.212 4.228</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>PTB training and validation XEs for AMC at λ ∈ {0, 0.8, 1} per word frequency. Note how DET dominates AMC on the training set, but AMC is better for rare words in the validation set.</figDesc><table><row><cell></cell><cell>num</cell><cell></cell><cell>training</cell><cell></cell><cell></cell><cell>validation</cell><cell></cell></row><row><cell cols="8">frequency targets DET ×0.8 AMC DET ×0.8 AMC</cell></row><row><cell>25000&lt;</cell><cell>13580</cell><cell>1.40</cell><cell>1.50</cell><cell>1.56</cell><cell>1.58</cell><cell>1.64</cell><cell>1.68</cell></row><row><cell>5000&lt;</cell><cell>26658</cell><cell>1.65</cell><cell>1.75</cell><cell>1.81</cell><cell>1.93</cell><cell>1.98</cell><cell>2.02</cell></row><row><cell>500&lt;</cell><cell>44702</cell><cell>2.19</cell><cell>2.30</cell><cell>2.36</cell><cell>2.58</cell><cell>2.63</cell><cell>2.66</cell></row><row><cell>&lt;500</cell><cell>29058</cell><cell>4.07</cell><cell>4.19</cell><cell>4.29</cell><cell>6.49</cell><cell>6.39</cell><cell>6.39</cell></row><row><cell>&lt;100</cell><cell>14222</cell><cell>4.24</cell><cell>4.38</cell><cell>4.49</cell><cell>7.81</cell><cell>7.64</cell><cell>7.61</cell></row><row><cell>&lt;20</cell><cell>5008</cell><cell>4.00</cell><cell>4.19</cell><cell>4.33</cell><cell>9.20</cell><cell>9.01</cell><cell>8.97</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Validation and test set perplexities on PTB with shared (S) or non-shared (NS) dropout masks for a small, 1 layer and a large, 4 layer LSTM with 10 and 24 million weights, respectively. Non-shared masks perform nearly as well as shared masks and as we have seen neither is "more variational" than the other.</figDesc><table><row><cell>dataset</cell><cell></cell><cell>10M</cell><cell>24M</cell></row><row><cell></cell><cell>S</cell><cell>NS</cell><cell>S</cell><cell>NS</cell></row><row><cell cols="5">validation 59.4 60.2 57.5 58.3</cell></row><row><cell>test</cell><cell cols="4">57.5 58.6 56.0 56.9</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We would like to thank Laura Rimell, Aida Nematzadeh, and Andriy Mnih for their valuable feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Understanding dropout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Baldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sadowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2814" to="2822" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Bayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Osendorfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniela</forename><surname>Korhammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nutan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Urban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smagt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1311.0701</idno>
		<title level="m">On fast dropout and its applicability to recurrent networks</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Dropout as a bayesian approximation: Representing model uncertainty in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">international conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1050" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A theoretically grounded application of dropout in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1019" to="1027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Concrete dropout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Hron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3584" to="3593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Google vizier: A service for black-box optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Golovin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Solnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhodeep</forename><surname>Moitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Kochanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Karro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sculley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1487" to="1495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Practical variational inference for neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2348" to="2356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Geoffrey E Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan R</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Variational dropout and the local reparameterization trick</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2575" to="2583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Sharpening jensen&apos;s inequality. The American Statistician</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Berg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Dropout with expectation-linear regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingkai</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoliang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
		<idno>abs/1609.08017</idno>
		<ptr target="http://arxiv.org/abs/1609.08017" />
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Filtering variational objectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Lawson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaud</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6573" to="6583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">On the state of the art of evaluation in neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gábor</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.05589</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Regularizing and optimizing lstm language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02182</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Risk versus uncertainty in deep learning: Bayes, bootstrap and the dangers of dropout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Osband</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Bayesian Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Regularization and nonlinearities for neural language models: when are they needed?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Pachitariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maneesh</forename><surname>Sahani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.5650</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Regularizing neural networks by penalizing confident output distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Pereyra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06548</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislau</forename><surname>Semeniuta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erhardt</forename><surname>Barth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.05118</idno>
		<title level="m">Recurrent dropout without memory loss</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Two problems with variational expectation maximisation for time-series models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maneesh</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sahani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bayesian Time series models</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3.1</biblScope>
			<biblScope unit="page" from="3" to="4" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fast dropout training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">international conference on machine learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="118" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">An empirical analysis of dropout in piecewise linear networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6197</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aiming</forename><surname>Lévy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.02573</idno>
		<title level="m">Data noising as smoothing in neural network language models</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.03953</idno>
		<title level="m">Breaking the softmax bottleneck: a high-rank rnn language model</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.2329</idno>
		<title level="m">Ilya Sutskever, and Oriol Vinyals. Recurrent neural network regularization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">The psycho-biology of language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Kingsley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zipf</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1935" />
			<pubPlace>Houghton, Mifflin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Zolna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devansh</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dendi</forename><surname>Suhubdy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00066</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Fraternal dropout. arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
