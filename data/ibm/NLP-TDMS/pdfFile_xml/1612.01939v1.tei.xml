<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Correlation Alignment for Unsupervised Domain Adaptation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2016-12-06">6 Dec 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochen</forename><surname>Sun</surname></persName>
							<email>baochens@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
							<email>saenko@bu.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochen</forename><forename type="middle">Sun</forename><surname>Microsoft</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochen</forename><surname>Sun</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Boston University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Introduction</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Correlation Alignment for Unsupervised Domain Adaptation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2016-12-06">6 Dec 2016</date>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this chapter, we present CORrelation ALignment (CORAL), a simple yet effective method for unsupervised domain adaptation. CORAL minimizes domain shift by aligning the second-order statistics of source and target distributions, without requiring any target labels. In contrast to subspace manifold methods, it aligns the original feature distributions of the source and target domains, rather than the bases of lower-dimensional subspaces. It is also much simpler than other distribution matching methods. CORAL performs remarkably well in extensive evaluations on standard benchmark datasets. We first describe a solution that applies a linear transformation to source features to align them with target features before classifier training. For linear classifiers, we propose to equivalently apply CORAL to the classifier weights, leading to added efficiency when the number of classifiers is small but the number and dimensionality of target examples are very high. The resulting CORAL Linear Discriminant Analysis (CORAL-LDA) outperforms LDA by a large margin on standard domain adaptation benchmarks. Finally, we extend CORAL to learn a nonlinear transformation that aligns correlations of layer activations in deep neural networks (DNNs). The resulting Deep CORAL approach works seamlessly with DNNs and achieves state-of-theart performance on standard benchmark datasets. Our code is available at: https: //github.com/VisionLearningGroup/CORAL</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Machine learning is very different from human learning. Humans are able to learn from very few labeled examples and apply the learned knowledge to new examples in novel conditions. In contrast, traditional machine learning algorithms assume that the training and test data are independent and identically distributed (i.i.d.) and supervised machine learning methods only perform well when the given extensive labeled data are from the same distribution as the test distribution. However, this assumption rarely holds in practice, as the data are likely to change over time and space. To compensate for the degradation in performance due to domain shift, domain adaptation <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b1">2]</ref> tries to transfer knowledge from source (training) domain to target (test) domain. Our approach is in line with most existing unsupervised domain adaptation approaches in that we first transform the source data to be as close to the target data as possible. Then a classifier trained on the transformed source domain is applied to the target data.</p><p>In this chapter, we mainly focus on the unsupervised scenario as we believe that leveraging the unlabeled target data is key to the success of domain adaptation. In real world applications, unlabeled target data are often much more abundant and easier to obtain. On the other side, labeled examples are very limited and require human annotation. So the question of how to utilize the unlabeled target data is more important for practical visual domain adaptation. For example, it would be more convenient and applicable to have a pedestrian detector automatically adapt to the changing visual appearance of pedestrians rather than having a human annotator label every frame that has a different visual appearance.</p><p>Our goal is to propose a simple yet effective approach for domain adaptation that researchers with little knowledge of domain adaptation or machine learning could easily integrate into their application. In this chapter, we describe a "frustratingly easy" (to borrow a phrase from <ref type="bibr" target="#b1">[2]</ref>) unsupervised domain adaptation method called CORrelation ALignment or CORAL <ref type="bibr" target="#b24">[25]</ref> in short. CORAL aligns the input feature distributions of the source and target domains by minimizing the difference between their second-order statistics. The intuition is that we want to capture the structure of the domain using feature correlations. As an example, imagine a target domain of Western movies where most people wear hats; then the "head" feature may be positively correlated with the "hat" feature. Our goal is to transfer such correlations to the source domain by transforming the source features.</p><p>In Section 2, we describe a linear solution to CORAL, where the distributions are aligned by re-coloring whitened source features with the covariance of the target data distribution. This solution is simple yet efficient, as the only computations it needs are (1) computing covariance statistics in each domain and (2) applying the whitening and re-coloring linear transformation to the source features. Then, supervised learning proceeds as usual-training a classifier on the transformed source features.</p><p>For linear classifiers, we can equivalently apply the CORAL transformation to the classifier weights, leading to better efficiency when the number of classifiers is small but the number and dimensionality of the target examples are very high. We present the resulting CORAL-Linear Discriminant Analysis (CORAL-LDA) <ref type="bibr" target="#b26">[27]</ref> in Section 3, and show that it outperforms standard Linear Discriminant Analysis (LDA) by a large margin on cross domain applications. We also extend CORAL to work seamlessly with deep neural networks by designing a layer that consists of a differentiable CORAL loss <ref type="bibr" target="#b28">[29]</ref>, detailed in Section 4. On the contrary to the linear CORAL, Deep CORAL learns a nonlinear transformation and also provides endto-end adaptation. Section 5 describes extensive quantitative experiments on several benchmarks, and Section 6 concludes the chapter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Linear Correlation Alignment</head><p>In this section, we present CORrelation ALignment (CORAL) for unsupervised domain adaptation and derive a linear solution. We first describe the formulation and derivation, followed by the main linear CORAL algorithm and its relationship to existing approaches. In this section and Section 3, we constrain the transformation to be linear. Section 4 extends CORAL to learn a nonlinear transformation that aligns correlations of layer activations in deep neural networks (Deep CORAL).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Formulation and Derivation</head><p>We describe our method by taking a multi-class classification problem as the running example. Suppose we are given source-domain training examples D S = {x i }, x ∈ R d with labels L S = {y i }, y ∈ {1, ..., L}, and target data D T = {u i }, u ∈ R d . Here both x and u are the d-dimensional feature representations φ (I) of input I. Suppose µ s , µ t and C S ,C T are the feature vector means and covariance matrices. Assuming that all features are normalized to have zero mean and unit variance, µ t = µ s = 0 after the normalization step, while C S = C T .</p><p>To minimize the distance between the second-order statistics (covariance) of the source and target features, we apply a linear transformation A to the original source features and use the Frobenius norm as the matrix distance metric:</p><formula xml:id="formula_0">min A CŜ −C T 2 F = min A A C S A −C T 2 F (1)</formula><p>where CŜ is covariance of the transformed source features D s A and · 2 F denotes the squared matrix Frobenius norm.</p><p>If rank(C S ) ≥ rank(C T ), then an analytical solution can be obtained by choosing A such that CŜ = C T . However, the data typically lie on a lower dimensional manifold <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b7">8]</ref>, and so the covariance matrices are likely to be low rank <ref type="bibr" target="#b13">[14]</ref>. We derive a solution for this general case, using the following lemma. Lemma 1. Let Y be a real matrix of rank r Y and X a real matrix of rank at most r, where r r</p><formula xml:id="formula_1">Y ; let Y = U Y Σ Y V Y be the SVD of Y , and Σ Y [1:r] , U Y [1:r] , V Y [1:</formula><p>r] be the largest r singular values and the corresponding left and right singular vectors of Y respectively. Then,</p><formula xml:id="formula_2">X * = U Y [1:r] Σ Y [1:r] V Y [1:r] is the optimal solution to the problem of min X X −Y 2 F . [1]</formula><p>Theorem 1. Let Σ + be the Moore-Penrose pseudoinverse of Σ , r C S and r C T denote the rank of C S and C T respectively. Then, A * = U S Σ +</p><formula xml:id="formula_3">S 1 2 U S U T [1:r] Σ T [1:r] 1 2 U T [1:r]</formula><p>is the optimal solution to the problem in Equation (12) with r = min(r C S , r C T ).</p><p>Proof. Since A is a linear transformation, A C S A does not increase the rank of C S . Thus, r CŜ r C S . Since C S and C T are symmetric matrices, conducting SVD on C S and C T gives C S = U S Σ S U S and C T = U T Σ T U T respectively. We first find the optimal value of CŜ through considering the following two cases: <ref type="bibr">:r]</ref> is the optimal solution to Equation <ref type="formula" target="#formula_18">(12)</ref> where r = r C T . Case 2. r C S r C T . Then, according to Lemma 1, CŜ = U T [1:r] Σ T [1:r] U T [1:r] is the optimal solution to Equation <ref type="formula" target="#formula_18">(12)</ref> where r = r C S .</p><formula xml:id="formula_4">Case 1. r C S &gt; r C T . The optimal solution is CŜ = C T . Thus, CŜ = U T Σ T U T = U T [1:r] Σ T [1:r] U T [1</formula><p>Combining the results in the above two cases yields that CŜ = U T [1:r] Σ T [1:r] U T <ref type="bibr">[1:r]</ref> is the optimal solution to Equation (12) with r = min(r C S , r C T ). We then proceed to solve for A based on the above result. Letting CŜ = A C S A, we can write</p><formula xml:id="formula_5">A C S A = U T [1:r] Σ T [1:r] U T [1:r] .</formula><p>Since C S = U S Σ S U S , we have</p><formula xml:id="formula_6">A U S Σ S U S A = U T [1:r] Σ T [1:r] U T [1:r] .</formula><p>This gives:</p><formula xml:id="formula_7">(U S A) Σ S (U S A) = U T [1:r] Σ T [1:r] U T [1:r] . Let E = Σ + S 1 2 U S U T [1:r] Σ T [1:r] 1 2 U T [1:r]</formula><p>, then the right hand side of the above equation can be re-written as E Σ S E. This gives</p><formula xml:id="formula_8">(U S A) Σ S (U S A) = E Σ S E</formula><p>By setting U S A to E, we obtain the optimal solution of A as   In practice, for the sake of efficiency and stability, we can avoid the expensive SVD steps and perform traditional data whitening and coloring. Traditional whitening adds a small regularization parameter λ to the diagonal elements of the covariance matrix to explicitly make it full rank and then multiplies the original features by its inverse square root (or square root for coloring.) This is advantageous because: (1) it is faster 1 and more stable, as SVD on the original covariance matrices 1 the entire CORAL transformation takes less than one minute on a regular laptop for dimensions as large as D S ∈ R 795×4096 and D T ∈ R 2817×4096 might not be stable and might be slow to converge; (2) as illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>, the performance is similar to the analytical solution in Equation (2) and very stable with respect to λ . In the experiments provided at the end of this chapter we set λ to 1. The final algorithm can be written in four lines of MATLAB code as illustrated in Algorithm 1.  T % re-coloring with target covariance One might instead attempt to align the distributions by whitening both source and target. As shown in <ref type="figure" target="#fig_1">Figure 1</ref>(d), this will fail as the source and target data are likely to lie on different subspaces due to domain shift. An alternative approach would be whitening the target and then re-coloring it with the source covariance. However, as demonstrated in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b7">8]</ref> and our experiments, transforming data from source to target space gives better performance. This might be due to the fact that by transforming the source to target space the classifier is trained using both the label information from the source and the unlabelled structure from the target.</p><formula xml:id="formula_9">A * = U S E = (U S Σ + S 1 2 U S )(U T [1:r] Σ T [1:r]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Algorithm</head><p>After CORAL transforms the source features to the target space, a classifier f w parametrized by w can be trained on the adjusted source features and directly applied to target features. For a linear classifier f w (I) = w T φ (I), we can apply an equivalent transformation to the parameter vector w (e.g., f w (I) = (w T A)φ (I)) instead of the features (e.g., f w (I) = w T (Aφ (I))). This results in added efficiency when the number of classifiers is small but the number and dimensionality of target examples is very high. For linear SVM, this extension is straightforward. In Section 3, we apply the idea of CORAL to another commonly used linear classifier-Linear Discriminant Analysis (LDA). LDA is special in the sense that its weights also use the covariance of the data. It is also extremely efficient for training a large number of classifiers <ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Relationship to Existing Methods</head><p>It has long been known that input feature normalization improves many machine learning methods, e.g., <ref type="bibr" target="#b14">[15]</ref>. However, CORAL does not simply perform feature normalization, but rather aligns two different distributions. Batch Normalization <ref type="bibr" target="#b14">[15]</ref> tries to compensate for internal covariate shift by normalizing each mini-batch to be zero-mean and unit-variance. However, as illustrated in <ref type="figure" target="#fig_1">Figure 1</ref>(a), such normalization might not be enough. Even if used with full whitening, Batch Normalization may not compensate for external covariate shift: the layer activations will be decorrelated for a source point but not for a target point. What's more, as mentioned in Section 2.2, whitening both domains is not a successful strategy.</p><p>Recent state-of-the-art unsupervised approaches project the source and target distributions into a lower-dimensional manifold and find a transformation that brings the subspaces closer together <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b12">13]</ref>. CORAL avoids subspace projection, which can be costly and requires selecting the hyper-parameter that controls the dimensionality of the subspace, k. We note that subspace-mapping approaches <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b7">8]</ref> only align the top k eigenvectors of the source and target covariance matrices. On the contrary, CORAL aligns the covariance matrices, which can only be re-constructed using all eigenvectors and eigenvalues. Even though the eigenvectors can be aligned well, the distributions can still differ a lot due to the difference of eigenvalues between the corresponding eigenvectors of the source and target data. CORAL is a more general and much simpler method than the above two as it takes into account both eigenvectors and eigenvalues of the covariance matrix without the burden of subspace dimensionality selection.</p><p>Maximum Mean Discrepancy (MMD) based methods (e.g., TCA <ref type="bibr" target="#b21">[22]</ref>, DAN <ref type="bibr" target="#b18">[19]</ref>) for domain adaptation can be interpreted as "moment matching" and can express arbitrary statistics of the data. Minimizing MMD with a polynomial kernel (k(x, y) = (1 + x y) q with q = 2) is similar to the CORAL objective, however, no previous work has used this kernel for domain adaptation nor proposed a closed form solution to the best of our knowledge. The other difference is that MMD based approaches usually apply the same transformation to both the source and target domain. As demonstrated in <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b7">8]</ref>, asymmetric transformations are more flexible and often yield better performance for domain adaptation tasks. Intuitively, symmetric transformations find a space that "ignores" the differences between the source and target domain while asymmetric transformations try to "bridge" the two domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">CORAL Linear Discriminant Analysis</head><p>In this section, we introduce how CORAL can be applied for aligning multiple linear classifiers. In particular, we take LDA as the example for illustration, considering LDA is a commonly used and effective linear classifier. Combining CORAL and LDA gives a new efficient adpative learning approach CORAL-LDA. We use the task of object detection as a running example to explain CORAL-LDA.</p><p>We begin by describing the decorrelation-based approach to detection proposed in <ref type="bibr" target="#b13">[14]</ref>. Given an image I, it follows the sliding-window paradigm, extracting a d-dimensional feature vector φ (I, b) at each window b across all locations and at multiple scales. It then scores the windows using a scoring function</p><formula xml:id="formula_10">f w (I, b) = w φ (I, b).<label>(3)</label></formula><p>In practice, all windows with values of f w above a predetermined threshold are considered positive detections. In recent years, use of the linear SVM as the scoring function f w , usually with Histogram of Gradients (HOG) as the features φ , has emerged as the predominant object detection paradigm. Yet, as observed by Hariharan et al. <ref type="bibr" target="#b13">[14]</ref>, training SVMs can be expensive, especially because it usually involves costly rounds of hard negative mining. Furthermore, the training must be repeated for each object category, which makes it scale poorly with the number of categories.</p><p>Hariharan et al. <ref type="bibr" target="#b13">[14]</ref> proposed a much more efficient alternative, learning f w with Linear Discriminant Analysis (LDA). LDA is a well-known linear classifier that models the training set of examples x with labels y ∈ {0, 1} as being generated by p(x, y) = p(x|y)p(y). p(y) is the prior on class labels and the class-conditional densities are normal distributions</p><formula xml:id="formula_11">p(x|y) = N (x; µ y , C S ),<label>(4)</label></formula><p>where the feature vector covariance C S is assumed to be the same for both positive and negative (background) classes. In our case, the feature is represented by x = φ (I, b). The resulting classifier is given by</p><formula xml:id="formula_12">w = C S −1 (µ 1 − µ 0 )<label>(5)</label></formula><p>The innovation in <ref type="bibr" target="#b13">[14]</ref> was to re-use C S and µ 0 , the background mean, for all categories, reducing the task of learning a new category model to computing the average positive feature, µ 1 . This was accomplished by calculating C S and µ 0 for the largest possible window and subsampling to estimate all other smaller window sizes. Also, C S was shown to have a sparse local structure, with correlation falling off sharply beyond a few nearby image locations. Like other classifiers, LDA learns to suppress non-discriminative structures and enhance the contours of the object. However it does so by learning the global covariance statistics once for all natural images, and then using the inverse covariance matrix to remove the non-discriminative correlations, and the negative mean to re- <ref type="figure">Fig. 3</ref> (a) Applying a linear classifier w learned by LDA to source data x is equivalent to (b) applying classifierŵ = C S −1/2 w to decorrelated points C S −1/2 x. (c) However, target points u may still be correlated after C S −1/2 u, hurting performance. (d) Our method uses target-specific covariance to obtain properly decorrelatedû. move the average feature. LDA was shown in <ref type="bibr" target="#b13">[14]</ref> to have competitive performance to SVM, and can be implemented both as an exemplar-based <ref type="bibr" target="#b20">[21]</ref> or as deformable parts model (DPM) <ref type="bibr" target="#b6">[7]</ref>.</p><p>We observe that estimating global statistics C S and µ 0 once and re-using them for all tasks may work when training and testing in the same domain, but in our case, the source training data is likely to have different statistics from the target data. <ref type="figure" target="#fig_5">Figure 4</ref> illustrates the effect of centering and decorrelating a positive mean using global statistics from the wrong domain. The effect is clear: important discriminative information is removed while irrelevant structures are not.</p><p>Based on this observation, we propose an adaptive decorrelation approach to detection. Assume that we are given labeled training data {x, y} in the source domain (e.g., virtual images rendered from 3D models), and unlabeled examples u in the target domain (e.g., real images collected in an office environment). Evaluating the scoring function f w (x) in the source domain is equivalent to first de-correlating the training featuresx = C S −1/2 x, computing their positive and negative class meanŝ µ 1 = C S −1/2 µ 1 andμ 0 = C S −1/2 µ 0 and then projecting the decorrelated feature onto the decorrelated difference between means, f w (x) =ŵ x, whereŵ = (μ 1 −μ 0 ). This is illustrated in <ref type="figure">Figure 3(a-b)</ref>.</p><p>However, as we saw in <ref type="figure" target="#fig_5">Figure 4</ref>, the assumption that the input is properly decorrelated does not hold if the input comes from a target domain with a different covariance structure. <ref type="figure">Figure 3</ref>(c) illustrates this case, showing that C S −1/2 u does not have isotropic covariance. Therefore, w cannot be used directly. We may be able to compute the covariance of the target domain on the unlabeled target points u, but not the positive class mean. Therefore, we would like to re-use the decorrelated mean differenceŵ, but adapt to the covariance of the target domain. In the rest of the chapter, we make the assumption that the difference between positive and negative means is the same in the source and target. This may or may not hold in practice, and we discuss this further in Section 5.</p><p>Let the estimated target covariance be C T . We first decorrelate the target input feature with its inverse square root, and then applyŵ directly, as shown in <ref type="figure">Figure 3(d)</ref>. The resulting scoring function is:</p><formula xml:id="formula_13">fŵ(u) =ŵ û = (C S −1/2 (µ 1 − µ 0 )) (C T −1/2 u) = ((C T −1/2 ) C S −1/2 (µ 1 − µ 0 )) u<label>(6)</label></formula><p>This corresponds to a transformation (C T −1/2 ) (C S −1/2 ) instead of the original whitening C S −1 being applied to the difference between means to compute w. Note that if source and target domains are the same, then (C T −1/2 ) (C S −1/2 ) equals to C S −1 since both C T and C S are symmetric. In this case, Equation 6 ends up the same as Equation <ref type="bibr" target="#b4">5</ref>.</p><p>In practice, either the source or the target component of the above transformation may also work, or even statistics from similar domains. However, as we will see in Section 5.2, dissimilar domain statistics can significantly hurt performance. Furthermore, if either source or target has only images of the positive category available, and cannot be used to properly compute background statistics, the other domain can still be used.</p><p>CORAL-LDA works in a purely unsupervised way. Here, we extend it to semisupervised adaptation when a few labeled examples are available in the target domain. Following <ref type="bibr" target="#b9">[10]</ref>, a simple adaptation method is used whereby the template learned on source positives is combined with a template learned on target positives, using a weighted combination. The key difference with our approach is that the target template uses target-specific statistics.</p><p>In <ref type="bibr" target="#b9">[10]</ref>, the author uses the same background statistics as <ref type="bibr" target="#b13">[14]</ref> which were estimated on 10,000 natural images from the PASCAL VOC 2010 dataset. Based on our analysis above, even though these background statistics were estimated from a very large amount of real image data, it will not work for all domains. In section 5.2, our results confirm this claim.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Deep CORAL</head><p>In this section, we extend CORAL to work seamlessly with deep neural networks by designing a differentiable CORAL loss. Deep CORAL enables end-to-end adaptation and also learns more a powerful nonlinear transformation. It can be easily integrated into different layers or network architectures. <ref type="figure" target="#fig_6">Figure 5</ref> shows a sample Deep CORAL architecture using our proposed correlation alignment layer for deep domain adaptation. We refer to Deep CORAL as any deep network incorporating the CORAL loss for domain adaptation. We first describe the CORAL loss between two domains for a single feature layer. Suppose the numbers of source and target data are n S and n T respectively. Here both x and u are the d-dimensional deep layer activations φ (I) of input I that we are trying to learn. Suppose D i j S (D i j T ) indicates the j-th dimension of the i-th source (target) data example and C S (C T ) denote the feature covariance matrices.</p><p>We define the CORAL loss as the distance between the second-order statistics (covariances) of the source and target features:</p><formula xml:id="formula_14">L CORAL = 1 4d 2 C S −C T 2 F<label>(7)</label></formula><p>where · 2 F denotes the squared matrix Frobenius norm. The covariance matrices of the source and target data are given by:</p><formula xml:id="formula_15">C S = 1 n S − 1 (D S D S − 1 n S (1 D S ) (1 D S ))<label>(8)</label></formula><formula xml:id="formula_16">C T = 1 n T − 1 (D T D T − 1 n T (1 D T ) (1 D T ))<label>(9)</label></formula><p>where 1 is a column vector with all elements equal to 1. The gradient with respect to the input features can be calculated using the chain rule:</p><formula xml:id="formula_17">∂ L CORAL ∂ D i j S = 1 d 2 (n S − 1) ((D S − 1 n S (1 D S ) 1 ) (C S −C T )) i j (10) ∂ L CORAL ∂ D i j T = − 1 d 2 (n T − 1) ((D T − 1 n T (1 D T ) 1 ) (C S −C T )) i j<label>(11)</label></formula><p>In our experiments, we use batch covariances and the network parameters are shared between the two networks, but other settings are also possible. To see how this loss can be used to adapt an existing neural network, let us return to the multi-class classification problem. Suppose we start with a network with a final classification layer, such as the ConvNet shown in <ref type="figure" target="#fig_6">Figure 5</ref>. As mentioned before, the final deep features need to be both discriminative enough to train a strong classifier and invariant to the difference between source and target domains. Minimizing the classification loss itself is likely to lead to overfitting to the source domain, causing reduced performance on the target domain. On the other hand, minimizing the CORAL loss alone might lead to degenerated features. For example, the network could project all of the source and target data to a single point, making the CORAL loss trivially zero. However, no strong classifier can be constructed on these features. Joint training with both the classification loss and CORAL loss is likely to learn features that work well on the target domain:</p><formula xml:id="formula_18">L = L CLASS + t ∑ i=1 λ i L CORAL i<label>(12)</label></formula><p>where t denotes the number of CORAL loss layers in a deep network and λ i is a weight that trades off the adaptation with classification accuracy on the source domain. As we show below, these two losses play counterparts and reach an equilibrium at the end of training, where the final features are discriminative and generalize well to the target domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We evaluate CORAL and Deep CORAL on object recognition <ref type="bibr" target="#b22">[23]</ref> using standard benchmarks and protocols. In all experiments we assume the target domain is unlabeled. For CORAL, we follow the standard procedure <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b3">4]</ref> and use a linear SVM as the base classifier. The model selection approach of <ref type="bibr" target="#b7">[8]</ref> is used to set the C parameter for the SVM by doing cross-validation on the source domain. For CORAL-LDA, as efficiency is the main concern, we evaluate it on the more time constrained taskobject detection. We follow the protocol of <ref type="bibr" target="#b9">[10]</ref> and use HOG features. To have a fair comparison, we use accuracies reported by other authors with exactly the same setting or conduct experiments using the source code provided by the authors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Object Recognition</head><p>In this set of experiments, domain adaptation is used to improve the accuracy of an object classifier on novel image domains. Both the standard Office <ref type="bibr" target="#b22">[23]</ref> and extended Office-Caltech10 <ref type="bibr" target="#b10">[11]</ref>   <ref type="table">Table 1</ref> Object recognition accuracies of all 12 domain shifts on the Office-Caltech10 dataset <ref type="bibr" target="#b10">[11]</ref> with SURF features, following the protocol of <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b22">23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Object Recognition with Shallow Features</head><p>We follow the standard protocol of <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b22">23]</ref> and conduct experiments on the Office-Caltech10 dataset with shallow features (SURF). The SURF features were encoded with 800-bin bag-of-words histograms and normalized to have zero mean and unit standard deviation in each dimension. Since there are four domains, there are 12 experiment settings, namely, A→C (train classifier on (A)mazon, test on (C)altech), A→D (train on (A)mazon, test on (D)SLR), A→W, and so on. We follow the standard protocol and conduct experiments in 20 randomized trials for each domain shift and average the accuracy over the trials. In each trial, we use the standard setting <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b22">23]</ref> and randomly sample the same number (20 for Amazon, Caltech, and Webcam; 8 for DSLR as there are only 8 images per category in the DSLR domain) of labelled images in the source domain as training set, and use all the unlabelled data in the target domain as the test set.</p><p>In <ref type="table">Table 1</ref>, we compare our method to five recent published methods: SVMA <ref type="bibr" target="#b5">[6]</ref>, DAM <ref type="bibr" target="#b4">[5]</ref>, GFK <ref type="bibr" target="#b10">[11]</ref>, SA <ref type="bibr" target="#b7">[8]</ref>, and TCA <ref type="bibr" target="#b21">[22]</ref> as well as the no adaptation baseline (NA). GFK, SA, and TCA are manifold based methods that project the source and target distributions into a lower-dimensional manifold. GFK integrates over an infinite number of subspaces along the subspace manifold using the kernel trick. SA aligns the source and target subspaces by computing a linear map that minimizes the Frobenius norm of their difference. TCA performs domain adaptation via a new parametric kernel using feature extraction methods by projecting data onto the learned transfer components. DAM introduces smoothness assumption to enforce the target classifier share similar decision values with the source classifiers. Even though these methods are far more complicated than ours and require tuning of hyperparameters (e.g., subspace dimensionality), our method achieves the best average performance across all the 12 domain shifts. Our method also improves on the no adaptation baseline (NA), in some cases increasing accuracy significantly (from 56% to 86% for D→W).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Object Recognition with Deep Features</head><p>For visual domain adaptation with deep features, we follow the standard protocol of <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b8">9]</ref> and use all the labeled source data and all the target data without labels on the standard Office dataset <ref type="bibr" target="#b22">[23]</ref>. Since there are 3 domains, we conduct experiments on all 6 shifts (5 runs per shift), taking one domain as the source and another as the target.</p><p>In this experiment, we apply the CORAL loss to the last classification layer as it is the most general case-most deep classifier architectures (e.g., convolutional neural networks, recurrent neural networks) contain a fully connected layer for classification. Applying the CORAL loss to other layers or other network architectures is also possible. The dimension of the last fully connected layer ( f c8) was set to the number of categories (31) and initialized with N (0, 0.005). The learning rate of f c8 was set to 10 times the other layers as it was training from scratch. We initialized the other layers with the parameters pre-trained on ImageNet <ref type="bibr" target="#b2">[3]</ref> and kept the original layer-wise parameter settings. In the training phase, we set the batch size to 128, base learning rate to 10 −3 , weight decay to 5 × 10 −4 , and momentum to 0.9. The weight of the CORAL loss (λ ) is set in such way that at the end of training the classification loss and CORAL loss are roughly the same. It seems be a reasonable choice as we want to have a feature representation that is both discriminative and also minimizes the distance between the source and target domains. We used Caffe <ref type="bibr" target="#b15">[16]</ref> and BVLC Reference CaffeNet for all of our experiments.</p><p>We compare to 7 recently published methods: CNN <ref type="bibr" target="#b16">[17]</ref> (no adaptation), GFK <ref type="bibr" target="#b10">[11]</ref>, SA <ref type="bibr" target="#b7">[8]</ref>, TCA <ref type="bibr" target="#b21">[22]</ref>, CORAL <ref type="bibr" target="#b24">[25]</ref>, DDC <ref type="bibr" target="#b29">[30]</ref>, DAN <ref type="bibr" target="#b18">[19]</ref>. GFK, SA, and TCA are manifold based methods that project the source and target distributions into a lowerdimensional manifold and are not end-to-end deep methods. DDC adds a domain confusion loss to AlexNet <ref type="bibr" target="#b16">[17]</ref> and fine-tunes it on both the source and target domain. DAN is similar to DDC but utilizes a multi-kernel selection method for better mean embedding matching and adapts in multiple layers. For direct comparison, DAN in this paper uses the hidden layer f c8. For GFK, SA, TCA, and CORAL, we use the f c7 feature fine-tuned on the source domain (FT 7 in <ref type="bibr" target="#b24">[25]</ref>) as it achieves better performance than generic pre-trained features, and train a linear SVM <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b24">25]</ref>. To have a fair comparison, we use accuracies reported by other authors with exactly the same setting or conduct experiments using the source code provided by the authors.</p><p>From <ref type="table" target="#tab_1">Table 2</ref> we can see that Deep CORAL (D-CORAL) achieves better average performance than CORAL and the other 6 baseline methods. In 3 out of 6 shifts, it    achieves the highest accuracy. For the other 3 shifts, the margin between D-CORAL and the best baseline method is very small ( 0.7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Domain Adaptation Equilibrium</head><p>To get a better understanding of Deep CORAL, we generate three plots for domain shift A→W. In <ref type="figure" target="#fig_9">Figure 6</ref>(a) we show the training (source) and testing (target) accuracies for training with v.s. without CORAL loss. We can clearly see that adding the CORAL loss helps achieve much better performance on the target domain while maintaining strong classification accuracy on the source domain.</p><p>In <ref type="figure" target="#fig_9">Figure 6</ref>(b) we visualize both the classification loss and the CORAL loss for training w/ CORAL loss. As the last fully connected layer is randomly initialized with N (0, 0.005), in the beginning the CORAL loss is very small while the classification loss is very large. After training for a few hundred iterations, these two losses are about the same and reach an equilibrium. In <ref type="figure" target="#fig_9">Figure 6</ref>(c) we show the CORAL distance between the domains for training w/o CORAL loss (setting the weight to 0). We can see that the distance is getting much larger ( 100 times larger compared to training w/ CORAL loss). Comparing <ref type="figure" target="#fig_9">Figure 6</ref>(b) and <ref type="figure" target="#fig_9">Figure 6</ref>(c), we can see that even though the CORAL loss is not always decreasing during training, if we set its weight to 0, the distance between source and target domains becomes much larger. This is reasonable as fine-tuning without domain adaptation is likely to overfit the features to the source domain. Our CORAL loss constrains the distance between source and target domain during the fine-tuning process and helps to maintain an equilibrium where the final features work well on the target domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Object Detection</head><p>Following protocol of <ref type="bibr" target="#b9">[10]</ref>, we conduct object detection experiment on the Office dataset <ref type="bibr" target="#b22">[23]</ref> with HOG features. We use the same setting as <ref type="bibr" target="#b9">[10]</ref>, performing detection on the Webcam domain as the target (test) domain, and evaluating on the same 783 image test set of 20 categories (out of 31). As source (training) domains, we use: the two remaining real-image domains in Office, Amazon and DSLR, and two domains that contain virtual images only, Virtual and Virtual-Gray, generated from 3d CAD models. The inclusion of the two virtual domains is to reduce human effort in annotation and facilitate future research <ref type="bibr" target="#b23">[24]</ref>. Examples of Virtual and Virtual-Gray are shown in <ref type="figure">Figure 8</ref>. Please refer to <ref type="bibr" target="#b25">[26]</ref> for detailed explanation of the data generation process. We also compare to <ref type="bibr" target="#b9">[10]</ref> who use corresponding ImageNet <ref type="bibr" target="#b2">[3]</ref> synsets as the source. Thus, there are four potential source domains (two synthetic and three real) and one (real) target domain. The number of positive training images per category in each domain is shown in <ref type="table" target="#tab_2">Table 3</ref>. <ref type="figure">Figure 7</ref> shows an overview of our evaluation.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of Mismatched Image Statistics</head><p>First, we explore the effect of mismatched precomputed image statistics on detection performance. For each source domain, we train CORAL-LDA detectors using the positive mean from the source, and pair it with the covariance and negative mean of other domains. The virtual and the Office domains are used as sources, and the test domain is always Webcam. The statistics for each of the four domains were calculated using all of the training data, following the same approach as <ref type="bibr" target="#b13">[14]</ref>. The pre-computed statistics of 10,000 real images from PASCAL, as proposed in <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b9">10]</ref>, are also evaluated. Detection performance, measured in Mean Average Precision (MAP), is shown in <ref type="table" target="#tab_3">Table 4</ref>. We also calculate the normalized Euclidean distance between pairs of domains as ( C 1 − C 2 )/( C 1 + C 2 ) + ( µ 1 0 − µ 2 0 )/( µ 1 0 + µ 2 0 ), and show the average distance between source and target in parentheses in <ref type="table" target="#tab_3">Table 4</ref>.</p><p>From these results we can see a trend that larger domain difference leads to poorer performance. Note that larger difference to the target domain also leads to lower performance, confirming our hypothesis that both source and target statistics matter. Some of the variation could also stem from our assumption about the difference of means being the same not quite holding true. Finally, the PASCAL statistics from <ref type="bibr" target="#b13">[14]</ref> perform the worst. Thus, in practice, statistics from either source domain or target domain or domains close to them could be used. However, unrelated statistics will not work even though they might be estimated from a very large amount of data as <ref type="bibr" target="#b13">[14]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unsupervised and Semi-supervised Adaptation</head><p>Next, we report the results of our unsupervised and semi-supervised adaptation technique. We use the same setting as <ref type="bibr" target="#b9">[10]</ref>, in which three positive and nine negative labeled images per category were used for semi-supervised adaptation. Target covariance in Equation 6 is estimated from 305 unlabeled training examples. We also followed the same approach to learn a linear combination between the unsupervised and supervised model via cross-validation. The results are presented in <ref type="table">Table 5</ref>. Please note that our target-only MAP is 52.9 compared to 36.6 in <ref type="bibr" target="#b9">[10]</ref>. This also confirms our conclusion that the statistics should come from a related domain. It is clear that both of our unsupervised and semi-supervised adaptation techniques outperform the method in <ref type="bibr" target="#b9">[10]</ref>. Furthermore, Virtual-Gray data outperforms Virtual, and DSLR does best, as it is very close to the target domain (the main difference between DLSR and Webcam domains is in the camera used to capture images). Finally, we compare our method trained on Virtual-Gray to the results of adapting from ImageNet reported by <ref type="bibr" target="#b9">[10]</ref>, in <ref type="figure">Figure 9</ref>. While their unsupervised models are learned from 150-2000 real ImageNet images per category and the background statistics are estimated from 10,000 PASCAL images, we only have 30 virtual images per category and the background statistics is learned from about 1,000 images. What's more, all the virtual images used are with uniform gray texturemap and white background. This clearly demonstrates the importance of domain-specific decorrelation, and shows that the there is no need to collect a large amount of real images to train a good classifier. <ref type="figure">Fig. 9</ref> Comparison of unsupervised and semi-supervised adaptation of virtual detectors using our method with the results of training on ImageNet and supervised adaptation from ImageNet reported in <ref type="bibr" target="#b9">[10]</ref>. Our semi-supervised adapted detectors achieve comparable performance despite not using any real source training data, and using only 3 positive images for adaptation, and even outperform ImageNet significantly for several categories (e.g., ruler).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source</head><p>Source-only <ref type="bibr" target="#b13">[14]</ref> Unsup-Ours SemiSup <ref type="bibr">[</ref>  <ref type="table">Table 5</ref> Top: Comparison of the source-only <ref type="bibr" target="#b13">[14]</ref> and semi-supervised adapted model of <ref type="bibr" target="#b9">[10]</ref> with our unsupervised-adapted and semi-supervised adapted models. Target domain is Webcam.</p><p>Mean AP across categories is reported on the Webcam test data, using different source domains for training. Bottom: Sample detections of the DSLR-UnsupAdapt-Ours detectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this chapter, we described a simple, effective, and efficient method for unsupervised domain adaptation called CORrelation ALignment (CORAL). CORAL minimizes domain shift by aligning the second-order statistics of source and target distributions, without requiring any target labels. We also developed novel domain adaptation algorithms by applying the idea of CORAL to three different scenarios.</p><p>In the first scenario, we applied a linear transformation that minimizes the CORAL objective to source features prior to training the classifier. In the case of linear classifiers, we equivalently applied the linear CORAL transform to the classifier weights, significantly improving efficiency and classification accuracy over standard LDA on several domain adaptation benchmarks. We further extended CORAL to learn a nonlinear transformation that aligns correlations of layer activations in deep neural networks. The resulting Deep CORAL approach works seamlessly with deep networks and can be integrated into any arbitrary network architecture to enable end-to-end unsupervised adaptation. One limitation of CORAL is that it captures second-order statistics only and may not preserve higher-order structure in the data. However, as demonstrated in this chapter, it works fairly well in practice, and can also potentially be combined with other domain-alignment loss functions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1 2 U</head><label>2</label><figDesc>T [1:r] ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 (</head><label>1</label><figDesc>a-c) Illustration of CORrelation ALignment (CORAL) for Domain Adaptation: (a) The original source and target domains have different distribution covariances, despite the features being normalized to zero mean and unit standard deviation. This presents a problem for transferring classifiers trained on source to target. (b) The same two domains after source decorrelation, i.e. removing the feature correlations of the source domain. (c) Target re-correlation, adding the correlation of the target domain to the source features. After this step, the source and target distributions are well aligned and the classifier trained on the adjusted source domain is expected to work well in the target domain. (d) One might instead attempt to align the distributions by whitening both source and target. However, this will fail since the source and target data are likely to lie on different subspaces due to domain shift. (Best viewed in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figures 1 ( 1 2 1 2</head><label>111</label><figDesc>a-c) illustrate the linear CORAL approach. Figure 1(a) shows example original source and target data distributions. We can think of the transformation A intuitively as follows: the first part U S Σ + S U S whitens the source data, while the second part U T [1:r] Σ T [1:r] U T [1:r] re-colors it with the target covariance. This is illustrated in Figure 1(b) and Figure 1(c) respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2</head><label>2</label><figDesc>Sensitivity of CORAL to the covariance regularization parameter λ with λ ∈ {0, 0.001, 0.01, 0.1, 1}. The plots show classification accuracy on target data for two domain shifts (blue and red). When λ = 0, there is no regularization and we use the analytical solution in Equation(2). Please refer to Section 5.1 for details of the experiment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Algorithm 1</head><label>1</label><figDesc>CORAL for Unsupervised Domain Adaptation Input: Source Data D S , Target Data D T Output: Adjusted Source Data D * s C S = cov(D S ) + eye(size(D S , 2)) C T = cov(D T ) + eye(size(D T , 2))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4</head><label>4</label><figDesc>Visualization of classifier weights of bicycle (decorrelated with mismatched-domain covariance (left) v.s. with same-domain covariance (right)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5</head><label>5</label><figDesc>Sample Deep CORAL architecture based on a CNN with a classifier layer. For generalization and simplicity, here we apply the CORAL loss to the f c8 layer of AlexNet<ref type="bibr" target="#b16">[17]</ref>. Integrating it into other layers or network architectures is also possible.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>A→D A→W D→A D→W W→A W→D AVG GFK 52.4±0.0 54.7±0.0 43.2±0.0 92.1±0.0 41.8±0.0 96.2±0.0 63.4 SA 50.6±0.0 47.4±0.0 39.5±0.0 89.1±0.0 37.6±0.0 93.8±0.0 59.7 TCA 46.8±0.0 45.5±0.0 36.4±0.0 81.1±0.0 39.5±0.0 92.2±0.0 56.9 CORAL 65.7±0.0 64.3±0.0 48.5±0.0 96.1±0.0 48.2±0.0 99.8±0.0 70.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>. w/ coral loss test acc. w/o coral loss training acc. w/ coral loss training acc. w/o coral loss Equa. (1) w/o coral loss</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 6</head><label>6</label><figDesc>Detailed analysis of shift A→W for training w/ v.s. w/o CORAL loss. (a): training and test accuracies for training w/ v.s. w/o CORAL loss.We can see that adding CORAL loss helps achieve much better performance on the target domain while maintaining strong classification accuracy on the source domain. (b): classification loss and CORAL loss for training w/ CORAL loss. As the last fully connected layer is randomly initialized with N (0, 0.005), CORAL loss is very small while classification loss is very large at the beginning. After training for a few hundred iterations, these two losses are about the same. (c): CORAL distance for training w/o CORAL loss (setting the weight to 0). The distance is getting much larger ( 100 times larger compared to training w/ CORAL loss).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 7 Fig. 8</head><label>78</label><figDesc>Overview of our evaluation on object detection. With CORAL-Linear Discriminant Analysis (LDA), we show that a strong object detector can be trained from virtual data only. Two sets of virtual images used in this section: (a) Virtual: background and texturemap from a random real ImageNet image; (b) Virtual-Gray: uniform gray texturemap and white background. Please refer to<ref type="bibr" target="#b25">[26]</ref> for detailed explanation of the data generation process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>datasets are used as benchmarks in this chapter. Office-Caltech10 contains 10 object categories from an office environment (e.g., keyboard, laptop, etc.) in 4 image domains: Webcam, DSLR, Amazon, and Caltech256. The standard Office dataset contains 31 (the same 10 categories from Office-Caltech10 plus 21 additional ones) object categories in 3 domains: Webcam, DSLR, and Amazon. 38.3 38.7 47.2 40.7 39.2 38.1 34.2 85.9 37.8 34.6 84.9 46.7</figDesc><table><row><cell></cell><cell>A→C A→D A→W C→A C→D C→W D→A D→C D→W W→A W→C W→D AVG</cell></row><row><cell>NA</cell><cell>35.8 33.1 24.9 43.7 39.4 30.0 26.4 27.1 56.4 32.3 25.7 78.9 37.8</cell></row><row><cell>SVMA</cell><cell>34.8 34.1 32.5 39.1 34.5 32.9 33.4 31.4 74.4 36.6 33.5 75.0 41.0</cell></row><row><cell>DAM</cell><cell>34.9 34.3 32.5 39.2 34.7 33.1 33.5 31.5 74.7 34.7 31.2 68.3 40.2</cell></row><row><cell>GFK</cell><cell>38.3 37.9 39.8 44.8 36.1 34.9 37.9 31.4 79.1 37.1 29.1 74.6 43.4</cell></row><row><cell>TCA</cell><cell>40.0 39.1 40.1 46.7 41.4 36.2 39.6 34.0 80.4 40.2 33.7 77.5 45.7</cell></row><row><cell>SA</cell><cell>39.9 38.8 39.6 46.1 39.4 38.9 42.0 35.0 82.3 39.3 31.8 77.9 45.9</cell></row><row><cell cols="2">CORAL 40.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>CORAL 66.8±0.6 66.4±0.4 52.8±0.2 95.7±0.3 51.5±0.3 99.2±0.1 72.1 Object recognition accuracies for all 6 domain shifts on the standard Office dataset with deep features, following the standard unsupervised adaptation protocol.</figDesc><table><row><cell>4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 #</head><label>3</label><figDesc></figDesc><table /><note>training examples for each source domain.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc>Virtual Virtual-Gray Amazon DSLR PASCAL Virtual 30.8 (0.1) 16.5 (1.0) 24.1 (0.6) 28.3 (0.2) 10.7 (0.5) Virtual-Gray 32.3 (0.6) 32.3 (0.5) 27.3 (0.8) 32.7 (0.6) 17.9 (0.7) MAP of CORAL-LDA trained on positive examples from each row's source domain and background statistics from each column's domain. The average distance between each set of background statistics and the true source and target statistics is shown in parentheses.</figDesc><table><row><cell>Amazon</cell><cell>39.9 (0.4) 30.0 (1.0) 39.2 (0.4) 37.9 (0.4) 18.6 (0.6)</cell></row><row><cell>DSLR</cell><cell>68.2 (0.2) 62.1 (1.0) 68.1 (0.6) 66.5 (0.1) 37.7 (0.5)</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A singular value thresholding algorithm for matrix completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Feng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><forename type="middle">J</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuowei</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. on Optimization</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1956" to="1982" />
			<date type="published" when="2010-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Frustratingly easy domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Daume</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Domain adaptation from multiple sources via auxiliary classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Domain transfer multiple kernel learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixin</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="465" to="479" />
			<date type="published" when="2012-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised visual domain adaptation using subspace alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amaury</forename><surname>Habrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Sebban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Interactive adaptation of real-time object detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Goehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Rodner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Geodesic flow kernel for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Domain adaptation for object recognition: An unsupervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning from multiple outlooks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maayan</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shie</forename><surname>Mannor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Discriminative decorrelation for clustering and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Bharath Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><forename type="middle">Darrell</forename><surname>Caffe</surname></persName>
		</author>
		<title level="m">Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">What you saw is not what you get: Domain adaptation using asymmetric kernel transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Transfer joint matching for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaguang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Ensemble of exemplar-svms for object detection and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Domain adaptation via transfer component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><forename type="middle">W</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">T</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adapting visual category models to new domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Correlation Alignment for Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochen</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts Lowell</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Return of frustratingly easy domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Generating large scale image datasets from 3d cad models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingchao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR&apos;15 Workshop on The Future of Datasets in Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">From virtual to reality: Fast adaptation of virtual object detectors to real domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Subspace distribution alignment for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep coral: Correlation alignment for deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2016 Workshops</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Deep domain confusion: Maximizing for domain invariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno>abs/1412.3474</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
