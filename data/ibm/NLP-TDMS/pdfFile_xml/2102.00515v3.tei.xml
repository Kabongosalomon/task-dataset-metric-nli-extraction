<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Classification of Shoulder X-Ray Images with Deep Learning Ensemble Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Uysal</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Electrical and Electronics Engineering</orgName>
								<orgName type="department" key="dep2">Faculty of Engineering</orgName>
								<orgName type="institution">Gazi University</orgName>
								<address>
									<postCode>06570</postCode>
									<settlement>Ankara</settlement>
									<region>TR</region>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fırat</forename><surname>Hardalaç</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Electrical and Electronics Engineering</orgName>
								<orgName type="department" key="dep2">Faculty of Engineering</orgName>
								<orgName type="institution">Gazi University</orgName>
								<address>
									<postCode>06570</postCode>
									<settlement>Ankara</settlement>
									<region>TR</region>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Peker</surname></persName>
							<email>ozan.peker@gazi.edu.tro.p.</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Electrical and Electronics Engineering</orgName>
								<orgName type="department" key="dep2">Faculty of Engineering</orgName>
								<orgName type="institution">Gazi University</orgName>
								<address>
									<postCode>06570</postCode>
									<settlement>Ankara</settlement>
									<region>TR</region>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tolga</forename><surname>Tolunay</surname></persName>
							<email>tolgatolunay@gazi.edu.tr</email>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Orthopaedics and Traumatology</orgName>
								<orgName type="department" key="dep2">Faculty of Medicine</orgName>
								<orgName type="institution">Gazi University</orgName>
								<address>
									<postCode>06570</postCode>
									<settlement>Ankara</settlement>
									<region>TR</region>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nil</forename><surname>Tokgöz</surname></persName>
							<email>nil.tokgoz@gazi.edu.tr*correspondence:uysal@gazi.edu.tr</email>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Department of Radiology</orgName>
								<orgName type="department" key="dep2">Faculty of Medicine</orgName>
								<orgName type="institution">Gazi University</orgName>
								<address>
									<postCode>06570</postCode>
									<settlement>Ankara</settlement>
									<region>TR</region>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Classification of Shoulder X-Ray Images with Deep Learning Ensemble Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.3390/app11062723</idno>
					<note>Accepted at Applied Sciences, MDPI, 2021, 11(6), 2723. for citation and final version please click here: https://doi.org/10.3390/app11062723 1 of 34 Accepted at Applied Sciences, MDPI, 2021, 11(6), 2723. Uysal, F.; Hardalaç, F.; Peker, O.; Tolunay, T.; Tokgöz, N. Classification of Shoulder X-ray Images with Deep Learning Ensemble Models. for citation and final version please click here: https://doi.org/10.3390/app11062723 Article</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>biomedical image classification</term>
					<term>bone fractures</term>
					<term>deep learning</term>
					<term>ensemble learning</term>
					<term>shoulder</term>
					<term>transfer learning</term>
					<term>X-ray</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fractures occur in the shoulder area, which has a wider range of motion than other joints in the body, for various reasons. To diagnose these fractures, data gathered from X-radiation (Xray), magnetic resonance imaging (MRI), or computed tomography (CT) are used. This study aims to help physicians by classifying shoulder images taken from X-ray devices as fracture / non-fracture with artificial intelligence. For this purpose, the performances of 26 deep learning-based pre-trained models in the detection of shoulder fractures were evaluated on the musculoskeletal radiographs (MURA) dataset, and two ensemble learning models (EL1 and EL2) were developed. The pretrained models used are ResNet, ResNeXt, DenseNet, VGG, Inception, MobileNet, and their spinal fully connected (Spinal FC) versions. In the EL1 and EL2 models developed using pre-trained models with the best performance, test accuracy was 0.8455,0.8472, Cohen's kappa was 0.6907, 0.6942 and the area that was related with fracture class under the receiver operating characteristic (ROC) curve (AUC) was 0.8862,0.8695. As a result of 28 different classifications in total, the highest test accuracy and Cohen's kappa values were obtained in the EL2 model, and the highest AUC value was obtained in the EL1 model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Having a wider and more varied range of movement than the other joints in the body, the shoulder has a flexible structure. The fractures in the shoulder may result from incidents such as dislocation of the shoulder and engaging in contact sports and motor vehicle accidents. The shoulder bone mainly consists of three different bones: the upper arm bone, named the 'humerus', the shoulder blade, named the 'scapula', and the collarbone, named the 'clavicle'. <ref type="figure" target="#fig_0">Figure 1</ref> shows the anatomic structure of the shoulder bone. The image in this figure was taken from the MURA dataset used in this study, and the markings thereon were placed by the physicians at Gazi University. As shown in <ref type="figure" target="#fig_0">Figure  1</ref>, the upper end of the humerus has a ball-like shape that connects with the scapula, called the glenoid. The types of shoulder fractures vary depending on age. While most fractures in children occur in the clavicle bone, the most common fracture in adults occur on the top part of the humerus, i.e., the proximal humerus. The types of shoulder bone fractures are divided into three categories in general: clavicle fractures, which are the most common shoulder fracture, frequently the result of a fall, scapula fractures, which rarely occur, and resulting fractures, which occur as cracks in the upper part of the arm in individuals over 65 years of age. The images from X-ray devices are primarily used for imaging of the Accepted at Applied Sciences, MDPI, 2021, 11 <ref type="bibr" target="#b5">(6)</ref>, 2723. <ref type="bibr">Uysal</ref> shoulder bone for diagnosis and treatment of such fractures, while MRI or CT devices may also be used when required <ref type="bibr" target="#b0">[1]</ref>. Deep learning classification procedures of shoulder bone X-ray images were carried out in the study. The main contributions of this study are as follows:</p><p>• The most suitable model for the classification of shoulder bone X-ray images as a fracture or non-fracture is determined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>An approach that can be used in similar studies is developed via new ensemble learning models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>The study can assist physicians who are not experts in the field in the classification stage, especially in cases of shoulder fractures, which are frequently encountered in the emergency departments of hospitals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>The method suggested in the study contributes to the literature with two different ensemble approaches. • With the first model proposed in the study, a performance study was conducted with transfer learning for the MURA dataset, which is widely used in X-ray studies. Thus, three models that give the best classification results have been combined into a single model, and the classification performance has been increased. The developed model can be used to classify many medical X-ray images. • With the second model, it is determined which model finds which class best by looking at the success of finding the classes on the models in the dataset. It is ensured that the model decides the prediction of that class. Thus, regardless of the dataset studied, a similar decision system can be designed with models that find the classes in a given dataset, and a higher performance can be achieved compared to a single model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>The proposed ensemble model approaches can be applied and generalized by performing similar preprocessing steps in other X-ray biomedical datasets. In addition, the proposed method can be easily used in studies with transfer learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>In this study, the classification of X-ray images of shoulder fractures in the MURA dataset was carried out into the categories of 'fracture' or "non-fracture' using built CNNbased deep learning models, new models thereof adapted to SpinalNet, and models developed based on ensemble learning. Therefore, two main topics have been taken into Accepted at Applied Sciences, MDPI, 2021, 11 <ref type="bibr" target="#b5">(6)</ref>, 2723. <ref type="bibr">Uysal</ref> account while examining the literature regarding the study. Upon examination of the studies conducted thus far, these topics are as follows:</p><p>• what the studies conducted using the MURA dataset are, and why this dataset is preferred in this study; • what kinds of studies have been conducted on the classification of shoulder bone images, and what kinds of innovations may be set forth based on the efficiency of deep learning models used in the classification procedures carried out in this study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Studies Conducted Using the MURA Dataset</head><p>The MURA dataset was first introduced to the literature in a paper published in the OpenReview platform, announced in the conference on "Medical Imaging with Deep Learning" held in Amsterdam in 2018. Following this publication, this dataset was made publicly available for academic studies in a competition called "Bone X-Ray Deep Learning Competition" by the Machine Learning group of the Stanford University. Being one of the largest public radiographic image datasets, MURA contains a total of 40,561 X-ray images in png format for the following parts of the body, labeled as either normal or abnormal (fracture): elbow, finger, forearm, hand, humerus, shoulder, and wrist. In this classification study conducted by Rajpurkar et al. using DenseNet-169 on this dataset, the AUC score representing the area under the overall Receiver Operator Characteristics (ROC) curve was 0.929, and the overall Cohen's kappa score was 0.705 <ref type="bibr" target="#b1">[2]</ref>. Following this first study in which this dataset was introduced to the literature, there have been various studies that have been conducted and published using all or a part of the dataset. These studies are as follows: an average precision (AP) value of 62.04% was achieved in the fracture detection procedure using the proposed deep CNN model after the fractures were marked by the physicians on the arm X-ray images in the MURA dataset by Guan et al. <ref type="bibr" target="#b2">[3]</ref>. The following classification accuracies were achieved by Galal et al. using a very small part of the elbow X-ray images in the MURA dataset (56 images for the train dataset, 24 images for the test): 97% with the support vector machine (SVM), 91.6% with random forest (RF), and 91.6% with naive Bayes <ref type="bibr" target="#b3">[4]</ref>. The classification by Liang and Gu with the multi-scale CNN and graph convolution network (GCN) they proposed using the entire MURA dataset achieved an overall Cohen's kappa score of 0.836. <ref type="bibr" target="#b4">[5]</ref>. The overall Cohen's kappa score achieved by Saif et al. in classification performed using the whole MURA dataset with the proposed capsule network architecture was 0.80115 <ref type="bibr" target="#b5">[6]</ref>. In the classification carried out by Cheng et al. using the dataset containing hip bone images and the whole MURA dataset, the highest accuracy achieved was 86.53% for humerus images with the proposed adversarial policy gradient augmentation (APGA) <ref type="bibr" target="#b6">[7]</ref>. The highest accuracy achieved in the classification performed by Pelka et al. using the whole MURA dataset was 79.85% with the InceptionV3 model <ref type="bibr" target="#b7">[8]</ref>. The AUC score was 0.88 in the classification carried out by Varma et al. on a Lower Extremity Radiographs Dataset (LERA) containing foot, knee, ankle, and hip data for an ImageNet and DenseNet161 model pre-trained with MURA <ref type="bibr" target="#b8">[9]</ref>. In the classification performed by Harini et al. on images of the finger, wrist, and shoulder in the MURA dataset using five different CNN-based built deep learning methods, the highest accuracy was 56.30% on the wrist data with DenseNet169 <ref type="bibr" target="#b9">[10]</ref>. The overall accuracy achieved for MURA with the proposed iterative fusion CNN (IFCNN) method is 73.4% in classification carried out by Fang et al. using the optical coherence tomography (OCT) dataset and the entire MURA dataset <ref type="bibr" target="#b10">[11]</ref>. The overall Cohen's kappa score achieved was 0.717 with the proposed deep CNN-based ensemble model in the classification carried out by Mondol et al. on elbow, finger, humerus, and wrist data in the MURA dataset <ref type="bibr" target="#b12">[12]</ref>. In the type classification performed by Pradhan et al. using the whole MURA dataset, 91.37% accuracy was achieved with the proposed deep CNN model <ref type="bibr" target="#b13">[13]</ref>.</p><p>In the classification carried out by Shao and Wang using the entire MURA dataset, a twostage method was developed, and the highest accuracy achieved was in humerus images with 88.5% for the SENet154 model, while the highest accuracy for the DenseNet201 model was achieved again on humerus images with 90.94% <ref type="bibr" target="#b14">[14]</ref>.</p><p>Although there are many different publicly available datasets on bone fractures in addition to the MURA dataset in the literature, the main reason for using only this dataset in this study is that MURA is one of the largest datasets for both normal (negative, nonfracture) and abnormal (positive, fracture) groups compared to the other public datasets. It is observed upon examination of the studies conducted via this dataset that classification and/or fracture detection procedures are carried out using the dataset in whole or in part. Only the X-ray images for shoulder bone in the MURA dataset were used in this study. The reason why only this type of dataset was used for classification despite the availability of seven different types of datasets and the reason why such a dataset contains shoulder data is that the shoulder dataset is the most balanced type in terms of distribution of the amount of data provided for both training and validation. Another reason for classification for a single type only is to be able to develop the most optimum model for fracture and non-fracture images on this type, using and developing as many different deep learning models as possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Classification Studies Carried Out on the Shoulder Bone</head><p>While some of the studies mentioned under the previous title with the MURA dataset include classification of shoulder bone images, there are also studies in the literature on this type of classification exclusively. In classification carried out by Chung et al. with the ResNet152 model on a total of 1891 patients, including 1376 proximal humeral fracture cases of four different types, a 96% accuracy was achieved <ref type="bibr" target="#b15">[15]</ref>. In the CNN model proposed by Sezers using a total of 219 shoulder MR images, a classification was made in three different groups as normal, edematous, and Hill-Sachs lesions with a 98.43% accuracy <ref type="bibr" target="#b16">[16]</ref>. The highest accuracy achieved in the classification carried out by Urban et al. on 597 X-ray images of shoulders with implants was 80.4% in the NASNet model pre-trained with ImageNet <ref type="bibr" target="#b17">[17]</ref>. In the classification performed by Sezer et al. using a total of 219 shoulder MR images grouped as 91 edematous, 49 Hill-Sachs lesions, and 79 normal, an 88% success rate was achieved with a kernel-based SVM, and 94% success was achieved with extreme learning machines <ref type="bibr" target="#b18">[18]</ref>. A 94.74% accuracy was achieved with the proposed CapsNet model in the classification performed by Sezers on a total of 1006 shoulder MR images, grouped as 316 normal, 311 degenerated, and 379 torn <ref type="bibr" target="#b19">[19]</ref>. Some other important studies on medical data classification and machine/deep learning approach in the literature are as follows: An accuracy of 93.5% was achieved in cardiac arrhythmia classification procedures with long short-term memory (LSTM) by Khan and Kim <ref type="bibr" target="#b20">[20]</ref>. ResNet18 and GoogLeNet models were used by Storey et al. to detect wrist bone abnormality <ref type="bibr" target="#b21">[21]</ref>. As a result of the classification made by Yin et al. for kidney disease with the multi-instance deep learning method, an accuracy of 88.6% was obtained <ref type="bibr" target="#b22">[22]</ref>. As a result of the detection of musculoskeletal abnormalities performed by Dias, the highest accuracy was obtained as 81.98% with the SqueezeNet model <ref type="bibr" target="#b23">[23]</ref>. An accuracy of 98.20% was achieved by Khan and Kim using spark machine learning and conv-autoencoder to detect and classify unpredictable malicious attacks <ref type="bibr" target="#b24">[24]</ref>. Transcriptional co-activators have been identified by Kegelman et al as regulators of bone fracture healing <ref type="bibr" target="#b25">[25]</ref>. The bone fracture process performed by Sharma et al. with machine learning and digital geometry achieved 92% classification accuracy <ref type="bibr" target="#b26">[26]</ref>.</p><p>It is observed upon examination of the recent classification studies on shoulder bone in the literature that the normal and abnormal (fracture, edematous, Hill-Sachs lesion, degenerated, or torn) images obtained from CT, MRI, or X-ray devices are classified by not only the traditional machine learning methods such as the SVM but also by deep learning-based methods such as ResNet, NasNet, and CapsNet. In this study, the classification was performed on normal and abnormal shoulder bone X-ray (fracture) images in the MURA dataset. In the performance thereof, as a practice different from the literature, built MobileNetV2) with a varying structure and number of layers were used by transfer learning with pre-trained ImageNet by also adding SpinalNet to the classification layers. Based on the results of classification achieved by the procedures performed herein, another classification was performed with ensemble learning. The main reason for applying transfer learning on built CNN models, adding SpinalNet, and applying ensemble learning is to contribute to the efficiency of deep learning models proposed in the classification of shoulder bone X-ray images.</p><p>Section 3 discusses the CNN-based built deep learning models, such as ResNet, DenseNet, VGG, and InceptionV3, used in classification as well as their adaptations with Spinal FC and the proposed ensemble learning models. Section 4 explains the open source dataset, including the shoulder bone X-ray images, data augmentation, and data pre-processing procedures applied thereon, a table containing the accuracy, recall, F1-score, and Cohen's kappa scores achieved by classification models, and the results of classification, including a confusion matrix, ROC curves, and AUC scores. The last section interprets the results achieved by classification models and discusses the contribution of this study to the literature and improvements that can be applied in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>A number of different deep-learning-based methods have been used and developed to classify X-ray images of the shoulder bone in png format and with three channels as normal and abnormal in this study. Firstly, classification was performed with ResNet (34,50,101,152), ResNeXt (50,101), DenseNet (169,201), VGG <ref type="bibr" target="#b13">(13,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b19">19)</ref>, InceptionV3, and MobileNetV2, which are CNN-based deep learning models with different structures and layers that are publicly available. Subsequently, new classification networks were established by replacement of the classification layer of each model used herein with the Spi-nalNet FC layer. Based on the results obtained herein, new ensemble learning models specific to this study were developed in order to further increase the classification accuracy. Built deep learning models, newly developed models with SpinalNet, and details of the ensemble learning models used for classification are described in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Classification Models Based on CNNs for Shoulder Bone X-ray Images</head><p>The CNN-based deep learning models currently available were used to classify Xray images of the shoulder bone. In training the models, training was firstly carried out with data in the training section of the dataset without pre-training, i.e., with a random weight as the initial weight. However, the required level could not be reached in training the network or at the end of classification by this application. Therefore, the transfer learning method was applied.</p><p>In this transfer learning method, two versions of each available deep learning model pre-trained with ImageNet data, i.e., the existing weights in the pre-trained models, were used. ImageNet is a dataset and benchmark that contains millions of images and hundreds of object categories, on which the procedures of image classification, single-object localization, and object detection are mainly performed <ref type="bibr" target="#b27">[27]</ref>. The shoulder bone X-ray images dataset used in this study was used for input of the deep learning models, which are specified in <ref type="figure" target="#fig_1">Figure 2</ref> and were trained with ImageNet data. The structure comprised of 1000 classes in the last layer of these models was made to have two classes in order to classify bone images specific to our study as normal and abnormal. Following the abovementioned procedures, classification was carried out by training the network with the shoulder dataset. Moreover, the last layer of 13 built deep learning models were fine-tuned with SpinalNet / Spinal FC; with the newly acquired models, shoulder bone X-ray images were trained and classified. The currently available ResNet, ResNeXt, DenseNet, VGG, InceptionV3, and Mo-bileNetV2 deep learning models, which can be used in classification, along with versions with different numbers of layers were some of the models used in the classification of shoulder bone X-ray images in this study. The required method-based details regarding these models and SpinalNet are described in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">ResNet</head><p>ResNet is a CNN-based deep learning model including more than one building blocks of residual learning depending on the number of layers <ref type="bibr" target="#b28">[28]</ref>. In part (a) of the <ref type="figure" target="#fig_2">Figure  3</ref>, there are k number of building blocks with 3 × 3 conv for ResNet34, and, in part (b), there are m number of building blocks with 1 × 1 and 3 × 3 conv and n number of building blocks with 1 × 1 conv for ResNet50, 101, and 152, respectively. In this study, four ResNet models with 34, 50, 101, and 152 layers, respectively, were used in classification. The models were adapted, and the structure is shown in <ref type="figure" target="#fig_3">Figure 4</ref>.  ResNeXt is a deep learning model for deep neural networks that reduces the number of parameters in ResNet. With this model, cardinality, which is an additional dimension for the width and depth of ResNet, defining the size of the set of transformations, is used <ref type="bibr" target="#b29">[29]</ref>. The structure of the ResNext block is shown in <ref type="figure" target="#fig_4">Figure 5</ref>. In this structure, there is a block structure with a cardinality value of 32, with r number of 1 × 1 and 3 × 3 conv and s number of 1 × 1 conv, respectively. Two ResNeXt models with 50 and 101 layers, respectively, were used in classification in this study. The models were adapted, and their structure is shown in <ref type="figure" target="#fig_5">Figure 6</ref>. </p><formula xml:id="formula_0">(a) (b)</formula><formula xml:id="formula_1">(c) (d)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3.">DenseNet</head><p>In a dense convolutional neural network, known as DenseNet, each layer affects every other layer in a feed-forward fashion <ref type="bibr" target="#b30">[30]</ref>. A DenseNet block consists of five layers. The first four are dense layers, and the last is a transition layer. If the growth rate value is (k) for each layer, it is 4 for this dense block. In the transition layer, there is a 2 × 2 average pool with a 1 × 1 conv and a stride of 2. In the dense layer, there are 1 × 1 and 3 × 3 convs with a stride of 1.</p><p>In this study, two DenseNet models with 169 and 201 layers, respectively, and a growth rate (k) of 32 were used in classification. These models were adapted, and their structure is shown in <ref type="figure" target="#fig_6">Figure 7</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4.">VGG</head><p>VGG is a CNN-based deep learning model with very small (3 × 3) convolution filters <ref type="bibr" target="#b31">[31]</ref>. In this study, three VGG models with 13, 16, and 19 layers, respectively, were used for the classification procedure. The structure of conv blocks was comprised of a combination of 2, 3, or 4 consecutive 3 × 3 conv (with k, m or n filters) layers in <ref type="figure" target="#fig_7">Figure 8</ref>, and these VGG-based models, which were used and adapted from a combination thereof, are shown in <ref type="figure" target="#fig_8">Figure 9</ref>.</p><p>Accepted at Applied Sciences, MDPI, 2021, 11 <ref type="formula">(6)</ref>    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.5.">InceptionV3</head><p>InceptionV3 is a CNN-based deep learning model that includes convolution factorization, which is called the inception module <ref type="bibr" target="#b32">[32]</ref>. The structure is shown in <ref type="figure" target="#fig_0">Figure 10</ref>. In this study, the InceptionV3 model was used in classification. This model was adapted, and its structure is shown in <ref type="figure" target="#fig_0">Figure 11</ref>.  <ref type="figure" target="#fig_0">Figure 11</ref>. The structure of the adapted InceptionV3-based model <ref type="bibr" target="#b32">[32]</ref>.</p><formula xml:id="formula_2">(a) (b) (c) (d) (e)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.6.">MobileNetV2</head><p>MobileNetV2 is a deep learning model that includes residual bottleneck layers <ref type="bibr" target="#b33">[33]</ref>. The structure of the convolution block of this model is shown in <ref type="figure" target="#fig_0">Figure 12</ref>. The MobileNetV2 model was used in classification in this study. The structure of this MobileNetV2-based model, which contains a total of 17 MobileNet bottleneck blocks, comprised of 13 bottlenecks with a stride value of 1, and 4 bottlenecks with a stride value of 2, is shown in <ref type="figure" target="#fig_0">Figure 13</ref>.</p><p>Accepted at Applied Sciences, MDPI, 2021, 11 <ref type="formula">(6)</ref>  <ref type="figure" target="#fig_0">Figure 13</ref>. The structure of the adapted MobileNetV2-based model <ref type="bibr" target="#b33">[33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">SpinalNet</head><p>In SpinalNet, the structure of the hidden layers is different from a normal neural network model. In a neural network, the hidden layers receive inputs in the first layer and then transfer the intermediate outputs to the next layer. However, the hidden layer in SpinalNet enables the previous layer to receive a certain part of its inputs and outputs. Therefore, the number of incoming weights in the hidden layer is lower than in normal neural networks <ref type="bibr" target="#b34">[34]</ref>.</p><p>The classification layer of the 13 above-mentioned CNN models based on ResNet, ResNeXt, DenseNet, VGG, InceptionV3, and MobileNetV2, used in classification, was adapted with SpinalNet / Spinal FC, and classification was also carried out with Spinal FC versions of these models. This allowed observation of the effect of SpinalNet on the classification of shoulder bone X-ray images. The details regarding this effect are described in Section 4. Moreover, the layer widths in Spinal FC of each classification model are as follows. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Proposed Classification Models Based on EL for Shoulder Bone X-Ray Images</head><p>Two ensemble models were established to further increase the accuracy of classification by examining the classification results of shoulder bone X-ray images performed with the CNN-based models and their SpinalNet versions used in the study, mentioned in the previous subsections. While choosing the sub-models required for ensemble learning, the accuracy rates in the classification results of each model, the confusion matrix scores, and the (normal / abnormal) AUC scores for each class were taken into consideration. The details of these two ensemble-based classification models are explained in the following subsections. It is observed upon examination of the figure above of the EL1 model proposed for classification of shoulder bone X-ray images as normal/abnormal (fracture) that three different CNN-based models are used. The "data" specified in this figure represent the entire dataset of shoulder bone X-ray images, "Model I" is the ResNeXt50 model with SpinalNet FC, "Model II" is the DenseNet169 model with a Standard FC layer, "Model III" is the DenseNet201 model with SpinalNet FC, "Outputs 1-3" are the outputs of these three models, and "Prediction" is the normal/abnormal (fracture) class types. In selection of the three models specified in this ensemble model, the parameters in the training outputs of the 26 CNN models presented in the previous title were taken into consideration. These parameters can be stated as Cohen's kappa score, AUC, and test accuracy. The steps of the procedure and a block diagram in <ref type="figure" target="#fig_0">Figure 15</ref> for the proposed EL1 ensemble model are provided as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>Step 1: The last layers of the three pre-trained sub-models in the EL1 ensemble model are adjusted as the Identity layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>Step 2: A final layer with 80 outputs for ResNext50 with Spinal FC, 1664 for Dense-Net169 with Standard FC, and 960 for DenseNet201 with Spinal FC is achieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>Step 3: These outputs are combined to form a single linear layer and a hidden layer with 2704 outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>Step 4: This hidden layer is connected to the classifying layer connected to the sigmoid activation function, the output of which is 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>Step 5: The network established as a result of these procedures is re-trained, providing results.    The structure of the EL2 model is explained in Algorithm 1. The block diagram of the proposed EL2 ensemble model is shown in <ref type="figure" target="#fig_0">Figure 17</ref>.  <ref type="figure" target="#fig_0">Figure 17</ref>. A block diagram of the proposed EL2 ensemble model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 EL2</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this study, CNN-based deep learning (DL) models were used to classify shoulder bone X-ray images as abnormal (indicating fracture). X-ray images of the shoulder bone in the open source MURA dataset were used. Data pre-processing/augmentation was performed first. Following this, classification was carried out using 13 different CNN models, SpinalNet versions of these models, and two ensemble learning models. The proposed classification models for normal/abnormal (fracture) shoulder bone X-ray images are shown in <ref type="figure" target="#fig_0">Figure 18</ref>.  <ref type="figure" target="#fig_0">Figure 18</ref>. The proposed models for classification of shoulder bone X-ray images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset of Shoulder Bone X-Ray Images</head><p>The MURA dataset is one of the largest among the published open-source radiography datasets. It contains finger, elbow, wrist, hand, forearm, humerus, and shoulder bone X-ray images <ref type="bibr" target="#b1">[2]</ref>. In this study, only the shoulder bone X-ray images within the MURA dataset were used mainly because it is the most balanced type in the MURA dataset in terms of distribution of the amount of data provided for both training and validation. This balanced distribution is presented in <ref type="figure" target="#fig_0">Figure 19</ref>. A balanced dataset can otherwise be obtained with data augmentation or synthetic data generation to avoid problems that may arise when working with an imbalanced dataset. Although the MURA dataset is an open-source dataset, only the training and validation datasets are publicly available. The classification models used in the first study and in studies conducted in competition conducted with the MURA dataset were tested using test data that are not publicly available. Due to the confidential nature of the test data and the inability to conduct testing with these data as other studies have, the validation data was used as test data in this study. <ref type="figure" target="#fig_0">Figure 19</ref>. The shoulder bone X-ray dataset <ref type="bibr" target="#b1">[2]</ref>.</p><p>These images, which initially had different resolutions and three channels, were first pre-processed and then converted to 320 × 320 × 3 pixels before being used for the deep learning model. The size 320 × 320 × 3 was chosen because it is the resolution most compatible with other studies using this dataset. The data formats are png, and no alterations were made in terms of the format type. The image type of the shoulder bone X-ray images, the number of training images, the number of test images, and the original and new image sizes are provided in the table below.  <ref type="table">Table 2</ref>. Details of the shoulder bone X-ray images used in the study <ref type="bibr" target="#b1">[2]</ref>. The table above shows that the image sizes were originally different but later converted to 320 × 320 × 3 in size. All images are in png format. The original quantity of normal images was 4211 in the training section and 285 in the test section, and that of abnormal images was 4168 in the training section and 278 in the test section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shoulder Bone X-Ray Images Image Types Train Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Data Augmentation for Shoulder Bone X-Ray Images</head><p>Since the results obtained in deep learning, particularly in classification studies, are very sensitive to the dataset, and since an increase in the amount of data in the training stage of the network has a positive effect on the training of the network, the quantity of shoulder bone X-ray images in the MURA dataset used in this study was increased by data augmentation. For this procedure, new images were obtained by rotating the images to the right or to the left to a maximum of 10 degrees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Data Pre-Processing for Shoulder Bone X-Ray Images</head><p>There is both noise and a dark background in the images observed in this study, which may adversely affect the classification and/or fracture detection processes. Various pre-processing steps were applied to the dataset to eliminate the abovementioned adverse effects as much as possible and to obtain more accurate results in the classification process. These pre-processing steps are listed below:</p><p>• Detection of the Corresponding Area: Most of the X-ray images in the used dataset were insufficient in terms of semantic information in relation to the image size. In order to eliminate such insufficiency, the images were first converted to gray-scale and then subjected to double thresholding and to an adaptive threshold value determined using Otsu's thresholding value method. In the gray-scale images, the withinclass variance value corresponding to all possible threshold values for the two color classes assumed as background and foreground was calculated. The threshold value that made this variance the smallest was the optimal threshold value. This method is known as Otsu's thresholding value method <ref type="bibr" target="#b35">[35]</ref>. Subsequently, the edge in the thresholded image was determined using edge detection methods. After this process, the original image was cropped based on the calculated values. • CLAHE Transformation: In the next step, the contrast-limited adaptive histogram equalization (CLAHE) transformation in the OpenCV library was used. In the transformation used, the input image was divided into parts by the user, with each part containing a histogram within itself. The histogram of each part was then adjusted based on the histogram cropping limit entered by the user, and all parts were finally brought together to obtain a Clahe-transformed version of the input image <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b37">37]</ref>. New outputs were achieved by contrast equalization of the cropped images by the CLAHE method. • Normalization and Standardization: In the last step, the images were normalized and standardized using the image-net values. For each class (normal/abnormal (fracture)), an original shoulder bone X-ray image and an image subjected to the abovementioned pre-processing steps are provided as examples in <ref type="figure" target="#fig_1">Figure 20</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Classification Results</head><p>Online servers were used as hardware in the process of classification of shoulder bone X-ray images in this study. All classification processes were carried out on Google Colab with nVIDIA Tesla T4 16GB GDDR6 graphics cards. In addition, the torchvision, seaborn, scikitlearn, matplotlib, and torch packages were used. The program codes written using the PyTorch deep learning library are publicly available at https://github.com/fatihuysal88/shoulder-c (accessed on 1 March 2021). A learning rate of 0.0001, an epoch number of 40, the Adam optimizer, and the cross-entropy loss function were used in all classification procedures performed with the ResNet, ResNeXt, Dense-Net, VGG, InceptionV3, and MobileNetV2 deep learning models. While the same learning rate, epoch number, optimizer, and loss function were used in the versions with SpinalNet of these models, Spinal FC was used in the classification layer. The learning rate value initially used with these models was not fixed, and it was decreased 10 times every 10 epochs to increase network learning success. The same parameters were used in the ensemble learning models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1.">Evaluation Metrics</head><p>In order to evaluate the results obtained in classification problems accurately and completely, the following must be obtained for each class: accuracy percentage, confusion matrix, precision, recall, F1-score from each classification model, ROC curves, and AUC scores. The accuracy percentage refers to what percentage of test data was correctly classified. The confusion matrix provides a table of the current status in the dataset and the number of correct and incorrect predictions in our classification model. This table contains true positive (TP), true negative (TN), false positive (FP), and false negative (FN) values. Precision refers to values predicted as positive. Recall is defined as the true positive rate. F1-score is the harmonic mean of precision and recall values. Cohen's kappa coefficient is a statistical method used to measure the reliability of agreement between two raters. The p0 value used in this calculation refers to the result of accuracy, and the pe value refers to the probability of random agreement. The following tables contain information on what TP, TN, FP, FN, and the confusion matrix refer to and how the accuracy, precision, recall, F1-score, Cohen's-kappa score, ROC curve, and AUC scores are calculated.              <ref type="figure" target="#fig_0">Figure 21</ref> shows that the highest training accuracy in classification is achieved by VGG13 among models with Standard FC and by InceptionV3 among models with Spinal FC. The test accuracy in <ref type="figure" target="#fig_1">Figure 22</ref>, the precision in <ref type="figure" target="#fig_1">Figure 23</ref>, the recall in <ref type="figure" target="#fig_1">Figure 24</ref>, the F1-score in <ref type="figure" target="#fig_1">Figure 25</ref>, and Cohen's kappa score in <ref type="figure" target="#fig_1">Figure 26</ref> show that the highest scores are achieved by DenseNet169 among models with Standard FC and by DenseNet201 among models with Spinal FC.</p><p>The confusion matrix values and AUC scores achieved as a result of 26 classification procedures, performed with DenseNet, Inception, MobileNet, ResNet, ResNeXt, and VGG  <ref type="table" target="#tab_9">Table 13</ref>. The table shows that the highest classification accuracy (TP value) out of the 278 Class 1 (positive, abnormal, fracture) images in the test data is achieved by the VGG13 model with Spinal FC, with 231 images. Moreover, out of 285 images in Class 0 (negative, normal) in the test data, the highest classification accuracy (TN value) was achieved by the VGG13 model with Standard FC, with 258 images. The highest AUC score in Class 1 is 0.8797, and the highest score in Class 0 is 0.8809, and both were achieved by the DenseNet169 model with Standard FC.</p><p>In addition to the 26 classification models that were used for the classification of shoulder bone X-ray images, two different ensemble models were developed, which further improved the classification results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3.">Classification Results of Our Ensemble Models</head><p>ResNeXt50 with Spinal FC, DenseNet169 with Standard FC, and DenseNet201 with Spinal FC were used in EL1:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>ResNeXt50 with Spinal FC was selected because the AUC score achieved by detecting fracture images is the second highest, with 0.8783, after DenseNet169 with Standard FC.   The ROC curve representing the change in the TP rate and FP rate of the EL1 model entails an AUC score of 0.8862 for Class 0 (normal) and Class 1 (abnormal, fracture) in <ref type="figure" target="#fig_1">Figure 28</ref>. This AUC score is also higher than the AUC achieved with ResNet169 with Spinal FC in the previous section.</p><p>Three different single classification models and a sub-ensemble model were used in the EL2 model: ResNet34 with Spinal FC, DenseNet201, DenseNet169 with Standard FC, and sub-ensemble models. The TP and TN values in the confusion matrix scores and recall scores from the 26 different classification models used in the first stage were taken into consideration when choosing the three single models used in the EL2 model and developing the sub-ensemble model. The results of the classification carried out with the EL2 model are provided in <ref type="table" target="#tab_9">Table 15</ref>     The ROC curves and AUC scores achieved as a result of the classification performed with the EL2 model are provided in <ref type="figure" target="#fig_2">Figure 30</ref>. The AUC scores are 0.8698 for Class 0 (normal, negative) and 0.8695 for Class 1 (abnormal, positive, fracture). Although these AUC scores appear to be slightly decreased compared to the EL1 model, since the test accuracy, Cohen's kappa scores, and the number of detected shoulder bone fracture images (TP) in the EL2 model are higher than the EL1 model, the EL2 model overall provides better results.</p><p>The test accuracy values obtained in classification studies vary depending on the amount and type of test data. In other classification studies on shoulder images in the MURA dataset, different approaches are used to obtain test data. Therefore, the comparison of classification results of the 28 models used in this study will be more appropriate. Comparison with the classification results obtained in other studies may be misleading due to the differences in the test data. Considering this, below is a table of other important shoulder bone classification studies available in the literature, regarding the dataset, amount, method used, and classification accuracies.  <ref type="bibr" target="#b19">[19]</ref> 1006 MR images CapsNet Normal/degenerated/torn: 0.9474</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions and Future Work</head><p>The aim of this study was to find the most optimal model for classifying shoulder bone X-ray images as normal or abnormal (fracture). In order to achieve this aim, the first objective was to classify fracture images using 26 classification models, i.e., Standard FC and Spinal FC versions of 13 different CNN-based models. Based on the results obtained therein, two different ensemble models (EL1 and EL2) were developed and used for classification procedures in order to further increase classification accuracy and Cohen's kappa score. Among the 28 different classifications models, the highest test accuracy and Cohen's kappa score were achieved in the EL2 model, and the highest AUC score was achieved in the EL1 model. In the EL1 model, the fully connected layers of the models trained on the dataset were combined and turned into a single fully connected layer. This new layer was linked to the classification layer. In this way, in the last training part performed in the EL1 model, the weights in the new fully connected layer were updated, and the best configuration of the three models was obtained. For this reason, the EL1 model can be used on different datasets because it updates the weights of the new FC layer for the best configuration on any dataset. In the EL2 model, the sub-models consulted in the decision mechanism consist of models that show the best performance on the dataset. For this reason, if one is working on their own dataset, they should determine their sub-models according to the performance results obtained with transfer learning. The reason for using transfer learning is the further improvement of classifications made with models that have not been pre-trained. With this approach, a serious increase in the classification problem was observed. Ensemble learning was mainly used to further improve the classification results obtained with 26 different deep learning models and to bring new approaches to the literature in this field. The contribution of the study to the literature is as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>In similar studies, binary classification is mostly performed. However, while there are mainly two classes (normal / abnormal) in this study, differently from the literature, multi-class classification was carried out, in order to determine the most compatible models to be used in ensemble models, developed by evaluating the outputs of each class of the 26 classification models initially used. This allowed the best results in this study to be achieved with ensemble models. • This is the first time that Spinal FC, which, compared to the Standard FC, has a lower number of weights in the hidden layer, was used in many models (Inception, Res-NeXt, and MobileNet). Moreover, SpinalNet was used on medical images for the first time, and it had a positive effect on more than half of the classification results. • A unique structure is introduced, since the reliability of the detection of classes was used as a basis when designing the EL2 model, which further improves classification results.</p><p>The aim of this study, focused on the detection of shoulder bone fractures in X-ray images, is to help physicians diagnose fractures in the shoulder and apply the required treatment. Following this study, a real-time mobile application that detects fractures and/or fracture areas in shoulder bones should be developed, specifically to help physicians performing emergency services. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>The anatomy of the shoulder bone.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Accepted at Applied Sciences, MDPI, 2021, 11(6), 2723. Uysal, F.; Hardalaç, F.; Peker, O.; Tolunay, T.; Tokgöz, N. Classification of Shoulder X-ray Images with Deep Learning Ensemble Models. for citation and final version please click here: https://doi.org/10.3390/app11062723 Standard FC/Spinal FC CNN-based deep learning methods subject to transfer learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Accepted at Applied Sciences, MDPI, 2021, 11(6), 2723. Uysal, F.; Hardalaç, F.; Peker, O.; Tolunay, T.; Tokgöz, N. Classification of Shoulder X-ray Images with Deep Learning Ensemble Models. for citation and final version please click here: https://doi.org/10.3390/app11062723 ResNet building block. (a) BB-I, k; (b) BB-II, m, n [28].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>The structure of the adapted ResNet-based models. (a) ResNet34; (b) ResNet50; (c) Res-Net101; (d) ResNet152.3.1.2. ResNeXt [28].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>ResNeXt building block (BB-III, r, s, C = 32)<ref type="bibr" target="#b29">[29]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Accepted at Applied Sciences, MDPI, 2021, 11(6), 2723. Uysal, F.; Hardalaç, F.; Peker, O.; Tolunay, T.; Tokgöz, N. Classification of Shoulder X-ray Images with Deep Learning Ensemble Models. for citation and final version please click here: https://doi.org/10.3390/app11062723 The structure of the adapted ResNeXt-based models. (a) ResNeXt50 (32 × 4d); (b) Res-NeXt101 (32 × 8d)<ref type="bibr" target="#b29">[29]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Accepted at Applied Sciences, MDPI, 2021, 11(6), 2723. Uysal, F.; Hardalaç, F.; Peker, O.; Tolunay, T.; Tokgöz, N. Classification of Shoulder X-ray Images with Deep Learning Ensemble Models. for citation and final version please click here: https://doi.org/10.3390/app11062723 The structure of the adapted DenseNet-based models. (a) DenseNet169; (b) DenseNet201 [30].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>The conv blocks of 2, 3, and 4 3 × 3 conv layers (CB-I, II, and III, respectively). (a) CB-I, k; (b) CB-II, m; (c) CB-III, n [31].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 .</head><label>9</label><figDesc>The structure of the adapted VGG-based models. (a) VGG13; (b) VGG16; (c) VGG19<ref type="bibr" target="#b31">[31]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 .</head><label>10</label><figDesc>The inception blocks containing convolution factorization (IB-A, B, C, D, and E). (a) IB-A; (b) IB-B; (c) IB-C; (d) IB-D; (e) IB-E [32].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 12 .</head><label>12</label><figDesc>MobileNet bottleneck blocks where stride = 1 and stride = 2, respectively (MB-I and -II). (a) MB-I; (b) MB-II [33].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 14 .</head><label>14</label><figDesc>3.3.1. EL1 (ResNext50 with Spinal FC, DenseNet169 with Standard FC, and DenseNet201 with Spinal FC) The EL1 (Ensemble learning-1) model was developed using a combination of Res-Next50 with Spinal FC, DenseNet169 with Standard FC, and DenseNet201 with Spinal FC. A schematic diagram of the developed model is shown in Figure 14. Accepted at Applied Sciences, MDPI, 2021, 11(6), 2723. Uysal, F.; Hardalaç, F.; Peker, O.; Tolunay, T.; Tokgöz, N. Classification of Shoulder X-ray Images with Deep Learning Ensemble Models. for citation and final version please click here: https://doi.org/10.3390/app11062723 The proposed EL1 classification model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 15 .• Part 1 : 2 : 6 :</head><label>15126</label><figDesc>A block diagram of the proposed EL1 ensemble model. 3.3.2. EL2 (ResNet34 with Spinal FC, DenseNet169 with Standard FC, DenseNet201 with Spinal FC, and ResNext50 with Spinal FC) The EL2 (Ensemble learning-2) model was developed using a combination of Res-Net34 with Spinal FC, DenseNet169 with Standard FC, DenseNet201 with Spinal FC, and a sub-ensemble model. A schematic diagram of the developed model is provided in Figure 16. It is observed upon examination of the figure that the EL2 model, differently from the EL1 model, consists of three single models and one sub-ensemble model and has a different structure. The "X-ray Images" specified in the figure refers to the dataset containing shoulder bone images, "Model I" to the ResNet34 model with Spinal FC, "Model II" to the DenseNet201 model with Spinal FC, "Model III" to the sub-ensemble model, "Model IV" to the DenseNet169 model with Standard FC, and "Predictions I-IV" refer to the outputs of the classification performed with these four models. The parts of the procedure for the proposed EL2 ensemble model are as follows: Accepted at Applied Sciences, MDPI, 2021, 11(6), 2723. Uysal, F.; Hardalaç, F.; Peker, O.; Tolunay, T.; Tokgöz, N. Classification of Shoulder X-ray Images with Deep Learning Ensemble Models. for citation and final version please click here: https://doi.org/10.3390/app11062723In the "Input" section, there is a shoulder X-ray image dataset that has been subjected to certain image processing techniques. • Part In the "Classification Network" section, the models that establish our ensemble model are defined. There are three single sub-models and a sub-ensemble model that constitute our ensemble model therein. Our sub-ensemble model is an architecture trained by connecting the predicted outputs of ResNet34 with Spinal FC, Dense-Net201, ResNeXt50, and DenseNet169 with Standard FC to a linear layer with eight inputs and two outputs. The evaluation of 26 models as single was effective in the selection of these four models. • Part 3: In the "Prediction" sections, there are normal / abnormal (fracture) class type outputs achieved as a result of the classification performed in the previous section. • Part 4: In the "Main Check" section, there is a main check mechanism that plays a role in determining the class of the input image. Therein, Models I and II suggest classifying the input image as abnormal in the final classification, while Models III and IV suggest classifying the input image as normal in the final classification. In cases where suggestions are not available, the classification is carried out with the sub-ensemble model. In the selection of the referred models for each class, Confusion Matrix and Recall parameters previously obtained for the 26 CNN models were taken into consideration. • Part 5: In the "Sub Check" section, there is a supplementary check mechanism under the main check mechanism. The aim here is to use the classification result of a model (Model IV for Class 1 (abnormal) and Model II for Class 0 (normal)) other than the two models referred to in the main check section as a supplement for the final classification process. • Part In the "Final Prediction" section, the final output determined as a result of the check mechanisms is achieved.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 16 .</head><label>16</label><figDesc>The proposed EL2 classification model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>Accepted at Applied Sciences, MDPI, 2021, 11(6), 2723. Uysal, F.; Hardalaç, F.; Peker, O.; Tolunay, T.; Tokgöz, N. Classification of Shoulder X-ray Images with Deep Learning Ensemble Models. for citation and final version please click here: https://doi.org/10.3390/app11062723</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 20 .</head><label>20</label><figDesc>Original and pre-processed X-ray images of normal / abnormal (fractured) shoulder bone. (a) original normal; (b) pre-processed normal; (c) original abnormal; (d) pre-processed abnormal [2]. In the figure above, section a shows an original version of a normal (negative) shoulder bone X-ray image; section b shows a cropped and CLAHE pre-processed version of this normal image; section c shows an original version of an abnormal (positive, fractured) shoulder bone X-ray image; section d shows a cropped and CLAHE pre-processed version of this abnormal image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>P0</head><label></label><figDesc>(TP + TN)/(TP + TN + FP + FN) Ppositive (TP + FP)(TP + FN)/(TP + TN + FP + FN) 2 Pnegative (FN + TN)(FP + TN)/(TP + TN + FP + FN) 2 PePpositive + Pnegative</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 21 .</head><label>21</label><figDesc>Training accuracy results of classification models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 23 .</head><label>23</label><figDesc>Precision results of classification models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 24 .</head><label>24</label><figDesc>Recall results of classification models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 26 .</head><label>26</label><figDesc>Cohen's kappa results of classification models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 27 .Figure 28 .</head><label>2728</label><figDesc>Confusion matrix results of EL1. The confusion matrix specified in Figure 27 was a result of the classification performed using the EL1 model. The following results are concluded as per the figure: TN: 249, FP: 36, FN: 51, and TP: 227. Accepted at Applied Sciences, MDPI, 2021, 11(6), 2723. Uysal, F.; Hardalaç, F.; Peker, O.; Tolunay, T.; Tokgöz, N. Classification of Shoulder X-ray Images with Deep Learning Ensemble Models. for citation and final version please click here: https://doi.org/10.3390/app11062723 ROC curves and AUC results of EL1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 29 .</head><label>29</label><figDesc>Confusion matrix results of EL2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Figure 29</head><label>29</label><figDesc>presents the confusion matrix scores achieved as a result of the classification carried out with the EL2 model: TN: 248, FP: 36, FN: 49 and TP: 229. For comparison, the TP in the EL1 model was 227.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Figure 30 .</head><label>30</label><figDesc>ROC curves and AUC results of EL2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>Author Contributions:</head><label></label><figDesc>Conceptualization, F.U., F.H., and O.P.; methodology, F.U., F.H., and O.P.; software, F.U., F.H., and O.P.; validation, F.U., F.H., and O.P.; formal analysis, F.U., F.H., and O.P.; investigation, F.U., F.H., and O.P.; resources, F.U., F.H., and O.P.; data curation, F.U., F.H., and O.P.; writing-original draft preparation, F.U., F.H., and O.P.; writing-review and editing, F.U., F.H., O.P., T.T., and N.T.; visualization, F.U., F.H., and O.P.; supervision, F.U., F.H., and O.P. All authors have read and agreed to the published version of the manuscript.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 1 .</head><label>1</label><figDesc>Layer with values of Spinal FCs used in classification models.</figDesc><table><row><cell>Models</cell><cell>Spinal FC Layer Width</cell><cell>Models</cell><cell>Spinal FC Layer Width</cell></row><row><cell>ResNet34</cell><cell>256</cell><cell>ResNeXt50</cell><cell>20</cell></row><row><cell>ResNet50</cell><cell>128</cell><cell>ResNeXt101</cell><cell>128</cell></row><row><cell>ResNet101,152</cell><cell>1024</cell><cell cols="2">DenseNet169, 201 240</cell></row><row><cell>VGG13</cell><cell>256</cell><cell>MobileNetV2</cell><cell>320</cell></row><row><cell>VGG16,19</cell><cell>512</cell><cell>InceptionV3</cell><cell>20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 3 .Table 4 .Table 5 .</head><label>345</label><figDesc>TP, TN, FP, and FN calculations. Confusion matrix. Calculation of Cohen's kappa score parameters.</figDesc><table><row><cell></cell><cell cols="2">Label Value</cell><cell>Prediction Value</cell></row><row><cell>TP</cell><cell>positive</cell><cell></cell><cell>positive</cell></row><row><cell>TN</cell><cell>negative</cell><cell></cell><cell>negative</cell></row><row><cell>FP</cell><cell>positive</cell><cell></cell><cell>negative</cell></row><row><cell>FN</cell><cell>negative</cell><cell></cell><cell>positive</cell></row><row><cell></cell><cell></cell><cell cols="2">Predicted Class Negative</cell><cell>Predicted Class Positive</cell></row><row><cell cols="3">Actual Class: Negative TN</cell><cell>FP</cell></row><row><cell cols="2">Actual Class: Positive</cell><cell>FN</cell><cell>TP</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 6 .</head><label>6</label><figDesc>Evaluation metrics. F1-score, and Cohen's kappa scores obtained as the result of the classification performed with each model are presented in the following figures and tables.</figDesc><table><row><cell>Confusion Matrix</cell><cell>TP, TN, FP, FN</cell></row><row><cell>Training/Testing Accuracy</cell><cell>(TP + TN)/(TP + TN + FP + FN)</cell></row><row><cell>Precision</cell><cell>TP/(TP + FP)</cell></row><row><cell>Recall</cell><cell>TP/(TP + FN)</cell></row><row><cell>F1-score</cell><cell>2TP/(2TP + FP + FN)</cell></row><row><cell>Cohen's kappa score</cell><cell>(p0 − pe)/(1 − pe)</cell></row><row><cell>ROC curve</cell><cell>TP rate-FP rate change</cell></row><row><cell>AUC scores</cell><cell>Area under the ROC curve</cell></row><row><cell cols="2">4.4.2. Classification Results of 13 CNN-Based Deep Learning Models with Standard</cell></row><row><cell>FC/Spinal FC</cell><cell></cell></row><row><cell cols="2">The training accuracy, test accuracy, precision, recall,</cell></row></table><note>Accepted at Applied Sciences, MDPI, 2021, 11(6), 2723. Uysal, F.; Hardalaç, F.; Peker, O.; Tolunay, T.; Tokgöz, N. Classification of Shoulder X-ray Images with Deep Learning Ensemble Models. for citation and final version please click here: https://doi.org/10.3390/app11062723</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 7 .</head><label>7</label><figDesc>Training accuracy results of classification models. Test accuracy results of classification models.</figDesc><table><row><cell>Models</cell><cell>Standart FC</cell><cell>Spinal Net</cell><cell>Models</cell><cell>Standart FC</cell><cell>Spinal Net</cell></row><row><cell>DenseNet169</cell><cell>0.8883</cell><cell>0.8666</cell><cell>ResNet101</cell><cell>0.8419</cell><cell>0.8225</cell></row><row><cell>DenseNet201</cell><cell>0.8934</cell><cell>0.8694</cell><cell>ResNet152</cell><cell>0.845</cell><cell>0.8346</cell></row><row><cell>InceptionV3</cell><cell>0.8882</cell><cell>0.8914</cell><cell>ResNeXt50</cell><cell>0.8707</cell><cell>0.8654</cell></row><row><cell>MobileNetV2</cell><cell>0.8647</cell><cell>0.8845</cell><cell>ResNeXt101</cell><cell>0.8539</cell><cell>0.8303</cell></row><row><cell>ResNet34</cell><cell>0.8673</cell><cell>0.8517</cell><cell>VGG13</cell><cell>0.9389</cell><cell>0.8507</cell></row><row><cell>ResNet50</cell><cell>0.8489</cell><cell>0.8395</cell><cell>VGG16</cell><cell>0.8698</cell><cell>0.812</cell></row><row><cell></cell><cell></cell><cell></cell><cell>VGG19</cell><cell>0.9055</cell><cell>0.8011</cell></row></table><note>Accepted at Applied Sciences, MDPI, 2021, 11(6), 2723. Uysal, F.; Hardalaç, F.; Peker, O.; Tolunay, T.; Tokgöz, N. Classification of Shoulder X-ray Images with Deep Learning Ensemble Models. for citation and final version please click here: https://doi.org/10.3390/app11062723</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 8 .</head><label>8</label><figDesc>Test accuracy results of classification models.</figDesc><table><row><cell>Models</cell><cell>Standart FC</cell><cell>Spinal Net</cell><cell>Models</cell><cell>Standart FC</cell><cell>Spinal Net</cell></row><row><cell>DenseNet169</cell><cell>0.8419</cell><cell>0.8152</cell><cell>ResNet101</cell><cell>0.817</cell><cell>0.8188</cell></row><row><cell>DenseNet201</cell><cell>0.8206</cell><cell>0.8294</cell><cell>ResNet152</cell><cell>0.8117</cell><cell>0.817</cell></row><row><cell>InceptionV3</cell><cell>0.8259</cell><cell>0.817</cell><cell>ResNeXt50</cell><cell>0.817</cell><cell>0.8241</cell></row><row><cell>MobileNetV2</cell><cell>0.8241</cell><cell>0.8099</cell><cell>ResNeXt101</cell><cell>0.8206</cell><cell>0.8082</cell></row><row><cell>ResNet34</cell><cell>0.8188</cell><cell>0.8206</cell><cell>VGG13</cell><cell>0.7797</cell><cell>0.8223</cell></row><row><cell>ResNet50</cell><cell>0.8188</cell><cell>0.8081</cell><cell>VGG16</cell><cell>0.785</cell><cell>0.801</cell></row><row><cell></cell><cell></cell><cell></cell><cell>VGG19</cell><cell>0.785</cell><cell>0.8046</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 9 .</head><label>9</label><figDesc>Precision results of classification models.</figDesc><table><row><cell>85%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>84%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>83%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>82%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>81%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>80%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>79%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>78%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>77%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>76%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>75%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DenseNet-169</cell><cell>DenseNet-201</cell><cell>Inception-v3</cell><cell>MobileNet-v2</cell><cell>ResNet-34</cell><cell>ResNet-50</cell><cell>ResNet-101</cell><cell>ResNet-152</cell><cell>ResNext-50</cell><cell>ResNext-101</cell><cell>VGG-13</cell><cell>VGG-16</cell><cell>VGG-19</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Standart FC</cell><cell cols="2">Spinal FC</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Models</cell><cell></cell><cell cols="3">Standart FC</cell><cell cols="2">Spinal Net</cell><cell cols="2">Models</cell><cell></cell><cell></cell><cell cols="2">Standart FC</cell><cell>Spinal Net</cell></row><row><cell cols="2">DenseNet169</cell><cell cols="2">0.845</cell><cell></cell><cell cols="2">0.815</cell><cell cols="3">ResNet101</cell><cell></cell><cell cols="2">0.815</cell><cell>0.82</cell></row><row><cell cols="2">DenseNet201</cell><cell cols="2">0.82</cell><cell></cell><cell>0.83</cell><cell></cell><cell cols="3">ResNet152</cell><cell></cell><cell cols="2">0.815</cell><cell>0.82</cell></row><row><cell>InceptionV3</cell><cell></cell><cell cols="2">0.825</cell><cell></cell><cell>0.82</cell><cell></cell><cell cols="3">ResNeXt50</cell><cell></cell><cell>0.82</cell><cell></cell><cell>0.825</cell></row><row><cell cols="2">MobileNetV2</cell><cell cols="2">0.83</cell><cell></cell><cell>0.81</cell><cell></cell><cell cols="3">ResNeXt101</cell><cell></cell><cell>0.82</cell><cell></cell><cell>0.815</cell></row><row><cell>ResNet34</cell><cell></cell><cell cols="2">0.825</cell><cell></cell><cell>0.82</cell><cell></cell><cell cols="2">VGG13</cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell>0.825</cell></row><row><cell>ResNet50</cell><cell></cell><cell cols="2">0.815</cell><cell></cell><cell>0.81</cell><cell></cell><cell cols="2">VGG16</cell><cell></cell><cell></cell><cell>0.79</cell><cell></cell><cell>0.805</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">VGG19</cell><cell></cell><cell></cell><cell cols="2">0.785</cell><cell>0.81</cell></row></table><note>Accepted at Applied Sciences, MDPI, 2021, 11(6), 2723. Uysal, F.; Hardalaç, F.; Peker, O.; Tolunay, T.; Tokgöz, N. Classification of Shoulder X-ray Images with Deep Learning Ensemble Models. for citation and final version please click here: https://doi.org/10.3390/app11062723</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Table 10 .</head><label>10</label><figDesc>Recall results of classification models.</figDesc><table><row><cell>Models</cell><cell>Standart FC</cell><cell>Spinal Net</cell><cell>Models</cell><cell>Standart FC</cell><cell>Spinal Net</cell></row><row><cell>DenseNet169</cell><cell>0.84</cell><cell>0.815</cell><cell>ResNet101</cell><cell>0.815</cell><cell>0.82</cell></row><row><cell>DenseNet201</cell><cell>0.815</cell><cell>0.83</cell><cell>ResNet152</cell><cell>0.81</cell><cell>0.815</cell></row><row><cell>InceptionV3</cell><cell>0.825</cell><cell>0.815</cell><cell>ResNeXt50</cell><cell>0.815</cell><cell>0.825</cell></row><row><cell>MobileNetV2</cell><cell>0.82</cell><cell>0.81</cell><cell>ResNeXt101</cell><cell>0.815</cell><cell>0.815</cell></row><row><cell>ResNet34</cell><cell>0.815</cell><cell>0.82</cell><cell>VGG13</cell><cell>0.78</cell><cell>0.825</cell></row><row><cell>ResNet50</cell><cell>0.82</cell><cell>0.81</cell><cell>VGG16</cell><cell>0.785</cell><cell>0.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell>VGG19</cell><cell>0.785</cell><cell>0.805</cell></row></table><note>Accepted at Applied Sciences, MDPI, 2021, 11(6), 2723. Uysal, F.; Hardalaç, F.; Peker, O.; Tolunay, T.; Tokgöz, N. Classification of Shoulder X-ray Images with Deep Learning Ensemble Models. for citation and final version please click here: https://doi.org/10.3390/app11062723</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head>Table 11 .</head><label>11</label><figDesc>F1-score results of classification models.</figDesc><table><row><cell>Models</cell><cell>Standart FC</cell><cell>Spinal Net</cell><cell>Models</cell><cell>Standart FC</cell><cell>Spinal Net</cell></row><row><cell>DenseNet169</cell><cell>0.84</cell><cell>0.815</cell><cell>ResNet101</cell><cell>0.82</cell><cell>0.82</cell></row><row><cell>DenseNet201</cell><cell>0.82</cell><cell>0.83</cell><cell>ResNet152</cell><cell>0.81</cell><cell>0.815</cell></row><row><cell>InceptionV3</cell><cell>0.825</cell><cell>0.815</cell><cell>ResNeXt50</cell><cell>0.815</cell><cell>0.825</cell></row><row><cell>MobileNetV2</cell><cell>0.82</cell><cell>0.81</cell><cell>ResNeXt101</cell><cell>0.82</cell><cell>0.815</cell></row><row><cell>ResNet34</cell><cell>0.82</cell><cell>0.82</cell><cell>VGG13</cell><cell>0.775</cell><cell>0.82</cell></row><row><cell>ResNet50</cell><cell>0.815</cell><cell>0.805</cell><cell>VGG16</cell><cell>0.785</cell><cell>0.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell>VGG19</cell><cell>0.785</cell><cell>0.805</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head>Table 12 .</head><label>12</label><figDesc>Cohen's kappa results of classification models.</figDesc><table><row><cell>Models</cell><cell>Standart FC</cell><cell>Spinal Net</cell><cell>Models</cell><cell>Standart FC</cell><cell>Spinal Net</cell></row><row><cell>DenseNet169</cell><cell>0.6834</cell><cell>0.6302</cell><cell>ResNet101</cell><cell>0.634</cell><cell>0.6372</cell></row><row><cell>DenseNet201</cell><cell>0.641</cell><cell>0.6588</cell><cell>ResNet152</cell><cell>0.6231</cell><cell>0.6332</cell></row><row><cell>InceptionV3</cell><cell>0.6514</cell><cell>0.6338</cell><cell>ResNeXt50</cell><cell>0.6338</cell><cell>0.648</cell></row><row><cell>MobileNetV2</cell><cell>0.6478</cell><cell>0.6195</cell><cell>ResNeXt101</cell><cell>0.641</cell><cell>0.6267</cell></row><row><cell>ResNet34</cell><cell>0.6372</cell><cell>0.6411</cell><cell>VGG13</cell><cell>0.558</cell><cell>0.6442</cell></row><row><cell>ResNet50</cell><cell>0.6375</cell><cell>0.6161</cell><cell>VGG16</cell><cell>0.5695</cell><cell>0.6014</cell></row><row><cell></cell><cell></cell><cell></cell><cell>VGG19</cell><cell>0.5698</cell><cell>0.6085</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_26"><head></head><label></label><figDesc>Accepted at Applied Sciences, MDPI, 2021, 11(6), 2723. Uysal, F.; Hardalaç, F.; Peker, O.; Tolunay, T.; Tokgöz, N. Classification of Shoulder X-ray Images with Deep Learning Ensemble Models. for citation and final version please click here: https://doi.org/10.3390/app11062723 models with Standard FC and Spinal FC with different numbers of layers, are provided in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_27"><head>Table 13 .</head><label>13</label><figDesc>Confusion matrix values and AUC scores of 26 classification models (a: with Standard FC, b: with Spinal FC).</figDesc><table><row><cell>Models</cell><cell>TP</cell><cell>FP</cell><cell>FN</cell><cell>TN</cell><cell cols="2">Class0: AUC Class1: AUC</cell></row><row><cell>DenseNet169a</cell><cell>222</cell><cell>33</cell><cell>56</cell><cell>252</cell><cell>0.8809</cell><cell>0.8797</cell></row><row><cell>DenseNet169b</cell><cell>218</cell><cell>44</cell><cell>60</cell><cell>241</cell><cell>0.8602</cell><cell>0.8598</cell></row><row><cell>DenseNet201a</cell><cell>225</cell><cell>48</cell><cell>53</cell><cell>237</cell><cell>0.8584</cell><cell>0.8653</cell></row><row><cell>DenseNet201b</cell><cell>228</cell><cell>46</cell><cell>50</cell><cell>239</cell><cell>0.8727</cell><cell>0.8724</cell></row><row><cell>InceptionV3a</cell><cell>218</cell><cell>38</cell><cell>60</cell><cell>247</cell><cell>0.8754</cell><cell>0.8707</cell></row><row><cell>InceptionV3b</cell><cell>220</cell><cell>45</cell><cell>58</cell><cell>240</cell><cell>0.8582</cell><cell>0.8585</cell></row><row><cell>MobileNetV2a</cell><cell>215</cell><cell>36</cell><cell>63</cell><cell>249</cell><cell>0.8777</cell><cell>0.8378</cell></row><row><cell>MobileNetV2b</cell><cell>216</cell><cell>45</cell><cell>62</cell><cell>240</cell><cell>0.8633</cell><cell>0.861</cell></row><row><cell>ResNet34a</cell><cell>215</cell><cell>39</cell><cell>63</cell><cell>246</cell><cell>0.8705</cell><cell>0.8767</cell></row><row><cell>ResNet34b</cell><cell>228</cell><cell>51</cell><cell>50</cell><cell>234</cell><cell>0.8617</cell><cell>0.8619</cell></row><row><cell>ResNet50a</cell><cell>224</cell><cell>48</cell><cell>54</cell><cell>237</cell><cell>0.8715</cell><cell>0.8662</cell></row><row><cell>ResNet50b</cell><cell>219</cell><cell>49</cell><cell>59</cell><cell>236</cell><cell>0.8588</cell><cell>0.8584</cell></row><row><cell>ResNet101a</cell><cell>228</cell><cell>53</cell><cell>50</cell><cell>232</cell><cell>0.8683</cell><cell>0.8703</cell></row><row><cell>ResNet101b</cell><cell>216</cell><cell>40</cell><cell>62</cell><cell>245</cell><cell>0.8609</cell><cell>0.861</cell></row><row><cell>ResNet152a</cell><cell>217</cell><cell>45</cell><cell>61</cell><cell>240</cell><cell>0.8648</cell><cell>0.8701</cell></row><row><cell>ResNet152b</cell><cell>220</cell><cell>45</cell><cell>58</cell><cell>240</cell><cell>0.8597</cell><cell>0.8606</cell></row><row><cell>ResNeXt50a</cell><cell>221</cell><cell>46</cell><cell>57</cell><cell>239</cell><cell>0.8644</cell><cell>0.8699</cell></row><row><cell>ResNeXt50b</cell><cell>223</cell><cell>44</cell><cell>55</cell><cell>241</cell><cell>0.8789</cell><cell>0.8783</cell></row><row><cell>ResNeXt101a</cell><cell>225</cell><cell>48</cell><cell>53</cell><cell>237</cell><cell>0.8772</cell><cell>0.8765</cell></row><row><cell>ResNeXt101b</cell><cell>219</cell><cell>46</cell><cell>59</cell><cell>239</cell><cell>0.8652</cell><cell>0.8561</cell></row><row><cell>VGG13a</cell><cell>181</cell><cell>27</cell><cell>97</cell><cell>258</cell><cell>0.8406</cell><cell>0.8415</cell></row><row><cell>VGG13b</cell><cell>231</cell><cell>35</cell><cell>65</cell><cell>250</cell><cell>0.8705</cell><cell>0.8737</cell></row><row><cell>VGG16a</cell><cell>203</cell><cell>46</cell><cell>75</cell><cell>239</cell><cell>0.8517</cell><cell>0.8523</cell></row><row><cell>VGG16b</cell><cell>204</cell><cell>38</cell><cell>74</cell><cell>247</cell><cell>0.8542</cell><cell>0.857</cell></row><row><cell>VGG19a</cell><cell>212</cell><cell>55</cell><cell>66</cell><cell>230</cell><cell>0.8374</cell><cell>0.8502</cell></row><row><cell>VGG19b</cell><cell>205</cell><cell>37</cell><cell>73</cell><cell>248</cell><cell>0.858</cell><cell>0.8539</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_28"><head></head><label></label><figDesc>DenseNet169 with Standard FC was selected because its classification had the highest test accuracy, Cohen's kappa score, and AUC score among all models used in this study. • DenseNet201 with Spinal FC was selected because the test accuracy and Cohen's kappa score were the second highest after DenseNet169 with Standard FC among all models and the highest among models with Spinal FC. The classification results of the EL1 model are provided in the following figures and tables. In Table 14, ResNeXt50b represents the ResNeXt50 model with Spinal FC, Dense-Net169a represents the DenseNet169 model with Standard FC, and DenseNet201b represents the DenseNet201 model with Spinal FC.</figDesc><table /><note>Accepted at Applied Sciences, MDPI, 2021, 11(6), 2723. Uysal, F.; Hardalaç, F.; Peker, O.; Tolunay, T.; Tokgöz, N. Classification of Shoulder X-ray Images with Deep Learning Ensemble Models. for citation and final version please click here: https://doi.org/10.3390/app11062723 •</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_29"><head>Table 14 .</head><label>14</label><figDesc>Classification results of EL1.The table above shows that, with the EL1 model, the test accuracy is 0.8455 and Cohen's kappa score is 0.6907. Therefore, the highest classification result obtained with DenseNet169 with Standard FC given in the previous section was exceeded.</figDesc><table><row><cell>Models</cell><cell cols="2">Test Acc. Pre.</cell><cell>Recall</cell><cell>F1-Score</cell><cell>Cohen's Kappa</cell></row><row><cell>ResNeXt50b</cell><cell>0.8241</cell><cell>0.825</cell><cell>0.825</cell><cell>0.825</cell><cell>0.648</cell></row><row><cell>DenseNet169a</cell><cell>0.8419</cell><cell>0.845</cell><cell>0.84</cell><cell>0.84</cell><cell>0.6834</cell></row><row><cell>DenseNet201b</cell><cell>0.8294</cell><cell>0.83</cell><cell>0.83</cell><cell>0.83</cell><cell>0.6588</cell></row><row><cell>EL1</cell><cell>0.8455</cell><cell>0.8631</cell><cell>0.8165</cell><cell>0.8455</cell><cell>0.6907</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_30"><head></head><label></label><figDesc>, where ResNet34b represents the ResNet34 model with Spinal FC, DenseNet169a represents the DenseNet169 model with Standard FC, Dense-Net201b represents the DenseNet201 model with Spinal FC, and SubEnsemble represents the developed sub-ensemble model.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_31"><head>Table 15 .</head><label>15</label><figDesc>Classification results of EL2.The table shows that Cohen's kappa is 0.6942, and the test accuracy score is 0.8472. Therefore, the EL2 model outperforms the 26 CNN-based models and the EL1 model.</figDesc><table><row><cell>Models</cell><cell>Test Acc.</cell><cell>Pre.</cell><cell>Recall</cell><cell cols="2">F1-score Cohen's Kappa</cell></row><row><cell>ResNet34b</cell><cell>0.8206</cell><cell>0.82</cell><cell>0.82</cell><cell>0.82</cell><cell>0.6411</cell></row><row><cell>DenseNet169a</cell><cell>0.8419</cell><cell>0.815</cell><cell>0.84</cell><cell>0.84</cell><cell>0.6834</cell></row><row><cell>DenseNet201b</cell><cell>0.8294</cell><cell>0.83</cell><cell>0.83</cell><cell>0.83</cell><cell>0.6588</cell></row><row><cell>SubEnsemble</cell><cell>0.8401</cell><cell>0.84</cell><cell>0.84</cell><cell>0.84</cell><cell>0.6799</cell></row><row><cell>EL2</cell><cell>0.8472</cell><cell>0.85</cell><cell>0.845</cell><cell>0.845</cell><cell>0.6942</cell></row></table><note>Accepted at Applied Sciences, MDPI, 2021, 11(6), 2723. Uysal, F.; Hardalaç, F.; Peker, O.; Tolunay, T.; Tokgöz, N. Classification of Shoulder X-ray Images with Deep Learning Ensemble Models. for citation and final version please click here: https://doi.org/10.3390/app11062723</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_32"><head></head><label></label><figDesc>Accepted at Applied Sciences, MDPI, 2021, 11(6), 2723. Uysal, F.; Hardalaç, F.; Peker, O.; Tolunay, T.; Tokgöz, N. Classification of Shoulder X-ray Images with Deep Learning Ensemble Models. for citation and final version please click here: https://doi.org/10.3390/app11062723</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_33"><head>Table 16 .</head><label>16</label><figDesc>Comparison with previous studies.</figDesc><table><row><cell>Studies</cell><cell>Dataset</cell><cell>Best Method</cell><cell>Classification Type and Test Accuracy</cell></row><row><cell>Our study</cell><cell>MURA Shoulder, 563 validation/test images, open data</cell><cell>EL2</cell><cell>Fracture/normal: 0.8472</cell></row><row><cell>Liang and Gu [5]</cell><cell>MURA dataset, 194 validation images</cell><cell>GCN</cell><cell>Fracture/normal: 0.9112</cell></row><row><cell></cell><cell>MURA dataset, various test</cell><cell></cell><cell></cell></row><row><cell>Saif et al. [6]</cell><cell>images, %50 test data, test quantity</cell><cell cols="2">Capsule Network Fracture/normal: 0.9208</cell></row><row><cell></cell><cell>not specified</cell><cell></cell><cell></cell></row><row><cell>Chung et al. [15]</cell><cell>1376 CT images</cell><cell>ResNet152</cell><cell>Normal/four different type: 0.96</cell></row><row><cell>Urban et al. [17]</cell><cell>597 X-ray images</cell><cell>NASNet</cell><cell>Implant: 0.804</cell></row><row><cell>Sezers [16]</cell><cell>219 MR images</cell><cell>CNN</cell><cell>Normal/edematous/Hill-Sachs lesions: 0.9843</cell></row><row><cell>Sezers [18]</cell><cell>219 MR images</cell><cell>Extreme learning machines</cell><cell>Normal/edematous/Hill-Sachs lesions: 0.94</cell></row><row><cell>Sezers</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflicts of Interest:</head><p>The authors declare no conflict of interest.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<ptr target="https://www.assh.org/handcare/condition/shoulder-fracture" />
		<title level="m">Shoulder Fracture</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>accessed on 1 September 2020</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Towards Radiologist-Level Abnormality Detection in Musculoskeletal Radiographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Irvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bagul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Ball</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Conference on Medical Imaging with Deep Learning</title>
		<meeting>the 1st Conference on Medical Imaging with Deep Learning<address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="4" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Arm fracture detection in X-rays based on improved deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Electr. Eng</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Automatic Recognition of Elbow Musculoskeletal Disorders using Cloud Application</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Galal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hisham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ghanim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nabil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 8th International Conference on Software and Information Engineering</title>
		<meeting>the 2019 8th International Conference on Software and Information Engineering<address><addrLine>Cairo, Egypt</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-04" />
			<biblScope unit="page" from="9" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Towards Robust and Accurate Detection of Abnormalities in Musculoskeletal Radiographs with a Multi-Network Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page">3153</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Abnormality Detection in Musculoskeletal Radiographs Using Capsule Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F M</forename><surname>Saif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shahnaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">P</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">O</forename><surname>Ahmad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="81494" to="81503" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adversarial Policy Gradient for Deep Learning Image Augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Iriondo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Calivá</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krogue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Majumdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pedoia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Medical Image Computing and Computer Assisted Intervention</title>
		<meeting>the 22nd International Conference on Medical Image Computing and Computer Assisted Intervention<address><addrLine>Shenzhen, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-10" />
			<biblScope unit="page" from="13" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Branding-Fusion of Meta Data and Musculoskeletal Radiographs for Multi-modal Diagnostic Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nensa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision Workshop</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision Workshop<address><addrLine>Seoul, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-10-28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Automated abnormality detection in lower extremity radiographs using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dunnmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Khandwala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Beaulieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shpanskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="578" to="583" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Musculoskeletal radiographs classification using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Harini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ramji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sriram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sowmya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Soman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning for Data Analytics: Foundations, Biomedical Applications and Challenges</title>
		<editor>Das, H., Pradhan, C.</editor>
		<editor>Dey, N.</editor>
		<meeting><address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="79" to="98" />
		</imprint>
	</monogr>
	<note>1st ed</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Iterative fusion convolutional neural networks for classification of optical coherence tomography images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Vis. Commun. Image Represent</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="327" to="333" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Classification of Shoulder X-ray Images with Deep Learning Ensemble Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Uysal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hardalaç</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Peker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tolunay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tokgöz</surname></persName>
		</author>
		<idno type="DOI">10.3390/app11062723</idno>
		<ptr target="https://doi.org/10.3390/app11062723" />
	</analytic>
	<monogr>
		<title level="j">Accepted at Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">2723</biblScope>
			<date type="published" when="2021" />
			<publisher>MDPI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep CNN-Based Ensemble CADx Model for Musculoskeletal Abnormality Detection from Radiographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Mondol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hashem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 5th International Conference on Advances in Electrical Engineering</title>
		<meeting>the 2019 5th International Conference on Advances in Electrical Engineering<address><addrLine>Dhaka, Bangladesh</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-09-28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Classification of Human Bones Using Deep Convolutional Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Dhaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chaudhary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IOP Conference Series: Materials Science and Engineering, International Conference on Startup Ventures: Technology Developments and Future Strategies</title>
		<meeting>the IOP Conference Series: Materials Science and Engineering, International Conference on Startup Ventures: Technology Developments and Future Strategies<address><addrLine>Rajasthan, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-10-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A Two Stage Method for Abnormality Diagnosis of Musculoskeletal Radiographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the International Conference on Pattern Recognition and Artificial Intelligence</title>
		<meeting>eeding of the International Conference on Pattern Recognition and Artificial Intelligence<address><addrLine>Zhongshan, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-10" />
			<biblScope unit="page" from="19" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Automated detection and classification of the proximal humerus fracture by using deep learning algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">R</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Orthop</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="468" to="473" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Convolutional neural network based diagnosis of bone pathologies of proximal humerus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sezer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">B</forename><surname>Sezer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="124" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Classifying shoulder implants in X-ray images using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Urban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Porhemmat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Feeley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Okada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Baldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Struct. Biotechnol. J</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="967" to="972" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Shoulder lesion classification using shape and texture features via composite kernel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sezer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">O</forename><surname>Sigirci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">B</forename><surname>Sezer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th Signal Processing and Communications Applications Conference (SIU)</title>
		<meeting>the 25th Signal Processing and Communications Applications Conference (SIU)<address><addrLine>Antalya</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-05" />
			<biblScope unit="page" from="15" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Capsule network-based classification of rotator cuffpathologies from MRI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sezer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">B</forename><surname>Sezer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Electr. Eng</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page">106480</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cardiac Arrhythmia Disease Classification Using LSTM Deep Learning Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Mater. Contin</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="427" to="443" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adaptive bone abnormality detection in medical imagery using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Storey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R F</forename><surname>Mtope</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International FLINS Conference</title>
		<meeting>the 14th International FLINS Conference<address><addrLine>Cologne, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-08" />
			<biblScope unit="page" from="18" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-instance Deep Learning with Graph Convolutional Neural Networks for Diagnosis of Kidney Diseases Using Ultrasound Imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Furth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Tasian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Uncertainty for Safe Utilization of Machine Learning in Medical Imaging</title>
		<meeting>the International Workshop on Uncertainty for Safe Utilization of Machine Learning in Medical Imaging<address><addrLine>Lima, Peru</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-10-17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Musculoskeletal Abnormality Detection on X-Ray Using Transfer Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D A</forename><surname>Dias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pompeu Fabra University</title>
		<imprint>
			<date type="published" when="2019-07" />
		</imprint>
	</monogr>
	<note type="report_type">Master&apos;s Thesis</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Toward Developing Efficient Conv-AE-Based Intrusion Detection System Using Heterogeneous Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">1771</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">YAP and TAZ Promote Periosteal Osteoblast Precursor Expansion and Differentiation for Fracture Repair</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Kegelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Nijsure</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Moharrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">B</forename><surname>Pearson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Dawahare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Boerckel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Bone Miner. Res</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="143" to="157" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bone Fractured Detection Using Machine Learning and Digital Geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mobile Radio Communications and 5G Networks, Lecture Notes in Networks and Systems</title>
		<editor>Marriwala, N., Tripathi, C.C.</editor>
		<editor>Kumar, D., Jain, S.</editor>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
	<note>1nd ed</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="27" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Aggregated Residual Transformations for Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="21" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Densely Connected Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="21" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition. arXiv</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Rethinking the Inception Architecture for Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="27" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">MobileNetV2: Inverted Residuals and Linear Bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition<address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="18" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M D</forename><surname>Kabir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M J</forename><surname>Jalali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Atiya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nahavandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Spinalnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.03347</idno>
		<title level="m">Deep neural networkwith gradual input</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A Threshold Selection Method from Gray-Level Histograms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Otsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst. Man Cybern</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="62" to="66" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Contrast-limited adaptive histogram equalization: Speed and effectiveness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Pizer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Ericksen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Yankaskas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Muller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Visualization in Biomedical Computing</title>
		<meeting>the First Conference on Visualization in Biomedical Computing<address><addrLine>Atlanta, GA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1990-05" />
			<biblScope unit="page" from="22" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Contrast limited adaptive histogram equalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zuiderveld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Graphics Gems IV</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
