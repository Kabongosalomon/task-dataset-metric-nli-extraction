<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Compound Probabilistic Context-Free Grammars for Grammar Induction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
							<email>yoonkim@seas.harvard.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Harvard University Cambridge</orgName>
								<address>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
							<email>cdyer@google.com</email>
							<affiliation key="aff1">
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
							<email>srush@seas.harvard.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">Harvard University Cambridge</orgName>
								<address>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Compound Probabilistic Context-Free Grammars for Grammar Induction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We study a formalization of the grammar induction problem that models sentences as being generated by a compound probabilistic context-free grammar.</p><p>In contrast to traditional formulations which learn a single stochastic grammar, our grammar's rule probabilities are modulated by a per-sentence continuous latent variable, which induces marginal dependencies beyond the traditional context-free assumptions. Inference in this grammar is performed by collapsed variational inference, in which an amortized variational posterior is placed on the continuous variable, and the latent trees are marginalized out with dynamic programming. Experiments on English and Chinese show the effectiveness of our approach compared to recent state-of-theart methods when evaluated on unsupervised parsing.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Grammar induction is the task of inducing hierarchical syntactic structure from data. Statistical approaches to grammar induction require specifying a probabilistic grammar (e.g. formalism, number and shape of rules), and fitting its parameters through optimization. Early work found that it was difficult to induce probabilistic context-free grammars (PCFG) from natural language data through direct methods, such as optimizing the log likelihood with the EM algorithm <ref type="bibr" target="#b45">(Lari and Young, 1990;</ref><ref type="bibr" target="#b8">Carroll and Charniak, 1992)</ref>. While the reasons for the failure are manifold and not completely understood, two major potential causes are the ill-behaved optimization landscape and the overly strict independence assumptions of PCFGs. More successful approaches to grammar induction have thus resorted to carefully-crafted auxiliary objectives <ref type="bibr" target="#b39">(Klein and Manning, 2002)</ref>, priors or Code: https://github.com/harvardnlp/compound-pcfg non-parametric models <ref type="bibr" target="#b44">(Kurihara and Sato, 2006;</ref><ref type="bibr" target="#b31">Johnson et al., 2007;</ref><ref type="bibr" target="#b48">Liang et al., 2007;</ref><ref type="bibr" target="#b82">Wang and Blunsom, 2013)</ref>, and manually-engineered features <ref type="bibr" target="#b28">(Huang et al., 2012;</ref><ref type="bibr" target="#b25">Golland et al., 2012)</ref> to encourage the desired structures to emerge.</p><p>We revisit these aforementioned issues in light of advances in model parameterization and inference. First, contrary to common wisdom, we find that parameterizing a PCFG's rule probabilities with neural networks over distributed representations makes it possible to induce linguistically meaningful grammars by simply optimizing log likelihood. While the optimization problem remains non-convex, recent work suggests that there are optimization benefits afforded by over-parameterized models <ref type="bibr" target="#b1">(Arora et al., 2018;</ref><ref type="bibr" target="#b86">Xu et al., 2018;</ref><ref type="bibr" target="#b21">Du et al., 2019)</ref>, and we indeed find that this neural PCFG is significantly easier to optimize than the traditional PCFG. Second, this factored parameterization makes it straightforward to incorporate side information into rule probabilities through a sentence-level continuous latent vector, which effectively allows different contexts in a derivation to coordinate. In this compound PCFG-continuous mixture of PCFGs-the context-free assumptions hold conditioned on the latent vector but not unconditionally, thereby obtaining longer-range dependencies within a tree-based generative process.</p><p>To utilize this approach, we need to efficiently optimize the log marginal likelihood of observed sentences. While compound PCFGs break efficient inference, if the latent vector is known the distribution over trees reduces to a standard PCFG. This property allows us to perform grammar induction using a collapsed approach where the latent trees are marginalized out exactly with dynamic programming. To handle the latent vector, we employ standard amortized inference using reparameterized samples from a variational posterior approximated from an inference network <ref type="bibr" target="#b37">(Kingma and Welling, 2014;</ref><ref type="bibr" target="#b64">Rezende et al., 2014)</ref>.</p><p>On standard benchmarks for English and Chinese, the proposed approach is found to perform favorably against recent neural approaches to unsupervised parsing <ref type="bibr" target="#b70">(Shen et al., 2018</ref><ref type="bibr" target="#b71">(Shen et al., , 2019</ref><ref type="bibr" target="#b20">Drozdov et al., 2019;</ref><ref type="bibr" target="#b34">Kim et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Probabilistic Context-Free Grammars</head><p>We consider context-free grammars (CFG) consisting of a 5-tuple G = (S, N , P, Σ, R) where S is the distinguished start symbol, N is a finite set of nonterminals, P is a finite set of preterminals, 1 Σ is a finite set of terminal symbols, and R is a finite set of rules of the form,</p><formula xml:id="formula_0">S → A, A ∈ N A → B C, A ∈ N , B, C ∈ N ∪ P T → w, T ∈ P, w ∈ Σ.</formula><p>A probabilistic context-free grammar (PCFG) consists of a grammar G and rule probabilities π = {π r } r∈R such that π r is the probability of the rule r. Letting T G be the set of all parse trees of G, a PCFG defines a probability distribution over t ∈ T G via p π (t) = r∈t R π r where t R is the set of rules used in the derivation of t. It also defines a distribution over string of terminals x ∈ Σ * via</p><formula xml:id="formula_1">p π (x) = t∈T G (x) p π (t),</formula><p>where T G (x) = {t | yield(t) = x}, i.e. the set of trees t such that t's leaves are x. We will slightly abuse notation and use p π (t | x) 1[yield(t) = x]p π (t) p π (x) to denote the posterior distribution over the unobserved latent trees given the observed sentence x, where 1[·] is the indicator function. 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Parameterization</head><p>The standard way to parameterize a PCFG is to simply associate a scalar to each rule π r with the 1 Since we will be inducing a grammar directly from words, P is roughly the set of part-of-speech tags and N is the set of constituent labels. However, to avoid issues of label alignment, evaluation is only on the tree topology.</p><p>2 Therefore when used in the context of a posterior distribution conditioned on a sentence x, the variable t does not include the leaves x and only refers to the unobserved nonterminal/preterminal symbols. constraint that they form valid probability distributions, i.e. each nonterminal is associated with a fully-parameterized categorical distribution over its rules. This direct parameterization is algorithmically convenient since the M-step in the EM algorithm <ref type="bibr" target="#b19">(Dempster et al., 1977)</ref> has a closed form. However, there is a long history of work showing that it is difficult to learn meaningful grammars from natural language data with this parameterization <ref type="bibr" target="#b8">(Carroll and Charniak, 1992)</ref>. 3 Successful approaches to unsupervised parsing have therefore modified the model/learning objective by guiding potentially unrelated rules to behave similarly.</p><p>Recognizing that sharing among rule types is beneficial, we propose a neural parameterization where rule probabilities are based on distributed representations. We associate embeddings with each symbol, introducing input embeddings w N for each symbol N on the left side of a rule (i.e. N ∈ {S} ∪ N ∪ P). For each rule type r, π r is parameterized as follows,</p><formula xml:id="formula_2">π S→A = exp(u A f 1 (w S ) + b A ) A ∈N exp(u A f 1 (w S ) + b A ) , π A→BC = exp(u BC w A + b BC ) B C ∈M exp(u B C w A + b B C ) , π T →w = exp(u w f 2 (w T ) + b w ) w ∈Σ exp(u w f 2 (w T ) + b w ) ,</formula><p>where M is the product space (N ∪P)×(N ∪P), and f 1 , f 2 are MLPs with two residual layers. Note that we do not use an MLP for rules of the type π A→BC , as it did not empirically improve results. See appendix A.1 for the full parameterization. Going forward, we will use</p><formula xml:id="formula_3">E G = {w U | U ∈ {S} ∪ N ∪ P} ∪ {u V | V ∈ N ∪ M ∪ Σ}</formula><p>to denote the set of input/output symbol embeddings for grammar G, and λ to refer to the parameters of the neural network f 1 , f 2 used to obtain the rule probabilities. A graphical model-like illustration of the neural PCFG is shown in <ref type="figure">Figure 1</ref> (left). It is clear that the neural parameterization does not change the underlying probabilistic assumptions. The difference between the two is analogous to the difference between count-based vs. feed-forward neural language models, where feedforward neural language models make the same Markov assumptions as the count-based models but are able to take advantage of shared, distributed representations. <ref type="figure">Figure 1</ref>: A graphical model-like diagram for the neural PCFG (left) and the compound PCFG (right) for an example tree structure. In the above, A 1 , A 2 ∈ N are nonterminals, T 1 , T 2 , T 3 ∈ P are preterminals, w 1 , w 2 , w 3 ∈ Σ are terminals. In the neural PCFG, the global rule probabilities π = πS ∪ πN ∪ πP are the output from a neural net run over the symbol embeddings EG, where πN are the set of rules with a nonterminal on the left hand side (πS and πP are similarly defined). In the compound PCFG, we have per-sentence rule probabilities πz = πz,S ∪ πz,N ∪ πz,P obtained from running a neural net over a random vector z (which varies across sentences) and global symbol embeddings EG. In this case, the context-free assumptions hold conditioned on z, but they do not hold unconditionally: e.g. when conditioned on z and A2, the variables A1 and T1 are independent; however when conditioned on just A2, they are not independent due to the dependence path through z. Note that the rule probabilities are random variables in the compound PCFG but deterministic variables in the neural PCFG.</p><formula xml:id="formula_4">A 1 A 2 T 3 T 1 T 2 w 1 w 2 w 3 π S π N π P E G N A 1 A 2 T 3 T 1 T 2 w 1 w 2 w 3 z γ c π z,S π z,N π z,P E G N</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Compound PCFGs</head><p>A compound probability distribution <ref type="bibr" target="#b65">(Robbins, 1951)</ref> is a distribution whose parameters are themselves random variables. These distributions generalize mixture models to the continuous case, for example in factor analysis which assumes the following generative process, z ∼ N(0, I),</p><p>x ∼ N(Wz, Σ).</p><p>Compound distributions provide the ability to model rich generative processes, but marginalizing over the latent parameter can be computationally intractable unless conjugacy can be exploited.</p><p>In this work, we study compound probabilistic context-free grammars whose distribution over trees arises from the following generative process: we first obtain rule probabilities via</p><formula xml:id="formula_5">z ∼ p γ (z), π z = f λ (z, E G ),</formula><p>where p γ (z) is a prior with parameters γ (spherical Gaussian in this paper), and f λ is a neural network that concatenates the input symbol embeddings with z and outputs the sentence-level rule probabilities π z ,</p><formula xml:id="formula_6">π z,S→A ∝ exp(u A f 1 ([w S ; z]) + b A ), π z,A→BC ∝ exp(u BC [w A ; z] + b BC ), π z,T →w ∝ exp(u w f 2 ([w T ; z]) + b w ),</formula><p>where [w; z] denotes vector concatenation. Then a tree/sentence is sampled from a PCFG with rule probabilities given by π z ,</p><formula xml:id="formula_7">t ∼ PCFG(π z ), x = yield(t).</formula><p>This can be viewed as a continuous mixture of PCFGs, or alternatively, a Bayesian PCFG with a prior on sentence-level rule probabilities parameterized by z, λ, E G . 4 Importantly, under this generative model the context-free assumptions hold conditioned on z, but they do not hold unconditionally. This is shown in <ref type="figure">Figure 1</ref> (right) where there is a dependence path through z if it is not conditioned upon. Compound PCFGs give rise to a marginal distribution over parse trees t via</p><formula xml:id="formula_8">p θ (t) = p(t | z)p γ (z) dz,</formula><p>where p θ (t | z) = r∈t R π z,r . The subscript in π z,r denotes the fact that the rule probabilities depend on z. Compound PCFGs are clearly more expressive than PCFGs as each sentence has its own set of rule probabilities. However, it still assumes a tree-based generative process, making it possible to learn latent tree structures. One motivation for the compound PCFG is that simple, unlexicalized grammars (such as the PCFG we have been working with) are unlikely to represent an adequate model of natural language, although they do facilitate efficient learn-ing and inference. <ref type="bibr">5</ref> We can in principle model richer dependencies through vertical/horizontal Markovization <ref type="bibr" target="#b30">(Johnson, 1998;</ref><ref type="bibr" target="#b41">Klein and Manning, 2003)</ref> and lexicalization <ref type="bibr" target="#b18">(Collins, 1997)</ref>. However such dependencies complicate training due to the rapid increase in the number of rules. Under this view, we can interpret the compound PCFG as a restricted version of some lexicalized, higher-order PCFG where a child can depend on structural and lexical context through a shared latent vector. <ref type="bibr">6</ref> We hypothesize that this dependence among siblings is especially useful in grammar induction from words, where (for example) if we know that watched is used as a verb then the noun phrase is likely to be a movie.</p><p>In contrast to the usual Bayesian treatment of PCFGs which places priors on global rule probabilities <ref type="bibr" target="#b44">(Kurihara and Sato, 2006;</ref><ref type="bibr" target="#b31">Johnson et al., 2007;</ref><ref type="bibr" target="#b82">Wang and Blunsom, 2013)</ref>, the compound PCFG assumes a prior on local, sentence-level rule probabilities. It is therefore closely related to the Bayesian grammars studied by , who also sample local rule probabilities from a logistic normal prior for training dependency models with valence (DMV) <ref type="bibr" target="#b40">(Klein and Manning, 2004)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Inference in Compound PCFGs</head><p>The expressivity of compound PCFGs comes at a significant challenge in learning and inference. Letting θ = {E G , λ} be the parameters of the generative model, we would like to maximize the log marginal likelihood of the observed sentence log p θ (x). In the neural PCFG the log marginal likelihood</p><formula xml:id="formula_9">log p θ (x) = log t∈T G (x) p θ (t),</formula><p>5 A piece of evidence for the misspecification of unlexicalized first-order PCFGs as a statistical model of natural language is that if one pretrains such a PCFG on supervised data and continues training with the unsupervised objective (i.e. log marginal likelihood), the resulting grammar deviates significantly from the supervised initial grammar while the log marginal likelihood improves <ref type="bibr" target="#b31">(Johnson et al., 2007)</ref>. Similar observations have been made for part-of-speech induction with Hidden Markov Models <ref type="bibr" target="#b54">(Merialdo, 1994)</ref>. <ref type="bibr">6</ref> Note that the compound "PCFG" is a slight misnomer because the model is no longer context-free in the usual sense. Another interpretation of the model is to view it as a vectorized version of indexed grammars <ref type="bibr" target="#b0">(Aho, 1968)</ref>, which extend CFGs by augmenting nonterminals with additional index strings that may be inherited or modified during derivation. Compound PCFGs instead equip nonterminals with a continuous vector that is always inherited. can be obtained by summing out the latent tree structure using the inside algorithm <ref type="bibr" target="#b3">(Baker, 1979)</ref>, which is differentiable and thus amenable to gradient-based optimization. 7 In the compound PCFG, the log marginal likelihood is given by,</p><formula xml:id="formula_10">log p θ (x) = log p θ (x | z)p γ (z) dz = log t∈T G (x) p θ (t | z)p γ (z) dz .</formula><p>Notice that while the integral over z makes this quantity intractable, when we condition on z, we can tractably perform the inner summation to obtain p θ (x | z) using the inside algorithm. We therefore resort to collapsed amortized variational inference. We first obtain a sample z from a variational posterior distribution (given by an amortized inference network), then perform the inner marginalization conditioned on this sample. The evidence lower bound ELBO(θ, φ; x) is then,</p><formula xml:id="formula_11">E q φ (z | x) [log p θ (x | z)] − KL[q φ (z | x) p γ (z)],</formula><p>and we can calculate p θ (x | z) given a sample z from a variational posterior q φ (z | x). For the variational family we use a diagonal Gaussian where the mean/log-variance vectors are given by an affine layer over max-pooled hidden states from an LSTM over x. We can obtain low-variance estimators for ∇ θ,φ ELBO(θ, φ; x) by using the reparameterization trick for the expected reconstruction likelihood and the analytical expression for the KL term (Kingma and Welling, 2014). We remark that under the Bayesian PCFG view, since the parameters of the prior (i.e. θ) are estimated from the data, our approach can be seen as an instance of empirical Bayes <ref type="bibr" target="#b66">(Robbins, 1956</ref>). 8</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">MAP Inference</head><p>After training, we are interested in comparing the learned trees against an annotated treebank. This requires inferring the most likely tree given a sentence, i.e. argmax t p θ (t | x). For the neural PCFG we can obtain the most likely tree by using 7 In the context of the EM algorithm, directly performing gradient ascent on the log marginal likelihood is equivalent to performing an exact E-step (with the insideoutside algorithm) followed by a gradient-based M-step, <ref type="bibr" target="#b67">(Salakhutdinov et al., 2003;</ref><ref type="bibr" target="#b5">Berg-Kirkpatrick et al., 2010;</ref><ref type="bibr" target="#b24">Eisner, 2016).</ref> the Viterbi version of the inside algorithm (CKY algorithm). For the compound PCFG, the argmax is intractable to obtain exactly, and hence we estimate it with the following approximation,</p><formula xml:id="formula_12">i.e. ∇ θ log p θ (x) = E p θ (t | x) [∇ θ log p θ (t)]</formula><formula xml:id="formula_13">argmax t p θ (t | x, z)p θ (z | x) dz = argmax t p θ t | x, µ φ (x) ,</formula><p>where µ φ (x) is the mean vector from the inference network. The above approximates the true posterior p θ (z | x) with δ(z − µ φ (x)), the Dirac delta function at the mode of the variational posterior. 9 This quantity is tractable as in the PCFG case. Other approximations are possible: for example we could use q φ (z | x) as an importance sampling distribution to estimate the first integral. However we found the above approximation to be efficient and effective in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data</head><p>We test our approach on the Penn Treebank (PTB) <ref type="bibr" target="#b51">(Marcus et al., 1993)</ref> with the standard splits (2-21 for training, 22 for validation, 23 for test) and the same preprocessing as in recent works <ref type="bibr" target="#b70">(Shen et al., 2018</ref><ref type="bibr" target="#b71">(Shen et al., , 2019</ref>, where we discard punctuation, lowercase all tokens, and take the top 10K most frequent words as the vocabulary. This setup is more challenging than traditional setups, which usually experiment on shorter sentences and use gold partof-speech tags.</p><p>We further experiment on Chinese with version 5.1 of the Chinese Penn Treebank (CTB) <ref type="bibr" target="#b87">(Xue et al., 2005)</ref>, with the same splits as in <ref type="bibr" target="#b10">Chen and Manning (2014)</ref>. On CTB we also remove punctuation and keep the top 10K word types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Hyperparameters</head><p>Our PCFG uses 30 nonterminals and 60 preterminals, with 256-dimensional symbol embeddings. The compound PCFG uses 64-dimensional latent vectors. The bidirectional LSTM inference network has a single layer with 512 dimensions, and the mean and the log variance vector for q φ (z | x) are given by max-pooling the hidden states of the LSTM and passing it through an affine layer. Model parameters are initialized with Xavier uniform initialization. For training we use Adam (Kingma and <ref type="bibr" target="#b35">Ba, 2015)</ref> with β 1 = 0.75, β 2 = 0.999 and learning rate of 0.001, with a maximum gradient norm limit of 3. We train for 10 epochs with batch size equal to 4. We employ a curriculum learning strategy <ref type="bibr" target="#b4">(Bengio et al., 2009)</ref> where we train only on sentences of length up to 30 in the first epoch, and increase this length limit by 1 each epoch. Similar curriculum-based strategies have used in the past for grammar induction <ref type="bibr" target="#b76">(Spitkovsky et al., 2012)</ref>. During training we perform early stopping based on validation perplexity. 10 Finally, to mitigate against overfitting to PTB, experiments on CTB utilize the same hyperparameters from PTB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baselines and Evaluation</head><p>While we induce a full stochastic grammar (i.e. a distribution over symbolic rewrite rules) in this work, directly assessing the learned grammar is itself nontrivial. As a proxy, we adopt the usual approach and instead evaluate the induced grammar as an unsupervised parsing system. However, even in this setting we observe that there is enough variation across prior work on to render a meaningful comparison difficult.</p><p>In particular, some important dimensions along which prior works vary include, (1) input data: earlier work on generally assumed gold (or induced) part-of-speech tags <ref type="bibr" target="#b40">(Klein and Manning, 2004;</ref><ref type="bibr" target="#b73">Smith and Eisner, 2004;</ref><ref type="bibr" target="#b7">Bod, 2006;</ref><ref type="bibr" target="#b74">Snyder et al., 2009)</ref>, while more recent works induce grammar directly from words <ref type="bibr" target="#b77">(Spitkovsky et al., 2013;</ref><ref type="bibr" target="#b70">Shen et al., 2018)</ref>; (2) use of punctuation: even within papers that induce parse trees directly from words, some papers employ heuristics based on punctuation as punctuation is usually a strong signal for start/end of constituents <ref type="bibr" target="#b69">(Seginer, 2007;</ref><ref type="bibr" target="#b60">Ponvert et al., 2011;</ref><ref type="bibr" target="#b77">Spitkovsky et al., 2013)</ref>, some train with punctuation <ref type="bibr" target="#b29">(Jin et al., 2018;</ref><ref type="bibr" target="#b20">Drozdov et al., 2019;</ref><ref type="bibr" target="#b34">Kim et al., 2019)</ref>, while others discard punctuation altogether for training <ref type="bibr" target="#b70">(Shen et al., 2018</ref><ref type="bibr" target="#b71">(Shen et al., , 2019</ref>; (3) train/test data: some works do not explicitly separate out train/test sets <ref type="bibr" target="#b62">(Reichart and Rappoport, 2010;</ref><ref type="bibr" target="#b25">Golland et al., 2012)</ref> while some do <ref type="bibr" target="#b28">(Huang et al., 2012;</ref><ref type="bibr" target="#b56">Parikh et al., 2014;</ref><ref type="bibr" target="#b27">Htut et al., 2018)</ref>  less of an issue for unsupervised structure learning, however in this work we follow the latter and separate train/test data. (4) evaluation: for unlabeled F 1 , almost all works ignore punctuation (even approaches that use punctuation during training typically ignore them during evaluation), but there is some variance in discarding trivial spans (width-one and sentence-level spans) and using corpus-level versus sentence-level F 1 . 11 In this paper we discard trivial spans and evaluate on sentence-level F 1 per recent work <ref type="bibr" target="#b70">(Shen et al., 2018</ref><ref type="bibr" target="#b71">(Shen et al., , 2019</ref>. Given the above, we mainly compare our approach against two recent, strong baselines with open source code: Parsing Predict Reading Network (PRPN) 12 <ref type="bibr" target="#b70">(Shen et al., 2018)</ref> and Ordered Neurons (ON) 13 <ref type="bibr" target="#b71">(Shen et al., 2019)</ref>. These approaches train a neural language model with gated attention-like mechanisms to induce binary trees, and achieve strong unsupervised parsing performance even when trained on corpora where punctuation is removed. Since the original results were on both language modeling and unsupervised parsing, their hyperparameters were presumably tuned to do well on both and thus may not be optimal for just unsupervised parsing. We  therefore tune the hyperparameters of these baselines for unsupervised parsing only (i.e. on validation F 1 ). <ref type="table" target="#tab_1">Table 1</ref> shows the unlabeled F 1 scores for our models and various baselines. All models soundly outperform right branching baselines, and we find that the neural PCFG/compound PCFG are strong models for grammar induction. In particular the compound PCFG outperforms other models by an appreciable margin on both English and Chinese. We again note that we were unable to induce meaningful grammars through a traditional PCFG with the scalar parameterization despite a thorough hyperparameter search. 14 See appendix A.2 for the full results broken down by sentence length for sentence-and corpus-level F 1 . <ref type="table" target="#tab_3">Table 2</ref> analyzes the learned tree structures. We compare similarity as measured by F 1 against gold, left, right, and "self" trees (top), where self F 1 score is calculated by averaging over all 6 pairs obtained from 4 different runs. We find that PRPN is particularly consistent across multiple runs. We also observe that different models are better at identifying different constituent labels, as measured by label recall (  Finally, all models seemed to have some difficulty in identifying SBAR/VP constituents which typically span more words than NP constituents, indicating further opportunities for improvement on unsupervised parsing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Induced Trees for Downstream Tasks</head><p>While the compound PCFG has fewer independence assumptions than the neural PCFG, it is still a more constrained model of language than standard neural language models (NLM) and thus not competitive in terms of perplexity: the compound PCFG obtains a perplexity of 196.3 while an LSTM language model (LM) obtains 86.2 (Table 3). <ref type="bibr">15</ref> In contrast, both PRPN and ON perform as well as an LSTM LM while maintaining good unsupervised parsing performance.</p><p>We thus experiment to see if it is possible to use the induced trees to supervise a more flexible generative model that can make use of <ref type="bibr">15</ref> We did manage to almost match the perplexity of an NLM by additionally conditioning the terminal probabilities on previous history, i.e. πz,</p><formula xml:id="formula_14">T →w t ∝ exp(u w f2([wT ; z; ht]) + bw),</formula><p>where ht is the hidden state from an LSTM over x&lt;t. However the unsupervised parsing performance was far worse (≈ 25 F1 on the PTB). top based on predicted frequency (therefore NT-04 is the most frequently-predicted nonterminal). For each nonterminal we visualize the proportion of correctly-predicted constituents that correspond to particular gold labels. For reference we also show the precision (i.e. probability of correctly predicting unlabeled constituents) in the rightmost column.</p><p>tree structures-namely, recurrent neural network grammars (RNNG) <ref type="bibr" target="#b23">(Dyer et al., 2016)</ref>. RNNGs are generative models of language that jointly model syntax and surface structure by incrementally generating a syntax tree and sentence. As with NLMs, RNNGs make no independence assumptions, and have been shown to outperform NLMs in terms of perplexity and grammaticality judgment when trained on gold trees <ref type="bibr" target="#b42">(Kuncoro et al., 2018;</ref><ref type="bibr" target="#b84">Wilcox et al., 2019)</ref>. We take the best run from each model and parse the training set, 16 and use the induced trees to supervise an RNNG for each model using the parameterization from <ref type="bibr" target="#b34">Kim et al. (2019)</ref>. <ref type="bibr">17</ref> We are also interested in syntactic evaluation of our models, and for this we utilize the framework and dataset from <ref type="bibr" target="#b52">Marvin and Linzen (2018)</ref>, where a model is presented two minimally different sentences such as:</p><p>the senators near the assistant are old *the senators near the assistant is old and must assign higher probability to grammatical sentence. he retired as senior vice president finance and administration and chief financial officer of the company oct. N kenneth j. unk who was named president of this thrift holding company in august resigned citing personal reasons the former president and chief executive eric w. unk resigned in june unk 's president and chief executive officer john unk said the loss stems from several factors mr. unk is executive vice president and chief financial officer of unk and will continue in those roles charles j. lawson jr. N who had been acting chief executive since june N will continue as chairman unk corp. received an N million army contract for helicopter engines boeing co. received a N million air force contract for developing cable systems for the unk missile general dynamics corp. received a N million air force contract for unk training sets grumman corp. received an N million navy contract to upgrade aircraft electronics thomson missile products with about half british aerospace 's annual revenue include the unk unk missile family already british aerospace and french unk unk unk on a british missile contract and on an air-traffic control radar system meanwhile during the the s&amp;p trading halt s&amp;p futures sell orders began unk up while stocks in new york kept falling sharply but the unk of s&amp;p futures sell orders weighed on the market and the link with stocks began to fray again on friday some market makers were selling again traders said futures traders say the s&amp;p was unk that the dow could fall as much as N points meanwhile two initial public offerings unk the unk market in their unk day of national over-the-counter trading friday traders said most of their major institutional investors on the other hand sat tight <ref type="table">Table 4</ref>: For each query sentence (bold), we show the 5 nearest neighbors based on cosine similarity, where we take the representation for each sentence to be the mean of the variational posterior.</p><p>Additionally, <ref type="bibr" target="#b34">Kim et al. (2019)</ref> report perplexity improvements by fine-tuning an RNNG trained on gold trees with the unsupervised RNNG (URNNG)-whereas the RNNG is is trained to maximize the joint log likelihood log p(t), the URNNG maximizes a lower bound on the log marginal likelihood log t∈T G (x) p(t) with a structured inference network that approximates the true posterior. We experiment with a similar approach where we fine-tune RNNGs trained on induced trees with URNNGs. We perform early stopping for both RNNG and URNNG based on validation perplexity. See appendix A.3 for the full experimental setup.</p><p>The results are shown in <ref type="table" target="#tab_5">Table 3</ref>. For perplexity, RNNGs trained on induced trees (Induced RNNG in <ref type="table" target="#tab_5">Table 3</ref>) are unable to improve upon an LSTM LM, in contrast to the supervised RNNG which does outperform the LSTM language model (Table 3, bottom). For grammaticality judgment however, the RNNG trained with compound PCFG trees outperforms the LSTM LM despite obtaining worse perplexity, 18 and performs on par with the RNNG trained on binarized gold trees. Finetuning with the URNNG results in improvements in perplexity and grammaticality judgment across the board (Induced URNNG in <ref type="table" target="#tab_5">Table 3</ref>). We also obtain large improvements on unsupervised parsing as measured by F 1 , with the fine-tuned URN-NGs outperforming the respective original models. 19 This is potentially due to an ensembling effect between the original model and the URNNG's structured inference network, which is parameter- <ref type="bibr">18</ref>  <ref type="bibr" target="#b42">Kuncoro et al. (2018</ref> also observe that models that achieve lower perplexity do not necessarily perform better on syntactic evaluation tasks. <ref type="bibr">19</ref>  <ref type="bibr" target="#b46">Li et al. (2019)</ref> similarly obtain improvements by refining a model trained on induced trees on classification tasks. ized as a neural CRF constituency parser <ref type="bibr" target="#b22">(Durrett and Klein, 2015;</ref><ref type="bibr" target="#b50">Liu et al., 2018)</ref>. 20</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Model Analysis</head><p>We analyze our best compound PCFG model in more detail. Since we induce a full set of nonterminals in our grammar, we can analyze the learned nonterminals to see if they can be aligned with linguistic constituent labels. <ref type="figure" target="#fig_0">Figure 2</ref> visualizes the alignment between induced and gold labels, where for each nonterminal we show the empirical probability that a predicted constituent of this type will correspond to a particular linguistic constituent in the test set, conditioned on its being a correct constituent (for reference we also show the precision). We observe that some of the induced nonterminals clearly align to linguistic nonterminals. Further results, including preterminal alignments to part-ofspeech tags, 21 are shown in appendix A.4.</p><p>We next analyze the continuous latent space. <ref type="table">Table 4</ref> shows nearest neighbors of some sentences using the mean of the variational posterior as the continuous representation of each sentence. We qualitatively observe that the latent space seems to capture topical information. <ref type="bibr">20</ref> While left as future work, it is possible to use the compound PCFG itself as an inference network. Also note that the F1 scores for the URNNGs in <ref type="table" target="#tab_5">Table 3</ref> are optimistic since we selected the best-performing runs of the original models based on validation F1 to parse the training set. Finally, as noted by <ref type="bibr" target="#b34">Kim et al. (2019)</ref>, a URNNG trained from scratch fails to outperform a right-branching baseline on this version of PTB where punctuation is removed. <ref type="bibr">21</ref> As a POS induction system, the many-to-one performance of the compound PCFG using the preterminals is 68.0. A similarly-parameterized compound HMM with 60 hidden states (an HMM is a particularly type of PCFG) obtains 63.2. This is still quite a bit lower than the state-of-the-art <ref type="bibr" target="#b81">(Tran et al., 2016;</ref><ref type="bibr" target="#b26">He et al., 2018;</ref><ref type="bibr" target="#b79">Stratos, 2019)</ref>, though comparison is confounded by various factors such as preprocessing. PCto terminate their contract with warner to support a coup in panama to suit the bureaucrats in brussels to thwart his bid for amr to prevent the pound from rising PC + to change our strategy of investing to offset the growth of minimills to be a lot of art to change our way of life to increase the impact of advertising PCraise the minimum grant for smaller states veto a defense bill with inadequate funding avoid an imminent public or private injury field a competitive slate of congressional candidates alter a longstanding ban on such involvement PC + generate an offsetting profit by selling waves change an export loss to domestic plus expect any immediate problems with margin calls make a positive contribution to our earnings find a trading focus discouraging much participation <ref type="table">Table 5</ref>: For each subtree, we perform PCA on the variational posterior mean vectors that are associated with that particular subtree and take the top principal component. We then list the top 5 constituents that had the lowest (PC -) and highest (PC +) principal component values.</p><p>We are also interested in the variation in the leaves due to z when the variation due to the tree structure is held constant. To investigate this, we use the parsed dataset to obtain pairs of the form</p><formula xml:id="formula_15">(µ φ (x (n) ), t (n) j ), where t (n) j</formula><p>is the j-th subtree of the (approximate) MAP tree t (n) for the n-th sentence. Therefore each mean vector µ φ (x (n) ) is associated with |x (n) | − 1 subtrees, where |x (n) | is the sentence length. Our definition of subtree here ignores terminals, and thus each subtree is associated with many mean vectors. For a frequently occurring subtree, we perform PCA on the set of mean vectors that are associated with the subtree to obtain the top principal component. We then show the constituents that had the 5 most positive/negative values for this top principal component in <ref type="table">Table 5</ref>. For example, a particularly common subtree-associated with 180 unique constituents-is given by (T-45 w3)) (T-35 w4)) (T-40 w5)) (T-22 w6))).</p><p>The top 5 constituents with the most negative/positive values are shown in the top left part of <ref type="table">Table 5</ref>. We find that the leaves [w 1 , . . . , w 6 ], which form a 6-word constituent, vary in a regular manner as z is varied. We also observe that root of this subtree (NT-04) aligns to prepositional phrases (PP) in <ref type="figure" target="#fig_0">Figure 2</ref>, and the leaves in Table 5 (top left) are indeed mostly PP. However, the model fails to identify ((T-40 w5) (T-22 w6)) as a constituent in this case (as well as well in the bottom right example). See appendix A.5 for more exam-ples. It is possible that the model is utilizing the subtrees to capture broad template-like structures and then using z to fill them in, similar to recent works that also train models to separate "what to say" from "how to say it" <ref type="bibr" target="#b85">(Wiseman et al., 2018;</ref><ref type="bibr">Chen et al., 2019a,b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Limitations</head><p>We report on some negative results as well as important limitations of our work. While distributed representations promote parameter sharing, we were unable to obtain improvements through more factorized parameterizations that promote even greater parameter sharing. In particular, for rules of the type A → BC, we tried having the output embeddings be a function of the input embeddings (e.g. u BC = g([w B ; w C ]) where g is an MLP), but obtained worse results. For rules of the type T → w, we tried using a character-level <ref type="bibr">CNN (dos Santos and Zadrozny, 2014;</ref><ref type="bibr" target="#b33">Kim et al., 2016)</ref> to obtain the output word embeddings u w <ref type="bibr" target="#b32">(Jozefowicz et al., 2016;</ref><ref type="bibr" target="#b81">Tran et al., 2016)</ref>, but found the performance to be similar to the wordlevel case. <ref type="bibr">22</ref> We were also unable to obtain improvements by making the variational family more flexible through normalizing flows <ref type="bibr" target="#b63">(Rezende and Mohamed, 2015;</ref><ref type="bibr" target="#b36">Kingma et al., 2016)</ref>. However, given that we did not exhaustively explore the full space of possible parameterizations, the above modifications could eventually lead to im-provements with the right setup.</p><p>Relatedly, the models were quite sensitive to parameterization (e.g. it was important to use residual layers for f 1 , f 2 ), grammar size, and optimization method. We also noticed some variance in results across random seeds, as shown in <ref type="table" target="#tab_3">Table 2</ref>. Finally, despite vectorized GPU implementations, training was significantly more expensive (both in terms of time and memory) than NLM-based unsupervised parsing systems due to the O(|R||x| 3 ) dynamic program, which makes our approach potentially difficult to scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Grammar induction and unsupervised parsing has a long and rich history in natural language processing. Early work on with pure unsupervised learning was mostly negative <ref type="bibr" target="#b45">(Lari and Young, 1990;</ref><ref type="bibr" target="#b8">Carroll and Charniak, 1992;</ref><ref type="bibr" target="#b9">Charniak, 1993)</ref>, though <ref type="bibr" target="#b58">Pereira and Schabes (1992)</ref> reported some success on partially bracketed data. <ref type="bibr" target="#b13">Clark (2001)</ref> and <ref type="bibr" target="#b39">Klein and Manning (2002)</ref> were some of the first successful statistical approaches. In particular, the constituent-context model (CCM) of <ref type="bibr" target="#b39">Klein and Manning (2002)</ref>, which explicitly models both constituents and distituents, was the basis for much subsequent work <ref type="bibr" target="#b40">(Klein and Manning, 2004;</ref><ref type="bibr" target="#b28">Huang et al., 2012;</ref><ref type="bibr" target="#b25">Golland et al., 2012)</ref>. Other works have explored imposing inductive biases through Bayesian priors <ref type="bibr" target="#b31">(Johnson et al., 2007;</ref><ref type="bibr" target="#b48">Liang et al., 2007;</ref><ref type="bibr" target="#b82">Wang and Blunsom, 2013)</ref>, modified objectives (Smith and <ref type="bibr" target="#b73">Eisner, 2004)</ref>, and additional constraints on recursion depth <ref type="bibr" target="#b55">(Noji et al., 2016;</ref><ref type="bibr" target="#b29">Jin et al., 2018)</ref>.</p><p>While the framework of specifying the structure of a grammar and learning the parameters is common, other methods exist. <ref type="bibr" target="#b7">Bod (2006)</ref> consider a nonparametric-style approach to unsupervised parsing by using random subsets of training subtrees to parse new sentences. <ref type="bibr" target="#b69">Seginer (2007)</ref> utilize an incremental algorithm to unsupervised parsing which makes local decisions to create constituents based on a complex set of heuristics. <ref type="bibr" target="#b60">Ponvert et al. (2011)</ref> induce parse trees through cascaded applications of finite state models.</p><p>More recently, neural network-based approaches have shown promising results on inducing parse trees directly from words. <ref type="bibr" target="#b70">Shen et al. (2018</ref><ref type="bibr" target="#b71">Shen et al. ( , 2019</ref> learn tree structures through soft gating layers within neural language models, while <ref type="bibr" target="#b20">Drozdov et al. (2019)</ref> combine recursive autoencoders with the inside-outside algorithm. <ref type="bibr" target="#b34">Kim et al. (2019)</ref> train unsupervised recurrent neural network grammars with a structured inference network to induce latent trees, and <ref type="bibr" target="#b72">Shi et al. (2019)</ref> utilize image captions to identify and ground constituents.</p><p>Our work is also related to latent variable PCFGs <ref type="bibr" target="#b53">(Matsuzaki et al., 2005;</ref><ref type="bibr" target="#b59">Petrov et al., 2006;</ref><ref type="bibr" target="#b17">Cohen et al., 2012)</ref>, which extend PCFGs to the latent variable setting by splitting nonterminal symbols into latent subsymbols. In particular, latent vector grammars <ref type="bibr" target="#b89">(Zhao et al., 2018)</ref> and compositional vector grammars <ref type="bibr" target="#b75">(Socher et al., 2013)</ref> also employ continuous vectors within their grammars. However these approaches have been employed for learning supervised parsers on annotated treebanks, in contrast to the unsupervised setting of the current work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>This work studies a neural network-based approach grammar induction with PCFGs. We first propose to parameterize a PCFG's rule probabilities with neural networks over distributed representations of latent symbols, and find that this neural PCFG makes it possible to induce linguistically meaningful grammars with simple maximum likelihood learning. We then extend the neural PCFG through a sentence-level continuous latent vector, which induces marginal dependencies beyond the traditional first-order contextfree assumptions. We show that this compound PCFG learns richer grammars and leads to improved performance when evaluated as an unsupervised parser. The collapsed amortized variational inference approach is general and can be used for generative models which admit tractable inference through partial conditioning. Learning deep generative models which exhibit such conditional Markov properties is an interesting direction for future work.</p><formula xml:id="formula_16">π z,S→A = exp(u A f 1 ([w S ; z]) + b A ) A ∈N exp(u A f 1 ([w S ; z]) + b A ) , π z,A→BC = exp(u BC [w A ; z] + b BC ) B C ∈M exp(u B C [w A ; z] + b B C ) , π z,T →w = exp(u w f 2 ([w T ; z]) + b w ) w ∈Σ exp(u w f 2 ([w T ; z]) + b w )</formula><p>.</p><p>Again f 1 , f 2 are as before where the first layer's input dimensions are appropriately changed to account for concatenation with z.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Corpus/Sentence F 1 by Sentence Length</head><p>For completeness we show the corpus-level and sentence-level F 1 broken down by sentence length in <ref type="table" target="#tab_7">Table 6</ref>, averaged across 4 different runs of each model. In <ref type="figure">Figure 1</ref> we use the following to refer to rule probabilities of different rule types for the neural PCFG (left),</p><formula xml:id="formula_17">π S = {π r | r ∈ L(S)}, π N = {π r | r ∈ L(A), A ∈ N }, π P = {π r | r ∈ L(T ), T ∈ P}, π = π S ∪ π N ∪ π P ,</formula><p>where L(A) denotes the set of rules with A on the left hand side. The set of rule probabilities for the compound PCFG (right) is similarly defined, π z,S = {π z,r | r ∈ L(S)}, π z,N = {π z,r | r ∈ L(A), A ∈ N }, π z,P = {π z,r | r ∈ L(T ), T ∈ P}, π z = π z,S ∪ π z,N ∪ π z,P .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Experiments with RNNGs</head><p>For experiments on supervising RNNGs with induced trees, we use the parameterization and hyperparameters from <ref type="bibr" target="#b34">Kim et al. (2019)</ref>, which uses a 2-layer 650-dimensional stack LSTM (with dropout of 0.5) and a 650-dimensional tree LSTM <ref type="bibr" target="#b80">(Tai et al., 2015;</ref><ref type="bibr" target="#b90">Zhu et al., 2015)</ref> as the composition function. Concretely, the generative story is as follows: first, the stack representation is used to predict the next action (SHIFT or REDUCE) via an affine transformation followed by a sigmoid. If SHIFT is chosen, we obtain a distribution over the vocabulary via another affine transformation over the stack representation followed by a softmax. Then we sample the next word from this distribution and shift the generated word onto the stack using the stack LSTM. If REDUCE is chosen, we pop the last two elements off the stack and use the tree LSTM to obtain a new representation. This new representation is shifted onto the stack via the stack LSTM. Note that this RNNG parameterization is slightly different than the original from <ref type="bibr" target="#b23">Dyer et al. (2016)</ref>, which does not ignore constituent labels and utilizes a bidirectional LSTM as the composition function instead of a tree LSTM. As our RNNG parameterization only works with binary trees, we binarize the gold trees with right binarization for the RNNG trained on gold trees (trees from the unsupervised methods explored in this paper are already binary). The RNNG also trains a discriminative parser alongside the generative model for evaluation with importance sampling. We use a CRF parser whose span score parameterization is similar similar to recent works <ref type="bibr" target="#b83">(Wang and Chang, 2016;</ref><ref type="bibr" target="#b78">Stern et al., 2017;</ref><ref type="bibr" target="#b38">Kitaev and Klein, 2018)</ref>: position embeddings are added to word embeddings, and a bidirectional LSTM with 256 hidden dimensions is run over the input representations to obtain the forward and backward hidden states. The score s ij ∈ R for a constituent  spanning the i-th and j-th word is given by,</p><formula xml:id="formula_18">s ij = MLP([ − → h j+1 − − → h i ; ← − h i−1 − ← − h j ]),</formula><p>where the MLP has a single hidden layer with ReLU nonlinearity followed by layer normalization <ref type="bibr" target="#b2">(Ba et al., 2016)</ref>. For experiments on fine-tuning the RNNG with the unsupervised RNNG, we take the discriminative parser (which is also pretrained alongside the RNNG on induced trees) to be the structured inference network for optimizing the evidence lower bound. We refer the reader to <ref type="bibr" target="#b34">Kim et al. (2019)</ref> and their open source implementation 23 for additional details. We also observe that as noted by <ref type="bibr" target="#b34">Kim et al. (2019)</ref>, a URNNG trained from scratch on this version of PTB without punctuation failed to outperform a right-branching baseline.</p><p>The LSTM language model baseline is the same size as the stack LSTM (i.e. 2 layers, 650 hidden units, dropout of 0.5), and is therefore equivalent to an RNNG with completely right branching trees. The PRPN/ON baselines for perplexity/syntactic evaluation in <ref type="table" target="#tab_5">Table 3</ref> also have 2 layers with 650 hidden units and 0.5 dropout. Therefore all models considered in <ref type="table" target="#tab_5">Table 3</ref> have roughly the same capacity. For all models we share input/output word embeddings <ref type="bibr">(Press and 23</ref> https://github.com/harvardnlp/urnng <ref type="bibr" target="#b61">Wolf, 2016)</ref>. Perplexity estimation for the RNNGs and the compound PCFG uses 1000 importanceweighted samples.</p><p>For grammaticality judgment, we modify the publicly available dataset from <ref type="bibr" target="#b52">Marvin and Linzen (2018)</ref>  <ref type="bibr">24</ref> to only keep sentence pairs that did not have any unknown words with respect to our PTB vocabulary of 10K words. This results in 33K sentence pairs for evaluation. <ref type="figure" target="#fig_4">Figure 3</ref> shows the part-of-speech alignments and <ref type="table" target="#tab_10">Table 7</ref> shows the nonterminal label alignments for the compound PCFG/neural PCFG.    alignment is the proportion of correctly-predicted constistuents that correspond to a particular gold label. We also show the predicted constituent frequency and accuracy (i.e. precision) on the right. Bottom line shows the frequency in the gold trees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Nonterminal/Preterminal Alignments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Subtree Analysis</head><p>(NT-13 (T-12 w 1 ) (NT-25 (T-39 w 2 ) (T-58 w 3 ))) would be irresponsible has been growing could be delayed 've been neglected can be held had been made can be proven had been canceled could be used have been wary (NT-04 (T-13 w 1 ) (NT-12 (T-60 w 2 ) (NT-18 (T-60 w 3 ) (T-21 w 4 )))) of federally subsidized loans in fairly thin trading of criminal racketeering charges in quiet expiration trading for individual retirement accounts in big technology stocks without prior congressional approval from small price discrepancies between the two concerns by futures-related program buying (NT-04 (T-13 w 1 ) (NT-12 (T-05 w 2 ) (NT-01 (T-18 w 3 ) (T-25 w 4 )))) by the supreme court in a stock-index arbitrage of the bankruptcy code as a hedging tool to the bankruptcy court of the bond market in a foreign court leaving the stock market for the supreme court after the new york (NT-12 (NT-20 (NT-20 (T-05 w 1 ) (T-40 w 2 )) (T-40 w 3 )) (T-22 w 4 )) a syrian troop pullout the frankfurt stock exchange a conventional soviet attack the late sell programs the house-passed capital-gains provision a great buying opportunity the official creditors committee the most active stocks a syrian troop withdrawal a major brokerage firm (NT-21 (NT-22 (NT-20 (T-05 w 1 ) (T-40 w 2 )) (T-22 w 3 )) (NT-13 (T-30 w 4 ) (T-58 w 5 ))) the frankfurt market was mixed the gramm-rudman targets are met the u.s. unit edged lower a private meeting is scheduled a news release was prepared the key assumption is valid the stock market closed wednesday the budget scorekeeping is completed the stock market remains fragile the tax bill is enacted (NT-03 (T-07 w 1 ) (NT-19 (NT-20 (NT-20 (T-05 w 2 ) (T-40 w 3 )) (T-40 w 4 )) (T-22 w 5 ))) have a high default risk rejected a reagan administration plan have a lower default risk approved a short-term spending bill has a strong practical aspect has an emergency relief program have a good strong credit writes the hud spending bill have one big marketing edge adopted the underlying transportation measure (NT-13 (T-12 w 1 ) (NT-25 (T-39 w 2 ) (NT-23 (T-58 w 3 ) (NT-04 (T-13 w 4 ) (T-43 w 5 ))))) has been operating in paris will be used for expansion has been taken in colombia might be room for flexibility has been vacant since july may be built in britain have been dismal for years will be supported by advertising has been improving since then could be used as weapons (NT-04 (T-13 w 1 ) (NT-12 (NT-06 (NT-20 (T-05 w 2 ) (T-40 w 3 )) (T-22 w 4 )) (NT-04 (T-13 w 5 ) (NT-12 (T-18 w 6 ) (T-53 w 7 ))))) for a health center in south carolina with an opposite trade in stock-index futures by a federal jury in new york from the recent volatility in financial markets of the appeals court in new york of another steep plunge in stock prices of the further thaw in u.s.-soviet relations over the past decade as pension funds of the service corps of retired executives by a modest recovery in share prices (NT-10 (T-55 w 1 ) (NT-05 (T-02 w 2 ) (NT-19 (NT-06 (T-05 w 3 ) (T-41 w 4 )) (NT-04 (T-13 w 5 ) (NT-12 (T-60 w 6 ) (T-21 w 7 )))))) to integrate the products into their operations to defend the company in such proceedings to offset the problems at radio shack to dismiss an indictment against her claiming to purchase one share of common stock to death some N of his troops to tighten their hold on their business to drop their inquiry into his activities to use the microprocessor in future products to block the maneuver on procedural grounds (NT-13 (T-12 w 1 ) (NT-25 (T-39 w 2 ) (NT-23 (T-58 w 3 ) (NT-04 (T-13 w 4 ) (NT-12 (NT-20 (T-05 w 5 ) (T-40 w 6 )) (T-22 w 7 )))))) has been mentioned as a takeover candidate would be run by the joint chiefs has been stuck in a trading range would be made into a separate bill had left announced to the trading mob would be included in the final bill only become active during the closing minutes would be costly given the financial arrangement will get settled in the short term would be restricted by a new bill (NT-10 (T-55 w) (NT-05 (T-02 w 1 ) (NT-19 (NT-06 (T-05 w 2 ) (T-41 w 3 )) (NT-04 (T-13 w 4 ) (NT-12 (T-60 w 5 ) (NT-18 (T-18 w 6 ) (T-53 w 7 )))))))</p><p>to supply that country with other defense systems to enjoy a loyalty among junk bond investors to transfer its skill at designing military equipment to transfer their business to other clearing firms to improve the availability of quality legal service to soften the blow of declining stock prices to unveil a family of high-end personal computers to keep a lid on short-term interest rates to arrange an acceleration of planned tariff cuts to urge the fed toward lower interest rates (NT-21 (NT-22 (T-60 w 1 ) (NT-18 (T-60 w 2 ) (T-21 w 3 ))) (NT-13 (T-07 w 4 ) (NT-02 (NT-27 (T-47 w 5 ) (T-50 w 6 )) (NT-10 (T-55 w 7 ) (NT-05 (T-47 w 8 ) (T-50 w 9 )))))) unconsolidated pretax profit increased N % to N billion amex short interest climbed N % to N shares its total revenue rose N % to N billion its pretax profit rose N % to N million total operating revenue grew N % to N billion its pretax profit rose N % to N billion its group sales rose N % to N billion fiscal first-half sales slipped N % to N million total operating expenses increased N % to N billion total operating expenses increased N % to N billion <ref type="table" target="#tab_8">Table 8</ref>: For each subtree (shown at the top of each set of examples), we perform PCA on the variational posterior mean vectors that are associated with that particular subtree and take the top principal component. We then list the top 5 constituents that had the lowest (left) and highest (right) principal component values.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Alignment of induced nonterminals ordered from</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>A neural PCFG/HMM obtains 68.2 and 63.4 respectively. 's capital structure in the company 's divestiture program by the company 's new board in the company 's core businesses on the company 's strategic plan PC + above the treasury 's N-year note above the treasury 's seven-year note above the treasury 's comparable note above the treasury 's five-year note measured the earth 's ozone layer exercise of stock options circulated by a handful of major brokers higher as a percentage of total loans common with a lot of large companies surprised by the storm of sell orders PC + brought to the u.s. against her will laid for the arrest of opposition activists uncertain about the magnitude of structural damage held after the assassination of his mother hurt as a result of the violations</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>(</head><label></label><figDesc>NT-04 (T-13 w1) (NT-12 (NT-20 (NT-20 (NT-07 (T-05 w2)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Preterminal alignment to part-of-speech tags for the compound PCFG (top) and the neural PCFG (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>URNNG † (Kim et al., 2019) − 45.4 − − DIORA † (Drozdov et al., 2019) − 58.9 − −</figDesc><table><row><cell></cell><cell>PTB</cell><cell>CTB</cell><cell></cell></row><row><cell>Model</cell><cell cols="3">Mean Max Mean Max</cell></row><row><cell>PRPN (Shen et al., 2018)</cell><cell>37.4 38.1</cell><cell>−</cell><cell>−</cell></row><row><cell>ON (Shen et al., 2019)</cell><cell>47.7 49.4</cell><cell>−</cell><cell>−</cell></row><row><cell>Left Branching</cell><cell>8.7</cell><cell>9.7</cell><cell></cell></row><row><cell>Right Branching</cell><cell>39.5</cell><cell>20.0</cell><cell></cell></row><row><cell>Random Trees</cell><cell>19.2 19.5</cell><cell cols="2">15.7 16.0</cell></row><row><cell>PRPN (tuned)</cell><cell>47.3 47.9</cell><cell cols="2">30.4 31.5</cell></row><row><cell>ON (tuned)</cell><cell>48.1 50.0</cell><cell cols="2">25.4 25.7</cell></row><row><cell>Scalar PCFG</cell><cell>&lt; 35.0</cell><cell>&lt; 15.0</cell><cell></cell></row><row><cell>Neural PCFG</cell><cell>50.8 52.6</cell><cell cols="2">25.7 29.5</cell></row><row><cell>Compound PCFG</cell><cell>55.2 60.1</cell><cell cols="2">36.0 39.8</cell></row><row><cell>Oracle Trees</cell><cell>84.3</cell><cell>81.1</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>. Maintaining train/test splits is</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Unlabeled sentence-level F1 scores on PTB and CTB test sets. Top shows results from previous work while the rest of the results are from this paper. Mean/Max scores are obtained from 4 runs of each model with different random seeds. Oracle is the maximum score obtainable with binarized trees, since we compare against the non-binarized gold trees per convention. Results with † are trained on a version of PTB with punctuation, and hence not strictly comparable to the present work. For URNNG/DIORA, we take the parsed test set provided by the authors from their best runs and evaluate F1 with our evaluation setup, which ignores punctuation.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>(Top) Mean F1 similarity against Gold, Left, Right, and Self trees. Self F1 score is calculated by averaging over all 6 pairs obtained from 4 different runs. (Bottom) Fraction of ground truth constituents that were predicted as a constituent by the models broken down by label (i.e. label recall).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2</head><label>2</label><figDesc>, bottom). While left as future work, this naturally suggests an ensemble approach wherein the empirical probabilities of constituents (obtained by averaging the predicted binary constituent labels from the different models) are used either to supervise another model or directly as potentials in a CRF constituency parser.</figDesc><table><row><cell></cell><cell cols="2">PPL Syntactic Eval.</cell><cell>F1</cell></row><row><cell>LSTM LM</cell><cell>86.2</cell><cell>60.9%</cell><cell>−</cell></row><row><cell>PRPN</cell><cell>87.1</cell><cell>62.2%</cell><cell>47.9</cell></row><row><cell>Induced RNNG</cell><cell>95.3</cell><cell>60.1%</cell><cell>47.8</cell></row><row><cell>Induced URNNG</cell><cell>90.1</cell><cell>61.8%</cell><cell>51.6</cell></row><row><cell>ON</cell><cell>87.2</cell><cell>61.6%</cell><cell>50.0</cell></row><row><cell>Induced RNNG</cell><cell>95.2</cell><cell>61.7%</cell><cell>50.6</cell></row><row><cell>Induced URNNG</cell><cell>89.9</cell><cell>61.9%</cell><cell>55.1</cell></row><row><cell>Neural PCFG</cell><cell>252.6</cell><cell>49.2%</cell><cell>52.6</cell></row><row><cell>Induced RNNG</cell><cell>95.8</cell><cell>68.1%</cell><cell>51.4</cell></row><row><cell>Induced URNNG</cell><cell>86.0</cell><cell>69.1%</cell><cell>58.7</cell></row><row><cell>Compound PCFG</cell><cell>196.3</cell><cell>50.7%</cell><cell>60.1</cell></row><row><cell>Induced RNNG</cell><cell>89.8</cell><cell>70.0%</cell><cell>58.1</cell></row><row><cell>Induced URNNG</cell><cell>83.7</cell><cell>76.1%</cell><cell>66.9</cell></row><row><cell>RNNG on Oracle Trees</cell><cell>80.6</cell><cell>70.4%</cell><cell>71.9</cell></row><row><cell>+ URNNG Fine-tuning</cell><cell>78.3</cell><cell>76.1%</cell><cell>72.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Results from training RNNGs on induced trees from various models (Induced RNNG) on the PTB. Induced URNNG indicates fine-tuning with the URNNG objective.</figDesc><table /><note>We show perplexity (PPL), grammaticality judgment perfor- mance (Syntactic Eval.), and unlabeled F1. PPL/F1 are cal- culated on the PTB test set and Syntactic Eval. is from Marvin and Linzen (2018)'s dataset. Results on top do not make any use of annotated trees, while the bottom two re- sults are trained on binarized gold trees. The perplexity numbers here are not comparable to standard results on the PTB since our models are generative model of sentences and hence we do not carry information across sentence bound- aries. Also note that all the RNN-based models above (i.e. LSTM/PRPN/ON/RNNG/URNNG) have roughly the same model capacity (see appendix A.3).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Average unlabeled F1 for the various models broken down by sentence length on the PTB test set. For example WSJ-10 refers to F1 calculated on the subset of the test set where the maximum sentence length is at most 10. Scores are averaged across 4 runs of the model with different random seeds. Oracle is the performance of binarized gold trees (with right branching binarization). Top shows sentence-level F1 and bottom shows corpus-level F1.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8</head><label>8</label><figDesc></figDesc><table><row><cell>lists more examples of constituents within</cell></row><row><cell>each subtree as the top principical component is</cell></row><row><cell>varied. Due to data sparsity, the subtree analysis is</cell></row><row><cell>performed on the full dataset. See section 5.2 for</cell></row><row><cell>more details.</cell></row><row><cell>24 https://github.com/BeckyMarvin/LM syneval</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Analysis of label alignment for nonterminals in the compound PCFG (top) and the neural PCFG (bottom). Label</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">In preliminary experiments we were indeed unable to learn linguistically meaningful grammars with this PCFG.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Under the Bayesian PCFG view, pγ(z) is a distribution over z (a subset of the prior), and is thus a hyperprior.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8"> See Berger (1985)  (chapter 4),<ref type="bibr" target="#b88">Zhang (2003)</ref>, and Cohen (2016) (chapter 3) for further discussion on compound models and empirical Bayes.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">Since p θ (t | x, z) is continuous with respect to z, we have p θ (t | x, z)δ(z − µ φ (x)) dz = p θ t | x, µ φ (x) .</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">However, we used F1 against validation trees on PTB to select some hyperparameters (e.g. grammar size), as is sometimes done in grammar induction. Hence our PTB results are arguably not fully unsupervised in the strictest sense of the term. The hyperparameters of the PRPN/ON baselines are also tuned using validation F1 for fair comparison.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11">Corpus-level F1 calculates precision/recall at the corpus level to obtain F1, while sentence-level F1 calculates F1 for each sentence and averages across the corpus. 12 https://github.com/yikangshen/PRPN 13 https://github.com/yikangshen/Ordered-Neurons</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14">Training perplexity was much higher than in the neural case, indicating significant optimization issues. However we did not experiment with online EM<ref type="bibr" target="#b47">(Liang and Klein, 2009)</ref>, and it is possible that such methods would yield better results.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16">The train/test F1 was similar for all models.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="22">It is also possible to take advantage of pretrained word embeddings by using them to initialize output word embeddings or directly working with continuous emission distributions<ref type="bibr" target="#b49">(Lin et al., 2015;</ref><ref type="bibr" target="#b26">He et al., 2018)</ref> </note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Phil Blunsom for initial discussions which seeded many of the core ideas in the present work. We also thank Yonatan Belinkov and Shay Cohen for helpful feedback, and Andrew Drozdov for providing the parsed dataset from their DIORA model. YK is supported by a Google Fellowship. AMR acknowledges the support of NSF 1704834, 1845664, AWS, and Oracle.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Model Parameterization</head><p>We associate an input embedding w N for each symbol N on the left side of a rule (i.e. N ∈ {S} ∪ N ∪ P) and run a neural network over w N to obtain the rule probabilities. Concretely, each rule type π r is parameterized as follows,</p><p>where M is the product space (N ∪P)×(N ∪P), and f 1 , f 2 are MLPs with two residual layers,</p><p>In the compound PCFG the rule probabilities π z given a latent vector z,</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Indexed Grammars-An Extension of Context-Free Grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfred</forename><surname>Aho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="647" to="671" />
			<date type="published" when="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On the Optimization of Deep Networks: Implicit Acceleration by Overparameterization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadav</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Layer Normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Trainable Grammars for Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">K</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Spring Conference of the</title>
		<meeting>the Spring Conference of the</meeting>
		<imprint>
			<publisher>Acoustical Society of America</publisher>
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Curriculum Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerome</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Painless Unsupervised Learning with Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Bouchard-Cote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Denero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Statistical Decision Theory and Bayesian Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">O</forename><surname>Berger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An All-Subtrees Approach to Unsupervised Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rens</forename><surname>Bod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Two Experiments on Learning Probabilistic Dependency Grammars from Corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glenn</forename><surname>Carroll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Workshop on Statistically-Based NLP Techniques</title>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Statistical Language Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A Fast and Accurate Dependency Parser using Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Controllable Paraphrase Generation with a Syntactic Exemplar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A Multi-task Approach for Disentangling Syntax and Semantics in Sentence Sepresentations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised Induction of Stochastic Context Free Grammars Using Distributional Clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
		<meeting>CoNLL</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Bayesian Analysis in Natural Language Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Morgan and Claypool</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Logistic Normal Priors for Unsupervised Probabilistic Grammar Induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Shared Logistic Normal Distributions for Soft Parameter Tying in Unsupervised Grammar Induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Spectral Learning of Latent-Variable PCFGs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dean</forename><forename type="middle">P</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyle</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ungar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Three Generative, Lexicalised Models for Statistical Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Maximum Likelihood from Incomplete Data via the EM Algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society, Series B</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unsupervised Latent Tree Induction with Deep Inside-Outside Recursive Auto-Encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Drozdov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Verga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Yadev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Gradient Descent Provably Optimizes Overparameterized Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyu</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnabas</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aarti</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Neural CRF Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Recurrent Neural Network Grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Inside-Outside and Forward-Backward Algorithms Are Just Backprop (Tutorial Paper)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Structured Prediction for NLP</title>
		<meeting>the Workshop on Structured Prediction for NLP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A Feature-Rich Constituent Context Model for Grammar Induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dave</forename><surname>Golland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Denero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unsupervised Learning of Syntactic Structure with Invertible Neural Projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junxian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Grammar Induction with Neural Language Models: An Unusual Replication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Phu Mon Htut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Improved Constituent Context Model with Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chew Lim</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of PACLIC</title>
		<meeting>PACLIC</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unsupervised Grammar Induction with Depth-bounded PCFG</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Finale</forename><surname>Doshi-Velez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Schuler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lane</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of TACL</title>
		<meeting>TACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">PCFG Models of Linguistic Tree Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="613" to="632" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Bayesian Inference for PCFGs via Markov chain Monte Carlo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.02410</idno>
		<title level="m">Exploring the Limits of Language Modeling</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Character-Aware Neural Language Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unsupervised Recurrent Neural Network Grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gábor</forename><surname>Melis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04934</idno>
		<title level="m">Improving Variational Inference with Autoregressive Flow</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Auto-Encoding Variational Bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Constituency Parsing with a Self-Attentive Encoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A Generative Constituent-Context Model for Improved Grammar Induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Corpus-based Induction of Syntactic Structure: Models of Dependency and Constituency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Accurate Unlexicalized Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">LSTMs Can Learn Syntax-Sensitive Dependencies Well, But Modeling Structure Makes Them Better</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Hale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Scalable Syntax-Aware Language Models Using Knowledge Distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Rimell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Variational Bayesian Grammar Induction for Natural Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenichi</forename><surname>Kurihara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taisuke</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Colloquium on Grammatical Inference</title>
		<meeting>International Colloquium on Grammatical Inference</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">The Estimation of Stochastic Context-Free Grammars Using the Inside-Outside Algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karim</forename><surname>Lari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech and Language</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="35" to="56" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">An Imitation Learning Approach to Unsupervised Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Keller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Online EM for Unsupervised models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">The Infinite PCFG using Hierarchical Dirichlet Processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Unsupervised POS Induction with Word Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chu-Cheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lori</forename><surname>Levin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Structured Alignment Networks for Matching Sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Building a Large Annotated Corpus of English: The Penn Treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Santorini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Targeted Syntactic Evaluation of Language Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Marvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Linzen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Probabilistic CFG with Latent Annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuya</forename><surname>Matsuzaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junichi</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Tagging English Text with a Probabilistic Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Merialdo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="155" to="171" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Using Left-corner Parsing to Encode Universal Structural Constraints in Grammar Induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Noji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Spectral Unsupervised Parsing with Additive Tree Metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ankur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Text Generation with Exemplar-based Adaptive Decoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><forename type="middle">P</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Bhuwan Dhingra, and Dipanjan Das</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Inside-Outside Reestimation from Partially Bracketed Corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Schabes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Learning Accurate, Compact, and Interpretable Tree Annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Barret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Thibaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Simpled Unsupervised Grammar Induction from Raw Text with Cascaded Finite State Methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elis</forename><surname>Ponvert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Erk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Using the Output Embedding to Improve Language Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofir</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL</title>
		<meeting>EACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Improved Fully Unsupervised Parsing with Zoomed Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Rappoport</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Variational Inference with Normalizing Flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Stochastic Backpropagation and Approximate Inference in Deep Generative Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Shakir Mohamed, and Daan Wierstra</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Asymptotically Subminimax Solutions of Compound Statistical Decision Problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herbert</forename><surname>Robbins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Berkeley Symposium on Mathematical Statistics and Probability</title>
		<meeting>the Second Berkeley Symposium on Mathematical Statistics and Probability<address><addrLine>Berkeley</addrLine></address></meeting>
		<imprint>
			<publisher>University of California Press</publisher>
			<date type="published" when="1951" />
			<biblScope unit="page" from="131" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">An Empirical Bayes Approach to Statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herbert</forename><surname>Robbins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Berkeley Symposium on Mathematical Statistics and Probability</title>
		<meeting>the Third Berkeley Symposium on Mathematical Statistics and Probability<address><addrLine>Berkeley</addrLine></address></meeting>
		<imprint>
			<publisher>University of California Press</publisher>
			<date type="published" when="1956" />
			<biblScope unit="page" from="157" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Optimization with EM and Expectation-Conjugate-Gradient</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Learning Character-level Representations for Part-of-Speech Tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cícero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bianca</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zadrozny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Fast Unsupervised Incremental Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Seginer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Neural Language Modeling by Jointly Learning Syntax and Lexicon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouhan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Wei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Ordered Neurons: Integrating Tree Structures into Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shawn</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Visually Grounded Neural Syntax Acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyue</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Annealing Techniques for Unsupervised Statistical Language Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Unsupervised Multilingual Grammar Induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tahira</forename><surname>Naseem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Parsing with Compositional Vector Grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Three Dependency-and-Boundary Models for Grammar Induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiyan</forename><surname>Spitkovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Alshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-CoNLL</title>
		<meeting>EMNLP-CoNLL</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Breaking Out of Local Optima with Count Transforms and Model Recombination: A Study in Grammar Induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiyan</forename><surname>Spitkovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Alshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">A Minimal Span-Based Neural Constituency Parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Mutual Information Maximization for Simple and Accurate Part-of-Speech Induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Stratos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Unsupervised Neural Hidden Markov Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Structured Prediction for NLP</title>
		<meeting>the Workshop on Structured Prediction for NLP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Collapsed Variational Bayesian Inference for PCFGs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
		<meeting>CoNLL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Graph-based Dependency Parsing with Bidirectional LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Structural Supervision Improves Learning of Non-Local Grammatical Dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Wilcox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Futrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Miguel Ballesteros, and Roger Levy</note>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Learning Neural Templates for Text Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><forename type="middle">M</forename><surname>Shieber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Benefits of Over-Parameterization with EM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arian</forename><surname>Maleki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">The Penn Chinese Treebank: Phrase Structure Annotation of a Large Corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><surname>Fu Dong Chiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="207" to="238" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">Compound Decision Theory and Empirical Bayes Methods. The Annals of Statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cun-Hui</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="379" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Gaussian Mixture Latent Vector Grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanpeng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory Over Tree Structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parinaz</forename><surname>Sobhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
