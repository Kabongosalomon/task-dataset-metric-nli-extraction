<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Structural Analysis of Sparse Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Stier</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Passau</orgName>
								<address>
									<addrLine>Innstraße 41</addrLine>
									<postCode>94032</postCode>
									<settlement>Passau</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Granitzer</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Passau</orgName>
								<address>
									<addrLine>Innstraße 41</addrLine>
									<postCode>94032</postCode>
									<settlement>Passau</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Structural Analysis of Sparse Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>artificial neural networks</term>
					<term>sparse network structures</term>
					<term>small- world neural networks</term>
					<term>scale-free</term>
					<term>architecture performance estimation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Sparse Neural Networks regained attention due to their potential of mathematical and computational advantages. We give motivation to study Artificial Neural Networks (ANNs) from a network science perspective, provide a technique to embed arbitrary Directed Acyclic Graphs into ANNs and report study results on predicting the performance of image classifiers based on the structural properties of the networks' underlying graph. Results could further progress neuroevolution and add explanations for the success of distinct architectures from a structural perspective.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Artificial Neural Networks (ANNs) are highly successful machine learning models and achieve human performance in various domains such as image processing and speech synthesis. The choice on architecture is crucial for their successbut ANN architectures, seen as network structures, have not been extensively studied from a network science perspective, yet. The motivation of studying ANNs from a network science perspective is manifold:</p><p>First of all, ANNs are inspired from biological neural networks (notably the human brain) which have scale-free properties and "are shown to be small-world networks" <ref type="bibr" target="#b24">[25]</ref>. This is contradictory to most successful models in various machine learning problem domains. However, with respect to their required neurons, those successful models have shown to be redundant by a magnitude. For example, Han et al. claim "on the ImageNet dataset, our method reduced the number of parameters of AlexNet by a factor of 9×" and "VGG-16 can be reduced by 13×" <ref type="bibr" target="#b7">[8]</ref> through pruning. Thus, biology and structural searches such as e.g. pruning suggest to develop sparse neural network structures <ref type="bibr" target="#b23">[24]</ref>.</p><p>Secondly, a look on the history of ANNs shows that, after major breakthroughs in the early 90s, the research community already tried to find characteristic networks structures by e.g. constructive and destructive approaches. However, network structures have only been studied in graph theory by then and the major remaining architectures from this research period are notably Convolutional Neural Networks (CNNs) <ref type="bibr" target="#b15">[16]</ref> and Long Short-Term Memory Networks (LSTMs) <ref type="bibr" target="#b11">[12]</ref>. New breakthroughs in the early 21st century led to recent trends of new architectures such as Highway Networks <ref type="bibr" target="#b22">[23]</ref> and Residual Networks <ref type="bibr" target="#b8">[9]</ref>. This suggests that further insights from revisiting and analysing the topology of sparse neural networks could be gained, particularly from a network science perspective as done in Mocanu et al. <ref type="bibr" target="#b18">[19]</ref>, which also "argue that ANNs, too, should not have fully-connected layers".</p><p>Last but not least, many researchers reported various advantages of sparse structures over unjustified densely stacked layers. <ref type="bibr">Glorot et al. argued</ref>, that sparsity is one of the factors for the success of Rectified Linear Units (ReLU) as activation functions because "using a rectifying non-linearity gives rise to real zeros of activations and thus truly sparse representations" <ref type="bibr" target="#b6">[7]</ref>. The optimization process with ReLUs might not only find good representations but also better structures through sparser connections. Sparser connections can also decrease the number of computations and thus time and energy consumption. Not only exists biological motivation but also mathematical and computational advantages have been reported for sparse neural network structures.</p><p>Looking at the structural design and training of ANNs as a search problem, one can argue to apply structural regularisation when postulating distinct structural properties. CNNs exploit sparsely connected neurons due to spatial relationships of the input features, which leads in conjunction with weight-sharing to very successful models. LSTMs overcome analytical issues in training by postulating structures with slightly changed training behaviour. We argue, that there can be new insights gained from studying ANNs from a network science perspective.</p><p>Sparse Neural Networks (SNNs), which we define as networks not being fully connecteted between layers, form another important, yet not well-understood structural regularisation. We mention three major approaches to study the influence of structural regularisation to properties of SNNs: 1) studying fixed sparse structures with empirical success or selected analytical advantages, 2) studying sparse structures obtained in search heuristics and 3) studying sparse structures with common graph theoretical properties.</p><p>The first approach studies selected models with fixed structure analytically or within empirical works. Exemplarily, LSTMs arose from analytical work <ref type="bibr" target="#b10">[11]</ref> on issues in the domain of time-dependent problems and have also been studied empirically since then. In larger network structures, LSTMs and CNNs provide fixed components which have been proven to be successful due to analytical research and much experience in empirical work. These insights provide foundations to construct larger networks in a bottom-up fashion.</p><p>On larger scale, sparse network structures can be approached by automatic construction, pruning, evolutionary techniques or even as a result of training regularisation or the choice of activation function. Despite the common motivation to find explanations for resulting sparse structures, a lot of those approaches indeed obtain sparse structures but fail to find an explanation for them.</p><p>Studying real-world networks has been addressed by the field of Network Science. However, as of now, Network Science has been hardly used to study the structural properties of ANNs and to understand the regularisation effects introduced by sparse network structures.</p><p>This article reports results on embedding sparse graph structures with characteristic properties into feed-forward networks and gives first insights into our study on network properties of sparse ANNs. Our Contributions comprise a technique to embed Directed Acyclic Graphs into Artificial Neural Networks (ANN) a comparison of Random Graph Generators as generators for structures of ANNs a performance estimator for ANNs based on structural properties of the networks' underlying graph</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>A lot of related works in fields such as network science, graph theory, and neuroevolution give motivation to study Sparse Structured Neural Networks (SNNs). From a network science perspective, SNNs are Directed Acyclic Graphs (DAGs) in case of non-recurrent and Directed Graphs in case of Recurrent Neural Networks. Directed acyclic and cyclic graphs are built, studied and characterized in graph theory and network science. Notably, already Erdös "aimed to show [..] that the evolution of a random graph shows very clear-cut features" <ref type="bibr" target="#b5">[6]</ref>. Graphs with distinct degree distributions have been characterised as small-world and scale-free networks with the works of Watts &amp; Strogatz <ref type="bibr" target="#b24">[25]</ref> and Barabási &amp; Albert <ref type="bibr" target="#b0">[1]</ref>. Both phenomens are "not merely a curiosity of social networks nor an artefact of an idealized model -it is probably generic for many large, sparse networks found in nature" <ref type="bibr" target="#b24">[25]</ref>. However, from a biological perspective, the question of how the human brain is organised, still remains open: According to Hilgetag et al. "a reasonable guess is that the large-scale neuronal networks of the brain are arranged as globally sparse hierarchical modular networks" <ref type="bibr" target="#b9">[10]</ref>. Concerning the combination of ANNs and Network Science Mocanu et al. claim that "ANNs perform perfectly well with sparsely-connected layers" and introduce a training procedure SET, which induces sparsity by magnitude-based pruning and randomly adding connections within the training phase <ref type="bibr" target="#b18">[19]</ref>. While their method is driven by the same motivation and inspiration, it is conceptually fundamentally different from our idea of finding characteristic properties of SNNs.</p><p>The work of Bourely et al. can be considered very close to our idea of creating SNNs before the training phase. They "propose Sparse Neural Network architectures that are based on random or structured bipartite graph topologies" <ref type="bibr" target="#b3">[4]</ref> but do not seem to base their construction on existing work in Network Science when transforming their randomly generated structures into ANNs.</p><p>There exists a vast amount of articles on automatic methods which yield sparsity. Besides well-established regularisation methods (e.g. L 1 -regularisation), new structural regularisation methods can be found: Srinivas et al. "introduce additional gate variables to perform parameter selection" <ref type="bibr" target="#b21">[22]</ref> and Louizos et al. "propose a practical method for L 0 norm regularisation" in which they "prune the network during training by encouraging weights to become exactly zero" <ref type="bibr" target="#b17">[18]</ref>. Besides regularisation one can also achieve SNNs through pruning, construction and evolutionary strategies. All of those domains achieve successes to some extent but seem to fail in providing explanations for why certain found architectures succeed and others do not.</p><p>The idea of predicting the model performance can already be found in a similar way in the works of <ref type="bibr">Klein</ref>   <ref type="bibr" target="#b14">[15]</ref>. They compare the idea with a human expert assessing the course of the learning curve of a model. Domhan et al. also "mimic the early termination of bad runs using a probabilistic model that extrapolates the performance from the first part of a learning curve" <ref type="bibr" target="#b4">[5]</ref>. Both are concerned with predicting the performance based on learning curve, not on structural network properties.</p><p>Baker et al. are closest to our method by "predicting the final performance of partially trained model configurations using features based on network architectures, hyperparameters, and time-series validation performance data" <ref type="bibr" target="#b1">[2]</ref>. They report good R 2 values (e.g. 0.969 for Cifar10 with MetaQNN CNNs) for predicting the performance and did so on slightly more complex datasets such as Cifar10, TinyImageNet and Penn Treebank. However, our motivation is to focus only on architecture parameters and include more characteristic properties of the network graph than only "including total number of weights and number of layers" which is independent of other hyperparameters and saves conducting an expensive training phase.</p><p>In the following chapters we give an introduction to Network Science, illustrate how Directed Acyclic Graphs are embedded into ANNs, provide details on generated graphs and their properties and visualize results from predicting the performance of built ANNs only with features based on structural properties of the underlying graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">A Network Science Perspective: Random Graph Generators</head><p>A graph G = (V, E) is defined by its naturally ordered vertices V and edges E ⊂ V ×V . The number of edges containing a vertex v ∈ V determines the degree of v. The "distribution function P (k) [..] gives the probability that a randomly selected node has exactly k edges" <ref type="bibr" target="#b0">[1]</ref>. It can be used as a first characteristic to differ between certain types of graphs. For a "random graph [it] is a Poisson distribution with a peak at P ( k )", with k being "the average degree of the network" <ref type="bibr" target="#b0">[1]</ref>. Graphs with degree distributions following a power-law tail P (k) ∼ k −γ are called scale-free graphs. Graphs with "relatively small characteristic path lengths" are called small-world graphs <ref type="bibr" target="#b24">[25]</ref>.</p><p>Graphs can be generated by Random Graph Generators (RGG). Various RGGs can yield very distinct statistical properties. The most prominent model to generate a random graph is the Erdős-Rényi-/ Gilbert-model (ERG-model) which connects a given number of vertices randomly. While this ERG-model yields a Poisson distribution for the degree distribution, other models have been developed to close the gap of generating large graphs with distinct characterstics found in nature. Notably, the first RGGs producing such graphs have been the Watts-Strogatz -model <ref type="bibr" target="#b24">[25]</ref> and the Barabási-Albert-model <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Embedding Arbitrary Structures into Sparse Neural Networks</head><p>After obtaining a graph with desired properties by a Random Graph Generator, this graph is embedded into a Sparse Neural Network with following steps:</p><p>1. Make graph directed (if not directed, yet), 2. compute a layer indexing for all vertices, 3. embed layered vertices between an input and output layer meeting the requirements of the dataset.</p><p>The first step conducts a transformation into a Directed Acyclic Graph following an approach in Barak et al. <ref type="bibr" target="#b2">[3]</ref> which "consider the class A of random acyclic directed graphs which are obtained from random graphs by directing all the edges from higher to lower indexed vertices". Given a graph and a natural ordering of its vertices, a DAG is obtained by directing all edges from higher to lower ordered vertices. This can be easily computed by setting the upper (or lower) triangle of the adjacency matrix to zero. Next, a layer indexing is computed to assign vertices into different layers within a feed-forward network. Vertices in a common layer can be represented in a unified layer vector. The indexing function</p><formula xml:id="formula_0">ind l (v) : V → N is recursively defined by v → max({ind l (s) | (s, v) ∈ E in v } ∪ {−1}</formula><p>) + 1 which then defines a set of layers L and a family of neurons indexed by I l for each layer l ∈ L. Finally, the layered DAG can be embed into an ANN for a given task, e.g. a classification task such as MNIST with 784 input and 10 output neurons. All neurons of the input layer are connected to neurons representing the vertices of the DAG with in-degree equal to zero (layer index zero). Succedingly, each vertex gets represented as a neuron and is connected according to the DAG. The build process finishes by connecting all neurons of the last layer of the DAG with neurons of the output layer. Following this approach, each resulting ANN has at least two fully connected layers in size depending on the DAG vertices of in-degree and out-degree equalling zero. <ref type="figure" target="#fig_2">Figure 1</ref> visualizes the steps from a random generated graph to a DAG and embedded into an ANN classifier.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Performance Estimation Through Structural Properties</head><p>In order to analyse the impact of the different structural elements, we conducted a supervised experiment predicting network performances on the structural properties only. We then analysed the most important features involved in the decision along with the prediction quality.</p><p>For this experiment we created an artificial dataset graphs10k based on two Random Graph Generators (RGG). Each graph in the dataset is transformed into a Sparse Neural Network (SNN), implemented in PyTorch <ref type="bibr" target="#b19">[20]</ref>. The resulting model is then trained and evaluated on MNIST <ref type="bibr" target="#b16">[17]</ref>. Based on obtained evaluation measures, three estimator models -Ordinary Linear Regression, Support Vector Machine and Random Forest -are trained by splitting graphs10k into a training and test set. The estimator models give opportunity to discuss influence of structural properties to the SNN performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The graphs10k dataset</head><p>To investigate which structural properties influence the performance of an Artificial Neural Network most, we created a dataset of graphs generated by Watts-Strogatz -and Barabási-Albert-models. The dataset comprises 10,000 graphs, randomly generated by having between 50 and 500 vertices. An exemplary property The dataset contains 5018 Barabási-Albert-graphs and 4982 Watts-Strogatzgraphs. The graphs have between 97 and 4,365 edges and on average 1,399.17 edges with a standard deviation of 954,12. In estimating the model performances the number of source and sink vertices are very prominent. Source vertices are those with no incoming edges, sink vertices those with no outgoing edges. On average the graphs have 79.75 source vertices with a standard deviation of 80.69 and 9.56 sink vertices with a standard deviation of 17.53.</p><p>Distributions within the graphs are reduced to four properties, namely minimum, arithmetic mean, maximum and standard deviation or variance. For the mean degree distribution, a vertex has on average 10.36 connected edges and this 1 Note, that according to <ref type="bibr">Karrer et al. "</ref>we do not at present know of any way to sample uniformly from the unordered ensemble" in the context of samling from possibile orderings of random acyclic graphs <ref type="bibr" target="#b13">[14]</ref>. "Bayesian networks are composed of directed acyclic graphs, and it is very hard to represent the space of such graphs. Consequently, it is not easy to guarantee that a given method actually produces a uniform distribution in that space." <ref type="bibr" target="#b12">[13]</ref> arithmetic mean has a standard deviation of 4.47. The variance of the degree distributions is on average 174.46 with a standard deviation of 259.08.</p><p>After each graph was embedded in a SNN, its accuracy value for test and validation set was obtained and added to the dataset. More detailed statistics on other properties are given in <ref type="table">Table 2</ref>.</p><p>With the presented embedding technique it could be found, that graphs based on the Barabási-Albert model could not achieve comparable performances to graphs based on the Watts-Strogatz model. Only 2618 of Barabási-Albert models achieved over 0.3 in accuracy (less than 50%). Excluding the Barabási-Albert model did not change the estimation results in subsection 4.2, therefore statistics are reported for all graphs combined. In future, the dataset will be enhanced to comprise more RGGs.</p><p>The test accuracy has three major peeks at arond 0.15, 0.3 and 0.94 as can be seen in the frequency distribution on the x-axis of 2b, depicting a joint plot of the test accuracy and the variance of a graphs' eccentricity 2 distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Estimators on graphs10k</head><p>Ordinary Linear Regression (OLS), Support Vector Machine (SVM) and Random Forest (RF) are used to estimate model performance values given their underlying structural properties.</p><p>The dataset was split into a train and test set with a train set size ratio of 0.7. In case of considering the whole dataset of 10,000 graphs the estimator models are trained on 7000 data points. The train-test-split was repeated 20 times with different constants used as initialization seed for the random state.</p><p>Different feature sets are considered to assess the influence of single features. The set Ω denotes all possible features as listed in <ref type="table" target="#tab_0">Table 1</ref>, Ω np contains all features except for the number of vertices, number of edges, number of source vertices and the number of sink vertices. These four features directly indicate numbers of trainable parameters in the model (features with no direct indication to the number of parameters). Ω op contains only those four properties.</p><p>Features with variance information only are considered in Ω var , namely the variances of the degree, eccentricity, neighborhood, path length, closeness and edge betweenness distributions. Compared to other properties of the distributions -such as the average, minimum or maximum -features with variance information have shown in experiments to have some influence on the network performance.</p><p>A manually selected set Ω small contains a reduced set of selected features, namely number source vertices, number sink vertices, degree distribution var, density, neighborhood var, path length var, closeness std, edge betweenness std, eccentricity var. <ref type="table" target="#tab_0">Table 1</ref> provides an overview of used feature sets and resulting feature importances for the RF estimator.</p><p>OLS achieved a R 2 score of 0.8631 on average for feature set Ω over 20 repetitions with a standard deviation of 0.0042. Pearson's r for OLS is on average ρ = 0.9291. SVM with RBF kernel achieved on average 0.9356 with 0.0018 in standard deviation. RF achieved a R 2 score of 0.9714 on average with a standard deviation of 0.0009. None of the other considered estimators reaches the RFestimator predicting the model performances.</p><p>The feature importances of the RF estimator are calculated with sklearn <ref type="bibr" target="#b20">[21]</ref> where "each feature importance is computed as the (normalized) total reduction of the criterion brought by that feature" 3 . They are listed in <ref type="table" target="#tab_0">Table 1</ref> for each used feature in scope of the used feature set. The number of sink vertices clearly have most influence to the performance of a MNIST-classifier.</p><p>Model  Features, which do not directly indicate the number of parameters, can be seen in the second column for Ω np and the three most important ones are highlighted in boldface. The three most important features are the variances of degree, eccentricity and neighborhood distributions of the graph. Considering six variance features together with the number of source and sink vertices leads to a R 2 value for RF of 0.9710 + − 0.0011, only deviating by 0.0004 from the best average R 2 value of 0.9714 + − 0.0009. Variance features alone already achieve an average R 2 of 0.9283 + − 0.0032 (see Ω var ). This gives indication, that e.g. in the case of eccentricity a higher diversity leads to better performance. Higher variances in those distributions imply different path lengths through the network which can be found in architectures such as Residual Networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion &amp; Future Work</head><p>This work presented motivations and approaches to study Artificial Neural Networks (ANNs) from a network science perspective. Directed Acyclic Graphs with characteristic properties from network science are obtained by Random Graph Generators (RGGs) and embedded into ANNs. A dataset of 10,000 graphs with Watts-Strogatz -and Barabási-Albert-models as RGGs was created to base experiments on. ANN models are trained on the presented structural embedding technique and performance values such as the accuracy on a validation set were obtained.</p><p>With both, structural properties and resulting performance values, three estimator models, namely an Ordinary Linear Regression, a Support Vector Machine and a Random Forest (RF) model are built. The Random Forest model is most successful in predicting ANN model performances based on the structural properties. Influence of the structural properties (features of the RF) on predicting ANN model performances is measured.</p><p>Clearly, the most important feature is the number of vertices and edges, directly determining the number of trainable parameters. There is, however, indication that the variance of distributions of properties such as the eccentricity, vertex degrees and path lengths of networks have influence for high performant models. This insight goes along with successful models such as Residual Networks and Highway Networks, which introduce more variance and thus lead to a more ensemble-like behaviour.</p><p>More such characteristic graph properties will help explain differences of various architectures. Understanding the interaction of graph properties could also be exploitet in the form of structural regularization -designing architectures or searching for them (e.g. in evolutionary approaches) could be driven by network scientific knowledge.</p><p>In future work, insights into the influence of structural properties on model performance will be hardened by more complex problem domains which assumably have more potential of revealing stronger differences. The approach with structural properties will be extended to recurrent architectures to reduce the gap between DAGs used in this work and directed cyclic networks, as they are found in nature. The performance prediction will also be integrated into neuroevolutionary methods, most likely leading to a performance boost by early stopping and improved regularized search.      <ref type="table">Table 2</ref>: Considered properties and their statistical distributions across the graphs10k dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>et al., Domhan et al. and Baker et al.. Klein et al. exploit "information in automatic hyperparameter optimization by means of a probabilistic model of learning curves across hyperparameter settings"</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a) Sketched graph from Random Graph Generator (RGG).(b) Transformed Directed Acyclic Graph (DAG) from RGG. (c) Embedded DAG between input and output layers of an Artificial Neural Network classifier.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 1 :</head><label>1</label><figDesc>Random (possibly undirected) graph, a directed transformation of it and the final embedding of the sparse structure within a Neural Network Classifier. In this example the classifier would be used for a problem with four input and two output neurons.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 :</head><label>2</label><figDesc>and eccentricity_var for graphs10k. (b) Jointplot of eccentricity variance vs. test accuracy 2a: An exemplary frequency distribution of graphs10k. With a uniformly distributed number of vertices, very different frequency distributions occur for other structural properties. 2b: Correlation between variance in eccentricity and accuracy shown in a joint plot. Axes contain distributions of each feature. The test accuracy distribution (on the top y-axis) shows that there only exist few graphs between 0.4 and 0.85 but three peaks at around 0.2, 0.35 and 0.95. For high accuracies an increasing variance in eccentricity can be observed. A low eccentricity variance, however, does not explain good performance on its own. frequency distribution of the number of edges is shown in 2a. The non-uniform distribution visualizes the difficulty of uniformly sampling in graph space 1 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>+ − 0.0009 0.9314 + − 0.0031 0.9664 + − 0.0010 0.9283 + − 0.0032 0.9710 + − 0.0011 0.9706 + − 0.0009 OLS R 2 0.8631 + − 0.0042 0.8476 + − 0.0053 0.6522 + − 0.0089 0.5968 + − 0.0103 0.7211 + − 0.0072 0.6907 + − 0.0077 SVM lin R 2 0.8620 + − 0.0045 0.8463 + − 0.0054 0.6432 + − 0.0114 0.5551 + − 0.0164 0.6943 + − 0.0101 0.6676 + − 0.0111 SVM rbf R 2 0.9356 + − 0.0018 0.9235 + − 0.0028 0.8561 + − 0.0041 0.7827 + − 0.0092 0.8781 + − 0.0045 0.8604 + − 0.0033 SVM pol R 2 0.9174 + − 0.0025 0.8998 + − 0.0032 0.5668 + − 0.0065 0.6839 + − 0.0117 0.8421 + − 0.0053 0.7543 + − 0.0083 Property RF Feature Importance number vertices 0.0009 0.0045 + − 0.0002 number edges 0.0013 0.0071 + − 0.0002 number source vertices 0.0636 0.0720 + − 0.0018 0.0643 + − 0.0019 0.0659 + − 0.0019 number sink vertices 0.9124 0.9164 + − 0.0019 0.9137 + − 0.0019 0.9139 + − 0.0020 degree distribution mean 0.0004 0.0083 + − 0.0054 degree distribution var 0.0009 0.4582 + − 0.0973 0.3685 + − 0.0866 0.0024 + − 0.0002 0.0069 + − 0+ − 0.0009 0.0133 + − 0.0018 0.0034 + − 0.0001 0.0067 + − 0+ − 0.0011 0.0036 + − 0.0001</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>055; p = 3.6e-08 Jointplot for accuracy_test and number_vertices for graphs10k.(a) Correlation between test accuracy and number of vertices of the underlying graph. 0.1; p = 1.1e-25Jointplot for accuracy_test and number_edges for graphs10k.(b) Correlation between test accuracy and number of edges of the underlying graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>0.15; p = 2.6e-54Jointplot for accuracy_test and density for graphs10k.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>(a) Correlation between test accuracy and density of the underlying graph. 32; p = 6.3e-240Jointplot for accuracy_test and path_length_var for graphs10k.(b) Correlation between test accuracy and path length variance of the underlying graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig</head><label></label><figDesc>Fig. 4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Structural properties for the SNNs' underlying graphs from the graphs10k dataset and their influence as features for the Random Forest model under different feature sets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>.8272 45.2005 1531.7422 101.3437 edge betweenness max 8.3333 441.7764 12259.0184 782.9232 edge betweenness std 1.0408 51.3070 1781.8689 117.5626</figDesc><table><row><cell>Property</cell><cell cols="3">Min Mean Max</cell><cell>Std</cell></row><row><cell>number vertices</cell><cell>50</cell><cell cols="2">269.66 490</cell><cell>129.56</cell></row><row><cell>number edges</cell><cell>97</cell><cell cols="2">1399.17 4365</cell><cell>954.12</cell></row><row><cell cols="2">number source vertices 1</cell><cell cols="2">79.7501 306</cell><cell>80.6862</cell></row><row><cell>number sink vertices</cell><cell>1</cell><cell cols="2">9.5550 125</cell><cell>17.5298</cell></row><row><cell>diameter</cell><cell>3</cell><cell>8.72</cell><cell>54</cell><cell>4.86</cell></row><row><cell>density</cell><cell cols="3">0.0041 0.0277 0.1653</cell><cell>0.0256</cell></row><row><cell cols="4">degree distribution mean 3.88 10.36 17.82</cell><cell>4.47</cell></row><row><cell cols="4">degree distribution var 0.4638 174.46 1274.97</cell><cell>259.08</cell></row><row><cell>eccentricity mean</cell><cell cols="3">1.7800 4.4104 29.2400</cell><cell>2.4913</cell></row><row><cell>eccentricity var</cell><cell cols="4">0.2784 4.6160 146.8862 9.6251</cell></row><row><cell>eccentricity max</cell><cell>3</cell><cell cols="2">8.7211 54</cell><cell>4.8585</cell></row><row><cell>neighborhood mean</cell><cell cols="3">4.8800 11.3557 18.8163</cell><cell>4.4670</cell></row><row><cell>neighborhood var</cell><cell cols="4">0.4571 173.7798 1272.3148 258.3377</cell></row><row><cell>neighborhood min</cell><cell>1</cell><cell cols="2">5.7161 14</cell><cell>2.8536</cell></row><row><cell>neighborhood max</cell><cell>7</cell><cell cols="2">75.8607 312</cell><cell>72.7306</cell></row><row><cell>path length mean</cell><cell cols="3">1.3008 2.7941 14.2195</cell><cell>1.4005</cell></row><row><cell>path length var</cell><cell cols="3">0.2229 1.9988 53.2144</cell><cell>3.8514</cell></row><row><cell>closeness min</cell><cell cols="3">0.0020 0.3229 0.5698</cell><cell>0.1275</cell></row><row><cell>closeness mean</cell><cell cols="3">0.0476 0.4007 0.6150</cell><cell>0.1132</cell></row><row><cell>closeness max</cell><cell cols="3">0.0514 0.5324 0.9672</cell><cell>0.1824</cell></row><row><cell>closeness std</cell><cell cols="3">0.0090 0.0306 0.1137</cell><cell>0.0151</cell></row><row><cell>edge betweenness min</cell><cell>1</cell><cell cols="2">1.0034 3</cell><cell>0.0467</cell></row><row><cell cols="2">edge betweenness mean 1</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>. 4</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The eccentricity of a vertex in a connected graph is the maximum distance to any other vertex.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Taken from http://scikit-learn.org/stable/modules/generated/sklearn. tree.DecisionTreeRegressor.html which references Breiman, Friedman, Classification and regression trees, 1984</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Statistical mechanics of complex networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Barabási</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Reviews of modern physics</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">47</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Accelerating neural architecture search using performance prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Naik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On the maximal number of strongly independent vertices in a random acyclic directed graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Barak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Erdös</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Algebraic Discrete Methods</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="508" to="514" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bourely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Boueri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Choromonski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05683</idno>
		<title level="m">Sparse neural networks topologies</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Speeding up automatic hyperparameter optimization of deep neural networks by extrapolation of learning curves</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Domhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: IJCAI</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="3460" to="3468" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On the evolution of random graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Erdös</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rényi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Publ. Math. Inst. Hung. Acad. Sci</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="17" to="61" />
			<date type="published" when="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Fourteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning both weights and connections for efficient neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1135" to="1143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Is the brain really a small-world network? Brain Structure and Function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Hilgetag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Goulas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">221</biblScope>
			<biblScope unit="page" from="2361" to="2366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<title level="m">Untersuchungen zu dynamischen neuronalen netzen. Diploma, Technische Universität München</title>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Random generation of bayesian networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Ide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">G</forename><surname>Cozman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Brazilian symposium on artificial intelligence</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="366" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Random graph models for directed acyclic networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Karrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">46110</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Falkner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<title level="m">Learning curve prediction with bayesian neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Handwritten digit recognition with a back-propagation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">E</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page" from="396" to="404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Burges</surname></persName>
		</author>
		<title level="m">The mnist database of handwritten digits</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Louizos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.01312</idno>
		<title level="m">Learning sparse neural networks through l 0 regularization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Mocanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mocanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gibescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liotta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.04780</idno>
		<title level="m">Evolutionary training of sparse artificial neural networks: a network science perspective</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Training sparse neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Subramanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="455" to="462" />
		</imprint>
	</monogr>
	<note>2017 IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00387</idno>
		<title level="m">Highway networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Analysing neural network topologies: a game theoretic approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gianini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Granitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ziegler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Procedia Computer Science</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="page" from="234" to="243" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Collective dynamics of &apos;small-world&apos;networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Watts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Strogatz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">393</biblScope>
			<biblScope unit="issue">6684</biblScope>
			<biblScope unit="page">440</biblScope>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
