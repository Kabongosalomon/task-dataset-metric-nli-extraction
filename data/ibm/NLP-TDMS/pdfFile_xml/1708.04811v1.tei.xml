<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Language Identification Using Deep Convolutional Recurrent Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-08-16">16 Aug 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Bartz</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Hasso Plattner Institute</orgName>
								<orgName type="institution">University of Potsdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Herold</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Hasso Plattner Institute</orgName>
								<orgName type="institution">University of Potsdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haojin</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Hasso Plattner Institute</orgName>
								<orgName type="institution">University of Potsdam</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Meinel</surname></persName>
							<email>meinel@hpi.detom.herold@student.hpi.de</email>
							<affiliation key="aff0">
								<orgName type="department">Hasso Plattner Institute</orgName>
								<orgName type="institution">University of Potsdam</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Language Identification Using Deep Convolutional Recurrent Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-08-16">16 Aug 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Language Identification (LID) systems are used to classify the spoken language from a given audio sample and are typically the first step for many spoken language processing tasks, such as Automatic Speech Recognition (ASR) systems. Without automatic language detection, speech utterances cannot be parsed correctly and grammar rules cannot be applied, causing subsequent speech recognition steps to fail. We propose a LID system that solves the problem in the image domain, rather than the audio domain. We use a hybrid Convolutional Recurrent Neural Network (CRNN) that operates on spectrogram images of the provided audio snippets. In extensive experiments we show, that our model is applicable to a range of noisy scenarios and can easily be extended to previously unknown languages, while maintaining its classification accuracy. We release our code and a large scale training set for LID systems to the community. â‹† equal contribution 1 https://www.apple.com/ios/siri/ 2 https://assistant.google.com/</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Intelligent assistants like Siri 1 or the Google Assistant 2 rely on ASR. Current ASR systems require users to manually specify the system's correct input language to work properly. However, as a sensible pre-processing step we can infer the spoken language using an automatic LID system. Traditional LID systems utilize domain-specific expert knowledge in the field of audio signal processing for extracting hand-crafted features from the audio samples. Lately, deep learning and artificial neural networks have become the state-of-the-art for many pattern recognition problems. Deep Neural Networks (DNNs) have become the best performing method for a range of computer vision tasks, such as image classification <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>, or object detection and recognition <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>.</p><p>In this paper, we address the problem of language identification from a computer vision perspective. We extract the target language of a given audio sample by utilizing a hybrid network constructed of a Convolutional Neural Network (CNN) combined with an Recurrent Neural Network (RNN). Our contributions can be summarized as follows: <ref type="bibr" target="#b0">(1)</ref> we propose a hybrid CRNN, combining the descriptive powers of CNNs with the ability to capture temporal features of RNNs. <ref type="bibr" target="#b1">(2)</ref> We perform extensive experiments with our proposed network and show its applicability to a range of scenarios and its extensibility to new languages. <ref type="bibr" target="#b2">(3)</ref> We release our code and a large scale training set for LID systems to the community <ref type="bibr" target="#b2">3</ref> .</p><p>The paper is structured in the following way: In section 2 we introduce related work in the field of LID systems. We showcase our system in section 3 and evaluate it on extensive experiments in section 4. We conclude our work in section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Traditional language identification systems are based on identity vector systems for spoken-language processing tasks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b18">19]</ref>. In the recent years systems using feature extractors solely based on neural networks, especially Long-Short Term Memory (LSTM) networks, became more popular <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b18">19]</ref>. These neural networks based systems are better suited to LID tasks, since they are both simpler in design and provide a higher accuracy than traditional approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Identity Vector Systems</head><p>Identity vector systems (i-vectors) have been introduced by Dehak et al. <ref type="bibr" target="#b3">[4]</ref> for the purpose of speaker verfication tasks. i-vectors are a special form of joined lowdimensional representations of speaker and channel factors, as found in earlier joint factor analysis supervectors.</p><p>Identity vectors are employed as a data representation in many systems and are fed as inputs to a classifier. Dehak et al. <ref type="bibr" target="#b3">[4]</ref> used Support Vector Machines (SVMs) with cosine kernels. Other researchers used logistic regression <ref type="bibr" target="#b10">[11]</ref> or neural networks with three to four layers <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13]</ref>. Gelly et al. <ref type="bibr" target="#b5">[6]</ref> proposed a complex system consisting of phonotactic components <ref type="bibr" target="#b19">[20]</ref>, identity vectors, a lexical system, and Bidirectional Long-Short Term Memory (BLSTM) networks for language identification. The extensive feature engineering with i-vectors results in very complex systems, with an increasing number of computational steps in their pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Neural Network Approaches</head><p>Approaches solely based on applying neural networks on input features like Mel-Frequency Cepstral Coefficients (MFCC) show that they reach state-of-the-art results, while being less complex.</p><p>Current research on language identification systems using DNNs mainly focuses on using different forms of LSTMs, working on input sequences of transformed audio data. Zazo et al. <ref type="bibr" target="#b18">[19]</ref> use Mel Frequency Cepstral Coefficients with Shifted Delta Coefficients (MFCC-SDC) features as input to their unidirectional LSTM, which is directly connected to a softmax classifier. The last prediction of the softmax classifier contains the predicted language. Gelly et al. <ref type="bibr" target="#b4">[5]</ref> use a BLSTM network to capture language information from the input (audio converted to Perceptual Linear Prediction (PLP) coefficients and their first and second order derivatives) in forward and backward direction. The resulting sequence features are fused together and used to classify the language of the input samples. Both approaches only consider sequences of features as input to their networks.</p><p>Lozano-Diez et al. <ref type="bibr" target="#b9">[10]</ref> perform language identification with the help of CNNs. The authors transform the input data to an image containing MFCC-SDC features. The x-axis of that image represents the time-domain and the y-axis describes the individual frequency bins. Besides plain classification of the input languages with the CNN, they also use the CNN as feature extractor for identity vectors. The authors achieve better performance when combining both the CNN features and identity vectors.</p><p>Our research differs from the mentioned works in the following way: (1) We utilize a strong convolutional feature extractor based on the VGG <ref type="bibr" target="#b16">[17]</ref> or Inception-v3 <ref type="bibr" target="#b17">[18]</ref> architecture. <ref type="bibr" target="#b1">(2)</ref> We use the extracted convolutional features as input to a BLSTM and generate our predictions solely based on a deep model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed System</head><p>In our work, we utilize the power of CNNs to capture spatial information, and the power of RNNs to capture information through a sequence of time steps for identifying the language from a given audio snippet. We developed a DNN based on a sequence recognition network presented by Shi et al. <ref type="bibr" target="#b15">[16]</ref>. In this section, we present the datasets we used for training the network, the audio representation used for training our models, and the structure of our proposed network in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>Since there are no large-scale, freely available datasets for LID tasks (datasets such as the NIST Language Recognition Evaluation are only available behind a paywall), we resorted to creating our own datasets for our experiments. We collected our datasets from two different sources: (1) we processed speeches, press conferences and statements from the European Parliament, and (2) we sourced data from news broadcast channels hosted on YouTube. We chose to collect data for 6 different languages, while making sure that we include languages with similar phonetics. Following this idea, we collected data for two Germanic languages (English and German), two Romance languages (French and Spanish), Russian, and Mandarin Chinese.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EU Speech Repository</head><p>The EU Speech Repository 4 is a collection of video resources for interpretation students. This dataset is provided for free and con-sists of debates of the European Parliament, as well as press conferences, interviews, and dedicated training materials from EU interpreters. Each audio clip is recorded in the speaker's native language and features only one speaker. The dataset consists of many different female and male speakers. From this dataset we collected 131 hours of speech data in four languages: English, German, French and Spanish.</p><p>YouTube News Collection We chose to use news broadcasts as a second data source to obtain audio snippets of similar quality to the EU Speech Repository (different speakers, mostly one speaker at a time and a single defined language). We gathered all data from YouTube channels such as the official BBC News 5 YouTube channel.</p><p>The obtained audio data has many desired properties. The quality of the audio recordings is very high and hundreds of hours are available online. News programs often feature guests or remote correspondents resulting in a good mix of different speakers. Further, news programs feature noise one would expect from a real-world situation: music jingles, nonspeech audio from video clips and transitions between reports. All in all, we were able to gather 1508 hours of audio data for this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Audio Representation</head><p>To make our gathered data compatible with our LID system, we need to do some preprocessing. As a first step, we encode all audio files in the uncompressed, lossless WAVE format, as this format allows for future manipulations without any deterioration in signal quality. In order to treat our audio snippets as images, we need to transfer the data into the image domain. We convert our audio data to spectrogram representations for training our models. The spectrograms are discretized using a Hann <ref type="bibr" target="#b1">[2]</ref> window and 129 frequency bins along the frequency axis (y-axis). As most phonemes in the English language do not exceed 3 kHz in conversational speech, we only included frequencies of up to 5kHz in the spectrograms. The time axis (x-axis) is rendered at 50 pixels per second. We split each audio sequence into nonoverlapping ten-second segments and discard all segments shorter than ten seconds, as we did not want to introduce padding, which might resemble unnatural pauses or silence. The resulting images are saved as grayscale, lossless 500Ã—129 PNG files, where frequency intensities are mapped to an eight-bit grayscale range.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Architecture</head><p>For our network architecture, we followed the overall structure of the network proposed by Shi et al. <ref type="bibr" target="#b15">[16]</ref> in their work on scene text recognition. This network architecture consists of two parts. The first part is a convolutional feature extractor that takes a spectrogam image representation of the audio file as input (see section 3.2). This feature extractor convolves the input image in several steps and produces a feature map with a height of one. This feature map is sliced along the x-axis and each slice is used as a time step for the subsequent BLSTM network. The design of the convolutional feature extractor is based on the well known VGG architecture <ref type="bibr" target="#b16">[17]</ref>. Our network uses five convolutional layers, where each layer is followed by the ReLU activation function <ref type="bibr" target="#b11">[12]</ref>, BatchNormalization <ref type="bibr" target="#b7">[8]</ref> and 2 Ã— 2 max pooling with a stride of 2. The kernel sizes and number of filters for each convolutional layer are (7Ã—7, 16), (5Ã—5, 32), (3Ã—3, 64), (3Ã—3, 128), (3Ã—3, 256), respectively. The BLSTM consists of two single LSTMs with 256 outputs units each. We concatenate both outputs to a vector of 512 dimensions and feed this into a fully-connected layer with 4/6 output units serving as the classifier. <ref type="figure" target="#fig_0">Figure 1</ref> provides a schematic overview of the network architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Using our gathered dataset and the network architecture introduced in section 3, we conducted several experiments, to assess the performance of our proposed network architecture on several kinds of input data. While performing our experiments we had a range of different questions in mind:</p><p>-Can we increase the classification accuracy with the help of a network, that combines a CNN with a LSTM, compared to a CNN-only approach? -Is the network able to reliably discriminate between languages? -Is the network robust against different forms of noise in the input data? -Can the network easily be extended to handle other languages as well?</p><p>First, we shortly introduce our experimental environment and the metrics used. Then, we show our results on the EU Speech Repository and YouTube News dataset. Following this, we show the results of our experiments on noise robustness. We conclude this section with a discussion about the extensibility of our model to new languages. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Environment</head><p>We implemented our proposed model using Keras <ref type="bibr" target="#b2">[3]</ref> with the Tensorflow <ref type="bibr" target="#b0">[1]</ref> backend. We splitted the datasets into a training (70 %), a validation (20 %) and a testing set (10 %), and all files were distributed equally between the languages. The European Speech dataset yields a total of about 19 000 training images, which amounts to roughly 53 hours of speech audio. The YouTube News dataset yields a total of about 194 000 training images, or 540 hours of speech audio. For training our networks, we used the Adam <ref type="bibr" target="#b8">[9]</ref> optimizer and resorted to using stochastic gradient descent during fine-tuning. We observed the following metrics: accuracy, recall, precision and F1 score. We indicate the used networks in the following way: (1) CNN -A network only consisting of the proposed convolutional feature extractor without the recurrent part.</p><p>(2) CRNN -The proposed hybrid CRNN model from section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">EU Speech Dataset</head><p>In order to verify our idea of applying CNNs for classifying image representations of audio data, we established a baseline with the smaller EU Speech Repository dataset. <ref type="figure">Figure 2</ref> shows the results for the two network architectures (CNN and CRNN). As can be seen, the CRNN architecture outperforms the plain network without the recurrent part significantly. This proves our assumption that combining a recurrent network with a convolutional feature extractor increases the accuracy for spoken language identification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">YouTube News</head><p>After achieving these promising results, we trained our networks on the considerably larger YouTube News dataset. We used only the same four languages that are available in both datasets in order to have comparable results between  <ref type="figure">Fig. 4</ref>: Performance of the trained networks on the YouTube News dataset these. First, we took the pre-trained CNN from our experiments on the EU Speech Repository dataset and fine-tuned this model with the data from the YouTube dataset. However, with an accuracy of 79 %, our CNN did not perform as accurately as expected. We argue that this is the case, because the EU dataset does not feature such a diverse range of situations, as the YouTube News dataset. Hence, we were unable to take advantage of the already trained convolutional features and the network did not converge on the new dataset.</p><p>We trained the CNN again, this time from scratch, and were able to achieve an accuracy of 90 % on this dataset. Our CRNN, using the trained CNN as feature extractor, is only able to increase the accuracy to 91 %. We argue that the CNN already learns to capture some time-related information from the input images.</p><p>As a next step, we evaluate how our proposed model architecture compares to a model with a stronger convolutional feature extractor. Therefore, we trained a model using Google's Inception-v3 layout <ref type="bibr" target="#b17">[18]</ref>. This network architecture is considerably deeper than our proposed CRNN architecture and hence should be able to extract more general features. With Inception-v3 we were able to achieve an accuracy of 95 % and 96 % using the CNN and CRNN, respectively. These results show that the deeper model is able to catch more general features on our large YouTube dataset. However, this increase in accuracy comes with an increase of computational cost, as the Inception-v3 model uses six times more parameters, than our initially proposed CNN. <ref type="figure">Figure 3</ref> shows the confusion matrix when evaluating language family pairs on our best-performing CRNN. Spanish and French separate very well with hardly any wrong classifications. German and English are more likely to be confused, but English has a slighty stronger bias towards French. All in all, the learned representations of the model are quite distinctive for each language. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Noise Robustness</head><p>In our next series of experiments, we added three different forms of noise to our test data. We evaluated our already trained models to judge their robustness to these changed conditions. First, we mixed the audio signal with randomly generated white noise, which has a strong audible presence, but still retains the identifiability of the language. Second, we added a periodic crackling noise, emulating analog telephony or a bad voice chat connection. For the last experiment, we added background music from different genres to our samples. We performed all experiments using our initially proposed CRNN and the Inception-v3 CRNN architectures. As expected we observed a decrease in accuracy and F1 score for both models, accross all experiments. The decrease in accuracy and F1 score is significantly higher for the initially proposed CRNN model, but relatively small for the Inception-v3 CRNN. We argue that this is the case because the Inception-v3 CRNN, with its deeper and more complex structure, is able to capture the frequency features in a more robust manner. <ref type="table">Table 1</ref> shows the results of our experiments with additional noise on the YouTube dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Extensibility of the network</head><p>In our last experiment, we evaluated how well our model is able to expand its capabilities to also include new languages, and how adding further languages affects the overall quality of the results produced by the network. We extended the existing set of four languages by two further languages spoken by millions around the globe: Mandarin Chinese and Russian. First, we fine-tuned our best performing CNN based on our proposed architecture. The resulting model served as the basis for training the CRNN as described earlier. Applied on the test set, we measure an accuracy of 92 % and an F1 score of 0.92. Both measurements match our previous evaluation with four languages on the YouTube News dataset, proving that the proposed CRNN architecture can indeed easily be extended to cover more languages. <ref type="figure" target="#fig_2">Figure 5</ref> shows individual performance measures for each language.</p><p>We note that Mandarin Chinese outperforms every other language with a top Accuracy of 96 %, which is likely due to the fact that the sound of Mandarin Chinese is very distinct compared to western languages. We also find that English  is now the worst performing language, which is in part due to a significant number of misclassifications as Russian samples.</p><p>In summary, we are content to find that the features learned by our model are universal in nature, as both new languages are rooted within their own language families and feature considerably different intonations. We believe that our proposed approach to language identification can be successfully applied to a wide variety of languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper we proposed a language identification system, that solves the language identification problem in the image domain, rather than the audio domain. We proposed a hybrid CRNN that consists of a convolutional feature extractor and a RNN that combines the extracted features over time. Using this architecture, we performed several experiments on different datasets to show the wide applicability of our model to various scenarios and its extensibility to new languages. In order to compensate for the lack of freely available datasets for language identification, we gathered more than 1508 hours of audio data from the EU Speech Repository and YouTube and offer them to the research community.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Our proposed CRNN network architecture consists of two parts. A CNN extracts local visual features from our input images. The output of the final convolutional layer t Ã— 1 Ã— c is sliced along the time axis into t time steps. Each time step represents the extracted frequency features, used as input to the LSTM. The final LSTM output is fed into a fully-connected layer for classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :Fig. 3 :</head><label>23</label><figDesc>Performance of the proposed network architectures on the EU Speech Repository dataset Confusion matrix for our best performing CRNN, trained on the YouTube News dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 :</head><label>5</label><figDesc>Individual performance measurements for each of our six target languages. Chinese performs best, while English performs worst. Overall, the model performance is consistent with the results of earlier experiments reported in section 4.3.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/HPI-DeepLearning/crnn-lid</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://webgate.ec.europa.eu/sr/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://www.youtube.com/user/bbcnews</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Tensorflow: Large-scale machine learning on heterogeneous distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The measurement of power spectra from the point of view of communications engineering-part i</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Blackman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Tukey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell Labs Technical Journal</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="185" to="282" />
			<date type="published" when="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">keras: Deep learning library for python. runs on tensorflow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://github.com/fchollet/keras" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Front-end factor analysis for speaker verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Kenny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dumouchel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ouellet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="788" to="798" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A divide-and-conquer approach for language identification based on recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Gauvain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Messaoudi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3231" to="3235" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Language recognition for dialects and closely related languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Gauvain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lamel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">B</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Messaoudi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<pubPlace>Odyssey, Bilbao, Spain</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Frame-by-frame language identification in short utterances using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gonzalez-Dominguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lopez-Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gonzalez-Rodriguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="49" to="58" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 32nd International Conference on Machine Learning</title>
		<meeting>The 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning Represenations</title>
		<meeting>the 3rd International Conference on Learning Represenations<address><addrLine>San Diego</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An end-to-end approach to language identification in short utterances using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lozano-Dez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zazo Candil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gonzlez Domnguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">T</forename><surname>Toledano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gonzlez-Rodrguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Speech and Communication Association</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Language recognition in ivectors space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martnez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Plchot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Matjka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twelfth Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th international conference on machine learning (ICML-10)</title>
		<meeting>the 27th international conference on machine learning (ICML-10)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Bat system description for nist lre</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Plchot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Matejka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Novotny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pesan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Brummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cumani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="166" to="173" />
			<pubPlace>Odyssey</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28</title>
		<editor>Cortes, C., Lawrence, N.D., Lee, D.D., Sugiyama, M., Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Language identification in short utterances using long short-term memory (lstm) recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zazo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lozano-Diez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gonzalez-Dominguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">T</forename><surname>Toledano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gonzalez-Rodriguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">146917</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Comparison of four approaches to automatic language identification of telephone speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Zissman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on speech and audio processing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
