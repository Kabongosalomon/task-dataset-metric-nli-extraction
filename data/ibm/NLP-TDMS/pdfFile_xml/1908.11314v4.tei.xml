<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Variational Denoising Network: Toward Blind Noise Modeling and Removal</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongsheng</forename><surname>Yue</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Mathematics and Statistics</orgName>
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
								<address>
									<settlement>Shaanxi</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">Hong Kong Polytechnic University</orgName>
								<address>
									<settlement>Kowloon, Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Yong</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">Hong Kong Polytechnic University</orgName>
								<address>
									<settlement>Kowloon, Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Mathematics and Statistics</orgName>
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
								<address>
									<settlement>Shaanxi</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">Hong Kong Polytechnic University</orgName>
								<address>
									<settlement>Kowloon, Hong Kong</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">DAMO Academy</orgName>
								<orgName type="institution">Alibaba Group</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Mathematics and Statistics</orgName>
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
								<address>
									<settlement>Shaanxi</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Faculty of Information Technology</orgName>
								<orgName type="institution">The Macau University of Science and Technology</orgName>
								<address>
									<settlement>Macau</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Variational Denoising Network: Toward Blind Noise Modeling and Removal</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Blind image denoising is an important yet very challenging problem in computer vision due to the complicated acquisition process of real images. In this work we propose a new variational inference method, which integrates both noise estimation and image denoising into a unique Bayesian framework, for blind image denoising. Specifically, an approximate posterior, parameterized by deep neural networks, is presented by taking the intrinsic clean image and noise variances as latent variables conditioned on the input noisy image. This posterior provides explicit parametric forms for all its involved hyper-parameters, and thus can be easily implemented for blind image denoising with automatic noise estimation for the test noisy image. On one hand, as other data-driven deep learning methods, our method, namely variational denoising network (VDN), can perform denoising efficiently due to its explicit form of posterior expression. On the other hand, VDN inherits the advantages of traditional model-driven approaches, especially the good generalization capability of generative models. VDN has good interpretability and can be flexibly utilized to estimate and remove complicated non-i.i.d. noise collected in real scenarios. Comprehensive experiments are performed to substantiate the superiority of our method in blind image denoising.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Image denoising is an important research topic in computer vision, aiming at recovering the underlying clean image from an observed noisy one. The noise contained in a real noisy image is generally accumulated from multiple different sources, e.g., capturing instruments, data transmission media, image quantization, etc. <ref type="bibr" target="#b38">[39]</ref>. Such complicated generation process makes it fairly difficult to access the noise information accurately and recover the underlying clean image from the noisy one. This constitutes the main aim of blind image denoising.</p><p>There are two main categories of image denoising methods. Most classical methods belong to the first category, mainly focusing on constructing a rational maximum a posteriori (MAP) model, involving the fidelity (loss) and regularization terms, from a Bayesian perspective <ref type="bibr" target="#b5">[6]</ref>. An understanding for data generation mechanism is required for designing a rational MAP objective, especially better image priors like sparsity <ref type="bibr" target="#b2">[3]</ref>, low-rankness <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b40">41]</ref>, and non-local similarity <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b26">27]</ref>. These methods are superior mainly in their interpretability naturally led by the Bayesian framework. They, however, still exist critical limitations due to their assumptions on both image prior and noise (generally i.i.d. Gaussian), possibly deviating from real spatially variant (i.e.,non-i.i.d.) noise, and their relatively low implementation speed since the algorithm needs to be re-implemented for any new coming image. Recently, deep learning approaches represent a new trend along this research line. The main idea is to firstly collect large amount of noisy-clean image pairs and then train a deep neural network denoiser on these training data in an end-to-end learning manner. This approach is especially superior in its effective accumulation of knowledge from large datasets and fast denoising speed for test images.</p><p>They, however, are easy to overfit to the training data with certain noisy types, and still could not be generalized well on test images with unknown but complicated noises.</p><p>Thus, blind image denoising especially for real images is still a challenging task, since the real noise distribution is difficult to be pre-known (for model-driven MAP approaches) and hard to be comprehensively simulated by training data (for data-driven deep learning approaches).</p><p>Against this issue, this paper proposes a new variational inference method, aiming at directly inferring both the underlying clean image and the noise distribution from an observed noisy image in a unique Bayesian framework. Specifically, an approximate posterior is presented by taking the intrinsic clean image and noise variances as latent variables conditioned on the input noisy image. This posterior provides explicit parametric forms for all its involved hyper-parameters, and thus can be efficiently implemented for blind image denoising with automatic noise estimation for test noisy images.</p><p>In summary, this paper mainly makes following contributions: 1) The proposed method is capable of simultaneously implementing both noise estimation and blind image denoising tasks in a unique Bayesian framework. The noise distribution is modeled as a general non-i.i.d. configurations with spatial relevance across the image, which evidently better complies with the heterogeneous real noise beyond the conventional i.i.d. noise assumption. 2) Succeeded from the fine generalization capability of the generative model, the proposed method is verified to be able to effectively estimate and remove complicated non-i.i.d. noises in test images even though such noise types have never appeared in training data, as clearly shown in <ref type="figure" target="#fig_4">Fig. 3.</ref> 3) The proposed method is a generative approach outputted a complete distribution revealing how the noisy image is generated. This not only makes the result with more comprehensive interpretability beyond traditional methods purely aiming at obtaining a clean image, but also naturally leads to a learnable likelihood (fidelity) term according to the data-self. 4) The most commonly utilized deep learning paradigm, i.e., taking MSE as loss function and training on large noisy-clean image pairs, can be understood as a degenerated form of the proposed generative approach. Their overfitting issue can then be easily explained under this variational inference perspective: these methods intrinsically put dominant emphasis on fitting the priors of the latent clean image, while almost neglects the effect of noise variations. This makes them incline to overfit noise bias on training data and sensitive to the distinct noises in test noisy images.</p><p>The paper is organized as follows: Section 2 introduces related work. Sections 3 presents the proposed full Bayesion model, the deep variational inference algorithm, the network architecture and some discussions. Section 4 demonstrates experimental results and the paper is finally concluded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>We present a brief review for the two main categories of image denoising methods, i.e., model-driven MAP based methods and data-driven deep learning based methods.</p><p>Model-driven MAP based Methods: Most classical image denoising methods belong to this category, through designing a MAP model with a fidelity/loss term and a regularization one delivering the pre-known image prior. Along this line, total variation denoising <ref type="bibr" target="#b35">[36]</ref>, anisotropic diffusion <ref type="bibr" target="#b29">[30]</ref> and wavelet coring <ref type="bibr" target="#b36">[37]</ref> use the statistical regularities of images to remove the image noise. Later, the nonlocal similarity prior, meaning many small patches in a non-local image area possess similar configurations, was widely used in image denoising. Typical ones include CBM3D <ref type="bibr" target="#b10">[11]</ref> and non-local means <ref type="bibr" target="#b8">[9]</ref>. Some dictionary learning methods <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b40">41]</ref> and Field-of-Experts (FoE) <ref type="bibr" target="#b34">[35]</ref>, also revealing certain prior knowledge of image patches, had also been attempted for the task. Several other approaches focusing on the fidelity term, which are mainly determined by the noise assumption on data. E.g., Mulitscale <ref type="bibr" target="#b22">[23]</ref> assumed the noise of each patch and its similar patches in the same image to be correlated Gaussian distribution, and LR-MoG <ref type="bibr" target="#b47">[48]</ref>, DP-GMM <ref type="bibr" target="#b42">[43]</ref> and DDPT <ref type="bibr" target="#b46">[47]</ref> fitted the image noise by using Mixture of Gaussian (MoG) as an approximator for noises.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data-driven Deep Learning based Methods:</head><p>Instead of pre-setting image prior, deep learning methods directly learn a denoiser (formed as a deep neural network) from noisy to clean ones on a large collection of noisy-clean image pairs. Jain and Seung <ref type="bibr" target="#b18">[19]</ref> firstly adopted a five layer convolution neural network (CNN) for the task. Then some auto-encoder based methods <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b1">2]</ref> were applied. Meantime, Burger et al. <ref type="bibr" target="#b9">[10]</ref> achieved the comparable performance with BM3D using plain multi-layer perceptron (MLP). Zhang et al. <ref type="bibr" target="#b43">[44]</ref> further proposed the denoising convolution network (DnCNN) and achieved state-of-the-art performance on Gaussian denoising tasks. Mao et al. <ref type="bibr" target="#b28">[29]</ref> proposed a deep fully convolution encoding-decoding network with symmetric skip connection. Tai et al. <ref type="bibr" target="#b37">[38]</ref> preposed a very deep persistent memory network (MemNet) to explicitly mine persistent memory through an adaptive learning process. Recently, NLRN <ref type="bibr" target="#b24">[25]</ref>, N3Net <ref type="bibr" target="#b31">[32]</ref> and UDNet <ref type="bibr" target="#b23">[24]</ref> all embedded the non-local property of image into DNN to facilitate the denoising task. In order to boost the flexibility against spatial variant noise, FFDNet <ref type="bibr" target="#b44">[45]</ref> was proposed by pre-evaluating the noise level and inputting it to the network together with the noisy image. Guo et al. <ref type="bibr" target="#b16">[17]</ref> and Brooks et al. <ref type="bibr" target="#b7">[8]</ref> both attempted to simulate the generation process of the images in camera.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Variational Denoising Network for Blind Noise Modeling</head><p>Given training set D = {y j , x j } n j=1 , where y j , x j denote the j th training pair of noisy and the expected clean images, n represents the number of training images, our aim is to construct a variational parametric approximation to the posterior of the latent variables, including the latent clean image and the noise variances, conditioned on the noisy image. Note that for the noisy image y, its training pair x is generally a simulated "clean" one obtained as the average of many noisy ones taken under similar camera conditions <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b0">1]</ref>, and thus is always not the exact latent clean image z. This explicit parametric posterior can then be used to directly infer the clean image and noise distribution from any test noisy image. To this aim, we first need to formulate a rational full Bayesian model of the problem based on the knowledge delivered by the training image pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Constructing Full Bayesian Model Based on Training Data</head><formula xml:id="formula_0">Denote y = [y 1 , · · · , y d ] T and x = [x 1 , · · · , x d ] T as any training pair in D, where d (width*height)</formula><p>is the size of a training image 1 . We can then construct the following model to express the generation process of the noisy image y:</p><formula xml:id="formula_1">y i ∼ N (y i |z i , σ 2 i ), i = 1, 2, · · · , d,<label>(1)</label></formula><p>where z ∈ R d is the latent clean image underlying y, N (·|µ, σ 2 ) denotes the Gaussian distribution with mean µ and variance σ 2 . Instead of assuming i.i.d. distribution for the noise as conventional <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b40">41]</ref>, which largely deviates the spatial variant and signal-depend characteristics of the real noise <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b7">8]</ref>, we models the noise as a non-i.i.d. and pixel-wise Gaussian distribution in Eq. (1).</p><p>The simulated "clean" image x evidently provides a strong prior to the latent variable z. Accordingly we impose the following conjugate Gaussian prior on z:</p><formula xml:id="formula_2">z i ∼ N (z i |x i , ε 2 0 ), i = 1, 2, · · · , d,<label>(2)</label></formula><p>where ε 0 is a hyper-parameter and can be easily set as a small value.</p><p>Besides, for σ 2 = {σ 2 1 , σ 2 2 , · · · , σ 2 d }, we also introduce a rational conjugate prior as follows:</p><formula xml:id="formula_3">σ 2 i ∼ IG σ 2 i | p 2 2 − 1, p 2 ξ i 2 , i = 1, 2, · · · , d,<label>(3)</label></formula><p>where IG(·|α, β) is the inverse Gamma distribution with parameter α and β, ξ = G (ŷ −x) 2 ; p represents the filtering output of the variance map (ŷ −x) 2 by a Gaussian filter with p × p window, andŷ,x ∈ R h×w are the matrix (image) forms of y, x ∈ R d , respectively. Note that the mode of above IG distribution is ξ i <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b41">42]</ref>, which is a approximate evaluation of σ 2 i in p × p window. Combining Eqs. (1)-(3), a full Bayesian model for the problem can be obtained. The goal then turns to infer the posterior of latent variables z and σ 2 from noisy image y, i.e., p(z, σ 2 |y).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Variational Form of Posterior</head><p>We first construct a variational distribution q(z, σ 2 |y) to approximate the posterior p(z, σ 2 |y) led by Eqs. (1)-(3). Similar to the commonly used mean-field variation inference techniques, we assume conditional independence between variables z and σ 2 , i.e., q(z, σ 2 |y) = q(z|y)q(σ 2 |y).</p><p>Based on the conjugate priors in Eqs. <ref type="bibr" target="#b1">(2)</ref> and <ref type="formula" target="#formula_3">(3)</ref>, it is natural to formulate variational posterior forms of z and σ 2 as follows:</p><formula xml:id="formula_5">q(z|y) = d i N (z i |µ i (y; W D ), m 2 i (y; W D )), q(σ 2 |y) = d i IG(σ 2 i |α i (y; W S ), β i (y; W S )), (5) ( ( | )|| ) ( ( 2 | )|| 2 ) ( , 2 ) [log , 2 ] ℒ( , 2 ; )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D-Net: Denoising Network</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S-Net: Sigma Network</head><p>Variational Posterior: where µ i (y; W D ) and m 2 i (y; W D ) are designed as the prediction functions for getting posterior parameters of latent variable z directly from y. The function is represented as a network, called denoising network or D-Net, with parameters W D . Similarly, α i (y; W S ) and β i (y; W S ) denote the prediction functions for evaluating posterior parameters of σ 2 from y, where W S represents the parameters of the network, called Sigma network or S-Net. The aforementioned is illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>. Our aim is then to optimize these network parameters W D and W S so as to get the explicit functions for predicting clean image z as well as noise knowledge σ 2 from any test noisy image y. A rational objective function with respect to W D and W S is thus necessary to train both the networks.</p><formula xml:id="formula_6">= , 2 2 = ( 2 | , )</formula><p>Note that the network parameters W D and W S are shared by posteriors calculated on all training data, and thus if we train them on the entire training set, the method is expected to induce the general statistical inference insight from noisy image to its underlying clean image and noise level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Variational Lower Bound of Marginal Data Likelihood</head><p>For notation convenience, we simply write µ i (y;</p><formula xml:id="formula_7">W D ), m 2 i (y; W D ), α i (y; W S ), β i (y; W S ) as µ i , m 2</formula><p>i , α i , β i in the following calculations. For any noisy image y and its simulated "clean" image x in the training set, we can decompose its marginal likelihood as the following form <ref type="bibr" target="#b6">[7]</ref>:</p><formula xml:id="formula_8">log p(y) = L(z, σ 2 ; y) + D KL q(z, σ 2 |y)||p(z, σ 2 |y) ,<label>(6)</label></formula><p>where</p><formula xml:id="formula_9">L(z, σ 2 ; y) = E q(z,σ 2 |y) log p(y|z, σ 2 )p(z)p(σ 2 ) − log q(z, σ 2 |y) ,<label>(7)</label></formula><p>Here</p><formula xml:id="formula_10">E p(x) [f (x)] represents the exception of f (x) w.r.t. stochastic variable x with probability density function p(x).</formula><p>The second term of Eq. <ref type="formula" target="#formula_8">(6)</ref> is a KL divergence between the variational approximate posterior q(z, σ 2 |y) and the true posterior p(z, σ 2 |y) with non-negative value. Thus the first term L(z, σ 2 ; y) constitutes a variational lower bound on the logarithm of marginal likelihood p(y), i.e.,</p><formula xml:id="formula_11">log p(y) ≥ L(z, σ 2 ; y).<label>(8)</label></formula><p>According to Eqs. (4), (5) and <ref type="formula" target="#formula_9">(7)</ref>, the lower bound can then be rewritten as:</p><formula xml:id="formula_12">L(z, σ 2 ; y) = E q(z,σ 2 |y) log p(y|z, σ 2 ) − D KL (q(z|y)||p(z)) − D KL q(σ 2 |y)||p(σ 2 ) . (9)</formula><p>It's pleased that all the three terms in Eq (9) can be integrated analytically as follows:</p><formula xml:id="formula_13">E q(z,σ 2 |y) log p(y|z, σ 2 ) = d i=1 − 1 2 log 2π − 1 2 (log βi − ψ(αi)) − αi 2βi (yi − µi) 2 + m 2 i ,<label>(10)</label></formula><formula xml:id="formula_14">DKL (q(z|y)||p(z)) = d i=1 (µi − xi) 2 2ε 2 0 + 1 2 m 2 i ε 2 0 − log m 2 i ε 2 0 − 1 ,<label>(11)</label></formula><formula xml:id="formula_15">DKL q(σ 2 |y)||p(σ 2 ) = d i=1 αi − p 2 2 + 1 ψ(αi) + log Γ p 2 2 − 1 − log Γ(αi) + p 2 2 − 1 log βi − log p 2 ξi 2 + αi p 2 ξi 2βi − 1 ,<label>(12)</label></formula><p>where ψ(·) denotes the digamma function. Calculation details are listed in supplementary material.</p><p>We can then easily get the expected objective function (i.e., a negtive lower bound of the marginal likelihood on entire training set) for optimizing the network parameters of D-Net and S-Net as follows:</p><formula xml:id="formula_16">min W D ,W S − n j=1 L(z j , σ 2 j ; y j ).<label>(13)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Network Learning</head><p>As aforementioned, we use D-Net and S-Net together to infer the variational parameters µ, m 2 and α, β from the input noisy image y, respectively, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. It is critical to consider how to calculate derivatives of this objective with respect to W D , W S involved in µ, m 2 , α and β to facilitate an easy use of stochastic gradient varitional inference. Fortunately, different from other related variational inference techniques like VAE <ref type="bibr" target="#b21">[22]</ref>, all three terms of Eqs. At the training stage of our method, the network parameters can be easily updated with backpropagation (BP) algorithm <ref type="bibr" target="#b14">[15]</ref> through Eq. <ref type="bibr" target="#b12">(13)</ref>. The function of each term in this objective can be intuitively explained: the first term represents the likelihood of the observed noisy images in training set, and the last two terms control the discrepancy between the variational posterior and the corresponding prior. During the BP training process, the gradient information from the likelihood term of Eq. <ref type="formula" target="#formula_1">(10)</ref> is used for updating both the parameters of D-Net and S-Net simultaneously, implying that the inference for the latent clean image z and σ 2 is guided to be learned from each other.</p><p>At the test stage, for any test noisy image, through feeding it into D-Net, the final denoising result can be directly obtained by µ. Additionally, through inputting the noisy image to the S-Net, the noise distribution knowledge (i.e., σ 2 ) is easily inferred. Specifically, the noise variance in each pixel can be directly obtained by using the mode of the inferred inverse Gamma distribution: σ 2 i = βi (αi+1) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Network Architecture</head><p>The D-Net in <ref type="figure" target="#fig_0">Fig. 1</ref> takes the noisy image y as input to infer the variational parameters µ and m 2 in q(z|y) of Eq. <ref type="formula">(5)</ref>, and performs the denoising task in the proposed variational inference algorithm. In order to capture multi-scale information of the image, we use a U-Net <ref type="bibr" target="#b33">[34]</ref> with depth 4 as the D-Net, which contains 4 encoder blocks ([Conv+ReLU]×2+Average pooling), 3 decoder blocks (Transpose Conv+[Conv+ReLU]×2) and symmetric skip connection under each scale. For parameter µ, the residual learning strategy is adopted as in <ref type="bibr" target="#b43">[44]</ref>, i.e., µ = y + f (y; W D ), where f (·; W D ) denotes the D-Net with parameters W D . As for the S-Net, which takes the noisy image y as input and outputs the predicted variational parameters α and β in q(σ 2 |y) of Eq (5), we use the DnCNN <ref type="bibr" target="#b43">[44]</ref> architecture with five layers, and the feature channels of each layer is set as 64.</p><p>It should be noted that our proposed method is a general framework, most of the commonly used network architectures <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b45">46]</ref> in image restoration can also be easily substituted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Some Discussions</head><p>It can be seen that the proposed method succeeds advantages of both model-driven MAP and datadriven deep learning methods. On one hand, our method is a generative approach and possesses fine interpretability to the data generation mechanism; and on the other hand it conducts an explicit prediction function, facilitating efficient image denoising as well as noise estimation directly through an input noisy image. Furthermore, beyond current methods, our method can finely evaluate and remove non-i.i.d. noises embedded in images, and has a good generalization capability to images with complicated noises, as evaluated in our experiments. This complies with the main requirement of the blind image denoising task.</p><p>If we set the hyper-parameter ε 2 0 in Eq.(2) as an extremely small value close to 0, it is easy to see that the objective of the proposed method is dominated by the second term of Eq. (10), which makes   the objective degenerate as the MSE loss generally used in traditional deep learning methods (i.e., minimizing n j=1 ||µ(y j ; W D ) − x j || 2 . This provides a new understanding to explain why they incline to overfit noise bias in training data. The posterior inference process puts dominant emphasis on fitting priors imposed on the latent clean image, while almost neglects the effect of noise variations. This naturally leads to its sensitiveness to unseen complicated noises contained in test images.</p><p>Very recently, both CBDNet <ref type="bibr" target="#b16">[17]</ref> and FFDNet <ref type="bibr" target="#b44">[45]</ref> are presented for the denoising task by feeding the noisy image integrated with the pre-estimated noise level into the deep network to make it better generalize to distinct noise types in training stage. Albeit more or less improving the generalization capability of network, such strategy is still too heuristic and is not easy to interpret how the input noise level intrinsically influence the final denoising result. Comparatively, our method is constructed in a sound Bayesian manner to estimate clean image and noise distribution together from the input noisy image, and its generalization can be easily explained from the perspective of generative model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head><p>We evaluate the performance of our method on synthetic and real datasets in this section. All experiments are evaluated in the sRGB space. We briefly denote our method as VDN in the following. The training and testing codes of our VDN is available at https://github.com/zsyOAOA/VDNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setting</head><p>Network training and parameter setting: The weights of D-Net and S-Net in our variational algorithm were initialized according to <ref type="bibr" target="#b17">[18]</ref>. In each epoch, we randomly crop N = 64 × 5000 patches with size 128 × 128 from the images for training. The Adam algorithm <ref type="bibr" target="#b20">[21]</ref> is adopted to optimize the network parameters through minimizing the proposed negative lower bound objective. The initial learning rate is set as 2e-4 and linearly decayed in half every 10 epochs until to 1e-6. The window size p in Eq. (3) is set as 7. The hyper-parameter ε 2 0 is set as 5e-5 and 1e-6 in the following synthetic and real-world image denoising experiments, respectively.</p><p>Comparison methods: Several state-of-the-art denoising methods are adopted for performance comparison, including CBM3D <ref type="bibr" target="#b10">[11]</ref>, WNNM <ref type="bibr" target="#b15">[16]</ref>, NCSR <ref type="bibr" target="#b13">[14]</ref>, MLP <ref type="bibr" target="#b9">[10]</ref>, DnCNN-B <ref type="bibr" target="#b43">[44]</ref>, Mem-Net <ref type="bibr" target="#b37">[38]</ref>, FFDNet <ref type="bibr" target="#b44">[45]</ref>, UDNet <ref type="bibr" target="#b23">[24]</ref> and CBDNet <ref type="bibr" target="#b16">[17]</ref>. Note that CBDNet is mainly designed for blind denoising task, and thus we only compared CBDNet on the real noise removal experiments.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiments on Synthetic Non-I.I.D. Gaussian Noise Cases</head><p>Similar to <ref type="bibr" target="#b44">[45]</ref>, we collected a set of source images to train the network, including 432 images from BSD <ref type="bibr" target="#b4">[5]</ref>, 400 images from the validation set of ImageNet <ref type="bibr" target="#b11">[12]</ref> and 4744 images from the Waterloo Exploration Database <ref type="bibr" target="#b25">[26]</ref>. Three commonly used datasets in image restoration (Set5, LIVE1 and BSD68 in <ref type="bibr" target="#b19">[20]</ref>) were adopted as test datasets to evaluate the performance of different methods. In order to evaluate the effectiveness and robustness of VDN under the non-i.i.d. noise configuration, we simulated the non-i.i.d. Gaussian noise as following,</p><formula xml:id="formula_17">n = n 1 M , n 1 ij ∼ N (0, 1),<label>(14)</label></formula><p>where M is a spatially variant map with the same size as the source image. We totally generated four kinds of M s as shown in <ref type="figure" target="#fig_3">Fig. 2</ref>. The first <ref type="figure" target="#fig_3">(Fig. 2 (a)</ref>) is used for generating noisy images of training data and the others <ref type="figure" target="#fig_3">(Fig. 2 (b)-(d)</ref>) generating three groups of testing data (denotes as Cases 1-3). Under this noise generation manner, the noises in training data and testing data are with evident difference, suitable to verify the robustness and generalization capability of competing methods.</p><p>Comparson with the State-of-the-art: <ref type="table" target="#tab_0">Table 1</ref> lists the average PSNR results of all competing methods on three groups of testing data. From <ref type="table" target="#tab_0">Table 1</ref>, it can be easily observed that: 1) The VDN outperforms other competing methods in all cases, indicating that VDN is able to handle such complicated non-i.i.d. noise; 2) VDN surpasses FFDNet about 0.25dB averagely even though FFDNet depends on the true noise level information instead of automatically inferring noise distribution as our method; 3) the discriminative methods MLP, DnCNN-B and UDNet seem to evidently overfit on training noise bias; 4) the classical model-driven method CBM3D performs more stably than WNNM and NCSR, possibly due to the latter's improper i.i.d. Gaussian noise assumption. <ref type="figure" target="#fig_4">Fig. 3</ref> shows the denoising results of different competing methods on one typical image in testing set of Case 2, and more denoising results can be found in the supplementary material. Note that we only display the top four best results from all due to page limitation. It can be seen that the denoised images by CBM3D and DnCNN-B still contain obvious noise, and FFDNet over-smoothes the image and loses some edge information, while our proposed VDN removes most of the noise and preserves more details.</p><p>Even though our VDN is designed based on the non-i.i.d. noise assumption and trained on the non-i.i.d. noise data, it also performs well on additive white Gaussian noise (AWGN) removal task. <ref type="table" target="#tab_1">Table 2</ref> lists the average PSNR results under three noise levels (σ = 15, 25, 50) of AWGN. It is easy to see that our method obtains the best or at least comparable performance with the state-of-the-art method FFDNet. Combining <ref type="table" target="#tab_0">Table 1</ref> and <ref type="table" target="#tab_1">Table 2</ref>, it should be rational to say that our VDN is robust and able to handle a wide range of noise types, due to its better noise modeling manner.</p><p>Noise Variance Prediction: The S-Net plays the role of noise modeling and is able to infer the noise distribution from the noisy image. To verify the fitting capability of S-Net, we provided the M    <ref type="figure" target="#fig_3">Fig. 2</ref> (b2-d2) for easy observation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experiments on Real-World Noise</head><p>In this part, we evaluate the performance of VDN on real blind denoising task, including two banchmark datasets: DND <ref type="bibr" target="#b30">[31]</ref> and SIDD <ref type="bibr" target="#b0">[1]</ref>. DND consists of 50 high-resolution images with realistic noise from 50 scenes taken by 4 consumer cameras. However, it does not provide any other additional noisy and clean image pairs to train the network. SIDD <ref type="bibr" target="#b0">[1]</ref> is another real-world denoising benchmark, containing 30, 000 real noisy images captured by 5 cameras under 10 scenes. For each noisy image, it estimates one simulated "clean" image through some statistical methods <ref type="bibr" target="#b0">[1]</ref>. About 80% (∼ 24, 000 pairs) of this dataset are provided for training purpose, and the rest as held for benchmark. And 320 image pairs selected from them are packaged together as a medium version of SIDD, called SIDD Medium Dataset 2 , for fast training of a denoiser. We employed this medium vesion dataset to train a real-world image denoiser, and test the performance on the two benchmarks. <ref type="table" target="#tab_2">Table 3</ref> lists PSNR results of different methods on SIDD benchmark 3 . Note that we only list the results of the competing methods that are available on the official benchmark website 2 . It is evident that VDN outperforms other methods. However, note that neither DnCNN-B nor CBDNet performs well, possibly because they were trained on the other datasets, whose noise type is different from SIDD. For fair comparison, we retrained DnCNN-B and CBDNet based on the SIDD dataset. The performance on the SIDD validation set is also listed in For easy visualization, on one typical denoising example, results of the best four competing methods are displayed in <ref type="figure" target="#fig_5">Fig. 4</ref>. Obviously, WNNM is ubable to remove the complex real noise, maybe because the low-rankness prior is insufficient to describe all the image information and the IID Gaussian noise assumption is in conflict with the real noise. With the powerful feature extraction    ability of CNN, DnCNN and CBDNet obtain much better denoising results than WNNM, but still with a little noise. However, the denoising result of our proposed VDN has almost no noise and is very close to the groundtruth.</p><p>In <ref type="figure" target="#fig_7">Fig. 5</ref>, we displayed the noise variance map predicted by S-Net on the two real benchmarks. The variance maps had been enlarged several times for easy visualization. It is easy to see that the predicted noise variance map relates to the image content, which is consistent with the well-known signal-depend property of real noise to some extent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Hyper-parameters Analysis</head><p>The hyper-parameter ε 0 in Eq. (2) determines how much does the desired latent clean image z depend on the simulated groundtruth x. As discussed in Section 3.6, the negative variational lower bound degenerates into MSE loss when ε 0 is setted as an extremely small value close to 0. The performance of VDN under different ε 0 values on the SIDD validation dataset is listed in <ref type="table" target="#tab_5">Table 5</ref>. For explicit comparison, we also directly trained the D-Net under MSE loss as baseline. From <ref type="table" target="#tab_5">Table 5</ref>, we can see that: 1) when ε 0 is too large, the proposed VDN obtains relatively worse results since the prior constraint on z by simulated groundtruth x becomes weak; 2) with ε 0 decreasing, the performance of VDN tends to be similar with MSE loss as analysised in theory; 3) the results of VDN surpasses MSE loss about 0.3 dB PSNR when ε 2 0 = 1e-6, which verifies the importantance of noise modeling in our method. Therefore, we suggest that the ε 2 0 is set as 1e-5 or 1e-6 in the real-world denoising task. In Eq. (3), we introduced a conjugate inverse gamma distribution as prior for σ 2 . The mode of this inverse gamma distribution ξ i provides a rational approximate evaluation for σ 2 i , which is a local estimation in a p × p window centered at the i th pixel. We compared the performance of VDN under different p values on the SIDD validation dataset in <ref type="table" target="#tab_6">Table 6</ref>. Empirically, VDN performs consistently well for the hyper-parameter p.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have proposed a new variational inference algorithm, namely varitional denoising network (VDN), for blind image denoising. The main idea is to learn an approximate posterior to the true posterior with the latent variables (including clean image and noise variances) conditioned on the input noisy image. Using this variational posterior expression, both tasks of blind image denoising and noise estimation can be naturally attained in a unique Bayesian framework. The proposed VDN is a generative method, which can easily estimate the noise distribution from the input data. Comprehensive experiments have demonstrated the superiority of VDN to previous works on blind image denoising. Our method can also facilitate the study of other low-level vision tasks, such as super-resolution and deblurring. Specifically, the fidelity term in these tasks can be more faithfully set under the estimated non-i.i.d. noise distribution by VDN, instead of the traditional i.i.d. Gaussian noise assumption.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The architecture of the proposed deep variational inference network. The red solid lines denote the forward process, and the blue dotted lines mark the gradient flow direction in the BP algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(10)-<ref type="bibr" target="#b11">(12)</ref> in the lower bound Eq. (9) are differentiable and their derivatives can be calculated analytically without the need of any reparameterization trick, largely reducing the difficulty of network training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>(a) The spatially variant map M for noise generation in training data. (b1)-(d1): Three different M s on testing data in Cases 1-3. (b2)-(d2): Correspondingly predicted M s by our method on the testing data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Image denoising results of a typical test image in Case 2. (a) Noisy image, (b) Groundtruth, (c) CBM3D (24.63dB), (d) DnCNN-B (27.83dB), (e) FFDNet (28.06dB), (f) VDN (28.32dB).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Denoising results on one typical image in the validation set of SIDD. (a) Noisy image, (b) Simulated "clean" image, (c) WNNM(21.80dB), (d) DnCNN (34.48dB), (e) CBDNet (34.84dB), (d) VDN (35.50dB).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>The noise variance map predicted by our proposed VDN on SIDD and DND benchmarks. (a1-a3): The noisy image, real noise (|y − x|) and noise variance map of one typical image of SIDD validation dataset. (b1-b2): The noisy image and predicted noise variance map of one typical image of DND dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The PSNR(dB) results of all competing methods on the three groups of test datasets. The best and second best results are highlighted in bold and Italic, respectively.</figDesc><table><row><cell>Cases</cell><cell>Datasets</cell><cell>CBM3D</cell><cell>WNNM</cell><cell>NCSR</cell><cell>MLP</cell><cell>Methods DnCNN-B</cell><cell>MemNet</cell><cell>FFDNet</cell><cell>FFDNetv</cell><cell>UDNet</cell><cell>VDN</cell></row><row><cell></cell><cell>Set5</cell><cell>27.76</cell><cell>26.53</cell><cell>26.62</cell><cell>27.26</cell><cell>29.85</cell><cell>30.10</cell><cell>30.16</cell><cell>30.15</cell><cell>28.13</cell><cell>30.39</cell></row><row><cell>Case 1</cell><cell>LIVE1</cell><cell>26.58</cell><cell>25.27</cell><cell>24.96</cell><cell>25.71</cell><cell>28.81</cell><cell>28.96</cell><cell>28.99</cell><cell>28.96</cell><cell>27.19</cell><cell>29.22</cell></row><row><cell></cell><cell>BSD68</cell><cell>26.51</cell><cell>25.13</cell><cell>24.96</cell><cell>25.58</cell><cell>28.73</cell><cell>28.74</cell><cell>28.78</cell><cell>28.77</cell><cell>27.13</cell><cell>29.02</cell></row><row><cell></cell><cell>Set5</cell><cell>26.34</cell><cell>24.61</cell><cell>25.76</cell><cell>25.73</cell><cell>29.04</cell><cell>29.55</cell><cell>29.60</cell><cell>29.56</cell><cell>26.01</cell><cell>29.80</cell></row><row><cell>Case 2</cell><cell>LIVE1</cell><cell>25.18</cell><cell>23.52</cell><cell>24.08</cell><cell>24.31</cell><cell>28.18</cell><cell>28.56</cell><cell>28.58</cell><cell>28.56</cell><cell>25.25</cell><cell>28.82</cell></row><row><cell></cell><cell>BSD68</cell><cell>25.28</cell><cell>23.52</cell><cell>24.27</cell><cell>24.30</cell><cell>28.15</cell><cell>28.36</cell><cell>28.43</cell><cell>28.42</cell><cell>25.13</cell><cell>28.67</cell></row><row><cell></cell><cell>Set5</cell><cell>27.88</cell><cell>26.07</cell><cell>26.84</cell><cell>26.88</cell><cell>29.13</cell><cell>29.51</cell><cell>29.54</cell><cell>29.49</cell><cell>27.54</cell><cell>29.74</cell></row><row><cell>Case 3</cell><cell>LIVE1</cell><cell>26.50</cell><cell>24.67</cell><cell>24.96</cell><cell>25.26</cell><cell>28.17</cell><cell>28.37</cell><cell>28.39</cell><cell>28.38</cell><cell>26.48</cell><cell>28.65</cell></row><row><cell></cell><cell>BSD68</cell><cell>26.44</cell><cell>24.60</cell><cell>24.95</cell><cell>25.10</cell><cell>28.11</cell><cell>28.20</cell><cell>28.22</cell><cell>28.20</cell><cell>26.44</cell><cell>28.46</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The PSNR(dB) results of all competing methods on AWGN noise cases of three test datasets.</figDesc><table><row><cell>Sigma</cell><cell>Datasets</cell><cell>CBM3D</cell><cell>WNNM</cell><cell>NCSR</cell><cell>MLP</cell><cell>Methods DnCNN-B</cell><cell>MemNet</cell><cell>FFDNet</cell><cell>FFDNete</cell><cell>UDNet</cell><cell>VDN</cell></row><row><cell></cell><cell>Set5</cell><cell>33.42</cell><cell>32.92</cell><cell>32.57</cell><cell>-</cell><cell>34.04</cell><cell>34.18</cell><cell>34.30</cell><cell>34.31</cell><cell>34.19</cell><cell>34.34</cell></row><row><cell>σ = 15</cell><cell>LIVE1</cell><cell>32.85</cell><cell>31.70</cell><cell>31.46</cell><cell>-</cell><cell>33.72</cell><cell>33.84</cell><cell>33.96</cell><cell>33.96</cell><cell>33.74</cell><cell>33.94</cell></row><row><cell></cell><cell>BSD68</cell><cell>32.67</cell><cell>31.27</cell><cell>30.84</cell><cell>-</cell><cell>33.87</cell><cell>33.76</cell><cell>33.85</cell><cell>33.68</cell><cell>33.76</cell><cell>33.90</cell></row><row><cell></cell><cell>Set5</cell><cell>30.92</cell><cell>30.61</cell><cell>30.33</cell><cell>30.55</cell><cell>31.88</cell><cell>31.98</cell><cell>32.10</cell><cell>32.09</cell><cell>31.82</cell><cell>32.24</cell></row><row><cell>σ = 25</cell><cell>LIVE1</cell><cell>30.05</cell><cell>29.15</cell><cell>29.05</cell><cell>29.16</cell><cell>31.23</cell><cell>31.26</cell><cell>31.37</cell><cell>31.37</cell><cell>31.09</cell><cell>31.50</cell></row><row><cell></cell><cell>BSD68</cell><cell>29.83</cell><cell>28.62</cell><cell>28.35</cell><cell>28.93</cell><cell>31.22</cell><cell>31.17</cell><cell>31.21</cell><cell>31.20</cell><cell>31.02</cell><cell>31.35</cell></row><row><cell></cell><cell>Set5</cell><cell>28.16</cell><cell>27.58</cell><cell>27.20</cell><cell>27.59</cell><cell>28.95</cell><cell>29.10</cell><cell>29.25</cell><cell>29.25</cell><cell>28.87</cell><cell>29.47</cell></row><row><cell>σ = 50</cell><cell>LIVE1</cell><cell>26.98</cell><cell>26.07</cell><cell>26.06</cell><cell>26.12</cell><cell>27.95</cell><cell>27.99</cell><cell>28.10</cell><cell>28.10</cell><cell>27.82</cell><cell>28.36</cell></row><row><cell></cell><cell>BSD68</cell><cell>26.81</cell><cell>25.86</cell><cell>25.75</cell><cell>26.01</cell><cell>27.91</cell><cell>27.91</cell><cell>27.95</cell><cell>27.95</cell><cell>27.76</cell><cell>28.19</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>The comparison results of different methods on SIDD benchmark and validation dataset.</figDesc><table><row><cell>Datasets</cell><cell></cell><cell></cell><cell cols="2">SIDD Benchmark</cell><cell></cell><cell></cell><cell cols="2">SIDD Validation</cell><cell></cell></row><row><cell>Methods</cell><cell>CBM3D</cell><cell>WNNM</cell><cell>MLP</cell><cell>DnCNN-B</cell><cell>CBDNet</cell><cell>VDN</cell><cell>DnCNN-B</cell><cell>CBDNet</cell><cell>VDN</cell></row><row><cell>PSNR</cell><cell>25.65</cell><cell>25.78</cell><cell>24.71</cell><cell>23.66</cell><cell>33.28</cell><cell>39.23</cell><cell>38.41</cell><cell>38.68</cell><cell>39.28</cell></row><row><cell>SSIM</cell><cell>0.685</cell><cell>0.809</cell><cell>0.641</cell><cell>0.583</cell><cell>0.868</cell><cell>0.971</cell><cell>0.909</cell><cell>0.901</cell><cell>0.909</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>The comparison results of all competing methods on DND benchmark dataset. Net as the input of FFDNet, and the denoising results are listed inTable 1(denoted as FFDNet v ). It is obvious that FFDNet under the real noise level and FFDNet v almost have the same performance, indicating that the S-Net effectively captures proper noise information. The predicted noise variance Maps on three groups of testing data are shown in</figDesc><table><row><cell>Methods</cell><cell>CBM3D</cell><cell>WNNM</cell><cell>NCSR</cell><cell>MLP</cell><cell>DnCNN-B</cell><cell>FFDNet</cell><cell>CBDNet</cell><cell>VDN</cell></row><row><cell>PSNR</cell><cell>34.51</cell><cell>34.67</cell><cell>34.05</cell><cell>34.23</cell><cell>37.90</cell><cell>37.61</cell><cell>38.06</cell><cell>39.38</cell></row><row><cell>SSIM</cell><cell>0.8507</cell><cell>0.8646</cell><cell>0.8351</cell><cell>0.8331</cell><cell>0.9430</cell><cell>0.9415</cell><cell>0.9421</cell><cell>0.9518</cell></row><row><cell cols="2">predicted by S-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Under same training conditions, VDN still outperforms DnCNN-B 0.87 PSNR and CBDNet 0.60dB PSNR, indicating the effectiveness and significance of our non-i.i.d. noise modeling manner. For easy visualization, on one typical denoising example, results of the best four competing methods are displayed in Fig. 4Table 4lists the performance of all competing methods on the DND benchmark<ref type="bibr" target="#b3">4</ref> . From the table, it is easy to be seen that our proposed VDN surpasses all the competing methods. It is worth noting that CBDNet has the same optimized network with us, containing a S-Net designed for estimating the noise distribution and a D-Net for denoising. The superiority of VDN compared with CBDNet mainly benefits from the deep variational inference optimization.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Performance of VDN under different ε 2</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">0 values on SIDD valida-</cell></row><row><cell cols="2">tion dataset (p = 7).</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ε 2 0</cell><cell>1e-4</cell><cell>1e-5</cell><cell>1e-6</cell><cell>1e-7</cell><cell>1e-8</cell><cell>MSE</cell></row><row><cell>PSNR</cell><cell>38.89</cell><cell>39.20</cell><cell>39.28</cell><cell>39.05</cell><cell>39.03</cell><cell>39.01</cell></row><row><cell>SSIM</cell><cell>0.9046</cell><cell>0.9079</cell><cell>0.9086</cell><cell>0.9064</cell><cell>0.9063</cell><cell>0.9061</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Performance of VDN under different p values on</figDesc><table><row><cell cols="4">SIDD validation dataset (ε 2 0 = 1e-6).</cell><cell></cell><cell></cell></row><row><cell>p</cell><cell>5</cell><cell>7</cell><cell>11</cell><cell>15</cell><cell>19</cell></row><row><cell>PSNR</cell><cell>39.26</cell><cell>39.28</cell><cell>39.26</cell><cell>39.24</cell><cell>39.24</cell></row><row><cell>SSIM</cell><cell cols="5">0.9089 0.9086 0.9086 0.9079 0.9079</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We use j (= 1, · · · , n) and i (= 1, · · · , d) to express the indexes of training data and data dimension, respectively, throughout the entire paper.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://www.eecs.yorku.ca/ kamel/sidd/index.php<ref type="bibr" target="#b2">3</ref> We employed the function 'compare_ssim' in scikit-image library to calculate the SSIM value, which is a little difference with the SIDD official results 4 https://noise.visinf.tu-darmstadt.de/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A high-quality denoising dataset for smartphone cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Abdelhamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Adaptive multi-column deep neural networks with application to robust image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Forest</forename><surname>Agostinelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Michael R Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1493" to="1501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">K-svd: An algorithm for designing overcomplete dictionaries for sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Aharon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfred</forename><surname>Bruckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on signal processing</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">4311</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Renoir -a dataset for real low-light noise image reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josue</forename><surname>Anaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Barbu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.8230</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Contour detection and hierarchical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="898" to="916" />
			<date type="published" when="2011-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Pattern recognition and machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Variational inference for dirichlet process mixtures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bayesian analysis</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="121" to="143" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dillon</forename><surname>Sharlet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.11127</idno>
		<title level="m">Unprocessing images for learned raw denoising</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A non-local algorithm for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bartomeu</forename><surname>Coll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J-M</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;05)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="60" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Image denoising: Can plain neural networks compete with bm3d</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Harold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Burger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Christian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Schuler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2392" to="2399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Image denoising by sparse 3-d transformdomain collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dabov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2080" to="2095" />
			<date type="published" when="2007-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Scalable multi-label annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conference on Human Factors in Computing Systems (CHI)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Nonlocal image restoration with bilateral variance estimation: a low-rank approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weisheng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangming</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="700" to="711" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Nonlocally centralized sparse representation for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weisheng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangming</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1620" to="1630" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Weighted nuclear norm minimization with application to image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangchu</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2862" to="2869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zifei</forename><surname>Shi Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.04686</idno>
		<title level="m">Toward convolutional blind denoising of real photographs</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Natural image denoising with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viren</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="769" to="776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1646" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization. international conference on learning representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multiscale image blind denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Lebrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Colom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Michel</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3149" to="3161" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Universal denoising networks: a novel cnn architecture for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stamatios</forename><surname>Lefkimmiatis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3204" to="3213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Non-local recurrent network for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bihan</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1673" to="1682" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Waterloo Exploration Database: New challenges for image quality assessment models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kede</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengfang</forename><surname>Duanmu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingbo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1004" to="1016" />
			<date type="published" when="2017-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Nonlocal transform-domain filter for volumetric data denoising and reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Maggioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Egiazarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Foi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="119" to="133" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sparse representation for color image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="69" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Image restoration using very deep convolutional encoder-decoder networks with symmetric skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiao</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Bin</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2802" to="2810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Scale-space and edge detection using anisotropic diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="629" to="639" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Benchmarking denoising algorithms with real photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Plotz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1586" to="1595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Neural nearest neighbors networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Plötz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1087" to="1098" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Neural nearest neighbors networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Plötz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31</title>
		<editor>S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1087" to="1098" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fields of experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">205</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Nonlinear total variation based noise removal algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanley</forename><surname>Leonid I Rudin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emad</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fatemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica D: nonlinear phenomena</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">1-4</biblScope>
			<biblScope unit="page" from="259" to="268" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Noise removal via bayesian wavelet coring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Eero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Edward H Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 3rd IEEE International Conference on Image Processing</title>
		<meeting>3rd IEEE International Conference on Image Processing</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1996" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="379" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Memnet: A persistent memory network for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4539" to="4547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Statistical calibration of ccd imaging process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghai</forename><surname>Tsin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Visvanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Eighth IEEE International Conference on Computer Vision. ICCV</title>
		<meeting>Eighth IEEE International Conference on Computer Vision. ICCV</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="480" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Image denoising and inpainting with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linli</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="341" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A trilateral weighted sparse coding scheme for real-world image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Robust online matrix factorization for dynamic background subtraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1726" to="1740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Hyperspectral image restoration under complex multi-band noises</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongsheng</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">1631</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3142" to="3155" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Ffdnet: Toward a fast and flexible solution for cnn-based image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4608" to="4622" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Residual dense network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yapeng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2472" to="2481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Blind image denoising via dependent dirichlet process tree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengyuan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianye</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pheng-Ann</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1518" to="1531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">From noise modeling to blind image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengyuan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pheng-Ann</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="420" to="429" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
