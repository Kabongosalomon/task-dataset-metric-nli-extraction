<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">NAOMI: Non-Autoregressive Multiresolution Sequence Imputation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukai</forename><forename type="middle">Liu</forename><surname>Caltech</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rose</forename><surname>Yu</surname></persName>
							<email>roseyu@northeastern.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Zheng</surname></persName>
							<email>stephan.zheng@salesforce.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">Zhan</forename><surname>Caltech</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisong</forename><forename type="middle">Yue</forename><surname>Caltech</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Northeastern University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<address>
									<settlement>Caltech</settlement>
									<country>Salesforce</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">NAOMI: Non-Autoregressive Multiresolution Sequence Imputation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Missing value imputation is a fundamental problem in spatiotemporal modeling, from motion tracking to the dynamics of physical systems. Deep autoregressive models suffer from error propagation which becomes catastrophic for imputing long-range sequences. In this paper, we take a non-autoregressive approach and propose a novel deep generative model: Non-AutOregressive Multiresolution Imputation (NAOMI) to impute long-range sequences given arbitrary missing patterns. NAOMI exploits the multiresolution structure of spatiotemporal data and decodes recursively from coarse to fine-grained resolutions using a divide-andconquer strategy. We further enhance our model with adversarial training. When evaluated extensively on benchmark datasets from systems of both deterministic and stochastic dynamics. In our experiments, NAOMI demonstrates significant improvement in imputation accuracy (reducing average error by 60% compared to autoregressive counterparts) and generalization for long-range sequences.</p><p>Recent studies <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref> have proposed to use deep generative models for learning flexible missing patterns from sequence data. However, all existing deep generative imputation methods are autoregressive: they model the value at cur- *</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The problem of missing values often arises in real-life sequential data. For example, in motion tracking, trajectories often contain missing data due to object occlusion, trajectories crossing, and the instability of camera motion <ref type="bibr" target="#b0">[1]</ref>. Missing values can introduce observational bias into training data, making the learning unstable. Hence, imputing missing values is of critical importance to the downstream sequence learning tasks. Sequence imputation has been studied for <ref type="figure">Figure 1</ref>: Imputation process of NAOMI in a basketball play given two players (purple and blue) and 5 known observations (black dots). Missing values are imputed recursively from coarse resolution to fine-grained resolution (left to right). decades in statistics literature <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>. Most statistical techniques are reliant on strong assumptions on missing patterns such as missing at random, and do not generalize well to unseen data. Moreover, existing methods do not work well when the proportion of missing data is high and the sequence is long. rent timestamp using the values from previous time-steps and impute missing data in a sequential manner. Hence, autoregressive models are highly susceptible to compounding error, which can become catastrophic for long-range sequence modeling. We observe in our experiments that existing autoregressive approaches struggle on sequence imputation tasks with long-range dynamics.</p><p>In this paper, we introduce a novel non-autoregressive approach for long-range sequence imputation. Instead of conditioning only on the previous values, we model the conditional distribution on both the history and the (predicted) future. We exploit the multiresolution nature of spatiotemporal sequence, and decompose the complex dependency into simpler ones at multiple resolutions. Our model, Non-autoregressive Multiresolution Imputation (NAOMI), employs a divide and conquer strategy to fill in the missing values recursively. Our method is general and can work with various learning objectives. We release an implementation of our model as an open source project. <ref type="bibr" target="#b1">2</ref> In summary, our contributions are as follows:</p><p>• We propose a novel non-autoregressive decoding procedure for deep generative models that can impute missing values for spatiotemporal sequences with long-range dependencies. • We introduce adversarial training using the generative adversarial imitation learning objective with a fully differentiable generator to reduce variance. • We conduct exhaustive experiments on benchmark sequence datasets including traffic time series, billiards and basketball trajectories. Our method demonstrates 60% improvement in accuracy and generates realistic sequences given arbitrary missing patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Missing Value Imputation Existing missing value imputation approaches roughly fall into two categories: statistical methods and deep generative models. Statistical methods often impose strong assumptions on the missing patterns. For example, mean/median averaging <ref type="bibr" target="#b3">[4]</ref>, linear regression <ref type="bibr" target="#b1">[2]</ref>, MICE <ref type="bibr" target="#b9">[10]</ref>, and k-nearest neighbours <ref type="bibr" target="#b10">[11]</ref> can only handle data missing at random. Latent variables models with EM algorithm <ref type="bibr" target="#b11">[12]</ref> can impute data missing not at random but are restricted to certain parametric models. Deep generative model offers a flexible framework of missing data imputation. For instance, <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b13">14]</ref> develop variants of recurrent neural networks to impute time series. <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b6">7]</ref> leverage generative adversarial training (GAN) <ref type="bibr" target="#b14">[15]</ref> to learn complex missing patterns. However, all the existing imputation models are autoregressive.</p><p>Non-Autoregressive Modeling Non-autoregressive models have gained competitive advantages over autoregressive models in natural language processing <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref> and speech <ref type="bibr" target="#b18">[19]</ref>. For instance, <ref type="bibr" target="#b18">[19]</ref> uses a normalizing flow model <ref type="bibr" target="#b19">[20]</ref> to train a parallel feed-forward network for speech synthesis. For neural machine translation, <ref type="bibr" target="#b15">[16]</ref> introduce a latent fertility model with a sequence of discrete latent variables. Similarly, <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref> propose a fully deterministic model to reduce the amount of supervision. All these works highlight the strength of non-autoregressive models in decoding sequence data in a scalable fashion. Our work is the first non-autoregressive model for sequence imputation tasks with a novel recursive decoding algorithm.</p><p>Generative Adversarial Training Generative adversarial networks (GAN) <ref type="bibr" target="#b14">[15]</ref> introduce a discriminator to replace maximum likelihood objective, which has sparked a new paradigm of generative modeling. For sequence data, using a discriminator for the entire sequence ignores the sequential dependency and can suffer from mode collapse. <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref> develop imitation and reinforcement learning to train GAN in the sequential setting. <ref type="bibr" target="#b20">[21]</ref> propose generative adversarial imitation learning to combine GAN and inverse reinforcement learning. <ref type="bibr" target="#b21">[22]</ref> develop GAN for discrete sequences using reinforcement learning. We use an imitation learning formula with a differentiable policy.</p><p>Multiresolution Generation Our method bears affinity with multiresolution generative models for images such as Progressive GAN <ref type="bibr" target="#b22">[23]</ref> and multiscale autoregressive density estimation <ref type="bibr" target="#b23">[24]</ref>. The key difference is that <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref> only capture spatial multiresolution structures and assume additive models for different resolutions. We deal with multiresolution spatiotemporal structures and generate predictions recursively. Our method is fundamentally different from hierarchical sequence models <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref>, as it only keeps track of the most relevant hidden states and update them on-the-fly, which is memory efficient and much faster to train.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Non-Autoregressive Multiresolution Sequence Imputation</head><p>Let X = (x 1 , x 2 , ..., x T ) be a sequence of T observations, where each time step x t ∈ R D . X have missing data, indicated by a masking sequence M = (m 1 , m 2 , ..., m T ). The masking m t is zero whenever x t is missing. Our goal is to replace the missing data with reasonable values for a collection of sequences. A common practice for missing value imputation is to directly model the distribution of the incomplete sequences. One can factorize the probability p(x 1 , · · · , x T ) = t p(x t |x &lt;t ) using chain rule and train a (deep) autoregressive model for imputation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>.</p><p>However, a key weakness of autoregressive models is their sequential decoding process. Since the current value is dependent on the previous time steps, autoregressive models often have to resort to sub-optimal beam search and are susceptible to error compounding for long-range sequences <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref>. This weakness is worsened in sequence imputation as the model cannot ground the known future, which leads to mismatch between the imputed values and ground truth at the observed points. To alleviate these issues, we instead take a non-autoregressive approach and propose a deep, non-autoregressive, multiresolution generative model NAOMI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">NAOMI Architecture and Imputation Strategy</head><p>As shown in <ref type="figure">Figure 2</ref>, NAOMI has two components: 1) a forward-backward encoder that maps the incomplete sequences to hidden representations, and 2) a multiresolution decoder that imputes missing values given the hidden representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2:</head><p>NAOMI architecture for imputing a sequence of length five. A forward-backward encoder encodes the incomplete sequence (x 1 , · · · , x 5 ) into hidden states. The decoder decodes recursively in a non-autoregressive manner: predict x 3 using hidden states h 1 , h 5 . After the prediction, the hidden states are updated. Then x 2 is imputed based on x 1 and the predicted x 3 , and similarly for x 4 . This process repeats until all missing values are filled.</p><p>Forward-backward encoder. We concatenate the observation and masking sequence as input I = [X, M ]. Our encoder models the conditional distribution of two sets of hidden states given the input: forward hidden states</p><formula xml:id="formula_0">H f = (h f 1 , . . . , h f T ) and backward hidden states H b = (h b 1 , . . . , h b T ): q(H f |I) = T t=1 q(h f t |h f &lt;t , I ≤t ) q(H b |I) = T t=1 q(h b t |h b &gt;t , I ≥t ),<label>(1)</label></formula><p>where h f t and h b t are the hidden states of the history and the future respectively. We parameterize the above distributions with a forward RNN f f and a backward RNN f b :</p><formula xml:id="formula_1">q(h f t |h f &lt;t , I ≤t ) = f f (h f t−1 , I t ) q(h b t |h b &gt;t , I ≥t ) = f b (h b t+1 , I t ).<label>(2)</label></formula><p>Multiresolution decoder. Given the joint hidden states H := [H f , H b ], the decoder learns the distribution of complete sequences p(X|H). We adopt a divide and conquer strategy and decode recursively from coarse to fine-grained resolutions. As shown in <ref type="figure">Figure 2</ref>, at each iteration, the decoder first identifies two known time steps as pivots (x 1 and x 5 in this example), and imputes close to their midpoint (x 3 ). One pivot is then replaced by the newly imputed step and the process repeats at a finer resolution for x 2 and x 4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Non-AutOregressive Multiresolution Imputation</head><p>1: Initialize generator G θ and discriminator D ω 2: repeat 3:</p><p>Sample complete sequences from training data X * ∼ C and mask M</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>Compute incomplete sequences X = X * M 5:</p><formula xml:id="formula_2">Initialize h f t , h b t using Eqn 2 for 0 ≤ t ≤ T 6:</formula><p>while X contains missing values do 7:</p><p>Find the smallest i and the smallest j &gt; i s.t. m i = m j = 1 and ∃t, i &lt; t &lt; j s.t. m t = 0 8:</p><p>Find the smallest r s.t. n r = 2 R−r ≤ (j − i)/2, thus the imputation point t = i + n r 9:</p><formula xml:id="formula_3">Decode x t using p(x t |H) = g (r) (h f i , h b j ), update X, M 10:</formula><p>Update the hidden states using</p><formula xml:id="formula_4">h f t = f f (h f t−1 , I t ), h b t = f b (h b t+1 , I t ) 11:</formula><p>end while <ref type="bibr">12:</ref> Update generator G θ by backpropagation <ref type="bibr">13:</ref> Train discriminator D ω with complete sequences X * and imputed sequencesX <ref type="bibr">14:</ref> until Converge Formally speaking, a decoder with R resolutions consists of a series of decoding functions g <ref type="bibr" target="#b0">(1)</ref> , . . . , g (R) , each of which predicts every n r = 2 R−r steps. The decoder first finds two known steps i and j as pivots, and then selects the missing step t that is close to the midpoint:</p><formula xml:id="formula_5">[(i + j)/2].</formula><p>Let r be the smallest resolution that satisfies n r ≤ (j − i)/2. The decoder updates the hidden states at time t using the forward states h f i and the backward states h b j . A decoding function g (r) then maps the hidden states to the distribution over the outputs:</p><formula xml:id="formula_6">p(x t |H) = g (r) (h f i , h b j ).</formula><p>If the dynamics are deterministic, g (r) directly outputs the imputed value. For stochastic dynamics, g (r) outputs the mean and the standard deviation of an isotropic Gaussian distribution, and the predictions are sampled from the Gaussian distribution using the reparameterize trick <ref type="bibr" target="#b27">[28]</ref>. The mask m t is updated to 1 after imputation and the process proceeds to the next resolution. The details of this decoding process are described in Algorithm 1. We encourage the reader to watch our demo video for a detailed visualization and imputed examples. <ref type="bibr" target="#b2">3</ref> Efficient hidden states update. NAOMI efficiently updates the hidden states by reusing the previous computation, which has the same time complexity as autoregressive models. <ref type="figure">Figure 3</ref> shows an example for a sequence of length nine. Grey blocks are the known time steps. Orange blocks are the target time step to be imputed. Hollow arrows denote forward hidden states updates, and black arrows represent backward hidden states updates. Grey arrows are the outdated hidden states updates. The dashed arrows represent the decoding steps. Earlier hidden states are stored in the imputed time steps and are reused. Therefore, forward hidden states h f only need to be updated once and backward hidden states h b are updated at most twice. <ref type="figure">Figure 3</ref>: NAOMI hidden states updating rule for a sequence of length nine. Note that backward hidden states h b 9→7 are updated twice when predictingx 6 . Complexity. The total run-time of NAOMI is O(T ). The memory usage is similar to that of bidirectional RNN (O(T )), except that we only need to save the latest hidden states for the forward encoder. The decoder hyperparameter R is picked such that 2 R is close to the most common missing interval size, and the run time scales logarithmically with the length of the sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Learning Objective</head><p>Let C = {X * } be the collection of complete sequences, G θ (X, M ) denote our generative model NAOMI parametrized by θ, and p(M ) denote the prior over the masking. The imputation model can be trained by optimizing the following objective:</p><formula xml:id="formula_7">min θ E X * ∼C,M ∼p(M ),X∼G θ (X,M ) T t=1 L(x t , x t ) .<label>(3)</label></formula><p>where L is some loss function. For deterministic dynamics, we use the mean squared error as our loss L(x t , x t ) = x t − x t 2 . For stochastic dynamics, we can replace L with a discriminator, which leads to the adversarial training objective. We use a similar formulation as generative adversarial imitation learning (GAIL) <ref type="bibr" target="#b20">[21]</ref>, which quantifies the distributional difference between generated and training data at the sequence level.</p><p>Adversarial training. Given the generator G θ in NAOMI and a discriminator D ω parameterized by ω, the adversarial training objective function is:</p><formula xml:id="formula_8">min θ max ω E X * ∼C T t=1 log D ω (x t , x t ) +E X * ∼C,M ∼p(M ),X∼G θ T t=1 log(1 − D ω (x t , x t )) ,<label>(4)</label></formula><p>GAIL samples the sequences directly from the generator and optimizes the parameters using policy gradient. This approach can suffer from high variance and require a large number of samples <ref type="bibr" target="#b28">[29]</ref>. Instead of sampling, we take a model-based approach and make our generator fully differentiable.</p><p>We apply the reparameterization trick <ref type="bibr" target="#b27">[28]</ref> at every time step by mapping the hidden states to mean and variance of a Gaussian distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate NAOMI in environments with diverse dynamics: real-world traffic time series, billiard ball trajectories from a physics engine, and team movements from professional basketball gameplay. We compare with the following baselines:</p><p>• Linear: linear interpolation, missing values are imputed using interpolated predictions from two closest known observations. • KNN <ref type="bibr" target="#b10">[11]</ref>: k nearest neighbours, missing values are imputed as the average of the k nearest neighboring sequences. • GRUI [9]: autoregressive model with GAN for time series imputation, modified to handle complete training sequence. The discriminator is applied once to the entire time series. • MaskGAN <ref type="bibr" target="#b6">[7]</ref>: autoregressive model with actor-critic GAN, trained using adversarial imitation learning with discriminator applied to every time step, uses a forward encoder only, and decodes at a single resolution. • SingleRes: autoregressive counterpart of our model, trained using adversarial imitation learning, uses a forward-backward encoder, but decodes at a single resolution. Without adversarial training, it reduces to BRITS <ref type="bibr" target="#b13">[14]</ref>. We randomly choose the number of steps to be masked, and then randomly sample the specific steps to mask in the sequence. Hence the model learns various missing patterns during training. We used the same masking scheme for all methods, including MaskGAN and GRUI. See Appendix for implementation and training details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Imputing Traffic Time Series</head><p>The PEMS-SF traffic time series <ref type="bibr" target="#b29">[30]</ref> data contains 267 training and 173 testing sequences of length 144 (sampled every 10 mins throughout the day). It is multivariate with 963 dimensions, representing the freeway occupancy rate collected from 963 different sensors. We generate a masking sequence for each data with 122 to 140 missing values.</p><p>Imputation accuracy L2 loss between imputed missing values and their ground-truth most accurately measures the quality of the generated sequence. As clearly shown in table 1, NAOMI outperforms others by a large margin, reducing L2 loss by 23% compared to the autoregressive baselines. KNN performs reasonably well, mostly because of the repeated daily traffic patterns in the training data. Simply finding a similar sequence in the training data is sufficient for imputation.  Generated Sequences. <ref type="figure" target="#fig_0">Figure 4</ref> visualizes the predictions from two best performing models: NAOMI (blue) and SingleRes (red). Black dots are observed time steps and black curves are the ground truth. NAOMI successfully captures the pattern of the ground truth time series, while SingleRes fails. NAOMI learns the multiscale fluctuation rooted in the ground truth, whereas SingleRes only learns some averaged behavior. This demonstrates the clear advantage of using multiresolution modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Imputing Billiards Trajectories</head><p>We generate 4000 training and 1000 test sequences of Billiards ball trajectories in a rectangular world using the simulator from <ref type="bibr" target="#b30">[31]</ref>. Each ball is initialized with a random position and random velocity and rolled-out for 200 timesteps. All balls have a fixed size and uniform density, and there is no friction. We generate a masking sequence for each trajectory with 180 to 195 missing values.</p><p>Imputation accuracy. Three defining characteristics of the physics in this setting are: (1) moving in straight lines; (2) maintaining unchanging speed; and (3) reflecting upon hitting a wall. Hence, we adopt four metrics to quantify the learned physics: (1) L 2 loss between imputed values and ground-truth; (2) Sinuosity to measure the straightness of the generated trajectories; (3) Average step size change to measure the speed change of the ball; and (4) Distance between reflection point and the wall to check whether the model has learned the physics underlying collision and reflection.</p><p>Comparison of all models w.r.t. these metrics are shown in <ref type="table" target="#tab_1">Table 2</ref>. Expert represents the ground truth trajectories from the simulator. Statistics closer to the expert are better. We observe that NAOMI has the best overall performance across almost all the metrics, followed by SingleRes baseline. It is expected that linear to perform the best w.r.t step change. By design, linear interpolation maintains a constant step size change that is the closest to the ground-truth.</p><p>Generated trajectories. We visualize the imputed trajectories in <ref type="figure" target="#fig_1">Figure 5</ref>. There are 8 known timesteps (black dots), including the starting position. NAOMI can successfully recover the original trajectory whereas SingleRes deviates significantly. Notably, SingleRes mistakenly predicts the   ball to bounce off the upper wall instead of the left wall. As such, SingleRes has to correct its behavior to match future observations, leading to curved and unrealistic trajectories. Another deviation can be seen near the bottom-left corner, where NAOMI produces trajectory paths that are truly parallel after bouncing off the wall twice, but SingleRes does not.</p><p>Robustness to missing proportion. <ref type="figure" target="#fig_2">Figure 6</ref> compares the performance of NAOMI and SingleRes as we increase the proportion of missing values. The median value and 25, 75 percentile values are displayed for each metric. As the dynamics are deterministic, higher missing portion usually means bigger gaps, making it harder to find the correct solutions. We can see both models' performance degrade drastically as we increase the percentage of missing values, but NAOMI still outperforms SingleRes in all metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Imputing Basketball Players Movement</head><p>The basketball tracking dataset contains the trajectories of professional basketball players on offense with 107,146 training and 13,845 test sequences. Each sequence contains the (x, y)-coordinates of 5 players for 50 timesteps at 6.25Hz and takes place in the left half-court. We generate a masking sequence for each trajectory with 40 to 49 missing values.  Imputation accuracy. Since the environment is stochastic (basketball players on offense aim to be unpredictable), measuring L2 loss between our model output and the ground-truth is not necessarily a good indicator of realistic trajectories <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref>. Hence, we follow previous work and compute domainspecific metrics to compare trajectory quality: (1) Average trajectory length to measure the typical player movement in 8 seconds;</p><p>(2) Average out-of-bound rate to measure the odds of trajectories going out of court boundaries; (3) Average step size change to quantify the player movement variance; (4) Max-Min path diff ; and (5) Average player distance to characterize the team coordination. <ref type="table" target="#tab_2">Table  3</ref> compares model performances using these metrics. Expert represents real human play, and the closer to the expert data, the better. NAOMI outperforms baselines in almost all the metrics.</p><p>Generated trajectories. We visualize imputed trajectories from all models in <ref type="figure" target="#fig_3">Figure 7</ref>. NAOMI produces trajectories that are the most consistent with known observations and have the most realistic player velocities and speeds. In contrast, other baseline models often fail in these regards. KNN generates trajectories with unnatural jumps as finding nearest neighbors becomes difficult with dense known observations. Linear fails to generate curvy trajectories when few observations are known. GRUI generates trajectories that are inconsistent with known observations. This is largely due to mode collapse caused by applying a discriminator to the entire sequence. MaskGAN, which relies on seq2seq and a single encoder, fails to condition on the future observations and predicts straight lines.</p><p>Robustness to missing proportion. <ref type="figure" target="#fig_4">Figure 8</ref> compares the performance of NAOMI and SingleRes as we increase the proportion of missing values. The median value and 25, 75 percentile values are displayed for each metric. Note that we always observe the first step. Generally speaking, more missing values make the imputation harder, and also brings more uncertainty to model predictions.</p><p>We can see that performance (average performance and imputation variance) of both models degrade with more missing values. However, at a certain percentage of missing values, the performance of imputation starts to improve for both models.</p><p>This shows an interesting trade-off between available information and number of constraints for generative models in imputation. More observations provide more information regarding the data distribution, but can also constrain the learned model output. As we reduce the number of observations, the model can learn more flexible generative distributions, without conforming to the constraints imposed by the observed time steps.</p><p>Learned conditional distribution. Our model is fully generative and learns the conditional distribution of the complete sequences given observations. As shown in <ref type="figure" target="#fig_5">Figure 9</ref>. For a given set of known observations, we use NAOMI to impute missing values with 50 different random seeds and overlay the generated trajectories. We can see that as the number of known observations increases, the variance of the learned conditional distribution decreases. However, we also observe some mode collapse in our model: the trajectory of the purple player in the ground truth is not captured in the conditional distribution in the first image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Forward Prediction</head><p>Forward prediction is a special case of imputation when all observations, except for a leading sequence, are missing. We show that NAOMI can also be trained to perform forward prediction without  modifying the model structure. We take a trained imputation model as initialization, and continue training for forward prediction by using the masking sequence m i = 0, ∀i ≥ 5 (first 5 steps are known). We evaluate forward prediction performance using the same metrics. <ref type="figure">Figure 10</ref> compares forward prediction performance in Billiards. Without any known observations in the future, autoregressive models like SingleRes are effective in learning consistent step changes, but NAOMI generates straighter lines and learns the reflection dynamics better than other baselines. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RNN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We propose a deep generative model NAOMI for imputing missing data in long-range spatiotemporal sequences. NAOMI recursively finds and predicts missing values from coarse to fine-grained resolutions using a non-autoregressive approach. Leveraging multiresolution modeling and adversarial training, NAOMI is able to learn the conditional distribution given very few known observations. Future work will investigate how to infer the underlying distribution when complete training sequences are not available. The trade-off between partial observations and external constraints is another direction for deep generative imputation models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 :</head><label>4</label><figDesc>Traffic time series imputation visualization. NAOMI successfully captures the multiresolution patterns of the data from observed steps, while SingleRes only learns a smoothed version of the original sequence and frequently deviates from ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Comparison of imputed billiards trajectories. Blue and red trajectories/curves represent NAOMI and the single-resolution baseline model respectively. White trajectories represent the groundtruth. There are 8 known observations (black dots). NAOMI almost perfectly recovers the ground-truth and achieves lower stepwise L2 loss of missing values than the baseline model (third row). The trajectory from the baseline first incorrectly bounces off the upper wall, which results in curved paths that deviate from the ground-truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 :</head><label>6</label><figDesc>Billiards model performance with increasing percentage of missing values. The median and 25, 75 percentile values are displayed at each number of missing steps. Statistics closer to the expert indicate better performance. NAOMI performs better than SingleRes for all metrics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>Comparison of imputed basketball trajectories. Black dots represent known observations (10 in first row, 5 in second). Overall, NAOMI produces trajectories that are the most consistent and have the most realistic player velocities and speeds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 :</head><label>8</label><figDesc>Basketball model performance with increasing percentage of missing values. The median and 25, 75 percentile values are displayed. Statistics closer to the expert indicate better performance. NAOMI performs better than SingleRes for all metrics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 :</head><label>9</label><figDesc>The generated conditional distribution of basketball trajectories given known observations (black dots) with sampled trajectories. As the number of known observations increases, the variance of the predictions, hence the model uncertainty decreases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Traffic data L2 loss comparison. NAOMI outperforms others, reducing L2 loss by 23% from the autoregressive counterpart.</figDesc><table><row><cell>Models</cell><cell cols="5">NAOMI SingleRes MaskGAN KNN GRUI Linear</cell></row><row><cell>L2 Loss (10 −4 )</cell><cell>3.54</cell><cell>4.51</cell><cell>6.02</cell><cell>4.58 15.24</cell><cell>15.59</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Metrics for billiards imputation accuracy. Statistics closer to the expert indicate better model performance. NAOMI has the best overall performance, reducing deviation from ground truth by 30% to 70% across all metrics compared to autoregressive baselines.</figDesc><table><row><cell>Models</cell><cell>Linear</cell><cell>KNN</cell><cell cols="5">GRUI MaskGAN SingleRes NAOMI Expert</cell></row><row><cell>Sinuosity</cell><cell>1.121</cell><cell cols="2">1.469 1.859</cell><cell>1.095</cell><cell>1.019</cell><cell>1.006</cell><cell>1.000</cell></row><row><cell>step change (10 −3 )</cell><cell>0.961</cell><cell cols="2">24.59 28.19</cell><cell>15.35</cell><cell>9.290</cell><cell>7.239</cell><cell>1.588</cell></row><row><cell>reflection to wall</cell><cell>0.247</cell><cell cols="2">0.189 0.225</cell><cell>0.100</cell><cell>0.038</cell><cell>0.023</cell><cell>0.018</cell></row><row><cell>L2 loss (10 −2 )</cell><cell>19.00</cell><cell cols="2">5.381 20.57</cell><cell>1.830</cell><cell>0.233</cell><cell>0.067</cell><cell>0.000</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Metrics for basketball imputation accuracy. Statistics closer to the expert indicate better model performance. NAOMI has the best overall performance, reducing deviation from ground truth by more than 70% compared to autoregressive baselines.</figDesc><table><row><cell>Models</cell><cell>Linear</cell><cell>KNN</cell><cell cols="5">GRUI MaskGAN SingleRes NAOMI Expert</cell></row><row><cell>Path Length</cell><cell>0.482</cell><cell cols="2">0.921 1.141</cell><cell>0.793</cell><cell>0.702</cell><cell>0.573</cell><cell>0.556</cell></row><row><cell>OOB Rate (10 −3 )</cell><cell>2.997</cell><cell cols="2">0.128 4.703</cell><cell>4.592</cell><cell>3.874</cell><cell>1.733</cell><cell>0.861</cell></row><row><cell>Step Change (10 −3 )</cell><cell>0.522</cell><cell cols="2">13.24 14.95</cell><cell>9.622</cell><cell>4.811</cell><cell>2.565</cell><cell>1.982</cell></row><row><cell>Path Difference</cell><cell>0.519</cell><cell cols="2">0.746 0.690</cell><cell>0.680</cell><cell>0.571</cell><cell>0.581</cell><cell>0.580</cell></row><row><cell>Player Distance</cell><cell>0.422</cell><cell cols="2">0.403 0.398</cell><cell>0.427</cell><cell>0.417</cell><cell>0.423</cell><cell>0.425</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Figure 10: Billiard Forward Prediction Comparison. Top: metrics for billiards prediction accuracy. Statistics closer to the expert indicate better model performance. Bottom: predicted billiards trajectories. Black dots represent known observations. NAOMI perfectly recovers the ground-truth.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">SingleRes NAOMI Expert</cell></row><row><cell>Sinuosity</cell><cell>1.054</cell><cell>1.038</cell><cell>1.020</cell><cell>1.00</cell></row><row><cell cols="2">Step Change (10 −3 ) 11.6</cell><cell>9.69</cell><cell>10.8</cell><cell>1.59</cell></row><row><cell>Reflection to wall</cell><cell>0.074</cell><cell>0.068</cell><cell>0.036</cell><cell>0.018</cell></row><row><cell>L2 Loss (10 −3 )</cell><cell>4.698</cell><cell>4.753</cell><cell>1.682</cell><cell>0.0</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/felixykliu/NAOMI</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://youtu.be/eoiK42w02w0</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. This work was supported in part by NSF #1564330, NSF #1850349, and DARPA PAI: HR00111890035.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">3d people tracking with gaussian process dynamical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="238" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On the estimation of arima models with missing values</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Craig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Ansley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Time series analysis of irregularly observed data</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1984" />
			<biblScope unit="page" from="9" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Multiple imputation for nonresponse in surveys</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Donald B Rubin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>John Wiley &amp; Sons</publisher>
			<biblScope unit="volume">81</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The treatment of missing values and its effect on classifier accuracy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Acuna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Rodriguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Classification, clustering, and data mining applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="639" to="647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Statistical analysis with missing data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Roderick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Little</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Donald B Rubin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>Wiley</publisher>
			<biblScope unit="volume">793</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Recurrent neural networks for multivariate time series with missing values</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengping</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Purushotham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">6085</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Maskgan: Better text generation via filling in the (blank)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Gain: Missing data imputation using generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsung</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Jordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihaela</forename><surname>Van Der Schaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multivariate time series imputation with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangrui</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1603" to="1614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">mice: Multivariate imputation by chained equations in r</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karin</forename><surname>S Van Buuren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Groothuis-Oudshoorn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of statistical software</title>
		<imprint>
			<biblScope unit="page" from="1" to="68" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The elements of statistical learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerome</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Springer series in statistics</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Missing data: A comparison of neural network and expectation maximization techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Fulufhelo V Nelwamondo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tshilidzi</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marwala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Science</title>
		<imprint>
			<biblScope unit="page" from="1514" to="1521" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Estimating missing data in temporal data streams using multi-directional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsung</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihaela</forename><surname>William R Zame</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Der Schaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Brits: Bidirectional recurrent imputation for time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6776" to="6786" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Nonautoregressive neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deterministic non-autoregressive neural sequence modeling by iterative refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elman</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">End-to-end non-autoregressive neural machine translation with connectionist temporal classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindřich</forename><surname>Libovickỳ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindřich</forename><surname>Helcl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Parallel wavenet: Fast high-fidelity speech synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Lockhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Cobo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stimberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improved variational inference with inverse autoregressive flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Durk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4743" to="4751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Generative adversarial imitation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4565" to="4573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Seqgan: Sequence generative adversarial nets with policy gradient</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lantao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2852" to="2858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Parallel multiscale autoregressive density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aäron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><forename type="middle">Gómez</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Belov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando</forename><surname>De Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Hierarchical multiscale recurrent neural networks. International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multiresolution recurrent neural networks: An application to dialogue response generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iulian</forename><surname>Vlad Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Klinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Tesauro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartik</forename><surname>Talamadupula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3288" to="3294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A hierarchical latent vector model for learning long-term structure in music</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Curtis</forename><surname>Hawthorne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Eck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">On the sample complexity of reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sham Machandranath Kakade</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
		<respStmt>
			<orgName>University of London London, England</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">UCI machine learning repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dheeru</forename><surname>Dua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Casey</forename><surname>Graff</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning visual predictive models of physics for playing billiards</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pulkit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Generating long-term trajectories using deep hierarchical networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisong</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Hobbs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1543" to="1551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Generating multi-agent trajectories using programmatic weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisong</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A theory for multiresolution signal decomposition: the wavelet representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Stephane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="1989" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="674" to="693" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
