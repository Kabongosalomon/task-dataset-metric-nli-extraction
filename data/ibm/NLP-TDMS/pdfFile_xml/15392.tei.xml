<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/yhou/git/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-02-07T08:48+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Combing Context and Commonsense Knowledge Through Neural Networks for Solving Winograd Schema Problems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Liu</surname></persName>
							<email>s:quanliu@mail.ustc.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">York University</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">񮽙 National Research Council Canada</orgName>
								<address>
									<settlement>Ottawa</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen-Hua</forename><surname>Ling</surname></persName>
							<email>zhling@ustc.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>񮽙</roleName><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">iFLYTEK Research</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
							<email>siwei@iflytek.com</email>
							<affiliation key="aff3">
								<orgName type="department">iFLYTEK Research</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Hu</surname></persName>
							<email>yuhu@iflytek.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">National Engineering Laboratory for Speech and Language Information Processing</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<region>Anhui</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Combing Context and Commonsense Knowledge Through Neural Networks for Solving Winograd Schema Problems</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper proposes a general framework to combine context and commonsense knowledge for solving the Winograd Schema (WS) and Pronoun Disambiguation Problems (PDP). In the proposed framework, commonsense knowledge bases (e.g. cause-effect word pairs) are quantized as knowledge constraints. The constraints guide us to learn knowledge enhanced embeddings (KEE) from large text corpus. Based on the pre-trained KEE models, this paper proposes two methods to solve the WS and PDP problems. The first method is an unsupervised method, which represents all the pronouns and candidate mentions in continuous vector spaces based on their contexts and calculates the semantic similarities between all the possible word pairs. The pronoun disambigua-tion procedure could then be implemented by comparing the semantic similarities between the pronoun (to be resolved) and all the candidate mentions. The second method is a supervised method, which extracts features for all the pronouns and candidate mentions and solves the WS problems by training a typical mention pair classification model. Similar to the first method, the features used in the second method are also extracted based on the KEE models. Experiments conducted on the available PDP and WS test sets show that, these two methods both achieve consistent improvements over the base-line systems. The best performance reaches 62% in accuracy on the PDP test set of the first Winograd Schema Challenge.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>In recent years, many AI challenges or competitions have been proposed to help evaluate the cognitive levels of machines ( <ref type="bibr" target="#b4">Levesque, Davis, and Morgenstern 2011;</ref><ref type="bibr" target="#b21">Weston et al. 2015;</ref><ref type="bibr" target="#b0">Clark 2015</ref>). Among those challenges, the Winograd Schema Challenge (WSC) has been proposed as an alternative to the Turing Test <ref type="bibr">(Levesque, Davis, and Morgen- stern 2011)</ref>. Turing first introduced the notion of testing a computer system's intelligence by assessing whether it could make a human judge think that she was conversing with a human rather a computer <ref type="bibr" target="#b17">(Turing 1950)</ref>. However, some recent efforts have merely on engaged surface-level conversation tricks to fool humans who do not delve deeply enough into a conversation, and make them think they are speaking to another human being <ref type="bibr" target="#b19">(Veselov, Demchenko, and Ulasen ;</ref><ref type="bibr" target="#b20">Warwick and Shah 2014)</ref>. To fix this issue, WSC is claimed to be a more suitable task which does not rely on human's subjective assessment. A Winograd schema (WS) question is a pair of sentences that differ only in one or two words which results in a different resolution of coreference. A wellknown example is "The city council refused the demonstrators a permit because they feared violence," where "they" refers to the council. But if one changes the verb "feared" to "advocated", the computer needs to know that "they" refers to "demonstrator", if it possesses real intelligence. To solve the problem, common sense knowledge is essential. Towards solving the final solution for WS problem, a similar test set, called pronoun disambiguation problem (PDP) is designed as the first round of the Winograd Schema Challenge <ref type="bibr" target="#b10">(Morgenstern, Davis, and Ortiz Jr 2016)</ref>. A typical example is "Mrs. March gave the mother tea and gruel, while she dressed the little baby as tenderly as if it had been her own." One way to reason that she in she dressed refers to Mrs. March and not the mother, is to realize that the phrase "as if it were her own" implies that it (the baby) is not actually her own; that is, she is not the mother and must, by process of elimination, be Mrs. March. Similar to the Winograd schemas, a substantial amount of commonsense knowledge appears to be necessary to disambiguate pronouns. This paper proposes neural network models to combine context and commonsense knowledge to solve both the WS and PDP problem. The main ideas are two-fold: 1) The first is to leverage context effectively. We believe that since context is a key information for learning word meanings, it should be useful for disambiguating the Winograd schema problems. In general, modeling context and learning word meanings could be very efficient through unsupervised learning that leverages large amounts of free texts. 2) In the common sense respect, we describe a simple but effec- <ref type="bibr">The AAAI 2017 Spring Symposium on Computational Context: Why It's Important, What It Means, and Can It Be Computed? Technical Report SS-17-03</ref> tive method utilizing commonsense knowledge <ref type="table">. To jointly  consider those two aspects, this paper proposes to combine  context and commonsense knowledge through neural net- works through a knowledge enhanced embeddings (KEE)</ref> framework. This paper further proposes two methods: 1) The first is the an unsupervised semantic similarity method (USSM), which represents all the pronouns and candidate mentions by composing their contexts based on the pretrained knowledge enhanced embeddings. We then calculate the semantic similarities between the embedding vectors of the pronoun under concern and all candidate mentions. The candidate with largest semantic similarity with respect to the pronoun will be predicted as the answer.</p><p>2) The second method is a neural knowledge activated method (NKAM), which extracts features based on the KEE models and trains a mention pair classifier with neural networks. For the experiment section, this paper conducts experiments on the official datasets of the first WSC challenge, including the PDP test set provided by commonsensereasoning.org, as well as a set of Winograd schemas manually created by <ref type="bibr" target="#b4">(Levesque, Davis, and Morgenstern 2011)</ref>. Experimental results all indicate that the proposed KEE method (combing context and knowledge) performs better comparing with the baseline models.</p><p>The remainder of the paper will start with introducing the main motivation. After that, we introduce the main methods proposed to solve the WS and PDP problems. We then present all the experiments, including setup, datasets, and results, before we conclude this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Motivation</head><p>In this section, we introduce the main motivation of this work. We will firstly present the main problems we aim to solve, i.e., the Winograd schema (WS) and the Pronoun Disambiguation Problem (PDP). After that, detailed descriptions would be given to illustrate our motivation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Winograd Schema (WS)</head><p>The Winograd schema (WS) evaluates a system's commonsense reasoning ability based on a traditional, very specific natural language processing task: coreference resolution <ref type="bibr" target="#b14">(Saba 2015)</ref>. The WS problems are carefully designed to be a task that cannot be easily solved without commonsense knowledge. In fact, even the solution of traditional coreference resolution problems rely on semantics or world knowledge (Strube 2016). As described in <ref type="bibr" target="#b4">(Levesque, Davis, and Morgenstern 2011)</ref>, a WS is a small reading comprehension test involving a single binary question.</p><p>• Joan made sure to thank Susan for all the help she had given. Who had given the help?</p><p>-Answer A: Joan -Answer B: Susan -Correct Answer: B</p><p>The correct answers to the above question are obvious for human beings. In each of the questions, the corresponding WS has the following four features:</p><p>1. Two parties are mentioned in a sentence by noun phrases. They can be two males, two females, two inanimate objects or two groups of people or objects.</p><p>2. A pronoun or possessive adjective is used in the sentence in reference to one of the parties, but is also of the right sort for the second party.</p><p>3. The question involves determining the referent of the pronoun or possessive adjective. Answer A is always the first party mentioned in the sentence (but repeated from the sentence for clarity), and Answer B is the second party.</p><p>4. There is a word (called the special word) that appears in the sentence and possibly the question. When it is replaced by another word (called the alternate word), everything still makes perfect sense, but the answer changes.</p><p>In this example, if we change the word given to received, the answer changes to A (i.e., Joan) since in our commonsense knowledge, we think that a person who receives help should makes sure to thank the person who provides help to him.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pronoun Disambiguation Problems (PDP)</head><p>The pronoun disambiguation problems (PDP) are complex coreference resolution problems, which are taken directly or modified from examples found in literature, biographies, autobiographies, essays, news analyses, and news stories; they may need some manual processing (Morgenstern, Davis, and Ortiz Jr 2016). Here is one typical PDP example:</p><p>• The Dakota prairie lay so warm and bright under the shining sun that it did not seem possible that it had ever been swept by the winds and snows of that hard winter.</p><p>-Snippet: it had ever been swept -Answer A: the prairie -Answer B: the sun -Correct Answer: A</p><p>In the PDP problem, the pronoun to be resolved is highlighted in bold. It is repeated again, with a snippet of context, and with several candidate answers, in the line following the passage. In the example shown here, we know that the prairie (rather than the sun) would be more likely to be swept by the winds. A difference between PDP and WS problems is that, the number of candidate noun phrases in each PDP problem would not always be two, but can be three, four, or even more. Therefore, the random-guess accuracy in the PDP problems will be less than 50% while the accuracy of a random-guess for WS is 50%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Motivation: Knowledge Enhanced Embeddings</head><p>Solving WS or PDP problems is not easy since it requires commonsense knowledge. In the paper, we propose to combine context and commonsense knowledge through neural networks for solving both problems. The main motivation is described as follows. First, since context is key for learning word meaning <ref type="bibr" target="#b2">(Harris 1954;</ref><ref type="bibr" target="#b9">Miller and Charles 1991)</ref>, we represent the pronoun and all the candidate mentions by their contexts. For instance, in the aforementioned PDP example, we represent the word prairie by The Dakota and lay so warm and bright. Meanwhile, the word sun could be represented by and bright under the shining and that it did not seem. Based on this, the pronoun disambiguation problems is solved by calculating the semantic similarities between the representations of the pronoun and all the corresponding candidates. Second, relying only on context is not good enough for tackling the WS and PDP problems. It is essential to find an effective strategy to combine context and commonsense knowledge. Therefore, in this paper, we propose 񮽙񮽙񮽙񮽙񮽙񮽙񮽙 񮽙񮽙񮽙񮽙 񮽙񮽙񮽙 <ref type="figure" target="#fig_5">Figure 1</ref>: Based on Knowledge Enhanced Embeddings, two levels considered to be useful for solving WS and PDP.</p><p>a knowledge enhanced embedding (KEE) model. As shown in <ref type="figure" target="#fig_5">Figure 1</ref>, the KEE model combines the context and commonsense knowledge, in the feature level. By training KEE model using large text corpora and commonsense KBs, we obtain useful distributed word representations. Based on the pre-trained word representations, we propose two effective methods in the model level, for finally solve the WS and PDP problems. The proposed two methods are semantic similarity method and neural network method. In the semantic similarity method, we represent all the pronouns and candidate mentions by composing their contexts from words. By further calculating the semantic similarities between the representations of each pronoun and the corresponding candidate mentions, the procedure to answer PDP or WS questions could then be implemented by finding the most similar candidate for the pronoun. In the neural network methods, the pre-trained KEE models are also used for extracting embedding features for all the pronouns and candidates. However, we do not calculate the semantic similarities. On the contrary, we train a typical neural mention pair classifier with supervised coreference training dataset. No matter how large the differences between these two methods, both of them are all implemented based on the KEE model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Proposed Methods</head><p>Based on the motivation, we introduce the KEE model and two methods to solve WS and PDP problems. Before introducing those methods, we describe the commonsense knowledge used in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Commonsense Knowledge</head><p>There have been open commonsense knowledge bases in the artificial intelligence community, e.g. Cyc (Lenat 1995),</p><p>ThoughtTreasure <ref type="bibr" target="#b11">(Mueller 1998</ref>) and ConcepNet ( <ref type="bibr" target="#b5">Liu and Singh 2004)</ref>. Cyc is an artificial intelligence project that attempts to assemble a comprehensive ontology and knowledge base of everyday common sense knowledge, with the goal to enable AI applications to perform human-like reasoning. Typical pieces of knowledge represented in the Cyc database are "every tree is a plant" and "plants die eventually". ConceptNet is a semantic network containing lots of things computers should know about the world, especially when understanding text written by people. It is built from nodes representing words or short phrases of natural language, and labeled relationships between them. For example, the triple (learn, MotivatedByGoal, knowledge) indicates that "we would learn because we want knowledge". Those existing commonsense KBs are well constructed; however, in this paper, we aim to find commonsense knowledge for solving the WS and PDP problems by the following requirements: 1) to avoid data sparseness problem, the vocabulary of the KB covers common words (not phrases) in daily life, e.g., common verbs, adjectives, etc.</p><p>2) The commonsense relationships between the nodes in the vocabulary cover common relations, e.g. cause-effect, entailment, etc. Based on these two requirements, this paper proposes to use the KB constructed by a recent work ( <ref type="bibr" target="#b7">Liu et al. 2016)</ref>, which collects word pairs with cause-effect relationships automatically. <ref type="figure" target="#fig_1">Figure 2</ref> shows the typical formula of the corresponding KB.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Knowledge Enhanced Embedding</head><p>To combine context and commonsense knowledge for solving the WS and PDP problems, this paper proposes to treat the commonsense knowledge as semantic constraints and learn knowledge enhanced embedding (KEE) based on the generated constraints. The idea to learn word embedding based on constraints is similar to the work of ( <ref type="bibr" target="#b6">Liu et al. 2015</ref>). The main difference is the way we generate the knowledge constraints. In this paper, we propose to create constraints as follows:</p><p>Knowledge constraints Since all the cause-effect pairs used in this paper contain the corresponding confidence weights, i.e. PMI values, we propose to generate semantic inequalities by randomly sampling two cause-effect pairs. More specifically, for each cause-effect pair, we will randomly sample 5 different pairs from the whole KB set and construct the inequalities by comparing their PMI values respectively. For instance, once we generate cause-effect pair (w i , w j ) and (w k , w g ) and the PMI value of pair (w i , w j ) is larger than pair (w k , w g ), we can make the inequality as:</p><formula xml:id="formula_0">sim(w i , w j ) &gt; sim(w k , w g )<label>(1)</label></formula><p>The idea to generate such inequality is similar to the physical meaning of lexical entailment <ref type="bibr" target="#b1">(Geffet and Dagan 2005;</ref><ref type="bibr" target="#b18">Turney and Mohammad 2015)</ref>. This paper assumes if a word tends to be the effect of another word, they should have similar context patterns. Note that the commonsense knowledge base used in this paper covers all the common verbs and adjectives, and the knowledge constraints would not influence the learning for the remaining words in the whole large vocabulary. This is important because the used verbs and adjectives play a central role in commonsense reasoning. Currently, incorporating more knowledge of other types of words, e.g., nouns, adverbs and prepositions is beyond our concern.</p><p>The main framework The main framework for learning knowledge enhanced embedding is shown in <ref type="figure" target="#fig_2">Figure 3</ref>.  The left part in this framework is the typical skip-gram model, which learns continuous word vectors from text corpora based on the aforementioned distributional hypothesis ( <ref type="bibr" target="#b8">Mikolov et al. 2013</ref>). Each word in vocabulary (size of V ) is mapped to a continuous embedding space by looking up an embedding matrix W <ref type="bibr">(1)</ref> . And W (1) is learned by maximizing the prediction probability, calculated by another prediction matrix W <ref type="bibr">(2)</ref> , of its neighbouring words within a context window. Given a sequence of training data, denoted as w 1 , w 2 , w 3 , ..., w T with T words, the skip-gram model aims to maximize the objective function:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>񮽙</head><formula xml:id="formula_1">Q = 1 T T 񮽙 t=1 񮽙 −c≤j≤c,j񮽙 =0 log p(w t+j |w t )<label>(2)</label></formula><p>where c is the size of context windows, w t denotes the input central word and w t+j for its neighbouring word. The skip-gram model computes the above conditional probability p(w t+j |w t ) using the following softmax function:</p><formula xml:id="formula_2">p(w t+j |w t ) = exp(w (2) t+j · w (1) t ) 񮽙 V k=1 exp(w (2) k · w (1) t )<label>(3)</label></formula><p>where w</p><p>(1) t and w <ref type="bibr">(2)</ref> k denotes row vectors in matrices W (1) and W <ref type="bibr">(2)</ref> , corresponding to word w t and w k respectively.</p><p>In this paper, we proposed to incorporate the commonsense knowledge as constraints into the word embedding training process. Assume the knowledge is represented by a large number of inequalities, denoted as the set S. This paper denotes s ij = sim(w </p><formula xml:id="formula_3">{W (1) , W (2) } = arg max W (1) ,W (2) Q(W (1) , W (2) ) (4) subject to s ij &gt; s kg ∀(i, j, k, g) ∈ S.</formula><p>(5) In this work, we formulate the above constrained optimization problem into an unconstrained one by casting all the constraints as a penalty term in the objective function:</p><formula xml:id="formula_4">Q 񮽙 = Q − β · D D = 񮽙 (i,j,k,g)∈S f (i, j, k, g)<label>(6)</label></formula><p>where β is a control parameter to balance the contribution of the penalty term in the optimization process. The function f (·) is a normalization function. This paper uses a hinge loss function like</p><formula xml:id="formula_5">f (i, j, k, g) = h(s kg − s ij ) where h(x) = max(0, x).</formula><p>The objective function in eq. (6) could be optimized using the standard stochastic gradient descent (SGD) algorithm. Generally, as shown in <ref type="figure" target="#fig_4">Figure 4</ref>, the work to combine context and commonsense knowledge is implemented in the proposed KEE model. After that, embeddings trained by the KEE would serve for the two methods (USSM and NKAM) designed to solve the WS and PDP problems in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unsupervised Semantic Similarity Method</head><p>The first method proposed in this paper for answering the WS and PDP problems, shown in <ref type="figure">Figure 5</ref>, is an unsupervised method. We introduce a straightforward unsupervised semantic similarity method (USSM), which aims to For one sentence, the function FOFE works as follows. Given a sequence of words, S = {w 1 , w 2 , ..., w T }, each word w t is first represented by a 1-of-K representation e t , from the first word t = 1 to the end of the sequence t = T , FOFE encodes each partial sequence (history) based on a simple recursive formula (with z 0 = 0) as:</p><formula xml:id="formula_6">z t = α · z t−1 + e t , (1 ≤ t ≤ T )<label>(7)</label></formula><p>where z t denotes the FOFE code for the partial sequence up to w t , and α, (0 &lt; α &lt; 1) is a constant forgetting factor to control the influence of the history on the current position. Assume we have three symbols in vocabulary, e.g., A, B, C, In this paper, we first use the FOFE method to encode the context of each word into a fixed-size code (of the vocabulary size). Then, we use the embedding matrix W (1) , learned by KEE as above, to project into a low-dimension space. These low-dimension vectors are used to calculate cosine distances to select the answer from the candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Neural Knowledge Activated Method</head><p>As shown in <ref type="figure" target="#fig_6">Figure 6</ref>, the second method proposed in this paper is an supervised method. The difference from the first semantic similarity method is that, it does not simply calculate the semantic similarities between the extracted embed- ding vectors of the pronoun (to be resolved) and all the candidate mentions, but instead uses the composed embedding vectors as input features and train a deep neural networks (DNN). The DNN model works as a mention pair classifier for judging whether two mentions are coreferent or not, which is a widely used technology in the coreference resolution community <ref type="bibr" target="#b13">(Ng 2010)</ref>. Since the features we extracted for training the DNN are composed from the knowledge enhanced embeddings, we call the method as neural knowledge activated method (NKAM) hereafter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>In this section, we present all the experiments conducted to evaluate the effectiveness of the proposed methods. This section would be started by introducing the experimental datasets and experimental setups. After that, experimental results and analysis would be given correspondingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head><p>For evaluating the effectiveness of the proposed methods and keep our methods comparable, all the experimental datasets investigated in this paper, including PDP test set 1 and WS test set 2 , are from the Winograd Schema Challenge (Morgenstern, Davis, and Ortiz Jr 2016).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental setup</head><p>To make clear all the settings for the proposed methods of this work, we describe the experimental setup as follows.  smaller size is a data set containing the first one billion characters from Wikipedia 3 , named as Wiki-Small in our experiments. The second corpus of a relatively large size is a snapshot of the Wikipedia articles from (Shaoul 2010), named as Wiki-Large in our experiments. Both Wikipedia corpora have been pre-processed by removing all the HTML metadata and hyper-links and replacing the digit numbers with English words. After text normalization, the Wiki-Small corpus contains totally 130 million words, for which we create a lexicon of 225,909 distinct words appearing more than 5 times in the corpus. Similarly, the Wiki-Large corpus contains about 1 billion words, for which we create a lexicon of 235,167 words, each appearing more than 60 times. In all the experiments of this paper, the settings for KEE are the same. The embedding dimension is set to be 100 while the context window size c in eq. (2) is set to be 5. The combination coefficient β in eq. (6) is set to be 0.01. The KEE models are trained by the stochastic gradient descents (SGD) algorithm. The initial learning rate is set as 0.025 and the learning rate is decreased linearly during the SGD model training process.</p><p>Setup for USSM and NKAM As for feature extraction in the USSM or NKAM methods, for both pronouns and candidate mentions, the context we utilize for feature extraction is the entire sentence. Meanwhile, the weight α in eq. <ref type="formula" target="#formula_6">(7)</ref> is set to 0.7 for context composition. In the USSM method, we use the popular Cosine similarity to evaluate the semantic similarity between any two mentions. On the other hand, for the NKAM method, this paper uses the popular coreference resolution datasets, i.e., <ref type="bibr">OntoNotes (Weischedel et al. 2013</ref>), to extract labelled mention pairs for model training. Considering the sentence length in the WS or PDP questions is usually less than 3, in this paper, we extract all the labelled mention pairs for pronouns within three adjacent sentences. We finally extract 306,903 training mention pairs. Meanwhile, the corresponding neural network has 1 hidden layer with 300 units. The non-linear activation function is rectified linear unit (ReLU) (Nair and Hinton 2010).</p><p>3 http://mattmahoney.net/dc/enwik9.zip. <ref type="table" target="#tab_5">Table 3</ref> shows the overall results. We divide the results by the text corpus we used for KEE training. In addition to experimenting the proposed two methods, i.e., USSM and NKAM, we also construct a system by combing the USSM and NKAM methods. For each pronoun and its candidate mentions, the system combination procedure is implemented by interpolating the scores calculated by the USSM and NKAM method (the interpolation coefficient is 70% for NKAM and 30% for USSM). From the results, we find the USSM method (as the unsupervised method) achieves a 41.7% and 48.3% accuracy on the PDP test set when the KEE models are only trained on texts (no commonsense knowledge combined, which is equal to the skip-gram models) from WikiSmall and Wiki-Large. When we use the pre-trained knowledge enhanced embeddings (context+knowledge), the corresponding performance improves to 48.3% and 55.0%. Similar performances are achieved in NKAM and the combined system as well. In the combined system, we obtain a 61.7% accuracy on the PDP test set, which is significant better than the system (53.3%) constructed solely based on the context (without combining with commonsense knowledge). Meanwhile, since the WS test set is carefully designed by human beings, the performance of the baseline systems, i.e., all the results shown in the rows of "KEE settings = Context", are poor. The best performance is 50.6% when using Wiki-Large as text corpus, in the combined system. This suggests that only relying on context is clearly not good enough. After applying KEE to the USSM and NKAM methods, we achieve a 52.8% accuracy on the whole Winograd schemas test set, which is 4.4% better than the corresponding system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>This paper proposes a general knowledge enhanced embedding (KEE) framework to combine context and commonsense knowledge for solving the Winograd Schema (WS) and Pronoun Disambiguation Problems (PDP). The KEE is a flexible framework to learn distributed representations under the supervision of commonsense knowledge from large text corpus. Using the pre-trained KEE representations, we further proposes two methods, i.e. unsupervised semantic similarity method and neural knowledge activated method, to solve the WS and PDP problems. Experiments conducted on the official datasets show that these two methods achieve consistent improvements over the baseline systems. Furthermore, investigations conducted in this paper also provide some insights on the Winograd Schema Challenge.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Copyright c 񮽙 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The typical formula of the commonsense KB: automatically constructed cause-effect word pairs. The KB contains a large number of cause-effect word and phrase pairs constructed from large text corpora. The vocabulary covered by the KB contains thousands of common verbs and adjectives. At shown in Figure 2, there are four pattern roles for both the cause and effect phrases. The four roles include (active,positive), (active,negative), (passive,positive) and (passive,negative). Table 1 shows some examples. In this paper, all the word pairs (rather than phrase pairs) of this KB are used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The framework for knowledge enhanced embeddings (KEE). The training process combines the text corpus and commonsense knowledge.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>j</head><label></label><figDesc>) as the semantic simi- larity hereafter. The final objective function becomes:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The main methods used in this paper.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>whose 1 -</head><label>1</label><figDesc>of-K codes are [1, 0, 0], [0, 1, 0] and [0, 0, 1] respec- tively. In this case, the FOFE code for the sequence {ABC} is [α 2 , α, 1], and that of {ABCBC} is [α 4 , α + α 3 , 1 + α 2 ].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The neural knowledge activated method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 shows</head><label>1</label><figDesc>some examples. In this paper, all the word pairs (rather than phrase pairs) of this KB are used.</figDesc><table>No 
Pairs 
Meaning 
1 
(win, happy) 
sb. win → happy. 
2 
(rob, be arrested) 
sb. rob → be arrested. 
3 
(confident, not afraid) sb. confident → not afraid 
4 
(be restricted, unable) sb. be restricted → unable 

Table 1: Typical examples of the cause-effect pairs. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>񮽙 񮽙񮽙񮽙 񮽙 񮽙񮽙񮽙 񮽙 񮽙񮽙񮽙񮽙񮽙 񮽙 񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙 񮽙 񮽙񮽙 񮽙 񮽙񮽙񮽙</head><label></label><figDesc></figDesc><table>񮽙 

񮽙 񮽙񮽙񮽙 
񮽙 񮽙񮽙񮽙񮽙񮽙 
񮽙 񮽙 
񮽙 

񮽙 񮽙 
񮽙 񮽙 
񮽙 񮽙 
񮽙 񮽙 

񮽙񮽙񮽙񮽙񮽙 񮽙 </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 lists</head><label>2</label><figDesc></figDesc><table>the 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 : Experimental datasets used in this paper.</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 : Overall experimental results. The random-guess accuracies for PDP test and WS test set are 45%, 50% respectively.</head><label>3</label><figDesc></figDesc><table></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Elementary school science and math tests as a driver for ai: take the Aristo challenge! In AAAI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Clark</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4019" to="4021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The distributional inclusion hypotheses and lexical entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Geffet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="107" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">S</forename><surname>Harris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Distributional structure. Word</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="146" to="162" />
			<date type="published" when="1954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Cyc: A large-scale investment in knowledge infrastructure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Lenat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="33" to="38" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The winograd schema challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Levesque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Morgenstern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page">47</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Conceptnetła practical commonsense reasoning tool-kit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BT technology journal</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="211" to="226" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning semantic word embeddings based on ordinal knowledge constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1501" to="1511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Evdokimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.07704</idno>
		<title level="m">Probabilistic reasoning via deep learning: Neural association models</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Contextual correlates of semantic similarity. Language and cognitive processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">G</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Planning, executing, and evaluating the winograd schema challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Morgenstern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Ortiz</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Magazine</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="50" to="54" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Natural language processing with Thought Treasure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Mueller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<pubPlace>Signiform New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Machine Learning (ICML-10)</title>
		<meeting>the 27th International Conference on Machine Learning (ICML-10)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Supervised noun phrase coreference research: The first fifteen years</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th annual meeting of the association for computational linguistics</title>
		<meeting>the 48th annual meeting of the association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1396" to="1411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">On the winograd schema challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Saba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shaoul</surname></persName>
		</author>
		<title level="m">The westbury lab wikipedia corpus. Edmonton</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
		<respStmt>
			<orgName>AB: University of Alberta</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The (non) utility of semantics for coreference resolution (corbon remix)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL 2016 workshop on Coreference Resolution Beyond OntoNotes</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Computing machinery and intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Turing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mind</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">236</biblScope>
			<biblScope unit="page" from="433" to="460" />
			<date type="published" when="1950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Experiments with three approaches to recognizing lexical entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">D</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">03</biblScope>
			<biblScope unit="page" from="437" to="476" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Veselov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Demchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ulasen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eugene goostman</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Good machine performance in turing&apos;s imitation game</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Warwick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computational Intelligence and AI in Games</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="289" to="299" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weischedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Franchini</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.05698</idno>
		<title level="m">Towards ai-complete question answering: A set of prerequisite toy tasks</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
	<note>Ontonotes release 5.0 ldc2013t19. Linguistic Data Consortium</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The fixed-size ordinally-forgetting encoding method for neural network language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
