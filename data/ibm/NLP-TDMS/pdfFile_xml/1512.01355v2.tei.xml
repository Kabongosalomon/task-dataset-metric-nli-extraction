<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Staple: Complementary Learners for Real-Time Tracking</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Valmadre</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Golodetz</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Miksik</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Staple: Complementary Learners for Real-Time Tracking</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>#1 DSST Staple DAT #20 #50 #90 #100 #1 colour histogram scores HOG #10 #50</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: Sometimes colour distributions are not enough to discriminate the target from the background. Conversely, template models (like HOG) depend on the spatial configuration of the object and perform poorly when this changes rapidly. Our tracker Staple can rely on the strengths of both template and colour-based models. Like DSST [10], its performance is not affected by non-distinctive colours (top). Like DAT [33], it is robust to fast deformations (bottom).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Correlation Filter-based trackers have recently achieved excellent performance, showing great robustness to challenging situations exhibiting motion blur and illumination changes. However, since the model that they learn depends strongly on the spatial layout of the tracked object, they are notoriously sensitive to deformation. Models based on colour statistics have complementary traits: they cope well with variation in shape, but suffer when illumination is not consistent throughout a sequence. Moreover, colour distributions alone can be insufficiently discriminative. In this paper, we show that a simple tracker combining complementary cues in a ridge regression framework can operate faster than 80 FPS and outperform not only all entries in the popular VOT14 competition, but also recent and far more sophisticated trackers according to multiple benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>We consider the widely-adopted scenario of short-term, single-object tracking, in which the target is only specified in the first frame (using a rectangle). Short-term implies that re-detection should not be necessary. The key challenge of tracking an unfamiliar object in video is to be robust to changes in its appearance. The task of tracking unfamiliar objects, for which training examples are not available in advance, is interesting because in many situations it is not feasible to obtain such a dataset. It is advantageous for the algorithm to perform above real-time for computationally intensive applications such as robotics, surveillance, video processing and augmented reality.</p><p>Since an object's appearance can vary significantly during a video, it is not generally effective to estimate its model from the first frame alone and use this single, fixed model to locate the object in all other frames. Most state-of-theart algorithms therefore employ model adaptation to take advantage of information present in later frames. The simplest, most widespread approach is to treat the tracker's predictions in new frames as training data with which to update the model. The danger of learning from predictions is that small errors can accumulate and cause model drift. This is particularly likely to happen when the appearance of the object changes.</p><p>In this paper, we propose Staple (Sum of Template And Pixel-wise LEarners), a tracker that combines two image patch representations that are sensitive to complementary factors to learn a model that is inherently robust to both colour changes and deformations. To maintain real-time speed, we solve two independent ridge-regression problems, exploiting the inherent structure of each representation. Compared to other algorithms that fuse the predictions of multiple models, our tracker combines the scores of two models in a dense translation search, enabling greater accuracy. A critical property of the two models is that their scores are similar in magnitude and indicative of their reliability, so that the prediction is dominated by the more confident.</p><p>We establish the surprising result that a simple combination of a Correlation Filter (using HOG features) and a global colour histogram outperforms many more complex trackers in multiple benchmarks while running at speeds in excess of 80 FPS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Online learning and Correlation Filters. Modern approaches to adaptive tracking often use an online version of an object detection algorithm. One approach that achieves strong results <ref type="bibr" target="#b38">[39]</ref> and has an elegant formulation is Struck <ref type="bibr" target="#b15">[16]</ref>, which seeks to minimise the structured output objective for localisation <ref type="bibr" target="#b2">[3]</ref>. However, the computation needed limits the number of features and training examples.</p><p>Correlation Filters instead minimise a least-squares loss for all circular shifts of the positive examples. Although this might seem a weaker approximation of the true problem, it enables the use of densely-sampled examples and highdimensional feature images in real-time using the Fourier domain. Initially applied to adaptive tracking in grayscale images by Bolme et al. <ref type="bibr" target="#b4">[5]</ref>, their extension to multiple feature channels <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b17">18]</ref> and therefore HOG features <ref type="bibr" target="#b6">[7]</ref> enabled the technique to achieve state-of-the-art performance in VOT14 <ref type="bibr" target="#b23">[24]</ref>. The winner of the challenge, DSST <ref type="bibr" target="#b7">[8]</ref>, incorporated a multi-scale template for Discriminative Scale-Space Tracking using a 1D Correlation Filter. One deficiency of Correlation Filters is that they are constrained to learn from all circular shifts. Several recent works <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b8">9]</ref> have sought to resolve this issue, and the Spatially Regularised (SRDCF) <ref type="bibr" target="#b8">[9]</ref> formulation in particular has demonstrated excellent tracking results. However, this was achieved at the cost of real-time operation. Robustness to deformation. Correlation Filters are inherently confined to the problem of learning a rigid template. This is a concern when the target experiences shape deformation in the course of a sequence. Perhaps the simplest method to achieve robustness to deformation is to adopt a representation that is insensitive to shape variation. Image histograms have this property, because they discard the position of every pixel. In fact, histograms can be considered orthogonal to Correlation Filters, since a Correlation Filter is learnt from circular shifts, whereas a histogram is invariant to circular shifts. However, histograms alone are often insufficient to discriminate the object from the background. While colour histograms were used in many early approaches to object tracking <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b30">31]</ref>, they have only recently been demonstrated to be competitive in modern benchmarks in the Distractor-Aware Tracker (DAT) <ref type="bibr" target="#b32">[33]</ref>, which uses adaptive thresholding and explicit suppression of regions with similar colours. In general, histograms may be constructed from any discrete-valued feature, including local binary patterns and quantised colours. For a histogram to provide robustness to deformation, the feature must be insensitive to the local changes that arise.</p><p>The chief alternative way to achieve robustness to deformation is to learn a deformable model. We believe it is ambitious to learn a deformable model from a single video in which the only supervision is the location in the first frame, and therefore adopt a simple bounding box. While our method outperforms recent sophisticated parts-based models <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b39">40]</ref> in benchmarks, deformable models have a richer representation that these evaluations do not necessarily reward. Our single-template tracker could be considered a component with which to construct a parts-based model.</p><p>Rather than use a deformable model, HoughTrack <ref type="bibr" target="#b13">[14]</ref> and PixelTrack <ref type="bibr" target="#b10">[11]</ref> accumulate votes from each pixel and then use the pixels that voted for the winning location to estimate the object's extent. However, these methods are yet to demonstrate competitive benchmark performances. Schemes to reduce model drift. Model drift is a result of learning from inaccurate predictions. Several works have aimed to prevent drift by modifying the training strategy rather than improving the predictions. TLD <ref type="bibr" target="#b20">[21]</ref> and PROST <ref type="bibr" target="#b33">[34]</ref> encode rules for additional supervision based on optical flow and a conservative appearance model. Other approaches avoid or delay making hard decisions. MIL-Track <ref type="bibr" target="#b0">[1]</ref> uses Multiple-Instance Learning to train with bags of positive examples. Supančič and Ramanan <ref type="bibr" target="#b34">[35]</ref> introduce self-paced learning for tracking: they solve for the optimal trajectory keeping the appearance model, then update the model using the most confident frames, and repeat. Grabner et al. <ref type="bibr" target="#b14">[15]</ref> treat tracking as online semi-supervised boosting, in which a classifier learnt in the first frame provides an anchor for the labels assigned to examples in later frames. Tang et al. <ref type="bibr" target="#b35">[36]</ref> apply co-training to tracking, learning two independent SVMs that use different features and then obtaining hard negatives from the combined scores. Of these methods, only MILTrack and TLD are found in current benchmarks, and neither has strong results.</p><p>Combining multiple estimates. Another strategy widely adopted to mitigate inaccurate predictions is to combine the estimates of an ensemble of methods, so that the weaknesses of the trackers are reciprocally compensated. In <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>, Kwon et al. make use of complementary basic trackers, built by combining different observation models and motion models, and then integrate their estimates in a sampling framework. Similarly, <ref type="bibr" target="#b37">[38]</ref> combines five independent trackers using a factorial HMM, modelling both the object trajectory and the reliability of each tracker across time. Rather than using trackers of different types, the Multi-Expert Entropy Minimisation (MEEM) tracker <ref type="bibr" target="#b40">[41]</ref> maintains a collection of past models and chooses the prediction of one according to an entropy criterion. We differ from these approaches in that a) both of our models are learnt in a common framework (specifically, ridge regres-sion), and b) this enables us to directly combine the scores of the two models in a dense search.</p><p>Long-term tracking with re-detection. Several recent works have adopted Correlation Filters for the problem of long-term tracking, where the performance of an algorithm will be greatly improved by its ability to re-detect the object. The Long-term Correlation Tracker (LCT) <ref type="bibr" target="#b29">[30]</ref> augments a standard Correlation Filter tracker with an additional Correlation Filter for confidence estimation and a random forest for re-detection, both of which are only updated in confident frames. The Multi-Store Tracker (MUSTer) <ref type="bibr" target="#b19">[20]</ref> maintains a long-term memory of SIFT keypoints for the object and background, using keypoint matching and MLESAC to locate the object. The confidence of the long-term memory is estimated using the number of inliers, and occlusions can be determined by considering the number of background keypoints that are located inside the rectangle. Since we consider mostly short-term benchmarks, and these long-term trackers are meta-algorithms that are built upon a short-term tracker, there is little value in a comparison. Note that the TLD <ref type="bibr" target="#b20">[21]</ref> and self-paced learning <ref type="bibr" target="#b34">[35]</ref> algorithms also incorporate some aspects that are well-suited to the long-term tracking problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Formulation and motivation</head><p>We adopt the tracking-by-detection paradigm, in which, in frame t, the rectangle p t that gives the target location in image x t is chosen from a set S t to maximise a score:</p><formula xml:id="formula_0">p t = arg max p∈St f T (x t , p); θ t−1 .<label>(1)</label></formula><p>The function T is an image transformation such that f (T (x, p); θ) assigns a score to the rectangular window p in image x according to the model parameters θ. The model parameters should be chosen to minimise a loss function L(θ; X t ) that depends on the previous images and the location of the object in those images</p><formula xml:id="formula_1">X t = {(x i , p i )} t i=1 : θ t = arg min θ∈Q {L(θ; X t ) + λR(θ)} .<label>(2)</label></formula><p>The space of model parameters is denoted Q. We use a regularisation term R(θ) with relative weight λ to limit model complexity and prevent over-fitting. The location p 1 of the object in the first frame is given. To achieve real-time performance, the functions f and L must be chosen not only to locate the object reliably and accurately, but also such that the problems in (1) and (2) can be solved efficiently.</p><p>We propose a score function that is a linear combination of template and histogram scores:</p><formula xml:id="formula_2">f (x) = γ tmpl f tmpl (x) + γ hist f hist (x) .<label>(3)</label></formula><p>The template score is a linear function of a K-channel feature image φ x : T → R K , obtained from x and defined on a finite grid T ⊂ Z 2 :</p><formula xml:id="formula_3">f tmpl (x; h) = u∈T h[u] T φ x [u] .<label>(4)</label></formula><p>In this, the weight vector (or template) h is another Kchannel image. The histogram score is computed from an M -channel feature image ψ x : H → R M , obtained from x and defined on a (different) finite grid H ⊂ Z 2 :</p><formula xml:id="formula_4">f hist (x; β) = g(ψ x ; β) .<label>(5)</label></formula><p>Unlike the template score, the histogram score is invariant to spatial permutations of its feature image, such that g(ψ) = g(Πψ) for any permutation matrix Π. We adopt a linear function of the (vector-valued) average feature pixel</p><formula xml:id="formula_5">g(ψ; β) = β T 1 |H| u∈H ψ[u] ,<label>(6)</label></formula><p>which can also be interpreted as the average of a scalarvalued score image</p><formula xml:id="formula_6">ζ (β,ψ) [u] = β T ψ[u] g(ψ; β) = 1 |H| u∈H ζ (β,ψ) [u] .<label>(7)</label></formula><p>To enable efficient evaluation of the score function in dense sliding-window search, it is important that both feature transforms commute with translation φ T (x) = T (φ x ). Not only does this mean that feature computation can be shared by overlapping windows, but also that the template score can be computed using fast routines for convolution, and that the histogram score can be obtained using a single integral image. Further acceleration is possible if the histogram weight vector β or feature pixels ψ[u] are sparse.</p><p>The parameters of the overall model are θ = (h, β), since the coefficients γ tmpl and γ hist can be considered implicit in h and β. The training loss that will be optimised to choose parameters is assumed to be a weighted linear combination of per-image losses:</p><formula xml:id="formula_7">L(θ, X T ) = T t=1 w t (x t , p t , θ) .<label>(8)</label></formula><p>Ideally, the per-image loss function should be of the form</p><formula xml:id="formula_8">(x, p, θ) = d(p, arg max q∈S f (T (x, q); θ)) ,<label>(9)</label></formula><p>in which d(p, q) defines the cost of choosing rectangle q when the correct rectangle is p. Although this function is non-convex, structured output learning can be used to optimise a bound on the objective <ref type="bibr" target="#b2">[3]</ref>, and this is the basis of Struck <ref type="bibr" target="#b15">[16]</ref>.  <ref type="figure">Figure 2</ref>: Template-related. In frame t, a training patch represented using HOG features is extracted at the estimated location p t and used to update the denominatord t and the numeratorr t of the modelĥ t in <ref type="bibr" target="#b20">(21)</ref>. In frame t + 1, features for the testing patch φ T (xt+1,pt) are extracted around the location in the previous image p t and convolved withĥ t in (4) to obtain the dense template response. Histogram-related. In frame t, foreground and background regions (relative to the estimated location) are used to update the frequencies of each colour bin ρ t (O) and ρ t (B) in <ref type="bibr" target="#b25">(26)</ref>. These frequencies enable us to compute the updated weights β t . In frame t + 1, a per-pixel score is computed in a search area centred at the position in the previous image, which is then used to compute the dense histogram response efficiently using an integral image <ref type="bibr" target="#b6">(7)</ref>. The final response is obtained with (3) and the new location p t+1 of the target is estimated at its peak. Best viewed in colour.</p><p>(This requires the property that the feature transform commutes with translation.) This approach has achieved strong results in tracking benchmarks <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b7">8]</ref> whilst maintaining high frame-rates. It may at first seem counter-intuitive to consider f hist distinct from f tmpl , when it is, in fact, a special case of f tmpl with h[u] = β for all u. However, a uniform template such as this would not be learnt from circular shifts, since the score that is obtained using a uniform template is invariant to circular shifts. The histogram score may thus be understood to capture an aspect of the object appearance that is lost when considering circular shifts.</p><p>To retain the speed and efficacy of the Correlation Filter without ignoring the information that can be captured by a permutation-invariant histogram score, we propose to learn our model by solving two independent ridge regression problems:</p><formula xml:id="formula_9">h t = arg min h L tmpl (h; X t ) + 1 2 λ tmpl h 2 β t = arg min β L hist (β; X t ) + 1 2 λ hist β 2<label>(10)</label></formula><p>The parameters h can be obtained quickly using the Correlation Filter formulation. While the dimension of β may be less than that of h, it may still be more expensive to solve for, since it cannot be learnt with circular shifts and therefore requires the inversion of a general matrix rather than a circulant matrix. The fast optimisation of the parameters β will be covered later in this section. Finally, we take a convex combination of the two scores, setting γ tmpl = 1 − α and γ hist = α, where α is a parameter chosen on a validation set. We hope that since the parameters of both score functions will be optimised to assign a score of 1 to the object and 0 to other windows, the magnitudes of the scores will be compatible, making a linear combination effective. <ref type="figure">Figure 2</ref> is a visual representation of the overall learning and evaluation procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Online least-squares optimisation</head><p>Two advantages of adopting a least-squares loss and a quadratic regulariser are that the solution can be obtained in closed form and the memory requirements do not grow with the number of examples. If L(θ; X ) is a convex quadratic function of the score f (x; θ) and f (x; θ) is linear in the model parameters θ to preserve convexity, then there exists a matrix A t and a vector b t such that <ref type="bibr" target="#b10">(11)</ref> and these are sufficient to determine the solution θ t = (A t + λI) −1 b t , regardless of the size of X t . If we adopt a recursive definition of the loss function</p><formula xml:id="formula_10">L(θ; X t ) + λ θ 2 = 1 2 θ T (A t + λI)θ + b T t θ + const.</formula><formula xml:id="formula_11">L(θ; X t ) = (1 − η)L(θ; X t−1 ) + η (x t , p t , θ)<label>(12)</label></formula><p>with adaptivity rate η, then we can simply maintain</p><formula xml:id="formula_12">A t = (1 − η)A t−1 + ηA t (13) b t = (1 − η)b t−1 + ηb t in which A t and b t define the per-image loss via (x t , p t , θ) = 1 2 θ T A t θ + θ T b t + const.<label>(14)</label></formula><p>Note that A t denotes the parameters estimated from frames 1 to t, whereas A t denotes the parameters estimated from frame t alone. We will be consistent in this notation. These parameters, which are sufficient to obtain a solution, are generally economical to compute and store if the number of features (the dimension of θ) is small or the matrix is redundant (e.g. sparse, low rank or Toeplitz). This technique for adaptive tracking with Correlation Filters using circulant matrices was pioneered by Bolme et al. <ref type="bibr" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Learning the template score</head><p>Under a least-squares Correlation Filter formulation, the per-image loss is</p><formula xml:id="formula_13">tmpl (x, p, h) = K k=1 h k φ k − y 2<label>(15)</label></formula><p>where h k refers to channel k of multi-channel image h, φ is short for φ T (x,p) , y is the desired response (typically a Gaussian function with maximum value 1 at the origin), and denotes periodic cross-correlation. This corresponds to linear regression from the circular shift of φ by δ pixels to the value y[δ] with a quadratic loss. Usingx to denote the Discrete Fourier Transform F x, the minimiser of the regularised objective tmpl (x, p, h) + λ h 2 is obtained <ref type="bibr" target="#b21">[22]</ref> h </p><formula xml:id="formula_14">[u] = (ŝ[u] + λI) −1r [u]<label>(16)</label></formula><formula xml:id="formula_15">s ij = φ j φ i , r i = y φ i<label>(17)</label></formula><p>or, in the Fourier domain, using * to denote conjugation and for element-wise multiplication,</p><formula xml:id="formula_16">s ij = (φ j ) * φ i ,r ij = (ŷ) * φ i .<label>(18)</label></formula><p>In practice, Hann windowing is applied to the signals to minimise boundary effects during learning. Instead of computing (16), we adopt the approximation found in the DSST code <ref type="bibr" target="#b7">[8]</ref>ĥ</p><formula xml:id="formula_17">[u] = 1/(d[u] + λ) ·r[u] .<label>(19)</label></formula><p>whered[u] = tr(ŝ[u]) or</p><formula xml:id="formula_18">d = K i=1 (φ i ) * φi<label>(20)</label></formula><p>This enables the algorithm to remain fast with a significant number of feature channels, since it is not necessary to factorise a matrix per pixel. The online version update iŝ</p><formula xml:id="formula_19">d t = (1 − η tmpl )d t−1 + η tmpld t (21) r t = (1 − η tmpl )r t−1 + η tmplr t</formula><p>whered andr are obtained according to <ref type="bibr" target="#b19">(20)</ref> and <ref type="formula" target="#formula_0">(18)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Learning the histogram score</head><p>Ideally, the histogram score should be learnt from a set of examples taken from each image, including the correct position as a positive example. Let W denote a set of pairs (q, y) of rectangular windows q and their corresponding regression target y ∈ R, including the positive example (p, 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The per-image loss is then hist</head><formula xml:id="formula_20">(x, p, β) = (q,y)∈W β T u∈H ψ T (x,q) [u] − y 2 .<label>(22)</label></formula><p>For an M -channel feature transform ψ, the solution is obtained by solving an M × M system of equations, which requires O(M 2 ) memory and O(M 3 ) time. If the number of features is large, this is infeasible. While there are iterative alternatives to matrix decomposition, such as co-ordinate descent, conjugate gradient and dual co-ordinate descent, it may still be difficult to achieve high frame-rates with these. We instead propose features of the special form ψ[u] = e k <ref type="bibr">[u]</ref> where e i is a vector that is one at index i and zero everywhere else, then the one-sparse inner product is simply a lookup β T ψ[u] = β k[u] as in the PLT method described in the VOT13 challenge <ref type="bibr" target="#b25">[26]</ref>. The particular type of features that we consider are quantised RGB colours, although a suitable alternative would be Local Binary Patterns. Recall from <ref type="formula" target="#formula_6">(7)</ref> that the histogram score can be considered an average vote. For efficiency, we therefore propose to apply linear regression to each feature pixel independently over object and background regions O and B ⊂ Z 2 using the per-image objective hist (x, p, β) =</p><formula xml:id="formula_21">1 |O| u∈O β T ψ[u] − 1 2 + 1 |B| u∈B β T ψ[u] 2 (23)</formula><p>where ψ is short-hand for ψ T (x,p) . Introducing the onehot assumption, the objective decomposes into independent terms per feature dimension hist (x, p, β) = M j=1</p><formula xml:id="formula_22">N j (O) |O| · (β j − 1) 2 + N j (B) |B| · (β j ) 2<label>(24)</label></formula><p>where N j (A) = |{u ∈ A : k[u] = j}| is the number of pixels in the region A of φ T (x,p) for which feature j is non-zero k[u] = j. The solution of the associated ridge regression problem is  for each feature dimension j = 1, . . . , M , where ρ j (A) = N j (A)/|A| is the proportion of pixels in a region for which feature j is non-zero. This expression has previously been used under probabilistic motivation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b32">33]</ref>. In the online version, the model parameters are updated</p><formula xml:id="formula_23">β j t = ρ j (O) ρ j (O)+ρ j (B)+λ<label>(</label></formula><formula xml:id="formula_24">ρ t (O) = (1 − η hist )ρ t−1 (O) + η hist ρ t (O) ρ t (B) = (1 − η hist )ρ t−1 (B) + η hist ρ t (B) (26)</formula><p>where ρ t (A) is the vector of ρ j t (A) for j = 1, . . . , M .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Search strategy</head><p>When searching for the target's position in a new frame, we consider rectangular windows that vary in translation/scale but not aspect ratio/orientation. Rather than search jointly in translation/scale, we search first in translation and subsequently in scale. We follow Danelljan et al. <ref type="bibr" target="#b7">[8]</ref> and learn a distinct, multi-scale template for scale search using a 1D Correlation Filter. This model's parameters are updated using the same scheme as the template learnt for translation. The histogram score is not suited to scale search because it will often prefer to shrink the target to find a window that is more purely foreground.</p><p>For both translation and scale, we search only in a region around the previous location. We also follow prior works that adopt Correlation Filters for tracking <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b7">8]</ref> in using a Hann window during search as well as for training. Together these can be considered an implicit motion model.</p><p>The size of the translation template is normalised to have a fixed area. This parameter can be tuned to trade tracking quality for speed, as shown in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Evaluation</head><p>We compare Staple to competing methods on two recent and popular benchmarks, VOT14 <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref> and OTB <ref type="bibr" target="#b38">[39]</ref>, and demonstrate state-of-the-art performance. To achieve an up-to-date comparison, we report the results of several recent trackers in addition to the baselines that are part of each benchmark, using the authors' own results to ensure a fair comparison. Therefore, for each evaluation, we can only compare against those methods that provide results for it. To aid in reproducing our experiments, we make the source code of our tracker and our results available on our website: www.robots.ox.ac.uk/˜luca/staple.html.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>#1 (Staple)</head><p>#2 #3 VOT14 winner <ref type="figure">Figure 3</ref>:</p><p>Accuracy-Robustness rank plot for report challenge.</p><p>Better trackers are closer to the top right corner.</p><p>In <ref type="table" target="#tab_3">Table 1</ref>, we report the values of the most important parameters we use. Contrary to standard practice, we do not choose the parameters of the tracker from the testing set, but instead use VOT15 as a validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">VOT14 and VOT15</head><p>The benchmark. VOT14 <ref type="bibr" target="#b23">[24]</ref> compares competing trackers on 25 sequences chosen from a pool of 394 to represent several challenging situations: camera motion, occlusion, illumination, size and motion change. Two performance measures are used. The accuracy of a tracker on a sequence is expressed as the average per-frame overlap between its predicted bounding box r t and the ground truth r GT using the intersection-over-union criterion S t = |rt∩rGT| |rt∪rGT| . The robustness of a tracker is its number of failures over the sequence, with a failure determined to have occurred when S t becomes zero. Since the focus of the benchmark is on short-term tracking, a tracker that fails is automatically reinitialised to the ground truth five frames after the failure.</p><p>Given the nature of the two performance measures, it is crucial to consider them jointly. Considering either in isolation is uninformative, since a tracker that fails frequently will be re-initialised more often and likely achieve higher accuracy, while zero failures can always be achieved by reporting that the object occupies the entire video frame. <ref type="table" target="#tab_5">Table 2</ref> and <ref type="figure">Figure 3</ref>, we used the most recent version of the VOT toolkit available at submission time (commit d3b2b1d). From VOT14 <ref type="bibr" target="#b24">[25]</ref> we only include the top performers: DSST <ref type="bibr" target="#b7">[8]</ref>, SAMF <ref type="bibr" target="#b28">[29]</ref>, KCF <ref type="bibr" target="#b17">[18]</ref>, DGT <ref type="bibr" target="#b5">[6]</ref>, PLT 14 and PLT 13. <ref type="table" target="#tab_5">Table 2</ref> reports the average accuracy and number of failures for each tracker, together with an overall ranking devised from both. <ref type="figure">Figure 3</ref> visualises independent ranks for each metric on two axes. Surprisingly, our simple method significantly outperforms all VOT14 entries, together with many recent   <ref type="bibr" target="#b7">[8]</ref> 0.491 152 DATs <ref type="bibr" target="#b32">[33]</ref> 0.442 124 <ref type="table">Table 3</ref>: Ranked result for the 60 sequences of VOT15.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results. To produce</head><p>trackers published after VOT14. In particular, it surpasses trackers like Correlation Filter-based DSST <ref type="bibr" target="#b7">[8]</ref>, SAMF <ref type="bibr" target="#b28">[29]</ref> and KCF <ref type="bibr" target="#b17">[18]</ref>, colour-based PixelTrack <ref type="bibr" target="#b10">[11]</ref>, DAT, DATs (with scale) <ref type="bibr" target="#b32">[33]</ref> and DGT <ref type="bibr" target="#b5">[6]</ref>, and also more complex and far slower methods like DMA <ref type="bibr" target="#b39">[40]</ref> and SRDCF <ref type="bibr" target="#b8">[9]</ref>, which operates below 10 FPS. It is interesting to observe how Staple performs in comparison to the second-best correlation and colour trackers, SRDCF and DATs: it achieves a 7% improvement in accuracy and 41% improvement in number of failures over SRDCF, and an 11% improvement in accuracy and 13% improvement in number of failures over DATs. Considering the two metrics individually, Staple is by far the best method in terms of accuracy and the fourth for number of failures, after DMA, PLT 13 and PLT 14.</p><p>However, all these trackers perform poorly in terms of accuracy, scoring at least 20% worse than Staple.</p><p>For completeness, we also present results for VOT15 in <ref type="table">Table 3</ref>, comparing Staple against the second-best performer in <ref type="table" target="#tab_5">Table 2</ref> (DATs) and the winner of VOT14 (DSST). Our performance is significantly better than DATs and DSST in terms of both accuracy (respectively +22% and +10%) and robustness to failures (+35% and +47%).</p><p>In this experiment, we have kept the hyper-parameters that were chosen for VOT15. However, this is in accord with convention, since the VOT benchmark has never included a validation set on the assumption that the hyperparameters would be simple enough not to vary significantly between datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">OTB-13</head><p>The benchmark. As with VOT, the idea of OTB-13 <ref type="bibr" target="#b38">[39]</ref> is to evaluate trackers on both accuracy and robustness to failure. Again, prediction accuracy is measured as intersection over union between the tracker's bounding box and ground truth. A success is detected when this value is above a threshold t o . In order not to set a specific value for such a threshold, the area under the curve of success rates at different values of t o is used as a final score.</p><p>Results. Our results for OTB have been obtained using exactly the same code and parameters used for VOT14/15. The only difference is that we are constrained to use onedimensional histograms for the few grayscale sequences present in the benchmark. <ref type="figure">Figure 4</ref> reports the results of OPE (one pass evaluation), SRE (spatial robustness evaluation) and TRE (temporal robustness evaluation). Staple performs significantly better than all the methods reported in <ref type="bibr" target="#b38">[39]</ref>, with an average relative improvement of 23% with respect to the best tracker evaluated in the original benchmark (Struck <ref type="bibr" target="#b15">[16]</ref>). Moreover, our method also outperforms recent trackers published after the benchmark such as MEEM <ref type="bibr" target="#b40">[41]</ref>, DSST <ref type="bibr" target="#b7">[8]</ref>, TGPR <ref type="bibr" target="#b12">[13]</ref>, EBT <ref type="bibr" target="#b37">[38]</ref> and also trackers that make use of deep conv-nets like CNN-SVM <ref type="bibr" target="#b18">[19]</ref> and SO-DLT <ref type="bibr" target="#b36">[37]</ref>, while running at a significantly higher frame rate. The only comparable method in terms of frame-rate is ACT <ref type="bibr" target="#b9">[10]</ref>, which however performs substantially worse in all the evaluations. Since ACT learns a colour template using Correlation Filters, this result shows that the improvement that Staple achieves by combining template and histogram scores cannot be attributed solely to the introduction of colour. On OTB, the only tracker performing better than Staple is the very recent SRDCF <ref type="bibr" target="#b8">[9]</ref>. However, it performs significantly worse on VOT14. Furthermore, it has a reported speed of only 5 FPS, which severely limits its applicability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Efficiency</head><p>With the above reported configuration, our MATLAB prototype runs at approximately 80 frames per second on a machine equipped with an Intel Core i7-4790K @4.0GHz. However, it is possible to achieve higher frame rates with a  relatively small loss in terms of performance, by adjusting the size of the patch from which the models are computed. For example (refer to <ref type="figure" target="#fig_1">Figure 5</ref>), using HOG cells of size 2 × 2 and a fixed area of 50 2 causes only a small increase in the number of failures, yet boosts the speed beyond 100 frames per second. The accuracy follows a similar trend.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Learning rate experiments</head><p>The learning rates η tmpl and η hist , used respectively for the template <ref type="bibr" target="#b20">(21)</ref> and histogram (26) model updates, determine the rate at which to replace old evidence from earlier frames with new evidence from the current frame. The lower the learning rate, the higher the relevance given to model instances learnt in earlier frames. The heatmap of <ref type="figure">Figure 6</ref> illustrates how maximal robustness is achieved at around 0.01 for both η tmpl and η hist . The accuracy follows a similar trend.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Merge factor experiments</head><p>In <ref type="figure" target="#fig_4">Figure 7</ref>, we show how the accuracy of Staple is significantly influenced by the choice of the merge factor α that regulates γ tmpl and γ hist in (3): the best performance is achieved around α = 0.3. The robustness follows a similar trend. <ref type="figure" target="#fig_4">Figure 7</ref> also shows that the strategy of merging  the dense responses of the two ridge regression problems achieves significantly better performance than a mere interpolation of the final estimates would, suggesting that choosing models with compatible (and complementary) dense responses is a winning choice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>By learning their model from circular shifts of positive examples, correlation filters fail to learn a component that is invariant to permutations. This makes them inherently sensitive to shape deformation. We therefore propose a simple combination of template and histogram scores that are learnt independently to preserve real-time operation. The resulting tracker, Staple, outperforms significantly more complex state-of-the-arts trackers in several benchmarks. Given its speed and simplicity, our tracker is a logical choice for applications that require computational effort themselves, and in which robustness to colour, illumination and shape changes is paramount.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>respectively. For a template of m = |T | pixels and K channels, this can be performed in O(Km log m) time and the sufficient statisticsd andr require O(Km) memory.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Number of failures (lower is better) in relation to speed for HOG cells of size 1 × 1, 2 × 2, 4 × 4 and 8 × 8.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :Figure 6 :</head><label>46</label><figDesc>Success plots for OPE (one pass evaluation), TRE (temporal robustness evaluation) and SRE (spatial robustness evaluation) on the OTB-13<ref type="bibr" target="#b38">[39]</ref> benchmark. Number of failures (lower is better) in relation to the learning rates η tmpl and η hist . Black points were obtained experimentally, others were interpolated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Accuracy (higher is better) vs. merge factor α.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>However, the optimisation problem is computationally expensive, limiting the number of features and training examples that can be used. Correlation Filters, by contrast, adopt a simplistic least-squares loss, but are able to learn from a relatively large number of training examples using quite high-dimensional representations by considering circular shifts of the feature image as examples.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Template-related</cell></row><row><cell></cell><cell>Update</cell><cell>,</cell></row><row><cell>HO G tra ini ng</cell><cell></cell><cell></cell><cell>Histogram-related</cell></row><row><cell>co lo ur tr ai ni ng</cell><cell>Update</cell><cell>Desired response y</cell></row><row><cell>TRAINING -frame x t , position p t</cell><cell>Update</cell><cell></cell></row><row><cell></cell><cell cols="2">Correlation</cell></row><row><cell></cell><cell></cell><cell>Template response</cell></row><row><cell>H O G te st in g</cell><cell></cell><cell></cell><cell>p t+1</cell></row><row><cell></cell><cell cols="2">Integral</cell><cell>Final response</cell></row><row><cell>colour</cell><cell cols="2">Image</cell></row><row><cell>testing</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Histogram response</cell></row><row><cell>TESTING -frame x t+1 , position p t</cell><cell>Per-pixel scores</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>The parameters we use for our experiments.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Ranked results for VOT14. First, second and third entries for accuracy, number of failures (over 25 sequences) and overall rank are reported. Lower rank is better.</figDesc><table><row><cell>Tracker</cell><cell>Accuracy</cell><cell># Failures</cell></row><row><cell>Staple (proposed) DSST</cell><cell>0.538</cell><cell>80</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements. This research was supported by</head><p>Apical Imaging, Technicolor, EPSRC, the Leverhulme Trust and the ERC grant ERC-2012-AdG 321162-HELIOS.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Robust Object Tracking with Online Multiple Instance Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Robust Real-Time Visual Tracking using Pixel-Wise Posteriors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bibby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning to localize objects with structured output regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Correlation Filters for Object Alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">N</forename><surname>Boddeti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">V K</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Visual Object Tracking using Adaptive Correlation Filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Bolme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Beveridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Draper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Lui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Robust Deformable and Occluded Object Tracking With Dynamic Graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">12</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Accurate Scale Estimation for Robust Visual Tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Häger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning Spatially Regularized Correlation Filters for Visual Tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Häger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adaptive Color Attributes for Real-Time Visual Tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van De Weijer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">PixelTrack: a fast adaptive algorithm for tracking non-rigid objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Duffner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Zero-Aliasing Correlation Filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISPA</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Transfer Learning Based Visual Tracking with Gaussian Processes Regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hough-based Tracking of Non-Rigid Objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Godec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semi-Supervised On-line Boosting for Robust Tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Struck: Structured Output Tracking with Kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Beyond Hard Negative Mining: Efficient Detector Learning via Block-Circulant Decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">High-Speed Tracking with Kernelized Correlation Filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Online tracking by learning discriminative saliency map with convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.06796</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">MUlti-Store Tracker (MUSTer): a Cognitive Psychology Inspired Approach to Object Tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Prokhorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Tracking-Learning-Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>TPAMI</publisher>
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multi-Channel Correlation Filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Galoogahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Correlation Filters with Limited Boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Galoogahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The Visual Object Tracking VOT2014 challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vojir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pflugfelder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nebehay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cehovin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.01313v2</idno>
		<title level="m">A Novel Performance Evaluation Methodology for Single-Target Trackers</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The Visual Object Tracking VOT2013 challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pflugfelder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cehovin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nebehay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vojir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gatt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCVW</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Visual Tracking Decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Tracking by Sampling Trackers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">A Scale Adaptive Kernel Correlation Filter Tracker with Feature Integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ECCVW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Long-term Correlation Tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">An adaptive color-based particle filter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nummiaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Koller-Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>IVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Color-Based Probabilistic Tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vermaak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gangnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">In Defense of Color-based Model-free Tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Possegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mauthner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">PROST: Parallel Robust Online Simple Tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Santner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Self-paced learning for long-term tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Supančič</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Co-Tracking Using Semi-Supervised Support Vector Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brennan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Transferring Rich Feature Hierarchies for Robust Visual Tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1501.04587</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Ensemble-Based Tracking: Aggregating Crowdsourced Structured Time Series Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Online object tracking: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Single target tracking using adaptive clustered decision trees and dynamic multilevel appearance models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stolkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">MEEM: Robust Tracking via Multiple Experts using Entropy Minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
