<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ScaleNAS: One-Shot Learning of Scale-Aware Representations for Visual Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Pai</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Duke University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Facebook Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Duke University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Facebook Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Yan</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">University of Nevada</orgName>
								<address>
									<settlement>Reno</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Duke University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Chandra</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Facebook Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiran</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Duke University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">ScaleNAS: One-Shot Learning of Scale-Aware Representations for Visual Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Scale variance among different sizes of body parts and objects is a challenging problem for visual recognition tasks. Existing works usually design a dedicated backbone or apply Neural architecture Search(NAS) for each task to tackle this challenge. However, existing works impose significant limitations on the design or search space. To solve these problems, we present ScaleNAS, a one-shot learning method for exploring scale-aware representations. Scale-NAS solves multiple tasks at a time by searching multi-scale feature aggregation. ScaleNAS adopts a flexible search space that allows an arbitrary number of blocks and crossscale feature fusions. To cope with the high search cost incurred by the flexible space, ScaleNAS employs one-shot learning for multi-scale supernet driven by grouped sampling and evolutionary search. Without further retraining, ScaleNet can be directly deployed for different visual recognition tasks with superior performance. We use Scale-NAS to create high-resolution models for two different tasks, ScaleNet-P for human pose estimation and ScaleNet-S for semantic segmentation. ScaleNet-P and ScaleNet-S outperform existing manually crafted and NAS-based methods in both tasks. When applying ScaleNet-P to bottom-up human pose estimation, it surpasses the state-of-the-art High-erHRNet. In particular, ScaleNet-P4 achieves 71.6% AP on COCO test-dev, achieving new state-of-the-art result.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep-learned representation can be generally categorized into low-resolution representation and high-resolution representation. Low-resolution representation is typically used in classification tasks while high-resolution representation is essential for visual recognition tasks such as semantic segmentation and human pose estimation. We focus on the high-resolution representation in this paper. There are three important yet challenging considerations when * equal contributions.  (b,c) Auto-DeepLab and dynamic routing allows feature fusion connection for each neighboring feature map and find the best architectures with single-path and multi-path, respectively. (d) We propose a more flexible feature fusion that allows crossing to remote feature maps to maximize multi-scale aggregation. designing high-resolution representation: 1) scale variance from different sizes of objects and scenes; 2) precise and informative feature maps are critical; 3) different deployment platforms have different model size requirements.</p><p>Challenge of scale variance: take semantic segmentation as an example, the variance of object size induces difficulty for pixel-level dense prediction, and thus scale representation is critical. In human pose estimation, it is challenging to localize human anatomical keypoints when there is a large scale variance in the scene such as tiny and large persons, the large difference in joint distance. We address this challenge by proposing a new multi-scale search space.</p><p>Challenge of high-resolution representation: to design high-resolution representations, earlier efforts recover high-resolution representations from low-resolution outputs, e.g., Hourglass <ref type="bibr" target="#b23">[24]</ref>, SegNet <ref type="bibr" target="#b1">[2]</ref>, U-Net <ref type="bibr" target="#b27">[28]</ref>. Recent works focus on maintaining high-resolution representation through the whole network and aggregating different scale of representation from parallel paths. E.g., HRNet <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref> and its variants use such multi-scale high-resolution networks to achieve state-of-the-art results on human pose estimation. However, multi-scale neural architectures usually have a large design space to explore and are prone to design redundancies. Our study reveals that when different scales of representation have different depths, the performance can be greatly improved.</p><p>Challenge of deriving a wide spectrum of models: previous methods can only derive one architecture at a time.</p><p>To obtain different sizes of model, further retraining is required for each candidate architecture, e.g., it takes O(N ) training time to derive N models. We propose one-shot based searching method to lower the training cost to O(1).</p><p>To address the above challenges, we propose ScaleNAS, a one-shot based searching method to explore scale-aware neural architectures. We tackle the scale variance challenge by proposing multi-scale aggregation search space to explore multi-scale aggregation and network depth for highresolution representation, see <ref type="figure" target="#fig_1">Figure 1</ref>. Under this search space, we propose a one-shot based searching method to discover multiple architectures simultaneously.</p><p>We name these elite architectures ScaleNet, which can be directly deployed without retraining while performing as well as stand-alone models. As demonstrated in Figure 2, ScaleNet outperforms manually crafted and NASbased models on semantic segmentation and human pose estimation. In semantic segmentation, ScaleNet surpasses HRNet, Auto-DeepLab, and dynamic routing by 1.3% -1.7% mIoU on CityScapes dataset with less computation cost. In human pose estimation, our ScaleNet-P4 obtains 71.6% AP on COCO test-dev2017, achieving a new stateof-the-art result on multi-person pose estimation leaderboard. We further study the patterns of ScaleNet by analyzing the trade-offs between convolution blocks and feature fusion for multi-scale neural architecture design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>High-Resolution Neural Architectures. Designing highresolution neural architectures is important yet challenging. For example, Hourglass <ref type="bibr" target="#b23">[24]</ref> uses a high-to-low followed by a low-to-high architecture to achieve high-resolution. SimpleBaseline <ref type="bibr" target="#b30">[31]</ref> uses transposed convolution layers to generate high-resolution representations. To prevent information loss from the encoding-decoding process, HR-Net <ref type="bibr" target="#b29">[30]</ref> proposed to keep high-resolution features at all times and use multi-scale feature fusion at the end of every four residual blocks to merge information from different scales. While this method has proven successful in many vision tasks, such manual design has many redundant operations and is not optimal as we demonstrated in <ref type="figure">Figure 3</ref>. NAS for High-resolution Architectures. Recent works proposed automating the design of neural architectures for semantic segmentation and human pose estimation. For example, Auto-DeepLab and dynamic routing <ref type="bibr" target="#b16">[17]</ref> are proposed to search architectures for semantic segmentation models. PoseNAS <ref type="bibr" target="#b2">[3]</ref>, AutoPose <ref type="bibr" target="#b11">[12]</ref> and PNFS <ref type="bibr" target="#b31">[32]</ref> are proposed to search architectures for human pose estimation.</p><p>However, the above NAS-based methods can only craft architectures for one task at a time. Given different platforms have different deployment constraints, previous works used adjustable factors (e.g., channel width) to scale neural architecture to different sizes <ref type="bibr" target="#b29">[30]</ref>. After scaling, it is required to retrain each of the scaled architecture. Therefore, to get N models, it requires O(N ) training time. Since this procedure is costly and time consuming, we proposed a one-shot based searching method that can derive N models in O(1) time without any retraining. One-shot Neural Architecture Search. One-shot NAS aims at searching a large neural architecture and sharing weights to different sub-networks <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b12">13]</ref>. While these weights have to adapt to different sub-networks, oneshot NAS suffers from low accuracy. Recent works applied various techniques to conquer the low accuracy problem of supernet. For example, BigNAS <ref type="bibr" target="#b33">[34]</ref> transforms the problem of training supernet to training a big singlestage model and applies sandwich rule to guarantee the performance for each sub-network. OFA <ref type="bibr" target="#b4">[5]</ref> proposed to use progressive shrinkage together with knowledge distillation to train a one-shot supernet. However, these methods are mainly designed for single-path neural architecture on relatively simple task (e.g., ImageNet classification). Directly applying existing one-shot training methods to multi-scale architecture yeilds sub-optimal performance. In this paper, we solve this problem by our proposed grouped sampling method to effectively explore a wide spectrum of subnetworks and further search elite architectures with our evolutionary method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">ScaleNAS</head><p>In this section, we first identify the search space problems in existing works by performing a search space exploration. Then we introduce our proposed multi-scale aggregation search space. Based on this search space, we train a one-shot based SuperScaleNet that contains a wide spectrum of architecture candidates. Finally, we employ multiscale topology evolution to derive elite ScaleNet based on a trained SuperScaleNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Search Space Exploration</head><p>Existing works that achieve state-of-the-art results on semantic segmentation and human pose estimation impose limitations on the design space. As shown in <ref type="figure" target="#fig_1">Figure 1</ref>, HRNet supports cross-scale feature fusions, but it uses four residual blocks in every scale of branchs. Such regular design results in redundancy and misses optimization opportunities as the depth for each branch can be altered to improve performance.</p><p>Auto-DeepLab <ref type="bibr" target="#b19">[20]</ref> includes multiple scale options in their search space to search a single-path neural architecture for semantic segmentation. Dynamic routing <ref type="bibr" target="#b16">[17]</ref> reused the search space form Auto-DeepLab to search a multipath neural architecture to achieve improved performance on semantic segmentation. Although Auto-DeepLab and dynamic routing search the connections between different scales of feature maps, the search space for fusion is limited to only neighboring scales. Such limitation restricts the representation ability for feature maps and cross-scale feature fusion can provide better information gathering for each scale of feature maps.</p><p>To illustrate our search space, we compare the proposed search space with existing works in <ref type="figure" target="#fig_1">Figure 1</ref>. Different from existing works, we provide flexible depth (number of residual blocks) for each scale of branch. In addition, we allow feature fusion cross to any other scale of branches. We use this search space to randomly sample neural architectures as ScaleNet-G series ( <ref type="figure">Figure 3</ref>) and train them on Cityscapes. Original HRNet has 108 residual blocks <ref type="bibr" target="#b14">[15]</ref> and 62 feature fusions. Residual block is composed of two 3 × 3 convolutions. Multi-scale fusion includes downsampling and upsampling. For downsampling, we use strided 3 × 3 convolution with stride 2. For upsampling, we use bilinear upsampling followed by a 1 × 1 convolution for aligning the number of channels <ref type="bibr" target="#b29">[30]</ref>. We create ScaleNet-G1 by using the same number of blocks as HRNet while using 12 less feature fusions with our proposed search space, we observe that there are some ScaleNet-G1 models perform better than HRNet while having less number of fusions. Therefore, the feature fusion position of HRNet may not be optimal as shown in <ref type="figure">Figure 3</ref>. Based on ScaleNet-G1, we create ScaleNet-G2 and ScaleNet-G3 by increasing the number of fusions to the expectation of 124 and 198, re-spectively. We notice that more feature fusions comes with a higher mIoU and inevitable comes with higher FLOPs.</p><p>To study the redundancy of number of blocks, we create G4 and G5 by decreasing number of blocks while keeping higher number of fusions. We observe that the mean accuracy of ScaleNet-G4, G5 are still higher than the original HRNet setting. Based on this observation, we envision that we can use neural architecture search to explore the tradeoffs and relationships between blocks and fusion connection. Cityscapes mIoU val <ref type="figure">Figure 3</ref>. Search space exploration. We train HRNet five times and record the mean and variance of their accuracy. HRNet has 108 resblocks (denoted as 'D') and 62 fusions(denoted as 'F'). We randomly sample 5 groups of ScaleNet with different resblocks and fusions based on our search space ( <ref type="figure" target="#fig_1">Figure 1(d)</ref>). 'ED' and 'EF' represents the expectation of blocks and fusions, respectively.</p><formula xml:id="formula_0">ScaleNet-G1(D108 EF50) ScaleNet-G2(D108 EF124) ScaleNet-G3(D108 EF198) ScaleNet-G4(ED95 EF124) ScaleNet-G5(ED95 EF198) HRNet(D108 F62)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Multi-scale Aggregation Search Space</head><p>Our goal is to design multi-scale neural architectures that can be adapted to multiple visual recognition tasks without retraining. Instead of searching a single model at a time, we aim at discovering a wide spectrum models that have different computation cost for different deployment scenarios.</p><p>We employ a stage-based search space design, which is inspired by the state-of-the-art architecture HRNet that can be adapted to multiple tasks. The architecture starts from a stem cell as the first stage and it is composed of two stride-2 3×3 convolutions. There are four stages in our search space. After the first stage, we gradually add one more highto-low branch for the following stages, i.e., stage2, stage3, and stage4 maintains two-resolution, three-resolution, and four-resolution branches respectively.</p><p>Based on the initial experiments shown in <ref type="figure">Figure 3</ref>, we observe that fusion percentages has positive correlation with accuracy. Thus four blocks per branch proposed by HRNet is not always optimal. We introduce two Controlling factors to form our search space:   in individual independent stage module. For simplicity, the depth of each branch is chosen from {d1, d2, d3, d4}. In this paper, we currently set it at {2, 3, 4, 5}</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Fusion percentage (f ).</head><p>Here the fusion percentage is defined as the probability of the out-degree fusion for each feature map. E.g., a feature map with fusion percentage of 100% means this feature map connects to every other scales of fusion in its current depth.</p><p>By relaxing the cross-scale feature fusion and enlarging the branch blocks, we have roughly 7×10 72 different neural network architectures in our search space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training one-shot SuperScaleNet</head><p>In this section, we explain how to design a one-shot training for multi-scale aggregation search space. Recall that cross-scale feature fusion can boost up performance (see <ref type="figure">Figure 3</ref>). Here we propose to jointly train SuperScaleNet together with a teacher model that has sufficient feature fusion connection.  <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b26">27]</ref>, we pretrain the teacher model on Ima-geNet until converge. Then, we initialize SuperScaleNet with the weights from the teacher model. During each training iteration, we sample a sub-network from Super-ScaleNet, pass one batch of training data to both sampled sub-network and teacher model. Next, we calculate the task loss using the true label, and knowledge distillation (KD) loss using the soft label given by teacher model. Finally, we update the supernet based on the combination of both task loss and KD loss. Our training objective is fomulated as follow:</p><p>min Ws archi (L task (P archi , y) + α · M SE(P archi , P t )). <ref type="formula">(1)</ref> Our main goal is to optimize the weights of Super-ScaleNet W s with the combination of true label loss and soft label loss. Here P archi stands for the prediction for each sampled architecture, y is the true label. P t stands for the prediction from teacher model. We use MSE to calculate the loss between sub-network prediction and teacher network prediction. The KD ratio (α) is set to 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Grouped Sampling</head><p>To train supernet, sampling plays a crucial role. Sandwich rule <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref> was proposed to train supernet, where the smallest model, the largest model, and 2 randomly sampled models are trained in every iteration. However, we observe that sandwich rule cannot guarantee to explore a wide spectrum of neural architectures. Therefore, we propose grouped sampling by dividing the whole search space into different sub-groups.  <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>, <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>}), the fusion percentage is selected from {f1, f2, f3} (e.g., {0.2, 0.5, 0.8}). In combination, we have a total of 9 (3×3) sub-groups. Compared with sandwich rule, grouped sampling is more suitable for multi-scale aggregation search space. Empirical justification is detailed in Section 4.3.</p><p>It is worth noting that unlike in OFA <ref type="bibr" target="#b4">[5]</ref> or BigNAS <ref type="bibr" target="#b33">[34]</ref>, where 4 sub-networks are sampled and their gradients are aggregated in each update step, our group sampling only get one sub-network in each iteration. Therefore, the cost of training a SuperScaleNet using group sampling is very low, e.g., equivalent to a standard model training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Multi-scale Architecture Topology Evolution</head><p>To explore sub-network from a trained supernet, existing one-shot based methods use coarse-to-fine selection, predictor-based method, etc. However, these methods are mainly designed for single-path neural architecture. In comparison, our search space includes multiple path in each stage and each stage has different number of scales. Therefore, we propose multi-scale architecture topology evolution, which provides more reasonable and controllable crossover and mutation over candidate architectures.</p><p>As described in Algorithm 1 and <ref type="figure" target="#fig_4">Figure 4</ref>, our topology evolution include the following four phases.</p><p>• Step1 Initialization. We uniformly sample n 0 subnetworks and record their architectures, accuracies, and FLOPs as a set D. n 0 equals to 1,000 in our experiments.</p><p>• Step2 Selection. We select top k models on the Pareto front of cost/accuracy trade-off curve in D as candidate group C. For each subnetwork in C, we do crossover and mutation to obtain next-generation offsprings. k is set to 100 in our experiments.</p><p>• Step3 Crossover. Since different stage module may have different number of branches, our crossover is inner-stage crossover. For each sub-network arch c in C, we allow a probability p c to swap stage module settings with another randomly selected sub-network. There are 8 stage modules (1,4,3 for stage 2, 3, 4, respectively) and p c is set to 0.25. Thus, each subnetwork is expected to have 2 modules been replaced.</p><p>• Step4 Mutation. After crossover, we do random mutation to switch on and off the fusion connections in every stage module with probability p m . p m is set to 0.5. These K offspring models along with their corresponding accuraices and FLOPs are recorded as set M. Then we update D = D ∪ M. We continue to</p><p>Step2 until we have N sub-networks in D. Here we set N as 2,000. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we evaluate ScaleNAS by searching neural architectures for semantic segmentation and human pose estimation. First we train SuperScaleNet on semantic segmentation with Ctiyscapes dataset <ref type="bibr" target="#b9">[10]</ref> and derive ScaleNet-S using ScaleNAS. Then we apply the same searching routine on top-down human pose estimation framework with COCO dataset <ref type="bibr" target="#b17">[18]</ref> to derive ScaleNet-P. In order to evaluate the generalizability of ScaleNet, we apply ScaleNet-P to HigherHRNet framework for bottom-up human pose estimation. Finally, we conduct ablation studies for ScaleNAS. Training setup. To stabilize training, we first train the teacher model with full depths and fusions on ImageNet-1k <ref type="bibr" target="#b10">[11]</ref> dataset. Following the training procedure in <ref type="bibr" target="#b29">[30]</ref>, we train teacher model for 100 epochs. More training details can be found in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Semantic Segmentation</head><p>Cityscapes. The Cityscapes <ref type="bibr" target="#b9">[10]</ref> is a widely used dataset for semantic segmentation tasks, which contains 5,000 high quality pixel-level finely annotated scene images. The dataset is divided into 2975/500/1525 images for training, validation, and testing, respectively. There are 30 classes, and 19 classes among them are used for evaluation. The mean of class-wise intersection over union (mIoU) is adopted as our evaluation metric. Implementation details. We first obtain the teacher model following the same training protocol in <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b29">30]</ref>. The teacher model is trained for 484 epochs with the batch </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Human Pose Estimation</head><p>For human pose estimation, we first search ScaleNet-P on top-down human pose estimation task using COCO <ref type="bibr" target="#b17">[18]</ref>. Then we reuse the searched ScaleNet-Pose on MPII <ref type="bibr" target="#b0">[1]</ref> and bottom-up pose estimation tasks. COCO. We train SuperScaleNet-Pose on COCO train2017 dataset (57K images and 150K person instances) and evaluate it on COCO val2017. To evaluate object keypoints, we use Object Keypoint Similairty (OKS). We break down the performance on different OKS: AP 50 and AP 75 . We also report the performance on different sizes of object. AP M and AP L stands for AP of medium object and large object, respectively. MPII. The MPII Human Pose dataset <ref type="bibr" target="#b0">[1]</ref> consists realworld full-body pose and annotations. There are around 25K images with 40K subjects, where 12K subjects are used for testing and the remaining subjects are used for training. We use the PCKh (head-normalized probability of correct keypoint) score as our evaluation metric, following <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b28">29]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Top-down Methods</head><p>Implementation details. We use the same workflow as semantic segmentation. Following the settings of <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref>, the teacher model and SuperScaleNet-Pose are trained for 210 epochs. More training details can be found in the supplementary material. Top-down results. <ref type="table">Table 2</ref> summarizes the results of topdown methods on COCO val2017 and MPII val, compared with other state-of-the-art methods. Under 256×192 input resolution, our ScaleNet-P2 outperforms manually designed SimpleBaseline <ref type="bibr" target="#b30">[31]</ref>(+3.2%) and NAS based Auto-Pose <ref type="bibr" target="#b11">[12]</ref>(+1.6%) by a large margin. In addition, ScaleNet-P2 is comparable with the strong HRNet <ref type="bibr" target="#b29">[30]</ref> baseline but with only 56% parameters and 55% FLOPs. With 384×288 input resolution, our ScaleNet-P3 achieves 76.3% AP on COCO val2017, outperforming PoseNFS-3 [32] by 3.3% AP with less computation cost. ScaleNet-P3 has the same accuracy as HRNet-W48 but uses only 42% parameters and 43% FLOPs. ScaleNet-P4 obtains 77.0% AP, surpassing its strong HRNet counterpart by 0.7% AP. For MPII, ScaleNet-P1 performs better on every body parts comparing with SimpleBaseline and HRNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Bottom-up Methods</head><p>We plug the elite architectures obtained from top-down human pose estimation into state-of-the-art bottom-up human estimation framework HigherHRNet <ref type="bibr" target="#b8">[9]</ref>. Implementation details. We adopt the standard training procedure on COCO train2017 as in <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b8">9]</ref> and report results on COCO val2017 and test-dev2017. The models are trained for 300 epochs. More training details can be found in the supplementary material.  and ScaleNet-P1 outperform their counterparts by 0.7% AP and 0.5% AP on COCO val2017 ,respectively. In particular, our ScaleNet-P4 obtains 71.6% AP on COCO test-dev2017 without using refinement or other post-processing techniques, achieving a new state-of-the-art result on multiperson pose estimation leaderboard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>We perform ablation study on each proposed technique. All results are conducted with SuperScaleNet-Seg-W32 on Cityscapes. For simplicity, we denote SuperScaleNet-Seg-W32 as supernet in this part. Impact of sampling. To study the impact of sampling technique, we train two supernets: one is based on our proposed method (Section 3.4), the other is based on state-of-theart sampling method -sandwich rule <ref type="bibr" target="#b33">[34]</ref>. We derive the Pareto front from these two supernets based on our proposed evolutionary search. In <ref type="figure" target="#fig_10">Figure 6</ref>  which has a wide spectrum of architectures. Impact of topology evolution. We demonstrate the performance contribution from searching method by comparing with random search. We use random sampling to sample the same amount of architectures as our evolutionary method and plot the Pareto front. As shown in <ref type="figure">Figure 7</ref>, under the same searching budget, our multi-scale topology evolution consistently achieves better performance on Pareto front, thanks to our inner-stage crossover and mutation techniques. Impact of knowledge distillation. We further study whether knowledge distillation (KD) plays an important role in accuracy gain. Based on the same training procedure in Section 4.1, we train ScaleNet-S1 from ImageNet pretrained weights (stand-alone) with and without KD. We use MSE loss with KD ratio 1 and the teacher model pretrained on Cityscapes. As shown in <ref type="table">Table 4</ref>, the accuracy of the stand-alone training is only slightly lower than directly taken from supernet. It suggests that KD is beneficial but not a dominant factor in the final accuracy.  <ref type="figure">Figure 7</ref>. Ablation study of topology evolution. Dynamic routing is at around the Pareto curve of random sampling. The Pareto front of grouped sampling consistently higher than random sampling and dynamic routing. <ref type="table">Table 4</ref>. Ablation study of knowledge distillation (KD). Comparison with stand-alone training with and without KD. The performance mIoU(%) is obtained on Cityscapes val.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>from stand-alone stand-alone supernet w/o KD w/ KD ScaleNet-S1 80.5 80.2 80.4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(b) Part of ScaleNAS-P1</head><p>(a) Part of ScaleNAS-S1 <ref type="figure">Figure 8</ref>. Architecture demonstration for ScaleNet-S1 and ScaleNet-P1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>Crafted architectures. We demonstrate the crafted architectures for both semantic segmentation and human pose estimation in <ref type="figure">Figure 8</ref>. We show the first module for each stage (full model description is in supplementary material). We observe that both architectures have various cross-scale feature fusion, which is important for handling large scale variance in these tasks. In addition, we observe later stages rely on heavier feature fusions while earlier stages have less feature fusions.</p><p>Elite architecture pattern analysis. Different hardware platforms have different computation constraints. We analyze the deployability of elite architectures with different computation cost. We record FLOPs, the number of fusions, <ref type="figure">Figure 9</ref>. The network pattern of elite sub-networks. We show the relationship between number of blocks and fusions for the elite sub-networks. and the number of blocks of Pareto front from the 2000 elite ScaleNets collected by our evolutionary method.</p><p>In <ref type="figure">Figure 9</ref>(a)(c), we observe larger elite models have more fusions than blocks. In addition, the number of fusions increases faster than the number of blocks. To further analyze the relationship between number of fusions and number of blocks, we demonstrate block-fusion ratio in <ref type="figure">Figure 9</ref>(b)(d).We observe that for the largest elite model, it requires two times more fusions than blocks. However, for small elite models, the number of fusions is only half of the number of blocks. This interesting observation provides important future design insights: 1) For edge devices, we should invest more computation cost on blocks than fusions. 2) To design larger models, it is preferable to invest computation cost on fusions over blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We present ScaleNAS, a one-shot learning method for scale-aware representations. To the best of our knowledge, ScaleNAS is the first of its kind one-shot NAS method that considers scale variance for multiple vision recognition tasks. To efficiently search a wide spectrum of neural architectures for different vision tasks, we rest upon the following key ideas: (i) A novel multi-scale feature aggregation search space that includes cross scale feature fusions and flexible depths. (ii) One-shot based training method driven by an efficient sampling technique to train multi-scale supernet. (iii) Multi-scale architecture topology evolution to efficiently search elite neural architectures. All the above novel ideas coherently make ScaleNAS outperform existing hand crafted and NAS-based methods on semantic segmentation and human pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Supplementary Material</head><p>This supplementary material provides more details of training SuperScaleNet on each task and also the extension of object detection task. For reproducibility, we provide full searching and training codes, as well as pretrained models. Please refer to README.md to see detailed instructions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Details of Search Space Exploration</head><p>In our main submission, we conduct initial search space exploration on semantic segmentation using Cityscapes. All models are trained from scratch for 48 epochs. Data augmentation strategies and other training protocols are the same as the teacher training part of Section 7.3 in this supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Details of Training Teacher Model on ImageNet</head><p>Following the instructions in <ref type="bibr" target="#b29">[30]</ref>, we use stochastic gradient descent (SGD) as the optimizer with 0.9 nesterov momentum and 0.0001 weight decay. The model is trained for 100 epochs with batch size 768. The initial learning rate is set to 0.3 and is reduced by 10 at epoch 30, 60, and 90. It takes ∼30 hours to train on 16 TESLA V100 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Details of Training SuperScaleNet on Semantic Segmentation</head><p>Teacher training: For a fair comparison, we follow the same training protocols in <ref type="bibr" target="#b29">[30]</ref>. We adopt the SGD optimizer with the momentum of 0.9 and the weight decay of 0.0005. The model is trained for 484 epochs with the batch size of 24 on 8 TESLA V100 GPUs. The initial learning rate is set to 0.01 and the cosine annual decay <ref type="bibr" target="#b21">[22]</ref> is used for decaying the learning rate. For data preprocessing, the training and validation image size is 512×1024 and 1024×2048, respectively. For data augmentation strategies, we use random cropping (from 1024×2048 to 512×1024), random scaling (between [0.5, 2]), and random horizontal flipping.</p><p>SuperScaleNet-Seg training: We follow the same training protocols as teacher training except the initial learning rate is set to 0.001. This is because the SuperScaleNet-Seg is initialized from the well-trained teacher, we only need to fine-tune each sub-network using a small learning rate.</p><p>It takes ∼40(60) hours to obtain SuperScaleNet-Seg-W32(W48), including the teacher training. With only twice the training cost as stand-alone model training, we can obtain a series of segmentation models in a wide spectrum of FLOPs without additional retraining. We further use multiscale topology evolution to explore elite ScaleNet-Seg.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.">Details of Training SuperScaleNet on Top-Down Human Pose Estimation</head><p>Teacher training: Following the training protocols of HRNet <ref type="bibr" target="#b29">[30]</ref>, we train the model for 210 epochs using the Adam optimizer <ref type="bibr" target="#b15">[16]</ref> with step learning rate decay <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b29">30]</ref>. The initial learning rate is set as 0.001, and is dropped to 0.0001 and 0.00001 at the 170th and 200th epochs, respectively. For data preprocessing, we extend the human detection box in height or width to a fixed aspect ratio -height : width = 4 : 3, and then crop the box from the image, which is resized to a fixed size, 256 × 192 or 384 × 288. For data augmentation strategies, we use random rotation ([-45 • , 45 • ]), random scale ([0.65, 1.35]), and flipping.  <ref type="bibr" target="#b15">[16]</ref>.</p><p>The models are trained on 8 TESLA V100 GPUs. It takes ∼50(75) hours to train SuperScaleNet-Pose-W32(W48), including the teacher training. After topology evolution, we further fine-tune the ScaleNet-P for 20 epochs (around 3 hours) for better performance.</p><p>MPII: We use the same data augmentation and training strategy for MPII, except that the input size is cropped to 256 × 256 for a fair comparison with SimpleBaseline <ref type="bibr" target="#b30">[31]</ref> and HRNet <ref type="bibr" target="#b28">[29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5.">Details of Training found architectures on Bottom-Up Human Pose Estimation</head><p>We train ScaleNet-P series on bottom-up human pose estimation framework, HigherHRNet <ref type="bibr" target="#b8">[9]</ref>. For a fair comparison, we use the exact same training routine as HigherHR-Net. Specifically, we train our model for 300 epochs using the Adam optimizer <ref type="bibr" target="#b15">[16]</ref> with step learning rate decay. The base learning rate is set to 0.001, and dropped to 1e-4 and 1e-5 at the 200th and 260th epochs, respectively. For data augmentation strategies, we use random rotation ([-30 • , 30 • ]), random scale ([0.75, 1.5]), random translation ([-40, 40]), random crop (512 × 512), and random flip. We use the top-down SuperScaleNet-Pose to initialize weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.6.">Results on Object Detection</head><p>We directly apply the ScaleNet-S2, which is obtained from semantic segmentation, to object detection task. We plug in the ScaleNet-S2 to two classic object detection frameworks, Faster R-CNN <ref type="bibr" target="#b26">[27]</ref> and Mask R-CNN <ref type="bibr" target="#b13">[14]</ref> as shown in <ref type="table" target="#tab_4">Table 5</ref>. We use the whole COCO trainval135 as training set and validate on COCO minival. For both Faster R-CNN and Mask R-CNN, the input images are resized to a short side of 800 pixels and a long side not exceeding 1333 pixels. We use SGD as optimizer with 0.9 momentum. For a fair comparison, all our models are trained for 12 epochs, known as 1× scheduler. We use 8 TESLA V100 GPUs for training with 16 global batch size. The initial learning rate is 0.02 and is divided by 10 at 8 and 11 epochs.</p><p>In the Faster R-CNN framework, our networks perform better than HRNet-w32 with less parameters and computation cost. Our ScaleNet-S2 is especially effective for small objects (1.1% improvement for AP S ). The reason is that our ScaleNet-S2 learns more high-resolution features which are beneficial for small objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.7.">Details of Found Architecture</head><p>Here we provide the architecture structures of our crafted architecture ScaleNet-P1 for human pose estimation and ScaleNet-S1 for semantic segmentation, see <ref type="figure" target="#fig_1">Figure 10</ref> and <ref type="figure" target="#fig_1">Figure 11</ref>, respectively. The interesting observation is that both architectures have more multi-scale feature fusion at later stages while with a relatively simple network structure at the early stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.8.">NAS Reproducibility Checklist</head><p>To ensure a fair comparison, we follow the guidelines provided by NAS reproducibility checklist <ref type="bibr" target="#b18">[19]</ref> and compare ScaleNAS with other NAS methods from different perspectives.</p><p>For all NAS methods you compare, did you use exactly the same NAS benchmark, including the same dataset (with the same training-test split), search space and code for training the architectures and hyperparameters for that code?</p><p>-For comparing with other NAS methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b16">17]</ref>  <ref type="figure" target="#fig_1">Figure 10</ref>. The full model of ScaleNet-S1.</p><formula xml:id="formula_1">Stage4_module1 Stage4_module2 Stage4_module3 Stage 2 Stage3_module2 Stage3_module1</formula><p>Stage3_module3 Stage3_module4 <ref type="figure" target="#fig_1">Figure 11</ref>. The full model of ScaleNet-P1. and collecting our results. All the package dependencies are described in requirements.txt of our attached codes. For hardware, we only trained and tested on NVIDIA TESLA V100 GPU.</p><p>Did you run ablation studies?</p><p>-Yes, we performed ablation studies for sampling method, searching method, and knowledge distillation. Detailed results can be found in Section 4.3 of our main submission.</p><p>Did you use the same evaluation protocol for the methods being compared?</p><p>-Yes, for evaluating on top-down human pose estimation, we followed the same evaluation protocol as HRNet paper. For comparing with bottomup human pose estimation, we used the same evaluation protocol as HigherHRNet. For comparing on semantic segmentation, we used the same evaluation protocol as HRNet as well.</p><p>Did you compare to random search?</p><p>-Yes, we compared our searching method with random search at Section 4.3 and <ref type="figure">Figure 7</ref> of our main submission. The results show that the Pareto front of our found architectures performs better than random search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Did you perform multiple runs of your experiments?</head><p>-Yes, during search space exploration, we trained the original HRNet 5 times and reported the mean and standard deviation of mIoU on Cityscapes. For SuperScaleNet, we only trained once. We expect further tuning and training should achieve better results. All of our experiment is highly reproducible and source code is provided.</p><p>Did you use tabular or surrogate benchmarks for indepth evaluations?</p><p>-No, existing surrogate benchmarks such as NASBench-101, NAS-Bench-201, NAS-Bench-1Shot1 are not application to our search space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Did you report how you tuned hyperparameters, and</head><p>what time and resources this required?</p><p>-Yes, among all the hyperparameters, we only tuned the learning rate for training Super-ScaleNet on semantic segmentation tasks. Since SuperScaleNet-Seg is initialized from the welltrained teacher, the original learning rate (0.01) used in HRNet is not suitable for our case. We tried three different learning rates, 0.005, 0.002, and 0.001. Each tuning cost 300 GPU hours, with a total cost of around 900 GPU hours on TESLA V100. We found that the learning rate of 0.001 shows the best performance, thus, we use 0.001 for semantic segmentation tasks.</p><p>Did you report all the details of your experimental setup?</p><p>-Yes, we comprehensively reported all of the configurations, including hyperparameter settings, training protocol in main submission and supplementary material.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Search space comparison. (a) HRNet uses fully connected multi-scale feature fusion at the end of every four blocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>The trade-off between computation cost (GFLOPs) and model performance. Left: semantic segmentation mIoU on Cityscapes val. Right: human pose estimation AP on COCO val. Our ScaleNet outperforms HRNet and NAS-based methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Workflow of ScaleNAS. ScaleNAS train a SuperScaleNet in the proposed search space and uses our proposed evolutionary method to explore elite architectures based on the trained SuperScaleNet. (a) Before training starts, we initialize SuperScaleNet by the teacher model. (b) During each iteration, we sample ScaleNet from the SuperScaleNet. (c) We use the task loss from true labels and the knowledge distillation (KD) loss from soft labels given by teacher to update SuperScaleNet.Feature mapsConv. operationSearchable Depth for each branch in each stage</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Search space overview of ScaleNAS. Our search space inherit the spirit of HRNet that has few stages. ScaleNAS adopts a flexible search space with arbitrary number of blocks and cross-scale feature fusions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4</head><label>4</label><figDesc>depicts the workflow of SuperScaleNet training. First, according to the search space proposed in Figure 5, we build the teacher model with full number of blocks and fully activated feature fusions. Visual recognition tasks usually rely on ImageNet pretrained to stabilize training</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Algorithm 1 3 :</head><label>13</label><figDesc>Multi-Scale Architecture Evolution Input: Search space S, Trained SuperScaleNet, initial population size n 0 , number of offspring k, crossover probability p c , mutation probability p m , number of final elite architectures N . Output: N Elite ScaleNet 1: Sample n 0 sub-networks to obtain initial population D = {arch d , d = 1, 2, 3, . . . , n 0 } 2: while len(D) &lt; N do Select top k models on the Pareto front as candidate group C = {arch c , c = 1, 2, 3, . . . , k} 4: for every sub-networks arch c in C do 5: crossover and mutation under probability p c and p m sequentially, generate offspring 6: Gather k offspring models as set M k = {arch m , m = 1, 2, . . . , k} 7: update D = D ∪ M</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Bottom-up results. Tabel 3 reports the results of bottomup methods on COCO val2017 and test-dev2017. By utilizing our ScaleNet-P as feature extractor, we boost the performance of bottom-up pose estimation. Our ScaleNet-P4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>, we show elite architectures and their corresponding accuracy from 220G to 320G. The Pareto front results suggest grouped sampling perform the best for multi-scale aggregation search space</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6 .</head><label>6</label><figDesc>Ablation study of sampling techniques. The Pareto front of grouped sampling steadily higher than the Pareto front achieved by sandwich rule.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Given a depth choice of {d1, d2, d3, d4}, we group the depth choice to {[d1, d2], [d2, d3], [d3, d4]} (e.g., {[2, 3],</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Semantic segmentation results on Cityscapes val (single scale and no flipping). The GFLOPs is calculated on the input size 1024 × 2048. 'D-X' equals to 'Dilated-X'. For existing segmentation NAS works, the total cost grows linear to the number of deployment scenarios N , while the cost of our ScaleNAS remains constant.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="2">#Params GFLOPs</cell><cell cols="4">mIoU Searching Cost Training Cost Total Cost(N =40) (%) (GPU hours) (GPU hours) (GPU hours)</cell></row><row><cell>DeepLabv3 [7]</cell><cell>D-ResNet-101</cell><cell cols="3">58.0M 1778.73 78.5</cell><cell>-</cell><cell>50N</cell><cell>-</cell></row><row><cell>DeepLabv3+ [8]</cell><cell>D-Xception-71</cell><cell cols="3">43.5M 1444.63 79.6</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PSPNet [35]</cell><cell>D-ResNet-101</cell><cell cols="3">65.9M 2017.63 79.7</cell><cell>-</cell><cell>100N</cell><cell>-</cell></row><row><cell>Auto-DeepLab [20]</cell><cell>Searched-F20-ASPP</cell><cell>-</cell><cell>333.3</cell><cell>79.7</cell><cell>72N</cell><cell>250N</cell><cell>12.9k</cell></row><row><cell>Dynamic Routing [17]</cell><cell>Layer33-PSP</cell><cell>-</cell><cell>270.0</cell><cell>79.7</cell><cell>180N</cell><cell>0</cell><cell>7.2k</cell></row><row><cell>ScaleNAS (Ours)</cell><cell>ScaleNet-S1</cell><cell>25.3M</cell><cell>265.5</cell><cell>80.5</cell><cell>200</cell><cell>400</cell><cell>600</cell></row><row><cell>ScaleNAS (Ours)</cell><cell>ScaleNet-S2</cell><cell>28.5M</cell><cell>309.5</cell><cell>81.1</cell><cell>200</cell><cell>400</cell><cell>600</cell></row><row><cell>Auto-DeepLab [20]</cell><cell>Searched-F48-ASPP</cell><cell>-</cell><cell>695.0</cell><cell>80.3</cell><cell>72N</cell><cell>350N</cell><cell>16.9k</cell></row><row><cell>HRNet [30]</cell><cell>HRNet-W48</cell><cell>65.8M</cell><cell>696.2</cell><cell>81.1</cell><cell>-</cell><cell>260N</cell><cell>-</cell></row><row><cell>ScaleNAS (Ours)</cell><cell>ScaleNet-S4</cell><cell>67.5M</cell><cell>673.6</cell><cell>82.0</cell><cell>300</cell><cell>600</cell><cell>900</cell></row><row><cell cols="4">size of 24. After the teacher model is trained, we use</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">our grouped sampling technique (Section 3.4) to further</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">fine tune the SuperScaleNet-Seg to support smaller sub-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">networks. More training details can be found in the sup-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>plementary material.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Segmentation results. Table 1 reports the comparison be- tween ScaleNAS and existing manual/NAS methods on se- mantic segmentation. Comparing with NAS (Auto-Deeplab and dynamic routing), ScaleNAS is much more efficient for multiple deployment scenarios. E.g., when there are 40 de- ployment scenarios, the total cost of ScaleNAS s 12× fewer than dynamic routing and 19× fewer than Auto-Deeplab, re- spectively. Without additional retraining, ScaleNet-S1 out- performs the dynamic routing Layer33-PSP by a 0.8% mar- gin under the similar cost. When comparing with manually designed HRNet-W48 or Searched-F48-ASPP, ScaleNet-S4 improves the mIoU to 82.0%, surpassing HRNet and Auto- Deeplab by 0.9% and 1.7% respectively.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .Table 3 .</head><label>23</label><figDesc>Top-down human pose estimation results.Comparison on COCO val2017. AutoPose* reports results without ImageNet pretraining.Comparison on MPII val. The GFLOPs is calculated on the input size 256 × 256. We reuse the searched ScaleNet-P and apply it to MPII dataset. Bottom-up human pose estimation results.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="3">Input size #Params GFLOPs</cell><cell>AP</cell><cell>AP50</cell><cell>AP75</cell><cell>APM</cell><cell>APL</cell><cell>AR</cell></row><row><cell>SimpleBaseline [31]</cell><cell>ResNet-152</cell><cell></cell><cell>68.6M</cell><cell>15.7</cell><cell>72.0</cell><cell>89.3</cell><cell>79.8</cell><cell>68.7</cell><cell>78.9</cell><cell>77.8</cell></row><row><cell>AutoPose [12] HRNet [30]</cell><cell>AutoPose* HRNet-W48</cell><cell>256×192</cell><cell>-63.6M</cell><cell>10.65 14.6</cell><cell>73.6 75.1</cell><cell>90.6 90.6</cell><cell>80.1 82.2</cell><cell>69.8 71.5</cell><cell>79.7 81.8</cell><cell>78.1 80.4</cell></row><row><cell>ScaleNAS (Ours)</cell><cell>ScaleNet-P2</cell><cell></cell><cell>35.6M</cell><cell>8.0</cell><cell>75.2</cell><cell>90.4</cell><cell>82.4</cell><cell>71.6</cell><cell>81.9</cell><cell>80.4</cell></row><row><cell>PNFS [32]</cell><cell>PoseNFS-3</cell><cell></cell><cell>-</cell><cell>14.8</cell><cell>73.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell></row><row><cell>SimpleBaseline [31]</cell><cell>ResNet-152</cell><cell></cell><cell>68.6M</cell><cell>35.6</cell><cell>74.3</cell><cell>89.6</cell><cell>81.1</cell><cell>70.5</cell><cell>79.7</cell><cell>79.7</cell></row><row><cell>HRNet [30]</cell><cell>HRNet-W48</cell><cell>384×288</cell><cell>63.6M</cell><cell>32.9</cell><cell>76.3</cell><cell>90.8</cell><cell>82.9</cell><cell>72.3</cell><cell>83.4</cell><cell>81.2</cell></row><row><cell>ScaleNAS (Ours)</cell><cell>ScaleNet-P3</cell><cell></cell><cell>26.2M</cell><cell>14.3</cell><cell>76.3</cell><cell>90.7</cell><cell>82.9</cell><cell>72.5</cell><cell>83.3</cell><cell>81.3</cell></row><row><cell>ScaleNAS (Ours)</cell><cell>ScaleNet-P4</cell><cell></cell><cell>64.3M</cell><cell>32.6</cell><cell>77.0</cell><cell>90.9</cell><cell>83.6</cell><cell>73.0</cell><cell>84.2</cell><cell>81.8</cell></row><row><cell>Method</cell><cell>Backbone</cell><cell>#Params</cell><cell>GFLOPs</cell><cell>mean</cell><cell cols="4">Head Shoulder Elbow Wrist</cell><cell>Hip</cell><cell>Knee</cell></row><row><cell>SimpleBaseline [31]</cell><cell>ResNet-152</cell><cell>68.6M</cell><cell>20.9</cell><cell>88.5</cell><cell>96.4</cell><cell>95.3</cell><cell>89.0</cell><cell>83.2</cell><cell>88.4</cell><cell>84.0</cell></row><row><cell>HRNet [30]</cell><cell>HRNet-W32</cell><cell>28.5M</cell><cell>9.5</cell><cell>90.3</cell><cell>97.1</cell><cell>95.9</cell><cell>90.3</cell><cell>86.4</cell><cell>89.1</cell><cell>87.1</cell></row><row><cell>ScaleNAS (Ours)</cell><cell>ScaleNet-P1</cell><cell>28.5M</cell><cell>9.3</cell><cell>91.0</cell><cell>97.3</cell><cell>96.5</cell><cell>91.5</cell><cell>87.3</cell><cell>90.0</cell><cell>87.5</cell></row><row><cell cols="4">Comparison on COCO val2017 w/o multi-scale test.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Backbone</cell><cell cols="2">Input #Params GFLOPs AP size</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>HRNet-W32</cell><cell>512 28.6M</cell><cell>47.9 67.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>HigherHRNet [9]</cell><cell cols="2">ScaleNet-P1(Ours) 512 28.6M HRNet-W48 640 63.8M</cell><cell>46.9 67.8 154.3 69.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">ScaleNet-P4(Ours) 640 64.4M</cell><cell>141.5 70.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Comparison on COCO test-dev 2017 w/ multi-scale test.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Backbone</cell><cell cols="2">Input #Params GFLOPs AP size</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Hourglass [23]</cell><cell>Hourglass</cell><cell cols="2">512 277.8M 206.9 63.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Hourglass w/ refine [23]</cell><cell>Hourglass</cell><cell cols="2">512 277.8M 206.9 65.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PersonLab [25]</cell><cell>ResNet-152</cell><cell>1401 68.7M</cell><cell>405.5 68.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>HigherHRNet [9]</cell><cell>HRNet-W48</cell><cell>640 63.8M</cell><cell>154.3 70.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ScaleNAS (ours)</cell><cell cols="2">ScaleNet-P4(Ours) 640 64.4M</cell><cell>141.5 71.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Object detection results on COCO minival in Faster R-CNN<ref type="bibr" target="#b26">[27]</ref> and Mask R-CNN<ref type="bibr" target="#b13">[14]</ref>. LS denotes learning rate scheduler. GFLOPs is calculated on the input size 800×1280. HRNet-w32* denotes our reimplementation. AP M AP L AP AP S AP M AP L We follow the same training protocols in teacher training. We do not reduce the learning rate as in SuperScaleNet-Seg training because the Adam optimizer can adjust the learning rate adaptively</figDesc><table><row><cell>Backbone</cell><cell cols="4">LS Params(M) GFLOPs AP AP S Faster R-CNN [27]</cell><cell>box</cell><cell></cell><cell>mask</cell></row><row><cell cols="2">HRNet-w32* 1×</cell><cell>47.2</cell><cell>285.4</cell><cell cols="2">39.8 22.8 43.7 51.0</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>/</cell></row><row><cell cols="2">ScaleNet-S2 1×</cell><cell>46.3</cell><cell>271.3</cell><cell cols="2">40.1 23.9 44.2 51.7</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>/</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Mask R-CNN [14]</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">HRNet-w32* 1×</cell><cell>49.9</cell><cell>353.9</cell><cell cols="5">40.8 23.8 44.5 52.4 36.4 19.5 39.7 48.9</cell></row><row><cell cols="2">ScaleNet-S2 1×</cell><cell>49.0</cell><cell>339.8</cell><cell cols="5">40.9 24.4 44.6 52.5 36.5 19.7 40.0 49.0</cell></row><row><cell cols="3">SuperScaleNet-Pose training:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>, we used the same dataset including train-test split. Our search space is essentially different from previous works. To train our architecture, we used the open-sourced repository from HR-Net and HigherHRNet with the only change of learning rate. Did you control for confounding factors (different hardware, versions of DL libraries, different runtimes for the different methods)? -Yes, for the version of DL libraries, we used Pytorch-1.1 for conducting all our experiments</figDesc><table><row><cell>Stage 2</cell><cell>Stage3_module1</cell><cell>Stage3_module2</cell><cell>Stage3_module3</cell><cell>Stage3_module4</cell><cell>Stage4_module1</cell><cell>Stage4_module2</cell><cell>Stage4_module3</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3686" to="3693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pose-native network architecture search for multi-person human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingyu</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="592" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Understanding and simplifying one-shot architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="550" to="559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Once for all: Train one network and specialize it for efficient deployment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhekai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00332</idno>
		<title level="m">Proxylessnas: Direct neural architecture search on target task and hardware</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Higherhrnet: Scale-aware representation learning for bottom-up human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Autopose: Searching multi-scale branch aggregation for pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wuyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.07018</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Single path oneshot neural architecture search with uniform sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyuan</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zechun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="544" to="560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Piotr Dollár, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning dynamic routing for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Best practices for scientific research on neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Lindauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.02453</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Autodeeplab: Hierarchical neural architecture search for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.09055</idno>
		<title level="m">Darts: Differentiable architecture search</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Associative embedding: End-to-end learning for joint detection and grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Personlab: Person pose estimation and instance segmentation with a bottom-up, part-based, geometric embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="269" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Efficient neural architecture search via parameter sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Melody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03268</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unet: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaorui</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wankou</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Cui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.07068</idno>
		<title level="m">Pose neural fabrics search</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Universally slimmable networks and improved training techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1803" to="1811" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.11142</idno>
		<title level="m">Ruoming Pang, and Quoc Le. Bignas: Scaling up neural architecture search with big single-stage models</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Psanet: Point-wise spatial attention network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="267" to="283" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
