<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient Image Retrieval via Decoupling Diffusion into Online and Offline Processing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Tokyo</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">National Institute of Informatics</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryota</forename><surname>Hinami</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Tokyo</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">National Institute of Informatics</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Matsui</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Tokyo</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Ly</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">National Institute of Informatics</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Southern California</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shin&amp;apos;ichi Satoh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Tokyo</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">National Institute of Informatics</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Efficient Image Retrieval via Decoupling Diffusion into Online and Offline Processing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Diffusion is commonly used as a ranking or re-ranking method in retrieval tasks to achieve higher retrieval performance, and has attracted lots of attention in recent years. A downside to diffusion is that it performs slowly in comparison to the naive k-NN search, which causes a non-trivial online computational cost on large datasets. To overcome this weakness, we propose a novel diffusion technique in this paper. In our work, instead of applying diffusion to the query, we precompute the diffusion results of each element in the database, making the online search a simple linear combination on top of the k-NN search process. Our proposed method becomes 10∼ times faster in terms of online search speed. Moreover, we propose to use late truncation instead of early truncation in previous works to achieve better retrieval performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>The success of deep neural networks on feature representation has led it to become a standard technique in image retrieval. Models pre-trained on popular datasets such as ImageNet <ref type="bibr" target="#b6">(Deng et al. 2009</ref>), Landmarks <ref type="bibr" target="#b0">(Babenko et al. 2014)</ref> etc. can be used to extract features of images. Particularly, convolutional layers have been proved to be most beneficial at retrieving images <ref type="bibr" target="#b0">(Babenko et al. 2014;</ref><ref type="bibr" target="#b22">Radenović, Tolias, and Chum 2016;</ref><ref type="bibr" target="#b10">Gordo et al. 2016;</ref><ref type="bibr" target="#b23">Razavian et al. 2016)</ref>. Nearest neighbor search is then used on the feature vectors to find the most similar images to a query.</p><p>Datasets are usually scraped from the internet, resulting in images with the same object/landmark shown in a variety of angles, lighting, and other conditions. The diversity often results in a manifolds in the feature space that are not conducive to ranking using a distance-based metric. Unlike the rigid distance metric used in k-NN search, diffusion <ref type="bibr" target="#b27">(Zhou et al. 2004b;</ref><ref type="bibr" target="#b26">2004a;</ref><ref type="bibr" target="#b8">Donoser and Bischof 2013;</ref><ref type="bibr" target="#b11">Grady 2006</ref>) exploits the intrinsic manifold structure of data based on a neighborhood graph. Such a graph consists of nodes and edges, where each node represent a feature vector from the database, with the edges connect each node to its neighbors with corresponding weights proportional to the pairwise affinities between nodes. Using this graph, diffusion performs a restartable random walk given a query Copyright c 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. as the initial state. The final state of random walk can be viewed as ranking scores showing the similarities of each image in database to the query. To obtain the convergence of the final state, there are two main approaches: running iterative random walk or computing the convergence state by the closed-form theorem proposed in <ref type="bibr" target="#b27">(Zhou et al. 2004b)</ref>. Diffusion has demonstrated its potential in improving retrieval performance <ref type="bibr" target="#b8">(Donoser and Bischof 2013;</ref><ref type="bibr" target="#b12">Iscen et al. 2017;</ref><ref type="bibr" target="#b21">Radenovic et al. 2018)</ref>, and is also utilized in other fields such as unsupervised learning <ref type="bibr" target="#b14">(Iscen et al. 2018b</ref>). Recently, other works have attempted to improve the efficiency of diffusion <ref type="bibr" target="#b12">(Iscen et al. 2017</ref>), but their speed up is still not sufficient enough to handle the amount of queries found in largescale image retrieval datasets. We notice that the main bottlenecks in speed of online diffusion processes come from the random walk and preparation steps. Inspired by the closed-form solution of diffusion, we find that the diffusion for each query can be converted to a linear combination of the pre-computed diffusion results of all database elements. Following this observation, our proposed method completely removes the random walk from the online stage. As a result, our work is able to improve the efficiency of the diffusion process by a factor of ten in a large-scale image retrieval setting.</p><p>In addition, the previous versions of diffusion utilize an early truncation that happens before the affinity matrix normalization process. In our proposed process, we propose to perform a late truncation after normalization, that results in significantly better performance. <ref type="figure" target="#fig_0">Fig. 1</ref> summarizes the efficiency and performance of the aforementioned methods. The source code to replicate our experiments is available at https://github.com/fyang93/diffusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Works</head><p>Although originally developed for ranking on manifolds <ref type="bibr" target="#b18">(Page et al. 1999;</ref><ref type="bibr" target="#b27">Zhou et al. 2004b;</ref><ref type="bibr" target="#b8">Donoser and Bischof 2013)</ref>, diffusion was soon applied to classification <ref type="bibr" target="#b26">(Zhou et al. 2004a)</ref>, and image segmentation <ref type="bibr" target="#b11">(Grady 2006)</ref>. Recently, some variants of diffusion <ref type="bibr" target="#b1">(Bai et al. 2017a;</ref><ref type="bibr" target="#b2">2017b;</ref><ref type="bibr" target="#b4">2019)</ref> are also developed to conduct diffusion processes on tensor product graph with a smoothness criterion, showing its potential for image retrieval tasks as a re-ranking method.</p><p>Query expansion, a common technique in image retrieval, can improve retrieval performance during query time. Average query expansion (AQE) <ref type="bibr" target="#b12">Iscen et al. 2017</ref>), a popular type of query expansion because of its simplicity, averages the features of the query's nearest neighbors to form a new query to run search again. When AQE is applied iteratively, the recomputation of the query is akin to traveling along the manifolds of the feature space. Although this traversal is similar to diffusion, AQE only utilizes the relationships between query and database images, but not between each of the database images with each other. With prior knowledge of the relationships between all of the database images, diffusion is thus better able to exploit the manifolds in the feature space than query expansion can.</p><p>In previous works of diffusion, the query is provided as a part of the database. However, in a real-world setting, queries are unavailable until they are issued by users. To tackle this issue without introducing any computational overhead, <ref type="bibr" target="#b12">(Iscen et al. 2017)</ref> uses the short list of k-NN search results to form a sparse initial state vector, instead of using a one-hot vector as the initial state. As a consequence, queries are not included in the neighborhood graph. The downside to this is that the graph needs to be stored and loaded during the search stage for random walk, which is both memory and computationally inefficient. Since the previous methods were evaluated on the Oxford ) and Paris <ref type="bibr" target="#b20">(Philbin et al. 2008</ref>) datasets, smaller datasets only containing 55 queries, the inefficiency of those methods did not have much impact on the total computation time. When these methods are used on large-scale datasets with many queries, the inefficiency during online search becomes magnified and intractable.</p><p>To tackle this inefficiency, past efforts have been made to scale diffusion up to handle larger datasets. <ref type="bibr" target="#b7">(Dong, Moses, and Li 2011)</ref> proposed to accelerate the construction of the affinity matrix denoting the graph. Iscen et al. reported that Dong's method is orders of magnitude faster than exhaustive search with only limited decreases in performance <ref type="bibr" target="#b12">(Iscen et al. 2017)</ref>. Another approach to improve efficency is using approximate nearest neighbor search (ANN). Compared to constructing the graph by exhaustive k-NN search, ANN search is faster and provides comparable accuracy <ref type="bibr">(Jegou, Douze, and Schmid 2011;</ref><ref type="bibr" target="#b9">Ge et al. 2014)</ref>. Most recently, <ref type="bibr" target="#b13">(Iscen et al. 2018a</ref>) approximated the affinity matrix by using a low-rank spectral decomposition to reduce the online computational cost. However, this method did not result in much improvement in terms of retrieval performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preliminaries of Diffusion</head><p>There are two main approaches to conducting diffusion: through iterative updates or solving the closed form directly. Both <ref type="bibr">Zhou et al. and Donoser et al.</ref> describe diffusion as a mechanism for spreading the query similarities over the manifolds <ref type="bibr" target="#b27">(Zhou et al. 2004b;</ref><ref type="bibr" target="#b8">Donoser and Bischof 2013)</ref>, while Iscen et al. utilizes the closed form theorem in <ref type="bibr" target="#b27">(Zhou et al. 2004b</ref>) and proposed an efficient solution <ref type="bibr" target="#b12">(Iscen et al. 2017)</ref>. We mainly follow the steps from <ref type="bibr" target="#b26">(Zhou et al. 2004a)</ref> and <ref type="bibr" target="#b12">(Iscen et al. 2017)</ref> below.</p><p>Problem setup. For image retrieval tasks, we define a database as χ = {x 1 , . . . , x n } ⊂ R d , where each x i is a feature vector. Images can be represented by a global feature that corresponds to the entire image, or multiple regional features corresponding to different regions of the image. In the following equations, x i can stand for either of these representations.</p><p>For most public datasets in the retrieval field, query and database images are both available. In our following example, queries are unseen to us until provided by users. We denote the query as Q = {q 1 , . . . , q m } ⊂ R d , where m = 1 when the query is described by a global feature and m &gt; 1 when it contains regional features.</p><p>Graph construction. For simplicity, we consider an example where we handle only one query image Q and include it into the database. The entire set is defined asχ = {q 1 , . . . , q m , x 1 , . . . , x n }, and we denote i-th element inχ asχ i . In addition, a local constraint is adopted so that the graph only contains similarities between pairs of elements that are nearest neighbors to each other according to <ref type="bibr" target="#b12">(Iscen et al. 2017)</ref>. The affinity matrix is defined as A = (a ij ) ∈ R (n+m)×(n+m) , where each element is obtained by</p><formula xml:id="formula_0">a ij = s(χ i ,χ j ) i = j,χ i ∈ NN k (χ j ),χ j ∈ NN k (χ i ) 0 otherwise , (1) ∀i, j ∈ {1, . . . , n + m}, denoting NN k (x) the k-NNs of x.</formula><p>Since the similarity metric s is usually symmetric and positive, A is a symmetric matrix. Eq. (1) allows A to be sparse, providing memory and computational efficiency. The degree matrix D is a diagonal matrix and each diagonal element is the corresponding row-wise sum of A, i.e. the element d ii in D is defined as n+m j=1 a ij . It's later used to symmetrically normalize A into the stochastic matrix S:</p><formula xml:id="formula_1">S = D −1/2 AD −1/2 .<label>(2)</label></formula><p>S is a variant of the typical transition matrix D −1 A, and both have the same eigenvalues and eigenvectors <ref type="bibr" target="#b8">(Donoser and Bischof 2013)</ref>.</p><p>Random walk. After the graph construction, the random walk is performed until it reaches a convergence state, resulting in final ranking scores for each of the images in the gallery. For the t-th step of random walk, the state is</p><formula xml:id="formula_2">recorded in a vector f t = [f t q , f t d ] ∈ R n+m , where f t q ∈ R m , f t d ∈ R n .</formula><p>We set the initial state to be a m-hot vector where f 0 q = 1 m , f 0 d = 0 n . The random walk iterates the following step:</p><formula xml:id="formula_3">f t+1 = αSf t + (1 − α)f 0 , α ∈ (0, 1).<label>(3)</label></formula><p>Essentially, there is a probability α to randomly walk from the current state f t or 1−α to restart from the initial state f 0 . Given the fact that α ∈ (0, 1) and the abstract eigenvalues of S is no larger than 1 according to the Perron-Frobenius theorem, this iteration converges to a closed-form solution <ref type="bibr" target="#b27">(Zhou et al. 2004b)</ref>:</p><formula xml:id="formula_4">f * = (1 − α)(I − αS) −1 f 0 .<label>(4)</label></formula><p>After convergence, the values in f * contain the similarities of each database element to the query, which will be used as ranking scores for re-ranking.</p><p>Decomposition. The above steps incorporate the query into the graph during the diffusion process. Grady proposed to decompose queries from the above operations <ref type="bibr" target="#b11">(Grady 2006)</ref>, and his technique was recently followed by <ref type="bibr" target="#b12">(Iscen et al. 2017</ref>). Note, the closed-form solution f * ∈ R n+m contains the ranking scores on both the query and database elements, but for the task of image retrieval, we only care about the ranking scores for database elements. This leads to the decomposition of the query and database ranking scores, so that the matrix S is split into 4 blocks</p><formula xml:id="formula_5">S = S qq S qd S dq S dd ,<label>(5)</label></formula><p>where S qq ∈ R m×m , S qd ∈ R m×n , S dq ∈ R n×m , and S dd ∈ R n×n . The decomposed solution then becomes</p><formula xml:id="formula_6">f * d = (1 − α)(I − αS dd ) −1 S dq f 0 q ∈ R n ,<label>(6)</label></formula><p>where S dd can be viewed as the transition matrix for random walk on the database side, and S dq = S qd consists of normalized similarities between the query and its nearest neighbors. Subsequently, we can then obtain the cleaner form:</p><formula xml:id="formula_7">f * d ∝ L −1 α y,<label>(7)</label></formula><formula xml:id="formula_8">where L α = I − αS dd ∈ R n×n , y = S dq f 0 q ∈ R n .</formula><p>and we can ignore the constant 1 − α since scores are used for ranking.</p><p>Truncation. An optional truncation step is used on large datasets, where the dataset is truncated to a smaller subset before normalization and random walk. Iscen et al. conduct truncation with only global features representing entire images <ref type="bibr" target="#b12">(Iscen et al. 2017)</ref>. Given the global feature vector q of a new query (m = 1, q = q 1 ), the indexes I = NN ID L (q) of features corresponding to the top ranked images is retrieved by L-NN search, where L is a limiting constant  <ref type="figure">Figure 2</ref>: Comparison between our implementation (green) and previous works' (blue). Our method truncates late while previous methods truncate early during diffusion.</p><p>that defines the maximum size of the subgraph (truncated graph). The affinity matrix denoting the subgraph is defined asÂ ∈ R L×L , and each elementâ ij inÂ satisfieŝ</p><formula xml:id="formula_9">a ij = s(χ Ii , χ Ij ) I i = I j , χ Ii ∈ NN k (χ Ij ), χ Ij ∈ NN k (χ Ii ) 0 otherwise , (8) ∀i, j ∈ {1, . . . , L}, where I i is the i-th index in the set I.</formula><p>After truncation,Â is normalized into a stochastic matrixŜ and then random walk is subsequently performed onŜ. We refer to the process of normalizing the truncated graph as subgraph normalization throughout the rest of the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposed Method</head><p>We propose a remarkably fast diffusion achieving state-ofthe-art retrieval performance. Iscen et al. reported that L −1 α is not sparse like L α making it less efficient to compute than using L α to solve L α f * d ∝ y online <ref type="bibr" target="#b12">(Iscen et al. 2017</ref>). Our method makes it possible to pre-compute and maintain a sparsified L −1 α offline to achieve better efficiency. Given a new query, its diffusion result can be obtained by linear combination according to Eq. (7). As a result, we achieve a substantial improvement in the online search speed. Moreover, we argue that the subgraph normalization that takes place in <ref type="bibr" target="#b12">(Iscen et al. 2017)</ref> has negative effects on the retrieval performance. After the offline computation to obtain L α from the entire matrix A, we apply slicing to L α to fetch the values on the corresponding rows and columns limited in the truncation subset.</p><p>In the following sections, we compare the time complexity between Iscen's method and our method to analyze the efficiency gains of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Complexity analysis of online diffusion</head><p>In prior works, the entire process of diffusion is performed during runtime when queries are processed. A combination of global and regional features are also used, but for simplicity, we choose to analyze Iscen's online diffusion with only global features in this section.</p><p>Given the global feature q of a new query, the pipeline of online diffusion can be broken down to the following steps: 1. Truncation: the nearest neighbors NN L (q) ⊂ χ of q is obtained by k-NN search for truncation 2. Graph construction: the truncated affinity matrixÂ denoting subgraph is constructed for the subset NN L (q) with a reciprocity check, then subgraph normalization is applied to form the matrixŜ andL α are created afterward 3. Initialization: the vector y = S dq f 0 q contains the similarities between the query and its nearest neighbors, which is subsequently truncated to fit the size ofL α 4. Random walk: the convergence state is solved by Eq. <ref type="formula" target="#formula_7">(7)</ref> to obtain the result of diffusion by conjugate gradient</p><p>The above steps include k-NN search, which is responsible for most of the time needed to process those steps. Recent k-NN search strategies are sufficiently efficient (Jegou, Douze, and Schmid 2011; Malkov and Yashunin 2016; Johnson, Douze, and Jégou 2017), so we do not discuss the complexity which is beyond the scope of this work. Constructing the affinity matrix denoting the subgraph also requires k-NN search, and the subgraph normalization according to Eq.</p><p>(2) costs O(Lk) time. Here, Lk is the number of non-zero entries in the truncated affinity matrix, and k is the parameter for the number of nearest neighbors in k-NN search. In the random walk step, the result is approximated by the early-terminated conjugate gradient (CG) <ref type="bibr" target="#b17">(Nocedal and Wright 2006;</ref><ref type="bibr" target="#b12">Iscen et al. 2017</ref>). Suppose we iterate CG for τ steps, its time complexity is O(Lkτ ).</p><p>From the above analysis, we can see that the cost to process each query is non-trivial and the most time-consuming steps are the subgraph construction and random walk. This motivates us to solve these inefficiencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>From online to offline</head><p>We propose a new form of diffusion that moves the online steps, processing the heavy computation steps during runtime, to offline, pre-computing those steps beforehand.</p><p>The right side of Eq. <ref type="formula" target="#formula_7">(7)</ref> can be considered as a linear combination of column vectors in L −1 α with weights in vector y. In other words, the results of any new query is merely the linear combination of the columns of L −1 α with corresponding weights in y. Unfortunately, the inverse of a large sparse matrix is hard to compute even though L α is positivedefinite. Despite this difficulty, it is still possible to compute the approximate inverse by either global iteration or a column-oriented algorithm, two approaches summarized in <ref type="bibr" target="#b24">(Saad 2003)</ref>. Global iteration computes the inverse on the entirety of the matrix, whereas the column-oriented algorithm computes it one column at a time. Between the two, the column-oriented algorithm is more appealing for parallelism since it computes each column separately. It also allows us to apply truncation in computing each column to make a sparser structure. Therefore, we choose to adopt the column-oriented strategy in our proposed method.</p><p>To compute each column of L −1 α , we first define a set of vectors {b 1 , . . . , b n } to be the column vectors of an identity matrix I ∈ R n×n . Then, according to the closed-form solution in Eq. (7) we solve:</p><formula xml:id="formula_10">L α c i = b i ,<label>(9)</label></formula><p>with conjugate gradient (CG) <ref type="bibr" target="#b17">(Nocedal and Wright 2006)</ref>, we obtain c i , the approximate i-th column vector in L −1 α . Essentially, c i can be viewed as the diffusion result of i-th . . , c i h } from L −1 α with the values in step 1 to obtain the diffusion result: f * d ∝ j v j c ij Note that the inverse matrix L −1 α obtained from the above procedure is a dense matrix, which is less memory efficient. We propose the sparsified version <ref type="figure" target="#fig_1">(Fig. 3</ref>) of inverse matrix L −1 α to provide better memory efficiency in the next section.</p><formula xml:id="formula_11">NN ID L (x1) NN ID L (x2) ... NN ID L (xn) L c1ĉ2 ...ĉ n n c11 0 . . . 0 0ĉ12 . . . 0 . . . . . . . . . . . . cn1 0 . . .ĉnn                   ID</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Database-side truncation</head><p>As we discussed, truncation is crucial for scaling up to large datasets. The time complexity of diffusion is deeply related to L, the size of subgraph after truncation. Thus, the process of random walk can be accelerated if the database is truncated to a smaller size. Despite increases in speed, Iscen et al. observed that truncation has a negative effect on the retrieval performance <ref type="bibr" target="#b12">(Iscen et al. 2017)</ref>. Contrary to Iscen's findings, we find that truncation by itself is not a detrimental practice. Rather, the order in which it is applied in relation to normalization is the important factor. We find that applying normalization after truncation is the reason for the decrease in retrieval performance. The subgraph after truncation contains incomplete manifolds and the later normalization raises the probabilities to transition to the nodes on such manifolds. This causes the random walk to be more likely to visit misleading nodes.</p><p>To tackle this issue, we normalize the complete matrix A to build L α , and then apply late truncation (slicing) to L α directly, as shown in <ref type="figure">Fig. 2</ref>. Denoting the indexes of the nearest neighbors of a query as I = NN ID L (q), the truncation is applied byl ij = l IiIj , ∀i, j ∈ {1, . . . , L},</p><p>Algorithm </p><formula xml:id="formula_13">col id ← NN ID k (q i )[j] 7: weight ← NN SIM k (q i )[j] 8: row ids ← NN ID L (x col id ) from sparsified L −1 α 9: f * d [row ids] ← f * d [row ids] + weight * ĉ col id 10: Aggregate scores in f * d to image level if needed</formula><p>where l ij is the element in L α whilel ij stands for the element inL α after truncation. From our experiments, such a truncation can provide a significant performance boost.</p><p>In previous works, diffusion cannot be performed without the query because truncation process is applied under the guide of queries with a subsequent random walk. This forces diffusion in those works to be computed online. Our proposed method differs by instead using the elements in the database themselves as queries, thus moving the entire diffusion process offline. In addition, truncation is applied to the database elements as previous works applied it on queries.</p><p>As an example, we take a database element x i and conduct L-NN search in χ to get the indexes J = NN ID L (x i ) of the short list. Since L α can be computed and cached beforehand, we slice L α with J to avoid subgraph normalization. The one-hot vector b i is also sliced tob i with the indexes J . Usually the indexes NN ID L (x i ) are sorted by the similarities in a descending order. Thus, the index i of x i is always at the top of NN ID L (x i ) since it is always most similar to itself. As a result, the truncated one-hot inital state vector b i = [1, 0, 0, . . . ] ∈ {0, 1} L is fixed regardless of i. Therefore, we use the same initial state vector for all database-side random walk.After the truncation on L α and b i , we obtain the truncated i-th column vector in L −1 α byL αĉi =b i , whereĉ i ∈ R L . This sparsifies the matrix L −1 α . <ref type="figure" target="#fig_1">Fig. 3</ref> shows the structure of the sparsified L −1 α . It costs O(Ln) memory to store the pre-computed L −1 α , which is more than O(L 2 ), the memory usage of Iscen's method.</p><p>To summarize, the late truncation directly applied on L α removes the negative effects of early truncation, and is completely pre-computed offline. In addition, the truncated initial state vector is fixed, meaning there is no extra overhead cost to build it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Online search</head><p>Once the sparsified inverse matrix is obtained, the online search results can be calculated by using Algorithm 1. We denote NN SIM k (q i ) as the similarities between the query feature q i and its nearest neighbors in χ. The online search thus becomes very fast because it only includes the k-NN search and linear combination. Moreover, since we only need k columns in L −1 α for each query, our approach can merely cost O(Lk) RAM during runtime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>This section presents the experimental setup and investigates the computational efficiency as well as retrieval performance of our methods for image retrieval. For the efficiency evaluation, we use a single core of Intel Xeon 2.80GHz CPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental setup</head><p>Datasets. We use the Oxford Buildings ) and Paris <ref type="bibr" target="#b20">(Philbin et al. 2008</ref>) datasets in our experiments. The datasets are referred to as Oxford5k and Paris6k respectively in correspondence with the size of each dataset. Another set of 100k random images from Flicker <ref type="bibr" target="#b20">(Philbin et al. 2008</ref>) are commonly used as distractors to enlarge the above datasets to Oxford105k and Paris106k. We measure the online computational time on the 55 queries of the datasets for Iscen's method <ref type="bibr" target="#b12">(Iscen et al. 2017</ref>) and our proposed method. For evaluation, we adopt the standard mean average precision (mAP) as a performance measurement.</p><p>Features. We use the 512 and 1,024 dimensional global R-MAC descriptors <ref type="bibr" target="#b25">(Tolias, Sicre, and Jégou 2015;</ref><ref type="bibr" target="#b10">Gordo et al. 2016</ref>) provided by Iscen et al. for fair comparison. We experiment on both global and regional features provided. For the Oxford and Paris datasets, there are 21 regional features per image on average. K-NN search. We conduct k-NN search by using the efficient FAISS toolkit 1 , containing a CPU version and a faster GPU version <ref type="bibr" target="#b15">(Johnson, Douze, and Jégou 2017)</ref>, which allows us to deal with the larger Oxford105k and Paris106k datasets, especially for offline pre-computation. Implementation details. We use the same graph construction parameters as in the previous work <ref type="bibr" target="#b12">(Iscen et al. 2017)</ref>. In particular, the parameter α to build L α is set to 0.99. For global features, 50 nearest neighbors of each database element are used for graph construction, and the initial state vector contains the similarities between the query and its 10 nearest neighbors. While for regional features, the corresponding numbers of nearest neighbors are set to 200, 200 respectively. Through our experiments, deviating from these parameters consistently resulted in worse performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Runtime computational efficiency</head><p>We evaluate the computational efficiency on each query for k-NN search, Iscen's method and our proposed method. Each method is run 10 times and the average computational time is used for comparison. The results on Oxford5k and Oxford105k datasets are shown in <ref type="figure" target="#fig_2">Fig. 4</ref>. Since most of the computation in our proposed method is already done offline on database side, we observe that our method can be remarkably fast during online search, close to the speed of k-NN search. <ref type="figure" target="#fig_2">Fig. 4</ref> shows that the average search time per query of our method with global features are ∼2ms and ∼10ms on Oxford5k and Oxford105k datasets respectively, and its extra computation over k-NN search is negligible. In contrast, Iscen's method is rather time-consuming since it has a lot of runtime processes. The search time per query for online diffusion without truncation is slower: ∼20ms on Oxford5k and ∼0.2s on Oxford105k. When truncation is applied during the offline diffusion process, the overhead to construct the graph during runtime causes it to be slow. We also observed a decrease in efficiency as the size of truncated graph grows as shown in <ref type="figure" target="#fig_2">Fig. 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-computational efficiency</head><p>Now that we have shown the online search of our proposed approach is very efficient, we investigate the offline computational cost. The offline process mainly consists of two parts: the database vs. database k-NN search for truncation and random walk processes on each of the database element. Since exhaustive k-NN search on a large scale dataset is time-consuming, we utilize approximate nearest neighbor (ANN) search. <ref type="figure" target="#fig_3">Figure 5</ref> shows the speed/accuracy trade-off when using ANN search for either truncation or graph construction. We adopt IVFADC (Jegou, Douze, and Schmid 2011) for ANN search using Faiss library <ref type="bibr" target="#b15">(Johnson, Douze, and Jégou 2017)</ref>, where the codebook size of coarse quantizer √ n ≈ 316, the number of subvectors M = 128 is used and the number of clusters to scan is varied. When compared to the exhaustive k-NN search, ANN is generally good at approximating the top results of the search but its ranking and scores can be out of order and imprecise. This makes it applicable to truncation which only requires a coarse set of the top results. Graph construction, however, would be negatively affected by even small differences in the scores of the search results. We therefore use ANN search only for truncation and use exhaustive k-NN search for graph construction. Since the k in k-NN search for graph construction is small compared to truncation, it can be efficiently processed even without using approximate search, especially by using Faiss on a GPU. As a result, the entire process of truncation and graph construction takes ∼1ms using a single GPU per image. Diffusion processes take ∼6 minutes to process the whole Oxford105k dataset using global features and a truncation size of 5,000 (3.4 ms per image). We measured using a single core of CPU, but these processes can be easily parallelized. Compared to the offline processing in <ref type="bibr" target="#b13">(Iscen et al. 2018a</ref>) which takes a few hours, our method is much faster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Influence of subgraph normalization</head><p>In <ref type="bibr" target="#b12">(Iscen et al. 2017)</ref>, truncation enables diffusion on large scale datasets but is described to be detrimental to retrieval performance. The authors claim that retrieval performance of diffusion grows as the percentage of the whole graph used grows, with a complete graph without any truncation having the best performance. However, we argue that the retrieval performance is mainly influenced by the early truncation leading to subgraph normalization. In order to avoid subgraph normalization, we first obtain the complete graph and apply late truncation (slicing) on the complete matrix L α in the process of diffusion. For comparison, we implement Iscen's method with and without subgraph normalization. We vary the truncation size L to observe the influence of subgraph normalization on the retrieval performance.</p><p>The experimental results with global features for the Oxford and Paris datasets are presented in <ref type="figure">Fig. 6</ref>. On the Oxford datasets, it is clear that the performance is significantly improved without subgraph normalization when the truncation size L is small, but its effectiveness on the Paris datasets is smaller. We observe that merely performing k-NN search on the Paris datasets already results in a high retrieval performance. This could mean that the Paris datasets have standard-shaped manifolds conducive to comparison by Euclidean distance, so it limits the benefits gained from diffusion.</p><p>While previous works always encouraged using larger values of L for performance gains, our method can achieve state-of-the-art performance with smaller L values found through validation. As a bonus, a smaller truncation size L will lead to acceleration of the offline computation.  <ref type="bibr" target="#b22">(Radenović, Tolias, and Chum 2016)</ref> and ResNet101 <ref type="bibr" target="#b10">(Gordo et al. 2016</ref>). <ref type="table">Table 1</ref> compares our method with other competitive methods that use global and regional features. Testing on all datasets using global features, we observe up to a 5% increase in mAP performance compared to the previous stateof-the-art. Similarly, on diffusion with regional features, we achieve competitive or better performance. We note that Iscen et al. used global features to guide the truncation in their regional diffusion. For each query, they first apply k-NN search using the global features to obtain the closest images to that query. Subsequently, these results are used as a bounded set to perform regional diffusion. In contrast, we only use regional features in our regional diffusion for simplicity. To exploit global features, we apply a simple late fusion by computing a weighted mean of scores from regional and global diffusion, setting the weight for regional diffusion to 0.75. This further increases the performance (proposed diffusion w/ late fusion in <ref type="table">Table 1</ref>), leading to better performance than Iscen et al. on all datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison to other methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we propose a novel efficient diffusion to achieve fast retrieval during runtime with significant improvement in retrieval performance. We experimentally show that our approach has a similar efficiency to k-NN search, which is 10∼ times faster than existing diffusion methods with global features. Moreover, our method achieves state-of-the-art performance on Oxford and Paris datasets. In conclusion, our method makes diffusion more practical for image retrieval on large-scale datasets, and has the potential to improve retrieval in other fields, such as text and video.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The efficiency vs. performance comparison between k-NN, Iscen's method<ref type="bibr" target="#b12">(Iscen et al. 2017)</ref>, and our proposed method on Oxford105k dataset with global image features. Our proposed method achieves better retrieval performance than diffusion by Iscen et al. with almost the same search speed (∼20ms per query) as k-NN search. We show the results of varying truncation sizes L.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>The data structure of sparsified matrix L −1 α , where the values in i-th column compose the column vectorĉ i and NN ID L (x i ) records the row indexes of each value inĉ i . database element b i . After the database-side diffusion, given a new query, the pipeline becomes 1. Initialization: prepare the initial state vector y for the query, where the indexes of non-zero entries are {i 1 , . . . , i h }, and their values are {v 1 , . . . , v h } 2. Linear combination: combine the corresponding columns {c i1 , .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Online search time per query measured for k-NN search, Iscen's method and our proposed method respectively, using global features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Speed/accuracy trade-off by using approximate nearest neighbor (ANN) search in truncation and graph construction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>1</head><label></label><figDesc>Online search 1: Input Q = {q 1 , . . . , q m } ← a new query 2: Output f * d ← a new array of n zeros 3: for i ← 1 to m do</figDesc><table><row><cell>4:</cell><cell>obtain NN SIM k (q i ), NN ID k (q i ) by k-NN search</cell></row><row><cell>5:</cell><cell>for j ← 1 to k do</cell></row><row><cell>6:</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Retrieval performance (mAP) vs. the size of truncated graph L using early truncation and late truncation.Table 1: Performance comparison with the state of the art. We used R-MAC features extracted with VGG</figDesc><table><row><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell>0.9</cell></row><row><cell>mAP</cell><cell>0.8</cell><cell>k-NN ours, early Iscen, early</cell><cell>mAP</cell><cell>0.8</cell><cell></cell><cell>k-NN ours, early Iscen, early</cell><cell>mAP</cell><cell>0.8 0.7</cell><cell></cell><cell>k-NN ours, early Iscen, early</cell><cell>mAP</cell><cell>0.8 0.7</cell><cell>k-NN ours, early Iscen, early</cell></row><row><cell></cell><cell></cell><cell>ours, late</cell><cell></cell><cell></cell><cell></cell><cell>ours, late</cell><cell></cell><cell></cell><cell></cell><cell>ours, late</cell><cell></cell><cell>ours, late</cell></row><row><cell></cell><cell>0.7</cell><cell>Iscen, late</cell><cell></cell><cell>10 2 0.7</cell><cell>10 3</cell><cell>Iscen, late</cell><cell></cell><cell>10 2 0.6</cell><cell>10 3</cell><cell>Iscen, late</cell><cell></cell><cell>10 2 0.6</cell><cell>10 3</cell><cell>Iscen, late</cell></row><row><cell></cell><cell cols="2">size of truncated graph</cell><cell></cell><cell cols="3">size of truncated graph</cell><cell></cell><cell></cell><cell cols="2">size of truncated graph</cell><cell></cell><cell>size of truncated graph</cell></row><row><cell></cell><cell>(a) Oxford5k</cell><cell></cell><cell></cell><cell cols="3">(b) Oxford105k</cell><cell></cell><cell></cell><cell>(c) Paris6k</cell><cell></cell><cell></cell><cell>(d) Paris106k</cell></row><row><cell></cell><cell>Figure 6: Method</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Feature</cell><cell></cell><cell cols="5">Global Regional Oxf5k Oxf105k Par6k Par106k</cell></row><row><cell>w/o regional feature</cell><cell cols="4">k-NN search k-NN + AQE (Chum et al. 2007) Iscen's diffusion (Iscen et al. 2017) Proposed diffusion k-NN search k-NN + AQE (Chum et al. 2007) Iscen's diffusion (Iscen et al. 2017) Proposed diffusion</cell><cell cols="2">R-MAC (VGG) R-MAC (ResNet)</cell><cell></cell><cell></cell><cell></cell><cell>79.5 85.4 85.7 89.7 83.9 89.6 87.1 92.6</cell><cell></cell><cell>72.1 79.7 82.7 86.8 80.8 88.3 87.4 91.8</cell><cell>84.5 88.4 94.1 94.7 93.8 95.3 96.5 97.1</cell><cell>77.1 83.5 92.5 92.9 89.9 92.7 95.4 95.6</cell></row><row><cell></cell><cell cols="4">R-match (Razavian et al. 2016)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>81.5</cell><cell></cell><cell>76.5</cell><cell>86.1</cell><cell>79.9</cell></row><row><cell>w/ regional feature</cell><cell cols="4">R-match + AQE (Chum et al. 2007) Iscen's diffusion (Iscen et al. 2017) Proposed diffusion Proposed diffusion w/ late fusion R-match (Razavian et al. 2016) R-match + AQE (Chum et al. 2007) Iscen's diffusion (Iscen et al. 2017)</cell><cell cols="2">R-MAC (VGG) R-MAC (ResNet)</cell><cell></cell><cell></cell><cell></cell><cell>83.6 93.2 91.8 93.5 88.1 91.0 95.8</cell><cell></cell><cell>78.6 90.3 88.6 91.2 85.7 89.6 94.2</cell><cell>87.0 96.5 93.9 96.1 94.9 95.5 96.9</cell><cell>81.0 92.6 89.2 93.8 91.3 92.5 95.3</cell></row><row><cell></cell><cell cols="2">Proposed diffusion</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>95.9</cell><cell></cell><cell>94.8</cell><cell>97.6</cell><cell>95.6</cell></row><row><cell></cell><cell cols="4">Proposed diffusion w/ late fusion</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>96.2</cell><cell></cell><cell>95.2</cell><cell>97.8</cell><cell>96.2</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/facebookresearch/faiss</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This study is supported by CREST (JPMJCR 1686).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural codes for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Slesarev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chigorin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Regularized diffusion process for visual retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Latecki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3967" to="3973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Ensemble diffusion for retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jan Latecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="774" to="783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Regularized diffusion process on bidirectional context for object retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Latecki</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Automatic ensemble diffusion for 3d shape and image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Latecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="88" to="101" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Total recall: Automatic query expansion with a generative feature model for object retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Efficient k-nearest neighbor graph construction for generic similarity measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Moses</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Diffusion processes for retrieval revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Donoser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Optimized product quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="744" to="755" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep image retrieval: Learning global representations for image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Almazán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Random walks for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Grady</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1768" to="1783" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Efficient diffusion on region manifolds: Recovering small objects with compact cnn representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Iscen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Avrithis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Furon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fast spectral ranking for similarity search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Iscen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Avrithis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Furon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mining on manifolds: Metric learning without labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Iscen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Avrithis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">; M</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. Jegou, H.; Douze</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="117" to="128" />
		</imprint>
	</monogr>
	<note>Product quantization for nearest neighbor search</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08734</idno>
		<title level="m">Billionscale similarity search with gpus</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">A</forename><surname>Malkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Yashunin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.09320</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Numerical optimization 2nd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Wright</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">The pagerank citation ranking: Bringing order to the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Winograd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<pubPlace>Stanford InfoLab</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Object retrieval with large vocabularies and fast spatial matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Lost in quantization: Improving particular object retrieval in large scale image databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Revisiting oxford and paris: Large-scale image retrieval benchmarking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Radenovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Iscen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Avrithis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cnn image retrieval learns from bow: Unsupervised fine-tuning with hard examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Radenović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Visual instance retrieval with deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ITE MTA</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="251" to="258" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Iterative methods for sparse linear systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Saad</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">82</biblScope>
			<pubPlace>SIAM</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Particular object retrieval with integral max-pooling of cnn activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sicre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning with local and global consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Lal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ranking on data manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
