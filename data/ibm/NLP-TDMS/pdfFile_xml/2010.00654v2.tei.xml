<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">VAEBM: A SYMBIOSIS BETWEEN VARIATIONAL AU- TOENCODERS AND ENERGY-BASED MODELS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhisheng</forename><surname>Xiao</surname></persName>
							<email>zxiao@uchicago.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Kreis</surname></persName>
							<email>kkreis@nvidia.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
							<email>jkautz@nvidia.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
							<email>avahdat@nvidia.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Computational and Applied Mathematics</orgName>
								<orgName type="institution">The University of Chicago</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">NVIDIA</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">VAEBM: A SYMBIOSIS BETWEEN VARIATIONAL AU- TOENCODERS AND ENERGY-BASED MODELS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2021</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Energy-based models (EBMs) have recently been successful in representing complex distributions of small images. However, sampling from them requires expensive Markov chain Monte Carlo (MCMC) iterations that mix slowly in high dimensional pixel space. Unlike EBMs, variational autoencoders (VAEs) generate samples quickly and are equipped with a latent space that enables fast traversal of the data manifold. However, VAEs tend to assign high probability density to regions in data space outside the actual data distribution and often fail at generating sharp images. In this paper, we propose VAEBM, a symbiotic composition of a VAE and an EBM that offers the best of both worlds. VAEBM captures the overall mode structure of the data distribution using a state-of-the-art VAE and it relies on its EBM component to explicitly exclude non-data-like regions from the model and refine the image samples. Moreover, the VAE component in VAEBM allows us to speed up MCMC updates by reparameterizing them in the VAE's latent space. Our experimental results show that VAEBM outperforms state-of-the-art VAEs and EBMs in generative quality on several benchmark image datasets by a large margin. It can generate high-quality images as large as 256×256 pixels with short MCMC chains. We also demonstrate that VAEBM provides complete mode coverage and performs well in out-of-distribution detection. * Work done during an internship at NVIDIA arXiv:2010.00654v2 [cs.LG]  </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Deep generative learning is a central problem in machine learning. It has found diverse applications, ranging from image <ref type="bibr" target="#b3">(Brock et al., 2018;</ref><ref type="bibr">Karras et al., 2019;</ref><ref type="bibr" target="#b56">Razavi et al., 2019)</ref>, music <ref type="bibr" target="#b7">(Dhariwal et al., 2020)</ref> and speech <ref type="bibr">(Ping et al., 2020;</ref><ref type="bibr" target="#b48">Oord et al., 2016a)</ref> generation, distribution alignment across domains <ref type="bibr" target="#b83">(Zhu et al., 2017;</ref><ref type="bibr" target="#b35">Liu et al., 2017;</ref><ref type="bibr" target="#b68">Tzeng et al., 2017)</ref> and semi-supervised learning <ref type="bibr">(Kingma et al., 2014;</ref><ref type="bibr" target="#b25">Izmailov et al., 2020)</ref> to 3D point cloud generation , light-transport simulation <ref type="bibr" target="#b39">(Müller et al., 2019)</ref>, molecular modeling <ref type="bibr" target="#b63">(Sanchez-Lengeling &amp; Aspuru-Guzik, 2018;</ref><ref type="bibr" target="#b47">Noé et al., 2019)</ref> and equivariant sampling in theoretical physics <ref type="bibr" target="#b26">(Kanwar et al., 2020)</ref>.</p><p>Among competing frameworks, likelihood-based models include variational autoencoders (VAEs) <ref type="bibr">(Kingma &amp; Welling, 2014;</ref><ref type="bibr" target="#b58">Rezende et al., 2014)</ref>, normalizing flows <ref type="bibr" target="#b57">(Rezende &amp; Mohamed, 2015;</ref><ref type="bibr" target="#b9">Dinh et al., 2016)</ref>, autoregressive models <ref type="bibr" target="#b49">(Oord et al., 2016b)</ref>, and energy-based models (EBMs) <ref type="bibr" target="#b29">(Lecun et al., 2006;</ref><ref type="bibr" target="#b60">Salakhutdinov et al., 2007)</ref>. These models are trained by maximizing the data likelihood under the model, and unlike generative adversarial networks (GANs) <ref type="bibr" target="#b14">(Goodfellow et al., 2014)</ref>, their training is usually stable and they cover modes in data more faithfully by construction.</p><p>Among likelihood-based models, EBMs model the unnormalized data density by assigning low energy to high-probability regions in the data space <ref type="bibr" target="#b77">(Xie et al., 2016;</ref><ref type="bibr" target="#b10">Du &amp; Mordatch, 2019)</ref>. EBMs are appealing because they require almost no restrictions on network architectures (unlike normalizing flows) and are therefore potentially very expressive. They also exhibit better robustness and out-of-distribution generalization <ref type="bibr" target="#b10">(Du &amp; Mordatch, 2019)</ref> because, during training, areas with high probability under the model but low probability under the data distribution are penalized explicitly. However, training and sampling EBMs usually requires MCMC, which can suffer from slow mode mixing and is computationally expensive when neural networks represent the energy function.</p><p>On the other hand, VAEs are computationally more efficient for sampling than EBMs, as they do not require running expensive MCMC steps. VAEs also do not suffer from expressivity limitations that normalizing flows face <ref type="bibr" target="#b11">(Dupont et al., 2019;</ref><ref type="bibr" target="#b28">Kong &amp; Chaudhuri, 2020)</ref>, and in fact, they have recently shown state-of-the-art generative results among non-autoregressive likelihood-based models <ref type="bibr">(Vahdat &amp; Kautz, 2020)</ref>. Moreover, VAEs naturally come with a latent embedding of data that allows fast traverse of the data manifold by moving in the latent space and mapping the movements to the data space. However, VAEs tend to assign high probability to regions with low density under the data distribution. This often results in blurry or corrupted samples generated by VAEs. This also explains why VAEs often fail at out-of-distribution detection <ref type="bibr" target="#b40">(Nalisnick et al., 2019)</ref>.</p><p>In this paper, we propose a novel generative model as a symbiotic composition of a VAE and an EBM (VAEBM) that combines the best of both. VAEBM defines the generative distribution as the product of a VAE generator and an EBM component defined in pixel space. Intuitively, the VAE captures the majority of the mode structure in the data distribution. However, it may still generate samples from low-probability regions in the data space. Thus, the energy function focuses on refining the details and reducing the likelihood of non-data-like regions, which leads to significantly improved samples.</p><p>Moreover, we show that training VAEBM by maximizing the data likelihood easily decomposes into training the VAE and the EBM component separately. The VAE is trained using the reparameterization trick, while the EBM component requires sampling from the joint energy-based model during training. We show that we can sidestep the difficulties of sampling from VAEBM, by reparametrizing the MCMC updates using VAE's latent variables. This allows MCMC chains to quickly traverse the model distribution and it speeds up mixing. As a result, we only need to run short chains to obtain approximate samples from the model, accelerating both training and sampling at test time.</p><p>Experimental results show that our model outperforms previous EBMs and state-of-the-art VAEs on image generation benchmarks including CIFAR-10, CelebA 64, LSUN Church 64, and CelebA HQ 256 by a large margin, reducing the gap with GANs. We also show that our model covers the modes in the data distribution faithfully, while having less spurious modes for out-of-distribution data. To the best of knowledge, VAEBM is the first successful EBM applied to large images.</p><p>In summary, this paper makes the following contributions: i) We propose a new generative model using the product of a VAE generator and an EBM defined in the data space. ii) We show how training this model can be decomposed into training the VAE first, and then training the EBM component. iii) We show how MCMC sampling from VAEBM can be pushed to the VAE's latent space, accelerating sampling. iv) We demonstrate state-of-the-art image synthesis quality among likelihood-based models, confirm complete mode coverage, and show strong out-of-distribution detection performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head><p>Energy-based Models: An EBM assumes p ψ (x) to be a Gibbs distribution of the form p ψ (x) = exp (−E ψ (x)) /Z ψ , where E ψ (x) is the energy function with parameters ψ and Z ψ = x exp (−E ψ (x)) dx is the normalization constant. There is no restriction on the particular form of E ψ (x). Given a set of samples drawn from the data distribution p d (x), the goal of maximum likelihood learning is to maximize the log-likelihood L(ψ) = E x∼p d (x) [log p ψ (x)], which has the derivative <ref type="bibr" target="#b74">(Woodford, 2006)</ref>:</p><formula xml:id="formula_0">∂ ψ L(ψ) = E x∼p d (x) [−∂ ψ E ψ (x)] + E x∼p ψ (x) [∂ ψ E ψ (x)]<label>(1)</label></formula><p>For the first expectation, the positive phase, samples are drawn from the data distribution p d (x), and for the second expectation, the negative phase, samples are drawn from the model p ψ (x) itself. However, sampling from p ψ (x) in the negative phase is itself intractable and approximate samples are usually drawn using MCMC. A commonly used MCMC algorithm is Langevin dynamics (LD) <ref type="bibr" target="#b41">(Neal, 1993)</ref>. Given an initial sample x 0 , Langevin dynamics iteratively updates it as:</p><formula xml:id="formula_1">x t+1 = x t − η 2 ∇ x E ψ (x t ) + √ ηω t , ω t ∼ N (0, I),<label>(2)</label></formula><p>where η is the step-size. 1 In practice, Eq. 2 is run for finite iterations, which yields a Markov chain with an invariant distribution approximately close to the original target distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VAE Decoder Energy Function VAE Prior</head><p>Figure 1: Our VAEBM is composed of a VAE generator (including the prior and decoder) and an energy function that operates on samples x generated by the VAE. The VAE component is trained first, using the standard VAE objective; then, the energy function is trained while the generator is fixed. Using the VAE generator, we can express the data variable x as a deterministic function of white noise samples z and x . This allows us to reparameterize sampling from our VAEBM by sampling in the joint space of z and x . We use this in the negative training phase (see Sec. 3.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Variational Autoencoders:</head><p>VAEs define a generative model of the form p θ (x, z) = p θ (z)p θ (x|z), where z is the latent variable with prior p θ (z), and p θ (x|z) is a conditional distribution that models the likelihood of data x given z. The goal of training is to maximize the marginal log-likelihood log p θ (x) given a set of training examples. However since the marginalization is intractable, instead, the variational lower bound on log p θ (x) is maximized with q φ (z|x) as the approximate posterior:</p><formula xml:id="formula_2">log p θ (x) ≥ E z∼q φ (z|x) [log p θ (x|z)] − D KL [q φ (z|x) p θ (z)] := L vae (x, θ, φ).<label>(3)</label></formula><p>The state-of-the-art VAE, NVAE <ref type="bibr">(Vahdat &amp; Kautz, 2020)</ref>, increases the expressivity of both prior and approximate posterior using hierarchical latent variables <ref type="bibr" target="#b61">(Kingma et al., 2016)</ref> where z is decomposed into a set of disjoint groups, z = {z 1 , z 1 , . . . , z L }, and the prior p θ (z) = l p θ (z l |z &lt;l ) and the approximate posterior q φ (z|x) = l q φ (z l |z &lt;l , x) are defined using autoregressive distributions over the groups. We refer readers to <ref type="bibr">Vahdat &amp; Kautz (2020)</ref> for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ENERGY-BASED VARIATIONAL AUTOENCODERS</head><p>One of the main problems of VAEs is that they tend to assign high probability to regions in data space that have low probability under the data distribution. To tackle this issue, we propose VAEBM, a generative model constructed by the product of a VAE generator and an EBM component defined in the data space. This formulation allows our model to capture the main mode structure of the data distribution using the VAE. But when training the joint VAEBM, in the negative training phase we sample from the model itself and can discover non-data-like samples, whose likelihood is then reduced by the energy function explicitly. The energy function defined in the pixel space also shares similarities with discriminator in GANs, which can generate crisp and detailed images.</p><p>Formally, we define the generative model in VAEBM as</p><formula xml:id="formula_3">h ψ,θ (x, z) = 1 Z ψ,θ p θ (x, z)e −E ψ (x) where p θ (x, z) = p θ (z)p θ (x|z)</formula><p>is a VAE generator and E ψ (x) is a neural network-based energy function, operating only in the x space, and Z ψ,θ = p θ (x)e −E ψ (x) dx is the normalization constant. VAEBM is visualized in <ref type="figure">Fig. 1</ref>. Marginalizing out the latent variable z gives</p><formula xml:id="formula_4">h ψ,θ (x) = 1 Z ψ,θ p θ (x, z)e −E ψ (x) dz = 1 Z ψ,θ p θ (x)e −E ψ (x) .<label>(4)</label></formula><p>Given a training dataset, the parameters of VAEBM, ψ, θ, are trained by maximizing the marginal log-likelihood on the training data:</p><formula xml:id="formula_5">log h ψ,θ (x) = log p θ (x) − E ψ (x) − log Z ψ,θ (5) ≥ E z∼q φ (z|x) [log p θ (x|z)] − D KL (q φ (z|x)||p(z)) Lvae(x,θ,φ) −E ψ (x) − log Z ψ,θ LEBM(x,ψ,θ) ,<label>(6)</label></formula><p>where we replace log p θ (x) with the variational lower bound from Eq. 3. Eq. 6 forms the objective function for training VAEBM. The first term corresponds to the VAE objective and the second term corresponds to training the EBM component. Next, we discuss how we can optimize this objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">TRAINING</head><p>The L EBM (x, ψ, θ) term in Eq. 6 is similar to the EBM training objective except that the log partition function depends on both ψ and θ. We show in Appendix A that log Z ψ,θ has the gradients</p><formula xml:id="formula_6">∂ ψ log Z ψ,θ = E x∼h ψ,θ (x,z) [−∂ ψ E ψ (x)] and ∂ θ log Z ψ,θ = E x∼h ψ,θ (x,z) [∂ θ log p θ (x)]</formula><p>. The first gradient can be estimated easily by evaluating the gradient of the energy function at samples drawn from the VAEBM model h ψ,θ (x, z) using MCMC. However, the second term involves computing the intractable ∂ ∂θ log p θ (x). In Appendix A, we show that estimating ∂ ∂θ log p θ (x) requires sampling from the VAE's posterior distribution, given model samples x ∼ h ψ,θ (x, z). To avoid the computational complexity of estimating this term, for example with a second round of MCMC, we propose a two-stage algorithm for training VAEBM. In the first stage, we train the VAE model in our VAEBM by maximizing the L vae (x, θ, φ) term in Eq. 6. This term is identical to the VAE's objective, thus, the parameters θ and φ are trained using the reparameterized trick as in Sec. 2. In the second stage, we keep the VAE model fixed and only train the EBM component. Since θ is now fixed, we only require optimizing L EBM (x, ψ, θ) w.r.t. ψ, the parameters of the energy function. The gradient of</p><formula xml:id="formula_7">L(ψ) = E x∼p d [L EBM (x, ψ, θ)] w.r.t. ψ is: ∂ ψ L(ψ) = E x∼p d (x) [−∂ ψ E ψ (x)] + E x∼h ψ,θ (x,z) [∂ ψ E ψ (x)] ,<label>(7)</label></formula><p>which decomposes into a positive and a negative phase, as discussed in Sec. 2.</p><p>Reparametrized sampling in the negative phase: For gradient estimation in the negative phase, we can draw samples from the model using MCMC. Naively, we can perform ancestral sampling, first sampling from the prior p θ (z), then running MCMC for p θ (x|z)e −E ψ (x) in x-space. This is problematic, since p θ (x|z) is often sharp and MCMC cannot mix when the conditioning z is fixed.</p><p>In this work, we instead run the MCMC iterations in the joint space of z and x. Furthermore, we accelerate the sampling procedure using reparametrization for both x and the latent variables z. Recall that when sampling from the VAE, we first sample z ∼ p(z) and then x ∼ p θ (x|z). This sampling scheme can be reparametrized by sampling from a fixed noise distribution (e.g., ( z , x ) ∼ p = N (0, I)) and deterministic transformations T θ such that</p><formula xml:id="formula_8">z = T z θ ( z ), x = T x θ (z( z ), x ) = T x θ (T z θ ( z ), x ).</formula><p>(8) Here, T z θ denotes the transformation defined by the prior that transforms noise z into prior samples z and T x θ represents the decoder that transforms noise x into samples x, given prior samples z. We can apply the same reparameterization when sampling from h ψ,θ (x, z). This corresponds to sampling ( x , z ) from the "base distribution":</p><formula xml:id="formula_9">h ψ,θ ( x , z ) ∝ e −E ψ (T x θ (T z θ ( z ), x)) p ( x , z ) ,<label>(9)</label></formula><p>and then transforming them to x and z via Eq. 8 (see Appendix B for derivation). Note that z and x have the same scale, as p ( x , z ) is a standard Normal distribution, while the scales of x and z can be very different. Thus, running MCMC sampling with this reparameterization in the ( x , z )space has the benefit that we do not need to tune the sampling scheme (e.g., step size in LD) for each variable. This is particularly helpful when z itself has multiple groups, as in our case.</p><p>The advantages of two-stage training: Besides avoiding the difficulties of estimating the full gradient of log Z ψ,θ , two-stage training has additional advantages. As we discussed above, updating ψ is computationally expensive, as each update requires an iterative MCMC procedure to draw samples from the model. The first stage of our training minimizes the distance between the VAE model and the data distribution, and in the second stage, the EBM further reduce the mismatch between the model and the data distribution. As the pre-trained VAE p θ (x) provides a good approximation to p d (x) already, we expect that a relatively small number of expensive updates for training ψ is needed. Moreover, the pre-trained VAE provides a latent space with an effectively lower dimensionality and a smoother distribution than the data distribution, which facilitates more efficient MCMC.</p><p>Alternative extensions: During the training of the energy function, we fix the VAE's parameters. In Appendix C, we discuss a possible extension to our training objective that also updates the VAE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RELATED WORK</head><p>Early variants of EBMs include models whose energy is defined over both data and auxiliary latent variables <ref type="bibr" target="#b59">(Salakhutdinov &amp; Hinton, 2009;</ref><ref type="bibr" target="#b22">Hinton, 2012)</ref>, and models using only data variables <ref type="bibr" target="#b21">(Hinton, 2002;</ref><ref type="bibr" target="#b38">Mnih &amp; Hinton, 2005)</ref>. Their energy functions are simple and they do not scale to high dimensional data. Recently, it was shown that EBMs with deep neural networks as energy function can successfully model complex data such as natural images <ref type="bibr" target="#b10">(Du &amp; Mordatch, 2019;</ref><ref type="bibr" target="#b45">Nijkamp et al., 2019b;</ref><ref type="bibr">a)</ref>. They are trained with maximum likelihood and only model the data variable. Joint EBMs <ref type="bibr" target="#b15">(Grathwohl et al., 2020a;</ref><ref type="bibr" target="#b34">Liu &amp; Abbeel, 2020)</ref> model the joint distribution of data and labels. In contrast, our VAEBM models the joint distribution of data and general latent variables.</p><p>Besides fundamental maximum likelihood training, other techniques to train EBMs exist, such as minimizing F-divergence <ref type="bibr" target="#b80">(Yu et al., 2020a)</ref> or Stein discrepancy <ref type="bibr" target="#b16">(Grathwohl et al., 2020b)</ref>, contrastive estimation <ref type="bibr" target="#b17">(Gutmann &amp; Hyvärinen, 2010;</ref> and denoising score matching <ref type="bibr" target="#b30">(Li et al., 2019)</ref>. Recently, noise contrastive score networks and diffusion models have demonstrated high quality image synthesis <ref type="bibr" target="#b64">(Song &amp; Ermon, 2019;</ref><ref type="bibr" target="#b23">Ho et al., 2020)</ref>. These models are also based on denoising score matching (DSM) <ref type="bibr" target="#b73">(Vincent, 2011)</ref>, but do not parameterize any explicit energy function and instead directly model the vector-valued score function. We view score-based models as alternatives to EBMs trained with maximum likelihood. Although they do not require iterative MCMC during training, they need very long sampling chains to anneal the noise when sampling from the model ( 1000 steps). Therefore, sample generation is extremely slow.</p><p>VAEBM is an EBM with a VAE component, and it shares similarities with work that builds connections between EBMs and other generative models. <ref type="bibr" target="#b82">Zhao et al. (2017)</ref>; Che et al. <ref type="formula" target="#formula_1">(2020)</ref>; ; Arbel et al. <ref type="formula" target="#formula_1">(2020)</ref> formulate EBMs with GANs, and use the discriminator to assign an energy. <ref type="bibr" target="#b76">Xiao et al. (2020)</ref>;  use normalizing flows that transport complex data to latent variables to facilitate MCMC sampling <ref type="bibr" target="#b24">(Hoffman et al., 2019)</ref>, and thus, their methods can be viewed as EBMs with flow component. However, due to their topology-preserving nature, normalizing flows cannot easily transport complex multimodal data, and their sample quality on images is limited. A few previous works combine VAEs and EBMs in different ways from ours.  and <ref type="bibr" target="#b71">Vahdat et al. (2018b;</ref><ref type="bibr">a;</ref> use EBMs for the prior distribution, and  jointly learn a VAE and an EBM with independent sets of parameters by an adversarial game.</p><p>Finally, as we propose two-stage training, our work is related to post training of VAEs. Previous work in this direction learns the latent structure of pre-trained VAEs <ref type="bibr" target="#b6">(Dai &amp; Wipf, 2019;</ref><ref type="bibr" target="#b75">Xiao et al., 2019;</ref><ref type="bibr" target="#b13">Ghosh et al., 2020)</ref>, and sampling from learned latent distributions improves sample quality. These methods cannot easily be extended to VAEs with hierarchical latent variables, as it is difficult to fit the joint distribution of multiple groups of variables. Our purpose for two-stage training is fundamentally different: we post-train an energy function to refine the distribution in data space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>In this section, we evaluate our proposed VAEBM through comprehensive experiments. Specifically, we benchmark sample quality in Sec. 5.1, provide detailed ablation studies on training techniques in Sec. 5.2, and study mode coverage of our model and test for spurious modes in Sec. 5.3. We choose NVAE <ref type="bibr">(Vahdat &amp; Kautz, 2020)</ref> as our VAE, which we pre-train, and use a simple ResNet as energy function E ψ , similar to <ref type="bibr" target="#b10">Du &amp; Mordatch (2019)</ref>. We draw approximate samples both for training and testing by running short Langevin dynamics chains on the distribution in Eq. 9. Note that in NVAE, the prior distribution is a group-wise auto-regressive Gaussian, and the conditional pixel-wise distributions in x are also Gaussian. Therefore, the reparameterization corresponds to shift and scale transformations. For implementation details, please refer to Appendix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">IMAGE GENERATION</head><p>In <ref type="table" target="#tab_0">Table 1</ref>, we quantitatively compare the sample quality of VAEBM with different generative models on (unconditional) CIFAR-10. We adopt Inception Score (IS)  and FID <ref type="bibr" target="#b20">(Heusel et al., 2017)</ref> as quantitative metrics. Note that FID reflects the sample quality more faithfully, as potential problems have been reported for IS on CIFAR-10 ( <ref type="bibr" target="#b2">Barratt &amp; Sharma, 2018)</ref>.</p><p>We observe that our VAEBM outperforms previous EBMs and other explicit likelihood-based models by a large margin. Note that introducing persistent chains during training only leads to slight improvement, while <ref type="bibr" target="#b10">Du &amp; Mordatch (2019)</ref> rely on persistent chains with a sample replay buffer. This is likely due to the efficiency of sampling in latent space. Our model also produces significantly better samples than NVAE, the VAE component of our VAEBM, implying a significant impact of our proposed energy-based refinement. We also compare our model with state-of-the-art GANs and   9.11 16.24 BigGAN <ref type="bibr" target="#b3">(Brock et al., 2018)</ref> 9.22 14.73 StyleGAN2 w/o ADA <ref type="bibr">(Karras et al., 2020a)</ref> 8.99 9.9</p><p>Others PixelIQN <ref type="bibr" target="#b50">(Ostrovski et al., 2018)</ref> 5.29 49.46 MoLM <ref type="bibr" target="#b55">(Ravuri et al., 2018)</ref> 7.90 18.9</p><p>recently proposed score-based models, and we obtain comparable or better results. Thus, we largely close the gap to GANs and score-models, while maintaining the desirable properties of models trained with maximum likelihood, such as fast sampling and better mode coverage.</p><p>Qualitative samples generated by our model are shown in <ref type="figure" target="#fig_0">Fig. 2a</ref> and intermediate samples along MCMC chains in <ref type="figure" target="#fig_0">Fig. 2b</ref>. We find that VAEBM generates good samples by running only a few MCMC steps. Initializing MCMC chains from the pre-trained VAE also helps quick equilibration. We also train VAEBM on larger images, including CelebA 64, CelebA HQ 256 <ref type="bibr" target="#b36">(Liu et al., 2015)</ref> and LSUN Church 64 <ref type="bibr" target="#b79">(Yu et al., 2015)</ref>. We report the FID scores for CelebA 64 and CelebA HQ 256 in <ref type="table" target="#tab_2">Tables 2 and 3</ref>  <ref type="table" target="#tab_3">Table 3</ref> is computed with full temperature samples.  reduce the gap between likelihood based models and GANs on this dataset. On LSUN Church 64, we obtain FID 13.51, which significantly improves the NVAE baseline FID 41.3. We show qualitative samples in <ref type="figure">Fig. 3</ref>. Appendix H contains additional samples and MCMC visualizations.</p><p>Our model can produce impressive samples by running very short MCMC chains, however, we find that when we run longer MCMC chains than training chains, most chains stay around the local mode without traversing between modes. We believe that the non-mixing is due to the long mixing time of Langevin Dynamics Neal et al. <ref type="formula" target="#formula_0">(2011)</ref>, as <ref type="bibr" target="#b45">Nijkamp et al. (2019b;</ref><ref type="bibr">a)</ref> also observe that models trained with short-run MCMC have non-mixing long-run chains. We conjecture that mixing can be improved by training and sampling with more advanced MCMC techniques that are known to mix faster, such as HMC Neal et al. <ref type="bibr">(2011)</ref>, and this will be left for future work.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">ABLATION STUDIES</head><p>In <ref type="table" target="#tab_4">Table 4</ref>, we compare VAEBM to several closely related baselines. All the experiments here are performed on CIFAR-10, and for simplicity, we use smaller models than those used in <ref type="table" target="#tab_0">Table 1</ref>. Appendix F summarizes the experimental settings and Appendix G provides qualitative samples.</p><p>Data space vs. augmented space: One key difference between VAEBM and previous work such as <ref type="bibr" target="#b10">Du &amp; Mordatch (2019)</ref> is that our model is defined on the augmented space (x, z), while their EBM only involves x. Since we pre-train the VAE, one natural question is whether our strong results are due to good initial samples x from the VAE, which are used to launch the MCMC chains. To address this, we train an EBM purely on x as done in <ref type="bibr" target="#b10">Du &amp; Mordatch (2019)</ref>. We also train another EBM only on x, but we initialize the MCMC chains with samples from the pre-trained NVAE instead of noise. As shown in line 3 of <ref type="table" target="#tab_4">Table 4</ref>, this initialization helps the EBM which is defined only on x. However, VAEBM in the augmented space outperforms the EBMs on x only by a large margin.</p><p>Adversarial training vs. sampling: The gradient for ψ in Eq. 7 is similar to the gradient updates of WGAN's discriminator <ref type="bibr" target="#b1">(Arjovsky et al., 2017)</ref>. The key difference is that we draw (approximate) samples from h ψ (x) by MCMC, while WGAN draws negative samples from a generator <ref type="bibr" target="#b5">(Che et al., 2020)</ref>. WGAN updates the generator by playing an adversarial game, while we only update the energy function E ψ . We compare these two methods by training ψ and θ with the WGAN objective and initializing θ with the NVAE decoder. As shown in line 4 of <ref type="table" target="#tab_4">Table 4</ref>, we significantly outperform the WGAN version of our model, implying the advantage of our method over adversarial training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">TEST FOR SPURIOUS OR MISSING MODES</head><p>We evaluate mode coverage on StackedMNIST. This dataset contains images generated by randomly choosing 3 MNIST images and stacking them along the RGB channels. Hence, the data distribution has 1000 modes. Following <ref type="bibr" target="#b33">Lin et al. (2018)</ref>, we report the number of covered modes and the KL divergence from the categorical distribution over 1000 categories from generated samples to true data <ref type="table" target="#tab_5">(Table 5</ref>). VAEBM covers all modes and achieves the lowest KL divergence even compared to GANs that are specifically designed for this task. Hence, our model covers the modes more equally.</p><p>We also plot the histogram of likelihoods for CIFAR-10 train/test images <ref type="figure" target="#fig_3">(Fig. 6</ref>, Appendix D) and present nearest neighbors of generated samples (Appendix I). We conclude that we do not overfit.</p><p>We evaluate spurious modes in our model by assessing its performance on out-of-distribution (OOD) detection. Specifically, we use VAEBM trained on CIFAR-10, and estimate unnormalized log h ψ,θ (x) on in-distribution samples (from CIFAR-10 test set) and OOD samples from several datasets. Following <ref type="bibr" target="#b40">Nalisnick et al. (2019)</ref>, we use area under the ROC curve (AUROC) as quantitative metric, where high AUROC indicates that the model correctly assigns low density to OOD samples. In <ref type="table" target="#tab_6">Table 6</ref>, we see that VAEBM has significantly higher AUROC than NVAE, justifying our argument that the energy function reduces the likelihood of non-data-like regions. VAEBM also performs better than IGEBM and JEM, while worse than HDGE. However, we note that JEM and HDGE are classifier-based models, known to be better for OOD detection <ref type="bibr" target="#b31">(Liang et al., 2018)</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">EXACT LIKELIHOOD ESTIMATE ON 2D TOY DATA</head><p>VAEBM is an explicit likelihood model with a parameterized density function. However, like other energy-based models, the estimation of the exact likelihood is difficult due to the intractable partition function log Z. One possible way to estimate the partition function is to use Annealed Importance Sampling (AIS) <ref type="bibr" target="#b42">(Neal, 2001)</ref>. However, using AIS to estimate log Z in high-dimensional spaces is difficult. In fact, <ref type="bibr" target="#b10">Du &amp; Mordatch (2019)</ref> report that the estimation does not converge in 2 days on CIFAR-10. Furthermore, AIS gives a stochastic lower bound on log Z, and therefore the likelihood computed with this estimated log Z would be an upper bound for the true likelihood. This makes the estimated likelihood hard to compare with the VAE's likelihood estimate, which is usually a lower bound on the true likelihood <ref type="bibr" target="#b4">(Burda et al., 2015)</ref>.</p><p>As a result, to illustrate that our model corrects the distribution learned by the VAE and improves the test likelihood, we conduct additional experiments on a 2-D toy dataset. We use the 25-Gaussians dataset, which is generated by a mixture of 25 two-dimensional isotropic Gaussian distributions arranged in a grid. This dataset is also studied in Che et al. <ref type="bibr">(2020)</ref>. The encoder and decoder of the VAE have 4 fully connected layers with 256 hidden units, and the dimension of the latent variables is 20. Our energy function has 4 fully connected layers with 256 hidden units.</p><p>In the 2-D domain, the partition function log Z can be accurately estimated by a numerical integration scheme. For the VAE, we use the IWAE bound <ref type="bibr" target="#b4">(Burda et al., 2015)</ref> with 10,000 posterior samples to estimate its likelihood. We use 100,000 test samples from the true distribution to evaluate the likelihood. Our VAEBM obtains the average log likelihood of -1.50 nats on test samples, which significantly improves the VAE, whose average test likelihood is -2.97 nats. As a reference, we also analytically compute the log likelihood of test samples under the true distribution, and the result is -1.10 nats.</p><p>We show samples from the true distribution, VAE and VAEBM in <ref type="figure">Figure 4</ref>. We observe that the VAEBM successfully corrects the distribution learned by the VAE and has better sample quality. Despite their impressive sample quality, denoising score matching models <ref type="bibr" target="#b64">(Song &amp; Ermon, 2019;</ref><ref type="bibr" target="#b23">Ho et al., 2020)</ref>  </p><formula xml:id="formula_10">∂ ∂θ log Z ψ,θ = ∂ ∂θ log p θ (x)e −E ψ (x) dx = 1 Z ψ,θ ∂p θ (x) ∂θ e −E ψ (x) dx = 1 Z ψ,θ p θ (x)e −E ψ (x) ∂ log p θ (x) ∂θ dx = h ψ,θ (x) ∂ log p θ (x) ∂θ dx = E x∼h ψ,θ (x,z) ∂ log p θ (x) ∂θ (10) Similarly, it is easy to show that ∂ ∂ψ log Z ψ,θ = E x∼h ψ,θ (x,z) − ∂E ψ (x) ∂ψ</formula><p>. Intuitively, both gradients encourage reducing the likelihood of the samples generated by the VAEBM model. Since, h ψ,θ is an EBM, the expectation can be approximated using MCMC samples.</p><p>Note that Eq. 10 is further expanded to:</p><formula xml:id="formula_11">∂ ∂θ log Z ψ,θ = E x∼h ψ,θ (x,z) E z ∼p θ (z |x) ∂ log p θ (x, z ) ∂θ ,</formula><p>which can be approximated by first sampling from VAEBM using MCMC (i.e., x ∼ h ψ,θ (x, z)) and then sampling from the true posterior of the VAE (i.e., z ∼ p θ (z |x)). The gradient term can be easily computed given the samples. Two approaches can be used to draw approximate samples from p θ (z |x). i) We can replace p θ (z |x) with the approximate posterior q φ (z |x). However, the quality of this estimation depends on how well q φ (z |x) matches the true posterior on samples generated by h ψ,θ (x, z), which can be very different from the real data samples. To bring q φ (z |x) closer to p θ (z |x), we can maximize the variational bound (Eq. 3) on samples generated from h ψ,θ (x, z) with respect to φ, the encoder parameters 2 . However, this will add additional complexity to training. ii) Alternatively, we can use MCMC sampling to sample z ∼ p θ (z |x). To speed up MCMC, we can initialize the z samples in MCMC with the original z samples that were drawn in the outer expectation (i.e., x, z ∼ h ψ,θ (x, z)). However, with this approach, the computational complexity of the gradient estimation for the negative phase is doubled, as we now require running MCMC twice, once for x, z ∼ h ψ,θ (x, z) and again for z ∼ p θ (z |x).</p><p>We can entirely avoid the additional computational complexity and the complications of estimating ∂ ∂θ log Z ψ,θ , if we assume that the VAE is held fixed when training the EBM component of our VAEBM. This way, we require running MCMC only to sample x ∼ h ψ,θ (x, z) to compute</p><p>Recall that the generative model of our EBM is</p><formula xml:id="formula_12">h ψ,θ (x, z) = e −E ψ (x) p θ (x, z) Z ψ,θ .<label>(13)</label></formula><p>We can apply the change of variable to h ψ,θ (x, z) in similar manner:</p><formula xml:id="formula_13">h ψ,θ ( x , z ) = h ψ,θ (T θ ( x , z )) |det (J T θ ( x , z ))| ,<label>(14)</label></formula><p>where J T θ is the Jacobian of T θ .</p><p>Since we have the relation</p><formula xml:id="formula_14">J f −1 • f = J −1 f<label>(15)</label></formula><p>for invertible function f , we have that</p><formula xml:id="formula_15">h ψ,θ ( x , z ) = h ψ,θ (T θ ( x , z )) |det (J T θ ( z , x ))| (16) = 1 Z ψ,θ e −E ψ (T θ ( x, z)) p θ (T θ ( x , z )) |det (J T θ ( x , z ))| (17) = 1 Z ψ,θ e −E ψ (T θ ( x, z)) p (T −1 θ (x, z)) det J T −1 θ (x, z) det (J T θ ( x , z )) (18) = 1 Z ψ,θ e −E ψ (T θ ( x, z)) p (T −1 θ (x, z)) (19) = 1 Z ψ,θ e −E ψ (T θ ( x, z)) p ( x , z ),<label>(20)</label></formula><p>which is the distribution in Eq. 9.</p><p>After we obtained samples ( x , z ) from the distribution in Eq. 20, we obtain (x, z) by applying the transformation T θ in Eq. 11.</p><formula xml:id="formula_16">B.1 COMPARISON OF SAMPLING IN ( x , z )-SPACE AND IN (x, z)-SPACE</formula><p>Above we show that sampling from h ψ,θ (x, z) is equivalent to sampling from h ψ,θ ( x , z ) and applying the appropriate variable transformation. Here, we further analyze the connections between sampling from these two distributions with Langevin dynamics. Since each component of x and z can be re-parametrzied with scaling and translation of standard Gaussian noise, without loss of generality, we assume a variable c (c can be a single latent variable in z or a single pixel in x) and write c = µ + σ .</p><p>Suppose we sample in the space with energy function f on c and step size η. The update for is</p><formula xml:id="formula_17">t+1 = t − η 2 ∇ f + √ ηω t , ω t ∼ N (0, I).</formula><p>Now we plug t+1 into the expression of c while noting that ∇ f = σ∇ c f . We obtain</p><formula xml:id="formula_18">c t+1 = µ + σ t+1 = µ + σ t − η 2 ∇ f + √ ηω t = µ + σ t − σ 2 η 2 ∇ c f + ησ 2 ω t = c t − σ 2 η 2 ∇ c f + ησ 2 ω t .</formula><p>Therefore, we see that running Langevin dynamics in ( x , z )-space is equivalent to running Langevin dynamics in (x, z)-space with step size for each component of z and x adjusted by its variance. However, considering the high dimensionality of x and z, the step size adjustment is difficult to implement.</p><p>The analysis above only considers a variable individually. More importantly, our latent variable z in the prior follows block-wise auto-regressive Gaussian distributions, so the variance of each component in z i depends on the value of z &lt;i . We foresee that because of this dependency, using a fixed step size per component of z will not be effective, even when it is set differently for each component. In contrast, all the components in ( x , z )-space have a unit variance. Hence, a universal step size for all the variables in this space can be used.</p><p>To further provide empirical evidence that adjusting the step size for each variable is necessary, we try sampling directly in (x, z)-space without adjusting the step size (i.e., use a universal step size for all variables). Qualitative results are presented in <ref type="figure" target="#fig_2">Figure 5</ref>. We examine several choices for the step size and we cannot obtain high-quality samples. In conclusion, the re-parameterization provides an easy implementation to adjust step size for each variable, and the adjustment is shown to be crucial to obtain good samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C EXTENSION TO TRAINING OBJECTIVE</head><p>In the first stage of training VAEBM, the VAE model is trained by maximizing the training data log-likelihood which corresponds to minimizing an upper bound on D KL (p d (x)||p θ (x)) w.r.t. θ.</p><p>In the second stage, when we are training the EBM component, we use the VAE model to sample from the joint VAEBM by running the MCMC updates in the joint space of z and x . Ideally, we may want to bring p θ (x) closer to h ψ,θ (x) in the second stage, because when p θ (x) = h ψ,θ (x), we will not need the expensive updates for ψ. We can bring p θ (x) closer to h ψ,θ (x) by minimizing D KL (p θ (x)||h ψ,θ (x)) with respect to θ which was recently discussed in the context of an EBMinterpretation of GANs by <ref type="bibr" target="#b5">Che et al. (2020)</ref>. To do so, we assume the target distribution h ψ,θ (x) is fixed and create a copy of θ, named θ , and we update θ by the gradient:</p><formula xml:id="formula_19">∇ θ D KL (p θ (x)||h ψ,θ (x)) = ∇ θ E x∼p θ (x) [E ψ (x)]<label>(21)</label></formula><p>In other words, one update step for θ that minimizes D KL (p θ (x)||h ψ,θ (x)) w.r.t. θ can be easily done by drawing samples from p θ (x) and minimizing the energy-function w.r.t. θ . Note that this approach is similar to the generator update in training Wasserstein GANs <ref type="bibr" target="#b1">(Arjovsky et al., 2017)</ref>. The above KL objective will encourage p θ (x) to model dominants modes in h ψ,θ (x). However, it may cause p θ (x) to drop modes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 DERIVATION</head><p>Our derivation largely follows Appendix A.2 of <ref type="bibr" target="#b5">Che et al. (2020)</ref>. Note that every time we update θ, we are actually taking the gradient w.r.t θ , which can be viewed as a copy of θ and is initialized as θ. In particular, we should note that the θ in h ψ,θ (x) is fixed. Therefore, we have</p><formula xml:id="formula_20">∇ θ D KL (p θ (x)||h ψ,θ (x)) = ∇ θ p θ (x) [log p θ (x) − log h ψ,θ (x)] dx = [∇ θ p θ (x)] [log p θ (x) − log h ψ,θ (x)] dx + p θ (x) [∇ θ log p θ (x) − ∇ θ log h ψ,θ (x)] dx =0 (22) = [∇ θ p θ (x)] [log p θ (x) − log h ψ,θ (x)] dx,<label>(23)</label></formula><p>where the second term in Eq. 22 is 0 because the log h ψ,θ (x) does not depend on θ and the expectation of the score function is 0:</p><formula xml:id="formula_21">p θ (x)∇ θ log p θ (x)dx = E x∼p θ (x) [∇ θ log p θ (x)] = 0.</formula><p>Recall that θ has the same value as θ before the update, so</p><formula xml:id="formula_22">log p θ (x) − log h ψ,θ (x) = log p θ (x) p θ (x)e −E ψ (x) + log Z ψ,θ = E ψ (x) + log Z ψ,θ .<label>(24)</label></formula><p>Plug Eq. 24 into Eq. 23, we have</p><formula xml:id="formula_23">∇ θ D KL (p θ (x)||h ψ,θ (x)) = ∇ θ p θ (x) [E ψ (x) + log Z ψ,θ ] dx = ∇ θ E x∼p θ (x) [E ψ (x)] ,<label>(25)</label></formula><formula xml:id="formula_24">since ∇ θ p θ (x) log Z ψ,θ dx = ∇ θ log Z ψ,θ p θ (x)dx = ∇ θ log Z ψ,θ = 0.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 RESULTS</head><p>We train VAEBM with an additional loss term that updates the parameter θ to minimize D KL (p θ (x)||h ψ,θ (x)) as explained above. Our experiment uses the same initial VAE as in Sec. 5.2, and details of the implementation are introduced in Appendix F. We obtain FID 14.0 and IS 8.05, which is similar to the results of plain VAEBM <ref type="bibr">(FID 12.96 and IS 8.15</ref>). Therefore, we conclude that training the model by minimizing D KL (p θ (x)||h ψ,θ (x)) does not improve the performance, and updating the decoder is not necessary. This is likely because the initial VAE is pulled as closely as possible to the data distribution already, which is also the target for the joint VAEBM h ψ,θ (x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D COMPARING LIKELIHOODS ON TRAIN AND TEST SET</head><p>In <ref type="figure" target="#fig_3">Figure 6</ref>, we plot a histogram of unnormalized log-likelihoods of 10k CIFAR-10 train set and test set images. We see that our model assigns similar likelihoods to both train and test set images. This indicates that VAEBM generalizes well to unseen data and covers modes in the training data well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E IMPLEMENTATION DETAILS</head><p>In this section, we introduce the details of training and sampling from VAEBM. NVAE: VAEBM uses NVAE as the p θ (x) component in the model. We train the NVAE with its official implementation 3 . We largely follow the default settings, with one major difference that we use a Gaussian decoder instead of a discrete logistic mixture decoder as in <ref type="bibr">Vahdat &amp; Kautz (2020)</ref>.</p><p>The reason for this is that we can run Langevin dynamics only with continuous variables. The number of latent variable groups for CIFAR-10, CelebA 64, <ref type="bibr">LSUN Church 64 and CelebA HQ 256 are 30,</ref><ref type="bibr">15,</ref><ref type="bibr">15 and 20,</ref><ref type="bibr">respectively.</ref> it helpful to regularize the sharpness of the energy network and better stabilize training. We use a coefficient 0.2 for the spectral regularization loss. We summarize some key hyper-parameters we used to train VAEBM in <ref type="table" target="#tab_8">Table 8</ref>.</p><p>On all datasets, we train VAEBM using the Adam optimizer <ref type="bibr">(Kingma &amp; Ba, 2015)</ref> and weight decay 3e−5. We use constant learning rates, shown in <ref type="table" target="#tab_8">Table 8</ref>. Following <ref type="bibr" target="#b10">Du &amp; Mordatch (2019)</ref>, we clip training gradients that are more than 3 standard deviations from the 2nd-order Adam parameters.</p><p>While persistent sampling using a sample replay buffer has little effect on CIFAR-10, we found it to be useful on large images such as CelebA HQ 256. When we do not use persistent sampling, we always initialize the LD chains with ( x , z ), sampled from a standard Gaussian. When we use persistent sampling in training, we keep a sample replay buffer that only stores samples of z , while x is always initialized from a standard Gaussian. The size of the replay buffer is 10,000 for CIFAR-10 and LSUN Church 64, and 8,000 for CelebA HQ 256. At every training iteration, we initialize the MCMC chains on z by drawing z from the replay buffer with probability p and from standard Gaussian with probability 1 − p. For CIFAR-10 and LSUN Church 64, we linearly increase p from 0 to 0.6 in 5,000 training iterations, and for CelebA HQ 256, we linearly increase p from 0 to 0.6 in 3,000 training iterations. The settings of Langevin dynamics are presented in <ref type="table" target="#tab_8">Table 8</ref>.</p><p>We do not explicitly set the number of training iterations. Instead, we follow <ref type="bibr" target="#b10">Du &amp; Mordatch (2019)</ref> to train the energy network until we cannot generate realistic samples anymore. This happens when the model overfits the training data and hence energies of negative samples are much larger than energies of training data. Typically, training takes around 25,000 iterations (or 16 epochs) on CIFAR-10, 20,000 iterations (or 3 epochs) on CelebA 64, 20,000 iterations (or 5 epochs) on LSUN Church 64, and 9,000 iterations (or 5 epochs) on CelebA HQ 256.</p><p>Test time sampling: After training the model, we generate samples for evaluation by running Langvin dynamics with ( x , z ) initialized from standard Gaussian, regardless of whether persistent sampling is used in training or not. We run slightly longer LD chains than training to obtain the best sample quality. In particular, our reported values are obtained from running 16 steps of LD for CIFAR-10, 20 steps of LD for CelebA64 and LSUN Church 64, and 24 steps for CelebA HQ 256. The step sizes are the same as training step sizes.</p><p>In CelebA HQ 256 dataset, we optionally use low temperature initialization for better visual quality.</p><p>To do this, we first draw samples from the VAE with low temperature and readjusted the BN statistics as introduced by <ref type="bibr">Vahdat &amp; Kautz (2020)</ref>, and then initialize the MCMC chain by ( x , z ) obtained by encoding the low-temperature samples using VAE's encoder without readjusted BN statistics.</p><p>Evaluation metrics: We use the official implementations of FID 4 and IS 5 . We compute IS using 50k CIFAR 10 samples, and we compute FID between 50k generated samples and training images, except for CelebA HQ 256 where we use 30k training images (the CelebA HQ dataset contains only 30k samples).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F SETTINGS FOR ABLATION STUDY</head><p>In this section, we present the details of ablation experiments in Sec. 5.2. Throughout ablation experiments, we use a smaller NVAE with 20 groups of latent variables trained on CIFAR-10. We use the same network architectures for the energy network as in <ref type="table">Table 7</ref>, with potentially different normalization techniques discussed below. We spent significant efforts on improving each method we compare against, and we report the settings that led to the best results.</p><p>WGAN initialized with NVAE decoder: We initialize the generator with the pre-trained NVAE decoder, and the discriminator is initialized by a CIFAR-10 energy network with random weights. We use spectral normalization and batch normalization in the discriminator as we found them necessary for convergence. We update the discriminator using the Adam optimizer with constant learning rate 5e−5, and update the generator using the Adam optimizer with initial learning rate 5e−6 and cosine decay schedule. We train the generator and discriminator for 40k iterations, and we reach convergence of sample quality towards the end of training.</p><p>EBM on x, w/ or w/o initializing MCMC with NVAE samples: We train two EBMs on data space similar to <ref type="bibr" target="#b10">Du &amp; Mordatch (2019)</ref>, where for one of them, we use the pre-trained NVAE to initialize the MCMC chains that draw samples during training. The setting for training these two EBMs are the same except for the initialization of MCMC. We use spectral normalization in the energy network and energy regularization in the training objective as done in <ref type="bibr" target="#b10">Du &amp; Mordatch (2019)</ref> because we found these modifications to improve performance. We train the energy function using the Adam optimizer with constant learning rate 1e−4. We train for 100k iterations, and we reach convergence of sample quality towards the end of training. During training, we draw samples from the model following the MCMC settings in <ref type="bibr" target="#b10">Du &amp; Mordatch (2019)</ref>. In particular, we use persistent sampling and sample from the sample replay buffer with probability 0.95. We run 60 steps of Langevin dynamics to generate negative samples and we clip gradients to have individual value magnitudes of less than 0.01. We use a step size of 10 for each step of Langevin dynamics. For test time sampling, we generate samples by running 150 steps of LD with the same settings as during training.</p><p>VAEBM with D KL (p θ (x)||h ψ,θ (x)) loss: We use the same network structure for E ψ as in VAEBM. We find persistent sampling significantly hurts the performance in this case, possibly due to the fact that the decoder is updated and hence the initial samples from the decoder change throughout training. Therefore, we do not use persistent training. We train the energy function using the Adam optimizer with constant learning rate 5e−5. We draw negative samples by running 10 steps of LD with step size 8e−5. We update the decoder with the gradient in Eq. 21 using the Adam optimizer with initial learning rate 5e−6 and cosine decay schedule. For test time sampling, we run 15 steps of LD with step size 5e−6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VAEBM:</head><p>The training of VAEBM in this section largely follows the settings described in Appendix E. We use the same energy network as for CIFAR-10, and we train using the Adam optimizer with constant learning rate 5e−5. Again, we found that the performance of VAEBM with or without persistent sampling is similar. We adopt persistent sampling in this section because it is faster. The setting for the buffer is the same as in Appendix E. We run 5 steps of LD with step size 8e−5 during training, and 15 steps of LD with the same step size in testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G QUALITATIVE RESULTS OF ABLATION STUDY</head><p>In <ref type="figure" target="#fig_4">Figure 7</ref>, we show qualitative samples from models corresponding to each item in <ref type="table" target="#tab_4">Table 4</ref>, as well as samples generated by VAEBM with additional D KL (p θ (x)||h ψ,θ (x)) loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H ADDITIONAL QUALITATIVE RESULTS</head><p>We present additional qualitative results in this section.</p><p>Additional samples and visualizations of MCMC on CIFAR-10 are in <ref type="figure">Figures 8 and 9</ref>, respectively.</p><p>Additional samples on CelebA 64 are in <ref type="figure">Figure 10</ref>.</p><p>Additional samples on LSUN Church 64 are in <ref type="figure">Figure 11</ref>. We visualize the effect of running MCMC by displaying sample pairs before and after MCMC in <ref type="figure" target="#fig_0">Figure 12</ref>.</p><p>Additional samples on CelebA HQ 256 generated by initializing VAE samples with temperature 0.7 are shown in <ref type="figure">Figure 13</ref>. Samples generated by initializing VAE samples with full temperature 1.0 are shown in <ref type="figure">Figure 14</ref>. We visualize the effect of running MCMC by displaying sample pairs We show nearest neighbors in the training set with generated samples on CIFAR-10 (in <ref type="figure" target="#fig_3">Figure 16</ref> and 17) and CelebA HQ 256 (in <ref type="figure" target="#fig_9">Figure 18 and 19)</ref>. We observe that the nearest neighbors are significantly different from the samples, suggesting that our models generalize well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J SETTINGS OF SAMPLING SPEED EXPERIMENT</head><p>We use the official implementation and checkpoints of NCSN at https://github.com/ ermongroup/ncsn. We run the experiments on a computer with a Titan RTX GPU. We use PyTorch 1.5.0 and CUDA 10.2.      </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>(a) CIFAR-10 samples generated by VAEBM. (b) Visualizing MCMC sampling chains. Samples are generated by running 16 LD steps. Chains are initialized with pre-trained VAE. We show intermediate samples at every 2 steps. See Appendix H for additional qualitative results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 4: Qualitative results on the 25-Gaussians dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Qualitative samples obtained from sampling in (x, z)-space with different step sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Histogram of unnormalized log-likelihoods on 10k CIFAR-10 train and test set images</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :Figure</head><label>7</label><figDesc>WGAN, initialized with NVAE decoder (c) EBM on x, MCMC initialized with NVAE samples (d) VAEBM with DKL(p θ (x)||h ψ,θ (x)) loss (e) VAEBM Qualitative results of ablation study in Sec. 5.2. and Appendix C before and after MCMC in Figure 15. Note that the samples used to visualize MCMC are generated by initializing MCMC chains with VAE samples with full temperature 1.0.Published as a conference paper at ICLR 2021</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 12 :</head><label>12</label><figDesc>Visualizing the effect of MCMC sampling on LSUN Church 64 dataset. For each subfigure, the top row contains initial samples from the VAE, and the bottom row contains corresponding samples after MCMC. We observe that MCMC sampling fixes the corrupted initial samples and refines the details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 15 :</head><label>15</label><figDesc>Visualizing the effect of MCMC sampling on CelebA HQ 256 dataset. Samples are generated by initializing MCMC with full temperature VAE samples. MCMC sampling fixes the artifacts of VAE samples, especially on hairs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 16 :</head><label>16</label><figDesc>CIFAR-10 nearest neighbors in pixel distance. Generated samples are in the leftmost column, and training set nearest neighbors are in the remaining columns.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 17 :</head><label>17</label><figDesc>CIFAR-10 nearest neighbors in Inception feature distance. Generated samples are in the leftmost column, and training set nearest neighbors are in the remaining columns.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 18 :</head><label>18</label><figDesc>CelebA HQ 256 nearest neighbors in pixel distance, computed on a 160 × 160 center crop to focus more on faces rather than backgrounds. Generated samples are in the leftmost column, and training set nearest neighbors are in the remaining columns.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 19 :</head><label>19</label><figDesc>CelebA HQ 256 nearest neighbors in Inception feature distance, computed on a 160×160 center crop. Generated samples are in the leftmost column, and training set nearest neighbors are in the remaining columns.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>IS and FID scores for unconditional generation on CIFAR-10.</figDesc><table><row><cell></cell><cell>Model</cell><cell>IS↑</cell><cell>FID↓</cell></row><row><cell>Ours</cell><cell>VAEBM w/o persistent chain VAEBM w/ persistent chain</cell><cell cols="2">8.21 12.26 8.43 12.19</cell></row><row><cell></cell><cell>IGEBM (Du &amp; Mordatch, 2019)</cell><cell cols="2">6.02 40.58</cell></row><row><cell></cell><cell cols="2">EBM with short-run MCMC (Nijkamp et al., 2019b) 6.21</cell><cell>-</cell></row><row><cell></cell><cell>F-div EBM (Yu et al., 2020a)</cell><cell cols="2">8.61 30.86</cell></row><row><cell>EBMs</cell><cell>FlowCE (Gao et al., 2020)</cell><cell>-</cell><cell>37.3</cell></row><row><cell></cell><cell>FlowEBM (Nijkamp et al., 2020)</cell><cell>-</cell><cell>78.12</cell></row><row><cell></cell><cell>GEBM (Arbel et al., 2020)</cell><cell>-</cell><cell>23.02</cell></row><row><cell></cell><cell>Divergence Triangle (Han et al., 2020)</cell><cell>-</cell><cell>30.1</cell></row><row><cell>Other Likelihood Models</cell><cell>Glow (Kingma &amp; Dhariwal, 2018) PixelCNN (Oord et al., 2016b) NVAE (Vahdat &amp; Kautz, 2020) VAE with EBM prior (Pang et al., 2020)</cell><cell cols="2">3.92 4.60 65.93 48.9 5.51 51.67 -70.15</cell></row><row><cell></cell><cell>NCSN (Song &amp; Ermon, 2019)</cell><cell cols="2">8.87 25.32</cell></row><row><cell>Score-based</cell><cell>NCSN v2 (Song &amp; Ermon, 2020)</cell><cell>-</cell><cell>31.75</cell></row><row><cell>Models</cell><cell>Multi-scale DSM (Li et al., 2019)</cell><cell>8.31</cell><cell>31.7</cell></row><row><cell></cell><cell>Denoising Diffusion (Ho et al., 2020)</cell><cell>9.46</cell><cell>3.17</cell></row><row><cell></cell><cell>SNGAN (Miyato et al., 2018)</cell><cell>8.22</cell><cell>21.7</cell></row><row><cell>GAN-based Models</cell><cell>SNGAN+DDLS (Che et al., 2020) SNGAN+DCD</cell><cell cols="2">9.09 15.42</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>. On CelebA 64, our model obtains results comparable with the best GANs. Although our model obtains worse results than some advanced GANs on CelebA HQ 256, we significantly Qualitative results on CelebA 64, LSUN Church 64 and CelebA HQ 256. For CelebA HQ 256, we initialize the MCMC chains with low temperature NVAE samples (t = 0.7) for better visual quality. On this dataset samples are selected for diversity. See Appendix H for additional qualitative results and uncurated CelebA HQ 256 samples obtained from higher temperature initializations.</figDesc><table><row><cell>(a) CelebA 64</cell><cell>(b) LSUN Church 64</cell></row><row><cell>(c) CelebA HQ 256</cell><cell></cell></row><row><cell>Figure 3:</cell><cell></cell></row></table><note>Note that the FID in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Generative performance on CelebA 64</figDesc><table><row><cell>Model</cell><cell>FID↓</cell></row><row><cell>VAEBM (ours)</cell><cell>5.31</cell></row><row><cell>NVAE (Vahdat &amp; Kautz)</cell><cell>14.74</cell></row><row><cell>Flow CE (Gao et al.)</cell><cell>12.21</cell></row><row><cell>Divergence Triangle (Han et al.)</cell><cell>24.7</cell></row><row><cell>NCSNv2 (Song &amp; Ermon)</cell><cell>26.86</cell></row><row><cell>COCO-GAN (Lin et al.)</cell><cell>4.0</cell></row><row><cell cols="2">QA-GAN (Parimala &amp; Channappayya) 6.42</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Generative performance on CelebA HQ 256</figDesc><table><row><cell>Model</cell><cell>FID↓</cell></row><row><cell>VAEBM (ours)</cell><cell>20.38</cell></row><row><cell>NVAE (Vahdat &amp; Kautz)</cell><cell>45.11</cell></row><row><cell>GLOW (Kingma &amp; Dhariwal)</cell><cell>68.93</cell></row><row><cell cols="2">Advers. LAE (Pidhorskyi et al.) 19.21</cell></row><row><cell>PGGAN (Karras et al.)</cell><cell>8.03</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparison for IS and FID on CIFAR-10 between several related training methods.</figDesc><table><row><cell>Model</cell><cell>IS↑</cell><cell>FID↓</cell></row><row><cell>NVAE (Vahdat &amp; Kautz)</cell><cell cols="2">5.19 55.97</cell></row><row><cell>EBM on x (Du &amp; Mordatch)</cell><cell cols="2">5.85 48.89</cell></row><row><cell cols="3">EBM on x, MCMC init w/ NVAE 7.28 29.32</cell></row><row><cell>WGAN w/ NVAE decoder</cell><cell cols="2">7.41 20.39</cell></row><row><cell>VAEBM (ours)</cell><cell cols="2">8.15 12.96</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Mode coverage on StackedMNIST.</figDesc><table><row><cell>Model</cell><cell cols="2">Modes↑ KL↓</cell></row><row><cell cols="2">VEEGAN (Srivastava et al.) 761.8</cell><cell>2.173</cell></row><row><cell>PacGAN (Lin et al.)</cell><cell>992.0</cell><cell>0.277</cell></row><row><cell>PresGAN (Dieng et al.)</cell><cell>999.6</cell><cell>0.115</cell></row><row><cell>InclusiveGAN (Yu et al.)</cell><cell>997</cell><cell>0.200</cell></row><row><cell>StyleGAN2 (Karras et al.)</cell><cell>940</cell><cell>0.424</cell></row><row><cell>VAEBM (ours)</cell><cell>1000</cell><cell>0.087</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Table forAUROC↑ of log p(x) computed on several OOD datasets. In-distribution dataset is CIFAR-10. Interp. corresponds to linear interpolation between CIFAR-10 images.</figDesc><table><row><cell>SVHN Interp. CIFAR100 CelebA</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>are slow at sampling, often requiring 1000 MCMC steps. Since VAEBM uses short MCMC chains, it takes only 8.79 seconds to generate 50 CIFAR-10 samples, whereas NCSN<ref type="bibr" target="#b64">(Song &amp; Ermon, 2019</ref>) takes 107.9 seconds, which is about 12× slower (see Appendix J for details). VAE and an energy network, the EBM component of the model. In this joint model, the EBM and the VAE form a symbiotic relationship: the EBM component refines the initial VAEdefined distribution, while the VAE's latent embedding space is used to accelerate sampling from the joint model and therefore enables efficient training of the energy function. We show that our model can be trained effectively in two stages with a maximum likelihood objective and we can efficiently sample it by running short Langevin dynamics chains. Experimental results demonstrate strong generative performance on several image datasets. Future work includes further scaling up the model to larger images, applying it to other domains, and using more advanced sampling algorithms.Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4401-4410, 2019. Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Training generative adversarial networks with limited data. arXiv preprint arXiv:2006.06676, 2020a. Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8110-8119, 2020b. Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015. Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In The International Conference on Learning Representations (ICLR), 2014. Durk P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. In Advances in neural information processing systems, pp. 10215-10224, 2018. DERIVING THE GRADIENT OF log Z ψ,θRecall that Z ψ,θ = p θ (x)e −E ψ (x) dx. For the derivative of log Z ψ,θ w.r.t. θ, we have:</figDesc><table><row><cell>6 CONCLUSIONS</cell></row><row><cell>We propose VAEBM, an energy-based generative model in which the data distribution is defined</cell></row><row><cell>jointly by a</cell></row></table><note>Durk P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling. Semi-supervised learning with deep generative models. In Advances in neural information processing systems, pp. 3581-3589, 2014. Durk P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling. Im- proved variational inference with inverse autoregressive flow. In Advances in neural information processing systems, pp. 4743-4751, 2016.A</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Important hyper-parameters for training VAEBM</figDesc><table><row><cell>Dataset</cell><cell cols="5">Learning rate Batch size Persistent # of LD steps LD Step size</cell></row><row><cell>CIFAR-10 w/o persistent chain</cell><cell>4e−5</cell><cell>32</cell><cell>No</cell><cell>10</cell><cell>8e−5</cell></row><row><cell>CIFAR-10 w/ persistent chain</cell><cell>4e−5</cell><cell>32</cell><cell>Yes</cell><cell>6</cell><cell>6e−5</cell></row><row><cell>CelebA 64</cell><cell>5e−5</cell><cell>32</cell><cell>No</cell><cell>10</cell><cell>5e−6</cell></row><row><cell>LSUN Church 64</cell><cell>4e−5</cell><cell>32</cell><cell>Yes</cell><cell>10</cell><cell>4e−6</cell></row><row><cell>CelebA HQ 256</cell><cell>4e−5</cell><cell>16</cell><cell>Yes</cell><cell>6</cell><cell>3e−6</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In principle one would require an accept/reject step to make it a rigorous MCMC algorithm, but for sufficiently small stepsizes this is not necessary in practice<ref type="bibr" target="#b41">(Neal, 1993)</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Maximizing ELBO with respect to φ corresponds to minimizing DKL(q φ (z|x)||p θ (z|x)) while θ is fixed.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/NVlabs/NVAE</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/bioinf-jku/TTUR 5 https://github.com/openai/improved-gan/tree/master/inception_score</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Published as a conference paper at ICLR 2021Figure 10: Additional CelebA 64 samples</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Published as a conference paper at ICLR 2021Figure 14: Additional CelebA HQ 256 samples. Initial samples from VAE for MCMC initializations are generated with full temperature 1.0. Samples are uncurated.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Network for energy function: We largely adopt the energy network structure for CIFAR-10 in <ref type="bibr" target="#b10">Du &amp; Mordatch (2019)</ref>, and we increase the depth of the network for larger images. There are 2 major differences between our energy networks and the ones used in <ref type="bibr" target="#b10">Du &amp; Mordatch (2019)</ref>: 1. we replace the LeakyReLU activations with Swish activations, as we found it improves training stability, and 2. we do not use spectral normalization <ref type="bibr" target="#b37">(Miyato et al., 2018)</ref>; instead, we use weight normalization with data-dependent initialization <ref type="bibr" target="#b61">(Salimans &amp; Kingma, 2016)</ref>. The network structure for each dataset is presented in <ref type="table">Table 7</ref>.</p><p>Training of energy function: We train the energy function by minimizing the negative log likelihood and an additional spectral regularization loss which penalizes the spectral norm of each convolutional layer in E ψ . The spectral regularization loss is also used in training NVAE, as we found</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Generalized energy based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Michael Arbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gretton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.05033</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Wasserstein gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A note on the inception score</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shane</forename><surname>Barratt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishi</forename><surname>Sharma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.01973</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Large scale gan training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.11096</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuri</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.00519</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">Importance weighted autoencoders. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Your GAN is secretly an energy-based model and you should use discriminator driven latent sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruixiang</forename><surname>Tong Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liam</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Paull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.06060</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Diagnosing and enhancing vae models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Wipf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Payne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00341</idno>
		<title level="m">Jukebox: A generative model for music</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Adji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dieng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Francisco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michalis K</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Titsias</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.04302</idno>
		<title level="m">Prescribed generative adversarial networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.08803</idno>
		<title level="m">Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Implicit generation and modeling with energy based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Mordatch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3608" to="3618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Augmented neural ODEs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilien</forename><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaud</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3140" to="3150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Flow contrastive estimation of energy-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiqi</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Nijkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying Nian</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7518" to="7528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">From variational to deterministic autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Vergari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scholkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger</editor>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Your classifier is secretly an energy based model and you should treat it like one</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan-Chieh</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joern-Henrik</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning the stein discrepancy for training and evaluating energy-based models without sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan-Chieh</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörn-Henrik</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Noise-contrastive estimation: A new estimation principle for unnormalized statistical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="297" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Divergence triangle for joint training of generator model, energy-based model, and inferential model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Nijkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitch</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Song-Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying Nian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8670" to="8679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Joint training of variational auto-encoder and latent energy-based model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Nijkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Song-Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying Nian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7978" to="7987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6626" to="6637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Training products of experts by minimizing contrastive divergence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Geoffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1771" to="1800" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A practical guide to training restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Geoffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural networks: Tricks of the trade</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="599" to="619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.11239</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">Denoising diffusion probabilistic models. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Sountsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">V</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Langmore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivas</forename><surname>Vasudevan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.03704</idno>
		<title level="m">Neutra-lizing bad geometry in hamiltonian monte carlo using neural transport</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Polina</forename><surname>Kirichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Finzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Gordon</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Equivariant flow-based sampling for lattice gauge theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gurtej</forename><surname>Kanwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Albergo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Boyda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Cranmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">C</forename><surname>Hackett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sébastien</forename><surname>Racanière</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phiala</forename><forename type="middle">E</forename><surname>Shanahan</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevLett.125.121601</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="page">121601</biblScope>
			<date type="published" when="2020-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Progressive growing of GANs for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">The expressive power of a class of normalizing flow models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamalika</forename><surname>Chaudhuri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.00392</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A tutorial on energy-based learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><forename type="middle">Aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu Jie</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Predicting structured data</title>
		<editor>G. Bakir, T. Hofman, B. Scholkopt, A. Smola, and B. Taskar</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Annealed denoising score matching: Learning energy-based models in high-dimensional spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zengyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Friedrich T</forename><surname>Sommer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.07762</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Enhancing the reliability of out-of-distribution image detection in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Coco-gan: generation by parts via conditional coordinating</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chieh</forename><surname>Hubert Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Che</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Sheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da-Cheng</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwann-Tzong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4512" to="4521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pacgan: The power of two samples in generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zinan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Khetan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giulia</forename><surname>Fanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sewoong</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1498" to="1507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.09070</idno>
		<title level="m">Hybrid discriminative-generative training via contrastive learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unsupervised image-to-image translation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="700" to="708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3730" to="3738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning nonlinear constraints with contrastive backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. 2005 IEEE International Joint Conference on Neural Networks</title>
		<meeting>2005 IEEE International Joint Conference on Neural Networks</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1302" to="1307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrice</forename><surname>Rousselle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Novák</surname></persName>
		</author>
		<idno type="DOI">10.1145/3341156</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Importance Sampling. ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Do deep generative models know what they don&apos;t know</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Nalisnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiro</forename><surname>Matsukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilan</forename><surname>Gorur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Lakshminarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Probabilistic inference using Markov chain Monte Carlo methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Annealed importance sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and computing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="125" to="139" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Mcmc using hamiltonian dynamics. Handbook of markov chain monte carlo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">On the anatomy of mcmcbased maximum likelihood learning of energy-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Nijkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitch</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying Nian</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.12370</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning non-convergent nonpersistent short-run mcmc toward energy-based model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Nijkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitch</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Song-Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying Nian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5232" to="5242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Nijkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiqi</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Sountsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivas</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Song-Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying Nian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.06897</idno>
		<title level="m">Learning energy-based model with flow-based backbone by neural transport mcmc</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Boltzmann generators: Sampling equilibrium states of many-body systems with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Noé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Köhler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.aaw1147</idno>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">365</biblScope>
			<biblScope unit="issue">6457</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03499</idno>
	</analytic>
	<monogr>
		<title level="m">Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio</title>
		<meeting><address><addrLine>Alex Graves</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<title level="m">Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Autoregressive quantile networks for generative modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Munos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.05575</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Learning latent space energy-based prior model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Nijkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Song-Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying Nian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.08205</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Quality aware generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kancharla</forename><surname>Parimala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumohana</forename><surname>Channappayya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2948" to="2958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Adversarial latent autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislav</forename><surname>Pidhorskyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Donald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gianfranco</forename><surname>Adjeroh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Doretto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="14104" to="14113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Waveflow: A compact flow-based model for raw audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kainan</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kexin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Song</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Learning implicit generative models with the method of learned moments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suman</forename><surname>Ravuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihaela</forename><surname>Rosca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.11006</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Generating diverse high-fidelity images with vq-vae-2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="14866" to="14876" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Variational inference with normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1278" to="1286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Deep boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial intelligence and statistics</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="448" to="455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Restricted boltzmann machines for collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Weight normalization: A simple reparameterization to accelerate training of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Durk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="901" to="909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Inverse molecular design using machine learning: Generative models for matter engineering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Lengeling</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alán</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.aat2663</idno>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">361</biblScope>
			<biblScope unit="issue">6400</biblScope>
			<biblScope unit="page" from="360" to="365" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Generative modeling by estimating gradients of the data distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11918" to="11930" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Improved techniques for training score-based generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09011</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxuan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiwei</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minkai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.01704</idno>
		<title level="m">Discriminator contrastive divergence: Semi-amortized generative modeling by exploring energy of the discriminator</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Veegan: Reducing mode collapse in gans using implicit variational learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akash</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lazar</forename><surname>Valkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Michael U Gutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3308" to="3318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7167" to="7176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">NVAE: A deep hierarchical variational autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">DVAE#: Discrete variational autoencoders with relaxed Boltzmann priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeny</forename><surname>Andriyash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">G</forename><surname>Macready</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">DVAE++: Discrete variational autoencoders with overlapping transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">G</forename><surname>Macready</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengbing</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Khoshaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeny</forename><surname>Andriyash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Undirected graphical models as approximate posteriors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeny</forename><surname>Andriyash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">G</forename><surname>Macready</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">A connection between score matching and denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1661" to="1674" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Notes on contrastive divergence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Woodford</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
		<respStmt>
			<orgName>Department of Engineering Science, University of Oxford</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep</note>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Generative latent flow: A framework for non-adversarial image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhisheng</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Amit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.10485</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Exponential tilting of generative models: Improving sample quality by training and sampling from latent energy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhisheng</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Amit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.08100</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">A theory of generative convnet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingnian</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2635" to="2644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">PointFlow: 3D Point Cloud Generation With Continuous Normalizing Flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4540" to="4549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lsun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.03365</idno>
		<title level="m">Construction of a large-scale image dataset using deep learning with humans in the loop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Training deep energy-based models with f-divergence minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lantao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Inclusive gan: Improving data and minority coverage in generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.03355</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Energy-based generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
