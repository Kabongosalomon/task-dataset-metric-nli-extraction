<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">META-LEARNING WITHOUT MEMORIZATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingzhang</forename><surname>Yin</surname></persName>
							<email>mzyin@utexas.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Tucker</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Google Research</orgName>
								<address>
									<addrLine>Brain team, 3 UC Berkeley, 4 Stanford</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyuan</forename><surname>Zhou</surname></persName>
							<email>mingyuan.zhou@mccombs.utexas.edu</email>
							<affiliation key="aff0">
								<address>
									<settlement>Austin</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
							<email>svlevine@eecs.berkeley.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
							<email>cbfinn@cs.stanford.edu</email>
						</author>
						<title level="a" type="main">META-LEARNING WITHOUT MEMORIZATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2020</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The ability to learn new concepts with small amounts of data is a critical aspect of intelligence that has proven challenging for deep learning methods. Meta-learning has emerged as a promising technique for leveraging data from previous tasks to enable efficient learning of new tasks. However, most meta-learning algorithms implicitly require that the meta-training tasks be mutually-exclusive, such that no single model can solve all of the tasks at once. For example, when creating tasks for few-shot image classification, prior work uses a per-task random assignment of image classes to N-way classification labels. If this is not done, the meta-learner can ignore the task training data and learn a single model that performs all of the meta-training tasks zero-shot, but does not adapt effectively to new image classes. This requirement means that the user must take great care in designing the tasks, for example by shuffling labels or removing task identifying information from the inputs. In some domains, this makes meta-learning entirely inapplicable. In this paper, we address this challenge by designing a meta-regularization objective using information theory that places precedence on data-driven adaptation. This causes the meta-learner to decide what must be learned from the task training data and what should be inferred from the task testing input. By doing so, our algorithm can successfully use data from non-mutually-exclusive tasks to efficiently adapt to novel tasks. We demonstrate its applicability to both contextual and gradientbased meta-learning algorithms, and apply it in practical settings where applying standard meta-learning has been difficult. Our approach substantially outperforms standard meta-learning algorithms in these settings.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The ability to learn new concepts and skills with small amounts of data is a critical aspect of intelligence that many machine learning systems lack. Meta-learning <ref type="bibr" target="#b27">(Schmidhuber, 1987)</ref> has emerged as a promising approach for enabling systems to quickly learn new tasks by building upon experience from previous related tasks <ref type="bibr" target="#b30">(Thrun &amp; Pratt, 2012;</ref><ref type="bibr" target="#b17">Koch et al., 2015;</ref><ref type="bibr" target="#b26">Santoro et al., 2016;</ref><ref type="bibr" target="#b25">Ravi &amp; Larochelle, 2016;</ref><ref type="bibr" target="#b6">Finn et al., 2017)</ref>. Meta-learning accomplishes this by explicitly optimizing for few-shot generalization across a set of meta-training tasks. The meta-learner is trained such that, after being presented with a small task training set, it can accurately make predictions on test datapoints for that meta-training task.</p><p>While these methods have shown promising results, current methods require careful design of the meta-training tasks to prevent a subtle form of task overfitting, distinct from standard overfitting in supervised learning. If the task can be accurately inferred from the test input alone, then the task training data can be ignored while still achieving low meta-training loss. In effect, the model will collapse to one that makes zero-shot decisions. This presents an opportunity for overfitting where the meta-learner generalizes on meta-training tasks, but fails to adapt when presented with training data from novel tasks. We call this form of overfitting the memorization problem in meta-learning because the meta-learner memorizes a function that solves all of the meta-training tasks, rather than learning to adapt.</p><p>Existing meta-learning algorithms implicitly resolve this problem by carefully designing the metatraining tasks such that no single model can solve all tasks zero-shot; we call tasks constructed in this Implementation and examples available here: https://github.com/google-research/ google-research/tree/master/meta_learning_without_memorization.</p><p>Published as a conference paper at ICLR 2020 way mutually-exclusive. For example, for N -way classification, each task consists of examples from N randomly sampled classes. The N classes are labeled from 1 to N , and critically, for each task, we randomize the assignment of classes to labels {1, 2, . . . , N } (visualized in Appendix <ref type="figure">Figure 3</ref>). This ensures that the task-specific class-to-label assignment cannot be inferred from a test input alone. However, the mutually-exclusive tasks requirement places a substantial burden on the user to cleverly design the meta-training setup (e.g., by shuffling labels or omitting goal information). While shuffling labels provides a reasonable mechanism to force tasks to be mutually-exclusive with standard few-shot image classification datasets such as MiniImageNet <ref type="bibr" target="#b25">(Ravi &amp; Larochelle, 2016)</ref>, this solution cannot be applied to all domains where we would like to utilize meta-learning. For example, consider meta-learning a pose predictor that can adapt to different objects: even if N different objects are used for meta-training, a powerful model can simply learn to ignore the training set for each task, and directly learn to predict the pose of each of the N objects. However, such a model would not be able to adapt to new objects at meta-test time.</p><p>The primary contributions of this work are: 1) to identify and formalize the memorization problem in meta-learning, and 2) to propose a meta-regularizer (MR) using information theory as a general approach for mitigating this problem without placing restrictions on the task distribution. We formally differentiate the meta-learning memorization problem from overfitting problem in conventional supervised learning, and empirically show that naïve applications of standard regularization techniques do not solve the memorization problem in meta-learning. The key insight of our metaregularization approach is that the model acquired when memorizing tasks is more complex than the model that results from task-specific adaptation because the memorization model is a single model that simultaneously performs well on all tasks. It needs to contain all information in its weights needed to do well on test points without looking at training points. Therefore we would expect the information content of the weights of a memorization model to be larger, and hence the model should be more complex. As a result, we propose an objective that regularizes the information complexity of the meta-learned function class (motivated by <ref type="bibr" target="#b1">Alemi et al. (2016)</ref>; <ref type="bibr" target="#b0">Achille &amp; Soatto (2018)</ref>). Furthermore, we show that meta-regularization in MAML can be rigorously motivated by a PAC-Bayes bound on generalization. In a series of experiments on non-mutually-exclusive task distributions entailing both few-shot regression and classification, we find that memorization poses a significant challenge for both gradient-based <ref type="bibr" target="#b6">(Finn et al., 2017)</ref> and contextual <ref type="bibr" target="#b9">(Garnelo et al., 2018a)</ref> meta-learning methods, resulting in near random performance on test tasks in some cases. Our meta-regularization approach enables both of these methods to achieve efficient adaptation and generalization, leading to substantial performance gains across the board on non-mutually-exclusive tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES</head><p>We focus on the standard supervised meta-learning problem (see, e.g., <ref type="bibr" target="#b6">Finn et al. (2017)</ref>). Briefly, we assume tasks T i are sampled from a task distribution p(T ). During meta-training, for each task, we observe a set of training data D i = (x i , y i ) and a set of test data D * i = (x * i , y * i ) with x i = (x i1 , . . . , x iK ), y i = (y i1 , . . . , y iK ) sampled from p(x, y|T i ), and similarly for D * i . We denote the entire meta-training set as</p><formula xml:id="formula_0">M = {D i , D * i } N i=1 .</formula><p>The goal of meta-training is to learn a model for a new task T by leveraging what is learned during meta-training and a small amount of training data for the new task D. We use θ to denote the meta-parameters learned during meta-training and use φ to denote the task-specific parameters that are computed based on the task training data.</p><p>Following <ref type="bibr" target="#b12">Grant et al. (2018)</ref>; <ref type="bibr" target="#b11">Gordon et al. (2018)</ref>, given a meta-training set M, we consider meta-learning algorithms that maximize conditional likelihood q(ŷ * = y * |x * , θ, D), which is composed of three distributions: q(θ|M) that summarizes meta-training data into a distribution on metaparameters, q(φ|D, θ) that summarizes the per-task training set into a distribution on task-specific parameters, and q(ŷ * |x * , φ, θ) that is the predictive distribution. These distributions are learned to</p><formula xml:id="formula_1">minimize − 1 N i E q(θ|M)q(φ|Di,θ) 1 K (x * ,y * )∈D * i log q(ŷ * = y * |x * , φ, θ) .<label>(1)</label></formula><p>For example, in MAML <ref type="bibr" target="#b6">(Finn et al., 2017)</ref>, θ and φ are the weights of a predictor network, q(θ|M) is a delta function learned over the meta-training data, q(φ|D, θ) is a delta function centered at a point defined by gradient optimization, and φ parameterizes the predictor network q(ŷ * |x * , φ) <ref type="bibr" target="#b12">(Grant et al., 2018)</ref>. In particular, to determine the task-specific parameters φ, the task training data D and θ are used in the predictor model</p><formula xml:id="formula_2">φ = θ + α K (x,y)∈D ∇ θ log q(y|x, φ = θ).</formula><p>Another family of meta-learning algorithms are contextual methods <ref type="bibr" target="#b26">(Santoro et al., 2016)</ref>, such as conditional neural processes (CNP) <ref type="bibr" target="#b10">(Garnelo et al., 2018b;</ref><ref type="bibr">a)</ref>. CNP instead defines q(φ|D, θ) as a mapping from D to a summary statistic φ (parameterized by θ). In particular, φ = a θ • h θ (D) is the output of an aggregator a θ (·) applied to features h θ (D) extracted from the task training data. Then θ parameterizes a predictor network that takes φ and x * as input and produces a predictive distribution q(ŷ * |x * , φ, θ).</p><p>In the following sections, we describe a common pitfall for a variety of meta-learning algorithms, including MAML and CNP, and a general meta-regularization approach to prevent this pitfall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">THE MEMORIZATION PROBLEM IN META-LEARNING</head><p>The ideal meta-learning algorithm will learn in such a way that generalizes to novel tasks. However, we find that unless tasks are carefully designed, current meta-learning algorithms can overfit to the tasks and end up ignoring the task training data (i.e., either q(φ|D, θ) does not depend on D or q(ŷ * |x * , φ, θ) does not depend on φ, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>), which can lead to poor generalization. This memorization phenomenon is best understood through examples.</p><p>Consider a 3D object pose prediction problem (illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>), where each object has a fixed canonical pose. The (x, y) pairs for the task are 2D grey-scale images of the rotated object (x) and the rotation angle relative to the fixed canonical pose for that object (y). In the most extreme case, for an unseen object, the task is impossible without using D because the canonical pose for the unseen object is unknown. The number of objects in the meta-training dataset is small, so it is straightforward for a single network to memorize the canonical pose for each training object and to infer the object from the input image (i.e., task overfitting), thus achieving a low training error without using D. However, by construction, this solution will necessarily have poor generalization to test tasks with unseen objects.</p><p>As another example, imagine an automated medical prescription system that suggests medication prescriptions to doctors based on patient symptoms and the patient's previous record of prescription responses (i.e., medical history) for adaptation. In the meta-learning framework, each patient represents a separate task. Here, the symptoms and prescriptions have a close relationship, so we cannot assign random prescriptions to symptoms, in contrast to the classification tasks where we can randomly shuffle the labels to create mutually-exclusiveness. For this non-mutually-exclusive task distribution, a standard meta-learning system can memorize the patients' identity information in the training, leading it to ignore the medical history and only utilize the symptoms combined with the memorized information. As a result, it may issue highly accurate prescriptions on the meta-training set, but fail to adapt to new patients effectively. While such a system would achieve a baseline level of accuracy for new patients, it would be no better than a standard supervised learning method applied to the pooled data.</p><p>We formally define (complete) memorization as: Definition 1 (Complete Meta-Learning Memorization). Complete memorization in meta-learning is when the learned model ignores the task training data such that I(ŷ * ; D|x * , θ) = 0 (i.e.,</p><formula xml:id="formula_3">q(ŷ * |x * , θ, D) = q(ŷ * |x * , θ) = E D |x * [q(ŷ * |x * , θ, D )]).</formula><p>Memorization describes an issue with overfitting the meta-training tasks, but it does not preclude the network from generalizing to unseen (x, y) pairs on the tasks similar to the training tasks. Memorization becomes an undesired problem for generalization to new tasks when I(y * ; D|x * ) I(ŷ * ; D|x * , θ) (i.e., the task training data is necessary to achieve good performance, even with exact inference under the data generating distribution, to make accurate predictions).</p><p>A model with the memorization problem may generalize to new datapoints in training tasks but cannot generalize to novel tasks, which distinguishes it from typical overfitting in supervised learning. In practice, we find that MAML and CNP frequently converge to this memorization solution ( <ref type="table" target="#tab_1">Table 2)</ref>. For MAML, memorization can occur when a particular setting of θ that does not adapt to the task training data can achieve comparable meta-training error to a solution that adapts θ. For example, if a setting of θ can solve all of the meta-training tasks (i.e., for all (x, y) in D and D * the predictive error is close to zero), the optimization may converge to a stationary point of the MAML objective where minimal adaptation occurs based on the task training set (i.e., φ ≈ θ). For a novel task where it is necessary to use the task training data, MAML can in principle still leverage the task training data because the adaptation step is based on gradient descent. However, in practice, the poor initialization of θ can affect the model's ability to generalize from a small mount of data. For CNP, memorization can occur when the predictive distribution network q(ŷ * |x * , φ, θ) can achieve low training error without using the task training summary statistics φ. On a novel task, the network is not trained to use φ, so it is unable to use the information extracted from the task training set to effectively generalize.</p><p>In some problem domains, the memorization problem can be avoided by carefully constructing the tasks. For example, for N -way classification, each task consists of examples from N randomly sampled classes. If the classes are assigned to a random permutation of N for each task, this ensures that the task-specific class-to-label assignment cannot be inferred from the test inputs alone. As a result, a model that ignores the task training data cannot achieve low training error, preventing convergence to the memorization problem. We refer to tasks constructed in this way as mutuallyexclusive. However, the mutually-exclusive tasks requirement places a substantial burden on the user to cleverly design the meta-training setup (e.g., by shuffling labels or omitting goal information) and cannot be applied to all domains where we would like to utilize meta-learning. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">META REGULARIZATION USING INFORMATION THEORY</head><p>At a high level, the sources of information in the predictive distribution q(ŷ * |x * , θ, D) come from the input, the meta-parameters, and the data. The memorization problem occurs when the model encodes task information in the predictive network that is readily available from the task training set (i.e., it memorizes the task information for each meta-training task). We could resolve this problem by encouraging the model to minimize the training error and to rely on the task training dataset as much as possible for the prediction of y * (i.e., to maximize I(ŷ * ; D|x * , θ)). Explicitly maximizing I(ŷ * ; D|x * , θ) requires an intractable marginalization over task training sets to compute q(ŷ * |x * , θ). Instead, we can implicitly encourage it by restricting the information flow from other sources (x * and θ) toŷ * . To achieve both low error and low mutual information betweenŷ * and (x * , θ), the model must use task training data D to make predictions, hence increasing the mutual information I(ŷ * ; D|x * , θ), leading to reduced memorization. In this section, we describe two tractable ways to achieve this.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">META REGULARIZATION ON ACTIVATIONS</head><p>Given θ, the statistical dependency between x * andŷ * is controlled by the direct path from x * toŷ * and the indirect path through D (see <ref type="figure" target="#fig_0">Figure 1)</ref>, where the latter is desirable because it leverages the task training data. We can control the information flow between x * and y * by introducing an intermediate stochastic bottleneck variable z * such that q(ŷ * |x * , φ, θ) = q(ŷ * |z * , φ, θ)q(z * |x * , θ) dz * (Alemi et al., 2016) as shown in <ref type="figure" target="#fig_3">Figure 4</ref>. Now, we would like to maximize I(ŷ * ; D|z * , θ) to prevent memorization. We can bound this mutual information by</p><formula xml:id="formula_4">I(ŷ * ; D|z * , θ) ≥I(x * ;ŷ * |θ, z * ) = I(x * ;ŷ * |θ) − I(x * ; z * |θ) + I(x * ; z * |ŷ * , θ) ≥I(x * ;ŷ * |θ) − I(x * ; z * |θ) =I(x * ;ŷ * |θ) − E p(x * )q(z * |x * ,θ) log q(z * |x * , θ) q(z * |θ) ≥I(x * ;ŷ * |θ) − E log q(z * |x * , θ) r(z * ) = I(x * ;ŷ * |θ) − E [D KL (q(z * |x * , θ)||r(z * ))]<label>(2)</label></formula><p>where r(z * ) is a variational approximation to the marginal, the first inequality follows from the statistical dependencies in our model (see <ref type="figure" target="#fig_3">Figure 4</ref> and Appendix A.2 for the proof). By simultaneously minimizing E [D KL (q(z * |x * , θ)||r(z * ))] and maximizing the mutual information I(x * ;ŷ * |θ), we can implicitly encourage the model to use the task training data D.</p><p>For non-mutually-exclusive problems, the true label y * is dependent on x * . If the model has the memorization problem and I(x * ;ŷ * |θ) = 0, then q(ŷ * |x * , θ, D) = q(ŷ * |x * , θ) = q(ŷ * |θ), which means the model predictions do not depend on x * or D. Hence, in practical problems, the predictions generated from the model will have low accuracy.</p><p>This suggests minimizing the training loss in Eq.</p><p>(1) can increase I(ŷ * ; D|x * , θ) or I(x * ;ŷ * |θ).</p><p>Replacing the maximization of I(x * ;ŷ * |θ) in Eq.</p><p>(2) with minimizing the training loss results in the following regularized training objective</p><formula xml:id="formula_5">1 N i E q(θ|M)q(φ|Di,θ) − 1 K (x * ,y * )∈D * i log q(ŷ * = y * |x * , φ, θ) + βD KL (q(z * |x * , θ)||r(z * )) (3) where log q(ŷ * |x * , φ, θ) is estimated by log q(ŷ * |z * , φ, θ) with z * ∼ q(z * |x * , θ)</formula><p>, β modulates the regularizer and r(z * ) can be set as N (z * ; 0, I). We refer to this regularizer as meta-regularization (MR) on the activations.</p><p>As we demonstrate in Section 6, we find that this regularizer performs well, but in some cases can fail to prevent the memorization problem. Our hypothesis is that in these cases, the network can sidestep the information constraint by storing the prediction of y * in a part of z * , which incurs a small penalty in Eq. (3) and small lower bound in Eq. (2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">META REGULARIZATION ON WEIGHTS</head><p>Alternatively, we can penalize the task information stored in the meta-parameters θ. Here, we provide an informal argument and provide the complete argument in Appendix A.3. Analogous to the supervised setting <ref type="bibr" target="#b0">(Achille &amp; Soatto, 2018)</ref>, given meta-training dataset M, we consider θ as random variable where the randomness can be introduced by training stochasticity. We model the stochasticity over θ with a Gaussian distribution N (θ; θ µ , θ σ ) with learned mean and variance parameters per dimension <ref type="bibr" target="#b3">(Blundell et al., 2015;</ref><ref type="bibr" target="#b0">Achille &amp; Soatto, 2018)</ref>. By penalizing I(y * 1:N , D 1:N ; θ|x * 1:N ), we can limit the information about the training tasks stored in the metaparameters θ and thus require the network to use the task training data to make accurate predictions. We can tractably upper bound it by</p><formula xml:id="formula_6">I(y * 1:N , D 1:N ; θ|x * 1:N ) = E log q(θ|M) q(θ|x * 1:N ) ≤ E [D KL (q(θ|M) r(θ))] ,<label>(4)</label></formula><p>where r(θ) is a variational approximation to the marginal, which we set to N (θ; 0, I). In practice, we apply meta-regularization to the meta-parameters θ that are not used to adapt to the task training data and denote the other parameters asθ. In this way, we control the complexity of the network that can predict the test labels without using task training data, but we do not limit the complexity of the network that processes the task training data. Our final meta-regularized objective can be written as</p><formula xml:id="formula_7">1 N i E q(θ;θµ,θσ)q(φ|Di,θ) − 1 K (x * ,y * )∈D * i log q(ŷ * = y * |x * , φ, θ,θ) + βD KL (q(θ; θ µ , θ σ )||r(θ))<label>(5)</label></formula><p>For MAML, we apply meta-regularization to the parameters uninvolved in the task adaptation. For CNP, we apply meta-regularization to the encoder parameters. The detailed algorithms are shown in Algorithm 1 and 2 in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">DOES META REGULARIZATION LEAD TO BETTER GENERALIZATION?</head><p>Now that we have derived meta regularization approaches for mitigating the memorization problem, we theoretically analyze whether meta regularization leads to better generalization via a PAC-Bayes bound. In particular, we study meta regularization (MR) on the weights (W) of MAML, i.e. MR-MAML (W), as a case study.</p><p>Meta regularization on the weights of MAML uses a Gaussian distribution N (θ; θ µ , θ σ ) to model the stochasticity in the weights. Given a task and task training data, the expected error is given by</p><formula xml:id="formula_8">er(θ µ , θ σ , D, T ) = E θ∼N (θ;θµ,θσ),φ∼q(φ|θ,D),(x * ,y * )∼p(x,y|T ) [L(x * , y * , φ)] ,<label>(6)</label></formula><p>where the prediction loss L(x * , y * , φ i ) is bounded 1 . Then, we would like to minimize the error on novel tasks</p><formula xml:id="formula_9">er(θ µ , θ σ ) = E T ∼p(T ),D∼p(x,y|T ) [er(θ µ , θ σ , D, T )]<label>(7)</label></formula><p>We only have a finite sample of training tasks, so computing er(Q) is intractable, but we can form an empirical estimate:</p><formula xml:id="formula_10">er(θ µ , θ σ , D 1 , D * 1 , ..., D n , D * n ) = 1 n n i=1 E θ∼N (θ;θµ,θσ),φi∼q(φ|θ,Di)   − 1 K (x * ,y * )∈D * i log q(ŷ * = y * |x * , φ i )   êr(θµ,θσ,Di,D * i )<label>(8)</label></formula><p>where for exposition we have assumed |D * i | = K are the same for all tasks. We would like to relate er(θ µ , θ σ ) andêr(θ µ , θ σ , D 1 , D * 1 , ..., D n , D * n ), but the challenge is that θ µ and θ σ are derived from the meta-training tasks D 1 , D * 1 , ..., D n , D * n . There are two sources of generalization error: (i) error due to the finite number of observed tasks and (ii) error due to the finite number of examples observed per task. Closely following the arguments in <ref type="bibr" target="#b2">(Amit &amp; Meir, 2018)</ref>, we apply a standard PAC-Bayes bound to each of these and combine the results with a union bound, resulting in the following Theorem. Theorem 1. Let P (θ) be an arbitrary prior distribution over θ that does not depend on the metatraining data. Then for any δ ∈ (0, 1], with probability at least 1 − δ, the following inequality holds uniformly for all choices of θ µ and θ σ ,</p><formula xml:id="formula_11">er(θµ, θσ) ≤ 1 n n i=1ê r(θµ, θσ, Di, D * i )+ 1 2(K − 1) + 1 2(n − 1) DKL(N (θ; θµ, θσ) P ) + log n(K + 1) δ ,<label>(9)</label></formula><p>where n is the number of meta-training tasks and K is the number of per-task validation datapoints.</p><p>We defer the proof to the Appendix A.4. The key difference from the result in <ref type="bibr" target="#b2">(Amit &amp; Meir, 2018)</ref> is that we leverage the fact that the task training data is split into training and validation.</p><p>In practice, we set P (θ) = r(θ) = N (θ; 0, I). If we can achieve a low value for the bound, then with high probability, our test error will also be low. As shown in the Appendix A.4, by a first order Taylor expansion of the the second term of the RHS in Eq.(9) and setting the coefficient of the KL</p><formula xml:id="formula_12">term as β = √ 1/2(K−1)+ √ 1/2(n−1) 2 √ log n(K+1)/δ</formula><p>, we recover the MR-MAML(W) objective (Eq. <ref type="formula" target="#formula_7">(5)</ref>). β tradesoff between the tightness of the generalization bound and the probability that it holds true. The result of this bound suggests that the proposed meta-regularization on weights does indeed improve generalization on the meta-test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK</head><p>Previous works have developed approaches for mitigating various forms of overfitting in metalearning. These approaches aim to improve generalization in several ways: by reducing the number of parameters that are adapted in MAML (Zintgraf et al., 2019), by compressing the task embedding <ref type="bibr" target="#b20">(Lee et al., 2019)</ref>, through data augmentation from a <ref type="bibr">GAN (Zhang et al., 2018)</ref>, by using an auxiliary objective on task gradients <ref type="bibr" target="#b13">(Guiroy et al., 2019)</ref>, and via an entropy regularization objective <ref type="bibr" target="#b15">(Jamal &amp; Qi, 2019)</ref>. These methods all focus on the setting with mutually-exclusive task distributions. We instead recognize and formalize the memorization problem, a particular form of overfitting that manifests itself with non-mutually-exclusive tasks, and offer a general and principled solution. Unlike prior methods, our approach is applicable to both contextual and gradientbased meta-learning methods. We additionally validate that prior regularization approaches, namely TAML (Jamal &amp; Qi, 2019), are not effective for addressing this problem setting.</p><p>Our derivation uses a Bayesian interpretation of meta-learning <ref type="bibr" target="#b29">(Tenenbaum, 1999;</ref><ref type="bibr" target="#b5">Fei-Fei et al., 2003;</ref><ref type="bibr">Edwards &amp; Storkey, 2016;</ref><ref type="bibr" target="#b12">Grant et al., 2018;</ref><ref type="bibr" target="#b11">Gordon et al., 2018;</ref><ref type="bibr" target="#b16">Kim et al., 2018;</ref><ref type="bibr" target="#b14">Harrison et al., 2018)</ref>. Some Bayesian meta-learning approaches place a distributional loss on the inferred task variables to constrain them to a prior distribution <ref type="bibr" target="#b10">(Garnelo et al., 2018b;</ref><ref type="bibr" target="#b11">Gordon et al., 2018;</ref><ref type="bibr" target="#b24">Rakelly et al., 2019)</ref>, which amounts to an information bottleneck on the latent task variables. Similarly Zintgraf et al. <ref type="formula" target="#formula_1">(2019)</ref>; <ref type="bibr" target="#b20">Lee et al. (2019)</ref>; <ref type="bibr" target="#b13">Guiroy et al. (2019)</ref> aim to produce simpler or more compressed task adaptation processes. Our approach does the opposite, penalizing information from the inputs and parameters, to encourage the task-specific variables to contain greater information driven by the per-task data.</p><p>We use PAC-Bayes theory to study the generalization error of meta-learning and meta-regularization. <ref type="bibr" target="#b23">Pentina &amp; Lampert (2014)</ref> extends the single task PAC-Bayes bound <ref type="bibr" target="#b21">(McAllester, 1999</ref>) to the multitask setting, which quantifies the gap between empirical error on training tasks and the expected error on new tasks. More recent research shows that, with tightened generalization bounds as the training objective, the algorithms can reduce the test error for mutually-exclusive tasks <ref type="bibr" target="#b8">(Galanti et al., 2016;</ref><ref type="bibr" target="#b2">Amit &amp; Meir, 2018)</ref>. Our analysis is different from these prior works in that we only include preupdate meta parameters in the generalization bound rather than both pre-update and post-update parameters. In the derivation, we also explicitly consider the splitting of data into the task training set and task validation set, which is aligned with the practical setting.</p><p>The memorization problem differs from overfitting in conventional supervised learning in several aspects. First, memorization occurs at the task level rather than datapoint level and the model memorizes functions rather than labels. In particular, within a training task, the model can generalize to new datapoints, but it fails to generalize to new tasks. Second, the source of information for achieving generalization is different. For meta-learning the information is from both the meta-training data and new task training data but in standard supervised setting the information is only from training data. Finally, the aim of regularization is different. In the conventional supervised setting, regularization methods such as weight decay <ref type="bibr" target="#b18">(Krogh &amp; Hertz, 1992)</ref>, dropout <ref type="bibr" target="#b28">(Srivastava et al., 2014)</ref>, the information bottleneck <ref type="bibr" target="#b32">(Tishby et al., 2000;</ref><ref type="bibr" target="#b31">Tishby &amp; Zaslavsky, 2015)</ref>, and Bayes-by-Backprop <ref type="bibr" target="#b3">(Blundell et al., 2015)</ref> are used to balance the network complexity and the information in the data. The aim of meta-regularization is different. It governs the model complexity to avoid one complex model solving all tasks, while allowing the model's dependency on the task data to be complex. We further empirically validate this difference, finding that standard regularization techniques do not solve the memorization problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTS</head><p>In the experimental evaluation, we aim to answer the following questions: (1) How prevalent is the memorization problem across different algorithms and domains? (2) How does the memorization problem affect the performance of algorithms on non-mutually-exclusive task distributions? (3) Is our meta-regularization approach effective for mitigating the problem and is it compatible with multiple types of meta-learning algorithms? (4) Is the problem of memorization empirically distinct from that of the standard overfitting problem?</p><p>To answer these questions, we propose several meta-learning problems involving non-mutuallyexclusive task distributions, including two problems that are adapted from prior benchmarks with mutually-exclusive task distributions. We consider model-agnostic meta-learning (MAML) and conditional neural processes (CNP) as representative meta-learning algorithms. We study both variants of our method in combination with MAML and CNP. When comparing with meta-learning algorithms with and without meta-regularization, we use the same neural network architecture, while other hyperparameters are tuned via cross-validation per-problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">SINUSOID REGRESSION</head><p>First, we consider a toy sinusoid regression problem that is non-mutually-exclusive. The data for each task is created in the following way: the amplitude A of the sinusoid is uniformly sampled from a set of 20 equally-spaced points {0.1, 0.3, · · · , 4}; u is sampled uniformly from [−5, 5] and y is sampled from N (A sin(u), 0.1 2 ). We provide both u and the amplitude A (as a one-hot vector) as input, i.e. <ref type="figure">x = (u, A)</ref>. At the test time, we expand the range of the tasks by randomly sampling the data-generating amplitude A uniformly from [0.1, 4] and use a random one-hot vector for the input to the network. The meta-training tasks are a proper subset of the meta-test tasks.</p><p>Without the additional amplitude input, both MAML and CNP can easily solve the task and generalize to the meta-test tasks. However, once we add the additional amplitude input which indicates the task identity, we find that both MAML and CNP converge to the complete memorization solution and fail to generalize well to test data <ref type="table" target="#tab_0">(Table 1</ref> and <ref type="figure">Appendix Figures 7 and 8)</ref>. Both meta-regularized MAML and CNP (MR-MAML) and (MR-CNP) instead converge to a solution that adapts to the data, and as a result, greatly outperform the unregularized methods. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">POSE PREDICTION</head><p>To illustrate the memorization problem on a more realistic task, we create a multi-task regression dataset based on the Pascal 3D data <ref type="bibr" target="#b35">(Xiang et al., 2014</ref>) (See Appendix A.5.1 for a complete description). We randomly select 50 objects for meta-training and the other 15 objects for meta-testing. For each object, we use MuJoCo <ref type="bibr" target="#b33">(Todorov et al., 2012)</ref> to render images with random orientations of the instance on a table, visualized in <ref type="figure" target="#fig_0">Figure 1</ref>. For the meta-learning algorithm, the observation (x) is the 128 × 128 gray-scale image and the label (y) is the orientation relative to a fixed canonical pose. Because the number of objects in the meta-training dataset is small, it is straightforward for a single network to memorize the canonical pose for each training object and to infer the orientation from the input image, thus achieving a low meta-training error without using D. However, this solution performs poorly at the test time because it has not seen the novel objects and their canonical poses.</p><p>Optimization modes and hyperparameter sensitivity. We choose the learning rate from {0.0001, 0.0005, 0.001} for each method, β from {10 −6 , 10 −5 , · · · , 1} for meta-regularization and report the results with the best hyperparameters (as measured on the meta-validation set) for each method. In this domain, we find that the convergence point of the meta-learning algorithm is determined by both the optimization landscape of the objective and the training dynamics, which vary due to stochastic gradients and the random initialization. In particular, we observe that there are two modes of the objective, one that corresponds to complete memorization and one that corresponds to successful adaptation to the task data. As illustrated in the Appendix, we find that models that converge to a memorization solution have lower training error than solutions which use the task training data, indicating a clear need for meta-regularization. When the meta-regularization is on the activations, the solution that the algorithms converge to depends on the learning rate, while MR on the weights consistently converges to the adaptation solution (See Appendix <ref type="figure">Figure 9</ref> for the sensitivity analysis). This suggests that MR on the activations is not always successful at preventing memorization. Our hypothesis is that there exists a solution in which the bottlenecked activations encode only the prediction y * , and discard other information. Such a solution can achieve both low training MSE and low regularization loss without using task training data, particularly if the predicted label contains a small number of bits (i.e., because the activations will have low information complexity).</p><p>However, note that this solution does not achieve low regularization error when applying MR to the weights because the function needed to produce the predicted label does not have low information complexity. As a result, meta-regularization on the weights does not suffer from this pathology and is robust to different learning rates. Therefore, we will use regularization on weights as the proposed methodology in the following experiments and algorithms in Appendix A.1.</p><p>Quantitative results. We compare MAML and CNP with their meta-regularized versions ( <ref type="table" target="#tab_1">Table 2)</ref>. We additionally include fine-tuning as baseline, which trains a single network on all the instances jointly, and then fine-tunes on the task training data. Meta-learning with meta-regularization (on weights) outperforms all competing methods by a large margin. We show test error as a function of the meta-regularization coefficient β in Appendix <ref type="figure">Figure 2</ref>. The curve reflects the trade-off when changing the amount of information contained in the weights. This indicates that β gives a knob that allows us to tune the degree to which the model uses the data to adapt versus relying on the prior. <ref type="figure">Figure 2</ref>: The performance of MAML and CNP with meta-regularization on the weights, as a function of the regularization strength β. We observe β provides us a knob with which we can control the degree to which the algorithm adapts versus memorizes. When β is small, we observe memorization, leading to large test error; when β is too large, the network does not store enough information in the weights to perform the task. Crucially, in the middle of these two extremes, meta-regularization is effective in inducing adaptation, leading to good generalization. The plot shows the mean and standard deviation across 5 meta-training runs. Comparison to standard regularization. We compare our meta-regularization with standard regularization techniques, weight decay <ref type="bibr" target="#b18">(Krogh &amp; Hertz, 1992)</ref> and Bayes-by-Backprop <ref type="bibr" target="#b3">(Blundell et al., 2015)</ref>, in <ref type="table" target="#tab_2">Table 3</ref>. We observe that simply applying standard regularization to all the weights, as in conventional supervised learning, does not solve the memorization problem, which validates that the memorization problem differs from the standard overfitting problem. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">OMNIGLOT AND MINIIMAGENET CLASSIFICATION</head><p>Next, we study memorization in the few-shot classification problem by adapting the few-shot Omniglot <ref type="bibr" target="#b19">(Lake et al., 2011)</ref> and MiniImagenet <ref type="bibr" target="#b25">(Ravi &amp; Larochelle, 2016;</ref><ref type="bibr" target="#b34">Vinyals et al., 2016)</ref> bench-marks to the non-mutually-exclusive setting. In the non-mutually-exclusive N-way K-shot classification problem, each class is (randomly) assigned a fixed classification label from 1 to N. For each task, we randomly select a corresponding class for each classification label and K task training data points and K task test data points from that class 2 . This ensures that each class takes only one classification label across tasks and different tasks are non-mutually-exclusive (See Appendix A.5.2 for details).</p><p>We evaluate MAML, TAML (Jamal &amp; Qi, 2019), MR-MAML (ours), fine-tuning, and a nearest neighbor baseline on non-mutually-exclusive classification tasks <ref type="table" target="#tab_3">(Table 4</ref>). We find that MR-MAML significantly outperforms previous methods on all of these tasks. To better understand the problem, for the MAML variants, we calculate the pre-update accuracy (before adaptation on the task training data) on the meta-training data in <ref type="table" target="#tab_4">Appendix Table 5</ref>. The high pre-update meta-training accuracy and low meta-test accuracy are evidence of the memorization problem for MAML and TAML, indicating that it is learning a model that ignores the task data. In contrast, MR-MAML successfully controls the pre-update accuracy to be near chance and encourages the learner to use the task training data to achieve low meta-training error, resulting in good performance at meta-test time.</p><p>Finally, we verify that meta-regularization does not degrade performance on the standard mutuallyexclusive task. We evaluate performance as a function of regularization strength on the standard 20-way 1-shot Omniglot task (Appendix <ref type="figure" target="#fig_0">Figure 10)</ref>, and we find that small values of β lead to slight improvements over MAML. This indicates that meta-regularization substantially improves performance in the non-mutually-exclusive setting without degrading performance in other settings.  <ref type="bibr" target="#b25">(Ravi &amp; Larochelle, 2016)</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION AND DISCUSSION</head><p>Meta-learning has achieved remarkable success in few-shot learning problems. However, we identify a pitfall of current algorithms: the need to create task distributions that are mutually exclusive. This requirement restricts the domains that meta-learning can be applied to. We formalize the failure mode, i.e. the memorization problem, that results from training on non-mutually-exclusive tasks and distinguish it as a function-level overfitting problem compared to the the standard label-level overfitting in supervised learning.</p><p>We illustrate the memorization problem with different meta-learning algorithms on a number of domains. To address the problem, we propose an algorithm-agnostic meta-regularization (MR) approach that leverages an information-theoretic perspective of the problem. The key idea is that by placing a soft restriction on the information flow from meta-parameters in prediction of test set labels, we can encourage the meta-learner to use task training data during meta-training. We achieve this by successfully controlling the complexity of model prior to the task adaptation.</p><p>The memorization issue is quite broad and is likely to occur in a wide range of real-world applications, for example, personalized speech recognition systems, learning robots that can adapt to different environments <ref type="bibr" target="#b22">(Nagabandi et al., 2018)</ref>, and learning goal-conditioned manipulation skills using trial-and-error data. Further, this challenge may also be prevalent in other conditional prediction problems, beyond meta-learning, an interesting direction for future study. By both recognizing the challenge of memorization and developing a general and lightweight approach for solving it, we believe that this work represents an important step towards making meta-learning algorithms applicable to and effective on any problem domain.</p><p>Ruixiang Zhang, Tong Che, Zoubin Ghahramani, Yoshua Bengio, and Yangqiu Song. Metagan: An adversarial approach to few-shot learning. In Advances in Neural Information Processing Systems, pp. 2365-2374, 2018.</p><p>Luisa M Zintgraf, Kyriacos Shiarlis, Vitaly Kurin, Katja Hofmann, and Shimon Whiteson. Fast context adaptation via meta-learning. In Thirty-sixth International Conference on Machine Learning (ICML 2019), 2019.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX</head><p>A.1 ALGORITHM</p><p>We present the detailed algorithm for meta-regularization on weights with conditional neural processes (CNP) in Algorithm 1 and with model-agnostic meta-learning (MAML) in Algorithm 2. For CNP, we add the regularization on the weights θ of encoder and leave other weightsθ unrestricted.</p><p>For MAML, we similarly regularize the weights θ from input to an intermediate hidden layer and leave the weightsθ for adaptation unregularized. In this way, we restrict the complexity of the pre-adaptation model not the post-adaptation model.</p><p>Algorithm 1: Meta-Regularized CNP input : Task distribution p(T ); Encoder weights distribution q(θ; τ ) = N (θ; τ ) with Gaussian parameters τ = (θ µ , θ σ ); Prior distribution r(θ) and Lagrangian multiplier β;θ that parameterizes feature extractor hθ(·) and decoder Tθ(·). Stepsize α. output: Network parameter τ ,θ.</p><p>Initialize τ ,θ randomly; while not converged do Sample a mini-batch of {T i } from p(T ); Sample θ ∼ q(θ; τ ) with reparameterization ;</p><formula xml:id="formula_13">for all T i ∈ {T i } do Sample D i = (x i , y i ), D * i = (x * i , y * i ) from T i ; Encode observation z i = g θ (x i ), z * i = g θ (x * i ) ; Compute task context φ i = a(hθ(z i , y i )) with aggregator a(·); Updateθ ←θ + α∇θ Ti log q(y * i |Tθ(z * i , φ i )) ; Update τ ← τ + α∇ τ [ Ti log q(y * i |Tθ(z * i , φ i )) − βD KL (q(θ; τ )||r(θ))]</formula><p>Algorithm 2: Meta-Regularized MAML input : Task distribution p(T ); Weights distribution q(θ; τ ) = N (θ; τ ) with Gaussian parameters τ = (θ µ , θ σ ); Prior distribution r(θ) and Lagrangian multiplier β; Stepsize α, α . output: Network parameter τ ,θ.</p><p>Initialize τ ,θ randomly; while not converged do Sample a mini-batch of {T i } from p(T ); Sample θ ∼ q(θ; τ ) with reparameterization ;</p><formula xml:id="formula_14">for all T i ∈ {T i } do Sample D i = (x i , y i ), D * i = (x * i , y * i ) from T i ; Encode observation z i = g θ (x i ), z * i = g θ (x * i )</formula><p>; Compute task specific parameter φ i =θ + α ∇θ log q(y i |z i ,θ) ;</p><formula xml:id="formula_15">Updateθ ←θ + α∇θ Ti log q(y * i |z * i , φ i ) ; Update τ ← τ + α∇ τ [ Ti log q(y * i |z * i , φ i ) − βD KL (q(θ; τ )||r(θ))]</formula><p>Algorithm 3: Meta-Regularized Methods in Meta-testing input : Meta-testing task T with training data D = (x, y) and testing input x * , optimized parameters τ,θ. output: Predictionŷ * for k from 1 to K do Sample θ k ∼ q(θ; τ ); Encode observation z k = g θ k (x), z * k = g θ k (x * ) ; Compute task specific parameter φ k = a(hθ(z k , y)) for MR-CNP and φ k =θ + α ∇θ log q(y|z k ,θ) for MR-MAML;</p><formula xml:id="formula_16">Predictŷ * k ∼ q(ŷ * |z * k , φ k ,θ) Return predictionŷ * = 1 K K k=1ŷ * k A.2 META REGULARIZATION ON ACTIVATIONS</formula><p>We show that I(x * ;ŷ * |z * , θ) ≤ I(ŷ * ; D|z * , θ). By <ref type="figure" target="#fig_3">Figure 4</ref>, we have that I(ŷ * ; x * |θ, D, z * ) = 0.</p><p>By the chain rule of mutual information we have</p><formula xml:id="formula_17">I(ŷ * ; D|z * , θ) =I(ŷ * ; D|z * , θ) + I(ŷ * ; x * |D, θ, z * ) =I(ŷ * ; x * , D|θ, z * ) =I(x * ;ŷ * |θ, z * ) + I(ŷ * ; D|x * , θ, z * ) ≥I(x * ;ŷ * |θ, z * )<label>(10)</label></formula><p>A.3 META REGULARIZATION ON WEIGHTS Similar to <ref type="bibr" target="#b0">(Achille &amp; Soatto, 2018)</ref>, we use ξ to denote the unknown parameters of the true data generating distribution. This defines a joint distribution p(ξ, M, θ) = p(ξ)p(M|ξ)q(θ|M). Furthermore, we have a predictive distribution q(ŷ * |x * , D, θ) = E φ|θ,D [q(ŷ * |x * , φ, θ)].</p><p>The meta-training loss in Eq. 1 is an upper bound for the cross entropy H p,q (y * 1:N |x * 1:N , D 1:N , θ). Using an information decomposition of cross entropy <ref type="bibr" target="#b0">(Achille &amp; Soatto, 2018)</ref>, we have </p><p>Here the only negative term is the I(y * 1:N , D 1:N ; θ|x * 1:N , ξ), which quantifies the information that the meta-parameters contain about the meta-training data beyond what can be inferred from the data generating parameters (i.e., memorization). Without proper regularization, the cross entropy loss can be minimized by maximizing this term. We can control its value by upper bounding it</p><formula xml:id="formula_19">I(y * 1:N , D 1:N ; θ|x * 1:N , ξ) = E log q(θ|M, ξ) q(θ|x * 1:N , ξ) = E log q(θ|M) q(θ|x * 1:N , ξ) = E [D KL (q(θ|M)||q(θ|x * 1:N , ξ))] ≤ E [D KL (q(θ|M)||r(θ))] ,</formula><p>where the second equality follows because θ and ξ are conditionally independent given M. This gives the regularization in Section 4.2.</p><p>A.4 PROOF OF THE PAC-BAYES GENERALIZATION BOUND First, we prove a more general result and then specialize it. The goal of the meta-learner is to extract information about the meta-training tasks and the test task training data to serve as a prior for test examples from the novel task. This information will be in terms of a distribution Q over possible models. When learning a new task, the meta-learner uses the training task data D and a model parameterized by θ (sampled from Q(θ)) and outputs a distribution q(φ|D, θ) over models. Our goal is to learn Q such that it performs well on novel tasks.</p><p>To formalize this, define er(Q, D, T ) = E θ∼Q(θ),φ∼q(φ|θ,D),(x * ,y * )∼p <ref type="bibr">(x,y|T )</ref> </p><formula xml:id="formula_20">[L(φ(x * ), y * )]<label>(12)</label></formula><p>where L(φ(x * ), y * ) is a bounded loss in [0, 1]. Then, we would like to minimize the error on novel tasks</p><formula xml:id="formula_21">er(Q) = min Q E T ∼p(T ),D∼p(x,y|T ) [er(Q, D, T )]<label>(13)</label></formula><p>Because we only have a finite training set, computing er(Q) is intractable, but we can form an empirical estimate:</p><formula xml:id="formula_22">er(Q, D 1 , D * 1 , ..., D n , D * n ) = 1 n n i=1 E θ∼Q(θ),φi∼q(φ|θ,Di)   1 K (x * ,y * )∈D * i L(φ(x * ), y * ))   êr(Q,Di,D * i )<label>(14)</label></formula><p>where for exposition we assume K = |D * i | is the same for all i. We would like to relate er(Q) and er(Q, D 1 , D * 1 , ..., D n , D * n ), but the challenge is that Q may depend on D 1 , D * 1 , ..., D n , D * n due to the learning algorithm. There are two sources of generalization error: (i) error due to the finite number of observed tasks and (ii) error due to the finite number of examples observed per task. Closely following the arguments in <ref type="bibr" target="#b2">(Amit &amp; Meir, 2018)</ref>, we apply a standard PAC-Bayes bound to each of these and combine the results with a union bound. Theorem. Let Q(θ) be a distribution over parameters θ and let P (θ) be a prior distribution. Then for any δ ∈ (0, 1], with probability at least 1 − δ, the following inequality holds uniformly for all distributions Q,</p><formula xml:id="formula_23">er(Q) ≤ 1 n n i=1ê r(Q, D i , D * i ) + 1 2(K − 1) + 1 2(n − 1) D KL (Q P ) + log n(K + 1) δ<label>(15)</label></formula><p>Proof. To start, we state a classical PAC-Bayes bound and use it to derive generalization bounds on task and datapoint level generalization, respectively.</p><p>Theorem 2. Let X be a sample space (i.e. a space of possible datapoints). Let P (X) be a distribution over X (i.e. a data distribution). Let Θ be a hypothesis space. Given a "loss function" l(θ, X) : Θ × X → [0, 1] and a collection of M i.i.d. random variables sampled from P (X), X 1 , ..., X M , let π be a prior distribution over hypotheses in Θ that does not depend on the samples but may depend on the data distribution P (X). Then, for any δ ∈ (0, 1], the following bound holds uniformly for all posterior distributions ρ over Θ</p><formula xml:id="formula_24">P E X i ∼P (X),θ∼ρ(·) [l(θ, Xi)] ≤ 1 M M m=1 E θ∼ρ(·) [l(θ, Xm] + 1 2(M − 1) DKL(ρ π) + log M δ , ∀ρ ≥1 − δ.<label>(16)</label></formula><p>Meta-level generalization First, we bound the task-level generalization, that is we relate er(Q) to 1 n n i=1 er(Q, D i , T i ). Letting the samples be X i = (D i , T i ), and l(θ, X n ) = E φi∼q(φ|Di,θ),(x * ,y * )∼Ti [L(φ(x * ), y * )], then Theorem 1 says that for any δ 0 ∼ (0, 1]</p><formula xml:id="formula_25">P er(Q) ≤ 1 n n i=1 er(Q, D i , T i ) + 1 2(n − 1) D KL (Q P ) + log n δ 0 , ∀Q ≥ 1 − δ 0 ,<label>(17)</label></formula><p>where P is a prior over θ.</p><p>Within task generalization Next, we relate er(Q, D i , T i ) toêr(Q, D i , D * i ) via the PAC-Bayes bound. For a fixed task i, task training data D i , a prior π(φ|T i ) that only depends on the training data, and any δ i ∈ (0, 1], we have that</p><formula xml:id="formula_26">P E (x * ,y * )∼p(x,y|Ti)ρ(φi) [L(φ i (x * ), y * )] ≤E ρ(φi)   1 K (x * ,y * )∈D * i L(φ i (x * ), y * )   + 1 2(K − 1) D KL (ρ||π) + log K δ i , ∀ρ ≥ 1 − δ i .</formula><p>Now, we choose π(φ|T i ) to be P (θ)q(φ|θ, D i )dθ and restrict ρ(φ) to be of the form Q(θ)q(φ|θ, D i )dθ for any Q. While, π and ρ may be complicated distributions (especially, if they are defined implicitly), we know that with this choice of π and ρ, D KL (ρ||π) ≤ D KL (Q||P ) (Cover &amp; <ref type="bibr" target="#b4">Thomas, 2012)</ref>, hence, we have</p><formula xml:id="formula_27">P er(Q, D i , T i ) ≤êr(Q, D i , D * i ) + 1 2(K − 1) D KL (Q P ) + log K δ i , ∀Q ≥ 1 − δ i<label>(18)</label></formula><p>Overall bound on meta-learner generalization Combining Eq. <ref type="formula" target="#formula_1">(17)</ref> and <ref type="formula" target="#formula_1">(18)</ref> using the union bound, we have</p><formula xml:id="formula_28">P er(Q) ≤ 1 n n i=1ê r(Q, D i , D * i ) + 1 2(K − 1) D KL (Q P ) + log K δ i + 1 2(n − 1) D KL (Q P ) + log n δ 0 , ∀Q ≥ 1 − i δ i + δ 0<label>(19)</label></formula><p>Choosing δ 0 = δ K+1 and δ i = Kδ n(K+1) , then we have:</p><formula xml:id="formula_29">P er(Q) ≤ 1 n n i=1ê r(Q, Di, D * i ) + 1 2(K − 1) + 1 2(n − 1) DKL(Q P ) + log n(K + 1) δ , ∀Q ≥1 − δ.<label>(20)</label></formula><p>Because n is generally large, by Taylor expansion of the complexity term we have</p><formula xml:id="formula_30">1 2(K − 1) + 1 2(n − 1) D KL Q||P ) + log n(K + 1) δ = 1 2 log n(K + 1)/δ 1 2(K − 1) + 1 2(n − 1) D KL Q||P ) + 2 log( n(K + 1) δ ) + o(1)</formula><p>Re-defining the coefficient of KL term as β and omitting the constant and higher order term, we recover the meta-regularization bound in Eq.(5) when Q(θ) = N (θ; θ µ , θ σ ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 EXPERIMENTAL DETAILS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5.1 POSE PREDICTION</head><p>We create a multi-task regression dataset based on the Pascal 3D data <ref type="bibr" target="#b35">(Xiang et al., 2014)</ref>. The dataset consists of 10 classes of 3D object such as "aeroplane", "sofa", "TV monitor", etc. Each class has multiple different objects and there are 65 objects in total. We randomly select 50 objects for meta-training and the other 15 objects for meta-testing. For each object, we use MuJoCo <ref type="bibr" target="#b33">(Todorov et al., 2012)</ref> to render 100 images with random orientations of the instance on a table, visualized in <ref type="figure" target="#fig_0">Figure 1</ref>. For the meta-learning algorithm, the observation (x) is the 128 × 128 gray-scale image and the label (y) is the orientation re-scaled to be within [0, 10]. For each task, we randomly sample meta batch-size of 10 tasks per iteration.</p><p>For MR-CNP, we use a convolutional encoder with a fully connected bottom layer to map the input image to a 20-dimensional latent representation z and z * for task training input x and test input x * respectively. The (z, y) are concatenated and mapped by the feature extractor and aggregator which are fully connected networks to the 200 dimensional task summary statistics φ. The decoder is a fully connected network that maps (φ, z * ) to the predictionŷ * .</p><p>For MR-MAML, we use a convolutional encoder to map the input image to a 14 × 14 dimensional latent representation z and z * . The pairs (z, y) are used in the task adaptation step to get a task specific parameter φ via gradient descent. Then z * is mapped to the predictionŷ * with a convolutional predictor parameterized by φ. The network is trained using 5 gradient steps with learning rate 0.01 in the inner loop for adaptation and evaluated using 20 gradient steps at the test-time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5.2 NON-MUTUALLY-EXCLUSIVE CLASSIFICATION</head><p>The Omniglot dataset consists of 20 instances of 1623 characters from 50 different alphabets. We randomly choose 1200 characters for meta-training and use the remaining for testing. The metatraining characters are partitioned into 60 disjoint sets for 20-way classification. The MiniImagenet dataset contains 100 classes of images including 64 training classes, 12 validation classes, and 24 test classes. We randomly partition the 64 meta-training classes into 13 disjoint sets for 5-way classification with one label having one less class of images than the others.</p><p>For MR-MAML we use a convolutional encoder similar to the pose prediction problem. The dimension of z and z * is 14 × 14 for Omniglot and 20 × 20 for MiniImagenet. We use a convolutional decoder for both datasets. Following <ref type="bibr" target="#b6">(Finn et al., 2017)</ref>, we use a meta batch-size of 16 for 20-way Omniglot classification and meta batch-size of 4 for 5-way MiniImagenet classification. The metalearning rate is chosen from {0.001, 0.005} and the β for meta-regularized methods are chosen from {10 −7 , 10 −6 , . . . , 10 −3 }. The optimal hyperparameters are chosen for each method separately via cross-validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 ADDITIONAL ILLUSTRATION AND GRAPHICAL MODEL</head><p>We show a standard few-shot classification setup in meta-learning to illustrate a mutually-exclusive task distribution and a graphical model for the regularization on the activations. <ref type="figure">Figure 3</ref>: An example of mutually-exclusive task distributions. In each task of mutually-exclusive few-shot classification, different classes are randomly assigned to the N -way classification labels. The same class, such as the dog and butterfly in this illustration, can be assigned different labels across tasks which makes it impossible for one model to solve all tasks simultaneously. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7 ADDITIONAL RESULTS</head><p>As shown in <ref type="figure">Figures 5, 7</ref> and 8, when meta-learning algorithms converge to the memorization solution, the test tasks must be similar to the train tasks in order to achieve low test error. For CNP, although the task training set contains sufficient information to infer the correct amplitude, this information is ignored and the regression curve at test-time is determined by the one-hot vector. As a result, CNP can only generalize to points from the curves it has seen in the training <ref type="figure">(Figure 7</ref> first row). On the other hand, MAML does use the task training data ( <ref type="figure">Figure 5, 8</ref> and <ref type="table" target="#tab_0">Table 1)</ref>, however, its performance is much worse than in the mutually-exclusive task. MR-MAML and MR-CNP avoid converging to a memorization solution and achieve excellent test performance on sinusoid task. <ref type="figure">Figure 5</ref>: Test MSE on the mutually-non-exclusive sinusoid problem as function of the number of gradient steps used in the inner loop of MAML and MR-MAML. For each trial, we calculate the mean MSE over 100 randomly generated meta-testing tasks. We report the mean and standard deviation over 5 random trials.</p><p>(a) CNP (b) MR-CNP (W) (c) MAML (d) MR-MAML (W) <ref type="figure">Figure 6</ref>: Visualization of the optimized weight matrix W that is connected to the inputs in the sinusoid regression example. The input x = (u, A) where u ∼ Unif(−5, 5), A is 20 dimensional one-hot vector and the intermediate layer is 100 dimensional, hence x ∈ R 21 and W ∈ R 21×100 . For both CNP and MAML, the meta-regularization restricts the part of weights that is connected to A close to 0. Therefore it avoids storing the amplitude information in weights and forces the amplitude to be inferred from the task training data D, hence preventing the memorization problem. In <ref type="table" target="#tab_4">Table 5</ref>, we report the pre-update accuracy for the non-mutually-exclusive classification experiment in Section 6.3. The pre-update accuracy is obtained by the initial parameters θ rather than the task adapted parameters φ. At the meta-training time, for both MAML and MR-MAML the post-update accuracy obtained by using φ gets close to 1. High pre-update accuracy reflects the memorization problem. For example, in 20-way 1-shot Omniglot example, the pre-update accuracy for MAML is 99.2% at the training time, which means only 0.8% improvement in accuracy is due to adaptation, so the task training data is ignored to a large extent. The pre-update training accuracy for MR-MAML is 5%, which means 95% improvement in accuracy during training is due to the adaptation. This explains why in <ref type="table" target="#tab_3">Table 4</ref>, the test accuracy of MR-MAML is much higher than that of MAML at the test-time, since the task training data is used to achieve fast adaptation.  ization strength β on the mutually-exclusive 20-way 1-shot Omniglot problem. The plot shows the mean and standard deviation across 5 meta-training runs. When β is small, MR-MAML slightly outperforms MAML, indicating that meta-regularization does not degrade performance on mutually-exclusive tasks. The accuracy numbers are not directly comparable to previous work (e.g., <ref type="bibr" target="#b6">(Finn et al., 2017)</ref>) because we do not use data augmentation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Left: An example of non-mutually-exclusive pose prediction tasks, which may lead to the memorization problem. The training tasks are non-mutually-exclusive because the test data label (right) can be inferred accurately without using task training data (left) in the training tasks, by memorizing the canonical orientation of the meta-training objects. For a new object and canonical orientation (bottom), the task cannot be solved without using task training data (bottom left) to infer the canonical orientation. Right: Graphical model for meta-learning. Observed variables are shaded. Without either one of the dashed arrows,Ŷ * is conditionally independent of D given θ and X * , which we refer to as complete memorization (Definition 1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>H</head><label></label><figDesc>p,q (y * 1:N |x * 1:N , D 1:N , θ) = H(y * 1:N |x * 1:N , D 1:N , ξ) + I(ξ; y * 1:N |x * 1:N , D 1:N , θ) + E [D KL (p(y * 1:N |x * 1:N , D 1:N , θ)||q(y * 1:N |x * 1:N , D 1:N , θ))] + I(D 1:N ; θ|x * 1:N , ξ) − I(y * 1:N , D 1:N ; θ|x * 1:N , ξ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Graphical model of the regularization on activations. Observed variables are shaded and Z is bottleneck variable. The complete memorization corresponds to the graph without the dashed arrows.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 :Figure 9 :</head><label>89</label><figDesc>Meta-test results on the non-mutually-exclusive sinusoid regression problem with MAML. For each row, the true amplitudes of the true curves (orange) are randomly sampled uniformly from [0.1, 4]. For illustrative purposes, we fix the one-hot vector component of the input. (a): Due to memorization, MAML adapts slowly and has large generalization error at test-time. (b) (c): Adding meta-regularization on both activation and weights recovers efficient adaptation. Sensitivity of activation regularization and weight regularization with respect to the learning rate on the pose prediction problem. For activation regularization, lower training loss corresponds to higher test MSE which indicates that the memorization solution is not solved. For weights regularization, lower training loss corresponds to lower test MSE which indicates proper training can converge to the adaptation solution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 10 :</head><label>10</label><figDesc>The test accuracy of MAML with meta-regularization on the weights as a function of the regular-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell>deviation in</cell></row></table><note>Test MSE for the non-mutually-exclusive sinusoid regression problem. We compare MAML and CNP against meta-regularized MAML (MR-MAML) and meta-regularized CNP (MR-CNP) where regularization is either on the activations (A) or the weights (W). We report the mean over 5 trials and the standard</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Meta-test MSE for the pose prediction problem. We compare MR-MAML (ours) with conventional MAML and fine-tuning (FT). We report the average over 5 trials and standard deviation in parentheses.</figDesc><table><row><cell>Method</cell><cell>MAML</cell><cell>MR-MAML (W) (ours)</cell><cell>CNP</cell><cell>MR-CNP (W) (ours)</cell><cell>FT</cell><cell>FT + Weight Decay</cell></row><row><cell>MSE</cell><cell>5.39 (1.31)</cell><cell>2.26 (0.09)</cell><cell>8.48 (0.12)</cell><cell>2.89 (0.18)</cell><cell>7.33 (0.35)</cell><cell>6.16 (0.12)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Meta-testing MSE for the pose prediction problem. We compare MR-CNP (ours) with conventional CNP, CNP with weight decay, and CNP with Bayes-by-Backprop (BbB) regularization on all the weights. We report the average over 5 trials and standard deviation in parentheses.</figDesc><table><row><cell>Methods</cell><cell>CNP</cell><cell cols="3">CNP + Weight Decay CNP + BbB MR-CNP (W) (ours)</cell></row><row><cell>MSE</cell><cell>8.48 (0.12)</cell><cell>6.86 (0.27)</cell><cell>7.73 (0.82)</cell><cell>2.89 (0.18)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Meta-test accuracy on non-mutually-exclusive (NME) classification. The fine-tuning and nearest- neighbor baseline results for MiniImagenet are from</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Meta-training pre-update accuracy on non-mutually-exclusive classification. MR-MAML controls the meta-training pre-update accuracy close to random guess and achieves low training error after adaptation.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In practice, L(x * , y * , φi) is MSE on a bounded target space or classification accuracy. We optimize the negative log-likelihood as a bound on the 0-1 loss.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We assume that the number of classes in the meta-training set is larger than N .</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENT</head><p>The authors would like to thank Alexander A. Alemi, Kevin Murphy, Luke Metz, Abhishek Kumar and the anonymous reviewers for helpful discussions and feedback. M. Yin and M. Zhou acknowledge the support of the U.S. National Science Foundation under Grant IIS-1812699.   </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Emergence of invariance and disentanglement in deep representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Achille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1947" to="1980" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Alexander A Alemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">V</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Murphy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00410</idno>
		<title level="m">Deep variational information bottleneck</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Meta-learning by adjusting priors based on extended pac-bayes theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Amit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Meir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="205" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Cornebise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.05424</idno>
		<title level="m">Weight uncertainty in neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Elements of information theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joy A</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thomas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02185</idno>
	</analytic>
	<monogr>
		<title level="m">Amos Storkey. Towards a neural statistician</title>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A bayesian approach to unsupervised one-shot learning of object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Ninth IEEE International Conference on Computer Vision</title>
		<meeting>Ninth IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="1134" to="1141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Probabilistic model-agnostic meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9516" to="9527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A theoretical framework for deep transfer learning. Information and Inference: A</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomer</forename><surname>Galanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamir</forename><surname>Hazan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the IMA</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="159" to="209" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><surname>Garnelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Saxton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murray</forename><surname>Shanahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Eslami</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.01613</idno>
		<title level="m">Conditional neural processes</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><surname>Garnelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Danilo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Teh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Neural processes. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bronskill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09921</idno>
		<title level="m">Metalearning probabilistic inference for prediction</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Recasting gradientbased meta-learning as hierarchical bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erin</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Griffiths</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.08930</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Towards understanding generalization in gradientbased meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Guiroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Pal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.07287</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Meta-learning priors for efficient online bayesian regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apoorva</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pavone</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.08912</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Task agnostic meta-learning for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><forename type="middle">Abdullah</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamal</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11719" to="11727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Bayesian model-agnostic meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesup</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ousmane</forename><surname>Dia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungwoong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Ahn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.03836</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML deep learning workshop</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A simple weight decay can improve generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Krogh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">A</forename><surname>Hertz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="950" to="957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">One shot learning of simple visual concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brenden</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the annual meeting of the cognitive science society</title>
		<meeting>the annual meeting of the cognitive science society</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Discrete infomax codes for meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoonho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonjae</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11656</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pac-bayesian model averaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcallester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="1999" />
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="164" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning to adapt in dynamic, real-world environments through metareinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anusha</forename><surname>Nagabandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignasi</forename><surname>Clavera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Fearing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Finn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.11347</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A pac-bayesian bound for lifelong learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasia</forename><surname>Pentina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="991" to="999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Efficient off-policy meta-reinforcement learning via probabilistic context variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Rakelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurick</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deirdre</forename><surname>Quillen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.08254</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2016</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Metalearning with memory-augmented neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1842" to="1850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Evolutionary principles in self-referential learning. On learning how to learn: The meta-meta-... hook.) Diploma thesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jurgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
		<respStmt>
			<orgName>Institut f. Informatik, Tech. Univ. Munich</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">A Bayesian framework for concept learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Brett Tenenbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Learning to learn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorien</forename><surname>Pratt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep learning and the information bottleneck principle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naftali</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noga</forename><surname>Zaslavsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Information Theory Workshop (ITW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naftali</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bialek</surname></persName>
		</author>
		<title level="m">The information bottleneck method. arXiv preprint physics/0004057</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Mujoco: A physics engine for model-based control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuel</forename><surname>Todorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Tassa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="5026" to="5033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Beyond pascal: A benchmark for 3d object detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roozbeh</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">For illustrative purposes, we fix the one-hot vector component of the input. (a): The vanilla CNP cannot adapt to new task training data at test-time and the shape of prediction curve (blue) is determined by the one-hot amplitude not the task training data. (b) (c): Adding meta-regularization on both activation and weights enables the CNP to use the task training data at meta-training</title>
	</analytic>
	<monogr>
		<title level="m">Meta-test results on the non-mutually-exclusive sinusoid regression problem with CNP</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note>For each row, the amplitudes of the true curves (orange) are randomly sampled uniformly from. and causes the model to generalize well at test-time</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
