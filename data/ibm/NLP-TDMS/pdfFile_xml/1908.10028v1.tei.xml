<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Attention-based Dropout Layer for Weakly Supervised Object Localization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Choe</surname></persName>
							<email>junsukchoe@yonsei.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">School of Integrated Technology</orgName>
								<orgName type="institution">Yonsei University</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjung</forename><surname>Shim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Integrated Technology</orgName>
								<orgName type="institution">Yonsei University</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Attention-based Dropout Layer for Weakly Supervised Object Localization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Weakly Supervised Object Localization (WSOL) techniques learn the object location only using image-level labels, without location annotations. A common limitation for these techniques is that they cover only the most discriminative part of the object, not the entire object. To address this problem, we propose an Attention-based Dropout Layer (ADL), which utilizes the self-attention mechanism to process the feature maps of the model. The proposed method is composed of two key components: 1) hiding the most discriminative part from the model for capturing the integral extent of object, and 2) highlighting the informative region for improving the recognition power of the model. Based on extensive experiments, we demonstrate that the proposed method is effective to improve the accuracy of WSOL, achieving a new state-of-the-art localization accuracy in CUB-200-2011 dataset. We also show that the proposed method is much more efficient in terms of both parameter and computation overheads than existing techniques.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Weakly Supervised Object Localization (WSOL) aims to identify the location of the object in a scene only using image-level labels, not location annotations. Existing approaches mine and track discriminative features of each class for object detection <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b5">6]</ref> and segmentation <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b49">50]</ref>. Because the discriminative power of each object part is different from another, these techniques tend to identify only the most discriminative part of the target object, incapable of covering entire extent of the object. For example, in the case of a person, the face may be more discriminative than the body which appearance changes dramatically due to clothing. In this case, existing WSOL techniques can localize only the face, not the entire region.</p><p>This problem can be critical in object localization. Specifically, Class Activation Mappings (CAM) <ref type="bibr" target="#b62">[63]</ref> utilize * Corresponding author.</p><p>Convolutional Neural Networks (CNN) classifier for learning the discriminative features. The key idea is that the classifier with a reasonable accuracy should observe the object region to decide the class label. In other words, the discriminative features should co-occur with the object region. From this idea, they perform localization by tracking spatial distribution of feature responses. Unfortunately, the classifiers tend to focus only on the most discriminative features to increase their classification accuracy. Therefore, the spatial distribution of feature responses also tends to cover only the most discriminative part of the object, which leads to localization accuracy degradation.</p><p>Recently, various techniques <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b59">60]</ref> have been proposed to address this issue. Most of them <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b19">20]</ref> erased the most discriminative region on the input image or feature map by zeroing that region during the training phase. These techniques are similar to the dropout <ref type="bibr" target="#b37">[38]</ref> in that they deactivate specific nodes of the feature map by setting them zero during the training phase. This prevents the model from relying solely on the most discriminative part for classification, instead encourages it to learn the less discriminative part as well. To achieve this goal, Hide-and-Seek (HaS) <ref type="bibr" target="#b34">[35]</ref> divides the input image into grid-like patches and randomly selects the patches to erase. While the random selection is simple and fast, it cannot effectively erase the most discriminative part.</p><p>For effectively removing only the most discriminative part, several techniques <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b19">20]</ref> have been proposed. These techniques re-train the model multiple times <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b16">17]</ref>, use additional classifiers <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b58">59]</ref>, or perform two forwardbackward propagations per one iteration <ref type="bibr" target="#b19">[20]</ref> for finding the most discriminative part. Consequently, huge additional computing resources are required to eliminate the most discriminative part effectively.</p><p>From previous methods, we conclude that the idea of erasing only the most discriminative part is effective to capture the full extent of object. However, existing methods require substantial computing resources to remove the most discriminative part accurately. Our goal is to erase the most discriminative part in an effective and efficient way. To this end, we propose an Attention-based Dropout Layer <ref type="figure">Figure 1</ref>. ADL block diagram. The self-attention map is generated by channelwise average pooling of the input feature map. Based on the self-attention map, we produce a drop mask using thresholding and an importance map using a sigmoid activation, respectively. The drop mask and the importance map are selected stochastically at each iteration and applied to the input feature map. Please note that this figure illustrates the case when the importance map is selected.</p><p>(ADL), a lightweight yet powerful method which utilizes self-attention mechanism to remove the most discriminative part of the target object.</p><p>Specifically, a self-attention map is obtained by performing channelwise average pooling on the input feature map. Based on the self-attention map, we produce two key components of ADL, a drop mask and an importance map. The drop mask is used to hide the most discriminative part during training. This induces the model to learn the less discriminative part as well. We obtain this drop mask by thresholding the self-attention map. The importance map is used to highlight informative region for improving the classification power of the model. Owing to the importance map, the more accurate self-attention map can be produced. The importance map is computed by applying sigmoid activation to the self-attention map. During training, either one of the drop mask or importance map is stochastically selected at each iteration, and then the selected one is applied to the input feature map by spatialwise multiplication. <ref type="figure">Figure 1</ref> shows the block diagram of the proposed method.</p><p>Compared to existing WSOL techniques, the proposed method is much more efficient in terms of both computation and parameter overheads. This is because we can find and erase the most discriminative region by a single forward-backward propagation in a single model. In addition, regardless of the model architecture, ADL can be easily applied to convolutional feature maps of the model to improve the localization accuracy. Compared to existing self-attention techniques <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b52">53]</ref>, the proposed method is greatly lightweight because there are no additional trainable parameters for extracting self-attention map.</p><p>The proposed method is lightweight and efficient, and also report excellent accuracy. Quantitatively, the proposed method achieves superior accuracy, more than 15 percentage points of accuracy improvement, over the existing stateof-the-art techniques <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b59">60]</ref> on CUB-200-2011 dataset <ref type="bibr" target="#b43">[44]</ref>, and comparable accuracy to the current state-of-the-art technique <ref type="bibr" target="#b59">[60]</ref> on ImageNet-1k dataset <ref type="bibr" target="#b29">[30]</ref>. We also observe consistent results in qualitative evaluation; the model with ADL learns the less discriminative part better than the vanilla model <ref type="bibr" target="#b62">[63]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Dropout. Dropout <ref type="bibr" target="#b37">[38]</ref> is a regularization technique to alleviate overfitting in neural networks. Specifically, dropout discards information by randomly zeroing each hidden node of the neural network during the training phase. In this way, the network can enjoy the ensemble effect of small subnetworks, thus achieving a good regularization effect. However, unlike fully connected layers, applying dropout to the convolutional feature map is not effective. One of the reasons is that spatially adjacent pixels are strongly correlated on the convolutional feature map; they share redundant contextual information. Hence, the conventional pixel-based dropout cannot completely discard the information on the convolutional feature map <ref type="bibr" target="#b41">[42]</ref>.</p><p>In order to apply dropout to the convolutional feature map, Tompson et al. <ref type="bibr" target="#b41">[42]</ref> proposed SpatialDropout that randomly drops partial channels of a feature map, rather than dropping each pixel. Based on this channel-based dropout, the problem of pixel-level dropout can be resolved. The proposed method differs from SpatialDropout in that we drop only strongly activated regions, rather than dropping entire region of channel. Owing to this region-based dropout, we could also bypass the problem of pixel-level dropout. Meanwhile, Park and Kwak <ref type="bibr" target="#b26">[27]</ref> proposed MaxDrop, which drops the maximally activated pixel through channelwise or spatialwise on the feature map. Similar to MaxDrop, the proposed method drops strongly activated part. However, we differ from MaxDrop in that we use attention mechanism to find the maximally activated part. In addition, the proposed method does not drop the maximally activated pixel, but maximally activated region.</p><p>Attention mechanism. Humans selectively use an important part of the data to make a decision <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13]</ref>. Similarly, when a query comes in, the artificial model does not process all the data equally, but focuses only on the important data. This process is called attention mechanism and is actively used in various fields such as machine translation <ref type="bibr" target="#b42">[43]</ref>, image captioning <ref type="bibr" target="#b54">[55]</ref>, image inpainting <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b22">23]</ref>, transfer learning <ref type="bibr" target="#b56">[57]</ref>, visual question answering <ref type="bibr" target="#b63">[64]</ref>, and generative model <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b59">60]</ref>. When the query is input itself, such attention is specifically called self-attention, which is effective to learn the meaningful representation for conducting the given task. For example, in the case of the classification task, the self-attention map appears in a form that emphasizes informative features for classification (e.g., the most discriminative part of the target object).</p><p>Recently, various methods <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b52">53]</ref> utilize the self-attention mechanism to enhance the accuracy of the CNN classification model. Residual Attention Networks (RAN) <ref type="bibr" target="#b45">[46]</ref> has improved the accuracy of the classification model using 3D self-attention map. However, the parameter overheads are very large because the raw feature map without any compression is used for attention extraction. Squeeze-and-Excitation (SE) <ref type="bibr" target="#b11">[12]</ref> increases the accuracy of the classification model using only 1D channel self-attention map. For extracting the self-attention map, the feature map is first compressed using Global Average Pooling (GAP) and then passed through 2-layer MLP. In this way, SE can significantly reduce the parameter overheads for attention extraction compared to RAN. However, the parameter overheads are still not negligible (e.g., 10% on ResNet50 <ref type="bibr" target="#b9">[10]</ref>).</p><p>Bottleneck Attention Module (BAM) <ref type="bibr" target="#b25">[26]</ref> and Convolutional Block Attention Module (CBAM) <ref type="bibr" target="#b52">[53]</ref> increase the accuracy of the classifier by utilizing both 1D channel and 2D spatial self-attention maps. They compute the spatial self-attention map using auxiliary convolutional layer(s). The computed self-attention map is applied to the input feature map for rewarding the informative region. Likewise, the proposed method uses the importance map for rewarding the informative region. However, the key difference from them is that we stochastically penalize that region using the drop mask. Also, unlike these techniques, we do not require additional trainable parameters for extracting the self-attention map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">ADL: Attention-based Dropout Layer</head><p>In this section, we present details of the proposed method, Attention-based Dropout Layer (ADL). ADL is applied on each feature map of classification model, and induces the model to learn the entire region of the object. ADL generates a self-attention map from input feature map, and produces a drop mask and an importance map. Although both components are computed from self-attention map, they play the opposite role. The drop mask penalizes the most discriminative part for inducing the model to cover the integral extent of the object. Meanwhile, the importance map rewards the most discriminative part for increasing the classification power of the model. During training, the drop mask or importance map is stochastically selected for each iteration. Then, the selected one is applied to the input feature map. By applying each component stochastically, we can enjoy their advantages simultaneously. ADL has two main hyperparameters: drop rate and γ. The drop rate indicates how frequently the drop mask is applied, and the γ controls the size of the region to be dropped. The example of each component is visualized in <ref type="figure">Figure 2</ref>.</p><p>Specifically, the input of ADL is a convolutional feature map F ⊆ R H×W ×C . Note that C is the number of channels, H and W are height and width, respectively. For simplicity, we omit the mini-batch dimension in this notation. We generate a self-attention map M att ⊆ R H×W by compressing F using channelwise average pooling. Because the model is trained for classification, the intensity of each pixel in the self-attention map is proportional to the discriminative power. In this way, we can approximate the spatial distribution of the most discriminative part efficiently.</p><p>To obtain the drop mask, we first set a drop threshold by prefixed ratio γ of maximum intensity of the self-attention map. Then, we produce the drop mask M drop ⊆ R H×W by setting each pixel to 0 if it is larger than drop threshold, and 1 if it is smaller. That is, the drop mask has 0 for the most discriminative region and 1 for otherwise. Note that the size of region to be dropped increases as γ decreases, and vice versa. The drop mask is applied to the input feature map by spatialwise multiplication. In this way, we can hide the most discriminative part from the model; we encourage the model to learn the less discriminative part for classification but meaningful region for localization. However, if the drop mask is applied at every iteration, the most discriminative part is never observed during the training phase. As a result, the classification accuracy of the model is significantly decreased, which adversely affects the localization accuracy. To remedy this, we stochastically apply the drop mask according to drop rate. When the drop mask is not applied, the importance map is applied instead. We generate the importance map M imp ⊆ R H×W from the selfattention map by applying sigmoid activation. That is, the intensity of each pixel in the importance map is close to 1 for the most discriminative region, and close to 0 for the less discriminative region. Like the drop mask, the importance map is applied to the input feature map by spatialwise multiplication. In this way, we can improve the classification accuracy of the model.</p><p>The proposed method is applied independently to each convolutional feature map. Therefore, it can be easily plugged into multiple feature maps of existing classification <ref type="figure">Figure 2</ref>. Drop mask and self-attention map at each layer of VGG-GAP <ref type="bibr" target="#b62">[63]</ref>. At lower-level layers, the self-attention maps include general features, while class-specific features are included in the self-attention maps at higher-level layers. The drop masks also erase most discriminative part more effectively at higher-level layers. Please note that the drop mask is overlaid with input image for better visualization. Because the importance map has a distribution very similar to that of the self-attention map, we do not visualize it. models for improving localization accuracy. In addition, it does not require any trainable parameters. That means, there is no parameter overheads even when applied to multiple feature maps at the same time. Furthermore, with ADL, the most discriminative region can be identified and erased efficiently, without auxiliary classifiers <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b58">59]</ref>, re-training <ref type="bibr" target="#b48">[49]</ref>, or additional forward-backward propagation <ref type="bibr" target="#b19">[20]</ref>.</p><p>ADL is an auxiliary module which is applied only during training. During the testing phase, ADL is deactivated. That is, our testing phase is identical to that of vanilla model. Therefore, the object localization can be performed using various heatmap extraction methods <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b19">20]</ref> without bells and whistles. Note that we do not compensate the different distributions between training and testing, as other dropout-based WSOL techniques <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b58">59]</ref>. Relation with other attention extraction methods. Our extraction method does not require trainable parameters, much lightweight compared to existing methods <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b52">53]</ref>. Thus, one might wonder how our method can produce semantically meaningful results despite its simplicity.</p><p>Recently, Zagorukyo and Komodakis <ref type="bibr" target="#b56">[57]</ref> showed that the informative region for transfer learning can be identified by applying the channelwise pooling to the convolutional feature map. That is, the self-attention map for transfer learning is obtained by the channelwise pooling. Inspired by this, CBAM <ref type="bibr" target="#b52">[53]</ref> utilized the self-attention map to improve the classification accuracy. Specifically, they refine the map using auxiliary convolutional layer and sigmoid activation. This refined self-attention map is applied to input feature map by spatialwise multiplication. In this way, the auxiliary convolutional layers are trained to refine the selfattention map for improving the classification accuracy.</p><p>However, from the empirical study, we observe that the self-attention map may not need to be refined by auxiliary layers. We conjecture that it is because existing convolutional layers in CNN model are sufficiently powerful to produce meaningful self-attention map. Hence, after computing the self-attention map by channelwise average pooling, we normalize this map using sigmoid activation and then multiply it to input feature map. Then, the gradient from the loss function updates existing convolutional layers so that the resultant self-attention map is useful for improving classification accuracy. For example, if the self-attention map fails to highlight the object region, this may degrade the classification accuracy. Hence, existing convolutional layers are trained to produce more accurate self-attention map. This is equivalent to assigning the role of the auxiliary convolutional layer used in CBAM to the existing convolutional layers in the model. Note that the similar principle was introduced by Lin et al. <ref type="bibr" target="#b21">[22]</ref>; they replace the fully connected layers of CNN classifier with the GAP layer.</p><p>The improvement of classification accuracy of our attention method may not be as great as that of CBAM. However, our method is much more efficient and can produce sufficiently meaningful results for our application. This is shown in our experimental results; our self-attention map is effective to increase classification accuracy and identify the most discriminative part of the target object. Relation between drop mask and importance map. In our model, the drop mask penalizes the most discriminative part, while the importance map rewards the most discriminative part. One might consider the drop mask and importance map are mutually exclusive. However, our experimental results support that they are not mutually exclusive. We believe that it is because the drop mask can be accurately produced by the importance map. Specifically, as the importance map improves the classification accuracy, the more accurate self-attention map can be produced. Consequently, the drop mask can more effectively erase the most discriminative region of the object. Relation between classification and localization. Previous study <ref type="bibr" target="#b34">[35]</ref> has reported that the classification accuracy is compromised while the localization accuracy is increased. They conjecture that this is caused by the usage of a drop mask. Because we also use a drop mask to erase the most discriminative part, such a trade-off relationship between the accuracy of localization and that of classification is consistently observed in our experiments. However, the proposed method can boost the classification power using the importance map, thus the accuracy degradation of classification is not as significant as other techniques. Relation with the current state-of-the-arts. The current state-of-the-art techniques for WSOL are Adversarial Complementary Learning (ACoL) <ref type="bibr" target="#b58">[59]</ref> and Self-Produced Guidance (SPG) <ref type="bibr" target="#b59">[60]</ref>. ACoL adds two auxiliary classifiers in parallel to the backbone feature extractor for finding the most discriminative part of the target object. The proposed method differs from ACoL in that we can find the most discriminative part without the additional classifier, which is much more efficient. Most recently, SPG has been proposed, a new WSOL technique that utilizes spatial distribution of the object and background. The classifier can learn the integral extent of the object using that distribution as auxiliary supervision. The proposed method differs from SPG in that SPG does not erase the most discriminative part of the object. In addition, SPG requires substantial computing resources for improving the localization accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>Dataset. We evaluate the performance of the proposed method in CUB-200-2011 <ref type="bibr" target="#b43">[44]</ref> and ImageNet-1k <ref type="bibr" target="#b29">[30]</ref>, respectively. The ImageNet-1k is a large-scale dataset with 1,000 different classes, consisting of approximately 1.3 million training images and 50,000 validation images. For this dataset, we train the model with the training set and evaluate the performance with the validation set.</p><p>The CUB-200-2011 includes 200 species of birds, consisting of 5,994 training images and 5,794 testing images. For this dataset, we train the model with the training set and evaluate the performance with the testing set. The intraclass variation of CUB-200-2011 is smaller than that of ImageNet-1k, because all classes of this dataset belong to birds. In this case, the extent of the most discriminative region might be quite small. For example, in Common Raven and White-necked Raven, there is no difference in appearance except the color of the neck. That is, the most discriminative part is the neck, which is very small compared to the entire area of the bird. Consequently, although CUB-200-2011 is not a large-scale dataset such as ImageNet-1k, this is a particularly challenging dataset to conduct WSOL. Implementation details. We use VGG <ref type="bibr" target="#b33">[34]</ref>, ResNet <ref type="bibr" target="#b9">[10]</ref>, MobileNetV1 <ref type="bibr" target="#b10">[11]</ref>, and InceptionV3 <ref type="bibr" target="#b39">[40]</ref> as backbone networks. Note that we replace the last pooling layer and two fully connected layers of VGG16 with a GAP layer, according to <ref type="bibr" target="#b62">[63]</ref>. We also use the customized InceptionV3 as a backbone, following the SPG <ref type="bibr" target="#b59">[60]</ref>. We plug SE block <ref type="bibr" target="#b11">[12]</ref> into ResNet50 for demonstrating the compatibility of ADL with other self-attention methods. For ResNet and Mo-bileNetV1, we set the stride of last strided convolution to 1 for enlarging the spatial resolution of heatmap to 14×14.</p><p>ADL is plugged in each feature map of the CNN model in a sequential way; the output of ADL is the input of the next layer. We use a pre-trained model which is trained with ImageNet-1k dataset <ref type="bibr" target="#b29">[30]</ref>, and then fine-tune the network. We extract the heatmap from classification model using CAM <ref type="bibr" target="#b62">[63]</ref>. Also, the bounding box is extracted from the heatmap using the same method as presented in <ref type="bibr" target="#b62">[63]</ref>. We implement the models using Tensorpack <ref type="bibr" target="#b53">[54]</ref> on Tensorflow <ref type="bibr" target="#b0">[1]</ref>, and train them using NVIDIA Titan Xp GPU.</p><p>Based on extensive ablation studies, we find that it is optimal to apply ADL to intermediate and higher-level layers of the network. Especially, for the intermediate layer, it is preferable to apply it to bottleneck part (e.g., pooling layer or strided convolution). We set the drop rate as 75%. For the drop threshold, we set γ to 80% for VGG-GAP and In-ceptionV3, 90% for ResNet, and 95% for MobileNetV1. However, the hyperparameters mentioned here are only the recommended settings. Note that the localization accuracy can be further improved when the optimal setting is used. Metrics. We use three evaluation metrics as <ref type="bibr" target="#b34">[35]</ref>: Top-1 classification accuracy (Top-1 Clas), Localization accuracy with known ground-truth class (GT-known Loc), and Top-1 localization accuracy (Top-1 Loc). Top-1 Clas determines that the answer is correct when the estimated class is equal to the ground truth class. GT-known Loc judges the answer as correct when the intersection over union (IoU) between the ground truth bounding box and estimated box for the ground truth class is 50% or more. Lastly, Top-1 Loc considers the answer as correct when both Top-1 Clas and GT- known Loc are correct. Please note that it is considered to be the most appropriate to use Top-1 Loc for evaluating overall localization performance, according to <ref type="bibr" target="#b29">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Ablation Study</head><p>In this subsection, we utilize pre-trained VGG-GAP <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b62">63]</ref> as a backbone network. For training, we plug ADLs in all the pooling layers and the conv5-3 layer, and then finetune the model using CUB-200-2011 dataset.</p><p>First, we visualize the self-attention map and drop mask in <ref type="figure">Figure 2</ref>. We observe that the self-attention maps of lower-level layers (i.e., pool1 and pool2) contain classagnostic general features. Meanwhile, the self-attention maps of higher-level layers (i.e., pool4 and conv5-3) contain the class-specific features. We also observe that the drop masks from higher-level layers erase the most discriminative part more accurately than those from lower-level layers.</p><p>Next, we investigate the effect of drop rate on accuracy. The upper part of <ref type="table">Table 1</ref> reports the results. From these results, we observe that the best localization accuracy can be achieved when the drop rate is 75%. Meanwhile, when the drop mask is applied at every iteration (i.e., drop rate 100%), the classification (Top-1 Clas) and localization (Top-1 Loc) accuracy are greatly reduced. This is because, as mentioned in Section 3, the model never observe the most discriminative part. As a result, the classification power of the model decreases significantly, which adversely influences localization accuracy. Given that accuracy degradation in GT-known Acc is relatively less than that of Top-1 Loc and that of Top-1 Clas, we can conclude that this is the result of the classification accuracy degradation.</p><p>We observe that the classification accuracy increases as the drop rate decreases. However, when the drop rate be- comes too low (drop rate from 25% to 0%), the classification accuracy decreases again (from 68.99% to 67.78%). We believe this is caused by overfitting. The drop mask is a dropout-based technique and its rationale is similar to MaxDrop <ref type="bibr" target="#b26">[27]</ref>. Thus, the drop mask with proper drop rate may prevent overfitting, increasing the classification accuracy. We consider that the analysis of regularization effect of the drop mask is beyond the scope of this paper. Yet, we plan to analyze this rigorously in future work. Third, we observe the effect of each component on the accuracy by deactivating the importance map or drop mask, respectively. The lower part of <ref type="table">Table 1</ref> summarizes the experimental results. From this, we can confirm that applying the drop mask and the importance map at the same time has better localization accuracy than applying only one of them. This supports the argument that the drop mask and importance map are not mutually exclusive.</p><p>When the importance map is applied alone, the classification accuracy increases but the localization accuracy decreases. We believe that this is because the classifier focuses more on the most discriminative part, guided by the importance map. This result supports our argument that the proposed lightweight attention method is effective to improve the classification accuracy. On the other hand, when drop mask is applied alone, the localization accuracy increases but the classification accuracy decreases. We believe that this is because the model utilizes less discriminative parts for classification, guided by the drop mask. These results also support the observation that the accuracy of localization and classification are in a trade-off relationship when applying the drop mask <ref type="bibr" target="#b34">[35]</ref>.</p><p>Lastly, we investigate the effects in accuracy upon the choice of feature maps where ADLs are employed and report the results in <ref type="table" target="#tab_1">Table 2</ref>. From these results, we can see that applying ADLs to additional convolutional feature maps further increases the localization accuracy. We find that the ADL can improve both localization and classification accuracy. However, the best localization accuracy can be achieved by sacrificing the classification accuracy. In addition, when the ADLs are applied to lower-level fea-  <ref type="table">Table 3</ref>. Quantitative evaluation results on CUB-200-2011 and ImageNet-1k. Bold text refers the best localization accuracy for each backbone network. We also underline the best score in each dataset. Overheads are computed based upon their backbone networks. The accuracy with asterisk* indicates that the score is from the original paper. We leave some Top-1 Clas scores blank, because they are not reported in the original paper <ref type="bibr" target="#b59">[60]</ref>. For reproducing baseline methods, we use hyperparameters suggested by their original papers <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b34">35]</ref>. Also, we train and test HaS and ADL under the same setting for a fair comparison.</p><p>ture maps such as pool2 and pool1, the localization accuracy rather decreases. We believe that this is because the lower-level feature maps include general features that are not related to the target class. Consequently, the most discriminative part cannot be effectively eliminated in lowerlevel feature maps using ADL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with State-of-the-art Methods</head><p>We compare the proposed method with various recent WSOL techniques including the state-of-the-art: CAM <ref type="bibr" target="#b62">[63]</ref>, HaS <ref type="bibr" target="#b34">[35]</ref>, ACoL <ref type="bibr" target="#b58">[59]</ref>, SPG <ref type="bibr" target="#b59">[60]</ref>. We report the accuracy of ACoL and SPG from their original paper. Meanwhile, we train the backbone networks using the same preprocessing method used in ACoL and SPG. Then, HaS or ADL are applied on the backbone networks. Considering the accuracy of vanilla model as baseline, we evaluate the accuracy gain of HaS and ADL, respectively. Please note that ACoL and SPG are the current state-of-the-art techniques for WSOL. In addition, among the techniques without parameter overheads, HaS performs the best. <ref type="figure" target="#fig_0">Figure 3</ref> visualizes the localization results on CUB-200-2011 and ImageNet-1k dataset for qualitative evaluation. From the results, we consistently observe that model with ADL captures the less discriminative parts better than vanilla model. For example, as seen from the left-most sample in the <ref type="figure" target="#fig_0">Figure 3</ref>, the heatmap and bounding box extracted from vanilla model only highlight the face of birds. Contrarily, the model with ADL covers not only the face, but also the entire part of the bird, from head to wing. In addition, from the right-most sample in the <ref type="figure" target="#fig_0">Figure 3</ref>, the vanilla model focuses only on the cylinder of revolver, whereas the model with ADL localizes the entire frame of the revolver.</p><p>Next, the quantitative evaluation results on CUB-200-2011 and ImageNet-1k datasets are summarized in <ref type="table">Table  3</ref>. To compare the computing resources required by each technique, we have described the number of parameters and both computation and parameter overheads along with Top-1 Loc and Top-1 Clas. ADL has no parameter overheads, and the computation overheads are nearly zero (e.g., 0.003% in ResNet50-SE) upon the backbone network. The proposed method is much more efficient than the existing state-of-the-art techniques, ACoL and SPG, in terms of both parameter and computation overheads.</p><p>We push further to maximize the efficiency of WSOL by employing MobileNetV1 <ref type="bibr" target="#b10">[11]</ref> as a backbone network. Due to the lightweight nature of MobileNetV1, it is inappropriate to employ ACoL or SPG which requires huge additional computing resources. On the other hand, ADL and HaS can be successfully employed despite a limited amount of resources. From the experimental results, we can observe that the accuracy gain of the proposed method is better than that of HaS. In addition, HaS has reduced classification accuracy against the baseline. This is caused by the trade-off relationship between localization and classification accuracy discussed in Section 3. Fortunately, the importance map of ADL can subside such a drawback by increasing the classification power. Consequently, the classification accuracy degradation of the ADL is not as significant as that of HaS.</p><p>In addition to its high efficiency, the proposed method achieves a new state-of-the-art localization accuracy on CUB-200-2011 dataset. When ResNet50-SE is employed as a backbone, the proposed method improves the localization accuracy by more than 15 percentage points over the state-of-the-art accuracy <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b59">60]</ref>. Please note that the number of parameters of ResNet50-SE with ADL is much smaller than that of ACoL and SPG. This achievement is quite impressive, considering that recent techniques are competing with the accuracy by 2-3 percentage points difference. Also, when the other three backbone networks are employed, the proposed method achieves better localization accuracy than the existing state-of-the-art techniques.</p><p>In the ImageNet-1k experiments, when VGG-GAP is used as a backbone, the accuracy of ADL is better than that of CAM, but slightly lower than that of ACoL. However, when ResNet50-SE is used as a backbone, localization accuracy of ADL is better than that of ACoL and comparable with that of SPG even though the required computing resources are much lower. In addition, when Incep-tionV3 is used as a backbone, comparable accuracy (0.11 percentage point difference) to SPG is achieved. In summary, we achieve new state-of-the-art accuracy on CUB-200-2011 dataset; on ImageNet-1k dataset, ADL achieves comparable accuracy with the current state-of-the-art technique <ref type="bibr" target="#b59">[60]</ref> despite its superior efficiency.</p><p>Discussion. We verified the proposed method on a singleobject detection task, following the current state-of-the-art methods <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b59">60]</ref>. However, it should be noted that the proposed method can be also used to improve the weakly supervised semantic segmentation accuracy. The classifier with ADL is the same as its vanilla version during testing, thus it can be easily combined with the weakly supervised semantic segmentation framework, such as <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b23">24]</ref>.</p><p>Next, to analyze the substantial difference in our accuracy gain between two datasets, we investigate our failure examples from ImageNet-1k experiments. From the failure case, we observe that the classifier extracts the discriminative features from the background which appears frequently with the target object. <ref type="figure" target="#fig_1">Figure 4</ref> illustrates such examples. In the case of the snowmobile class, the target object often co-occurs with snow. The vanilla model only focuses on the snowmobile, while the model with ADL learns not only the snowmobile, but also the snow and tree. This is because the background frequently appearing with the object might be the less discriminative region.</p><p>ImageNet-1k includes a wide variety of classes where specific types of background co-occur with the target object. In this case, the background has a certain level of discriminative power. Therefore, the model is likely to learn the background features when the most discriminative part is dropped. Meanwhile, since all classes of CUB-200-2011 belong to birds, similar backgrounds appear regardless of the classes (e.g. sky, tree). In other words, the background of this dataset is nearly independent of classes, thus the background is not a discriminative region <ref type="bibr" target="#b60">[61]</ref>. As a result, the model does not learn the features from the background although the most discriminative part is hidden. This explains the gap of our accuracy gain for two datasets; ADL has remarkable performance to induce the classifier to learn the less discriminative parts, as supported by CUB-200-2011 evaluations. We believe that this problem might be critical for all WSOL methods inducing the classifier to learn the less discriminative part. Currently, it seems non-trivial to solve this problem, thus we will address this issue in future work. Lastly, we note that the gap is not caused by the scale of dataset because ADL rarely fails for ImageNet-1k classes sharing similar background statistics (e.g., various breeds of dogs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We presented an Attention-based Dropout Layer (ADL), a novel weakly supervised object localization method that induces the CNN classifier to learn entire extent of the object. The proposed method is much more efficient and lightweight than existing state-of-the-art methods. In addition, the proposed method has achieved excellent performance; new state-of-the-art accuracy on CUB-200-2011, and comparable accuracy with current state-of-the-arts on ImageNet-1k. We also demonstrate that the proposed method can be easily applied to various CNN classifiers to improve the localization accuracy. For the future work, we will analyze the regularization effect of the drop mask. In addition, we will address the problem that the model learns the less discriminative region from outside of the object.  <ref type="table">Table 1</ref>. As the drop rate decreases, we can see that the model focuses only on the most discriminative part of the object.  <ref type="table">Table 1</ref>. When only an importance map is applied, the model focuses only on the most discriminative part object. On the other hand, when the drop mask is applied, the model can localize the entire extent of object. We note that the best localization result can be obtained using both drop mask and importance map.  <ref type="table" target="#tab_1">Table 2</ref>. We can see that the localization accuracy increases when the ADLs are applied on multiple feature maps. <ref type="figure">Figure 8</ref>. More qualitative evaluation results. The left image in each figure is input image. The red bounding box is ground truth, while the green bounding box is estimates. The middle image is heatmap and the right image shows the overlap between the input image and the heatmap. We also compared our method and the vanilla model side by side.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Qualitative evaluation results of VGG-GAP [63] on CUB-200-2011 and ImageNet-1k. The left image in each figure is input image.The red bounding box is ground truth, while the green bounding box is estimates. The middle image is heatmap and the right image shows the overlap between the input image and the heatmap. We also compared our method and the vanilla model side by side.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>The failure case on ImageNet-1k experiments. The target class is snowmobile. The model with ADL learns the less discriminative region which is not included in object. Specifically, the model captures not only the snowmobile, but also snow and tree.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Qualitative evaluation results corresponding to</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>Qualitative evaluation results corresponding to</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>Qualitative evaluation results corresponding to</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Effects in accuracy upon the choice of the feature maps to employ ADL. Bold text refers the best localization accuracy, while italic text refers the best classification accuracy.</figDesc><table><row><cell>Applied</cell><cell>GT-Known</cell><cell>Top-1</cell><cell>Top-1</cell></row><row><cell>feature map</cell><cell>Acc (%)</cell><cell>Clas (%)</cell><cell>Loc (%)</cell></row><row><cell>N/A</cell><cell>51.09</cell><cell>67.55</cell><cell>34.41</cell></row><row><cell>conv 5-3</cell><cell>57.99</cell><cell>68.95</cell><cell>41.73</cell></row><row><cell>+ pool4</cell><cell>68.22</cell><cell>67.17</cell><cell>48.02</cell></row><row><cell>+ pool3</cell><cell>75.41</cell><cell>65.27</cell><cell>52.36</cell></row><row><cell>+ pool2</cell><cell>71.85</cell><cell>63.76</cell><cell>48.46</cell></row><row><cell>+ pool1</cell><cell>74.78</cell><cell>62.25</cell><cell>49.69</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: a system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Weakly supervised deep detection networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2846" to="2854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Control of goaldirected and stimulus-driven attention in the brain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maurizio</forename><surname>Corbetta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gordon L Shulman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Neuroscience</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">201</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Weakly supervised cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Diba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><forename type="middle">Mohammad</forename><surname>Pazandeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="914" to="922" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A dualnetwork progressive approach to weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMMM</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="279" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Few-example object detection with model communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Wildcat: Weakly supervised learning of deep convnets for image classification, pointwise localization and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibaut</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Mordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="642" to="651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">C-WSL: Count-guided weakly supervised localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingfei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruichi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vlad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="152" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-fold MIL training for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Ramazan Gokberk Cinbis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2409" to="2416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A model of saliency-based visual attention for rapid scene analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ernst</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1254" to="1259" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep self-taught learning for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1377" to="1385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">ContextLocNet: Context-Aware deep network models for weakly supervised localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vadim</forename><surname>Kantorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="350" to="365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Weakly supervised object boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="183" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Donggeun Yoo, and In So Kweon. Two-Phase learning for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghyeon</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3534" to="3543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Seed, expand and constrain: Three principles for weakly-supervised image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="695" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Weakly supervised object localization with progressive domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3512" to="3520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Tell me where to look: Guided attention inference network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kuan-Chuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9215" to="9223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Towards computational baby learning: A weakly-supervised approach for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luoqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="999" to="1007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Network in network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Image inpainting for irregular holes using partial convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guilin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitsum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">J</forename><surname>Reda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="85" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Exploiting saliency for object segmentation from image level labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5038" to="5047" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Is object localization for free? -Weakly-supervised learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="685" to="694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Joon-Young Lee, and In So Kweon. BAM: Bottleneck Attention Module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongchan</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Analysis on the dropout effect in convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungheon</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nojun</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="189" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Image transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Constrained convolutional neural networks for weakly supervised segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1796" to="1804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Weakly supervised object localization using things and stuff transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miaojing</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3534" to="3543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ICLRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Hide-and-Seek: Forcing a network to be meticulous for weakly-supervised object and action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><forename type="middle">Kumar</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3544" to="3553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">On learning to localize objects with minimal supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun Oh</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaid</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Weakly-supervised discovery of visual pattern configurations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun Oh</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1637" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Pronet: Learning to propose objectspecific boxes for cascaded neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3485" to="3493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Attention networks for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mrigank</forename><surname>Eu Wern Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Rochan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Efficient object localization using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="648" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">The Caltech-UCSD Birds-200-2011 Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<idno>CNS-TR-2011-001</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Weakly supervised object localization with latent category learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiqiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="431" to="445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Residual attention network for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengqing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Selective convolutional descriptor aggregation for fine-grained image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu-Shen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Hao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2868" to="2881" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Object region mining with adversarial erasing: A simple classification to semantic segmentation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1568" to="1576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Stc: A simple to complex framework for weaklysupervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2314" to="2320" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Ts2c: tight box mining with surrounding segmentation context for weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="454" to="470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Revisiting dilated convolution: A simple approach for weakly-and semi-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7268" to="7277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Joon-Young Lee, and In So Kweon. CBAM: Convolutional Block Attention Module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongchan</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<ptr target="https://github.com/tensorpack/" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Generative image inpainting with contextual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5505" to="5514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Top-down neural attention by excitation backprop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><forename type="middle">Adel</forename><surname>Bargal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1084" to="1102" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Adversarial complementary learning for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1325" to="1334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Self-produced guidance for weaklysupervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="597" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Picking deep filter responses for finegrained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkai</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1134" to="1142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Ml-locnet: Improving object localization with multi-view learning network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="240" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Visual7w: Grounded question answering in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4995" to="5004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Soft proposal networks for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanzhao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbin</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1841" to="1850" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
