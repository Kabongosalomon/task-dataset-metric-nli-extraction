<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Shortformer: Better Language Modeling using Shorter Inputs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofir</forename><surname>Press</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science &amp; Engineering</orgName>
								<orgName type="department" key="dep2">Facebook AI Research</orgName>
								<orgName type="institution" key="instit1">University of Washington</orgName>
								<orgName type="institution" key="instit2">Allen Institute for AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">♦</forename></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science &amp; Engineering</orgName>
								<orgName type="department" key="dep2">Facebook AI Research</orgName>
								<orgName type="institution" key="instit1">University of Washington</orgName>
								<orgName type="institution" key="instit2">Allen Institute for AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science &amp; Engineering</orgName>
								<orgName type="department" key="dep2">Facebook AI Research</orgName>
								<orgName type="institution" key="instit1">University of Washington</orgName>
								<orgName type="institution" key="instit2">Allen Institute for AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science &amp; Engineering</orgName>
								<orgName type="department" key="dep2">Facebook AI Research</orgName>
								<orgName type="institution" key="instit1">University of Washington</orgName>
								<orgName type="institution" key="instit2">Allen Institute for AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">♣</forename></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science &amp; Engineering</orgName>
								<orgName type="department" key="dep2">Facebook AI Research</orgName>
								<orgName type="institution" key="instit1">University of Washington</orgName>
								<orgName type="institution" key="instit2">Allen Institute for AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">G</forename><surname>Allen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science &amp; Engineering</orgName>
								<orgName type="department" key="dep2">Facebook AI Research</orgName>
								<orgName type="institution" key="instit1">University of Washington</orgName>
								<orgName type="institution" key="instit2">Allen Institute for AI</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Shortformer: Better Language Modeling using Shorter Inputs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We explore the benefits of decreasing the input length of transformers. First, we show that initially training the model on short subsequences, before moving on to longer ones, both reduces overall training time and, surprisingly, gives a large improvement in perplexity. We then show how to improve the efficiency of recurrence methods in transformers, which let models condition on previously processed tokens (when generating sequences that are larger than the maximal length that the transformer can handle at once). Existing methods require computationally expensive relative position embeddings; we introduce a simple alternative of adding absolute position embeddings to queries and keys instead of to word embeddings, which efficiently produces superior results. By combining these techniques, we increase training speed by 65%, make generation nine times faster, and substantially improve perplexity on WikiText-103, without adding any parameters. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent progress in NLP has been driven by scaling up transformer <ref type="bibr" target="#b21">(Vaswani et al., 2017)</ref> language models <ref type="bibr" target="#b11">Lewis et al., 2019;</ref><ref type="bibr" target="#b16">Raffel et al., 2019;</ref><ref type="bibr">Brown et al., 2020)</ref>. In particular, recent work focuses on increasing the size of input subsequences, which determines the maximum number of tokens a model can attend to <ref type="bibr" target="#b0">(Baevski and Auli, 2018;</ref><ref type="bibr" target="#b19">Sukhbaatar et al., 2019;</ref><ref type="bibr" target="#b10">Kitaev et al., 2020;</ref><ref type="bibr" target="#b17">Roy et al., 2020)</ref>.</p><p>One issue with language models is the requirement of segmenting data into subsequences, both for training and inference, since memory constraints limit a language model to handling at most a few thousand tokens at once, while most training and evaluation datasets are much longer.</p><p>We challenge the assumption that longer input subsequences are always better by showing that existing transformers do not effectively use them. Further, we introduce new techniques based on shorter input subsequences that improve both efficiency and perplexity.</p><p>We first investigate how input subsequence length affects transformer language models ( §3). Naïve evaluation-where we split the large evaluation set into multiple nonoverlapping subsequences, each evaluated independently-initially supports the commonly-held belief that models that handle longer subsequences achieve better perplexity.</p><p>However, when we evaluate each model with a sliding window, generating one token at a time, we find that models that use subsequences of more than 1,024 tokens do not further improve performance.</p><p>We conclude that models that use longer subsequences perform better during naïve evaluation not because they are better at modeling, but because they divide the evaluation set into longer subsequences. This helps because of an issue we call the early token curse: by default, early tokens in a subsequence will have short histories to attend to. Using longer subsequenes means less tokens will suffer from the early token curse. For example, when using subsequences of length 1,024, about 94% of tokens get to attend to more than 64 preceding tokens. If we use subsequences of length 128, only 50% of tokens get to attend to 64 or more preceding tokens.</p><p>Based on this analysis, we explore how to improve models by shrinking their input subsequence length. We introduce two techniques.</p><p>Staged Training ( §4) First, we show that initially training on shorter subsequences (before moving to longer ones) leads not only to much faster training, but it surprisingly also leads to large perplexity improvements, suggesting that long subse-quences are harmful early in training.</p><p>Position-Infused Attention ( §5) Second, we consider a natural way to avoid the early token curse during training and inference: attend to cached representations from the previously evaluated subsequence <ref type="bibr" target="#b5">(Dai et al., 2019)</ref>. This approach interferes with conventional absolute position embeddings in a way that forced <ref type="bibr">Dai et al. to</ref> introduce relative position embeddings, which are computationally expensive. We introduce a fast, simple alternative: instead of adding absolute position embeddings to the word embeddings-thereby entangling a word's content and positional informationwe add them to the keys and queries in the selfattention mechanism (but not to the values). This does not increase parameter count or runtime. Token representations can then be cached and reused in subsequent computations. We also show that when using this method, shorter subsequence models perform better than longer ones.</p><p>We show that using position-infused attention and caching for token-by-token generation (as done when generating a string using a trained model such as <ref type="bibr">GPT-3 (Brown et al., 2020)</ref>) is nine times faster than using the baseline, even though our model achieves better perplexity, uses less memory, and does not require extra parameters.</p><p>Finally, we show additive gains from combining staged training and position-infused attention (Shortformer, §6), resulting in a faster transformer that achieves better perplexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Experimental Setup</head><p>Transformer language models map a list of tokens x n−L:n−1 to a probability distribution over the next token x n . We refer to the list of tokens as the current input subsequence (whose length is L). Causal masking lets us make L predictions at once, with the prediction for token i + 1 conditioned on the ith token and all previous inputs x n−L:i−1 , but not on future inputs. We define the number of tokens the model can attend to at each timestep as its effective context window.</p><p>Note that L is not to be confused with the (typically much greater) length of a training or evaluation dataset.</p><p>Language Model Inference During inference, language models can be used for two distinct tasks: generation and evaluation. In order to define these tasks, we first define nonoverlapping and sliding window inference.</p><p>Nonoverlapping Inference To evaluate a string whose length is longer than L, one approach is to evaluate each subsequence of L (or fewer) tokens independently. This approach is fast and is commonly used during training, but if it is used tokens in one subsequence cannot condition on those in the previous subsequence, giving rise to the early token curse discussed in §1. <ref type="figure" target="#fig_1">Figure 1(a)</ref> is a visualization of this approach.</p><p>Sliding Window Inference An alternative to the above approach is to use a sliding window during inference. Here, we choose a stride S between 1 and L − 1 and advance the window by S tokens after each subsequence has been evaluated. 2 This means that L − S tokens from the previous block will be re-encoded, and S new tokens will be generated. The advantage of this approach is that in each subsequence after the first, all new tokens have at least L − S previous tokens to condition on. Since tokens must be re-encoded multiple times, this approach is much slower. When S = L − 1, we generate one token every inference pass, which yields the most accurate estimate of the model's perplexity, but is also the slowest approach. Minimal and Maximal Effective Context Window Sizes Note that in the nonoverlapping approach, the minimal and maximal effective context window sizes are 0 and L, respectively. In the sliding window approach, the maximal context window size is still L, but the minimal context window size is now L − S.</p><p>Evaluation vs. Generation In generation, we use a model to generate a new sequence, as in demonstrations of GPT-3. In evaluation, we use a model to assign a perplexity score to an existing sequence. Generation can only be done with a sliding window with stride S = 1, which we refer to as token-by-token generation.</p><p>During generation, we input a single token, get a prediction from the model about the next token and use a strategy to pick the next token (such as beam search or picking the token with the highest probability); the process is then repeated. Evaluation can be done using either nonoverlapping inference or with a sliding window of any stride. This is because we already know all the tokens in the target sequence, and so we can simultaneously make predictions for multiple timesteps using causal masking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Setup</head><p>Our baseline is the model of <ref type="bibr" target="#b0">Baevski and Auli (2018)</ref>, henceforth B&amp;A, trained on WikiText-103 <ref type="bibr" target="#b12">(Merity et al., 2016)</ref>. The WikiText-103 training set contains about 103 million tokens from English Wikipedia. The B&amp;A model has 16 transformer layers of dimension 1,024, with 16 heads in each self-attention sublayer, and feedforward sublayers with an inner dimension of 4,096. This model ties the word embedding and softmax matrices <ref type="bibr" target="#b14">(Press and Wolf, 2017;</ref><ref type="bibr" target="#b7">Inan et al., 2017)</ref>. The baseline has a subsequence length of 3,072 tokens. In all of our experiments, other than varying the subsequence length, we modify no other hyperparameters, including the random seed and number of training epochs. This model achieves 18.65 ± 0.24 perplexity on the development set <ref type="bibr" target="#b13">(Press et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">How Does Context Window Size Affect Transformers?</head><p>Segmenting a corpus into subsequences will result in different effective context windows for different tokens, depending on where they fall within a segment. Subsequence length L is an upper bound on the effective context window at each timestep. When predicting the second token, the model will be able to attend only to the first token. When predicting the third token, the model will be able to attend to the first two tokens, and so on, up to the Lth timestep where the model can attend to all input tokens when predicting token L + 1. The baseline is the last row. We measure speed in tokens per second per GPU, and we use a batch size of 1 for inference. Token-by-token inference was computed with a sliding window stride of S = L − 1 (to generate one token at a time); see §2. <ref type="table" target="#tab_0">Table 1</ref> explores the effect of subsequence length in the B&amp;A model on training runtime, and development-set perplexity and runtime. <ref type="bibr">3</ref> We fix the number of tokens in each batch to 9,216, but vary the subsequence length L and batch size (so the product of the batch size and subsequence length remains at 9,216). We report results for both nonoverlapping inference and inference with a sliding window with stride S = L − 1, which generates only one new token per forward pass; it thus has the maximal effective context window for each generated token. We empirically find that performance increases as S increases (not shown in <ref type="table" target="#tab_0">Table 1</ref>), although it is not a strictly increasing function of S. <ref type="bibr">4</ref> These experiments lead us to the following conclusions:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Context Window Size Matters</head><p>Training on long sequences is expensive. The computational complexity of self-attention grows quadratically with subsequence length. We observe that models trained on subsequences of length 256 are twice as fast as models trained on subsequences of 3,072 tokens, but gains for even shorter subsequence lengths are negligible.</p><p>Long subsequence lengths can improve results. When using the naïve evaluation approach, nonoverlapping evaluation, we see a monotonic decrease in validation perplexity when increasing L.</p><p>Increasing the minimum effective context window size is more important than increasing the maximum one. Using a sliding window for token-by-token inference offers a truer test of a model's generalization ability, and significantly improves results for all models. Here, we see negligible improvement between the models trained with subsequence lengths of 1,024 and 3,072 tokens (0.05 perplexity). This approach improves results by increasing the minimum amount of context available to each token. This indicates that long contexts may not be beneficial to transformer language models, but very short contexts are harmful. However, using sliding windows at inference time can be very expensive since each token is encoded many times. For example, token-by-token inference for the L = 3,072 model is almost 300 times slower than nonoverlapping inference.</p><p>Based on these findings, we introduce methods for training and inference with short subsequence length transformers, and show that they improve both efficiency and perplexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Training Subsequence Length</head><p>Results in Section 3 demonstrate that models that use shorter subsequences can be effective at test time, and are much faster to train. We now further explore the effect of shortening subsequence lengths during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Staged Training</head><p>We propose a two-stage training routine that initially uses short input subsequences followed by long subsequences. This method was previously applied to speed up the training of BERT <ref type="bibr" target="#b6">(Devlin et al., 2019)</ref>, but here we thoroughly study it and reveal that it also improves perplexity.</p><p>Importantly, we use sinusoidal position embeddings (which are not learned); learned position embeddings, which we do not consider, would create a dependency between the parameterization and the subsequence length. In our experiments, we neither modify nor reset the state of the optimization algorithm between the two stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiments</head><p>We use the experimental setup described in §2. We do not change any hyperparameters. The only modification we apply is to reduce subsequence length while correspondingly increasing batch size to keep the number of tokens per batch constant. As in the baseline, all models are trained for 205 epochs.</p><p>In all experiments we train our model for two stages; in the second stage we always use a subsequence length of 3,072 tokens. <ref type="table" target="#tab_1">Table 2</ref> shows the time each training routine takes to match the baseline model's performance on the validation set of WikiText-103. Many configurations match this performance in less than half the time it takes to train the baseline itself; some models reach baseline performance in only 37% of the time needed to train the baseline.</p><p>Although all models take less time to train than the baseline, <ref type="table" target="#tab_2">Table 3</ref> shows that many outperform it. For example, the best model-trained with subsequence length L = 128 until epoch 50outperforms the baseline by 1.1 perplexity despite completing training in 87% of the time it takes the baseline model to do so. The model that trains with L = 128 until epoch 100 achieves similarly strong results (17.62 perplexity) and finishes training in 74% of the time it takes the baseline to train.</p><p>Both results are very robust to the choice of initial stage subsequence length and number of epochs. <ref type="table" target="#tab_2">Table 3</ref> shows that all models with an initial stage of L = 1,024 tokens or less that switch to the second stage at epoch 125 or before beat the baseline by a large amount at the end of training. Additionally, <ref type="table" target="#tab_1">Table 2</ref> shows that those models match the baseline's perplexity in at most 71% of the time it takes to train the baseline.   We additionally explored using more than two stages (up to six), but this did not outperform our simple two-stage curriculum. We also found that setting L to less than 3,072 tokens in the second stage degraded performance (when using nonoverlapping evaluation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Repositioning Position Embeddings</head><p>As noted in §3, using sliding windows at inference time substantially improves performance by increasing the minimum effective context window size.</p><p>But sliding window evaluation is very slow. One way to solve this would be to let the tokens in the subsequence currently being generated attend to those in the previously generated subsequence.</p><p>In this case, the same token representation would be used in different positions since a token generated near the end of one subsequence would be cached and reused near the start of the next subsequence. However, transformer model representations entangle positional and content information, so a cached token representation would encode an incorrect position when re-used in a new position.</p><p>TransformerXL <ref type="bibr" target="#b5">(Dai et al., 2019)</ref> uses relative position embeddings to solve this problem. However, that approach is slower and uses more parameters and memory than the baseline transformer. <ref type="bibr">5</ref> Our solution solves this problem without using extra parameters, memory, or runtime. We also find that when using our method, we can use much shorter input subsequences and still achieve superior performance.</p><p>Transformer Language Models The baseline transformer LM, given a token list T of length L and a tensor P containing the first L position embeddings, produces L next-token predictions using the following procedure:</p><p>1. We embed each word in T . This produces tensor X.</p><p>2. We add the position embedding of each index to the word at that index: X = X + P.</p><p>3. We feed X through each transformer layer. The self-attention sublayer in each transformer layer is invoked as follows:</p><p>self-attention(key=X, query=X, value=X) <ref type="bibr">5</ref> To produce the self-attention coefficients between q queries and k keys TransformerXL computes two dot products of size q · k; the unmodified attention sublayer and the PIA method that we introduce both compute only one dot product of size q · k. We also benchmarked the TransformerXL model using its publicly released code and found that their relative position embeddings slow inference by 22% and require 26% more parameters than their implementation of the unmodified self-attention sublayer.  <ref type="figure">Figure 2</ref>: A visualization of the inputs to the self-attention sublayer, conventionally (left) and with our approach (right), for L = 3. The subsequence the cat sat is used to predict on; next, cat sat on is used to predict the. 4. The outputs of the last transformer layer are transformed using the softmax layer, giving L next-token probability distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Position-Infused Attention (PIA)</head><p>We propose to let the transformer model reuse previous outputs by making each output contain no positional information. To do this, we modify the model so that it does not add position embeddings at the beginning of the computation (step 2), but rather adds position embeddings to the query and key vectors at each layer (but not to the value vectors). The outputs at each layer are the transformed, weighted sums of the value vectors, and, since the value vectors in our model do not contain positional information, the outputs also do not. Formally, steps 1 and 4 do not change, step 2 is omitted, and step 3 is modified to invoke the self-attention sublayer as follows:</p><p>self-attention(key=X+P, query=X+P, value=X) <ref type="figure">Figure 2</ref> (right) provides a visualization of the PIA method.</p><p>Although the outputs of the PIA sublayer contain no positioning information, the attention mechanism can still compute position-dependent outputs because positional information is added to the query and key vectors. Our method can be implemented in just a few lines of code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">PIA Enables Caching</head><p>In the unmodified transformer, to generate a string whose length is longer than L, we would have to split it into separate subsequences, and we would not be able to attend to the previous subsequence when generating the current one.</p><p>Using PIA, we are able to store and attend to the outputs of the previous subsequence, as these vectors no longer contain any positioning information.</p><p>Therefore, all our PIA models use a cache, where the representations from the previous forward pass are stored, and attended to in the subsequent forward pass.</p><p>Caching makes generation faster. The complexity of the attention mechanism is O(q·k) where q is the number of queries (outputs) we have and k is the number of key-value pairs we attend to (inputs). When generating a sequence whose length is greater than L, using token-by-token generation in the unmodified transformer (with subsequence length L), the attention sublayer takes O(L 2 ) time (since we have L queries and L keys). Using PIA and caching, we can reuse L − 1 of the previous outputs at every layer, and so our attention sublayer takes O(L) time (since we now have just one query and L keys).</p><p>Our approach is useful in scenarios where we need to evaluate or generate sequences that are longer than the model's subsequence length. Therefore, it would not be applicable to conventional sentence-level neural machine translation, where sequence lengths are short.</p><p>Most language models, including that of <ref type="bibr" target="#b0">Baevski and Auli (2018)</ref>, train on their data as nonoverlapping subsequences. This means that the training subsequences can be shuffled at each epoch and consumed in random order. However, when using PIA, we would like to cache the previously processed subsequence and so do not shuffle the data, so that the cached subsequence is the previously occurring subsequence.</p><p>See <ref type="figure" target="#fig_1">Figure 1(c)</ref> for a visualization of training with a cache that contains previously outputted representations of the previous subsequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experiments</head><p>We use the experimental setup described in §2.</p><p>The B&amp;A baseline achieves 18.65 on the development set. If we apply our PIA method to the B&amp;A model (without caching), performance degrades to 19.35 perplexity, but the model's speed and memory usage do not change. Disabling data shuffling in the PIA model-a necessary change for recurrent-like training that caches previously computed subsequence representations-achieves similar performance, at 19.44 perplexity.</p><p>Our next set of experiments, use the recurrentstyle training of <ref type="bibr" target="#b5">Dai et al. (2019)</ref>, where we receive L new tokens at every training iteration and attend to L previously computed, cached, representations (of the subsequence of tokens that came immediately prior to the L new tokens ( §5)). As before, we generate L tokens at every training iteration. This means that the maximal and minimal effective context window sizes for each model are L + L and L , respectively.</p><p>In all of the experiments we present with PIA and caching, we set L = L because a manual exploration of different models where L = L did not yield better results. <ref type="table" target="#tab_4">Table 4</ref> compares the results of our models that use PIA and caching to the baseline, on the WikiText-103 development set. The evaluation and generation speeds are shown in the nonoverlapping (N.o.) and sliding window (S.W., with stride S = 1) speed columns, respectively. 6 Unlike in the baseline model, token by token generation in our model achieves the same perplexity as nonoverlapping evaluation, since even in nonoverlapping evaluation our minimal context windows are large. This is because when using subsequence length L, the model receives L new tokens and attends to L tokens from the previously computed subsequence. This also means that our model, with subsequence length 512, has attention matrices of size 512 · 1,024 (since we have 512 queries-one per every new token-and 1,024 keys and 1,024 values-one per every new token and every cached 6 Note that <ref type="bibr" target="#b0">Baevski and Auli (2018)</ref> show that the baseline model can also achieve 17.92 during evaluation, when S = 2,560, with a speed of 2.5k tokens per second.  token). In the baseline, all attention matrices are of size 3,072 · 3,072. <ref type="table" target="#tab_4">Table 4</ref> shows that as we increase subsequence length, perplexity improves, peaking at 512 before starting to degrade. Our best model achieves a perplexity of 17.85, which is multiple standard deviations better than baseline performance (18.65). It runs just 1% slower than the baseline during inference (because it caches previous representations, while the baseline does not) but uses much less memory since its attention matrices are much smaller, as mentioned above. Our best model trains 55% faster than the baseline.</p><p>Token-by-Token Generation A benefit of caching previously computed representations is that we can use that cache to do token-by-token generation efficiently. Our model does this at a speed of 46 tokens per second. This is nine times faster than the baseline transformer with L = 3,072 generating token-by-token even though our model achieves better perplexity and uses much less memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Shortformer Results</head><p>To check whether the gains from both Staged Training ( §4) and PIA ( §5) are additive, we take our best PIA model, which has a subsequence length of 512, and apply Staged Training to it, initially training it with a subsequence length of between 32 to 256.  Training was applied to the unmodified baseline, the results are very robust to the choice of initial stage subsequence length, with all of the different choices improving perplexity over the model that does not use Staged Training.</p><p>The best model (the one with initial subsequence length 128), which we call Shortformer, achieves 17.47 perplexity and trains 65% faster than the baseline model. Since its attention matrices are of dimension 512 · 1,024 (whereas the baseline uses matrices of dimension 3,072 · 3,072), our model uses much less memory. The number of parameters in our model is identical to that of the baseline model. <ref type="figure" target="#fig_4">Figure 3</ref>, in the appendix, compares our best models using each of the methods we presented (and their combination) to the baseline. It shows that combining PIA and Staged Training (Shortformer) is the both the fastest to train and also has the best perplexity, when using nonoverlapping evaluation. Evaluation speed is similar for all of these models.</p><p>Finally, in <ref type="table" target="#tab_8">Table 6</ref> we evaluate our best models on the test set of WikiText-103. Shortformer is almost twice as fast to train as the baseline, and achieves superior results. Similarly to the best model from §5.3, Shortformer, is nine times faster than the baseline for token-by-token generation.</p><p>Since it uses caching and PIA, sliding window evaluation cannot be applied to Shortformer. By training only the baseline with Staged Training (and no PIA or caching), we obtain a model (our best model from Section 4.2) that, when sliding window evaluation is used, obtains even better results, but that model is much slower than Shortformer (second to last row, <ref type="table" target="#tab_8">Table 6</ref>).</p><p>Our Shortformer model outperforms the baseline perplexity and performs within the standard  deviation of the Sandwich transformer <ref type="bibr" target="#b13">(Press et al., 2020)</ref> and TransformerXL. It does not outperform the result of the kNN-LM, which makes orthogonal improvements that can be applied to any existing language model, at the price of slower decoding.</p><p>Combining it with our approach may give further gains.</p><p>7 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Position-Infused Attention</head><p>TransformerXL <ref type="bibr" target="#b5">(Dai et al., 2019)</ref> caches and attends to previous representations using an attention sublayer that uses relative positioning <ref type="bibr" target="#b18">(Shaw et al., 2018)</ref>. It runs much slower than the unmodified attention sublayer, requires extra parameters and requires internally modifying the self-attention sublayer, while our PIA method does not.</p><p>In parallel with our work, <ref type="bibr" target="#b8">Ke et al. (2020)</ref> compute the attention coefficients by summing two independent attention matrices, one based on position-position interactions and the other only on content-content interactions. This method requires modifying the self-attention sublayer, while our PIA method does not. As a result, our attention coefficients are based on interactions between representations that contain both positioning and content information (just as in the unmodified attention sublayer). <ref type="bibr" target="#b8">Ke et al. (2020)</ref> present results only for BERT, which uses smaller subsequences than those used in our language models. <ref type="bibr" target="#b2">Bengio et al. (2009)</ref> introduced curriculum learning, which improves models' performance by ini-tially training on easier inputs before progressing to harder ones. Our staged training approach ( §4) does not change the order in which the training examples are given to the network, but instead modifies the subsequence length. Initially we look at many short subsequences and then, during the second stage of training, we look at fewer, but longer subsequences. <ref type="bibr" target="#b6">Devlin et al. (2019)</ref> previously used a staged training method by training the BERT model for the first 90% of training on shorter subsequences (of length 128) before moving on to longer subsequences (of length 512) for the last 10% of training. They use this method to speed up training, but we show that Staged Training also improves perplexity, and we conduct a thorough evaluation of different configurations for this approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Staged Training</head><p>Many recent papers have explored how to improve the efficiency of transformers by reducing the quadratic cost of self-attention, motivated by scaling to longer sequences <ref type="bibr" target="#b10">(Kitaev et al., 2020;</ref><ref type="bibr" target="#b17">Roy et al., 2020;</ref><ref type="bibr" target="#b20">Tay et al., 2020)</ref>. We show that longer sequences can be harmful and instead demonstrate improved results with shorter sequences, which naturally also improves efficiency.</p><p>One way to reduce the memory consumption of a transformer model is to sparsify the attention matrix by letting each token attend only to a subset of the nearby tokens <ref type="bibr" target="#b1">Beltagy et al., 2020)</ref>. Our approach, training on shorter subsequence lengths, is much more efficient since we do not sparsify an attention matrix: we use multiple, but much smaller, attention matrices, which are dense. Since attention uses memory and computation in a way that scales quadratically with input size, by splitting the inputs into multiple subsequences that are each processed independently, we use much less memory and run faster. The model of <ref type="bibr" target="#b1">Beltagy et al. (2020)</ref> uses different, sparse attention patterns at every layer, with the number of neighboring tokens that each tokens attends to growing in each subsequent layers. In our method, we let each layer attend to the same number of tokens at every stage. Similar to our method, <ref type="bibr" target="#b1">Beltagy et al. (2020)</ref> let each token attend a growing number of neighbors as training progresses, but they use five stages, which we found not to be superior to the two stages in our method.</p><p>The adaptive attention span model of <ref type="bibr" target="#b19">Sukhbaatar et al. (2019)</ref> learns the maximum effective context window sizes for each head at each layer inde-pendently. Like our method, the context window sizes are smaller at the beginning of training and get longer as training progresses. We show that a simple approach of manually choosing two subsequence lengths is highly effective. In addition, keeping subsequence lengths equal across all heads and layers lets us save memory and runtime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>Our results challenge the conventional wisdom that scaling transformer language models to longer sequences improves results. We show that by initially training on shorter subsequences and then progressing to longer ones via staged training, we can improve perplexity and reduce training time. We additionally define a new method, position-infused attention, that enables caching and efficiently attending to previously computed representations; we show that models using this method do not require large input subsequences. We finally demonstrate that these two methods can be combined to produce a speedier and more accurate language model.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 1(b) is a visualization of this.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Dark red is the current subsequence, light red is the previous one. (a) During nonoverlapping training and inference, subsequences are processed independently. (b) Sliding window inference with stride S = 1. (c) Caching, discussed in §5.2, where each subsequence attends to representations of the previous one. (In the iteration after this one, tokens 4, 5 and 6 become the cache, and are given position embeddings 1, 2 and 3, while the three new tokens get position embeddings 4, 5, and 6.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Validation performance vs. training speed for the baseline and our best staged training model, our best PIA and caching model, and our best combined model (Shortformer). All of the models are evaluated using nonoverlapping evaluation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell>Train</cell><cell></cell><cell cols="2">Inference</cell><cell></cell></row><row><cell>Subseq.</cell><cell></cell><cell cols="2">Nonoverlapping</cell><cell cols="2">Sliding Window (Token-by-token)</cell></row><row><cell>Length</cell><cell cols="5">Speed ↑ PPL ↓ Speed ↑ PPL ↓ Speed ↑</cell></row><row><cell>32</cell><cell>28.3k</cell><cell>35.37</cell><cell>2.4k</cell><cell>24.98</cell><cell>74</cell></row><row><cell>64</cell><cell>28.5k</cell><cell>28.03</cell><cell>4.8k</cell><cell>21.47</cell><cell>69</cell></row><row><cell>128</cell><cell>28.9k</cell><cell>23.81</cell><cell>9.2k</cell><cell>19.76</cell><cell>70</cell></row><row><cell>256</cell><cell>28.1k</cell><cell>21.45</cell><cell>14.8k</cell><cell>18.86</cell><cell>63</cell></row><row><cell>512</cell><cell>26.1k</cell><cell>20.10</cell><cell>18.1k</cell><cell>18.41</cell><cell>37</cell></row><row><cell>1024</cell><cell>22.9k</cell><cell>19.11</cell><cell>18.3k</cell><cell>17.97</cell><cell>18</cell></row><row><cell>1536</cell><cell>18.4k</cell><cell>19.05</cell><cell>17.1k</cell><cell>18.14</cell><cell>11</cell></row><row><cell>3072</cell><cell>13.9k</cell><cell>18.65</cell><cell>14.7k</cell><cell>17.92</cell><cell>5</cell></row></table><note>: How subsequence length affects performance of the B&amp;A model on the WikiText-103 development set.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Time until we match the baseline's performance (on the dev. set, with nonoverlapping eval.) as a fraction of the time it takes to train the baseline (smaller is better). Some models never match the baseline, and so those cells are empty. All models have a subsequence length of 3,072 tokens at the end of training.17.57 17.58 18.19 18.06 18.20 18.77 50 17.81 17.59 17.52 18.08 18.01 18.14 18.62 75 17.93 17.61 17.55 18.01 18.05 18.03 18.57 Initial Stage Epochs 100 18.14 17.67 17.62 18.00 18.10 18.00 18.51 125 18.61 17.88 17.70 18.00 18.13 17.98 18.49 150 19.45 18.37 17.98 18.01 18.15 18.00 18.49 175 21.16 19.51 18.57 18.23 18.20 18.08 18.57 200 35.38 28.03 23.80 21.45 19.63 18.56 18.84</figDesc><table><row><cell></cell><cell cols="2">Initial Stage Subseqence Length</cell></row><row><cell>32</cell><cell>64</cell><cell>128 256 512 1024 1536</cell></row><row><cell>25 17.94</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>The perplexity of each model at the end of training (on the dev. set, with nonoverlapping eval.). All models have a subsequence length of 3,072 tokens at the end of training. The B&amp;A baseline achieves 18.65 ± 0.24 perplexity.When we use nonoverlapping evaluation, the B&amp;A baseline obtains 18.65 perplexity on the development set; our best model obtains 17.52 perplexity. When we use sliding window evaluation (following Baevski &amp; Auli, we use stride S = 2560), our best model obtains 16.89 perplexity, a large improvement on the 17.92 B&amp;A result in that setting. On the test set, using the same sliding window evaluation, our model obtains 17.56 perplexity, a substantial gain over the B&amp;A baseline's 18.70 test-set perplexity.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>: Validation perplexity for PIA models trained</cell></row><row><cell>with different subsequence lengths (L). Each PIA</cell></row><row><cell>model attends to L new and L cached tokens in every</cell></row><row><cell>feedforward pass. N.o. is Nonoverlapping and S.W. is</cell></row><row><cell>Sliding Window, where we always use S = 1 (token-</cell></row><row><cell>by-token generation) in this table. We measure speed</cell></row><row><cell>in tokens per second per GPU, and we use a batch size</cell></row><row><cell>of 1 for inference.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5</head><label>5</label><figDesc>shows the results. As in §4.2, where Staged</figDesc><table><row><cell>First Stage</cell><cell>Train</cell><cell>Inference</cell></row><row><cell>Subseq. Length</cell><cell>Speed ↑</cell><cell>PPL ↓</cell></row><row><cell>32</cell><cell>21.6k</cell><cell>17.66</cell></row><row><cell>64</cell><cell>22.6k</cell><cell>17.56</cell></row><row><cell>128</cell><cell>22.9k</cell><cell>17.47</cell></row><row><cell>256</cell><cell>22.5k</cell><cell>17.50</cell></row><row><cell>PIA without Staged Training</cell><cell>21.5k</cell><cell>17.85</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Dev. set results for models that use PIA, caching, and Staged Training (with final subseq. length of 512). Speed is measured in tok./sec. per GPU. Evaluation speed is the same for all models, at 14.5k tok./sec.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell>: Comparison of our best models to other</cell></row><row><cell>strong LMs evaluating the WikiText-103 test set. Trans-</cell></row><row><cell>formerXL has 257M parameters, all other models have</cell></row><row><cell>247M. In this table, S = 2,560.  *  TransformerXL runs</cell></row><row><cell>on an older version of PyTorch than all other models,</cell></row><row><cell>which might impact speed.  *  *  kNN-LM (Khandelwal</cell></row><row><cell>et al., 2020) requires a 400GB datastore.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Total time it takes to train each model, as a fraction of the time it takes to train the baseline.</figDesc><table><row><cell></cell><cell></cell><cell>Initial Stage Subsequence Length</cell></row><row><cell></cell><cell></cell><cell>32 64 128 256 512 1024 1536</cell></row><row><cell></cell><cell cols="2">25 136 123 122 146 144 155</cell></row><row><cell>Initial Stage Epochs</cell><cell cols="2">50 135 124 122 136 144 149 179 75 143 128 125 136 144 145 181 100 158 135 130 136 145 142 175 125 190 149 141 140 146 144 174 150 176 160 154 153 151 174 175 191 178 177 176 189</cell></row><row><cell></cell><cell>200</cell><cell>202</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Epoch at which each model matches the baseline. Some models never match the baseline, and so those cells are empty.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Our code is available at https://github.com/ ofirpress/shortformer</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Nonoverlapping inference can be viewed as sliding window inference with stride L.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">For consistency across models, throughout the paper we always run inference with a batch size of one. This leads models shorter than L = 512 to run slowly, although during batched evaluation they are slightly faster than the 512 model.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">For example, the L = 3,072 model's performance peaked at S = 2,560 (used in<ref type="bibr" target="#b0">Baevski and Auli (2018)</ref>) and then stopped improving. Thus, the result shown inTable 1for that model with S = 3,071 can also be achieved with S = 2,560 even though that runs 500 times faster, at 2.5k tok./sec.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Tim Dettmers, Jungo Kasai, Gabriel Ilharco, Hao Peng, Sewon Min, Mandar Joshi, Omer Levy, Luke Zettlemoyer, Julian Michael, Myle Ott, and Sam Shleifer for their valuable feedback and fruitful discussions.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>Initial Stage Subsequence Length</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Adaptive input representations for neural language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno>abs/1809.10853</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Longformer: The long-document transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jérôme</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="DOI">10.1145/1553374.1553380</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning, ICML &apos;09</title>
		<meeting>the 26th Annual International Conference on Machine Learning, ICML &apos;09<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
	<note>Association for Computing Machinery</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
		<imprint>
			<pubPlace>Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam Mc-Candlish, Alec Radford, Ilya Sutskever</pubPlace>
		</imprint>
	</monogr>
	<note>and Dario Amodei. 2020. Language models are few-shot learners</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://openai.com/blog/sparse-transformers" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Transformer-xl: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Jaime</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2978" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Tying word vectors and word classifiers: A loss framework for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khashayar</forename><surname>Hakan Inan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Khosravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guolin</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Rethinking positional encoding in language pre-training</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generalization through Memorization: Nearest Neighbor Language Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Urvashi</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal ; Abdelrahman Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ves</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.13461</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Pointer sentinel mixture models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improving transformer models by reordering their sublayers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofir</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.270</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2996" to="3005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Using the output embedding to improve language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofir</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="157" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Efficient content-based sparse attention with routing transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurko</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Saffar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Self-attention with relative position representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-2074</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="464" to="468" />
		</imprint>
	</monogr>
	<note>Short Papers. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Adaptive attention span in transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.07799</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Efficient transformers: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<idno>100 0.75 0.75 0.74 0.75 0.77 0.80 0.87 125 0.68 0.68 0.68 0.69 0.71 0.75 0.84 150 0.62 0.62 0.61 0.62 0.65 0.70 0.81 175 0.56 0.56 0.55 0.56 0.59 0.66 0.78 200 0.49 0.49 0.48 0.50 0.53 0.61 0.75</idno>
		<title level="m">Initial Stage Epochs</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
