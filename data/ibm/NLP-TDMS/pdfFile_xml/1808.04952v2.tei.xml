<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PFCNN: Convolutional Neural Networks on 3D Surfaces Using Parallel Frames</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqi</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilin</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Pan</surname></persName>
							<email>haopan@microsoft.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
							<email>yangliu@microsoft.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tong</surname></persName>
							<email>xtong@microsoft.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">PFCNN: Convolutional Neural Networks on 3D Surfaces Using Parallel Frames</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Surface meshes are widely used shape representations and capture finer geometry data than point clouds or volumetric grids, but are challenging to apply CNNs directly due to their non-Euclidean structure. We use parallel frames on surface to define PFCNNs that enable effective feature learning on surface meshes by mimicking standard convolutions faithfully. In particular, the convolution of PFCNN not only maps local surface patches onto flat tangent planes, but also aligns the tangent planes such that they locally form a flat Euclidean structure, thus enabling recovery of standard convolutions. The alignment is achieved by the tool of locally flat connections borrowed from discrete differential geometry, which can be efficiently encoded and computed by parallel frame fields. In addition, the lack of canonical axis on surface is handled by sampling with the frame directions. Experiments show that for tasks including classification, segmentation and registration on deformable geometric domains, as well as semantic scene segmentation on rigid domains, PFCNNs achieve robust and superior performances without using sophisticated input features than state-of-the-art surface based CNNs. * Joint first author. Work done during internship at Microsoft. † Corresponding author.</p><p>tain pointwise N -direction frames <ref type="figure">(Fig. 1)</ref> to define a novel PFCNN framework whose convolution mimics standard image convolutions more faithfully.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Applying CNNs to 3D geometric domains is critical for deep learning beyond the 2D images. Unlike regular 2D images, 3D geometric data can be represented in different forms, posing challenges to standard CNNs. For example, volumetric grids regularly sample R 3 on which CNNs can be trivially deployed, but they are memory consuming and inflexible for capturing fine geometric details. For representation efficiency, 3D objects and scenes are frequently encoded by their boundary surfaces discretized as triangle meshes. However, the curved and irregularly sampled meshes do not admit the standard CNNs designed for flat image domains with regular pixel grids. While several surface based CNNs have been proposed to tackle this problem, in this paper we use parallel frame fields that con-Similar to standard CNNs, the PFCNN convolution works on a local surface patch each time and maps it onto the flat tangent space where the convolution kernel is parameterized, as done by many previous surface based CNNs <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b31">32]</ref>. Different from the previous approaches, however, we also align the tangent spaces of different surface points such that they locally form a flat Euclidean structure, on which the surface-based feature maps and convolution kernels can be moved as in the standard image domain. For images, such translation operations are formally captured by the translation equivariance property of convolution <ref type="bibr" target="#b3">[4]</ref>, which is a key factor contributing to the effectiveness of CNNs by enabling shared trainable weights and thus significantly reducing the amount of network parameters to avoid overfitting and achieve generalization <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b12">13]</ref>; our surface based convolution is shown to reproduce the imagedomain translation equivariance locally.</p><p>We adopt the tool of locally flat connections from discrete differential geometry <ref type="bibr" target="#b44">[45]</ref> to align the tangent spaces. The locally flat connection is encoded by the field of pointwise tangential N -direction frames ( <ref type="figure">Fig. 1</ref>) that is efficiently computed to be parallel and aligned to salient geometric features to better capture semantics. In addition, because there exists no canonical axis on a surface, we sample the axes using the same N frame directions and organize the resulting feature maps with an N -cover space of the domain surface <ref type="bibr" target="#b8">[9]</ref>; on each sheet of the cover space, the canonical axis is selected and the convolution is readily defined. Furthermore, to handle the irregular mesh vertices, for each patch we resample with a regular grid and apply standard shaped convolution kernels on it.</p><p>The PFCNNs resemble standard CNNs so that efficient network structures can be leveraged accordingly. Through experiments of deformable shape classification, segmentation and matching as well as rigid scene segmentation, we show that PFCNNs using only raw input signals achieve superior performances than competing surface CNN frameworks. In addition, we do extensive ablation studies to validate the components of our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>We briefly review 3D neural networks by classifying them according to the forms of domain representation, and focus on the most related works that use surface meshes.</p><p>3D neural networks for volumetric grids, point clouds and multi-view representations. The earliest works for 3D deep learning directly extend CNNs to 3D volumetric grids <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b28">29]</ref>, which are later improved for computational efficiency by using adaptive grids like octrees that use high resolution only around the boundary surfaces <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b45">46]</ref>. Point sets also conveniently encode 3D shapes, for which the set-based PointNet <ref type="bibr" target="#b32">[33]</ref> is proposed and later extended by PointNet++ <ref type="bibr" target="#b34">[35]</ref> to take advantage of the local surface patch structure. Similarly, more works utilize the local patch structures of 3D point clouds, by e.g. tangent plane projection <ref type="bibr" target="#b41">[42]</ref>, localization with lattice structure <ref type="bibr" target="#b39">[40]</ref>, or localized kernel functions <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b42">43]</ref>. Multi-view representations encode 3D data with a set of 2D images <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b33">34]</ref>, on which standard CNNs are applied to extract intermediate features and aggregated for final output. The PFCNNs presented in this paper work on surface meshes, which have been used pervasively for 3D representation due to their high efficiency for capturing geometry to the fine details.</p><p>Patch-based surface CNNs. A series of works extend standard CNNs to curved surface domains by applying convolution operations on localized geodesic patches; they differ mainly in the specific ways of convolution computation. Masci et al. <ref type="bibr" target="#b27">[28]</ref> parameterize each geodesic patch in polar coordinates, upon which the convolution operation is computed by rotating the kernel function for a set of discrete angles and convolving it with input features; the convolved features for different angles are further pooled for output. With such an approach, it is hard to capture anisotropic or directional signals. Later Boscaini et al. <ref type="bibr" target="#b2">[3]</ref> propose anisotropic CNNs that extend <ref type="bibr" target="#b27">[28]</ref> by aligning the convolution kernels to frames of principal curvature directions, thus removing angular pooling and ambiguity, and show improved performances on various tasks. Xu et al. <ref type="bibr" target="#b47">[48]</ref> use a similar convolution on n-ring neighboring faces with fixed cardinality for shape segmentation. MoNet <ref type="bibr" target="#b29">[30]</ref> extends the geodesic convolutions by modeling the convolution kernel as a mixture of Gaussians whose bases and coefficients are fully trainable rather than functions of fixed parameterizations. TextureNet <ref type="bibr" target="#b15">[16]</ref> imposes locally rectangular grids define by 4-directional fields on the geodesic patches, and extract the features for center or corner grid points separately to handle the grid orientation ambiguity. Multi-directional CNNs <ref type="bibr" target="#b31">[32]</ref> make the further step of resolving the orientation alignment of geodesic patches by using parallel transport to match the directional convolution responses for different surface points, which enables effective propagation of directional signals. Different from these <ref type="figure">Figure 1</ref>. For patch-based surface CNNs, the key problem is how to align the tangent spaces of different surface points. (a) the parallel transport is path-dependent and maps the vector in TxM directly to the blue one in TzM but to the red dashed one by going through TyM. (b) by building a flat connection encoded by the parallel 4-direction frame field, our approach has pathindependent translation as in image domain. manifold based works, SplineCNN <ref type="bibr" target="#b10">[11]</ref> defines 3D spline convolution kernels for extracting features on surface and is inherently a volumetric approach focusing on handling the irregular sampling of meshes.</p><formula xml:id="formula_0">T x M T z M T y M (a) (b)</formula><p>Our PFCNN follows the geodesic convolution paradigm, but differs from others in the convolution computation. Indeed, our framework closely relates to the latest parallel transport approach of <ref type="bibr" target="#b31">[32]</ref>, but we align the tangent spaces with locally flat connections that not only approximate the parallel transport but also induce a locally Euclidean structure suitable for defining convolutions as for images. In addition, the locally flat connections can be adapted to capture salient geometric features like sharp creases, which further improves performance. As a result, our PFCNNs show superior performances than the previous patch-based surface CNNs on diverse tasks (Sec. 6).</p><p>Surface CNNs using atlas maps. Another series of works deal with a surface domain by mapping it to a 2D atlas image, on which standard convolutions are applied. Sinha et al. <ref type="bibr" target="#b38">[39]</ref> use the geometry image to map a 3D surface of sphere topology to the planar domain and feed the map to CNNs for shape recognition. Maron et al. <ref type="bibr" target="#b26">[27]</ref> note the geometry images have gaps between charts of the atlas map and propose to parameterize a surface of sphere topology conformally to the flat image with a toric topology, where standard convolutions with cross boundary cyclic padding are applied. Such convolutions are shown to be conformally translation equivariant but the conformal scaling distortion is uneven for different surface regions. Li et al. <ref type="bibr" target="#b24">[25]</ref> handle the gaps of an atlas map by modulating the convolution to jump across the gaps, while the mapping distortion is loosely constrained by subdividing the charts. In comparison, our framework works with surfaces of general topology and automatically preserves the original signals with minimal distortion due to the local patch paradigm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Overview</head><p>To represent the boundary of a 3D object, we consider a surface mesh M = (V, F ), with V = {v i } the vertices with embedding v i ∈ R 3 , and F = {f i = (v i0 , v i1 , v i2 )} the faces with corners indexing the vertices. Denote the unit normal vector at vertex v i as n vi ∈ R 3 , and tangent plane as T vi M on which we can project the local geodesic patch and apply standard image-like convolutions. As reviewed in Sec. 2, while most patch-based surface CNNs follow this general approach, the key challenge is how to coordinate the convolutions for tangent planes of different vertices ( <ref type="figure">Fig. 1</ref>). We resolve this challenge by building locally flat connections that align the tangent planes into locally flat Euclidean domains, thus enabling effective weight sharing and translation equivariance that mimic behavior on 2D images.</p><p>In Sec. 4 we briefly review the standard Euclidean convolutions with their translation equivariance property, the notion of connections from differential geometry, locally flat connections encoded by N -direction frame fields and Ncover spaces for organizing convolution and feature maps. In Sec. 5 we present the extended convolution on surfaces using parallel frames that achieves local translation equivariance and handles irregular vertex sampling on meshes, and the new layers that constitute a PFCNN model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Convolution on Euclidean domains</head><p>The convolution operation of a CNN exploits the translation equivariance of 2D images <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b3">4]</ref>. Let f, k : Ω ⊂ R 2 → R be two functions defined on the image Ω, and k is the convolution kernel usually with a local spatial support. Define the convolution operator as f k(x) = y∈Ω k(y − x)f (y)dy. A planar translation of the imagebased function by a vector</p><formula xml:id="formula_1">v ∈ R 2 is τ v (f (x)) = f (x − v).</formula><p>Translation equivariance simply means that the planar translation commutes with convolution, i.e.</p><formula xml:id="formula_2">τ v (f k) = τ v (f ) k.</formula><p>(1)</p><p>CNNs parameterize the convolution kernel with trainable weights, which can be shared for different image regions and therefore lead to less overfitting and more generality. As will be discussed next, on curved surface domains the notion of translation is only locally meaningful, which poses difficulty for effective weight sharing of the convolution kernels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Connections and locally flat connections</head><p>Connections generalize the notion of translation onto curved manifolds with non-Euclidean metric <ref type="bibr" target="#b23">[24]</ref>. Intuitively, a (linear) connection ∇ : T M × T M → T M measures the linear differentiation of moving tangent plane T x M along a vector v ∈ T x M infinitesimally. Therefore, a geodesic curve γ : [0, 1] → M as the "straight line" on a surface has ∇γγ = 0, i.e. the curve tangent vector moves straightly along itself. Indeed, the patch-based multi-directional geodesic CNN(MDGCNN) <ref type="bibr" target="#b31">[32]</ref> connects the convolutions for two surface patches by translating the tangent planes along the geodesic curve connecting the two patch centers, which provides a natural extension of translation on 2D images.</p><p>However, the problem with parallel transporting along the geodesic curves is that the mapping is path dependent. Consider three nearby points x, y, z ∈ M, and denote the transport of tangent planes along the geodesic curve between x, y as τ x,y : T x M → T y M. In general, we have τ y,z • τ x,y = τ x,z , where • is composition, with the difference caused by the curvature of the triangular surface patch bounded by the geodesic curves ( <ref type="figure">Fig. 1(a)</ref>).</p><p>In this paper, we propose to use a construction called locally flat (or trivial) connections <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b35">36]</ref> to achieve the pathindependent tangent space mapping for all surface patches except at a few singular points. The idea of locally flat connections is to concentrate the surface curvature onto a sparse set of singular points and leave the rest majority of surface area with tangent space mappings as in a Euclidean domain, which in turn paves the way for convolutions as on images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">N -direction frame fields and cover space</head><p>One way of encoding the locally flat connections for meshes is through N -direction frame fields <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b44">45]</ref>. An N -direction field at x ∈ M gives a frame of N rotationally symmetric directions u i x ∈ T x M, i = 1, · · · , N ; thus two consecutive vectors in the sequence differ by an angle 2π N . A transport (or matching) τ x,y between two tangent planes of x, y can thus be defined by identifying u i x with u j y , which simply amounts to a change of bases. In particular, we use the principal matching which chooses j such that τ x,y (u i x ) − u j y is minimal, where τ x,y is the parallel transport between x, y along geodesics.</p><p>In addition, a vertex x is singular if and only if it has a loop of neighboring vertices [p 1 , · · · , p n ] such that u i p1 mapped by τ pn,p1 •τ pn−1,pn •· · ·•τ p1,p2 does not return to itself ( <ref type="figure">Fig. 2(c)</ref>). Therefore on a patch containing no singular vertex, the transport τ x,y remains the same regardless of the path taken between x, y [6] ( <ref type="figure">Fig. 1(b)</ref>). On the other hand, the concentrated curvature at a singular vertex can only be multiples of 2π N , which explains the usage of N symmetric directions: larger N allows for more flexible singularities and flat connections. We discuss the choice of N later.</p><p>By solving for smooth (or parallel) frame fields that deviate minimally from the parallel transport and align to salient geometric features of the underlying surface <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b44">45]</ref> (see Appendix B for details), we obtain locally flat connections that closely approximate the linear connection while also having consistency among deformed shapes, therefore supporting improved feature learning by the extended surface convolutions.</p><p>While now we can translate tangent planes, another challenge unique to a surface domain rather than 2D images is the lack of canonical axes for the tangent planes. By randomly fixing an axis on one tangent plane, we risk significantly biasing the feature learning. Instead, a more robust approach is to sample several directions on the tangent planes as axes, and properly aggregate the learned features for the final output. Fortunately, the N -direction frames provide a uniform sampling of tangent directions, which motivates introducing their associated N -cover space that allows to organize the feature learning over multiple axes.</p><p>N -cover space. A frame field induces an N -cover space over the domain surface <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b8">9]</ref>. Intuitively, the cover space consists of N copies M i , i = 1, · · · , N of the base surface, with each copy M i having a unit vector field u</p><formula xml:id="formula_3">σx(i) x , where σ x (i) indexes the vector of the N -direction frame at x for the sheet M i ; in addition, u σx(i) x</formula><p>and u σy(i) y are connected by τ x,y the principal matching <ref type="figure">(Fig. 2)</ref>. The unit vector field is well-defined everywhere on the cover space, except at singular vertices where different sheets of the cover space coincide. In this paper, we use the vector field as the canonical axes and compute surface convolution on the cover space; around singular vertices, our framework degenerates to a strategy similar to the parallel transport method <ref type="bibr" target="#b31">[32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Surface based PFCNN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Surface convolution via parallel frames</head><p>Given a surface mesh M equipped with a parallel Ndirection frame field u i x , the surface convolution for a vertex v i with its feature vector F j vi , j = 1, · · · , N on the σ −1 vi (j)th cover sheet is computed by the following steps:</p><p>1. Choose u j vi as the x-axis of the tangent plane T vi M. Thus the local coordinate system is encoded by the 2×</p><formula xml:id="formula_4">3 matrix F j vi = (u j vi , n vi × u j vi ) T . 2.</formula><p>For each vertex v k in the neighboring geodesic patch N vi , project it onto the tangent plane as v k under co-</p><formula xml:id="formula_5">ordinate system F j vi . Let u l v k = τ vi,v k (u j vi )</formula><p>; the projected point has feature vector F l v k . Resample the projected feature map into a regular grid, denoted F j Nv i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Convolve F j</head><p>Nv i with regular kernels K defined under F j vi . The responses constitute the feature vector of v i for the next network layer.</p><p>Before presenting details for step 2, we remark that translation equivariance in the form of (1) indeed holds locally:</p><formula xml:id="formula_6">τ vi,v k (f k) = τ vi,v k (f ) k,<label>(2)</label></formula><p>where we assume f is a function defined on the tangent plane T vi M, and k is the convolution kernel supported on tangent planes. The equality holds because: on the left hand side, f k returns a function on T vi M which is then transported by the flat connection τ vi,v k to a function on T v k M, while on the right hand side f is first transported onto T v k M and then convolved with k on T v k M; since the transport τ vi,v k only changes the underlying coordinate system bases, the functions f, k when defined using local coordinates do not change at all by the transport, which makes the equality trivially true. In addition, on a patch without singular vertices, the transport and equation holds regardless of the path taken between two vertices, which is different from the path-dependent parallel transport <ref type="bibr" target="#b31">[32]</ref>; for patches with singular vertices, because the transport minimizes deviation from the parallel transport (Sec. 4.3), our convolution closely resembles the parallel transport approach.</p><p>Projection to tangent space and resampling. Previous patch-based surface CNNs use various kinds of geodesic curve tracing to impose a polar coordinate system onto the neighborhood patch N vi and map each neighboring point onto the tangent plane <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b31">32]</ref>. We follow a similar approach adapted from <ref type="bibr" target="#b4">[5]</ref> that is simpler to compute and works even for point clouds, thus enabling easy extension of our framework to point clouds. In particular, we modulate the geodesic coordinates computation using the local axes F j vi , and re-triangulate the projected neighboring points with Delaunay triangulation to avoid flipped triangles, over which a regular grid in the shape of convolution kernels is then resampled and feature vectors interpolated. The operation is encoded by a sparse tensor S that is precomputed for a surface mesh and can be applied efficiently with standard NN libraries. See Appendix C for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">PFCNN structures</head><p>In this section we present the detailed structures of layers specific to PFCNNs. These layers can be combined with standard CNN layers and stacked into networks such as U-Net <ref type="bibr" target="#b37">[38]</ref> and ResNet <ref type="bibr" target="#b13">[14]</ref>.</p><p>Input layers. The PFConv takes as input a group of N feature maps corresponding to the N cover sheets. These features can be constructed by simply duplicating the original input for N copies, i.e. |V |×C in → |V |×N ×C in , where C in is the input per-vertex feature length, or can be computed by further utilizing the local coordinate systems for different cover sheets. Indeed, we find that for tasks on deformable domains, e.g. non-rigid shape classification, segmentation and registration, a simple but effective input feature that is invariant to global rigid transformations is the normal vector and height from tangent plane in local</p><formula xml:id="formula_7">coordinates, i.e. F l v k = F j vi n v k , n T vi n v k , n T vi (v k − v i ) for each patch vertex v k ∈ N vi .</formula><p>In this case, the input layer constructs an expanded |V |×N ×H×W ×C in feature map directly by sampling the local features with regular grids (Sec. 5.1), where H×W is the spatial shape of the subsequent convolution kernel to be applied.</p><p>Output layers. For the final per-vertex output we need to reduce the grouped feature maps to an aggregation, i.e. |V |×N ×C → |V |×C. The reduction operation can be in different forms, e.g. taking the maximum or average among N parallel channels, or being learned implicitly by a standard 1×1 convolution. The outputs can be further aggregated over all vertices into a single output for the whole shape, as in classification tasks.</p><p>Convolution layers. Given an input feature map F in of shape |V |×N ×C in , the convolution layer first vectorizes it into vec(F in ), multiplies with the sparse matrix S of shape (|V |×N ×H×W, |V |×N ) that does the feature map resampling (Sec. 5.1), and reshapes the result vector into a tensor of shape |V |×N ×H×W ×C in ; it then multiplies with the convolution kernel of shape H×W ×C in ×C out to obtain the output feature map |V |×N ×C out . In case the input layer provides an expanded feature map with local features as discussed above, the convolution is a simple multiplication with the kernel. In addition, the special case of 1×1 convolution on each cover sheet through a C in ×C out kernel skips the feature resampling step and is directly multiplied with F in to obtain the output. Note the same convolution kernel is shared for all N cover sheets of feature maps, as the different cover sheets effectively sample the canonical axes over the surface domain.</p><p>Pooling/Unpooling layers. Pooling and unpooling layers effectively change the spatial resolution of learned features. For surface meshes the different domain resolutions can be built through a hierarchy of simplified meshes M i with M 1 = M and each coarse vertex v ∈ V i+1 corresponding to a subset of dense vertices {v k } ⊂ V i , using e.g. <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">15]</ref>. We adapt the simplification process so that their N -direction frames are also mapped, i.e. F j v corresponds to F l v k the closest axes by rotation. Pooling is</p><formula xml:id="formula_8">then defined as F j v = Pool({F l v k }),</formula><p>where Pool(·) takes channel-wise maximum or average; the layer has a signature of |V i |×N ×C → |V i+1 |×N ×C in terms of feature map shapes. Unpooling is the inverse operation of pooling.</p><p>Throughout the paper we assume batch size one, although using larger batch size is trivial as long as each mesh of a batch has the same number of vertices on every domain resolution. We have implemented the above layers with Tensorflow; the code is publicly available 1 . <ref type="bibr" target="#b0">1</ref> Code and data are available at https://github.com/msraig/pfcnn. <ref type="table">Table 1</ref>. Results on SHREC'15 non-rigid shape classification. PN+ is PointNet++ <ref type="bibr" target="#b34">[35]</ref>; "raw" means using spatial coordinates as input, "en" means using an ensemble of intrinsic shape descriptors. MDG is MDGCNN <ref type="bibr" target="#b31">[32]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>We test the PFCNN framework and compare it mainly with the state-of-the-art MDGCNN <ref type="bibr" target="#b31">[32]</ref> on deformable domain tasks involving shape classification, segmentation and registration where MDGCNN achieves uniformly superior performances than other methods, and with the state-of-theart TextureNet <ref type="bibr" target="#b15">[16]</ref> on the scene semantic segmentation task which has a rigid underlying domain. We further do ablation study on the impact of parallel frames, cover space grouped feature maps and layer normalization, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Deformable domain tasks</head><p>For fair comparison, we use 5×5 convolution kernel for PFCNN and a larger 4(radial)×8(angular) kernel for MDGCNN, and the same network structure for both methods in each task, except for registration where the same number of convolution layers are adopted. Network and training details are provided in Appendix D.</p><p>Classification. The SHREC'15 non-rigid shape classification challenge <ref type="bibr" target="#b25">[26]</ref> has 1200 shapes represented by surface meshes that belong to 50 categories. We use a network with three levels of resolution and the localized normal vectors as input features (Sec. 5.2) for PFCNN. As shown in Table. 1, our results outperform PointNet++ <ref type="bibr" target="#b34">[35]</ref> even when it uses an ensemble of sophisticated input features, e.g. WKS and HKS, that are agnostic to non-rigid deformations. We are on par with MDGCNN that uses as input the SHOT descriptor <ref type="bibr" target="#b43">[44]</ref> which is rotation invariant and more sophisticated than our raw input.</p><p>Human body segmentation. The human body segmentation dataset proposed by <ref type="bibr" target="#b26">[27]</ref> contains labeled meshes of diverse human identities and poses from various sources, split by 381/18 for training and testing. The meshes have very different scales which we normalize first. The mesh resolutions are also very different, with the number of vertices varying from 3k to 12k, yet our network works well on the these data without remeshing. The network is a U-Net like structure with three levels of domain resolutions.</p><p>To compare with MDGCNN, we test on both the original meshes and the resampled meshes generated with its open sourced code. Testing results are reported in <ref type="table" target="#tab_0">Table 2</ref> (i) and visualized in <ref type="figure" target="#fig_1">Fig. 3</ref>. Note that the ground truth labeling for different samples are not always consistent, which hinders the possibility of achieving very high accuracy. For example, in <ref type="figure" target="#fig_1">Fig. 3</ref> the third column GT mistakenly labels the shank to thigh. But our method correctly segments this part and has better coverage than MDGCNN. Still for some shapes which are dissimilar to the training data, e.g. the first column in <ref type="figure" target="#fig_1">Fig. 3</ref> which has exceptional hair, both methods fail to segment the hair properly, although our method captures the face better.</p><formula xml:id="formula_9">(ii) (iii)</formula><p>Human body registration by vertex classification. We test with the non-rigid human body registration task proposed by the FAUST dataset <ref type="bibr" target="#b1">[2]</ref>. In one scenario, the registration is achieved by classifying each input mesh vertex of a body shape into its corresponding vertex on the template mesh, as done in previous works <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b10">11]</ref>. We use a simple network consisting of a sequence of convolutions in the same level-of-detail for PFCNN, and a two-level network for MDGCNN, following their original setting.</p><p>The meshes in the FAUST dataset have the same topology with the template, which may be exploited unfairly to learn correspondences. Following MDGCNN, we remesh them to 5k vertices and different topologies. Using the nearest vertex as the correspondence between original meshes and the remeshed ones, we can get the ground truth vertex correspondence to the remeshed template, to supervise the registration task by classifying each vertex to 5k classes.</p><p>We achieved 92.01% accuracy on the remeshed data, as compared to 94.5% accuracy on the original meshes. To fully compare with MDGCNN, we also test variations of their networks with more radial bins and angular directions and different normalizations (more discussions in Sec. 6.3). The accuracies within bounded geodesic errors are plotted in <ref type="figure">Fig. 4</ref>; our results have even better zero-error accuracy than their best with 4×16 kernels and instance normalization. The visual results are shown in <ref type="figure">Fig. 5</ref>; we can see that our results have a smoother mapping to the template shape.  In Appendix E, we test with a more challenging scenario of non-rigid registration by regression on noisy real scans with diverse and high genus meshes, where our results are again considerably better and more robust than MDGCNN.</p><p>To summarize, compared with the parallel transport based convolution by MDGCNN, using parallel frames that induce locally path-independent transport and the alignment to salient geometric features enables more efficient feature learning for our convolution; the difference is more obvious for finer scale tasks like segmentation and registration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Semantic scene segmentation</head><p>In this section, we evaluate on a widely used indoor scene semantic segmentation task provided by the ScanNet dataset <ref type="bibr" target="#b6">[7]</ref>. While indoor scenes generally have rigid geometry dominated by flat walls and floors, PFCNN is still shown to achieve good performances, improving over the state-of-the-art TangentConv <ref type="bibr" target="#b41">[42]</ref> and TextureNet <ref type="bibr" target="#b15">[16]</ref> that use tangential and local patch convolutions.</p><p>We use a network with U-Net structure and three levels of domain resolutions. We follow <ref type="bibr" target="#b15">[16]</ref> to prepare the training data by cropping small chunks from a whole scene and <ref type="table">Table 3</ref>. Results on ScanNet segmentation task. mIoU is the class mean intersection over union. mA is the class mean accuracy. oA is the overall accuracy, which is significantly biased toward floors and walls that are dominant in scenes. Ours* uses a network with more convolution layers. <ref type="bibr" target="#b41">[42]</ref> [ (i) (ii) (iii) <ref type="figure">Figure 6</ref>. Example indoor scenes of ScanNet segmented by comparing methods. (i) is the ground truth segmentation; (ii) is the results of <ref type="bibr" target="#b15">[16]</ref>; (iii) shows our results. Our results have more regular boundaries separating regions of larger consistency. training on these chunks which are randomly rotated around the upright direction for augmentation. For network input, we follow <ref type="bibr" target="#b41">[42]</ref> to include the height above ground, normal vector, color and distance from the local tangent plane for each mesh vertex of a surface patch, rather than the localized normal vector as discussed in Sec. 5.2, while <ref type="bibr" target="#b15">[16]</ref> uses additional high resolution texture images as input. For fair comparison, we have used a network with similar amount of trainable parameters to <ref type="bibr" target="#b15">[16]</ref>; we also explore the effect of increasing the network size and report a better performance.</p><p>The result statistics of comparing methods and ours on validation sets are shown in <ref type="table">Table 3</ref>; our results have much better mean IoU and mean accuracy than theirs, which shows our network can better distinguish smaller objects than just the dominant segments like floors and walls. <ref type="figure">Fig. 6</ref> show some visual results. The black regions in (i) are unlabeled data; our method predicts reasonable labels for these regions. The boundaries separating different objects in our results are cleaner than <ref type="bibr" target="#b15">[16]</ref>, like the boundary between windows and the wall in the first row and the door and wall in the third row; our segments are also more regular and consistent. See Appendix E for more detailed data and visual results on both validation and test sets.</p><p>Considering that all three methods use tangent space convolutions, the results demonstrate that our locally translation equivariant convolution as the key difference is more effective in learning features.  <ref type="table">Table 5</ref>. Accuracy and runtime cost of different frame field symmetry orders N , on the non-rigid registration task by vertex classification. The costs are measured on an RTX2080 GPU.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Ablation study</head><p>In this section, we evaluate how the core constructions and hyper parameters of PFCNNs affect performance. We also study the impact of normalization on deformable domain tasks, as well as the behavior around singular vertices. Using frames and grouped features. We evaluate the performances of different configurations that add components of the PFCNN construction one-by-one onto a baseline model. The evaluations are done on the task of human body registration by vertex classification (Sec. 6.1).</p><p>• Baseline model. When using principal curvature frames as coordinate frames of the tangent plane, we have a baseline model similar to a bunch of recent previous works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b41">42]</ref>. Using the network structure similar to PFCNN but without aligning the tangent planes by flat connections or feature map grouping by cover sheets, the trainable convolution kernel parameters are actually 16 times of PFCNN. However, the accuracy for this baseline configuration is 83.29% (Table 4), much lower than PFCNN.</p><p>• Principle curvature frames as 4-direction field. As a modification to the baseline model, we regard the principal curvature frames as a 4-direction frame field and apply the PFCNN network. The result accuracy is 89.8% <ref type="table" target="#tab_2">(Table 4</ref>), much higher than the baseline model, while using only 1/16 trainable parameters. The improvement demonstrates that even if the frame field is not globally optimized to be smooth or aligned to salient features, by using its encoded flat connections that enable local translation equivariance and its induced cover space feature maps that sample tangent directions, the feature learning is significantly improved.</p><p>• Full PFCNN model. By additionally optimizing for a parallel frame field that aligns to geometric features, the PFCNN framework further improves to 92.01% registration accuracy <ref type="table" target="#tab_2">(Table 4</ref>).</p><p>Frame symmetry order. As discussed in Sec. 4.3, when the rotational symmetry order N of the frame field gets larger, the frame field has more flexibility to achieve both smoothness and alignment to salient features. However, an increased N also leads to larger computational cost, as the size of feature maps to compute increases too. We tested the different N values again with the registration by vertex classification task, but modified the network structure to make sure each group of the feature map has the same size (i.e. 64), so that for different N the amount of trainable convolution kernel parameters remains the same. The performances for different N are shown in <ref type="table">Table 5</ref>. We can see that the choice of N = 4 strikes a balance between accuracy and computational overhead: for N &lt; 4 the accuracy is notably lower due to the limited field smoothness, and for N &gt; 4 the computational cost is higher, with the extra runtime roughly in proportion to the number of axes sampled. We have used N = 4 for all the other experiments in this paper.</p><p>Normalization. It is well known that normalization can speed up the training procedure and make it more stable.</p><p>Here we study the impact of different normalizations on surface based CNNs more closely. For the registration by classification task, we test our method and MDGCNN with batch normalization (BN) and instance normalization (IN). Note that since the batch size is one, the difference between BN and IN is that, the channel wise statistics of moving mean and average are used in testing stage for BN but not IN. The result is shown in <ref type="figure">Fig. 4</ref>. We find that with IN both our method and MDGCNN achieve better performances. We repeat the experiments on the shape classification task (Sec 6.1); the result is shown in <ref type="table" target="#tab_4">Table 6</ref>. From all these experiments, we can see that IN is better than BN for these tasks on deformable domains. We argue that this is because the diversely deformed shapes do not share common statistics of channel-wise mean and variance, akin to the observation in image style transfer <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b16">17]</ref> that these statistics encode styles rather than content.</p><p>Frame field singularity. As discussed in Sec. 5.1, near singularities the translation equivariance is no longer pathindependent, but our scheme degenerates to being similar to MDGCNN using parallel transport. To find the relationship between the singularity of vertices and prediction error, we compare the distribution of singular vertices and the error map of geodesic distance between predicted vertex and the ground truth correspondence vertex. The distribution is shown in <ref type="figure" target="#fig_5">Fig. 7</ref>; we can see that the singular vertices mainly distribute on the nose, fingers or toes but the error maps of different shapes do not reflect these similarity. We also compare the accuracy of singular vertices and all ver- tices in the registration by classification task. In particular, on the original dataset and the remeshed dataset, the registration accuracy of singular vertices versus that of all vertices are, 93.4%/94.5% and 90.2%/92.2%, indicating no clear correlation of singular vertices and prediction errors. Such robustness can be attributed to the degenerated convolution with path-dependent translation equivariance, the consistency of singularities across shapes (see Appendix B) and the capability of learned filters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We have presented a surface mesh based PFCNN framework that closely mimics the standard image based CNNs and has local translation equivariance for convolutions. It is enabled by using parallel N -direction frames that both encode flat connections on the surface to define pathindependent translation, and sample tangent plane canonical axes to organize the convolutions by the N -cover spaces. The PFCNNs are shown to be more effective at fine-scale feature learning than previous surface based CNNs. In the future, we would like to investigate how the PFCNN framework can handle surface generation tasks, where the frame field also needs to be generated rather than precomputed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Overview</head><p>In the appendix, we provide the detailed algorithms and additional discussions for computing the parallel N -direction frame fields, the mapping of neighborhood patches onto tangent planes and the resampling of feature maps there. The network structures and training details of experiments in the text, as well as additional results and comparisons with previous methods are also presented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Computing the parallel frame fields</head><p>Given a 3D surface mesh, the smooth or parallel frame field that approximates parallel transport of tangent spaces for neighboring points can be efficiently constructed <ref type="bibr" target="#b44">[45]</ref>. In particular, we adopt the complex number based approach <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b7">8]</ref> to encode the N -direction fields. We identify the tangent plane T x M with the complex plane, and a set of unit length vectors {u · e ik 2π N |k = 0, · · · , N − 1} ⊂ C forming a rotationally symmetric N -direction frame can be conveniently encoded by their common N -th order power z = u N ∈ C. To compute a smooth frame field that a) deviates from the parallel transport minimally and b) aligns with salient geometric features of the domain surface, we solve the following optimization problem:</p><formula xml:id="formula_10">min {zi} . i∼j z i − t ji z j 2 + λ i w i z i − z 0 i 2 ,<label>(3)</label></formula><p>where i, j are neighboring vertices on the surface mesh, t ji ∈ C is the discrete parallel transport along the edge ij that rotates the tangent plane of j to identify with that of i <ref type="bibr" target="#b20">[21]</ref>, λ is the weight for the second curvature direction alignment term, w i = tanh(|k max − k min |) measures the anisotropy at the i-th vertex using its maximum and minimum principle curvature values k max , k min , and z 0 i is the complex N -th order power of the maximum curvature direction at the vertex. The first term is a discretization of the Dirichlet energy of the frame field that measures its variation and encourages parallelism. The second term encourages alignment of the frame field to strong anisotropic directions and salient geometric features of the surface.</p><p>As shown in <ref type="figure">Fig. 8</ref>, the smooth frame fields aligned with salient geometric features show strong consistency among deformed shapes, and the singular points are placed consistently at regions with high curvature.</p><p>Alignment to anisotropy. We test how different balances of field smoothness and alignment to strong anisotropy of the surfaces affect performances. We generate four different sets of frames for the registration task, using λ = 0, 0.01, 0.1, 1 respectively. The testing accuracies are reported in <ref type="table" target="#tab_5">Table 7</ref>, where "SF", meaning smooth frames without curvature direction alignment, corresponds to λ = 0 and z = 1 to prevent degenerate solutions. From <ref type="figure">Figure 8</ref>. The smooth frame fields (shown as crosses) aligned with salient geometric features have strong consistency among diverse human body shapes. The singular vertices marked as red points are also distributed similarly across the shapes, concentrating on regions of high curvature, e.g. nose, finger tips, and toes. the results, we see that a mild alignment to strong surface anisotropic directions is helpful in achieving the best performances. Therefore, we have used λ = 0.01 for all tasks shown in other parts of the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Tangent plane projection and resampling</head><p>The algorithm for building the convolution structure on a local patch of a mesh vertex is illustrated in Alg. 1 in pseudo code. For each mesh vertex, the algorithm first does a flood searching of K neighbor vertices in O(K) and projects the vertices onto the tangent plane using local coordinate systems. It then triangulates the projected vertices into a Delaunay triangulation in O(K log K), and samples H×W grid points against the triangulation in O(HW log K). The sampled grid points are finally stored into the sparse tensor that will be reshaped as a sparse matrix and readily multiplied with feature maps in each convolution operation (Sec. 5.2). Note that all vertices can be processed in parallel.</p><p>In the algorithm we have abused notations slightly, using t[0] of a tuple to represent the vertex, its index, and its spatial position; the exact meaning should be clear from context. For a given level of domain resolution, the patch size parameter d is set to be the average edge length of all meshes in the given level of the training dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Tangent plane projection and feature map resampling for a local patch</head><p>Input: v i ∈ V , frames F , transport τ , patch side length d, conv kernel shape H×W Output: updated sparse tensor S of shape |V |×N ×H×W ×|V |×N // Flood to find and project neighbor vertices </p><formula xml:id="formula_11">Q = [(v i , (0, 0), 0)], P = {}, visited = {v i }; while Q not empty do t =dequeue(Q), P = P ∪ t; if dist(v i , t[0]) &gt; √ 2d or t[1] &gt; √ 2d then continue; end for v k ∼ t[0], v k / ∈ visited do visited = visited ∪ v k ; u l v k = τ t[0],v k (u t[2] t[0] ); v k = 0.5 · (F t[2] t[0] + F l v k )(v k − t[0]) + t[1]; enqueue(Q, (v k , v k ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Network structures and training details</head><p>We have used convolution kernels with spatial size 5×5 for all deformable domain tasks, and 3×3 for the semantic scene segmentation. All our networks have been trained with the Adam solver <ref type="bibr" target="#b19">[20]</ref> and batch size one, with fixed learning rate 10 −4 .</p><p>The network structure used for SHREC'15 non-rigid shape classification is shown in <ref type="figure">Fig. 9</ref>. It is trained for 50 epochs on a single GPU. For the variant network without any normalization layers, it needs to train for 100 epochs until convergence.</p><p>The network used for the human body segmentation task is shown in <ref type="figure">Fig 10.</ref> It has three level-of-details. The loss function is the summation of cross entropy between pre-  <ref type="figure">Figure 9</ref>. The network used for SHREC'15 non-rigid shape classification task. Each box represents a feature map of shape V ×C, where C is the total feature size for all N =4 cover sheets and given by numbers aside the boxes, and V the number of surface vertices. The input feature map is a 4-channel feature of H×W grid points for each vertex (Sec. 5.2). The "convolution through residual block" contains two sequential residual blocks, with each block made by two convolutions that retain the input feature size. All convolution operations except the last one are followed with instance normalization and ReLU. Global average pooling is a standard average pooling over all vertices. For this dataset there are around 10k, 1700, 300 vertices for the three level-of-details, respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Nx32 Nx32</head><p>Nx64 Nx32 <ref type="figure">Figure 10</ref>. The network used for the human body segmentation task. See caption of <ref type="figure">Fig. 9</ref> for detailed explanation. The number of vertices for the three level-of-details are V, V /3, V /9, where V is the number of vertices of each mesh in the dataset. In the original dataset, V varies from 3k to 12k. For the remeshed data, V is around 7k.</p><p>dicted segmentation label and the ground truth label for each mesh vertex. The network is trained for 50 epochs. To obtain the predicted per-face segmentation labels, we sample points for each face of a test mesh and project the points onto closest vertices of our remeshed models, whose labels are used to vote for the face label of the original test mesh. The network used for the human body registration task by vertex classification, and testing different frame field symmetry ordersis is shown in <ref type="figure">Fig. 11</ref>. The network is trained for 400 epochs. <ref type="figure">Figure 11</ref>. The network used for human body registration through a classification of mesh vertices into 6890 or 5000. The number of surface vertices is 6890 for the original dataset and 5000 for the remeshed dataset. See caption of <ref type="figure">Fig. 9 for detailed</ref>   <ref type="figure">Figure 12</ref>. The regression network used for human body regression task. See caption of <ref type="figure">Fig. 9</ref> for detailed explanation. The number of surface vertices are around 10k, 3.2k, 1k for the three level-ofdetails respectively.  <ref type="figure" target="#fig_1">Figure 13</ref>. The network used for ScanNet segmentation task. See caption of <ref type="figure">Fig. 9</ref> for detailed explanation. The input include the 7-channel feature of each vertex and the 1-channel local height feature for each grid point. The number of vertices for the three level-of-details are V, V /3, V /9, where V is the number of vertices of each cropped chunk, with the crop method same as <ref type="bibr" target="#b15">[16]</ref>.</p><p>The network used for the ScanNet semantic scene segmentation task is shown in <ref type="figure" target="#fig_1">Fig. 13</ref>. The network outputs, for each vertex, the probability distribution of 21 segmentation labels, which is compared with ground truth label using cross entropy during training. It is trained for 30 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. More results and comparisons</head><p>Shrec'15 classification We show some results in the classification task in <ref type="figure">Fig. 14;</ref> the single misclassified shape by <ref type="bibr">Figure 14.</ref> In the first row left shows the single incorrectly classified shape by our method; it is an "ant" misclassified as "spider" (an example shown on the right, is indeed confusingly similar to "ant"). In the second row we show more shapes in the SHREC'15 dataset, which are "camel", "horse" and "cat". our method is a challenging "ant" that looks similar to the wrong label "spider".</p><p>Non-rigid registration by fitting template embedding. In this part we present an application that resolves the nonrigid registration problem with an approach different from the per-vertex classification (Sec. 6.1). We notice that the registration by classification has severe limitations in real applications: to classify each vertex to 6k classes for example is not scalable when there are many input vertices, and the classification error does not measure at all how far away a mis-classified vertex is from ground-truth. Thus we propose a novel but simple method for non-rigid registration that uses a surface-based CNN for direct regression of the template embedding in R 3 . We evaluate on the real scans of the FAUST dataset, which has 80 meshes for training and 20 for test. The meshes are noisy, with diverse and high genus for different poses of a same person (see <ref type="figure" target="#fig_7">Fig. 15</ref> for statistics), which is frequently due to the merging of spatially intersecting components. Since the raw scans are very dense meshes, we have remeshed each raw scan to simpler meshes with the number of vertices around 10k. For each real scan there is a registered deformed template mesh, which provides the ground truth embedding for supervision and testing. To be specific, we project a vertex of the real scan to the closest point on the registered deformed template mesh, and take  <ref type="figure" target="#fig_5">Figure 17</ref>. The ratio of vertices whose error is bellow given threshold. The per-vertex error is the geodesic distance between the predicted point position and ground truth on the template surface, normalized by square root of surface area. Our accuracy under 0.03 is 97.98% while <ref type="bibr" target="#b31">[32]</ref> is 69.37%.</p><p>its position and normal vectors on the rest pose template as the supervising regression target. The network for this point-wise regression is a standard UNet structure as shown in <ref type="figure">Fig. 12</ref>. For each vertex of an input raw scan mesh, the output contains the position and normal vectors of the corresponding point on the rest pose template mesh. The training loss is</p><formula xml:id="formula_12">L = 1 V V i   p i − p 0 i 1 + w reg V i j∼i p i − p j 1   + w n V V i   n i − n 0 i 1 + w reg V i j∼i n i − n j 1   + w con E i∼j |n i · (p i − p j )|,</formula><p>where V is the number of vertices of the raw scan mesh, p the regressed vertex position, p 0 the target position, n the regressed vertex normal, n 0 the target normal, w n = 0.1 to normalize different scales between position and normal in the dataset, w reg = 0.2 the weight for Laplacian regularization terms of position and normal, w con = 20 the weight for normal and position consistency, V i the number of neighboring vertices of the i-th vertex, and E the number of directed mesh edges. We use l 1 norm for these losses because there are noisy vertices in the raw scans which do not have valid target points on the template surface. We train the network for 200 epochs on single GPU using Adam solver with a fixed learning 1 × 10 −4 . Geodesic errors of the network predictions on the test set are shown in <ref type="figure" target="#fig_5">Fig. 17</ref>. Following <ref type="bibr" target="#b18">[19]</ref>, the geodesic error for a surface point x with predicted position y and ground truth point y * on the template surface M is computed as (x) = d M (y,y * ) √ |M| , where d M (·, ·) computes the geodesic distance of two points projected onto the surface M, and |M| is its area for normalization. Visual results are shown in <ref type="figure" target="#fig_8">Fig. 16</ref>. It is clear that our results are better than MDGCNN both quantitatively and qualitatively on these real scans, and the difference seems to be more obvious than the registration by vertex classification task on the clean meshes (Sec. 6.1).</p><p>More results of ScanNet segmentation. We present the per-category prediction accuracy (measured by IoU) of comparing methods for ScanNet semantic segmentation in <ref type="table">Table 8</ref> and <ref type="table" target="#tab_10">Table 9</ref>. For Ours*, the network structure is similar to the network shown in <ref type="figure" target="#fig_1">Fig. 13</ref>, but each "PFConv residual block" contains three sequential residual blocks and the feature sizes in three levels are changed to 128, 256, 512, respectively. More visual results are shown in <ref type="figure">Fig. 18</ref>. <ref type="figure">Figure 18</ref>. More results of Scannet segmentation.(i) is the ground truth segmentation; (ii) is the results of <ref type="bibr" target="#b41">[42]</ref> (iii) is the results of <ref type="bibr" target="#b15">[16]</ref>; (iv) shows our results. (v) is the result of our method with deeper network. Our method gives clearer boundaries, like the boundary between window and wall in the second row, the boundary between picture and wall and the boundary of sink in the last row. <ref type="table">Table 8</ref>. Per-category IoU on ScanNet validation set. The abbreviations respectively stand for "bathtub, bed, bookshelf, cabinet, chair, counter, curtain, desk, door, floor, otherfurniture, picture, refrigerator, shower, curtain, sink, sofa, table, toilet, wall, window". The highest accuracies both among the three comparing results and among the four comparing results with our additional increased network are marked in bold.</p><formula xml:id="formula_13">(i) (ii) (iii) (iv) (v)</formula><p>Method mIoU bath bed book cab chr cntr crtn desk door flr other pic refrg shwr sink sofa tab toil wall wdw <ref type="bibr" target="#b41">[42]</ref> 49. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 . 4 -</head><label>24</label><figDesc>direction frame fields and the corresponding cover spaces. (a)&amp;(b): a field without singular vertex and the four separate sheets of the cover space. (c)&amp;(d): a field with a singular vertex on the cube-corner shaped surface and the four sheets of cover space that are connected and coincide at the singular vertex.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Results of human body segmentation. (i) the groundtruth labeling; (ii) the results of MDGCNN; (iii) our results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .Figure 5 .</head><label>45</label><figDesc>Accuracy within given geodesic error for the non-rigid registration by vertex classification. Visual comparison of our method and MDGCNN on non-rigid registration. (i) the ground truth mapping; (ii) the best results of MDGCNN with 4 bins, 16 directions; (iii) our result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>N</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>(i) shows the singular vertices in red and (ii) shows the prediction error map. There is no clear correlation between the singular vertices and the erroneous predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>l)); end end // Triangulate the projected points DT (P ) = Delaunay triangulation of {t[1]|t ∈ P }; // Resample with a regular grid sized d×d for j = 1, · · · , N do for grid point p r,c , 1 ≤ r ≤ H, 1 ≤ c ≤ W do find the containing triangle in DT (P ) with vertices corresponding to (t a , t b , t c ) ⊂ P ; compute barycentric weights (w a , w b , w c ); S(i, j, c, r, t a [0], t a [2]) = w a ; S(i, j, c, r, t b [0], t b [2]) = w b ; S(i, j, c, r, t c [0], t c [2]) = w c ; end end</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 15 .</head><label>15</label><figDesc>Genus of the meshes in the FAUST real scan dataset. More than half of the meshes have genus larger than 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 16 .</head><label>16</label><figDesc>Results of non-rigid human body registration through regression of the template embedding coordinates. (i) shows the texture mapping using the groundtruth correspondence. (ii) is the results of<ref type="bibr" target="#b31">[32]</ref>. (iii) shows our results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>using SHOT features as input. Results on human body segmentation. Our method outperforms MDGCNN on both original data and the remeshed data.</figDesc><table><row><cell></cell><cell>PN+(raw)</cell><cell cols="2">PN+(en)</cell><cell cols="2">MDG Ours</cell></row><row><cell>Accu.(%)</cell><cell>60.18</cell><cell>96.09</cell><cell></cell><cell>99.5</cell><cell>99.5</cell></row><row><cell></cell><cell>Original</cell><cell></cell><cell></cell><cell cols="2">Remeshed</cell></row><row><cell>Method</cell><cell>MDGCNN</cell><cell>Ours</cell><cell cols="2">MDGCNN</cell><cell>Ours</cell></row><row><cell>Accu.(%)</cell><cell>88.2</cell><cell>91.45</cell><cell></cell><cell>89.53</cell><cell>91.79</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Testing accuracy of different convolution methods on the non-rigid registration task by vertex classification. PCF means using the principle curvature directions as tangent plane axes. PCF as FF means using the principal directions as the 4-direction frame field for our PFCNN framework.</figDesc><table><row><cell></cell><cell>PCF</cell><cell>PCF as FF</cell><cell>Ours</cell></row><row><cell>Accu.(%)</cell><cell>83.29</cell><cell>89.80</cell><cell>92.01</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .</head><label>6</label><figDesc>Classification accuracy with different normalization.</figDesc><table><row><cell></cell><cell cols="2">Ours</cell><cell cols="2">MDGCNN</cell></row><row><cell cols="2">Normalization BN</cell><cell>IN</cell><cell>BN</cell><cell>IN</cell></row><row><cell>Accuracy(%)</cell><cell>11</cell><cell>99.5</cell><cell cols="2">14.0 99.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 .</head><label>7</label><figDesc>Testing accuracy of different frame alignment choices, on the FAUST non-rigid registration task by classification. SF means smoothness only without alignment to surface anisotropy. The other numbers are used as the curvature direction alignment weight λ for computing the smooth frame field.</figDesc><table><row><cell>SF</cell><cell>0.01</cell><cell>0.1</cell><cell>1</cell></row><row><cell>Accu.(%) 88.56</cell><cell>92.01</cell><cell>91.97</cell><cell>90.77</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>explanation.</figDesc><table><row><cell>NxHxWx4 Nx64 Nx64</cell><cell></cell><cell>Nx128 Nx64 Nx64 Nx6</cell><cell>6</cell></row><row><cell></cell><cell>Nx64 Nx64</cell><cell>Nx128 Nx64</cell></row><row><cell>Feature reduce</cell><cell></cell><cell></cell></row><row><cell>PFConv, ReLU PFConv residual block</cell><cell></cell><cell></cell></row><row><cell>Max pooling</cell><cell></cell><cell></cell></row><row><cell>Average unpooling</cell><cell cols="2">Nx64 Nx64</cell></row><row><cell>PFConv 1x1 Copy</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 .</head><label>9</label><figDesc>1 68.0 63.8 56.3 41.7 73.6 45.6 33.2 40.7 34.9 91.9 26.2 14.5 31.7 28.1 44.2 62.8 51.5 68.8 67.9 38.3 [16] 58.1 67.6 67.3 71.3 46.8 78.1 44.4 52.5 47.5 44.8 94.4 40.2 21.1 35.2 51.3 51.7 64.0 63.5 80.3 75.0 46.0 Ours 63.3 79.7 70.3 73.7 55.6 81.0 53.9 70.1 53.1 50.0 93.7 42.3 30.3 46.3 55.6 60.1 66.4 60.9 87.3 78.7 56.5 Ours* 66.2 81.6 73.0 77.0 56.8 83.3 62.8 70.9 55.8 52.3 94.0 46.4 33.1 51.7 60.9 61.2 72.3 65.0 87.7 80.0 58.6 Per-category IoU on ScanNet test set. See caption of Table 8 for explanations. Method mIoU bath bed book cab chr cntr crtn desk door flr other pic refrg shwr sink sofa tab toil wall wdw [42] 43.8 43.7 64.6 47.4 36.9 64.5 35.3 25.8 28.2 27.9 91.8 29.8 14.7 28.3 29.4 48.7 56.2 42.7 61.9 63.3 35.2 [16] 56.6 67.2 66.4 67.1 49.4 71.9 44.5 67.8 41.1 39.6 93.5 35.6 22.5 41.2 53.5 56.5 63.6 46.4 79.4 68.0 56.8 Ours 60.2 74.6 71.2 67.4 53.5 75.6 41.6 68.1 42.0 43.4 93.8 40.1 27.0 51.2 51.1 61.2 69.4 48.3 84.7 77.7 61.5 Ours* 62.2 79.7 69.7 75.0 57.7 79.2 47.6 68.5 36.6 46.8 94.2 41.4 30.7 53.2 49.4 68.1 71.5 47.5 88.0 79.6 59.3</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Point convolutional neural networks by extension operators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matan</forename><surname>Atzmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haggai</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaron</forename><surname>Lipman</surname></persName>
		</author>
		<idno>71:1-71:12</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">FAUST: Dataset and evaluation for 3D mesh registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning shape correspondence with anisotropic convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Rodolà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="3189" to="3197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Geometric deep learning: Going beyond euclidean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Parallel transport unfolding: A connection-based manifold learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Budninskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gloria</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leman</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiying</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Desbrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Appl. Algebra Geom</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="266" to="291" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Trivial connections on discrete surfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keenan</forename><surname>Crane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Mathieu Desbrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schrder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Scannet: Richly-annotated 3D reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Designing n-polyvector fields with complex polynomials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Diamanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Vaxman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniele</forename><surname>Panozzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Boris A Dubrovin</surname></persName>
		</author>
		<title level="m">Anatolij Timofeevič Fomenko, and Sergeȋ Novikov. Modern geometrymethods and applications: Part II: The geometry and topology of manifolds</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Jonathon Shlens, and Manjunath Kudlur. A learned representation for artistic style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<idno>abs/1610.07629</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">SplineCNN: Fast geometric deep learning with continuous B-spline kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Eric</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Weichert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heinrich</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Surface simplification using quadric error metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Garland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">S</forename><surname>Heckbert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="209" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<ptr target="http://www.deeplearningbook.org.1" />
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Progressive meshes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugues</forename><surname>Hoppe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="99" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Texturenet: Consistent local parametrizations for learning from highresolution signals on meshes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haotian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Niessner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06-02" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Arbitrary style transfer in real-time with adaptive instance normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1501" to="1510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Quadcover -surface parameterization using branched coverings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Kälberer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Polthier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="375" to="384" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Blended intrinsic maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaron</forename><surname>Lipman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph. (SIGGRAPH)</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2011-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Globally optimal direction fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Knöppel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keenan</forename><surname>Crane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Pinkall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Schröder</surname></persName>
		</author>
		<idno>59:1-59:10</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph. (SIGGRAPH)</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Gradientbased learning applied to document recognition. Proceedings of the IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998-11" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<title level="m">Connections. In Riemannian Manifolds</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1997" />
			<biblScope unit="page" from="47" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Cross-atlas convolution for parameterization invariant learning on textured mesh surface</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingmin</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6143" to="6152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Non-rigid 3D Shape Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Elnaghy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>El-Sana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Furuya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Giachetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Guler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Limberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">U</forename><surname>Nakanishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Neto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G</forename><surname>Nonato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ohbuchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pevzner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pickup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rosin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Unal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurographics Workshop on 3D Object Retrieval. The Eurographics Association</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on surfaces via seamless toric covers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meirav</forename><surname>Haggai Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Galun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miri</forename><surname>Aigerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadav</forename><surname>Trope</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Dym</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><forename type="middle">G</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaron</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lipman</surname></persName>
		</author>
		<idno>71:1-71:10</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph. (SIGGRAPH)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<title level="m">Geodesic convolutional neural networks on riemannian manifolds. In ICCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="832" to="840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Voxnet: A 3D convolutional neural network for real-time object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="922" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Rodolà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Film: Visual reasoning with a general conditioning layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harm De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multi-directional geodesic neural networks via equivariant convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Poulenard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maks</forename><surname>Ovsjanikov</surname></persName>
		</author>
		<idno>236:1-236:14</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph. (SIGGRAPH ASIA)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3D classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Volumetric and multi-view cnns for object classification on 3D data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyuan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5648" to="5656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Point-net++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Geometry Aware Direction Field Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Vallet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Lévy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Octnet: Learning deep 3D representations at high resolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gernot</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Osman Ulusoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3577" to="3586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unet: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep learning 3D shape surfaces using geometry images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayan</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Ramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="223" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Splatnet: Sparse lattice networks for point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2530" to="2539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learned-Miller. Multi-view convolutional neural networks for 3D shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Tangent convolutions for dense prediction in 3D</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian-Yi</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Kpconv: Flexible and deformable convolution for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugues</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Emmanuel</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatriz</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Unique signatures of histograms for local surface description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuele</forename><surname>Salti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luigi</forename><forename type="middle">Di</forename><surname>Stefano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="356" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Vaxman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcel</forename><surname>Campen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Diamanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniele</forename><surname>Panozzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bommes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Hildebrandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirela</forename><surname>Ben-Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Directional Field Synthesis, Design, and Processing</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">O-cnn: Octree-based convolutional neural networks for 3D shape analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng-Shuai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">72</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">3D shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Directionally convolutional networks for 3D shape segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relu</forename><surname>Pfconv</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
