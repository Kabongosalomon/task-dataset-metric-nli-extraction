<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RMDL: Random Multimodel Deep Learning for Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamran</forename><surname>Kowsari</surname></persName>
							<affiliation key="aff5">
								<orgName type="department">Sensing Systems for Health Lab</orgName>
								<orgName type="institution">University of Virginia</orgName>
								<address>
									<settlement>Charlottesville</settlement>
									<region>VA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mojtaba</forename><surname>Heidarysafa</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><forename type="middle">E</forename><surname>Brown</surname></persName>
							<affiliation key="aff6">
								<orgName type="department">Data Science Institute</orgName>
								<orgName type="institution">University of Virginia Charlottesville</orgName>
								<address>
									<region>VA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff7">
								<orgName type="department">Predictive Technology Laboratory</orgName>
								<orgName type="institution">University of Virginia</orgName>
								<address>
									<settlement>Charlottesville</settlement>
									<region>VA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiana</forename><forename type="middle">Jafari</forename><surname>Meimandi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><forename type="middle">E</forename><surname>Barnes</surname></persName>
							<affiliation key="aff5">
								<orgName type="department">Sensing Systems for Health Lab</orgName>
								<orgName type="institution">University of Virginia</orgName>
								<address>
									<settlement>Charlottesville</settlement>
									<region>VA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="department">Data Science Institute</orgName>
								<orgName type="institution">University of Virginia Charlottesville</orgName>
								<address>
									<region>VA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of System and Information Engineering</orgName>
								<orgName type="institution">University of Virginia Charlottesville</orgName>
								<address>
									<region>VA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of System and Information Engineering</orgName>
								<orgName type="institution">University of Virginia Charlottesville</orgName>
								<address>
									<region>VA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of System and Information Engineering</orgName>
								<orgName type="institution">University of Virginia Charlottesville</orgName>
								<address>
									<region>VA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Department of System and Information Engineering</orgName>
								<orgName type="institution">University of Virginia Charlottesville</orgName>
								<address>
									<region>VA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Department of System and Information Engineering</orgName>
								<orgName type="institution">University of Virginia Charlottesville</orgName>
								<address>
									<region>VA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">RMDL: Random Multimodel Deep Learning for Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3206098.3206111</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS • Information systems → Decision support systems</term>
					<term>Data mining; KEYWORDS Data Mining</term>
					<term>Text Classification</term>
					<term>Image Classification</term>
					<term>Deep Neural Networks</term>
					<term>Deep Learning</term>
					<term>Supervised Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The continually increasing number of complex datasets each year necessitates ever improving machine learning methods for robust and accurate categorization of these data. This paper introduces Random Multimodel Deep Learning (RMDL): a new ensemble, deep learning approach for classification. Deep learning models have achieved state-of-the-art results across many domains. RMDL solves the problem of finding the best deep learning structure and architecture while simultaneously improving robustness and accuracy through ensembles of deep learning architectures. RDML can accept as input a variety data to include text, video, images, and symbolic. This paper describes RMDL and shows test results for image and text data including MNIST, CIFAR-10, WOS, Reuters, IMDB, and 20newsgroup. These test results show that RDML produces consistently better performance than standard methods over a broad range of data types and classification problems. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Categorization and classification with complex data such as images, documents, and video are central challenges in the data science community. Recently, there has been an increasing body of work using deep learning structures and architectures for such problems. However, the majority of these deep architectures are designed for a specific type of data or domain. There is a need to develop more general information processing methods for classification and categorization across a broad range of data types. While many researchers have successfully used deep learning for classification problems (e.g., see <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b50">51]</ref>), the central problem remains as to which deep learning architecture (DNN, CNN, or RNN) and structure (how many nodes (units) and hidden layers) is more efficient for different types of data and applications. The favored approach to this problem is trial and error for the specific application and dataset.</p><p>This paper describes an approach to this challenge using ensembles of deep learning architectures. This approach, called Random Multimodel Deep Learning (RMDL), uses three different deep learning architectures: Deep Neural Networks (DNN), Convolutional Neural Netwroks (CNN), and Recurrent Neural Networks (RNN). Test results with a variety of data types demonstrate that this new approach is highly accurate, robust and efficient.</p><p>The three basic deep learning architectures use different feature space methods as input layers. For instance, for feature extraction from text, DNN uses term frequency-inverse document frequency (TF-IDF) <ref type="bibr" target="#b42">[43]</ref>. RDML searches across randomly generated hyperparameters for the number of hidden layers and nodes (desity) in each hidden layer in the DNN. CNN has been well designed for image classification. RMDL finds choices for hyperparameters in CNN using random feature maps and random numbers of hidden layers. CNN can be used for more than image data. The structures for CNN used by RMDL are 1D convolutional layer for text, 2D for images and 3D for video processings. RNN architectures are used primarily for text classification. RMDL uses two specific RNN structures: Gated Recurrent Units (GRUs) and Long Short-Term Memory (LSTM). The number of GRU or LSTM units and hidden layers used by the RDML are also the results of search over randomly generated hyperparameters.</p><p>The main contributions of this work are as follows: I) Description of an ensemble approach to deep learning which makes the final model more robust and accurate. II) Use of different optimization techniques in training the models to stabilize the classification task. III) Different feature extraction approaches for each Random Deep Leaning (RDL) model in order to better understand the feature space (specially for text and video data). IV) Use of dropout in each individual RDL to address over-fitting. V) Use of majority voting among the n RDL models. This majority vote from the ensemble of RDL models improves the accuracy and robustness of results. Specifically, if k number of RDL models produce inaccuracies or overfit classifications and n &gt; k, the overall system is robust and accurate VI) Finally, the RMDL has ability to process a variety of data types such as text, images and videos.</p><p>The rest of this paper is organized as follows: Section 2 gives related work for feature extraction, other classification techniques, and deep learning for classification task; Section 3 describes current techniques for classification tasks which are used as our baseline; Section 4 describes Random Multimodel Deep Learning methods and the architecture for RMDL including Section 4.1 shows feature extraction in RMDL, Section 4.2 talks about overall view of RMDL; Section 4.3 addresses the deep learning structure used in this model, Section 4.4 discusses optimization problem; Section 5.1 talks about evaluation of these techniques; Section 5 shows the experimental results which includes the accuracy and performance of RMDL; and finally, Section 6 presents discussion and conclusions of our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Researchers from a variety of disciplines have produced work relevant to the approach described in this paper. We have organized this work into three areas: I) Feature extraction; II) Classification methods and techniques (baseline and other related methods); and III) Deep learning for classification.</p><p>Feature Extraction: Feature extraction is a significant part of machine learning especially for text, image, and video data. Text and many biomedical datasets are mostly unstructured data from which we need to generate a meaningful and structures for use by machine learning algorithms. As an early example, L. Krueger et. al. in 1979 <ref type="bibr" target="#b25">[26]</ref> introduced an effective method for feature extraction for text categorization. This feature extraction method is based on word counting to create a structure for statistical learning. Even earlier work by H. Luhn <ref type="bibr" target="#b33">[34]</ref> introduced weighted values for each word and then G. <ref type="bibr">Salton et. al. in 1988 [44]</ref> modified the weights of words by frequency counts called term frequency-inverse document frequency (TF-IDF). The TF-IDF vectors measure the number of times a word appears in the document weighted by the inverse frequency of the commonality of the word across documents. Although, the TF-IDF and word counting are simple and intuitive feature extraction methods, they do not capture relationships between words as sequences. Recently, T. Mikolov et. al. <ref type="bibr" target="#b35">[36]</ref> introduced an improved technique for feature extraction from text using the concept of embedding or placing the word into a vector space based on context. This approach to word embedding, called Word2Vec, solves the problem of representing contextual word relationships in a computable feature space. Building on these ideas, J. Pennington et. al. in 2014 <ref type="bibr" target="#b40">[41]</ref> developed a learning vector space representation of the words called Glove and deployed it in Stanford NLP lab. The RMDL approach described in this paper uses Glove for feature extraction from textual data.</p><p>Classification Methods and Techniques: Over the last 50 years, many supervised learning classification techniques have been developed and implemented in software to accurately label data. For example, the researchers, K. Murphy in 2006 <ref type="bibr" target="#b37">[38]</ref> and I. Rish in 2001 <ref type="bibr" target="#b41">[42]</ref> introduced the Naïve Bayes Classifier (NBC) as a simple approach to the more general respresentation of the supervised learning classification problem. This approach has provided a useful technique for text classification and information retrieval applications. As with most supervised learning classification techniques, NBC takes an input vector of numeric or categorical data values and produce the probability for each possible output labels. This approach is fast and efficient for text classification, but NBC has important limitations. Namely, the order of the sequences in text is not reflected on the output probability because for text analysis, naïve bayes uses a bag of words approach for feature extraction. Because of its popularity, this paper uses NBC as one of the baseline methods for comparison with RMDL. Another popular classification technique is Support Vector Machines (SVM), which has proven quite accurate over a wide variety of data. This technique constructs a set of hyper-planes in a transformed feature space. This transformation is not performed explicitly but rather through the kernal trick which allows the SVM classifier to perform well with highly nonlinear relationships between the predictor and response variables in the data. A variety of approaches have been developed to further extend the basic methodology and obtain greater accuracy. C. <ref type="bibr">Yu et. al. in 2009 [54]</ref> introduced latent variables into the discriminative model as a new structure for SVM, and S. <ref type="bibr">Tong et. al. in 2001 [50]</ref> added active learning using SVM for text classification. For a large volume of data and datasets with a huge number of features (such as text), SVM implementations are computationally complex. Another technique that helps mediate the computational complexity of the SVM for classification tasks is stochastic gradient descent classifier (SGDClassifier) <ref type="bibr" target="#b17">[18]</ref> which has been widely used in both text and image classification. SGDClassifier is an iterative model for large datasets. The model is trained based on the SGD optimizer iteratively.</p><p>Deep Learning: Neural networks derive their architecture as a relatively simply representation of the neurons in the human's brain. They are essentially weighte combinations of inputs the pass through multiple non-linear functions. Neural networks use an iterative learning method known as back-propagation and an optimizer (such as stochastic gradient descent (SGD)).</p><p>Deep Neural Networks (DNN) are based on simple neural networks architectures but they contain multiple hidden layers. These networks have been widely used for classification. For example, D.</p><p>CireşAn et. al. in 2012 <ref type="bibr" target="#b9">[10]</ref> used multi-column deep neural networks for classification tasks, where multi-column deep neural networks use DNN architectures. Convolutional Neural Networks (CNN) provide a different architectural approach to learning with neural networks. The main idea of CNN is to use feed-forward networks with convolutional layers that include local and global pooling layers. A. Krizhevsky in 2012 <ref type="bibr" target="#b24">[25]</ref> used CNN, but they have used 2D convolutional layers combined with the 2D feature space of the image. Another example of CNN in <ref type="bibr" target="#b27">[28]</ref> showed excellent accuracy for image classification. This architecture can also be used for text classification as shown in the work of <ref type="bibr" target="#b20">[21]</ref>. For text and sequences, 1D convolutional layers are used with word embeddings as the input feature space. The final type of deep learning architecture is Recurrent Neural Networks (RNN) where outputs from the neurons are fed back into the network as inputs for the next step. Some recent extensions to this architecture uses Gated Recurrent Units (GRUs) <ref type="bibr" target="#b8">[9]</ref> or Long Short-Term Memory (LSTM) units <ref type="bibr" target="#b14">[15]</ref>. These new units help control for instability problems in the original network architecure. RNN have been successfully used for natural language processing <ref type="bibr" target="#b36">[37]</ref>. Recently, Z. Yang et. al. in 2016 <ref type="bibr" target="#b52">[53]</ref> developed hierarchical attention networks for document classification. These networks have two important characteristics: hierarchical structure and an attention mechanism at word and sentence level.</p><p>New work has combined these three basic models of the deep learning structure and developed a novel technique for enhancing accuracy and robustness.  <ref type="bibr" target="#b22">[23]</ref> introduced hierarchical deep learning for text classification (HDLTex) which is a combination of all deep learning techniques in a hierarchical structure for document classification has improved accuracy over traditional methods. The work in this paper builds on these ideas, spcifically the work of <ref type="bibr" target="#b22">[23]</ref> to provide a more general approach to supervised learning for classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">BASELINE</head><p>In this paper, we use both contemporary and traditional techniques of document and image classification as our baselines. The baselines of image and text classification are different due to feature extraction and structure of model; thus, text and image classification's baselines are described separately in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Text Classification Baselines</head><p>Text classification techniques which are used as our baselines to evaluate our model are as follows: regular deep models such as Recurrent Neural Networks (RNN), Convolutional Neural Networks (CNN), and Deep Neural Networks (DNN). Also, we have used two different techniques of Support Vector Machine (SVM), naïve bayes classification (NBC), and finally Hierarchical Deep Learning for Text Classification (HDLTex) <ref type="bibr" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Deep</head><p>Learning. The baseline, we used in this paper is Deep Learning without hierarchical levels. An example of hierarchical levels' structure is <ref type="bibr" target="#b52">[53]</ref> that has been used as one of our baselines for text classification. In our methods' Section 4, we will explain the basic models of deep learning such as DNN, CNN, and RNN which are used as part of RMDL model. <ref type="bibr" target="#b5">[6]</ref> in 1963. The early 1990s, nonlinear version was addressed in <ref type="bibr" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Support Vector Machine (SVM). The original version of SVM was introduced by Vapnik, VN and Chervonenkis, A Ya</head><p>Multi-class SVM. The original version of SVM is used for binary classification, so for multi class we need to generate Multimodel or MSVM. One-Vs-One is a technique for multi-class SVM and needs to build N(N-1) classifiers.</p><p>The natural way to solve k-class problem is to construct a decision function of all k classes at once <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b51">52]</ref>. Another technique of multi-class classification using SVM is All-against-One. In SVM, many different methods are available for feature extraction such as word sequences feature extracting <ref type="bibr" target="#b54">[55]</ref>, and Term frequencyinverse document frequency (TF-IDF).</p><p>String Kernel. The basic idea of String Kernel (SK) is using Φ(.) for mapping string in the feature space; therefore, the only different between the three techniques are the way they map the string into feature space. For many applications such as text, DNA, and protein classification, Spectrum Kernel (SP) is addressed <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b31">32]</ref>. The basic idea of SP is counting number of time a word appears in string x i as feature map where defining feature maps from x → R l k Mismatch Kernel is the other stable way to map the string into feature space. The key idea is using k which stands for k − mer or size of the word and allow to have m mismatch in feature space <ref type="bibr" target="#b30">[31]</ref>. The main problem of SVM for string sequences is time complexity of these models. S. Ritambhara et. al. in 2017 <ref type="bibr" target="#b46">[47]</ref> addressed the problem of time for gap k-mers kernel called GaKCo which is used only for protein and DNA sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Stacking Support Vector Machine (SVM).</head><p>Stacking SVMs is used as another baseline method for comparison with RMDL, but this technique is used only for hierarchical labeled datasets. The stacking SVM provides an ensemble of individual SVM classifiers and generally produces more accurate results than single-SVM models <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b47">48]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">Naïve Bayes Classification (NBC)</head><p>. This technique has been used in industry and academia for a long time, and it is the most traditional method of text categorization which is widely used in Information Retrieval <ref type="bibr" target="#b34">[35]</ref>. If the number of n documents, fit into k categories, the predicted class as output is c ∈ C. Naïve bayes is a simple algorithm using naïve bayes rule described as follows:</p><formula xml:id="formula_0">P(c | d) = P(d | c)P(c) P(d)<label>(1)</label></formula><p>where d is document, c indicates classes.</p><formula xml:id="formula_1">C MAP = arд max c ∈C P(d | c)P(c) = arд max c ∈C P(x 1 , x 2 , ..., x n | c)p(c)<label>(2)</label></formula><p>The baseline of this paper is word level of NBC <ref type="bibr" target="#b19">[20]</ref> as follows:</p><formula xml:id="formula_2">P(c j | d i ;θ ) = P(c j |θ )P(d i | c j ;θ j ) P(d i |θ )<label>(3)</label></formula><p>3.1.5 Hierarchical Deep Learning for Text Classification (HDL-Tex). This technique is used as one of our baselines for hierarchical labeled datasets. When documents are organized hierarchically, multi-class approaches are difficult to apply using traditional supervised learning methods. The HDLTex <ref type="bibr" target="#b22">[23]</ref> introduced a new approach to hierarchical document classification that combines multiple deep learning approaches to produce hierarchical classification. The primary contribution of HDLTex research is hierarchical classification of documents. A traditional multi-class classification technique can work well for a limited number of classes, but performance drops with increasing number of classes, as is present in hierarchically organized documents. HDLTex solved this problem by creating architectures that specialize deep learning approaches for their level of the document hierarchy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Image Classification Baselines</head><p>For image classification, we have five baselines as follows: Deep L2-SVM <ref type="bibr" target="#b48">[49]</ref>, Maxout Network <ref type="bibr" target="#b13">[14]</ref>, BinaryConnect <ref type="bibr" target="#b10">[11]</ref>, PCANet-1 <ref type="bibr" target="#b3">[4]</ref>, and gcForest <ref type="bibr" target="#b55">[56]</ref>. Deep L2-SVM: This technique is known as deep learning using linear support vector machines which simply softmax is replaced with linear SVMs <ref type="bibr" target="#b48">[49]</ref>. Maxout Network: I. Goodfellow et. al. in 2013 <ref type="bibr" target="#b13">[14]</ref> defined a simple novel model called maxout (named because its outputs' layer is a set of max of inputs' layer, and it is a natural companion to dropout). Their design both facilitates optimization by using dropout, and also improves the accuracy of dropout's model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">METHOD</head><p>The novelty of this work is in using multi random deep learning models including DNN, RNN, and CNN techniques for text and image classification. The method section of this paper is organized as follows: first we describe RMDL and we discuss three techniques of deep learning architectures (DNN, RNN, and CNN) which are trained in parallel. Next, we talk about multi optimizer techniques that are used in different random models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Feature Extraction and Data Pre-processing</head><p>The feature extraction is divided into two main parts for RMDL (Text and image). Text and sequential datasets are unstructured data, while the feature space is structured for image datasets.</p><p>4.1.1 Image and 3D Object Feature Extraction. Image features are the followings: h × w × c where h denotes the height of the image, w represents the width of image, and c is the color that has 3 dimensions (RGB). For gray scale datasets such as MN IST dataset, the feature space is h × w. A 3D object in space contains n cloud points in space and each cloud point has 6 features which are (x, y, z, R, G, and B). The 3D object is unstructured due to number of cloud points since one object could be different with others. However, we could use simple instance down/up sampling to generate the structured datasets.</p><p>4.1.2 Text and Sequences Feature Extraction. In this paper we use several techniques of text feature extraction which are word embedding (GloVe and Word2vec) and also TF-IDF. In this paper, we use word vectorization techniques <ref type="bibr" target="#b15">[16]</ref> for extracting features; Besides, we also can use N-gram representation as features for neural deep learning <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b18">19]</ref>. For example, feature extraction in this model for the string "In this paper we introduced this technique" would be composed of the following:</p><p>• Feature count(1) { (In 1) , (this 2), (paper 1), (we 1), (introduced 1), (technique 1) } • Feature count(2) { (In 1) , (this 2), (paper 1), (we 1), (introduced 1), (technique 1), (In this 1), (This Paper 1), ( paper we 1), ( we introduced 1), (introduced this 1), ( this technique 1) } Documents enter our models via features extracted from the text. We employed different feature extraction approaches for the deep learning architectures we built. For CNN and RNN, we used the text vector-space models using 200 dimensions as described in GloVe <ref type="bibr" target="#b40">[41]</ref>. A vector-space model is a mathematical mapping of the word space, defined as follows:</p><formula xml:id="formula_3">d j = (w 1, j , w 2, j , ..., w i, j ..., w l j , j )<label>(4)</label></formula><p>where l j is the length of the document j, and w i, j is the GloVe word embedding vectorization of word i in document j.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Random Multimodel Deep Learning</head><p>Random Multimodel Deep Learning is a novel technique that we can use in any kind of dataset for classification. An overview of this technique is shown in <ref type="figure">Figure 2</ref>  </p><formula xml:id="formula_4">M(y i1 , y i2 , ..., y in ) = 1 2 + ( n j=1 y i j ) − 1 2 n<label>(5)</label></formula><p>Where n is the number of random models, and y i j is the output prediction of model for data point i in model j  vote for finalŷ i . Therefore,ŷ i is given as follows:</p><formula xml:id="formula_5">y i = ŷ i1 . . .ŷ i j . . .ŷ in T (6)</formula><p>Where n is number of random model, andŷ i j shows the prediction of label of document or data point of D i ∈ {x i , y i } for model j andŷ i, j is defined as follows:</p><formula xml:id="formula_6">y i, j = arд max k [so f tmax(y * i, j )]<label>(7)</label></formula><p>After all RDL models (RMDL) are trained, the final prediction is calculated using majority vote of these models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Deep Learning in RMDL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Deep Neural Networks.</head><p>Deep Neural Networks' structure is designed to learn by multi connection of layers that each layer only receives connection from previous and provides connections only to the next layer in hidden part. The input is a connection of feature space with first hidden layer for all random models. The output layer is number of classes for multi-class classification and only one output for binary classification. But our main contribution of this paper is that we have many training DNN for different purposes. In our techniques, we have multi-classes DNNs where each learning models is generated randomly (number of nodes in each layer and also number of layers are completely random assigned). Our implementation of Deep Neural Networks (DNN) is discriminative trained model that uses standard back-propagation algorithm using sigmoid (equation 8), ReLU <ref type="bibr" target="#b38">[39]</ref>  </p><formula xml:id="formula_7">f (x) = max(0, x) (9) σ (z) j = e z j K k =1 e z k<label>(8)</label></formula><p>∀ j ∈{1, . . . , K } Given a set of example pairs (x, y), x ∈ X , y ∈ Y , the goal is to learn from these input and target space using hidden layers. In text classification, the input is string which is generated by vectorization of text. In <ref type="figure">Figure 2</ref> the left model shows how DNN contribute in RMDL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Recurrent Neural Networks (RNN).</head><p>Another neural network architecture that contributes in RMDL is Recurrent Neural Networks (RNN). RNN assigns more weights to the previous data points of sequence. Therefore, this technique is a powerful method for text, string and sequential data classification but also could be used for image classification as we did in this work. In RNN the neural net considers the information of previous nodes in a very sophisticated method which allows for better semantic analysis of structures of dataset. General formulation of this concept is given in <ref type="figure" target="#fig_1">Equation 11</ref> where x t is the state at time t and u t refers to the input at step t.</p><formula xml:id="formula_9">x t = F (x t −1 , u t , θ )<label>(11)</label></formula><p>More specifically, we can use weights to formulate the Equation 11 with specified parameters in Equation 12</p><formula xml:id="formula_10">x t = W rec σ (x t −1 ) + W in u t + b<label>(12)</label></formula><p>Where W rec refers to recurrent matrix weight, W in refers to input weights, b is the bias and σ denotes an element-wise function. Again, we have modified the basic architecture for use RMDL. preserve long term dependency in a more effective way in comparison to the basic RNN. This is particularly useful to overcome vanishing gradient problem <ref type="bibr" target="#b39">[40]</ref>. Although LSTM has a chain-like structure similar to RNN, LSTM uses multiple gates to carefully regulate the amount of information that will be allowed into each node state. <ref type="figure">Figure 3</ref> shows the basic cell of a LSTM model. A step by step explanation of a LSTM cell is as following:  <ref type="figure">Figure 3</ref> shows the structure of these gates with a graphical representation. An RNN can be biased when later words are more influential than the earlier ones. To overcome this bias Convolutional Neural Network (CNN) models (discussed in Subsection 4.3.3 were introduced which deploys a max-pooling layer to determine discriminative phrases in a text <ref type="bibr" target="#b26">[27]</ref>.</p><formula xml:id="formula_11">i t =σ (W i [x t , h t −1 ] + b i ),<label>(13)</label></formula><formula xml:id="formula_12">C t = tanh(W c [x t , h t −1 ] + b c ), (14) f t =σ (W f [x t , h t −1 ] + b f ),<label>(15)</label></formula><formula xml:id="formula_13">C t =i t * C t + f t C t −1 , (16) o t =σ (W o [x t , h t −1 ] + b o ), (17) h t =o t tanh(C t ),<label>(18)</label></formula><p>Gated Recurrent Unit (GRU): Gated Recurrent Unit (GRU) is a gating mechanism for RNN which was introduced by [9] and <ref type="bibr" target="#b6">[7]</ref>. GRU is a simplified variant of the LSTM architecture, but there are differences as follows: GRU contains two gates, a GRU does not possess internal memory (the C t −1 in <ref type="figure">Figure 3)</ref>; and finally, a second non-linearity is not applied (tanh in <ref type="figure">Figure 3</ref>). A step by step explanation of a GRU cell is as following:</p><formula xml:id="formula_14">z t = σ д (W z x t + U z h t −1 + b z ),<label>(19)</label></formula><p>Where z t refers to update gate vector of t, x t stands for input vector, W , U and b are parameter matrices and vector, σ д is activation function that could be sigmoid or ReLU.</p><formula xml:id="formula_15">r t = σ д (W r x t + U r h t −1 + b r ),<label>(20)</label></formula><formula xml:id="formula_16">h t = z t • h t −1 + (1 − z t ) • σ h (W h x t + U h (r t • h t −1 ) + b h ) (21)</formula><p>Where h t is output vector of t, r t stands for reset gate vector of t, z t is update gate vector of t, σ h indicates the hyperbolic tangent function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Convolutional Neural Networks (CNN). The final deep learning approach which contributes in RMDL is Convolutional</head><p>Neural Networks (CNN) that is employed for document or image classification. Although originally built for image processing with architecture similar to the visual cortex, CNN have also been effectively used for text classification <ref type="bibr" target="#b28">[29]</ref>; thus, in RMDL, this technique is used in all datasets. In the basic CNN for image processing an image tensor is convolved with a set of kernels of size d × d. These convolution layers are called feature maps and can be stacked to provide multiple filters on the input. To reduce the computational complexity CNN use pooling which reduces the size of the output from one layer to the next in the network. Different pooling techniques are used to reduce outputs while preserving important features <ref type="bibr" target="#b44">[45]</ref>. The most common pooling method is max pooling where the maximum element is selected in the pooling window. In order to feed the pooled output from stacked featured maps to the final layer, the maps are flattened into one column. The final layers in a CNN are typically fully connected. In general, during the back propagation step of a convolutional neural network not only the weights are adjusted but also the feature detector filters. A potential problem of CNN used for text is the number of 'channels', Σ (size of the feature space). This might be very large (e.g. 50K), for text but for images this is less of a problem (e.g. only 3 channels of RGB) <ref type="bibr" target="#b16">[17]</ref>. This means the dimensionality of the CNN for text is very high.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Optimization</head><p>In this paper we use two types of stochastic gradient optimizer in our neural networks implementation which are RMSProp and Adam optimizer:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Stochastic Gradient Descent (SGD)</head><p>Optimizer. SGD has been used as one of our optimizer that is shown in equation 22. It uses a momentum on re-scaled gradient which is shown in equation 23 for updating parameters. The other technique of optimizer that is used is RMSProp which does not do bias correction. This will be a significant problem while dealing with sparse gradient.  <ref type="bibr" target="#b21">[22]</ref>.</p><formula xml:id="formula_17">θ ← θ − α ∇ θ J (θ, x i , y i ) (22) θ ← θ − γθ + α ∇ θ J (θ, x i , y i )<label>(23)</label></formula><formula xml:id="formula_18">θ ← θ − α √v + ϵm (24) д i,t = ∇ θ J (θ i , x i , y i ) (25) m t = β 1 m t −1 + (1 − β 1 )д i,t<label>(26)</label></formula><formula xml:id="formula_19">m t = β 2 v t −1 + (1 − β 2 )д 2 i,t<label>(27)</label></formula><p>Where m t is the first moment and v t indicates second moment that both are estimated.m t = m t</p><formula xml:id="formula_20">1−β t 1 andv t = v t 1−β t 2 4.4.</formula><p>3 Multi Optimization rule. The main idea of using multi model with different optimizers is that if one optimizer does not provide a good fit for a specific datasets, the RMDL model with n random models (some of them might use different optimizers) could ignore k models which are not efficient if and only if n &gt; k. The <ref type="figure">Figure 4</ref> provides a visual insight on how three optimizers work better in the concept of majority voting. Using multi techniques of optimizers such as SGD, adam, RMSProp, Adagrad, Adamax, and so on helps the RMDL model to be more stable for any type of datasets. In this research, we only used two optimizers (Adam and RMSProp) for evaluating our model, but the RMDL model has the capability to use any kind of optimizer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTAL RESULTS</head><p>In this section, experimental results are discussed including evaluation of method, experimental setup, and datasets. Also, we discuss the hardware and frameworks which are used in RMDL; finally, a comparison between our empirical results and the baselines has been presented. Moreover, losses and accuracies of this model for each individual RDL (in each epoch) is shown in <ref type="figure" target="#fig_6">Figure 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Evaluation</head><p>In this work, we report accuracy and Micro F1-Score which are given as follows:</p><formula xml:id="formula_21">Precision micr o = L l =1 T P l L l =1 T P l + F P l (28) Recall micr o = L l =1 T P l L l =1 T P l + F N l (29) F 1 − Score micr o = L l =1 2T P l L l =1 2T P l + F P l + F N l<label>(30)</label></formula><p>However, the performance of our model is evaluated only in terms of F1-score for evaluation as in <ref type="table" target="#tab_4">Tables 1 and 3</ref>. Formally, given I = {1, 2, · · · , k } a set of indices, we define the i t h class as C i . If we denote l = |I | and for T P i -true positive of C i , F P i -false positive, F N ifalse negative, and T N i -true negative counts respectively then the above definitions apply for our multi-class classification problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental Setup</head><p>Two types of datasets (text and image) has been used to test and evaluate our approach performance. However, in theory the model has capability to solve classification problems with a variety of data including video, text, and images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Text Datasets.</head><p>For text classification, we used 4 different datasets, namely, W OS , Reuters, IMDB, and 20newsдroups. Web Of Science (WOS) dataset <ref type="bibr" target="#b23">[24]</ref> is a collection of academic articles' abstracts which contains three corpora (5736, 11967, and 46985 documents) for <ref type="bibr" target="#b10">(11,</ref><ref type="bibr" target="#b33">34</ref>, and 134 topics). The Reuters-21578 news dataset contains 10, 788 documents which are divided into 7, 769 documents for training and 3, 019 for testing with total of 90 classes. IMDB dataset contains 50, 000 reviews that is splitted into a set of 25, 000 highly popular movie reviews for training, and 25, 000 for testing. 20NewsGroup dataset includes 19, 997 documents with maximum length of 1, 000 words. In this dataset, we have 15, 997 for training and 4, 000 samples are used for validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Image datasets.</head><p>For image classification, two traditional and ground truth datasets are used, namely, MNIST hand writing dataset and CIFAR. MNIST: this dataset contains handwritten number k ∈ {0, 1, ..., 9} and input feature space is in 28 × 28 × 1 format. The training and the test set contains 60, 000 and 10, 000 data point examples respectively. CIFAR: This dataset consists of 60, 000 images with 32×32×3 format assigned in 10 classes, with 6, 000 images per class that is splitted into 50, 000 training and 10, 000 test images. Classes are airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Hardware</head><p>All of the results shown in this paper are performed on Central Process Units (CPU) and Graphical Process Units (GPU). Also, RMDL can be implemented using only GPU, CPU, or both. The processing units that has been used through this experiment was intel on Xeon E5-2640 (2.6 GHz) with 12 cores and 64 GB memory (DDR3). Also, we have used three graphical cards on our machine which are two Nvidia GeForce GTX 1080 Ti and Nvidia Tesla K20c.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Framework</head><p>This work is implemented in Python using Compute Unified Device Architecture (CUDA) which is a parallel computing platform and Application Programming Interface (API) model created by Nvidia.</p><p>We used T ensor Felow and Keras library for creating the neural networks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Empirical Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.5.1</head><p>Image classification. <ref type="table" target="#tab_5">Table 2</ref> shows the error rate of RMDL for image classification. The comparison between the RMDL with baselines (as described in Section 3.2), shows that the error rate of the RMDL for MNIST dataset has been improved to 0.51, 0.41, and 0.21 for 3, 9 and 15 random models respectively. For the CIFAR-10 datasets, the error rate has been decreased for RMDL to 9.89, 9.1, 8.74, and 8.79,using 3, 9, 15, and 30 RDL respectively. <ref type="table" target="#tab_4">Table 1</ref> shows that for four ground truth datasets, RMDL improved the accuracy in comparison to the baselines. In <ref type="table" target="#tab_4">Table 1</ref>  TF-IDF is equal to 88.45%. The accuracy of 20NewsGroup dataset is 86.73%, 87.62%, and 87.91% for 3, 9, and 15 random models respectively, whereas the accuracy of DNN is 86.50%, CNN <ref type="bibr" target="#b52">[53]</ref> is 82.91%, RNN <ref type="bibr" target="#b52">[53]</ref> is 83.75%, Naïve Bayes Classifier is 81.67%, SVM <ref type="bibr" target="#b54">[55]</ref> is 84.57%, and SVM <ref type="bibr" target="#b4">[5]</ref> using TF-IDF is equal to 86.00%. <ref type="figure" target="#fig_6">Figure 5</ref> indicates accuracies and losses of RMDL which are shown with 9 (RDLs) for text classification and 15 RDLs for image classification. As shown in <ref type="figure" target="#fig_6">Figure 5a</ref>, 4 RDLs' loss of MNIST dataset are increasing over each epoch (RDL 6, RDL 9, RDL 14 and RDL 15) after 40 epochs, but RMDL model contains 15 RDL models; thus, the accuracy of the majority votes for these models as presented in <ref type="table" target="#tab_5">Table 2</ref> is competing with our baselines. In <ref type="figure" target="#fig_6">Figure 5a</ref>, for CIFAR dataset, the models do not have overfitting problem, but for MNIST datasets at least 4 models' losses are increasing over each epoch after 40 iterations (RDL 4, RDL 5, RDL 6, and RDL 9); although the accuracy and F1-measure of these 4 models will drop after 40 epochs, the majority votes' accuracy is robust and efficient which means RMDL will ignore them due to majority votes between 15 models. The <ref type="figure" target="#fig_6">Figure 5a</ref> shows the loss value over each epoch of two ground truth datasets, CIFAR and IMDB for 15 random deep learning models (RDL). <ref type="figure" target="#fig_6">Figure 5b</ref> presents the accuracy of 15 random models for Reuters-21578 respectively. In <ref type="figure" target="#fig_6">Figure 5b</ref>, the accuracy of Random Deep Learning (RDLs) model is addressed over each epoch for WOS-5736 (Web Of Science dataset with 17 categories and 5, 736 documents), the majority votes of these models as shown in <ref type="table" target="#tab_4">Table 1</ref> is competing with our baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.2">Document categorization.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSION AND CONCLUSION</head><p>The classification task is an important problem to address in machine learning, given the growing number and size of datasets that need sophisticated classification. We propose a novel technique to solve the problem of choosing best technique and method out of many possible structures and architectures in deep learning. This paper introduces a new approach called RMDL (Random Multimodel Deep Learning) for the classification that combines multi deep learning approaches to produce random classification models. Our evaluation on datasets obtained from the Web of Science (WOS), Reuters, MNIST, CIFAR, IMDB, and 20NewsGroups shows that combinations of DNNs, RNNs and CNNs with the parallel learning architecture, has consistently higher accuracy than those obtained by conventional approaches using naïve Bayes, SVM, or single deep learning model. These results show that deep learning methods can provide improvements for classification and that they provide flexibility to classify datasets by using majority vote.</p><p>The proposed approach has the ability to improve accuracy and efficiency of models and can be use across a wide range of data types and applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>BinaryConnect: M. Courbariaux et. al. in 2015 [11] worked on training Deep Neural Networks (DNN) with binary weights during propagations. They have introduced a binarization scheme for binary weights during forward and backward propagations (BinaryConnect) which is mainly used for image classification. BinaryConnect is used as our baseline for RMDL on image classification. PCANet: I. Chan et. al. in 2015<ref type="bibr" target="#b3">[4]</ref> is simple way of deep learning for image classification which uses CNN structure. Their technique is one of the basic and efficient methods of deep learning. The CNN structure they've used, is part of RMDL with significant differences that they use: I) cascaded principal component analysis (PCA); II) binary hashing; and III) blockwise histograms, and also number of hidden layers and nodes in RMDL is selected automatically. gcForest (Deep Forest): Z. Zhou et. al. in 2017<ref type="bibr" target="#b55">[56]</ref> introduced a decision tree ensemble approach with high performance as an alternative to deep neural networks. Deep forest creates multi level of forests as decision trees.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Overview of RDML: Random Multimodel Deep Learning for classification that includes n Random models which are d random model of DNN classifiers, c models of CNN classifiers, and r RNN classifiers where r + c + d = n.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>The RMDL model structure (section 4 . 2 )</head><label>42</label><figDesc>includes three basic architectures of deep learning in parallel. We describe each individual model separately. The final model contains d random DNNs (Section 4.3.1), r RNNs (Section 4.3.2), and c CNNs models (Section 4.3.3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(equation 9) as activation function. The output layer for multi-class classification, should use So f tmax equation 10. f (x) = 1 1 + e −x ∈ (0, 1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 Figure 2 :</head><label>22</label><figDesc>left side shows this extended RNN architecture. Several problems arise from RNN when the error of the gradient descent algorithm is back propagated through the network: vanishing gradient and exploding gradient<ref type="bibr" target="#b1">[2]</ref>. Long Short-Term Memory (LSTM): To deal with these problems Long Short-Term Memory (LSTM) is a special type of RNN that Random Multimodel Deep Learning (RDML) architecture for classification which includes 3 Random models, a DNN classifier at left, a Deep CNN classifier at middle, and a Deep RNN classifier at right (each unit could be LSTM or GRU).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>4. 4 . 2 Figure 3 :Figure 4 :</head><label>4234</label><figDesc>Adam Optimizer. Adam is another stochastic gradient optimizer which uses only the first two moments of gradient (v and m that are shown in equation 24, 25, 26, and 27) and average over them. It can handle non-stationary of objective function as in RMSProp while overcoming the sparse gradient issue that was a tanh Top Figure is a cell of GRU, and bottom Figure is a cell of LSTM This figure Shows multi SGD optimizer drawback in RMSProp</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>This sub-figure indicates MNIST and CIFAR-10 loss function for 15 Random Deep Learning (RDL) model. The MNNST shown as 120 epoch and CIFAR has 200 epoch This sub-figure indicates WOS-5736 (Web Of Science dataset with 11 categories and 5736 documents) accuracy function for 9 Random Deep Learning (RDL) model, and bottom figure indicates Reuters-21578 accuracy function for 9 Random Deep Learning (RDL) model This figure shows results of individual RDLs (accuracy and loss) for each epoch as part of RMDL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The work of M. Turan et. al. in 2017 [51] and M. Liang et. al.in 2015 [33] implemented innovative combinations of CNN and RNN called A Recurrent Convolutional Neural Network (RCNN). K. Kowsari et. al. in 2017</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>which contains multi Deep Neural Networks (DNN), Deep Convolutional Neural Networks (CNN), and Deep Recurrent Neural Networks (RNN). The number of layers and nodes for all of these Deep learning multi models are generated randomly (e.g. 9 Random Models in RMDL constructed of 3 CNNs, 3 RNNs, and 3 DNNs, all of them are unique due to randomly creation).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Where equation 13 is input gate, Equation 14 shows candid memory cell value, Equation 15 is forget gate activation, Equation 16 is new memory cell value, and Equation 17 and 18 show output gate value. In the above description all b represents bias vectors and all W represent weight matrices and x t is used as input to the memory cell at time t. Also, i, c, f , o indices refer to input, cell memory, forget and output gates respectively.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc>Accuracy comparison for text classification.</figDesc><table><row><cell cols="6">W.1 (WOS-5736) refers to Web of Science dataset, W.2</cell></row><row><cell cols="6">represents W-11967, W.3 is WOS-46985, and R stands for</cell></row><row><cell>Reuters-21578</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Model</cell><cell>W.1</cell><cell cols="2">Dataset W.2 W.3</cell><cell>R</cell></row><row><cell></cell><cell>DNN</cell><cell cols="3">86.15 80.02 66.95</cell><cell>85.3</cell></row><row><cell></cell><cell>CNN [53]</cell><cell cols="3">88.68 83.29 70.46</cell><cell>86.3</cell></row><row><cell></cell><cell>RNN [53]</cell><cell cols="3">89.46 83.96 72.12</cell><cell>88.4</cell></row><row><cell></cell><cell>NBC</cell><cell>78.14</cell><cell>68.8</cell><cell>46.2</cell><cell>83.6</cell></row><row><cell>Baseline</cell><cell>SVM [55]</cell><cell cols="3">85.54 80.65 67.56</cell><cell>86.9</cell></row><row><cell cols="2">SVM (TF-IDF) [5]</cell><cell cols="4">88.24 83.16 70.22 88.93</cell></row><row><cell cols="5">Stacking SVM [48] 85.68 79.45 71.81</cell><cell>NA</cell></row><row><cell cols="2">HDLTex [23]</cell><cell cols="3">90.42 86.07 76.58</cell><cell>NA</cell></row><row><cell></cell><cell>3 RDLs</cell><cell cols="4">90.86 87.39 78.39 89.10</cell></row><row><cell>RMDL</cell><cell>9 RDLs</cell><cell cols="4">92.60 90.65 81.92 90.36</cell></row><row><cell></cell><cell>15 RDLs</cell><cell cols="4">92.66 91.01 81.86 89.91</cell></row><row><cell></cell><cell>30 RDLs</cell><cell cols="4">93.57 91.59 82.42 90.69</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Error rate comparison for Image classification (MNIST and CIFAR-10 datasets)</figDesc><table><row><cell></cell><cell>Methods</cell><cell cols="2">MNIST CIFAR-10</cell></row><row><cell></cell><cell>Deep L2-SVM [49]</cell><cell>0.87</cell><cell>11.9</cell></row><row><cell></cell><cell>Maxout Network [14]</cell><cell>0.94</cell><cell>11.68</cell></row><row><cell>Baseline</cell><cell>BinaryConnect [11]</cell><cell>1.29</cell><cell>9.90</cell></row><row><cell></cell><cell>PCANet-1 [4]</cell><cell>0.62</cell><cell>21.33</cell></row><row><cell></cell><cell>gcForest [56]</cell><cell>0.74</cell><cell>31.00</cell></row><row><cell></cell><cell>3 RDLs</cell><cell>0.51</cell><cell>9.89</cell></row><row><cell>RMDL</cell><cell>9 RDLs 15 RDLs</cell><cell>0.41 0.21</cell><cell>9.1 8.74</cell></row><row><cell></cell><cell>30 RDLs</cell><cell>0.18</cell><cell>8.79</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>, we evaluated our empirical results by four different RMDL models (using<ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b14">15,</ref> and 30 RDLs). For Web of Science (WOS-5,736) the accuracy is improved to 90.86, 92.60, 92.66, and 93.57 respectively. For Web of Science (WOS-11,967), the accuracy is increased to 87.39, 90.65, 91.01, and 91.59 respectively, and for Web of Science (WOS-46,985) the accuracy has increased to 78.39, 81.92, 81.86, and 82.42 respectively. The accuracy of Reuters-21578 is 88.95, 90.29, 89.91, and 90.69 respectively. We report results for other ground truth datasets such as Large Movie Review Dataset (IMDB) and 20NewsGroups. As it is mentioned inTable 3, for two ground truth datasets, RMDL improves the accuracy. InTable 3, we evaluated our empirical results of two datasets (IMDB reviewer and 20NewsGroups).The accuracy of IMDB dataset is 89.91, 90.13, and 90.79 for 3, 9, and 15 RDLs respectively, whereas the accuracy of DNN is 88.55%, CNN<ref type="bibr" target="#b52">[53]</ref> is 87.44%, RNN<ref type="bibr" target="#b52">[53]</ref> is 88.59%, Naïve Bayes Classifier is 83.19%, SVM<ref type="bibr" target="#b54">[55]</ref> is 87.97%, and SVM<ref type="bibr" target="#b4">[5]</ref> using</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Test</cell><cell></cell><cell></cell><cell>Train</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>RDL1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>RDL2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>RDL3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>RDL4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>RDL5</cell></row><row><cell>loss CIFAR</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>RDL6 RDL7 RDL8 RDL9 RDL10 RDL11</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>RDL12</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>RDL13</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>RDL14</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>RDL15</cell></row><row><cell></cell><cell></cell><cell cols="2">epoch</cell><cell></cell><cell></cell><cell>epoch</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>RDL1</cell></row><row><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>RDL2 RDL3</cell></row><row><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>RDL4 RDL5</cell></row><row><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>RDL6 RDL7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>RDL8</cell></row><row><cell>0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>RDL9</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>RDL10</cell></row><row><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>RDL11</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>RDL12</cell></row><row><cell>0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>RDL13</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>RDL14</cell></row><row><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>RDL15</cell></row><row><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.05</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell>20</cell><cell>40</cell><cell>60</cell><cell>80</cell><cell>100</cell><cell>120</cell></row><row><cell></cell><cell></cell><cell></cell><cell>epoch</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Accuracy comparison for text classification on IMDB and 20NewsGroup datasets</figDesc><table><row><cell></cell><cell>Model</cell><cell cols="2">Dataset IMDB 20NewsGroup</cell></row><row><cell></cell><cell>DNN</cell><cell>88.55</cell><cell>86.50</cell></row><row><cell></cell><cell>CNN [53]</cell><cell>87.44</cell><cell>82.91</cell></row><row><cell>Baseline</cell><cell>RNN [53]</cell><cell>88.59</cell><cell>83.75</cell></row><row><cell></cell><cell cols="2">Naïve Bayes Classifier 83.19</cell><cell>81.67</cell></row><row><cell></cell><cell>SVM [55]</cell><cell>87.97</cell><cell>84.57</cell></row><row><cell></cell><cell>SVM(TF-IDF) [5]</cell><cell>88.45</cell><cell>86.00</cell></row><row><cell></cell><cell>3 RDLs</cell><cell>89.91</cell><cell>86.73</cell></row><row><cell>RMDL</cell><cell>9 RDLs</cell><cell>90.13</cell><cell>87.62</cell></row><row><cell></cell><cell>15 RDLs</cell><cell>90.79</cell><cell>87.91</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<title level="m">Matthieu Devin, and others. 2016. Tensorflow: Large-scale machine learning on heterogeneous distributed systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrice</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A training algorithm for optimal margin classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bernhard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><forename type="middle">M</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><forename type="middle">N</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT92. ACM</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="144" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">PCANet: A simple deep learning baseline for image classification?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Tsung-Han Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zinan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="5017" to="5032" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Turning from TF-IDF to TF-IGM for term weighting in text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="245" to="260" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Early history of support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Ya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chervonenkis</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Inference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="13" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Keras: Deep learning library for theano and tensorflow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://keras.io/k" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multicolumn deep neural network for traffic sign classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Cireşan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ueli</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="333" to="338" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Binaryconnect: Training deep neural networks with binary weights during propagations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Pierre</forename><surname>David</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3123" to="3131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mining the peanut gallery: Opinion extraction and semantic classification of product reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kushal</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Pennock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW. ACM</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="519" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mismatch string kernels for SVM protein classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleazar</forename><surname>Eskin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christina</forename><forename type="middle">S</forename><surname>Noble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leslie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1417" to="1424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1302.4389</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">Maxout networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Word vectorization using relations among words for neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hajime</forename><surname>Hotta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanobu</forename><surname>Kittaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masafumi</forename><surname>Hagiwara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Electronics, Information and Systems</title>
		<imprint>
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="page" from="75" to="82" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Effective use of word order for text categorization with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rie</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.1058</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bangla text document categorization using stochastic gradient descent (sgd) classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fasihul</forename><surname>Kabir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabbir</forename><surname>Siddique</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CCIP. IEEE</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
	<note>Mohammed Rokibul Alam Kotwal, and Mohammad Nurul Huda</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">N-grambased author profiles for authorship attribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlado</forename><surname>Kešelj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Cercone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Calvin</forename><surname>Thomas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In PACLING</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="255" to="264" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Some effective techniques for naive bayes text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang-Bum</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung-Soo</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1457" to="1466" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>Hae-Chang Rim, and Sung Hyon Myaeng</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5882</idno>
		<title level="m">Convolutional neural networks for sentence classification</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">HDLTex: Hierarchical Deep Learning for Text Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamran</forename><surname>Kowsari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Donald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mojtaba</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiana</forename><forename type="middle">Jafari</forename><surname>Heidarysafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Meimandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><forename type="middle">E</forename><surname>Gerber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barnes</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICMLA.2017.0-134</idno>
		<ptr target="https://doi.org/10.1109/ICMLA.2017.0-134" />
	</analytic>
	<monogr>
		<title level="m">16th IEEE International Conference on Machine Learning and Applications (ICMLA)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="364" to="371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Web of Science Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamran</forename><surname>Kowsari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Donald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mojtaba</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiana</forename><forename type="middle">Jafari</forename><surname>Heidarysafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Meimandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><forename type="middle">E</forename><surname>Gerber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barnes</surname></persName>
		</author>
		<idno type="DOI">10.17632/9rw3vkcfy4.6</idno>
		<ptr target="https://doi.org/10.17632/9rw3vkcfy4.6" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Letter detection with rapid serial visual presentation: Evidence against word superiority at feature extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Human Perception and Performance</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">657</biblScope>
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Recurrent Convolutional Neural Networks for Text Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">333</biblScope>
			<biblScope unit="page" from="2267" to="2273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Gradientbased learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajesh</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual international conference on machine learning</title>
		<meeting>the 26th annual international conference on machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="609" to="616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Mismatch string kernels for discriminative protein classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleazar</forename><surname>Christina S Leslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adiel</forename><surname>Eskin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Stafford</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Noble</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="467" to="476" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The spectrum kernel: A string kernel for SVM protein classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleazar</forename><surname>Christina S Leslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Stafford</forename><surname>Eskin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Noble</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific symposium on biocomputing</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="566" to="575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural network for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3367" to="3375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A statistical approach to mechanized encoding and searching of literary information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><surname>Peter Luhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Journal of research and development</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="309" to="317" />
			<date type="published" when="1957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Introduction to information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prabhakar</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raghavan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Cambridge university press</publisher>
			<biblScope unit="volume">1</biblScope>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
	<note>Hinrich Schütze, and others</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2010-01" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Naive bayes classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kevin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Murphy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
		<respStmt>
			<orgName>University of British Columbia</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th international conference on machine learning</title>
		<meeting>the 27th international conference on machine learning</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1310" to="1318" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">An empirical study of the naive Bayes classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irina</forename><surname>Rish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI 2001 workshop on empirical methods in artificial intelligence</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="41" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Understanding inverse document frequency: on theoretical arguments for IDF</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of documentation</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="503" to="520" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Term-weighting approaches in automatic text retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Buckley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information processing &amp; management</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="513" to="523" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Evaluation of pooling operations in convolutional architectures for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominik</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Behnke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Neural Networks-ICANN</title>
		<imprint>
			<biblScope unit="volume">2010</biblScope>
			<biblScope unit="page" from="92" to="101" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Machine learning in automated text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM computing surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1" to="47" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Gakco: a fast gapped k-mer string kernel using counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ritambhara</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arshdeep</forename><surname>Sekhon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamran</forename><surname>Kowsari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Lanchantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beilun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjun</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="356" to="373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Hierarchical text classification and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aixin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ee-Peng</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM. IEEE</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="521" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Deep learning using linear support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichuan</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.0239</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Support vector machine active learning with applications to text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="45" to="66" />
			<date type="published" when="2001-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehmet</forename><surname>Turan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasin</forename><surname>Almalioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helder</forename><surname>Araujo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ender</forename><surname>Konukoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Metin</forename><surname>Sitti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.06822</idno>
		<title level="m">Deep EndoVO: A Recurrent Convolutional Neural Network (RCNN) based Visual Odometry Approach for Endoscopic Capsule Robots</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Multi-class support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Watkins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, University of London</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Hierarchical Attention Networks for Document Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hovy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning structural svms with latent variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Nam John</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML. ACM</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1169" to="1176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Text classification based on multi-word with support vector machine. Knowledge-Based Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taketoshi</forename><surname>Yoshida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xijin</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="879" to="886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08835</idno>
		<title level="m">Deep forest: Towards an alternative to deep neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
