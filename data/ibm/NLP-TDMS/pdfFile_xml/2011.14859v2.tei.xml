<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Doubly Stochastic Subspace Clustering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Lim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">René</forename><surname>Vidal</surname></persName>
							<email>rvidal@jhu.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Mathematical Institute for Data Science</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><forename type="middle">D</forename><surname>Haeffele</surname></persName>
							<email>bhaeffele@jhu.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Mathematical Institute for Data Science</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Doubly Stochastic Subspace Clustering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Many state-of-the-art subspace clustering methods follow a two-step process by first constructing an affinity matrix between data points and then applying spectral clustering to this affinity. Most of the research into these methods focuses on the first step of generating the affinity, which often exploits the self-expressive property of linear subspaces, with little consideration typically given to the spectral clustering step that produces the final clustering. Moreover, existing methods often obtain the final affinity that is used in the spectral clustering step by applying ad-hoc or arbitrarily chosen postprocessing steps to the affinity generated by a self-expressive clustering formulation, which can have a significant impact on the overall clustering performance. In this work, we unify these two steps by learning both a selfexpressive representation of the data and an affinity matrix that is well-normalized for spectral clustering. In our proposed models, we constrain the affinity matrix to be doubly stochastic, which results in a principled method for affinity matrix normalization while also exploiting known benefits of doubly stochastic normalization in spectral clustering. We develop a general framework and derive two models: one that jointly learns the self-expressive representation along with the doubly stochastic affinity, and one that sequentially solves for one then the other. Furthermore, we leverage sparsity in the problem to develop a fast activeset method for the sequential solver that enables efficient computation on large datasets. Experiments show that our method achieves state-of-the-art subspace clustering performance on many common datasets in computer vision. arXiv:2011.14859v2 [cs.LG] 19 Apr 2021 1 Note that the l 1 norm Cp + Cq 1 can be replaced by the sum of the entries of Cp + Cq because of the nonnegativity constraints.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Subspace clustering seeks to cluster a set of data points that are approximately drawn from a union of low dimensional linear (or affine) subspaces into clusters, where each linear (or affine) subspace defines a cluster (i.e., every point in a given cluster lies in the same subspace) <ref type="bibr" target="#b40">[41]</ref>. The most common class of subspace clustering algorithms for cluster-ing a set of n data points proceed in two stages: 1) Learning an affinity matrix A ∈ R n×n that defines the similarity between pairs of datapoints; 2) Applying a graph clustering technique, such as spectral clustering <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b38">39]</ref>, to produce the final clustering. In particular, arguably the most popular model for subspace clustering is to construct the affinity matrix by exploiting the 'self-expressive' property of linear (or affine) subspaces, where a point within a given subspace can be represented as a linear combination of other points within the subspace <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b40">41]</ref>. For a dataset X ∈ R d×n of n, d-dimensional data points, this is typically captured by an optimization problem of the form:</p><formula xml:id="formula_0">min C 1 2 X − XC 2 F + λθ(C),<label>(1)</label></formula><p>where the first term captures the self-expressive property, X ≈ XC, and the second term, θ, is some regularization term on C to encourage that a given point is primarily represented by other points from its own subspace and to avoid trivial solutions such as C = I. Once the self-expressive representation C has been learned from <ref type="bibr" target="#b0">(1)</ref>, the final affinity A is then typically constructed by rectifying and symmetrizing C, e.g., A = (|C| + |C |)/2. However, as we detail in Section 2.1 and illustrate in <ref type="figure">Figure</ref> 1, many subspace clustering methods require ad-hoc or unjustified postprocessing procedures on C to work in practice. These postprocessing steps serve to normalize C to produce an affinity A with better spectral clustering performance, but they add numerous arbitrary hyperparameters to the models and receive little mention in the associated papers. Likewise, it is well-established that some form of Laplacian normalization is needed for spectral clustering to be successful <ref type="bibr" target="#b42">[43]</ref>, for which the practitioner again has multiple choices of normalization strategy.</p><p>In this paper, we propose a new subspace clustering framework which explicitly connects the self-expressive step, the affinity normalization, and the spectral clustering step. We develop novel scalable methods for this framework that address issues with and empirically outperform other subspace clustering models. To motivate our proposed models, we first discuss the desired properties of subspace  <ref type="figure" target="#fig_1">Figure 1</ref>. Diagram comparing our DSSC framework with existing methods for self-expressive affinity-based subspace clustering. (Top row) Most existing methods focus on computing the self-expressive matrix C, but they are also are reliant on choices of postprocessing and affinity normalization, which often take the form of ad-hoc procedures that are not well-studied. (Bottom row) Our DSSC models learn a doubly stochastic affinity matrix A ∈ Ωn along with the self-expressive matrix C. The doubly stochastic affinity does not require postprocessing or normalization to be used for spectral clustering and has numerous desirable properties for subspace clustering.</p><p>clustering affinities that one would like for successful spectral clustering. Specifically, we desire affinities A that have the following properties:</p><p>(A1) Well-normalized for spectral clustering. We should better leverage knowledge that A will be input to spectral clustering. Many forms of ad-hoc postprocessing in subspace clustering perform poorly-justified normalization, and there is also a choice of normalization to be made in forming the graph Laplacian <ref type="bibr" target="#b42">[43]</ref>. (C1) Sparsity. Much success in subspace clustering has been found with enforcing sparsity on C <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b52">53]</ref>.</p><p>In particular, many sparsity-enforcing methods are designed to ensure that the nonzero entries of the affinity matrix correspond to pairs of points which belong to the same subspace. This leads to high clustering accuracy, desirable computational properties, and provable theoretical guarantees. (C2) Connectivity. There should be sufficiently many edges in the underlying graph of A so that all points within the same subspace are connected <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b51">52]</ref>. Thus, there is a trade-off between sparsity and connectivity that subspace clustering methods must account for.</p><p>Properties (C1) and (C2) are well studied in the subspace clustering literature, and can be enforced on the self-expressive C, since rectifying and symmetrizing a C with these properties, for example, maintains the properties. However, property (A1) must be enforced on A; thus, (A1) is often neglected in several aspects (see Section 2.1), and is mostly handled by ad-hoc postprocessing methods, if handled at all. In working towards (A1), we first constrain A ≥ 0, since nonnegativity of the affinity is necessary for interpretability and alignment with spectral clustering theory. Beyond this, spectral clustering also benefits from having rows and columns in the affinity matrix of the same scale, and many forms of Laplacian normalization for spectral clustering normalize rows and/or columns to have similar scale <ref type="bibr" target="#b42">[43]</ref>. In particular, one form of normalization that is well established in the spectral clustering literature is to constrain the rows and columns to have unit l 1 norm. Because A is additionally constrained to be nonnegative, this is equivalent to requiring each row sum and column sum of A to be 1, resulting in constraints that restrict the affinities A to be in the convex set of doubly stochastic matrices <ref type="bibr" target="#b16">[17]</ref>:</p><formula xml:id="formula_1">Ω n = {A ∈ R n×n | A ≥ 0, A1 = 1, A 1 = 1}. (2)</formula><p>Doubly stochastic matrices have been thoroughly studied for spectral clustering, and doubly stochastic normalization has been shown to significantly improve the performance of spectral clustering <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b47">48]</ref>. Moreover, doubly stochastic matrices are invariant to the most widely-used types of Laplacian normalization <ref type="bibr" target="#b42">[43]</ref>, which removes the need to choose a Laplacian normalization scheme. Further, the authors of <ref type="bibr" target="#b54">[55]</ref> show that various Laplacian normalization schemes can be viewed as attempting to approximate a given affinity matrix with a doubly stochastic matrix under certain distance metrics.</p><p>Beyond being well-normalized for spectral clustering (A1), the family of doubly stochastic matrices can also satisfy properties (C1) and (C2). Our proposed methods will give control over the sparsity-connectivity trade-off through interpretable parameters, and we in practice learn A that are quite sparse. Additionally, doubly stochastic matrices have a guarantee of a certain level of connectivity due to the row sum constraint, which prohibits solutions with all-zero rows (that can occur in other subspace clustering methods). The convexity of Ω n along with the sparsity of our learned A allow us to develop novel scalable algorithms for doubly stochastic projection and hence scalable methods for subspace clustering with doubly stochastic affinities. Contributions. In this work, we develop a framework that unifies the self-expressive representation step of subspace clustering with the spectral clustering step by formulating a model that jointly solves for a self-expressive representation C and a doubly stochastic affinity matrix A. While our general model is non-convex, we provide a convex relaxation that is provably close to the non-convex model and that can be solved by a type of linearized ADMM <ref type="bibr" target="#b30">[31]</ref>. A closer analysis also allows us to formulate a sequential algorithm to quickly compute an approximate solution, in which we first efficiently learn a self-expressive matrix and then subsequently fit a doubly stochastic matrix by a regularized optimal transport problem <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b27">28]</ref>. We leverage inherent sparsity in the problem to develop a scalable active-set method for computing in this sequential model. Finally, we validate our approach with experiments on various standard subspace clustering datasets, where we demonstrate that our models significantly improve on the current state-of-the-art in a variety of performance metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Related Work</head><p>Self-expressive and Affinity Learning. Existing methods that have attempted to unify the self-expressive and spectral clustering steps in subspace clustering include: Structured SSC <ref type="bibr" target="#b24">[25]</ref>, which jointly learns a self-expressive representation and a binary clustering matrix, and Block Diagonal Representation <ref type="bibr" target="#b28">[29]</ref> and SC-LALRG <ref type="bibr" target="#b49">[50]</ref>, which jointly learn a self-expressive representation and a constrained affinity for spectral clustering. However, all of these methods are non-convex, require iterative optimization methods, and necessitate expensive spectral computations in each iteration. As such, their scalability is greatly limited. In contrast, our methods have convex formulations, and we can compute the true minimizers very efficiently with our developed algorithms. Doubly Stochastic Clustering. Doubly stochastic constraints have been used in standard spectral clustering methods where the input affinity is known, but they have not been used to directly learn an affinity matrix for a self-expressive representation, as is desired in subspace clustering.</p><p>Specifically, Zass and Shashua find strong performance by applying standard spectral clustering to the nearest doubly stochastic matrix to the input affinity in Frobenius norm <ref type="bibr" target="#b54">[55]</ref>. Also, there has been work on learning doubly stochastic approximations of input affinity matrices subject to rank constraints <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b48">49]</ref>. Further, Landa et al. show that doubly stochastic normalization of the ubiqitous Gaussian kernel matrix by diagonal matrix scaling is robust to heteroskedastic additive noise on the data points <ref type="bibr" target="#b21">[22]</ref>.</p><p>To the best of our knowledge, only one subspace clustering method <ref type="bibr" target="#b23">[24]</ref> utilizes doubly stochastic constraints, but they enforce very expensive semi-definite constraints, use ad-hoc non-convex postprocessing, and do not directly apply spectral clustering to the doubly stochastic matrix. In contrast, we develop scalable methods with principled convex formulations that do not require postprocessing before applying spectral clustering.</p><p>Scalable Subspace Clustering. Existing methods or making subspace clustering scalable leverage sparsity in the self-expressive representation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b52">53]</ref>, leverage structure in the final affinity <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b1">2]</ref>, and/or use greedy heuristics <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b50">51]</ref> to efficiently compute subspace clusterings. Our scalable method is able to exploit sparsity in both the selfexpressive representation and affinity construction, is easily parallelizable (allowing a simple and fast GPU implementation), and is fully convex, with no use of greedy heuristics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Doubly Stochastic Models</head><p>In this section, we further detail the motivation for learning doubly stochastic affinities for subspace clustering. Then we develop two models for subspace clustering with doubly stochastic affinities. Our J-DSSC model jointly learns a self-expressive matrix and doubly stochastic affinity, while our A-DSSC model is a fast approximation that sequentially solves for a self-expressive matrix and then a doubly stochastic affinity that approximates it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Benefits of Doubly Stochastic Affinities</head><p>As discussed above, existing subspace clustering methods such as SSC <ref type="bibr" target="#b10">[11]</ref>, LRR <ref type="bibr" target="#b26">[27]</ref>, EDSC <ref type="bibr" target="#b18">[19]</ref>, and deep subspace clustering networks (DSC-Net) <ref type="bibr" target="#b19">[20]</ref> require various ad-hoc postprocessing methods to achieve strong clustering performance on certain datasets. For example, it has recently been shown that if one removes the ad-hoc postprocessing steps from DSC-Net <ref type="bibr" target="#b19">[20]</ref> then the performance drops considerably -performing no better than simple baseline methods and far below state-of-the-art <ref type="bibr" target="#b13">[14]</ref>. Common postprocessing steps include keeping only the top l entries of each column of C, normalizing columns of C, and/or using SVD-based postprocessing, each of which introduces extra hyperparameters and degrees of freedom for the practitioner. Likewise, the practitioner is also required to make a choice on the particular type of Laplacian normalization to use <ref type="bibr" target="#b42">[43]</ref>.</p><p>To better connect the self-expressive step with the subsequent spectral clustering step and to avoid the need for ad-hoc postprocessing methods, we propose a framework for directly learning an affinity A that already has desired properties (A1) and (C1)-(C2) for spectral clustering. Restricting A ∈ Ω n to be in the convex set of doubly stochastic matrices achieves these goals while allowing for highly efficient computation.</p><p>As noted in the introduction, doubly stochastic matrices are already nonnegative, so we do not need to take absolute values of some computed matrix. Also, each row and column of a doubly stochastic matrix sums to one, so they each have the same scale in l 1 norm -removing the need to postprocess by scaling rows or columns. Importantly for (A1), doubly stochastic matrices are invariant to most forms of Laplacian normalization used in spectral clustering. For example, for a symmetric affinity matrix A with row sums D = diag(A1), widely used Laplacian variants include the unnormalized Laplacian D − A, the normalized Laplacian I − D −1/2 AD −1/2 , and the random walk Laplacian</p><formula xml:id="formula_2">I − D −1 A [43]</formula><p>. When A is doubly stochastic, the matrix of row sums satisfies D = I, so all of the normalization variants are equivalent and give the same Laplacian I − A.</p><p>In addition, many types of regularization and constraints that have been proposed for subspace clustering tend to desire sparsity (C1) and connectivity (C2), in the sense that they want A ij to be small in magnitude or zero when x i and x j belong to different subspaces and to be nonzero for sufficiently many pairs (i, j) where x i and x j belong to the same subspace <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b45">46]</ref>. The doubly stochastic matrices learned by our models can be tuned to achieve any desired sparsity level. Also, doubly stochastic affinities are guaranteed a certain level of connectedness, as each row must sum to 1. This means that there cannot be any zero rows in the learned affinity, unlike in methods that compute representations one column at a time such as SSC <ref type="bibr" target="#b10">[11]</ref>, EnSC <ref type="bibr" target="#b51">[52]</ref>, and SSC-OMP <ref type="bibr" target="#b52">[53]</ref>, where it is possible that a point is never used in the self-expressive representation of other points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Joint Learning: J-DSSC</head><p>In developing our model to jointly learn a self-expressive matrix and doubly stochastic affinity, we build off of the general regularized self-expression form in <ref type="bibr" target="#b0">(1)</ref>. In addition to learning a self-expressive matrix C we also wish to learn a doubly stochastic affinity matrix A ∈ Ω n . Self-expressive formulations (roughly) model C ij as proportional to the likelihood that x i and x j are in the same subspace, so we desire that our normalized affinity A be close to |C| (optionally after scaling A by a constant to match the scale of |C|), which we incorporate via the use of a penalty function Θ(·, ·). Thus, we have a general framework:</p><formula xml:id="formula_3">min C,A 1 2 X − XC 2 F + λθ(C) + γΘ(|C| , µA) s.t. A ∈ Ω n , diag(C) = 0<label>(3)</label></formula><p>The zero-diagonal constraint on C is enforced to prevent each point from using itself in its self-representation, as this is not informative for clustering. While our framework is general, and can be used with e.g. low-rank penalties, we choose a simple mix of l 2 and l 1 regularization that is effective and admits fast algorithms. For Θ, we use an l 2 distance penalty, as l 2 projection of an affinity matrix onto the doubly stochastic matrices often improves spectral clustering performance <ref type="bibr" target="#b54">[55]</ref> and results in desirable sparsity properties <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b3">4]</ref>. As a result, our model takes the form of an optimization problem over C and A:</p><formula xml:id="formula_4">min C,A 1 2 X − XC 2 F + η 1 2 |C| − η 2 A 2 F + η 3 C 1 s.t. A ∈ Ω n , diag(C) = 0,<label>(4)</label></formula><p>where η 1 , η 2 &gt; 0, η 3 ≥ 0 are hyperparameters. The objective (4) is not convex because of the|C| term in the C-to-A-difference loss. To alleviate this issue, we relax the problem by separating the self-expressive matrix C into two nonnegative matrices C p , C q ≥ 0, so that C p − C q approximately takes the role of C and C p + C q approximately takes the role of |C| (with the approximation being exact if the nonzero support of C p and C q do not overlap). Thus, our convex model is given by:</p><formula xml:id="formula_5">min Cp,Cq,A 1 2 X − X[C p − C q ] 2 F + η 1 2 [C p + C q ] − η 2 A 2 F + η 3 C p + C q 1 s.t. A ∈ Ω n , C p , C q ∈ R n×n ≥0,diag=0<label>(5)</label></formula><p>where R n×n ≥0,diag=0 is the set of n × n real matrices with nonnegative entries and zero diagonal 1 . This problem is now convex and can be efficiently solved to global optimality, as we discuss in Section 3.1. We refer to this model as Joint Doubly Stochastic Subspace Clustering (J-DSSC).</p><p>If the optimal C p and C q have disjoint support (where the support of a matrix is the set of indices (i, j) where the matrix is nonzero), then by solving <ref type="bibr" target="#b4">(5)</ref> we also obtain a solution for (4), since we can take C = C p − C q , in which case |C| = C p − C q = C p + C q . However, this final equality does not hold when the optimal C p and C q have overlapping nonzero support. The following proposition shows that our relaxation (5) is equivalent to the original problem (4) for many parameter settings of (η 1 , η 2 , η 3 ). In cases where the supports of C p and C q overlap, we can also bound the magnitude of the overlapping entries (hence guaranteeing a close approximation of the solution to (4) from solutions to <ref type="formula" target="#formula_5">(5)</ref>). A proof is given in the appendix. <ref type="formula" target="#formula_5">(5)</ref> is the same as that of (4), when taking C = C * p − C * q . In particular, the supports of C * p and C * q are disjoint.</p><formula xml:id="formula_6">Proposition 1. Consider (5) with parameters η 1 &gt; 0 and η 2 , η 3 ≥ 0. Let (C * p , C * q , A * ) be an optimal solution. 1) If η 1 η 2 ≤ η 3 ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>then the solution of the relaxation</head><p>2) If η 1 η 2 &gt; η 3 , then the supports of C * p and C * q may overlap. At any index (i, j) for which (C * p ) ij &gt; 0 and (C * q ) ij &gt; 0, it holds that</p><formula xml:id="formula_7">max (C * p ) ij , (C * q ) ij &lt; η 1 η 2 − η 3 η 1 .<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Sequential Approximation: A-DSSC</head><p>To see an alternative interpretation of the model in (4), note the following by expanding the second term of (4):</p><formula xml:id="formula_8">(4) = min C,A 1 2 X − XC 2 F + η 1 2 C 2 F + η 3 C 1 − η 1 |C| , η 2 A + η 1 η 2 2 2 A 2 F s.t. A ∈ Ω n , diag(C) = 0<label>(7)</label></formula><p>From this form, one can see that for an uninformative initialization of the affinity as A = I, the minimization w.r.t. C takes the form of Elastic Net Subspace Clustering <ref type="bibr" target="#b51">[52]</ref>,</p><formula xml:id="formula_9">min C 1 2 X − XC 2 F + η 1 2 C 2 F + η 3 C 1 s.t. diag(C) = 0 (8)</formula><p>Likewise, for a fixed C, one can observe that the above problem w.r.t. A is a special case of a quadratically regularized optimal transport problem <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b27">28]</ref>:</p><formula xml:id="formula_10">min A −|C| , A + η 2 2 A 2 F s.t. A ∈ Ω n .<label>(9)</label></formula><p>Thus, η 2 controls the regularization on A, with lower η 2 encouraging sparser A and higher η 2 encouraging denser and more uniform A. In particular, as η 2 → 0, (9) approaches a linear assignment problem, which has permutation matrix solutions (i.e., maximally sparse solutions) <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b16">17]</ref>. In contrast, as η 2 → ∞, the optimal solution is densely connected and approaches the uniform matrix 1 n 11 . Hence, we consider an alternating minimization process to obtain approximate solutions for C and A, where we first initialize A = I and solve (8) for C. Then, holding C fixed we solve (9) for A. Taking the solution to this problem as the final affinity A, we obtain our one-step approximation to J-DSSC, which we refer to as Approximate Doubly Stochastic Subspace Clustering (A-DSSC) 2 . For this model, we develop a fast algorithm in Section 3, show state-of-theart clustering performance in Section 4, and show empirically in the appendix that A-DSSC well approximates the optimization problem of J-DSSC.</p><p>Besides being an approximation to the joint model <ref type="formula" target="#formula_5">(5)</ref>, A-DSSC also has an interpretation as a postprocessing method for certain subspace clustering methods that can be expressed as in <ref type="bibr" target="#b7">(8)</ref>, such as SSC, EnSC, and LSR <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b29">30]</ref>. Instead of arbitrarily making choices about how to postprocess A and form the normalized Laplacian, we instead take A to be doubly stochastic; as discussed above, this provides a principled means of generating an affinity A that is suitably normalized for spectral clustering from a well-motivated convex optimization problem (9). <ref type="bibr" target="#b1">2</ref> Note that additional alternating minimization steps can also be used as <ref type="bibr" target="#b6">(7)</ref> is convex w.r.t. C if A is held fixed (for any feasible A) and vice-versa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Doubly Stochastic Subspace Clustering.</head><p>Input: Data matrix X, parameters η 1 , η 2 , η <ref type="bibr" target="#b2">3</ref> Compute A by J-DSSC (5) or A-DSSC (8), <ref type="bibr" target="#b8">(9)</ref>. Apply spectral clustering on Laplacian I − 1 2 (A + A ). Output: Clustering result.</p><formula xml:id="formula_11">Algorithm 2 Scalable A-DSSC Active-set method. Input: Data matrix X, parameters η 1 , η 2 , η 3 Compute C in (8) by an EnSC or LSR solver. Initialize S (see appendix). Initialize α • and β • ∈ R n . Compute A • = 1 η2 |C| − α • 1 − 1β • + . while A • is not doubly stochastic do Compute α • and β • in (13) by L-BFGS. Compute A • = 1 η 2 |C| − α•1 − 1β • + . supp(S) ← supp(A • ) ∪ supp(S). end while Output: Optimal A = A • .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Optimization and Implementation</head><p>Here we outline the optimization procedures to compute J-DSSC and A-DSSC. The basic algorithm for subspace clustering with our models is in Algorithm 1. Further algorithmic and implementation details are in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Joint Solution for J-DSSC</head><p>In order to efficiently solve (5), we develop an algorithm in the style of linearized ADMM <ref type="bibr" target="#b30">[31]</ref>. We reparameterize the problem by introducing additional variables, and iteratively take minimization steps over an augmented Lagrangian as well as dual ascent steps on dual variables. As this procedure is rather standard, we delegate the full description of it to the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Approximation Solution for A-DSSC</head><p>Solving for C. As previously suggested, the approximate model A-DSSC can be more efficiently solved than J-DSSC. First, the minimization over C in <ref type="bibr" target="#b7">(8)</ref> can be solved by certain scalable subspace clustering algorithms. In general, it is equivalent to EnSC, for which scalable algorithms have been developed <ref type="bibr" target="#b51">[52]</ref>. Likewise, in the special case of η 1 &gt; 0, η 3 = 0, it is equivalent to LSR <ref type="bibr" target="#b29">[30]</ref>, which can be computed by a single n × n linear system solve.</p><p>Solving for A. Prior work has focused on computing the doubly stochastic projection (9) through the primal <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b36">37]</ref>, but we will instead solve it through the dual. The dual gives an unconstrained optimization problem over two vectors α, β ∈ R n , which is easier to solve as it eliminates the coupled constraints in the primal. We give computa-tional evidence in the appendix that the dual allows for signficantly faster computation, with over an order of magnitude improvement in certain settings. Moreover, the dual allows us to develop a highly scalable active-set method in Section 3.3, which also admits a simple GPU implementation -thus allowing for even further speed-up over existing methods. Now, the dual takes the form:</p><formula xml:id="formula_12">max α,β −1 (α+β)− 1 2η 2 |C| − α1 − 1β + 2 F<label>(10)</label></formula><p>in which [·] + denotes half-wave rectification, meaning that [x] + = max{0, x}, applied entry-wise. The optimal matrix A is then recovered as:</p><formula xml:id="formula_13">A = 1 η 2 |C| − α1 − 1β + .<label>(11)</label></formula><p>We use L-BFGS <ref type="bibr" target="#b25">[26]</ref> to solve for α and β in (10).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Scalable Active-Set Method for A-DSSC</head><p>While A-DSSC can be computed more efficiently than J-DSSC, the doubly stochastic projection step still requires O(n 2 ) operations to evaluate the objective (10) and its subgradient. This limits its direct applicability to larger datasets with high n. We take advantage of the sparsity of the optimal A in the parameter regimes that we choose, and develop a significantly more efficient active-set method for computing the doubly stochastic projection in <ref type="bibr" target="#b8">(9)</ref>.</p><p>Let S ∈ {0, 1} n×n be a binary support matrix and let S c be its complement. The basic problem that we consider is</p><formula xml:id="formula_14">min A −|C|, A + η 2 2 A 2 F s.t. A ∈ Ω n , A S c = 0.<label>(12)</label></formula><p>Again, we use the dual to develop an efficient solver:</p><formula xml:id="formula_15">max α•,β • ∈R n −(α • + β • ) 1 − 1 2η 2 |C| − α • 1 − 1β • + S 2 F .<label>(13)</label></formula><p>Here, the dual objective and its subgradient can be evaluated in O(|S|) time, where |S| is the number of nonzeros in S. In practice, we tend to only need |S| that is substantially smaller than n 2 , so this results in major efficiency increases. Also, we need only compute the elements in C that are in the support S; these elements can be precomputed and stored at a low memory cost for faster L-BFGS iterations.</p><p>In particular, this allows using LSR (i.e. setting η 3 = 0) in large datasets; even though the LSR C is dense and takes O(n 2 ) memory, we need only compute C S, which can be done efficiently with closed form matrix computations and takes O(|S|) memory (see appendix). As stated above, the objective and subgradient computations can be easily implemented on GPU -allowing even further speed-ups. For a dual solution (α • , β • ) of (13), the primal solution</p><formula xml:id="formula_16">is 1 η2 [|C| − α • 1 − 1β • ] + S.</formula><p>We show in the appendix that if the affinity with unrestricted support <ref type="bibr" target="#b8">(9)</ref>. Thus, if we iteratively update an initial support S, then the row and column sums of A • provide stopping criteria that indicate optimality. On the other hand, if A • is not doubly stochastic, then the support of the true minimizer A of the unrestricted problem (9) is not contained in supp(S), so supp(S) is too small. In this case, we update the support by supp(S) ← supp(A • ) ∪ supp(S).</p><formula xml:id="formula_17">A • = 1 η2 [|C| − α • 1 − 1β • ] + is doubly stochastic, then A • is the primal optimal solution to</formula><p>Thus, we have an algorithm that initializes a support and then iteratively solves problems of restricted support (13) until the unrestricted affinity A • is doubly stochastic. This is formalized in Algorithm 2. The appendix discusses choices of initial support. We can prove the following correctness result: Proposition 2. Algorithm 2 computes an optimal A for (9) in a finite number of steps.</p><p>A proof is given in the appendix. In practice, we find that only one or two updates of the support are necessary to find the optimal A, and the algorithm runs very quickly, solving problem (9) several orders of magnitude faster than previously proposed algorithms <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b36">37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we empirically study our J-DSSC and A-DSSC models. We show that they achieve state-of-theart subspace clustering performance on a variety of real datasets with multiple performance metrics. In the appendix, we provide more experiments that demonstrate additional strengths and interesting properties of our models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>Algorithms. We compare against several state-of-the-art subspace clustering algorithms, which are all affinity-based: SSC <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>, EnSC <ref type="bibr" target="#b51">[52]</ref>, LSR <ref type="bibr" target="#b29">[30]</ref>, LRSC <ref type="bibr" target="#b41">[42]</ref>, TSC <ref type="bibr" target="#b15">[16]</ref>, SSC-OMP <ref type="bibr" target="#b52">[53]</ref>, and S 3 COMP <ref type="bibr" target="#b5">[6]</ref>. For these methods, we run the experiments on our own common framework, and we note that the results we obtain are similar to those that have been reported previously in the literature on the same datasets for these methods. We report partial results from the paper of S 3 COMP <ref type="bibr" target="#b5">[6]</ref>, which is included as it is a recent well-performing model. Besides TSC, which uses an inner product similarity, these methods all compute an affinity based on some self-expressive loss.</p><p>Although methods for clustering data in a union of subspaces are not directly comparable to subspace clustering neural networks <ref type="bibr" target="#b13">[14]</ref>, which cluster data supported in a union of non-linear manifolds, we still include some comparisons. In particular, we run experiments with DSC-Net <ref type="bibr" target="#b19">[20]</ref> as a representative neural network based method and include further comparisons (that are qualitatively similar) to other neural networks <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b0">1]</ref> in the appendix. However, we note that recent work <ref type="bibr" target="#b13">[14]</ref> has shown that DSC-Net (and many related works for network-based subspace clustering) are often fundamentally ill-posed, calling into question the validity of the results from these methods.</p><p>Metrics. As is standard in evaluations of clustering, we use clustering accuracy (ACC) <ref type="bibr" target="#b52">[53]</ref> and normalized mutual information (NMI) <ref type="bibr" target="#b20">[21]</ref> metrics, where we take the denominator in NMI to be the arithmetic average of the entropies. Also, we consider a subspace-preserving error (SPE), which is given by 1 n n i=1 j ∈Sy i A ij / A i 1 , where S yi denotes the (subspace) cluster that point x i belongs to, and A i is the ith column of A. This measures the proportion of mass in the affinity that is erroneously given to points in different (subspace) clusters, and is often used in evaluation of subspace clustering algorithms <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b39">40]</ref>. We report the sparsity of learned affinities by the average number of nonzeros per column (NNZ). Although sparse affinities are generally preferred, the sparsest affinity is not necessarily the best.</p><p>Datasets. We test subspace clustering performance on the Extended Yale-B dataset <ref type="bibr" target="#b11">[12]</ref>, Columbia Object Image Library (COIL-40 and COIL-100) <ref type="bibr" target="#b32">[33]</ref>, UMIST face dataset <ref type="bibr" target="#b12">[13]</ref>, ORL face dataset <ref type="bibr" target="#b37">[38]</ref>, MNIST <ref type="bibr" target="#b22">[23]</ref>, and EMNIST-Letters <ref type="bibr" target="#b6">[7]</ref>. We run experiments with the raw pixel data from the images of certain datasets; for COIL, UMIST, MNIST, and EMNIST we run separate experiments with features obtained from a scattering convolution network <ref type="bibr" target="#b4">[5]</ref> reduced to dimension 500 by PCA <ref type="bibr" target="#b46">[47]</ref>. This has been used in previous subspace clustering works <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b5">6]</ref> to better embed the raw pixel data in a union of linear subspaces.</p><p>We do not evaluate DSC-Net <ref type="bibr" target="#b19">[20]</ref> on scattered features as the network architecture is only compatible with image data. For MNIST (n=70,000) and EMNIST (n=145,600), we can only run scalable methods, as neural networks and other methods that form dense n-by-n matrices have unmanageable memory and runtime costs.</p><p>Clustering setup. For each baseline method we learn the C from that method, form the affinity asÂ = (|C| +|C| )/2, take the normalized Laplacian L = I − D −1/2Â D −1/2 (where D = diag(Â1)), and then apply spectral clustering with the specified number of clusters k <ref type="bibr" target="#b42">[43]</ref>. For our DSSC methods, we formÂ = (A + A )/2 and use L = I −Â directly for spectral clustering (recall our affinity is invariant to Laplacian normalization). For the spectral clustering we take the k eigenvectors corresponding to the k smallest eigenvalues of L as an embedding of the data (we take k + 1 eigenvectors for MNIST as in <ref type="bibr" target="#b5">[6]</ref>, as well as for EMNIST for similar reasons). After normalizing these embeddings to have unit l 2 norm, we obtain clusterings by random initializations of k-means clustering, compute the accuracy and NMI for each clustering, then report the average accuracy and NMI as the final result. We emphasize that for each method we obtain a nonnegative, symmetric affinity by a shared postprocessing of C and/or A instead of using different ad-hoc postprocessing methods that may confound the comparisons. Since DSC-Net is highly reliant on its ad-hoc postprocessing <ref type="bibr" target="#b13">[14]</ref>, we also report results with its postprocessing strategy applied (shown as DSC-Net-PP).</p><p>Parameter choices. As is often done in subspace clustering evaluation, we choose hyperparameters for each method by searching over some set of parameters and reporting the results that give the highest clustering accuracy. For DSC-Net, we use the suggested parameters in <ref type="bibr" target="#b19">[20]</ref> where applicable, and search over hyperparameters for novel datasets.</p><p>We give details on model hyperparameters in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results</head><p>Experimental results are reported in <ref type="table">Tables 1 and 2</ref>. On each dataset, one of our DSSC methods achieves the highest accuracy and NMI among all non-neural-network methods. In fact, on scattered data, DSSC achieves higher accuracy and NMI than the neural method DSC-Net, even when allowing DSC-Net to use its ad-hoc postprocessing. DSC-Net with postprocessing does achieve better performance on Yale-B and ORL than our methods, though without postprocessing our models outperform it; further, note that our method is considerably simpler than DSC-Net which requires training a neural network and hence requires choices of width, activations, stochastic optimizer, depth, and so on. Moreover, our models substantially outperform DSC-Net on both the raw pixel data and scattered embeddings of UMIST, COIL-40, and COIL-100 -achieving perfect clusterings of COIL-40 and near-perfect clusterings of COIL-100 -establishing new states-of-the-art to the best of our knowledge. Likewise, among the scalable methods our models achieve very strong performance on (E)MNIST.</p><p>We also see that A-DSSC, which can be viewed as running SSC, EnSC or LSR followed by a principled doubly stochastic postprocessing, outperforms SSC, EnSC, and LSR across all datasets. This suggests that the doubly stochastic affinity matrices learned by our models are indeed effective as inputs into spectral clustering and that our method can also potentially be combined with other subspace clustering methods as a principled normalization step. Beyond clustering accuracy, our models also learn affinity matrices with other desirable properties, such as having the lowest SPE among all methods -indicating that our learned affinities place mass in the correct subspaces. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we propose models based on learning doubly stochastic affinity matrices that unify the self-expressive modeling of subspace clustering with the subsequent spectral clustering. We develop efficient algorithms for computing our models, including a novel active-set method that is highly scalable and allows for clustering large computer vision datasets. We demonstrate experimentally that our methods achieve substantial improvements in subspace clustering accuracy over state-of-the-art models, without using any of the ad-hoc postprocessing steps that many other methods rely on. Our work further suggests many promising directions for future research by better merging the self-expressive modeling of subspace clustering with spectral clustering (or other graph clustering).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices A. A-DSSC Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Derivation of A-DSSC Dual</head><p>Here, we give a derivation for the dual (10) of the minimization over A in A-DSSC, as well as the dual with restricted support <ref type="bibr" target="#b12">(13)</ref>. It suffices to derive the restricted support dual, as the full dual follows by setting S as the all ones matrix. Now, recall that the primal problem is:</p><formula xml:id="formula_18">arg min A −η1|C| , η2A + η1η 2 2 2 A 2 F s.t. A ∈ Ωn, A S c = 0 (14) = arg min A −|C| , A S + η2 2 A S 2 F s.t. A S ∈ Ωn. (15)</formula><p>Introducing Lagrange multipliers α and β ∈ R n for the row and column sum constraints, we have the equivalent problem:</p><formula xml:id="formula_19">min A≥0 max α,β∈R n −|C| , A S + η2 2 A S 2 F + α, (A S)1 − 1 + β, (A S) 1 − 1<label>(16)</label></formula><p>As the primal problem is convex and has a strictly feasible point, strong duality holds by Slater's condition, so this is equivalent to:</p><formula xml:id="formula_20">max α,β∈R n min A≥0 −|C| , A S + η2 2 A S 2 F + α, (A S)1 − 1 + β, (A S) 1 − 1 (17) = max α,β∈R n −(α+β) 1+min A≥0 −|C| , A S + η2 2 A S 2 F + α1 + 1β , A S<label>(18)</label></formula><p>Letting K = |C| − α1 − 1β , the inner minimization takes the form:</p><formula xml:id="formula_21">min A≥0 −K, A S + η2 2 A S 2 F (19) =η2 min A≥0 − 1 η2 K, A S + 1 2 A S 2 F (20) = − 1 2η2 K 2 F + η2 min A≥0 1 2 1 η2 K − A S 2 F (21) = − 1 2η2 K 2 F + 1 2η2 K− S c 2 F (22) = − 1 2η2 K+ S 2 F<label>(23)</label></formula><p>Where [K]− is min{K, 0} taken elementwise. Thus, the final version of the dual is <ref type="bibr" target="#b23">24)</ref> and the optimal value of A is as given as</p><formula xml:id="formula_22">max α•,β • ∈R n −(α• + β • ) 1 − 1 2η2 |C| − α•1 − 1β • + S 2 F<label>(</label></formula><formula xml:id="formula_23">A = 1 η2 |C| − α•1 − 1β • + S (25)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Active-Set Method</head><p>Here, we give a proof that our active-set method in Algorithm 2 converges to an optimal solution in finitely many steps. We start with a lemma that shows the correctness of our stopping criterion. Lemma 1. Let S contain a doubly stochastic zero-nonzero pattern in its support, and let (α•, β • ) be a solution to the dual of the problem with restricted support (13) with parameter η2.</p><formula xml:id="formula_24">Then A• = 1 η 2 |C| − α•1 − 1β • +</formula><p>is optimal for the original problem with unrestricted support if and only if A• is doubly stochastic.</p><p>Proof. It is clear that any optimal solution is doubly stochastic, since these are exactly the feasibility conditions. For the other direction, suppose that A• is doubly stochastic. Note that a subgradient of the restricted support dual objective (13) is given by:</p><formula xml:id="formula_25">∇α • (13) = −1 + 1 η2 [|C| − α•1 − 1β • ]+ S 1 (26) ∇ β • (13) = −1 + 1 η2 [|C| − α•1 − 1β • ]+ S 1 (27)</formula><p>In the case of unrestricted support (i.e. full support), the subgradients exactly measure how far the row sums and column sums of A• deviate from 1. Since A• is doubly stochastic, the row and column sums are 1, so the subgradients are zero. Thus, α• and β • are optimal for the unrestricted support dual <ref type="bibr" target="#b9">(10)</ref>, which means that we may recover the optimal primal variable as</p><formula xml:id="formula_26">1 η 2 [|C| − α•1 − 1β ]+, which is exactly A•.</formula><p>With this result, we can directly prove convergence and correctness. Proposition 2. Algorithm 2 computes an optimal A for (9) in a finite number of steps.</p><p>Proof. First, note that the initialization of S gives a feasible problem, since the support contains a permutation matrix, which is doubly stochastic. In each iteration, we compute the matrix</p><formula xml:id="formula_27">A• = 1 η 2 |C| − α•1 − 1β • + .</formula><p>If A• is doubly stochastic, then we terminate and A• is optimal for the unrestricted support problem by Lemma 1. If A• is not doubly stochastic, then A• = A• S since A• S is doubly stochastic. Thus, the support is updated to a new support that has not been seen before, since it is strictly larger. There are only finitely many choices of support, so there are only finitely many iterations of this procedure, thus proving finite convergence of our algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Support Initialization</head><p>The primal (12) may not be feasible for certain choices of support S with too many zeros, since there is not always a doubly stochastic matrix with a given zero-nonzero pattern. One would like to have an intialization for the support that is both feasible, so that our algorithm converges, and that approximately contains the true support of the optimal A, so that the convergence is fast. One reasonable guess for the support of A is the top k entries of each row, since these entries contribute more to the inner product of the objective. Also, for the linear assignment problem, which is the limiting η2 = 0 case, the top k entries of each row have been shown to contain the support of the solution A with high probability <ref type="bibr" target="#b2">[3]</ref>. However, a top k graph is not guaranteed to be feasible for reasonably sized k, as the following lemma shows: Lemma 2. Let n and k be integers with n ≥ k ≥ 3.</p><p>1. If k &lt; n/2, then there exists a graph G on n nodes with minimum degree k such that there is no doubly stochastic matrix with support contained in the support of G.</p><p>2. If k ≥ n/2, then there is a doubly stochastic matrix in the support of any graph on n nodes with minimum degree k.</p><p>Proof. Let k &lt; n/2, so that n − k &gt; k. Consider the graph with adjacency matrix</p><formula xml:id="formula_28">0 n−k,n−k 1 n−k 1 k 1 k 1 n−k 1 k 1 k − I k,k<label>(28)</label></formula><p>Each row has either k or n − 1 ones, so each node has at least degree k. However, there can be no permutation matrix with support contained in the support of this graph. This is because the first n − k nodes only have k unique neighbors, and n − k &gt; k. Now, let k ≥ n/2, and let G be a graph on n nodes with minimum degree k ≥ n/2. In each connected component of the graph with m nodes, the minimum degree is at least n/2 ≥ m/2. Thus, Dirac's theorem on Hamiltonian cycles <ref type="bibr" target="#b7">[8]</ref> states that each connected component has a Hamiltonian cycle -a cycle that visits each node in the component. Taking the permutation matrix that is the sum of these cycles gives us a permutation matrix contained in the support of this graph, so we are done since permutation matrices are doubly stochastic.</p><p>One way in which the graph <ref type="bibr" target="#b27">(28)</ref> occurs is if there is a hub and spoke structure in the graph, in which there are k closely connected nodes forming a hub and all other n − k nodes are distant from each other and are connected to the k nodes in the hub. A top-k graph can certainly have this structure if C is a similarity derived from some euclidean distance. In our case, C is a self-expressive affinity, so it is possible that tighter bounds may be derived under some conditions, but we leave this to future work.</p><p>Choosing a top-k support initialization for some k ≥ n/2 defeats the purpose of the active-set method, as computing a solution in this support would still require O(n 2 ) computations per iteration. In practice, we choose a small k that leads to a tractable problem size, and add some randomly sampled permutation matrices to the support to guarantee feasibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Self-Expressive Computation</head><p>For a general dual problem <ref type="bibr" target="#b12">(13)</ref> with restricted support S, we need only evaluate Cij for (i, j) ∈ supp(S). These Cij can be computed online in each iteration if memory is an issue, or it can be precomputed and stored in O(|S|) memory for faster objective and subgradient evaluations.</p><p>In particular, computing a subset of a dense LSR <ref type="bibr" target="#b29">[30]</ref> solution C, or computing it online allows us to scale LSR to datasets with many points like MNIST and EMNIST where the entire dense nby-n C cannot be stored. To see how this is done, note that the solution to LSR with parameter γ (i.e. a solution to <ref type="bibr" target="#b7">(8)</ref> with η1 = γ, η3 = 0) with no zero diagonal constraint is given as</p><formula xml:id="formula_29">C = (X X + γI) −1 X X<label>(29)</label></formula><p>The Woodbury matrix identity <ref type="bibr" target="#b16">[17]</ref> gives that this is equal to</p><formula xml:id="formula_30">1 γ I − 1 γ 2 X I + 1 γ X X −1 X X X (30) = 1 γ X X − 1 γ 2 X I + 1 γ XX −1 XX X (31) = X 1 γ I − 1 γ 2 I + 1 γ XX −1 XX M X<label>(32)</label></formula><p>so we may compute Cij = X i MXj. Note that M is d-by-d, and we do not ever need to form an n-by-n matrix in this computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5. Active-set Runtime Comparison</head><p>The computation of A in A-DSSC (9) is simply application of the proximal operator for projection onto the doubly stochastic matrices Ωn. To view it as a Frobenius-norm projection of an input matrix onto the doubly stochastic matrices, note that arg min</p><formula xml:id="formula_31">A∈Ωn −|C|, A + γ 2 A 2 F<label>(33)</label></formula><p>= arg min</p><formula xml:id="formula_32">A∈Ωn 1 γ |C| − A 2 F<label>(34)</label></formula><p>In the past, algorithms for computing this have used the primal formulation. Zass and Shashua <ref type="bibr" target="#b54">[55]</ref> use an alternating projections algorithm, while Rontsis and Goulart <ref type="bibr" target="#b36">[37]</ref> use an ADMM based algorithm. As explained in Section 3.2, we note that the dual <ref type="formula" target="#formula_0">(10)</ref> is a special case of the dual of a regularized optimal transport problem <ref type="bibr" target="#b3">[4]</ref>, so it can be computed with the same algorithms that these regularized optimal transport problems use. Our Algorithm 2 gives a fast way to compute this dual by leveraging problem sparsity.</p><p>Here, we compare the performance of these different methods on various input matrices |C|. The different input matrices we consider are:</p><p>• D1, |C| is the A-DSSC affinity for the Yale-B dataset, so n = 2414.</p><p>• D2, |C| is the A-DSSC affinity for scattered COIL, so n = 7200.</p><p>• D3, |C| is 1/2(|G| + |G |), scaled to have maximum element 1, where G is a standard Gaussian random matrix of size 2000 × 2000. We take γ = .5. <ref type="table">Table 3</ref>. Runtime (in seconds) of different doubly stochastic projection algorithms. The data Di are defined in Section A.5. "NC" indicates that the algorithm did not converge in 5000 iterations.</p><p>Method</p><formula xml:id="formula_33">D 1 D 2 D 3 D 4</formula><p>Alt Proj <ref type="bibr" target="#b54">[55]</ref> NC NC 118 264 ADMM <ref type="bibr" target="#b36">[37]</ref> 54.1 NC 26.2 111 Dual <ref type="bibr" target="#b3">[4]</ref> 11.8 568 1.69 13.7 Active-set (ours) 1.64 12.2 0.49 2.05</p><p>For each experiment, we run the algorithm until the row and column sums of the iterates are all within 10 −4 of 1. Runtimes are given in <ref type="table">Table 3</ref>. Similarly to previous work <ref type="bibr" target="#b36">[37]</ref>, we find that alternating projections has serious convergence issues; it also fails to converge to numerically sparse affinities. Our active-set method converges orders of magnitudes faster than the other methods. We emphasize that these performance gains are further amplified on larger datasets, where memory considerations can make it so that the other methods cannot even be run. Moreover, all of these experiments were run on CPU, while our method also has a GPU implementation that allows for very efficient computation on large datasets. For instance, we can compute full A-DSSC on scattered MNIST (n = 70,000) and achieve 99% clustering accuracy on a GeForce RTX 2080 GPU in less than 30 seconds. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. J-DSSC Details</head><formula xml:id="formula_34">1 2 X − Z 2 F + η1 2 [Cp + Cq] − η2A 2 F + η3 Cp + Cq 1 s.t. A ≥ 0, Cp, Cq ∈ R n×n ≥0,diag=0 , Y 1 = Y1 = 1, Y = A, Z = X[Cp − Cq]<label>(35)</label></formula><p>The augmented Lagrangian then takes the form:</p><formula xml:id="formula_35">L(Cp, Cq, A, Y, Z, λ, Λ) = 1 2 X − Z 2 F + η1 2 [Cp + Cq] − η2A 2 F + η3 Cp + Cq 1 + λ1, Y 1 − 1 + λ2, Y1 − 1 + Λ1, Y − A + ρ 2 Y 1 − 1 2 2 + ρ 2 Y1 − 1 2 2 + ρ 2 Y − A 2 F + Λ2, Z − X[Cp − Cq] + ρ 2 Z − X[Cp − Cq] 2 F + I ≥0 (A) + I ≥0,diag=0 (Cp) + I ≥0,diag=0 (Cq)<label>(36)</label></formula><p>where IS is the indicator function for the set S, which takes values of 0 in S and ∞ outside of S, and where ρ &gt; 0 is a chosen constant.</p><p>In each iteration, we take a linearized ADMM step in Cp and Cq, then alternatingly minimize over A, Y, and Z, and lastly take gradient ascent steps on each of the Lagrange multipliers. For a step size τ &gt; 0, the linearized ADMM step over Cp takes the form of a gradient descent step on: </p><formula xml:id="formula_36">h(Cp) = Λ2, Z − X[Cp − Cq] + ρ 2 Z − X[Cp − Cq] 2 F ,<label>(37)</label></formula><formula xml:id="formula_37">+ 1 2τ C p − Cp 2 F + I ≥0,diag=0 (Cp) (38)</formula><p>For a matrix E, let [E]+ be the half-wave rectification of E, and let [E] +,d=0 be the matrix E with all negative entries and the diagonal set to zero. Then the linearized ADMM updates are given as follows:</p><formula xml:id="formula_38">C p ← Cp − τ −X Λ2 + ρX (X[Cp − Cq] − Z)<label>(39)</label></formula><p>Cp</p><formula xml:id="formula_39">← 1 η1 + 1 τ 1 τ C p − η1Cq + η1η2A − η311 +,d=0 (40) C q ← Cq − τ (X Λ2 − ρX (X[Cp − Cq] − Z))<label>(41)</label></formula><p>Cq</p><formula xml:id="formula_40">← 1 η1 + 1 τ 1 τ C q − η1Cp + η1η2A − η311 +,d=0<label>(42)</label></formula><p>The sequential minimizations over A then Y then Z, holding the other variables fixed and using the most recent value of each variable, take the form:</p><formula xml:id="formula_41">A ← 1 η1η 2 2 + ρ (η1η2[Cp + Cq] + Λ1 + ρY) + (43) Y ← 1 ρ V − 1 n + 1 PV11 − 11 V 1 n + 1 P (44) where V = ρA + 2ρ11 − 1λ 1 − λ21 − Λ1<label>(45)</label></formula><formula xml:id="formula_42">P = I − 1 2n + 1 11 (46) Z ← 1 1 + ρ X − Λ2 + ρX[Cp − Cq]<label>(47)</label></formula><p>The dual ascent steps on the Lagrange multipliers take the form:</p><formula xml:id="formula_43">λ1 ← λ1 + ρ(Y 1 − 1) (48) λ2 ← λ2 + ρ(Y1 − 1) (49) Λ1 ← Λ1 + ρ(Y − A) (50) Λ2 ← Λ2 + ρ(Z − X[Cp − Cq])<label>(51)</label></formula><p>Thus, the full J-DSSC algorithm is given in Algorithm 3.</p><p>To derive the minimization steps for a variable E that is either Cp, Cq, A, or Z, we use the method of completing the square to change the corresponding minimization into one of the form min E∈S E − E 2 F for some feasible set S and some matrix E . For instance, in the case of Cp, the set S is the set of n×n nonnegative matrices with zero diagonal, and the solution is [E ] +,d=0 .</p><p>To minimize over Y, we use different approaches often used in dealing with row or column sum constraints <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b44">45]</ref>.</p><formula xml:id="formula_44">Define V = ρA + 2ρ11 − 1λ 1 − λ21 − Λ1.</formula><p>Setting the gradient of the augmented Lagrangian with respect to Y equal to zero, we can show that</p><formula xml:id="formula_45">ρY + ρY11 + ρ11 Y = V<label>(52)</label></formula><p>Multiplying both sides by 1 on the right and noting that 1 T 1 = n, we have</p><formula xml:id="formula_46">ρ(I + nI + 11 )Y1 = V1 (53) ρY1 = (I + nI + 11 ) −1 V1<label>(54)</label></formula><p>Likewise, multiplying by 1 on the left gives that</p><formula xml:id="formula_47">ρ1 Y = 1 V(I + nI + 11 ) −1<label>(55)</label></formula><p>We use the Woodbury matrix identity to compute the inverse</p><formula xml:id="formula_48">(I + nI + 11 ) −1 = 1 n + 1 I − 1 2n + 1 11 .<label>(56)</label></formula><p>We define P = I − 1 2n+1 11 , so the inverse is 1 n+1 P. Thus, using both <ref type="bibr" target="#b53">(54)</ref> and <ref type="bibr" target="#b54">(55)</ref> in the left hand side of <ref type="bibr" target="#b51">(52)</ref> gives that</p><formula xml:id="formula_49">ρY + 1 n + 1 PV11 + 11 V 1 n + 1 P = V<label>(57)</label></formula><p>and thus the minimizing Y is given as</p><formula xml:id="formula_50">Y = 1 ρ V − 1 n + 1 PV11 − 11 V 1 n + 1 P .<label>(58)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Implementation Details</head><p>We implement J-DSSC in the Julia programming language. For A-DSSC, we have separate implementations in Julia and Python, where the Python implementation uses PyTorch for GPU computations. For the J-DSSC algorithm, we take the step sizes to be ρ = .5 and τ = .0001 for our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Complexity Analysis</head><p>Let X ∈ R d×n , so there are n data points in R d . For J-DSSC, each update on Cp and Cq takes O(dn 2 ) operations due to matrix multiplications. Each update on A and Y takes O(n 2 ) operations, while each update on Z takes O(dn 2 ). The dual ascent steps also take O(dn 2 ) operations in total. Thus, each iteration of the linearized ADMM part takes O(dn 2 ). We note in particular that J-DSSC does not require the solution of any linear systems or any singular value decompositions, while many other subspace clustering methods do.</p><p>For A-DSSC, solving for C can be done by some efficient EnSC, SSC, or LSR solver. As discussed in Section A.4, we need only compute Ci,j for (i, j) ∈ supp(S) of some support S, and this can be done online or as a precomputation step. The quadratically regularized optimal transport step takes O(|S|) operations per objective function evaluation and gradient evaluation.</p><p>The spectral clustering step requires a partial eigendecomposition of I − 1 2 (A + A ), although we note that in practice the A that is learned tends to be sparse for the parameter settings that we choose, so we can leverage sparse eigendecomposition algorithms to compute the k eigenvectors corresponding to the smallest eigenvalues. Recall that all subspace clustering methods that use spectral clustering (which is the majority of methods) require computing an eigendecomposition of equivalent size, though not all subspace clustering affinities can leverage sparse eigendecomposition methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Model Selection</head><p>Our method J-DSSC has three hyperparameters η1, η2, η3 to set. If we fix A, the problem is simply EnSC <ref type="bibr" target="#b51">[52]</ref>. This means that we may choose the parameters η1 and η3 in J-DSSC as is done in EnSC. As, noted in Section 2.2, η2 controls the sparsity of the final A, with smaller η2 tending to give sparser A. Thus, we choose η2 such that A is within a desired sparsity range. To avoid solutions that are too sparse, we can consider the number of connected components of the graph associated to the solution A; if we want to obtain k clusters, we can adjust η2 so that the number of connected components is at most k.</p><p>The parameters for A-DSSC can be selected in an analogous manner to those of J-DSSC. Another interesting consideration is that A-DSSC does very well when η3 = 0, meaning that there is no l1 regularization on C and thus C is the solution to an LSR problem <ref type="bibr" target="#b29">[30]</ref>. In this case, C can be computed efficiently with one linear system solve, and we need only set the two parameters η1 and η2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Proof of Proposition 1</head><p>Proof. Suppose that there is an index (i, j) such that Define matrices C p and C q such that C p matches C * p at every index but (i, j) and similarly for C q . At index (i, j), we define</p><formula xml:id="formula_51">a := (C * p )ij &gt; 0, b := (C * q )ij &gt; 0.<label>(59)</label></formula><formula xml:id="formula_52">(C p )ij = max(a − b, 0), (C q )ij = max(b − a, 0) (60)</formula><p>We consider the value of the expanded J-DSSC objective <ref type="formula" target="#formula_8">(7)</ref> at</p><formula xml:id="formula_53">(C p , C q , A * ) compared to the value at (C * p , C * q , A * ). Note that C p − C q = C * p − C * q .</formula><p>Thus, there are only three summands in the objective that are different between the two groups of variables. Suppose a ≥ b. The differences are computed as:</p><formula xml:id="formula_54">η1 2 C p + C q 2 F − η1 2 C * p + C * q 2 F = η1 2 |a − b| 2 − (a + b) 2 (61) = −2η1ab − η1 [C p + C q ], η2A * + η1 [C * p + C * q ], η2A * = η1η2(A * )ij −|a − b| + (a + b) (62) = 2η1η2(A * )ijb η3 C p + C q 1 − η3 C * p + C * q 1 = η3 |a − b| − (a + b) (63) = −2η3b</formula><p>Now, we bound the difference between the objective functions for the two variables:</p><formula xml:id="formula_55">0 &lt; −2η1ab + 2η1η2(A * )ijb − 2η3b ≤ −2η1ab + 2η1η2b − 2η3b</formula><p>where the first bound is due to (C * p , C * q , A * ) being the optimal solution, and the upper bound is due to (A * )ij ≤ 1. From these bounds, we have that</p><formula xml:id="formula_56">b ≤ a &lt; η1η2 − η3 η1 .<label>(64)</label></formula><p>Where we recall that we assumed b ≤ a. A similar upper bound holds with b taking the place of a in the case where b &gt; a. Now, note that a &gt; 0 means that η1η2 &gt; η3, thus proving 1) through the contrapositive. This bound clearly gives the one in (6), so we have also proven 2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Further Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1. Comparison with Neural Networks</head><p>In <ref type="table" target="#tab_2">Table 4</ref>, we list the clustering accuracy of subspace clustering network models against our DSSC models. These results are taken directly from the corresponding papers, which means that they do not follow our common evaluation framework as used in Section 4 of the main paper. Thus, the neural network methods may still use their ad-hoc postprocessing methods, which have been shown to significantly affect empirical performance <ref type="bibr" target="#b13">[14]</ref>. Nonetheless, DSSC still significantly outperforms neural methods on COIL-100, UMIST, and MNIST. In fact, self-expressive neural models generally cannot compute clusterings for datasets on the scale of MNIST or EMNIST, as they require a dense n-by-n matrix of parameters for the self-expressive layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2. Behavior of DSSC Models</head><p>Here, we explore the quality of our approximation A-DSSC by comparing the natural extension of this approximation to multiple steps of alternating minimization over C in <ref type="bibr" target="#b7">(8)</ref> and A in <ref type="bibr" target="#b8">(9)</ref>. In particular, we note that additional alternating minimization steps do not bring a significant benefit over just a single alternating minimization iteration. <ref type="figure" target="#fig_3">Figure 3</ref> displays the results of our A-DSSC model on the UMIST face dataset <ref type="bibr" target="#b12">[13]</ref> using the experimental setup described in Section 4.1. We see that a single alternating minimization step over C then A, which is equivalent to our A-DSSC model, already achieves most of the decrease in the objective (5) -giving a relative error of less than 1% compared to the global minimum obtained by the full J-DSSC solution. Moreover, the clustering accuracy is robust to additional alternating minimization steps beyond a single A-DSSC step -additional steps do not increase clustering accuracy from spectral clustering on A. Also, spectral clustering on A achieves much higher accuracy than spectral clustering on C, thus showing the utility of the doubly stochastic model.</p><p>We also investigate the sparsity and connectivity of the affinity A learned by our models. In <ref type="figure">Figure 2</ref>, we show sparsity plots of the learned matrices A for J-DSSC models with varying η2 on the first five classes on the Extended Yale-B dataset <ref type="bibr" target="#b11">[12]</ref>. The points are sorted by class, so intra-subspace connections are on the block diagonal, and inter-subspace connections are on the off-diagonal blocks. Here, it can be seen that the sparsity level of the recovered A affinity is controlled by choice of the η2 parameter even when there is no sparse regularization on C (i.e., η3 = 0), with the number of nonzeros increasing as η2 increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Data Details</head><p>For our experiments, recall that we run subspace clustering on the Extended Yale-B face dataset <ref type="bibr" target="#b11">[12]</ref>, Columbia Object Image Library (COIL-40 and COIL-100) <ref type="bibr" target="#b32">[33]</ref>, UMIST face dataset <ref type="bibr" target="#b12">[13]</ref>, 25 and η3 = 0 (so there is no l1 regularization on C), while varying η2. Red dashed lines mark the boundaries between classes. Dark blue points mark nonzero entries in A. The learned affinity A is sparser as η2 decreases, while it is more connected as η2 increases.   ORL face dataset <ref type="bibr" target="#b37">[38]</ref>, MNIST digits dataset <ref type="bibr" target="#b22">[23]</ref>, and EMNIST-Letters dataset <ref type="bibr" target="#b6">[7]</ref>. Sample images from some of these datasets are shown in <ref type="figure" target="#fig_4">Figure 4</ref>. Basic statistics and properties of these datasets are given in <ref type="table" target="#tab_3">Table 5</ref>.</p><p>The scattered data is computed as in <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b52">53]</ref>, by using a scattering convolution network <ref type="bibr" target="#b4">[5]</ref> to compute a feature vector of 3,472 dimensions for UMIST, MNIST, and EMNIST, and 3 · 3,472 = 10,416 dimensions for COIL (since there are three color channels), then projecting to 500 dimensions by PCA. For both pixel data and scattered data, we normalize each data point to have unit l2 norm for all experiments, noting that scaling a data point does not change its subspace membership.</p><p>For Yale-B, we remove the 18 images labeled as corrupted in the dataset, leaving 2414 images for our experiments. Adding the corrupted images back in does not appear to qualitatively change our results. We resize Yale-B to size 48 × 42. The COIL, UMIST, and ORL images are resized to size 32 × 32. Here, we give parameter settings for the experiments in the paper. The chosen parameter settings for our DSSC models are given in <ref type="table" target="#tab_4">Table 6</ref>. We note that the experiments on UMIST to explore the objective value over iterations of alternating minimization in <ref type="figure" target="#fig_3">Figure 3</ref> use the same parameter settings as in this table.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Parameter Settings</head><p>The parameters searched over for the non-neural methods are given in <ref type="table" target="#tab_5">Table 7</ref>.</p><p>For DSC-Net, we use the hyperparameters and pre-trained autoencoders as in their original paper <ref type="bibr" target="#b19">[20]</ref> for Yale-B, COIL-100, and ORL. For DSC-Net on UMIST, we take a similar architecture to that of <ref type="bibr" target="#b57">[58]</ref>, with encoder kernel sizes of 5 × 5, 3 × 3, 3 × 3, as well as 15, 10, 5 channels per each layer. The regularization parameters are set as λ1 = 1 and λ2 = .2. The DSC-Net post- processing on UMIST is taken to be similar to that used for ORL. For COIL-40, we use one encoder layer of kernel size 3 × 3, 20 channels, and regularization parameters λ1 = 1, λ2 = 100; we also use the same postprocessing that is used for COIL-100.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>B. 1 .</head><label>1</label><figDesc>Linearized ADMM for J-DSSC Algorithm 3 J-DSSC Input: Data matrix X, parameters η 1 , η 2 , η 3 , step sizes ρ and τ while Not converged do Update Cp and Cq by linearized ADMM steps (39) to (42) Update A, Y, and Z by minimization steps (43) to (47) Update λ1, λ2, Λ1, and Λ2 by dual ascent steps (48) to (51) end while Apply spectral clustering on Laplacian I − 1 2 (A + A ) Output: Clustering result Here, we give details on the computation of J-DSSC by linearized ADMM. We reparameterize the problem (5) by introducing additional variables Y = A and Z = X[Cp −Cq]. This gives us the equivalent problem min Cp,Cq ,A,Y,Z</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>(a) η 2 = . 1 (Figure 2 .</head><label>212</label><figDesc>b) η 2 = .2 (c) η 2 = .5 (d) η 2 = 1.0 Affinity matrices A learned by J-DSSC on the first five classes of Yale-B. We fix η1 = .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Objective value and clustering accuracy for different number of alternating minimization steps over C in (8) then A in (9) on the UMIST dataset. Our A-DSSC model is equivalent to 1 alternating minimization step. (Left) Shows relative error in the objective value compared to the true minimum. (Right) Shows clustering accuracy of using either A or C in spectral clustering.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Sample images from Extended Yale-B (top row), COIL-100 (middle row), and UMIST (bottom row).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .Table 2</head><label>12</label><figDesc>Subspace clustering results on different datasets. '-' indicates that we do not have results, and 'N/A' indicates that the method cannot be run for the given dataset. Best results with respect to ACC, NMI, and SPE are bolded.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Shallow Affinity-Based</cell><cell></cell><cell></cell><cell>Neural</cell><cell cols="2">Doubly Stochastic</cell></row><row><cell>Dataset</cell><cell>Metric</cell><cell>SSC</cell><cell>EnSC</cell><cell>LSR</cell><cell cols="8">LRSC TSC SSC-OMP S 3 COMP DSC-Net DSC-Net-PP J-DSSC A-DSSC</cell></row><row><cell></cell><cell>ACC</cell><cell>.654</cell><cell>.652</cell><cell>.659</cell><cell>.662</cell><cell>.514</cell><cell>.780</cell><cell>.874</cell><cell>.691</cell><cell>.971</cell><cell>.924</cell><cell>.917</cell></row><row><cell>Yale-B</cell><cell>NMI SPE</cell><cell>.734 .217</cell><cell>.734 .218</cell><cell>.743 .869</cell><cell>.739 .875</cell><cell>.629 .217</cell><cell>.844 .179</cell><cell>-.203</cell><cell>.746 .881</cell><cell>.961 .038</cell><cell>.952 .080</cell><cell>.947 .080</cell></row><row><cell></cell><cell>NNZ</cell><cell>22.9</cell><cell>23.7</cell><cell>2413</cell><cell>2414</cell><cell>4.3</cell><cell>9.1</cell><cell>-</cell><cell>2414</cell><cell>22.1</cell><cell>14.5</cell><cell>14.4</cell></row><row><cell></cell><cell>ACC</cell><cell>.799</cell><cell>.801</cell><cell>.577</cell><cell>.567</cell><cell>.813</cell><cell>.411</cell><cell>-</cell><cell>.543</cell><cell>.751</cell><cell>.899</cell><cell>.922</cell></row><row><cell>COIL-40</cell><cell>NMI SPE</cell><cell>.940 .013</cell><cell>.930 .017</cell><cell>.761 .926</cell><cell>.736 .877</cell><cell>.916 .057</cell><cell>.605 .025</cell><cell>--</cell><cell>.743 .873</cell><cell>.887 .105</cell><cell>.963 .012</cell><cell>.967 .008</cell></row><row><cell></cell><cell>NNZ</cell><cell>3.1</cell><cell>6.2</cell><cell>2879</cell><cell>2880</cell><cell>4.5</cell><cell>2.9</cell><cell>-</cell><cell>2880</cell><cell>13.1</cell><cell>8.9</cell><cell>5.2</cell></row><row><cell></cell><cell>ACC</cell><cell>.996</cell><cell>.993</cell><cell>.734</cell><cell>.753</cell><cell>.941</cell><cell>.489</cell><cell>-</cell><cell>N/A</cell><cell>N/A</cell><cell>1.00</cell><cell>1.00</cell></row><row><cell>COIL-40</cell><cell>NMI</cell><cell>.998</cell><cell>.995</cell><cell>.868</cell><cell>.871</cell><cell>.981</cell><cell>.711</cell><cell>-</cell><cell>N/A</cell><cell>N/A</cell><cell>1.00</cell><cell>1.00</cell></row><row><cell>(Scattered)</cell><cell>SPE</cell><cell>.0002</cell><cell>0.00</cell><cell>.787</cell><cell>.816</cell><cell>.004</cell><cell>.056</cell><cell>-</cell><cell>N/A</cell><cell>N/A</cell><cell>.00006</cell><cell>.00003</cell></row><row><cell></cell><cell>NNZ</cell><cell>2.2</cell><cell>4.0</cell><cell>2879</cell><cell>2880</cell><cell>4.3</cell><cell>9.1</cell><cell>-</cell><cell>N/A</cell><cell>N/A</cell><cell>13.8</cell><cell>8.9</cell></row><row><cell></cell><cell>ACC</cell><cell>.704</cell><cell>.680</cell><cell>.492</cell><cell>.476</cell><cell>.723</cell><cell>.313</cell><cell>.789</cell><cell>.493</cell><cell>.635</cell><cell>.796</cell><cell>.824</cell></row><row><cell>COIL-100</cell><cell>NMI SPE</cell><cell>.919 .052</cell><cell>.901 .044</cell><cell>.753 .945</cell><cell>.733 .946</cell><cell>.904 .057</cell><cell>.588 .052</cell><cell>-.032</cell><cell>.752 .958</cell><cell>.875 .384</cell><cell>.943 .049</cell><cell>.946 .037</cell></row><row><cell></cell><cell>NNZ</cell><cell>3.1</cell><cell>6.8</cell><cell>7199</cell><cell>7200</cell><cell>3.6</cell><cell>3.0</cell><cell>-</cell><cell>7200</cell><cell>44.7</cell><cell>9.9</cell><cell>5.8</cell></row><row><cell></cell><cell>ACC</cell><cell>.954</cell><cell>.967</cell><cell>.642</cell><cell>.654</cell><cell>.915</cell><cell>.397</cell><cell>-</cell><cell>N/A</cell><cell>N/A</cell><cell>.961</cell><cell>.984</cell></row><row><cell>COIL-100</cell><cell>NMI</cell><cell>.991</cell><cell>.990</cell><cell>.846</cell><cell>.850</cell><cell>.975</cell><cell>.671</cell><cell>-</cell><cell>N/A</cell><cell>N/A</cell><cell>.992</cell><cell>.997</cell></row><row><cell>(Scattered)</cell><cell>SPE</cell><cell>.002</cell><cell>.004</cell><cell>.891</cell><cell>.905</cell><cell>.009</cell><cell>.055</cell><cell>-</cell><cell>N/A</cell><cell>N/A</cell><cell>.004</cell><cell>.001</cell></row><row><cell></cell><cell>NNZ</cell><cell>2.3</cell><cell>4.2</cell><cell>7199</cell><cell>7200</cell><cell>4.3</cell><cell>9.1</cell><cell>-</cell><cell>N/A</cell><cell>N/A</cell><cell>10.4</cell><cell>6.6</cell></row><row><cell></cell><cell>ACC</cell><cell>.537</cell><cell>.562</cell><cell>.462</cell><cell>.494</cell><cell>.661</cell><cell>.509</cell><cell>-</cell><cell>.456</cell><cell>.708</cell><cell>.732</cell><cell>.725</cell></row><row><cell>UMIST</cell><cell>NMI SPE</cell><cell>.718 .079</cell><cell>.751 .085</cell><cell>.645 .811</cell><cell>.662 .824</cell><cell>.829 .035</cell><cell>.680 .091</cell><cell>--</cell><cell>.611 .834</cell><cell>.848 .393</cell><cell>.858 .036</cell><cell>.851 .034</cell></row><row><cell></cell><cell>NNZ</cell><cell>4.9</cell><cell>6.6</cell><cell>574</cell><cell>575</cell><cell>3.7</cell><cell>2.9</cell><cell>-</cell><cell>575</cell><cell>23.2</cell><cell>5.1</cell><cell>4.8</cell></row><row><cell></cell><cell>ACC</cell><cell>.704</cell><cell>.806</cell><cell>.524</cell><cell>.531</cell><cell>.714</cell><cell>.401</cell><cell>-</cell><cell>N/A</cell><cell>N/A</cell><cell>.873</cell><cell>.888</cell></row><row><cell>UMIST</cell><cell>NMI</cell><cell>.834</cell><cell>.903</cell><cell>.701</cell><cell>.711</cell><cell>.855</cell><cell>.510</cell><cell>-</cell><cell>N/A</cell><cell>N/A</cell><cell>.939</cell><cell>.935</cell></row><row><cell>(Scattered)</cell><cell>SPE</cell><cell>.038</cell><cell>.029</cell><cell>.758</cell><cell>.804</cell><cell>.030</cell><cell>.294</cell><cell>-</cell><cell>N/A</cell><cell>N/A</cell><cell>.020</cell><cell>.018</cell></row><row><cell></cell><cell>NNZ</cell><cell>2.7</cell><cell>4.5</cell><cell>574</cell><cell>575</cell><cell>3.8</cell><cell>14.5</cell><cell>-</cell><cell>N/A</cell><cell>N/A</cell><cell>7.2</cell><cell>5.4</cell></row><row><cell></cell><cell>ACC</cell><cell>.774</cell><cell>.774</cell><cell>.709</cell><cell>.679</cell><cell>.783</cell><cell>.664</cell><cell>-</cell><cell>.758</cell><cell>.845</cell><cell>.785</cell><cell>.790</cell></row><row><cell>ORL</cell><cell>NMI SPE</cell><cell>.903 .240</cell><cell>.903 .268</cell><cell>.856 .884</cell><cell>.834 .874</cell><cell>.896 .272</cell><cell>.832 .305</cell><cell>--</cell><cell>.878 .885</cell><cell>.915 .421</cell><cell>.906 .176</cell><cell>.910 .159</cell></row><row><cell></cell><cell>NNZ</cell><cell>12.3</cell><cell>14.4</cell><cell>399</cell><cell>400</cell><cell>9.3</cell><cell>5.0</cell><cell>-</cell><cell>400</cell><cell>20.7</cell><cell>10.8</cell><cell>9.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Shallow Affinity-Based</cell><cell cols="2">Doubly Stochastic</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Dataset</cell><cell cols="6">Metric SSC EnSC TSC SSC-OMP S 3 COMP</cell><cell>A-DSSC</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ACC</cell><cell>.963</cell><cell>.963</cell><cell>.980</cell><cell>.574</cell><cell>.963</cell><cell>.990</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">MNIST</cell><cell>NMI</cell><cell>.915</cell><cell>.915</cell><cell>.946</cell><cell>.624</cell><cell>-</cell><cell>.971</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">(Scattered)</cell><cell>SPE</cell><cell>.098</cell><cell>.098</cell><cell>.028</cell><cell>.301</cell><cell>.301</cell><cell>.017</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>NNZ</cell><cell>39.3</cell><cell>40.2</cell><cell>18.1</cell><cell>25.4</cell><cell>-</cell><cell>14.7</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ACC</cell><cell>.638</cell><cell>.644</cell><cell>.698</cell><cell>.304</cell><cell>-</cell><cell>.744</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">EMNIST</cell><cell>NMI</cell><cell>.745</cell><cell>.746</cell><cell>.785</cell><cell>.385</cell><cell>-</cell><cell>.832</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">(Scattered)</cell><cell>SPE</cell><cell>.173</cell><cell>.174</cell><cell>.106</cell><cell>.216</cell><cell>-</cell><cell>.100</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>NNZ</cell><cell>22.3</cell><cell>24.7</cell><cell>6.3</cell><cell>3.7</cell><cell>-</cell><cell>17.5</cell><cell></cell><cell></cell></row></table><note>. Large-scale subspace clustering results. '-' indicates that we do not have results for the method. Best results are bolded.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>followed by application of a proximal operator. Letting C p be the intermediate value of Cp after the gradient descent step, Cp is updated as the solution to min</figDesc><table><row><cell>Cp</cell><cell>η1 2</cell><cell>[Cp + Cq] − η2A</cell><cell>2</cell></row></table><note>F + η3 Cp + Cq 1</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Clustering accuracy comparison with neural network models. '-' indicates that we do not have results for the method. We take results directly from the corresponding papers (besides DSC-Net on UMIST, which we run). Note that this is not under our main paper evaluation framework, as we cannot control postprocessing of neural models<ref type="bibr" target="#b13">[14]</ref>.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Neural</cell><cell></cell><cell></cell><cell cols="2">Doubly Stochastic</cell></row><row><cell>Dataset</cell><cell cols="2">DSC-Net [20] DASC [58]</cell><cell cols="5">kSCN[57] S 2 ConvSCN [56] MLRDSC-DA [1] J-DSSC (Ours) A-DSSC (Ours)</cell></row><row><cell>Yale-B</cell><cell>.973</cell><cell>.986</cell><cell>-</cell><cell>.985</cell><cell>.992</cell><cell>.924</cell><cell>.917</cell></row><row><cell>COIL-100</cell><cell>.690</cell><cell>-</cell><cell>-</cell><cell>.733</cell><cell>.793</cell><cell>.961</cell><cell>.984</cell></row><row><cell>ORL</cell><cell>.860</cell><cell>.883</cell><cell>-</cell><cell>.895</cell><cell>.897</cell><cell>.785</cell><cell>.790</cell></row><row><cell>UMIST</cell><cell>.708</cell><cell>.769</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>.873</cell><cell>.888</cell></row><row><cell>MNIST</cell><cell>-</cell><cell>-</cell><cell>.871</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>.990</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Statistics and properties of datasets.</figDesc><table><row><cell>Dataset</cell><cell cols="3"># Images (n) Dimension # Classes</cell></row><row><cell>Yale-B</cell><cell>2,414</cell><cell>48 × 42</cell><cell>38</cell></row><row><cell>COIL-40</cell><cell>2,880</cell><cell>32 × 32</cell><cell>40</cell></row><row><cell>COIL-100</cell><cell>7,200</cell><cell>32 × 32</cell><cell>100</cell></row><row><cell>UMIST</cell><cell>575</cell><cell>32 × 32</cell><cell>20</cell></row><row><cell>ORL</cell><cell>400</cell><cell>32 × 32</cell><cell>40</cell></row><row><cell>MNIST</cell><cell>70,000</cell><cell>28 × 28</cell><cell>10</cell></row><row><cell>EMNIST</cell><cell>145,600</cell><cell>28 × 28</cell><cell>26</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .</head><label>6</label><figDesc>Parameter settings for J-DSSC and A-DSSC.</figDesc><table><row><cell></cell><cell></cell><cell>J-DSSC</cell><cell></cell><cell></cell><cell>A-DSSC</cell><cell></cell></row><row><cell>Dataset</cell><cell>η 1</cell><cell>η 2</cell><cell>η 3</cell><cell>η 1</cell><cell>η 2</cell><cell>η 3</cell></row><row><cell>Yale-B</cell><cell>.25</cell><cell>.2</cell><cell>0</cell><cell>.5</cell><cell>.1</cell><cell>0</cell></row><row><cell>COIL-40</cell><cell>25</cell><cell>.01</cell><cell>.1</cell><cell>25</cell><cell>.001</cell><cell>0</cell></row><row><cell>COIL-40 (Scattered)</cell><cell>.25</cell><cell>.2</cell><cell>0</cell><cell>50</cell><cell>.001</cell><cell>0</cell></row><row><cell>COIL-100</cell><cell>25</cell><cell>.01</cell><cell>.1</cell><cell>50</cell><cell>.0005</cell><cell>0</cell></row><row><cell>COIL-100 (Scattered)</cell><cell>.25</cell><cell>.1</cell><cell>0</cell><cell>.1</cell><cell>.025</cell><cell>0</cell></row><row><cell>UMIST</cell><cell>1</cell><cell>.05</cell><cell>0</cell><cell>.5</cell><cell>.05</cell><cell>0</cell></row><row><cell>UMIST (Scattered)</cell><cell>.01</cell><cell>.2</cell><cell>0</cell><cell>.5</cell><cell>.01</cell><cell>0</cell></row><row><cell>ORL</cell><cell>1</cell><cell>.1</cell><cell>.1</cell><cell>1</cell><cell>.05</cell><cell>0</cell></row><row><cell>MNIST (Scattered)</cell><cell cols="3">N/A N/A N/A</cell><cell>10</cell><cell>.001</cell><cell>0</cell></row><row><cell>EMNIST (Scattered)</cell><cell cols="3">N/A N/A N/A</cell><cell>50</cell><cell>.001</cell><cell>0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 .</head><label>7</label><figDesc>Parameters searched over for different methods. Method Parameters and Variant SSC [10] γ ∈ {1, 5, 10, 25, 50, 100}, noisy data variant EnSC [52] γ ∈ {.1, 1, 5, 10, 50, 100}, λ ∈ {.9, .95} {2, 3, . . . , 14, 15} SSC-OMP [53] kmax ∈ {2, 3, . . . , 14, 15} J-DSSC (Ours) η1 ∈ {.01, .25, 1, 25}, η2 ∈ {.01, .05, .1, .2}, η3 ∈ {0, .1} A-DSSC (Ours) η1 ∈ {.1, 1, 10, 25, 50}, η2 ∈ {.0005, .001, .01, .025, .05, .1}, η3 = 0</figDesc><table><row><cell>LSR [30]</cell><cell>λ ∈ {.01, .1, .5, 1, 10, 50, 100},</cell></row><row><cell></cell><cell>diag(C) = 0 variant</cell></row><row><cell>LRSC [42]</cell><cell>τ ∈ {.1, 1, 10, 50, 100, 500, 1000}, α =</cell></row><row><cell></cell><cell>τ /2, noisy data and relaxed constraint vari-</cell></row><row><cell></cell><cell>ant</cell></row><row><cell>TSC [16]</cell><cell>q ∈</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">• D4, |C| is an LSR affinity at η1 = 1 for a union of ten 5dimensional subspaces in R 15 , as in<ref type="bibr" target="#b50">[51]</ref>. We take n = 4000 and γ = .01.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep subspace clustering with data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahdi</forename><surname>Abavisani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Naghizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Linear-time subspace clustering via bipartite graph modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacov</forename><surname>Hel-Or</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="2234" to="2246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The ζ (2) limit in the random assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aldous</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Random Structures &amp; Algorithms</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Smooth and sparse optimal transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivien</forename><surname>Seguy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Rolet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Invariant scattering convolution networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stéphane</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Stochastic sparse subspace clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Guang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Emnist: Extending mnist to handwritten letters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeed</forename><surname>Afshar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tapson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andre</forename><surname>Van Schaik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Some theorems on abstract graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirac</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the London Mathematical Society</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="1952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Greedy feature selection for subspace clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Eva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Aswin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">G</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baraniuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2487" to="2517" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sparse subspace clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Elhamifar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">René</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sparse subspace clustering: Algorithm, theory, and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Elhamifar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">René</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2765" to="2781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">From few to many: Illumination cone models for face recognition under variable lighting and pose. IEEE transactions on pattern analysis and machine intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Athinodoros</forename><forename type="middle">S</forename><surname>Georghiades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Kriegman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Characterising virtual eigensignatures for general purpose face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nigel</forename><forename type="middle">M</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Allinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Face Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A critique of self-expressive deep subspace clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Haeffele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rene</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The doubly stochastic single eigenvalue problem: A computational approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Harlev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Charles R Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Experimental Mathematics</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Robust subspace clustering via thresholding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Heckel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Bölcskei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Matrix analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles R Johnson</forename><surname>Horn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Cambridge university press</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Spectra of convex hulls of matrix groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Jankowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Charles R Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linear Algebra and its Applications</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficient dense subspace clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="461" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep subspace clustering networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Entropy and correlation: Some comments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tarald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kvalseth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="517" to="519" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Doublystochastic normalization of the gaussian kernel is robust to heteroskedastic noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Landa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Ronald R Coifman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kluger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.00402</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Membership representation for detecting block-diagonal structure in low-rank or sparse subspace clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsik</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeogjin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nojun</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1648" to="1656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Structured sparse subspace clustering: A joint affinity learning and subspace clustering framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Guang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">René</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2988" to="3001" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">On the limited memory bfgs method for large scale optimization. Mathematical programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nocedal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="503" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Robust recovery of subspace structures by low-rank representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangcan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ju</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="171" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Quadratically regularized optimal transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Manns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Meyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Mathematics &amp; Optimization</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Subspace clustering by block diagonal representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canyi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="487" to="501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Robust and efficient subspace segmentation via least squares regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can-Yi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhong-Qiu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De-Shuang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Alternating proximal gradient method for convex minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqian</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Scientific Computing</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Graph connectivity in sparse subspace clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Behrooz</forename><surname>Nasihatkon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="2137" to="2144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Columbia object image library (coil-100)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sameer A Nene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Nayar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Murase</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
		<respStmt>
			<orgName>Columbia University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The constrained laplacian rank algorithm for graphbased clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiping</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Michael I Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirtieth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Scalable sparse subspace clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="430" to="437" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Computational optimal transport: With applications to data science. Foundations and Trends® in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Peyré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Cuturi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="355" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Optimal approximation of doubly stochastic matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikitas</forename><surname>Rontsis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">J</forename><surname>Goulart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS, 2020. 4, 5</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Parameterisation of a stochastic model for human face identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ferdinando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><forename type="middle">C</forename><surname>Samaria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 1994 IEEE workshop on applications of computer vision</title>
		<meeting>1994 IEEE workshop on applications of computer vision</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1994" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="888" to="905" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A geometric analysis of subspace clustering with outliers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahdi</forename><surname>Soltanolkotabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><forename type="middle">J</forename><surname>Candes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2195" to="2238" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Subspace clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">René</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="52" to="68" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Low rank subspace clustering (lrsc)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">René</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrike</forename><surname>Von</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luxburg</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">A tutorial on spectral clustering. Statistics and computing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="395" to="416" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning a bistochastic data similarity matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnd Christian</forename><surname>Konig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE International Conference on Data Mining</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="551" to="560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Structured doubly stochastic matrix for graph based clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiping</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International conference on Knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD International conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Graph connectivity in noisy sparse subspace clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yining</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aarti</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Principal component analysis. Chemometrics and intelligent laboratory systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svante</forename><surname>Wold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Esbensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Geladi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="37" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Efficient semidefinite spectral clustering via lagrange duality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanzi</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3522" to="3534" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Low-rank doubly stochastic matrix decomposition for cluster analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jukka</forename><surname>Corander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erkki</forename><surname>Oja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="6454" to="6478" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Subspace clustering via learning an adaptive low-rank graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengli</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongze</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbin</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3716" to="3728" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A divide-and-conquer framework for large-scale subspace clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Donnat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">René</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 50th Asilomar Conference on Signals, Systems and Computers</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Oracle based active set algorithm for scalable elastic net subspace clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Guang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">René</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Scalable sparse subspace clustering by orthogonal matching pursuit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">René</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A unifying approach to hard and probabilistic clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Zass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amnon</forename><surname>Shashua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tenth IEEE International Conference on Computer Vision (ICCV&apos;05</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="294" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Doubly stochastic normalization for spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Zass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amnon</forename><surname>Shashua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Selfsupervised convolutional subspace clustering network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Guang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianbiao</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Scalable deep k-subspace clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrtash</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Deep adversarial subspace clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunqing</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
