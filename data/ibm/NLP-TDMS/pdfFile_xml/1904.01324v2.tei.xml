<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Monocular 3D Human Pose Estimation by Generation and Ordinal Ranking CVAE Decoder Ordinal Maps Joint Heatmaps Ordinal Matrix RGB Image MultiPoseNet … Generated 3D Samples</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Sharma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavan</forename><surname>Teja Varigonda</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Indian Institute of Technology</orgName>
								<address>
									<addrLine>3 Axogyan AI</addrLine>
									<settlement>Saarbrücken Bombay Bangalore</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prashast</forename><surname>Bindal</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Indian Institute of Technology</orgName>
								<address>
									<addrLine>3 Axogyan AI</addrLine>
									<settlement>Saarbrücken Bombay Bangalore</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Sharma</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Jain</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Indian Institute of Technology</orgName>
								<address>
									<addrLine>3 Axogyan AI</addrLine>
									<settlement>Saarbrücken Bombay Bangalore</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Monocular 3D Human Pose Estimation by Generation and Ordinal Ranking CVAE Decoder Ordinal Maps Joint Heatmaps Ordinal Matrix RGB Image MultiPoseNet … Generated 3D Samples</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>= &gt; &gt; &gt; &lt; = &lt; &lt; &lt; &gt; = &gt; &lt; &gt; = &lt;</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Oracle 2DPoseNet</head><p>OrdinalNet <ref type="figure">Figure 1</ref>: A schematic illustration of our modular framework. We tackle the ambiguity in the 2D-to-3D mapping by training a CVAE to generate 3D-pose samples conditioned on the 2D-pose, that are scored and weighted-averaged using joint-ordinal relations, which are regressed together with the 2D-pose. Our upper-bound performance is obtained by using an Oracle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Monocular 3D human-pose estimation from static images is a challenging problem, due to the curse of dimensionality and the ill-posed nature of lifting 2D-to-3D. In this paper, we propose a Deep Conditional Variational Autoencoder based model that synthesizes diverse anatomically plausible 3D-pose samples conditioned on the estimated 2D-pose. We show that CVAE-based 3D-pose sample set is consistent with the 2D-pose and helps tackling the inherent ambiguity in 2D-to-3D lifting. We propose two strategies for obtaining the final 3D pose-(a) depthordering/ordinal relations to score and weight-average the candidate 3D-poses, referred to as OrdinalScore, and (b) with supervision from an Oracle. We report close to stateof-the-art results on two benchmark datasets using Ordi-nalScore, and state-of-the-art results using the Oracle. We also show that our pipeline yields competitive results without paired image-to-3D annotations. The training and evaluation code is available at https://github.com/ ssfootball04/generative_pose.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Accurate 3D human-pose estimation from a monocular RGB image finds applications to robotics, vir-tual/augmented reality, surveillance, and human computer interaction. The diverse variations in background, clothing, pose, occlusions, illumination, and camera parameters in real-world scenarios makes it a challenging problem. The popular 3D-pose annotated datasets do not cover these variations appropriately. Recent advancements in real-world 2D-pose estimation <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b41">42]</ref> has led to several multi-stage architectures, where the 3D-pose is regressed either from both the image features and an intermediate 2D representation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b44">45]</ref>, or only the estimated 2D-pose <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b45">46]</ref>. Unfortunately, regression based approaches using only the estimated 2D-pose, ignore the ambiguity in lifting 2D human-pose to 3D: an inherently illposed problem. Motivated by this shortcoming, we propose to learn a generative 3D-pose model conditioned on the corresponding 2D-pose that affords sampling diverse samples from the learnt 3D-pose distribution. To the best of our knowledge, we are the first to employ a Deep Conditional Variational Autoencoder <ref type="bibr" target="#b31">[32]</ref> (CVAE for short) for 2D-to-3D generative human-pose modeling and demonstrate its advantages over direct regression based approaches. We also show that our generative 2D-to-3D module can be trained on a separate MoCap dataset that doesn't have any intersection with the evaluation image-to-3D dataset, and still performs reasonably well. Therefore, our modular approach tackles the infeasibility (or high cost) of obtaining 3D-pose annotation for images in real-world and works well with separately collected 2D-pose annotations of real-world images and indoor motion capture data <ref type="bibr" target="#b42">[43]</ref>.</p><p>Our pipeline is depicted in <ref type="figure">Figure 1</ref>. First, the 2DPoseNet head of a deep convolutional network backbone, C, estimates 2D pose,P 2D , from a monocular RGB image, I. The estimated 2D pose,P 2D , and a latent code z, sampled from a prior distribution p(z) ∼ N (0, 1), are fed to the decoder of the MultiPoseNet CVAE to sample a 3D pose,P k 3D . Multiple samples, z k ∈ {z 1 , z 2 . . . z K }, from p(z) yield a diverse set of 3D pose samples, S = {P k 3D : k ∈ {1, 2, . . . K}}, consistent withP 2D . Then we employ pairwise depth ordering of body-joints encoded in the estimated joint-ordinal relation matrix,M , obtained from the OrdinalNet head of C, to obtain scores, {f (P k 3D ) : k ∈ {1, 2 . . . K}} for the elements of S. These scores are finally fed to Softmax operator to obtain a probability distribution over S, reflecting the consistency of the 3D-pose samples to the predicted ordinal relations. The final 3D pose,P 3D , is computed as the expectation of this distribution. Moreover, in order to estimate the upper-bound performance of our generative model, we also report the accuracy w.r.t. the sample,P Oracle</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3D</head><p>, that is the closest match to the ground truth 3D-pose, P 3D . The Oracle upper-bound outperforms all existing state-of-the-art methods, without leveraging recently introduced ordinal dataset, temporal information, or end-to-end training of the multi-stage architectures. This observation supports the strength of our CVAE-based generative model for 2D-to-3D lifting.</p><p>A summary of our contributions is as follows -</p><p>• We tackle the inherent ill-posed problem of lifting 2D-to-3D human-pose by learning a deep generative model that synthesizes diverse 3D-pose samples conditioned on the estimated 2D-pose. • We employ CVAE for 3D human-pose estimation for the first time. • We derive joint-ordinal depth relations from an RGB image and employ them to rank 3D-pose samples. • We show that the oracle-based pose sample obtained from our proposed generative model achieves stateof-the-art results on two benchmark datasets, Hu-man3.6M <ref type="bibr" target="#b10">[11]</ref> and Human-Eva <ref type="bibr" target="#b28">[29]</ref>. • We show competitive performance over Baseline even when our 2D-to-3D module is trained on a separate MoCap dataset with no images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Lifting 2D to 3D Our approach belongs to the large body of work that obtains 3D-pose from estimated 2D-pose. In <ref type="bibr" target="#b26">[27]</ref>, a set of 3D-shape bases, pre-trained using 3D mocap data <ref type="bibr" target="#b6">[7]</ref>, is used to learn a sparse representation of human 3D-pose by optimising for reprojection error. It was extended by <ref type="bibr" target="#b46">[47]</ref> via convex relaxation to address bad initialisation in this scheme. Anatomical constraints to regularize the predicted poses w.r.t. limb lengths were introduced in <ref type="bibr" target="#b39">[40]</ref>. Further use of anatomical constraints in the form of joint-angle-limits and learned pose priors was proposed in <ref type="bibr" target="#b0">[1]</ref> to extend <ref type="bibr" target="#b26">[27]</ref>. In <ref type="bibr" target="#b19">[20]</ref>, Euclidean inter-joint distance matrix was used to represent 2D and 3D poses with multidimensional scaling to obtain 3D-pose from the predicted 3D distance matrix. Some approaches, <ref type="bibr" target="#b2">[3]</ref>, estimate the 3D-pose and shape by fitting a 3D statistical model <ref type="bibr" target="#b17">[18]</ref> to 2D-pose and leverage inter-penetration constraints. Different from all the previous approaches we employ CVAE to implicitly learn the anatomical constraints and sample 3Dpose candidates.</p><p>The method in <ref type="bibr" target="#b12">[13]</ref>, builds upon the framework of <ref type="bibr" target="#b2">[3]</ref> to describe a model that estimates the shape, underlying 3Dpose and camera parameters using a re-projection and adversarial loss, which can be trained with 2D-pose datasets and unpaired MoCap datasets. In <ref type="bibr" target="#b18">[19]</ref>, a baseline model is proposed that uses a simple fully connected linear network for this task which surprisingly outperforms past approaches. Unlike these discriminative approaches that predict only one 3D-pose from a given 2D-pose, we generate a diverse sample set of 3D-poses.</p><p>Hypothesis Generation Some previous approaches sample multiple 3D-poses via heuristics. The work in <ref type="bibr" target="#b16">[17]</ref>, finds the nearest neighbors in a learned latent embedding of human images to estimate the 3D-pose. The approaches in <ref type="bibr" target="#b15">[16]</ref> and <ref type="bibr" target="#b30">[31]</ref>, enumerate 3D-poses using "kinematic-flipping" of the 3D joints, for estimation and tracking, respectively. The Bayesian framework from <ref type="bibr" target="#b29">[30]</ref> employs a latent-variable generative model with a set of HOG-based 2D part detectors and performs inference using evolutionary algorithms. More recently, <ref type="bibr" target="#b4">[5]</ref> retrieves 3D-pose using nearest neighbor search. <ref type="bibr" target="#b11">[12]</ref> uses the pose prior model of <ref type="bibr" target="#b0">[1]</ref> to generate multiple hypothesis from a seed 3D-pose, while <ref type="bibr" target="#b38">[39]</ref> use "skeleton maps" at different scales to regress 3D-pose hypothesis. Unlike the previous methods, our CVAE based generative model implicitly learns an anatomically consistent pose prior conditioned on the input 2D-pose. It affords efficient sampling of a set of candidate 3D-poses without requiring expensive MCMC or graphical model inference or an existing MoCap library. Also, it doesn't need additional image features or structural cues. Closest to our approach are prior arts that employ generative models for hand-pose estimation. In <ref type="bibr" target="#b32">[33]</ref>, one-to-one correspondence is assumed between hand-pose samples in different modalities-RGB, Depth, 2D-pose &amp; 3D-pose-and a joint latent space is learned via multi-modal VAE. Unfortunately, this assumption between 2D-and-3D poses ignores the inherent ambi-guity in 2D-to-3D lifting, while, we explicitly tackle it via CVAE-based probabilistic framework. The work in <ref type="bibr" target="#b3">[4]</ref> generates multiple hand-poses from depth-map to address the prediction uncertainty due to occlusions/missing-values in the input depth-map and uses Maximum-Expected-Utility (MEU) to obtain a pointwise prediction from the generated samples. We use CVAE for generation and employ geometry-inspired ordinal scoring to score and merge multiple samples. <ref type="bibr" target="#b37">[38]</ref> learns a probabilistic mapping from depth-map to 3D-pose, to exploit unlabeled data, which is not provably ill-posed. We, however, employ CVAE inspired probabilistic framework to tackle the provable illposed nature of 2D-to-3D pose lifting.</p><p>Ordinal Relations Ordinal relations have previously been explored to estimate depth <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b5">6]</ref> and reflectance <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b20">21]</ref>. Recently, <ref type="bibr" target="#b23">[24]</ref> and <ref type="bibr" target="#b27">[28]</ref> used 2D datasets with ordinal annotations as weak supervision for monocular 3D-pose estimation by imposing a penalty for violation of ordinal depth constraints. Our ordinal prediction network is similar in spirit to <ref type="bibr" target="#b25">[26]</ref> that uses a Structural-SVM conditioned on HOG features to predict pose-bits that capture qualitative attributes to facilitate 3D-pose prediction and image retrieval. Unlike <ref type="bibr" target="#b25">[26]</ref>, we leverage deep-networks to jointly predict the 2D-pose and depth-ordinal, and generate a diverse sample set of 3D-poses. Concurrent with our work, <ref type="bibr" target="#b40">[41]</ref> also predict depth ranking and regress 3D-pose from 2D-pose with depth rankings in a coarse-to-fine network. We differ in the formulation of predicting ordinals as spatial maps, which co-locate with the 2D-pose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Approach</head><p>In this Section, we describe the proposed approach. Sec. 3.1 discusses 2DPoseNet to obtain 2D-pose from an input RGB image followed by Sec. 3.2 that describes our novel MultiPoseNet, for generating multiple 3D-pose samples conditioned on the estimated 2D-pose. In Sec. 3.3, we discuss OrdinalNet to obtain joint-ordinal relations from the image and the estimated 2D-pose. Finally, Sec. 3.4 and 3.5 describe our strategies for predicting the final 3D-pose from the generated samples : (a) by scoring the generated sample set using ordinal relations, referred to as OrdinalScore, and (b) by using supervision from an Oracle with access to the ground truth 3D-pose, referred to as OracleScore.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">2DPoseNet: 2D-Pose from Image</head><p>We use the Stacked Hourglass Model <ref type="bibr" target="#b21">[22]</ref> with two stacks, as our backbone C. The 2DPoseNet head applies a 1x1 convolution to the intermediate feature representations to regress per-joint heatmaps (Gaussian bumps at target location), from which the predicted 2D pose in pixel coordinates,P 2D , is obtained using Argmax operator. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">MultiPoseNet: Multiple 3D-Poses from 2D</head><p>Recently, Variational Auto-encoders and Generative Adversarial Networks have demonstrated tremendous success in density estimation and synthetic sample generation. Specifically, CVAEs can generate realistic samples conditioned on input variables which is well suited for multimodal regression mappings <ref type="bibr" target="#b31">[32]</ref>. Therefore, we extend the Baseline regression model from <ref type="bibr" target="#b18">[19]</ref> into a CVAE to tackle the inherent multi-modality of the 2D-to-3D pose mapping and sample an accurate and diverse 3D-pose candidate set</p><formula xml:id="formula_0">S = {P k 3D : k ∈ {1, 2, .</formula><p>. . K}} conditioned on the estimated 2D-poseP 2D . We observe that S has diverse anatomically plausible samples and contains a close match to the actual ground-truth, P 3D . The detailed architecture for Mul-tiPoseNet is depicted in <ref type="figure" target="#fig_0">Figure 2</ref>.</p><p>Training The 3D-pose generating CVAE <ref type="bibr" target="#b31">[32]</ref> consists of • Recognition Network, or Encoder : Enc(P 3D ,P 2D ), which operates on an input 3D-pose P 3D and a condi-tionP 2D to output the mean and diagonal covariance for the posterior q(ẑ|P 3D ,P 2D ). • Decoder : Dec(ẑ,P 2D ), which reconstructs the ground truth P 3D by taking as input a latentẑ sampled from the posterior q(ẑ|P 3D ,P 2D ) and the condition 2D-poseP 2D .</p><p>During training, we optimize the following:</p><formula xml:id="formula_1">L CV AE = λ 1 KL(q(ẑ|P 3D ,P 2D )||p(z|P 2D )) (1) + λ 2 E z∼q(ẑ|P 3D ,P 2D ) ||P 3D − Dec(ẑ,P 2D )|| 2 2 ,</formula><p>where the prior distribution p(z|P 2D )) is assumed to be N (0, I), and KL(x||y) is the Kullback-Leibler divergence with λs used as hyper-parameters to weight the losses. The expectation in the second term for the reconstruction loss is taken over K train number of samples. At inference time, the Encoder network is discarded, and z is drawn from the prior p(z) ∼ N (0, I), which introduces inconsistency between the prediction and training pipelines. To remedy this, we set the Encoder equal to the prior network p(z) ∼ N (0, I), that leads to the Gaussian Stochastic Neural Network framework, or GSNN, proposed in <ref type="bibr" target="#b31">[32]</ref>. Combining the two we get a hybrid training objective, weighted with α:</p><formula xml:id="formula_2">L GSN N = E z∼N (0,1) ||P 3D − Dec(z,P 2D )|| 2 2 (2) L hybrid = αL CV AE + (1 − α)L GSN N ,<label>(3)</label></formula><p>Inference We sample z ∼ N (0, 1), and feed (z,P 2D ) to the</p><formula xml:id="formula_3">Decoder, to obtain S test = {P k 3D : k ∈ {1, 2, . . . , K test }}.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">OrdinalNet: Image to Joint-Ordinal Relations</head><p>The backbone architecture for OrdinalNet is same as our 2DPoseNet i.e. C. In order to obtain joint-ordinal relations, we augment C with two additional hourglass stacks. For each human-body joint location j ∈ {1, 2, . . . , N }, three ordinal maps (ÔM 1j ,ÔM 2j , andÔM 3j ) are predicted to capture the lesser than, greater than and equal depth relations between joint j and all other joints i ∈ {1, 2, . . . , N }. The ground-truth ordinal maps are generated so that for each joint j there is a Gaussian peak for joint i ∈ {1, 2, . . . , N } in one of the three ordinal maps ( OM 1j , OM 2j , and OM 3j ), depending on the depth relation between joint i and joint j. We combine the intermediate feature representations and 2D-pose heatmaps from backbone C and 2DPoseNet as the input, and use L2 loss over predicted ordinal maps, for training our Or-dinalNet. We post-process our estimated ordinal relations via non-maximal suppression on the predicted ordinal maps and associate each peak to its nearest joint-location, which are finally converted into a 16 × 16 joint-ordinal relation matrixM . The relation between depths D i , D j of joints i, j ∈ {1, 2, . . . , N } and ground-truth matrix M is:</p><formula xml:id="formula_4">M ij =    1 : D i − D j &gt; 0 2 : D i − D j &lt; 0 3 : D i − D j ≈ 0</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">OrdinalScore: Scoring and Aggregating Generated 3D samples</head><p>So far we have generated a diverse set of estimated 3D-poses fromP 2D only. Next, we seek motivation from the fact that under orthogonal camera projection with constant bone length constraint 2D-pose and joint-ordinal relations between keypoints can almost resolve the true 3Dpose <ref type="bibr" target="#b35">[36]</ref>. The estimated ordinal matrixM is used to assign scores to each of the samplesP k 3D ∈ S by the scoring function:</p><formula xml:id="formula_5">f (P k 3D ) = i,j 1(M ij == g(P k 3D ) ij )<label>(4)</label></formula><p>where 1(condition) is an indicator function, where g(P k 3D ) is the function that computes the 16×16 ordinal matrix for a given 3D-pose and g(P k 3D ) ij represents the ordinal relation of joint i and j.</p><p>The set of scores for the sampled 3D-poses obtained from an image, F = {f (P k 3D ) : k ∈ {1, 2, . . . |S|}}, is passed through a Softmax operator parameterized by temperature T to obtain a probability distribution function,</p><formula xml:id="formula_6">p(P k 3D ) = e T f (P k 3D ) / k e T f (P k 3D )</formula><p>. The final outputP 3D is computed as the expectation over the candidates-</p><formula xml:id="formula_7">P 3D = |S| k p(P k 3D ).P k 3D<label>(5)</label></formula><p>The temperature-based Softmax affords a fine control over the contribution strength of high-score samples vs. the lowscoring samples towards the final aggregation, which makes it robust to noisy pose candidates with respect to the predicted ordinal matrixM .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Supervision from an Oracle</head><p>The upper-bound accuracy for our approach is given by choosing the closest sample,P oracle</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3D</head><p>, to the ground-truth, P 3D , from S using an Oracle that has access to P 3D .</p><formula xml:id="formula_8">P oracle 3D = argmin s∈S P 3D − s 2<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>This section discusses the empirical evaluation of the proposed approach. First, we describe the benchmarks that we employed for quantitative evaluation, and provide some important implementation details of our approach. Then, we present quantitative results and compare our method with the state-of-the-art, and provide ablation studies to analyze the performance of our generative model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We make use of the following datasets for training various modules of our pipeline : CMU Mocap motion capture dataset consists of diverse 3D-poses with 144 different subjects performing different actions. We obtain 2D projections from the 3D skeletons using virtual cameras from multiple views, with assumed intrinsic parameters. We employ the obtained 2D-to-3D pose data to train MultiPoseNet and the Baseline model from <ref type="bibr" target="#b18">[19]</ref> for experiments under unpaired setting, while 2DPoseNet and OrdinalNet are trained on Human3.6M. Therefore, effectively we train our networks without using any image-to-3D ground-truth data. Human3.6M dataset consists of 3.6 million 3D-poses. It consists of videos and MoCap data of 5 female and 6 male subjects, captured from 4 different viewpoints while they are performing common activities (talking on the phone, walking, greeting, eating, etc.).</p><p>HumanEva-I is a small dataset containing 3 subjects (S1, S2, S3) with 3 camera views and fewer actions than Hu-man3.6M. This is a standard dataset for 3D-pose estimation used for benchmarking in previous works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>Data Pre-processing: We take a tight 224 × 224 crop around the person in the input RGB image, I, using groundtruth bounding boxes. Following <ref type="bibr" target="#b18">[19]</ref>, we process the 3Dposes in camera coordinates and apply standard normalization to the 2D-pose inputs and 3D-pose outputs by subtracting the mean and dividing by the standard deviation, and zero-center the 3D-pose around the hip joint. The 2D-pose contains N=16, and the 3D-pose contains N=17 and N=16 joints for Human3.6M and HumanEva-I respectively. 2DPoseNet: We use publicly available Stacked-Hourglass pretrained on MPII <ref type="bibr" target="#b1">[2]</ref> as backbone C and 2DPoseNet, and finetune on Human3.6M and HumanEva-I, following <ref type="bibr" target="#b18">[19]</ref>. MultiPoseNet: Its architecture is based on the Baseline model in <ref type="bibr" target="#b18">[19]</ref> (details in supplementary material). At training time, the expectation in Eq.1 is estimated using K train = 10 samples. λ 1 , λ 2 and α are set to 10, 100, and 0.5 respectively. The network is trained for 200 epochs using Adam <ref type="bibr" target="#b13">[14]</ref>, starting with a learning rate of 2.5e-4 with exponential decay and mini-batches size of 256. At test time, we generate K test = 200 3D-pose candidates to get a diverse sample set S. MultiPoseNet takes 10 hours to train on a Titan 1080ti GPU. OrdinalNet: We freeze the weights of our backbone C and 2DPoseNet after fine-tuning, and train the OrdinalNet module using ground-truth ordinal maps for 60 epochs with standard L2 Loss. OrdinalNet takes 12 hours to train, on a Titan 1080ti GPU. OrdinalScore The temperature, T , is obtained using crossvalidation and set to 0.9 for ground truth ordinals, and 0.3 for predicted ordinals. In practice, OrdinalNet can sometimes predict contradictory relations i.eM ij =M ji ,M ii = 3; we resolve it by setting the diagonal entries ofM to 3 and mask out elements whereM ij =M ji during scoring. Note that for Human3.6M, the ordinal relations w.r.t the extra joint in the 3D-pose are not taken into account by the scoring function in Eq.4. Runtime Details The run-time for different modules of our pipeline are -OrdinalNet: 20ms/image, MultiPoseNet: 0.5ms/sample, we take 200 samples/image for inference. The entire pipeline runs at 10 fps on a commodity graphics card, which is slightly worse than other real-time methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Quantitative Evaluation</head><p>In this sub-section, we report the results of our model and compare it against the prior state-of-the-art on Human3.6M and HumanEva-I dataset. We report three evaluation metrics to demonstrate the benefits of our approach: PRED Ordinals: Uses the OrdinalScore strategy with the ordinal relations predicted by OrdinalNet.</p><p>GT Ordinals: Uses the OrdinalScore strategy with the ground truth ordinal relations.</p><p>Oracle: Uses the Oracle for final prediction, which gives the best results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Evaluation on Human3.6M</head><p>Following the literature, we use two standard protocols to train and evaluate our results. Protocol-1: The training set consists of 5 subjects (S1, S5, S6, S7, S8), while the test set includes 2 subjects (S9, S11). The original 50FPS frame rate is down-sampled to 10 FPS and the evaluation is carried out on sequences coming from all 4 cameras and all trials. The reported error metric is Mean Per Joint Position Error (MPJPE) i.e. the Euclidean distance from the estimated 3D-pose,P 3D , to the ground-truth, P 3D , averaged over 17 joints of the Human3.6M skeletal model. Protocol-2: Subjects S1, S5, S6, S7, S8 and S9 are used for training and S11 for testing. The error metric used is Procrustes Aligned MPJPE (PA MPJPE) which is the MPJPE calculated after rigidly aligning the predicted pose with the ground-truth. <ref type="table" target="#tab_1">Table 1</ref> and <ref type="table" target="#tab_3">Table 2</ref> show our results for Protocol-1 and Protocol-2, respectively. In the paired setting, we train each module, that is, 2DPoseNet, OrdinalNet and Multi-PoseNet, using paired image-to-3D pose annotations from Human3.6M. Under this setting, we achieve competitive results using PRED Ordinals for scoring. The use of GT Ordinals takes us close to the state-of-the-art. We are worse only to the methods that either use additional ordinal training data <ref type="bibr" target="#b23">[24]</ref>, temporal information <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10]</ref> and/or soft-argmax <ref type="bibr" target="#b34">[35]</ref> (denoted by *s), all of which is compatible with our approach and is expected to improve the performance further. Finally, we outperform all existing methods using Oracle supervision. Although it's an unfair comparison, it demonstrates that our CVAE-generated sample set contains candidate poses that are very close to the ground-truth pose, thus validating our sample-generation based approach. Without Paired 3D Supervision: The modular nature of our pipeline allows us to train the 2D-to-3D lifting module on a separate MoCap library that has no intersection with the training images for 2DPoseNet, OrdinalNet. It affords training our pipeline without the costly and laborious acquisition of paired image-to-3D annotations. We demonstrate it by training MultiPoseNet on the CMU MoCap dataset, which consists of only 3D MoCap data, and report the results on the test-set of Human3.6M. Note that the MoCap dataset is only needed for training, not for testing. The 3Dposes from CMU MoCap are virtually projected to their corresponding 2D-projections, with the camera at the origin and pelvis at a distance of 5500mm. We have used the intrinsic camera parameters from Human3.6M to bring the   <ref type="bibr" target="#b18">[19]</ref> in the unpaired setting were obtained using their publicly available code. * -use additional ordinal training data from MPII and LSP. ** -use temporal information. *** -use soft-argmax for end-to-end training. These strategies are complementary with our approach.   <ref type="bibr" target="#b18">[19]</ref> in the unpaired setting were obtained using their publicly available code.</p><p>distribution of 2D-projections closer to the Human3.6M test set. We also rotate the 3D-poses by 90, 180, and 270 degrees, for data augmentation. The obtained 2D-to-3D pose dataset is used to train the Baseline model <ref type="bibr" target="#b18">[19]</ref> and Mul-tiPoseNet. The estimated 2D-poses and ordinals are obtained from 2DPoseNet and OrdinalNet, both of which are trained on Human3.6M. We emphasize that Human3.6M is only used for learning 2D-pose and ordinal estimation, therefore, we don't use any image-to-3D annotation during training. Since, two different sources are used for the image-to-2D/ordinal and 2D-to-3D modules, we call it unpaired setting. The results of these experiments are reported in <ref type="table" target="#tab_1">Table 1</ref> and 2 in the bottom rows.</p><p>Our PRED Ordinals based method outperforms the Baseline regression model <ref type="bibr" target="#b18">[19]</ref> and with the use of GT Or-dinals and Oracle the performance only increases. It shows that our framework can learn without image-to-3D annotation and is also robust to domain shift.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Evaluation on HumanEva-I</head><p>Under the protocol from <ref type="bibr" target="#b14">[15]</ref>, we evaluate our model on HumanEva-I. Training uses subjects S1, S2, S3 under different view-points and action-sequences Jogging and Walking, while testing is carried out on the validation sequences for all three subjects as testing data. All the modules are trained using HumanEva-I. The model error is reported as the reconstruction error after rigid transformation. We obtain state-of-the-art results using the Oracle estimate and close to state-of-the-art with PRED Ordinals and GT Ordinals on HumanEva-I, reported in <ref type="table">Table 3</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">OrdinalNet Accuracy</head><p>The OrdinalNet accuracy is obtained by comparing the ground-truth ordinals, M , with the predicted ordinals,M . The results on the validation set for Human3.6M and HumanEva-I are 86.8% and 81% respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation Studies</head><p>Effect of Increasing Sample Set Size: In <ref type="figure" target="#fig_1">Figure 3a</ref>, we plot the value of different error estimates on Protocol-1 of Human3.6 with increasing number of samples. MEAN denotes the uniform average of all samples. We observe that the MEAN improves with the number of samples, but satu-rates quickly. The Oracle performance keeps on improving with the number of samples, which validates the intuition that the chance of obtaining close to ground-truth pose increases with more samples. Consequently, the estimated 3D-pose, either using PRED Ordinals or GT Ordinals, keeps improving with more samples, as is evident from their respective curves. This demonstrates that the proposed ordinal scoring is an effective strategy for weighted averaging of the generated samples. Sampling Baseline: Here, we compare a Baseline sampling strategy against our CVAE-based generative sampling. Baseline sampling treats each joint-location as independent Gaussian distribution with the mean as the out- <ref type="figure">Figure 5</ref>: Samples from MultiPoseNet and Baseline ( using a variance of 100 ) mapped to Euclidean space using ISOMAP <ref type="bibr" target="#b36">[37]</ref>. Note that MultiPoseNet produces much more diverse samples that are likely to be near the GT pose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Jogging</head><p>Walking S1 S2 S3 S1 S2 S3 Avg Kostrikov et al. <ref type="bibr" target="#b14">[15]</ref> 44  <ref type="table">Table 3</ref>: Results of our model on HumanEva-I dataset and a comparison with previous work. Numbers reported are mean reconstruction error in mm computed after rigid transformation.</p><p>put of the Baseline regression model <ref type="bibr" target="#b18">[19]</ref> and variance from {1,5,10,20,100,400}. Each joint-location is sampled independently to obtain a 3D-pose. Oracle supervision is used for both Baseline sampling and our MultiPoseNet sampling to obtain the final 3D-pose. <ref type="figure" target="#fig_1">Figure 3b</ref> shows the comparison of MultiPoseNet with Baseline sampling on Protocol-1 of Human3.6 with increasing number of samples. It's evident that Baseline performs poorly and does not improve steeply with more number of samples. It also begins to worsen with higher variance of 400mm as the samples become more absurd. On the other hand, MultiPoseNet improves its estimate by close to 20mm and the slope of the curve indicates further potential gains by sampling more.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Sample Diversity</head><p>Qualitative Analysis: To assess the feasibility of the proposed approach to generate a diverse set of plausible 3Dpose candidates from a given 2D-pose, we show the MEAN pose, per-joint standard deviation, and a few candidate 3D poses for two different images from the Human3.6M test set in <ref type="figure" target="#fig_2">Figure 4</ref>. We observe meaningful variations across dif-ferent body parts and poses with relatively higher variance around, the hardest to predict, wrist and elbow joints. Visualisation Using Dimensionality Reduction: To visualize the distribution of generated candidate 3D-poses, we map the samples from MultiPoseNet and Baseline sampling (with a variance of 100) into Euclidean space using Isomap <ref type="bibr" target="#b36">[37]</ref>. <ref type="figure">Fig. 5</ref> shows 1000 samples using both Multi-PoseNet and Baseline sampling for two different input 2Dposes, along with the ground truth 3D-pose and the MEAN estimate of MultiPoseNet. Interestingly, the samples from Baseline are clustered narrowly around the MEAN, whereas MultiPoseNet samples are diverse and are more likely to be near the GT 3D-pose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Future Work</head><p>This article presented a novel framework for monocular 3D-pose estimation that uses a conditional variational autoencoder for sampling 3D-pose candidates which are scored and weighted-averaged together using ordinal relations, predicted from a deep CNN. The proposed method achieves close to state-of-the-art results on two benchmark datasets using OrdinalScore, and state-of-the-art results using an Oracle with access to the ground truth 3D-pose. The CVAE has been shown to learn a generative model that synthesizes diverse 3D-pose samples consistent with the input 2D-pose, thereby dealing with the ambiguity in lifting from 2D-to-3D. It can also be trained without paired image-to-3D annotations, and still yields competitive results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgements</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>MultiPoseNet architecture in training. Note: in GSNN, we sample z ∼ N (0, I) and only need the Decoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>(a) Oracle vs OrdinalScore vs MEAN (b) MultiPoseNet vs Baseline sampling Ablation studies. (a) Effect of increasing number of samples on Oracle, OrdinalScore and MEAN estimate (b) Comparison of MultiPoseNet versus Baseline sampling using Oracle supervision.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Sample diversity on Human3.6M test-set. From L-R: Input Image, MEAN Pose with per-joint standard deviation around each joint, and 3 different SAMPLES overlaid on top of MEAN pose. MEAN is solid and SAMPLE is dashed, with displacement field in between. Note that wrist and elbow show maximum variance. Best viewed in color with zoom.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Direct. Discuss Eating Greet Phone Photo Pose Purch. Sitting SitingD Smoke Wait WalkD Walk WalkT Avg</figDesc><table><row><cell></cell><cell>Protocol 1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Pavlakos et al. [25]</cell><cell>67.4</cell><cell>71.9</cell><cell>66.7 69.1 72.0</cell><cell>77.0</cell><cell cols="2">65.0 68.3</cell><cell>83.7</cell><cell>96.5</cell><cell>71.7</cell><cell>65.8</cell><cell>74.9</cell><cell>59.1</cell><cell>63.2</cell><cell>71.9</cell></row><row><cell></cell><cell>Zhou et al. [45]</cell><cell cols="3">54.82 60.70 58.22 71.4 62.0</cell><cell>65.5</cell><cell cols="2">53.8 55.6</cell><cell>75.2</cell><cell>111.6</cell><cell>64.1</cell><cell>66.0</cell><cell>51.4</cell><cell>63.2</cell><cell>55.3</cell><cell>64.9</cell></row><row><cell></cell><cell>Martinez et al. [19]</cell><cell>51.8</cell><cell>56.2</cell><cell>58.1 59.0 69.5</cell><cell>78.4</cell><cell cols="2">55.2 58.1</cell><cell>74.0</cell><cell>94.6</cell><cell>62.3</cell><cell>59.1</cell><cell>65.1</cell><cell>49.5</cell><cell>52.4</cell><cell>62.9</cell></row><row><cell></cell><cell>Sun et al. [34]</cell><cell>52.8</cell><cell>54.8</cell><cell>54.2 54.3 61.8</cell><cell>67.2</cell><cell cols="2">53.1 53.6</cell><cell>71.7</cell><cell>86.7</cell><cell>61.5</cell><cell cols="2">53.4 61.6</cell><cell>47.1</cell><cell>53.4</cell><cell>59.1</cell></row><row><cell></cell><cell>Fang et al. [9]</cell><cell>50.1</cell><cell>54.3</cell><cell>57.0 57.1 66.6</cell><cell>73.3</cell><cell cols="2">53.4 55.7</cell><cell>72.8</cell><cell>88.6</cell><cell>60.3</cell><cell>57.7</cell><cell>62.7</cell><cell>47.5</cell><cell>50.6</cell><cell>60.4</cell></row><row><cell></cell><cell>*Pavlakos et al. [24]</cell><cell>48.5</cell><cell>54.4</cell><cell>54.4 52.0 59.4</cell><cell>65.3</cell><cell cols="2">49.9 52.9</cell><cell>65.8</cell><cell>71.1</cell><cell>56.6</cell><cell>52.9</cell><cell>60.9</cell><cell>44.7</cell><cell>47.8</cell><cell>56.2</cell></row><row><cell></cell><cell>**Hossain et al.-[10]</cell><cell>44.2</cell><cell>46.7</cell><cell>52.3 49.3 59.9</cell><cell>59.4</cell><cell cols="2">47.5 46.2</cell><cell>59.9</cell><cell>65.6</cell><cell>55.8</cell><cell>50.4</cell><cell>52.3</cell><cell>43.5</cell><cell>45.1</cell><cell>51.9</cell></row><row><cell>PAIR</cell><cell>**Dabral et al.-[8]</cell><cell>44.8</cell><cell>50.4</cell><cell>44.7 49.0 52.9</cell><cell>61.4</cell><cell cols="2">43.5 45.5</cell><cell>63.1</cell><cell>87.3</cell><cell>51.7</cell><cell>48.5</cell><cell>37.6</cell><cell>52.2</cell><cell>41.9</cell><cell>52.1</cell></row><row><cell></cell><cell>***Sun et al. [35]</cell><cell>47.5</cell><cell>47.7</cell><cell>49.5 50.2 51.4</cell><cell>43.8</cell><cell cols="2">46.4 58.9</cell><cell>65.7</cell><cell>49.4</cell><cell>55.8</cell><cell>47.8</cell><cell>38.9</cell><cell>49.0</cell><cell>43.8</cell><cell>49.6</cell></row><row><cell></cell><cell cols="2">Ours (PRED Ordinals) 48.6</cell><cell>54.5</cell><cell>54.2 55.7 62.6</cell><cell>72.0</cell><cell>50.5</cell><cell>54.3</cell><cell>70.0</cell><cell>78.3</cell><cell>58.1</cell><cell>55.4</cell><cell>61.4</cell><cell>45.2</cell><cell>49.7</cell><cell>58.0</cell></row><row><cell></cell><cell>Ours (GT Ordinals)</cell><cell>42.9</cell><cell>48.1</cell><cell>47.8 50.2 56.1</cell><cell>65.0</cell><cell cols="2">44.9 48.6</cell><cell>61.8</cell><cell>69.9</cell><cell>52.6</cell><cell>50.4</cell><cell>56.0</cell><cell>42.1</cell><cell>45.1</cell><cell>52.1</cell></row><row><cell></cell><cell>Ours (Oracle)</cell><cell>37.8</cell><cell>43.2</cell><cell>43.0 44.3 51.1</cell><cell>57.0</cell><cell cols="2">39.7 43.0</cell><cell>56.3</cell><cell>64.0</cell><cell>48.1</cell><cell>45.4</cell><cell>50.4</cell><cell>37.9</cell><cell>39.9</cell><cell>46.8</cell></row><row><cell></cell><cell>Martinez et al. [19]</cell><cell>109.9</cell><cell>112</cell><cell cols="2">103.8 115.3 119.3 119.3</cell><cell cols="10">114 116.6 118.9 127.3 112.2 119.8 113.4 119.8 111.9 115.6</cell></row><row><cell>UNPAIR</cell><cell cols="2">Ours (PRED Ordinals) 99.9 Ours (GT Ordinals) 97.9</cell><cell>102.7 100.5</cell><cell cols="12">97.9 105.9 112.0 111.7 103.9 109.4 111.7 119.4 104.8 110.8 103.2 106.9 102.3 106.8 95.4 103.7 109.4 108.5 102.0 108.0 107.9 115.4 102.2 108.9 100.8 105.8 100.8 104.4</cell></row><row><cell></cell><cell>Ours (Oracle)</cell><cell>92.6</cell><cell>94.6</cell><cell cols="6">90.6 98.4 103.8 103.3.6 96.6 101.8 101.7 108.8</cell><cell cols="5">96.6 102.7 95.3 100.6 96.1</cell><cell>98.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell>Detailed results on Human3.6M under Protocol 1(no rigid alignment in post-processing). Error is in millime-</cell></row><row><cell>ters(mm). Top: Paired methods (PAIR), Bottom: unpaired methods (UNPAIR). Results for</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Detailed results on Human3.6M under Protocol 2(rigid alignment in post-processing). Top: Paired methods (PAIR), Bottom: unpaired methods (UNPAIR). Results for</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>.0 30.9 41.7 57.2 35.0 33.3 40.3 Yasin et al. [43] 35.8 32.4 41.6 46.6 41.4 35.4 38.9 Moreno-Noguer et al. [20] 19.7 13.0 24.9 39.7 20.0 21.0 26.9 Pavlakos et al. [25] 22.1 21.9 29.0 29.8 23.6 26.0 25.5 Martinez et al. [19] 19.7 17.4 46.8 26.9 18.2 18.6 24.6 Ours (PRED Ordinals) 19.3 12.5 41.8 40.9 22.1 18.6 25.9 Ours (GT Ordinals) 19.1 12.4 41.5 40.6 21.9 18.5 25.7 Ours (Oracle) 17.4 11.0 39.5 38.5 20.1 16.7 23.9</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This research was funded in part by Mercedes Benz Research and Development, India. We also thank Bernt Schiele for providing valuable feedback on the manuscript.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pose-conditioned joint angle limits for 3d human pose reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ijaz</forename><surname>Akhter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Keep it smpl: Automatic estimation of 3d human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Disco nets: Dissimilarity coefficient networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Bouchacourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pawan</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">3d human pose estimation = 2d pose estimation + matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Hang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Singleimage depth perception in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Carnegie mellon university graphics lab -motion capture library</title>
		<ptr target="http://mocap.cs.cmu.edu/.2" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
		<respStmt>
			<orgName>CMU</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning 3d human pose from structure and motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Dabral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Mundhada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uday</forename><surname>Kusupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Safeer</forename><surname>Afaque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning pose grammar to encode human body configuration for 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoshu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanlu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In AAAI</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Exploiting temporal information for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imtiaz</forename><surname>Mir Rayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generating multiple diverse hypotheses for human 3d pose consistent with 2d joint detections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Jahangiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Depth sweep regression forests for estimating 3d human pose from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Kostrikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Proposal maps driven mcmc for estimating human body pose in static images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai</forename><surname>Mun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isaac</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Maximummargin structured learning with deep networks for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weichen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Smpl: A skinned multiperson linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM transactions on graphics</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julieta</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rayat</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">3d human pose estimation from a single image via distance matrix regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesc</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning lightness from human judgement on relative reflectance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuya</forename><surname>Narihira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Neural body fitting: Unifying deep learning and model based human pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3DV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Ordinal depth supervision for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric prediction for single-image 3d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Konstantinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
	<note>Derpanis, and Kostas Daniilidis</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Posebits for monocular human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Reconstructing 3d human pose from 2d image landmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">It&apos;s all relative: Monocular 3d human pose estimation from weakly supervised data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Matteo Ruggero Ronchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Eng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Humaneva: Synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Alexandru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A joint model for 2d and 3d pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariadna</forename><surname>Quattoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carme</forename><surname>Torras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesc</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Kinematic jump processes for monocular 3d human tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning structured output representation using deep conditional generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Seonwook Park, and Otmar Hilliges. Cross-modal deep variational hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Spurr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Compositional human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxiang</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Reconstruction of articulated objects from point correspondences in a single uncalibrated image. Comput</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Camillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vis. Image Underst</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="349" to="363" />
			<date type="published" when="2000-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A global geometric framework for nonlinear dimensionality reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vin</forename><forename type="middle">De</forename><surname>Joshua B Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John C</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Crossing nets: Combining gans and vaes with a shared latent space for hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Probst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Deepskeleton: Skeleton map for 3d human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingfu</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.10796</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Robust estimation of 3d human poses from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Drpose3d: Depth ranking in 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhuang</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shih-En</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A dual-source approach for 3d pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hashim</forename><surname>Yasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umar</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjorn</forename><surname>Kruger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning data-driven reflectance priors for intrinsic image decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Towards 3d human pose estimation in the wild: A weakly-supervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Sparseness meets deepness: 3d human pose estimation from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kosta</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Sparse representation for 3d shape estimation: A convex relaxation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spyridon Leonardos, and Kostas Daniilidis</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning ordinal relationships for mid-level vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
