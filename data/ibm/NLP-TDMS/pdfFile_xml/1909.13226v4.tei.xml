<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PolarMask: Single Shot Instance Segmentation with Polar Representation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
							<email>xieenze@hku.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Sensetime Group Ltd 3 Xi&apos;an Jiaotong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peize</forename><surname>Sun</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoge</forename><surname>Song</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Nanjing University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Nanjing University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Sensetime Group Ltd 3 Xi&apos;an Jiaotong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">The University of Adelaide</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">PolarMask: Single Shot Instance Segmentation with Polar Representation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we introduce an anchor-box free and single shot instance segmentation method, which is conceptually simple, fully convolutional and can be used by easily embedding it into most off-the-shelf detection methods. Our method, termed PolarMask, formulates the instance segmentation problem as predicting contour of instance through instance center classification and dense distance regression in a polar coordinate. Moreover, we propose two effective approaches to deal with sampling high-quality center examples and optimization for dense distance regression, respectively, which can significantly improve the performance and simplify the training process. Without any bells and whistles, PolarMask achieves 32.9% in mask mAP with single-model and single-scale training/testing on the challenging COCO dataset.</p><p>For the first time, we show that the complexity of instance segmentation, in terms of both design and computation complexity, can be the same as bounding box object detection and this much simpler and flexible instance segmentation framework can achieve competitive accuracy. We hope that the proposed PolarMask framework can serve as a fundamental and strong baseline for single shot instance segmentation task. Code is available at: github.com/xieenze/PolarMask.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Instance segmentation is one of the fundamental tasks in computer vision, which enables numerous downstream vision applications. It is challenging as it requires to predict both the location and the semantic mask of each instance in an image. Therefore intuitively instance segmentation can be solved by bounding box detection then semantic segmentation within each box, adopted by two-stage methods, such * indicates equal contribution.   as Mask R-CNN <ref type="bibr" target="#b14">[15]</ref>. Recent trends in the vision community have spent more effort in designing simpler pipelines of bounding box detectors <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b18">19]</ref> and subsequent instance-wise recognition tasks including instance segmentation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b34">35]</ref>, which is also the main focus of our work here. Thus, our aim is to design a conceptually simple mask prediction module that can be easily plugged into many off-the-shelf detectors, enabling instance segmentation.</p><p>Instance segmentation is usually solved by binary classification in a spatial layout surrounded by bounding boxes, shown in <ref type="figure" target="#fig_2">Figure 1</ref> . prediction is luxurious, especially in the single-shot methods. Instead, we point out that masks can be recovered successfully and effectively if the contour is obtained. An intuitive method to locate contours is shown in <ref type="figure" target="#fig_2">Figure 1(c)</ref>, which predicts the Cartesian coordinates of the point composing the contour. Here we term it as Cartesian Representation. The second approach is Polar Representation, which applies the angle and the distance as the coordinate to locate points, shown in <ref type="figure" target="#fig_2">Figure 1(d)</ref>.</p><p>In this work, we design an instance segmentation method based on the Polar Representation since its inherent advantages are as follows: <ref type="bibr" target="#b0">(1)</ref> The origin point of the polar coordinate can be seen as the center of object. <ref type="bibr" target="#b1">(2)</ref> Starting from the origin point, the point in contour is determined by the distance and angle. (3) The angle is naturally directional and makes it very convenient to connect the points into a whole contour. We claim that Cartesian Representation may exhibit first two properties similarly. However, it lacks the advantage of the third property.</p><p>We instantiate such an instance segmentation method by using the recent object detector FCOS <ref type="bibr" target="#b28">[29]</ref>, mainly for its simplicity. Note that, it is possible to use other detectors such as RetinaNet <ref type="bibr" target="#b22">[23]</ref>, YOLO <ref type="bibr" target="#b26">[27]</ref> with minimal modification to our framework. Specifically, we propose Polar-Mask, formulating instance segmentation as instance center classification and dense distance regression in a polar coordinate, shown in <ref type="figure" target="#fig_3">Figure 2</ref>. The model takes an input image and predicts the distance from a sampled positive location (candidates of the instance center) to the instance contour at each angle, and after assembling, outputs the final mask. The overall pipeline of PolarMask is almost as simple and clean as FCOS. It introduces negligible computation overhead. Simplicity and efficiency are the two key factors for single-shot instance segmentation, and PolarMask achieves them successfully.</p><p>Furthermore, PolarMask can be viewed as a generalization of FCOS. In other words, FCOS is a special case of PolarMask since bounding boxes can be viewed as the simplest mask with only 4 directions. Thus, one is suggested to use PolarMask over FCOS for instance recognition wherever mask annotation is available <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b19">20]</ref>.</p><p>In order to maximize the advantages of Polar Representation, we propose Polar Centerness and Polar IoU Loss to deal with sampling high-quality center examples and optimization for dense distance regression, respectively. They improve the mask accuracy by about 15% relatively, showing considerable gains under stricter localization metrics. Without bells and whistles, PolarMask achieves 32.9% in mask mAP with single-model and single-scale training/testing on the challenging COCO dataset <ref type="bibr" target="#b23">[24]</ref>.</p><p>The main contributions of this work are three-fold:</p><p>â€¢ We introduce a brand new framework for instance segmentation, termed PolarMask, to model instance masks in the polar coordinate, which converts instance segmentation to two parallel tasks: instance center classification and dense distance regression. The main desirable characteristics of PolarMask is being simple and effective. â€¢ We propose the Polar IoU Loss and Polar Centerness, tailored for our framework. We show that the proposed Polar IoU loss can largely ease the optimization and considerably improve the accuracy, compared with standard loss such as the smooth-l 1 loss. In parallel, Polar Centerness improves the original idea of "Centreness" in FCOS, leading to further performance boost. â€¢ For the first time, we show that the complexity of instance segmentation, in terms of both design and computation complexity, can be the same as bounding box object detection. We further demonstrate this much simpler and flexible instance segmentation framework achieves competitive performance compared with more complex one-stage methods, which typically involve multi-scale training and longer training time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Two-Stage Instance Segmentation. Two-stage instance segmentation often formulates this task as the paradigm of "Detect then Segment" <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b17">18]</ref>. They often detect bounding boxes then perform segmentation in the area of each bounding box. The main idea of FCIS <ref type="bibr" target="#b20">[21]</ref> is to predict a set of position-sensitive output channels fully convolutionally. These channels simultaneously address object classes, boxes, and masks, making the system fast. Mask R-CNN <ref type="bibr" target="#b14">[15]</ref>, built upon Faster R-CNN, simply adds an additional mask branch and use RoI-Align to replace RoI-Pooling <ref type="bibr" target="#b11">[12]</ref> for improved accuracy. Following Mask R-CNN, PANet <ref type="bibr" target="#b24">[25]</ref> introduces bottom-up path augmentation, adaptive feature pooling, and fully-connected fusion to boost up the performance of instance segmentation. Mask Scoring R-CNN <ref type="bibr" target="#b17">[18]</ref> re-scores the confidence of mask from classification score by adding a mask-IoU branch, which makes the network to predict the IoU of mask and groundtruth.</p><p>In summary, the above methods typically consist of two steps, first detecting bounding box and then segmenting in each bounding box. They can achieve state-of-the-art performance but are often slow.</p><p>One Stage Instance Segmentation. Deep Watershed Transform <ref type="bibr" target="#b0">[1]</ref> uses fully convolutional networks to predict the energy map of the whole image and use the watershed algorithm to yield connected components corresponding to object instances. InstanceFCN <ref type="bibr" target="#b5">[6]</ref> uses instancesensitive score maps for generating proposals. It first produces a set of instance-sensitive score maps, then an assembling module is used to generate object instances in a sliding window. The recent YOLACT <ref type="bibr" target="#b1">[2]</ref> first generates a set of prototype masks, the linear combination coefficients for each instance, and bounding boxes, then linearly combines the prototypes using the corresponding predicted coefficients and then crops with a predicted bounding box. TensorMask <ref type="bibr" target="#b3">[4]</ref> investigates the paradigm of dense slidingwindow instance segmentation, using structured 4D tensors to represent masks over a spatial domain. ExtremeNet <ref type="bibr" target="#b34">[35]</ref> uses keypoint detection to predict 8 extreme points of one instance and generates an octagon mask, achieving relatively reasonable object mask prediction. The backbone of ExtremeNet is HourGlass <ref type="bibr" target="#b25">[26]</ref>, which is very heavy and of-ten needs longer training time.</p><p>Polar representation was firstly used in <ref type="bibr" target="#b27">[28]</ref> to detect cells in microscopic images, where the problem is much simpler as there are only two categories. Concurrent to our work is the work of ESESeg <ref type="bibr" target="#b30">[31]</ref>, which also employs the polar coordinate to model instances. However, our Po-larMask achieves significantly better performance than ES-ESeg due to very different designs other than the polar representation. Note that most of these methods do not model instances directly and they can sometimes be hard to optimize (e.g., longer training time, more data augmentation and extra labels). Our PolarMask directly models instance segmentation with a much simpler and flexible way of two paralleled branches: classifying each pixel of mass-center of instance and regressing the dense distance of rays between mass-center and contours. The most significant advantage of PolarMask is being simple and efficient compared with the above methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Method</head><p>In this section, we first briefly introduce the overall architecture of the proposed PolarMask. Then, we reformulate instance segmentation with the proposed Polar Representation. Next, we introduce a novel concept of Polar Centerness to ease the procedure of choosing high-quality center samples. Finally, we introduce a new Polar IoU Loss to optimize the dense regression problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Architecture</head><p>PolarMask is a simple, unified network composed of a backbone network <ref type="bibr" target="#b15">[16]</ref>, a feature pyramid network <ref type="bibr" target="#b21">[22]</ref>, and two or three task-specific heads, depending on whether predicting bounding boxes. <ref type="bibr" target="#b0">1</ref> The settings of the backbone and feature pyramid network are the same as FCOS <ref type="bibr" target="#b28">[29]</ref>. While there exist many stronger candidates for those components, we align these settings with FCOS to show the simplicity and effectiveness of our instance modeling method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Polar Mask Segmentation</head><p>In this section, we will describe how to model instances in the polar coordinate in detail.</p><p>Polar Representation. Given an instance mask, we firstly sample a candidate center (x c , y c ) of the instance and the point located on the contour (x i , y i ), i = 1, 2, ..., N . Then, starting from the center, n rays are emitted uniformly with the same angle interval âˆ†Î¸ (e.g., n = 36, âˆ†Î¸ = 10 â€¢ ), whose length is determined from the center to the contour. In this way, we model the instance mask in the polar coordinate as one center and n rays. Since the angle interval is pre-defined, only the length of the ray needs to be predicted. Therefore, we formulate the instance segmentation as instance center classification and dense distance regression in a polar coordinate.</p><p>Mass Center. There are many choices for the center of the instance, such as box center or mass-center. How to choose a better center depends on its effect on mask prediction performance. Here we verify the upper bound of box center and mass-center and conclude that mass-center is more advantageous. Details are in <ref type="figure">Figure 7</ref>. We explain that the mass-center has a greater probability of falling inside the instance, compared with the box center. Although for some extreme cases, such as a donut, neither mass-center nor box center lies inside the instance. We leave it for further research.</p><p>Center Samples. Location (x, y) is considered as a center sample if it falls into areas around the mass-center of any instance. Otherwise, it is a negative sample. We define the region for sampling positive pixels to be 1.5Ã— strides <ref type="bibr" target="#b28">[29]</ref> of the feature map from the mass-center to left, top, right and bottom. Thus each instance has about 9âˆ¼16 pixels near the mass-center as center examples. It has two advantages: (1) Increasing the number of positive samples from 1 to 9âˆ¼16 can largely avoid imbalance of positive and negative samples. Nevertheless, focal loss <ref type="bibr" target="#b22">[23]</ref> is still needed when training the classification branch. (2) Mass-center may not be the best center sample of an instance. More candidate points make it possible to automatically find the best center of one instance. We will discuss it in detail in Section 3.3.</p><p>Distance Regression. Given a center sample (x c , y c ) and the contour point of an instance, the length of n rays {d 1 , d 2 , . . . , d n } can be computed easily. More details are in supplementary materials. Here we mainly discuss some corner cases:</p><p>â€¢ If one ray has multiple intersection points with the contour of instance, we directly choose the one with the maximum length. â€¢ If one ray, which starts from the center outside of the mask, does not have intersection points with the contour of an instance at some certain angles, we set its regression target as the minimum value (e.g., = 10 âˆ’6 ).</p><p>We argue that these corner cases are the main obstacles of restricting the upper bound of Polar Representation from reaching 100% AP. However, it is not supposed to be seen as Polar Representation being inferior to the non-parametric Pixel-wise Representation. The evidence is two-fold. First, even the Pixel-wise Representation still has certain gap with the upper bound of 100% AP in practice, since some operation, such as down-sampling, is indispensable. Second, current performance is far away from the upper bound regardless of the Pixel-wise Representation or Polar Representation. Therefore, the research effort is suggested to better spend on improving the practical performance of models, rather than the theoretical upper bound.</p><p>The training of the regression branch is non-trivial. First, the mask branch in PolarMask is actually a dense distance regression task since every training example has n rays (e.g., n = 36). It may cause an imbalance between the regression loss and classification loss. Second, for one instance, its n rays are relevant and should be trained as a whole, rather than being seen as a set of independent regression examples. Therefore, we put forward the Polar IoU Loss, discussed in detail in Section 3.4.</p><p>Mask Assembling. During inference, the network outputs the classification and centerness, we multiply centerness with classification and obtain final confidence scores. We only assemble masks from at most 1k top-scoring predictions per FPN level, after thresholding the confidence scores at 0.05. The top predictions from all levels are merged and non-maximum suppression (NMS) with a threshold of 0.5 is applied to yield the final results. Here we introduce the mask assembling process and a simple NMS process.</p><p>Given a center sample (x c , y c ) and n ray's length {d 1 , d 2 , . . . , d n }, we can calculate the position of each corresponding contour point with the following formula:</p><formula xml:id="formula_0">x i = cos Î¸ i Ã— d i + x c (1) y i = sin Î¸ i Ã— d i + y c .<label>(2)</label></formula><p>Starting from 0 â€¢ , the contour points are connected one by one, shown in <ref type="figure" target="#fig_4">Figure 3</ref> and finally assembles a whole contour as well as the mask.</p><p>We apply NMS to remove redundant masks. To simplify the process, We calculate the smallest bounding boxes of masks and then apply NMS based on the IoU of generated boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Polar Centerness</head><p>Centerness <ref type="bibr" target="#b28">[29]</ref> is introduced to suppress these lowquality detected objects without introducing any hyperparameters and it is proven to be effective in object bounding box detection. However, directly transferring it to our system can be sub-optimal since its centerness is designed for bounding boxes and we care about mask prediction.</p><p>Given a set {d 1 , d 2 , . . . , d n } for the length of n rays of one instance. We propose Polar Centerness:</p><formula xml:id="formula_1">Polar Centerness = min({d 1 , d 2 , . . . , d n }) max({d 1 , d 2 , . . . , d n })<label>(3)</label></formula><p>It is a simple yet effective strategy to re-weight the points so that the closer d min and d max are, higher weight the point is assigned. We add a single layer branch, in parallel with the classification branch to predict Polar Centerness of a location, as shown in <ref type="figure" target="#fig_3">Figure 2</ref>. Polar Centerness predicted by the network is multiplied to the classification score, thus can downweight the low-quality masks. Experiments show that Polar Centerness improves accuracy especially under stricter localization metrics, such as AP 75 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Polar IoU Loss</head><p>As discussed above, the method of polar segmentation converts the task of instance segmentation into a set of regression problems. In most cases in the field of object detection and segmentation, smooth-l 1 loss <ref type="bibr" target="#b12">[13]</ref> and IoU loss <ref type="bibr" target="#b32">[33]</ref> are the two effective ways to supervise the regression problems. Smooth-l 1 loss overlooks the correlation between samples of the same objects, thus, resulting in less accurate localization. IoU loss, however, considers the optimization as a whole, and directly optimizes the metric of interest, IoU. Nevertheless, computing the IoU of the predicted mask and its ground-truth is tricky and very difficult to implement parallel computations. In this work, we derive an easy and effective algorithm to compute mask IoU based on the polar vector representation and achieve competitive performance.</p><p>We introduce Polar IoU Loss starting from the definition of IoU, which is the ratio of interaction area over union area between the predicted mask and ground-truth. As shown in <ref type="figure">Figure 5</ref>, in the polar coordinate system, for one instance, mask IoU is calculated as follows:</p><formula xml:id="formula_2">IoU = 2Ï€ 0 1 2 min(d, d * ) 2 dÎ¸ 2Ï€ 0 1 2 max(d, d * ) 2 dÎ¸<label>(4)</label></formula><p>where regression target d and predicted d * are length of the ray, angle is Î¸. Then we transform it to the discrete form 2</p><formula xml:id="formula_3">IoU = lim N â†’âˆž N i=1 1 2 d 2 min âˆ†Î¸ i N i=1 1 2 d 2 max âˆ†Î¸ i<label>(6)</label></formula><p>When N approaches infinity, the discrete form is equal to continuous form. We assume that the rays are uniformly emitted, so âˆ†Î¸ = 2Ï€ N , which further simplifies the expression. We empirically observe that the power form has little impact on the performance (Â±0.1 mAP difference) if it is discarded and simplified into the following form:</p><formula xml:id="formula_4">Polar IoU = n i=1 d min n i=1 d max<label>(7)</label></formula><p>Polar IoU Loss is the binary cross entropy (BCE) loss of Polar IoU. Since the optimal IoU is always 1, the loss is actually is negative logarithm of Polar IoU: <ref type="bibr" target="#b1">2</ref> For notation convenience, we define:</p><formula xml:id="formula_5">Polar IoU Loss = log n i=1 d max n i=1 d min<label>(8)</label></formula><formula xml:id="formula_6">d min = min(d, d * ), dmax = max(d, d * ).<label>(5)</label></formula><p>rays AP AP50 AP75 APS APM APL <ref type="bibr" target="#b17">18</ref>  Our proposed Polar IoU Loss exhibits two advantageous properties: (1) It is differentiable, enabling back propagation; and it is very easy to implement parallel computations, thus facilitating a fast training process. (2) It predicts the regression targets as a whole. It improves the overall performance by a large margin compared with smooth-l 1 loss, shown in our experiments. (3) As a bonus, Polar IoU Loss is able to automatically keep the balance between classification loss and regression loss of dense distance prediction. We will discuss it in detail in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We present results of instance segmentation on the challenging COCO benchmark <ref type="bibr" target="#b23">[24]</ref>. Following common practice <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b3">4]</ref>, we train using the union of 80K train images and a 35K subset of val images (trainval35k), and report ablations on the remaining 5K val. images (minival). We also compare results on test-dev. We adopt the 1Ã— training strategy <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b2">3]</ref>, single scale training and testing of image short-edge as 800 unless otherwise noted.</p><p>Training Details.</p><p>In ablation study, ResNet-50-FPN <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b21">22]</ref> is used as our backbone networks and the same hyper-parameters with FCOS <ref type="bibr" target="#b28">[29]</ref> are used. Specifically, our network is trained with stochastic gradient descent (SGD) for 90K iterations with the initial learning rate being 0.01 and a mini-batch of 16 images. The learning rate is reduced by a factor of 10 at iteration 60K and 80K, respec-tively. Weight decay and momentum are set as 0.0001 and 0.9, respectively. We initialize our backbone networks with the weights pre-trained on ImageNet <ref type="bibr" target="#b8">[9]</ref>. The input images are resized to have their shorter side being 800 and their longer side less or equal to 1333.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Ablation Study</head><p>Verification of Upper Bound. The first concern about PolarMask is that it might not depict the mask precisely. In this section, we prove that this concern may not be necessary. Here we verify the upper bound of PolarMask as the IoU of predicted mask and ground-truth when all of the rays regress to the distance equal to ground-truth. The verification results on different numbers of rays are shown in <ref type="figure">Figure 7</ref>. It can be seen that IoU is approaching to nearly perfect (above 90%) when the number of rays increases, which shows that Polar Segmentation is able to model the mask very well. Therefore, the concern about the upper bound of PolarMask is not necessary. Also, it is more reasonable to use mass-center than bounding box-center as the center of an instance because the bounding box center is more likely to fall out of the instance.</p><p>Number of Rays. It plays a fundamental role in the whole system of PolarMask. <ref type="table" target="#tab_0">From Table 1a</ref> and <ref type="figure">Figure 7</ref>, more rays show higher upper bound and better AP. For example, 36 rays improve by 1.5% AP compared to 18 rays. Also, too many rays, 72 rays, saturate the performance since Ground-truth % <ref type="figure">Figure 7</ref> -Upper Bound Analysis. More rays can model instance mask with higher IoU with Ground Truth, and mass-center is more friendly to represent an instance than box-center. With more rays , e.g. 90 rays improve 0.4% compared to 72 rays; and result is saturated with 120 rays.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Smooth-l1</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Polar IoU</head><p>it already depicts the mask contours well and the number of rays is no longer the main factor constraining the performance. Polar IoU Loss vs. Smooth-l 1 Loss. We test both Polar IoU Loss and Smooth-l 1 Loss in our architecture. We note that the regression loss of Smooth-l 1 Loss is significantly larger than the classification loss since our architecture is a task of dense distance prediction. To cope with the imbalance, we select different factor Î± to regression loss in Smooth-l 1 Loss. Experiment results are shown in <ref type="table" target="#tab_0">Table 1b</ref>. Our Polar IoU Loss achieves 27.7% AP without balancing regression loss and classification loss. In contrast, the best setting for Smooth-l 1 Loss achieves 25.1% AP, a gap of 2.6% AP, showing that Polar IoU Loss is more effective than Smooth-l 1 loss for training the regression task of distances between mass-center and contours.</p><p>We hypothesize that the gap may come from two folds. First, the Smooth-l 1 Loss may need more hyper-parameter search to achieve better performance, which can be timeconsuming compared to the Polar IoU Loss. Second, Polar IoU Loss predicts all rays of one instance as a whole, which is superior to Smooth-l 1 Loss.</p><p>In <ref type="figure" target="#fig_7">Figure 6</ref> we compare some results using the Smoothl 1 Loss and Polar IoU Loss respectively. Smooth-l 1 Loss exhibits systematic artifacts, suggesting that it lacks supervision of the level of the whole object. PolarMask shows more smooth and precise contours.</p><p>Polar Centerness vs. Centerness. Visualization results can be found in the supplementary material. The comparison experiments are shown in <ref type="table" target="#tab_0">Table 1c</ref>. Polar Centerness improves by 1.4% AP overall.</p><p>Particularly, AP 75 and AP L are raised considerably, 2.3% AP and 2.6% AP, respectively. We explain as follows. On the one hand, low-quality masks make more negative effect on high-IoU. On the other hand, large instances have more possibility of large difference between maximum and minimum lengths of rays, which is exactly the problem that Polar Centerness is committed to solve.</p><p>Box Branch. Most of previous methods of instance segmentation require the bounding box to locate area of object and then segment the pixels inside the object. In contrast, PolarMask is capable to directly output the mask without bounding box.</p><p>In this section, we test whether the additional bounding box branch can help improve the mask AP as follows. From <ref type="table" target="#tab_0">Table 1d</ref>, we can see that bounding box branch makes little difference to performance of mask prediction. Thus, we do not have the bounding box prediction head in PolarMask for simplicity and faster speed.</p><p>Backbone Architecture. <ref type="table" target="#tab_0">Table 1e</ref> shows the results of PolarMask on different backbones. It can be seen that better feature extracted by deeper and advanced design networks improve the performance as expected.</p><p>Speed vs. Accuracy. Larger image sizes yield higher accuracy, in slower inference speeds. <ref type="table" target="#tab_0">Table 1f</ref> shows the speed and accuracy trade-off for different input image scales, defined by the shorter image side. The FPS is reported on one V100 GPU. Note that here we report the entire inference time, all post-processing included. It shows   that PolarMask has a strong potentiality to be developed as a real-time instance segmentation application with simple modification. We also report more results in different benchmarks in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison against state-of-the-art</head><p>We evaluate PolarMask on the COCO dataset and compare test-dev results to state-of-the-art methods including both one-stage and two-stage models, shown in <ref type="table" target="#tab_1">Table 2</ref>. Po-larMask outputs are visualized in <ref type="figure" target="#fig_8">Figure 8</ref>. For data augmentation, we randomly scale the shorter side of images in the range from 640 to 800 during the training.</p><p>Without any bells and whistles, PolarMask is able to achieve competitive performance with more complex onestage methods. With a simpler pipeline and half training epochs, PolarMask outperforms YOLACT with 0.9 mAP. Moreover, the best PolarMask with deformable convolutional layers <ref type="bibr" target="#b7">[8]</ref> can achieve 36.2 mAP, which is comparable with state-of-the-art methods.</p><p>In the supplementary material, we compare the FPS between TensorMask and PolarMask with the same image size and device. PolarMask can run at 12.3 FPS with the ResNet-101 backbone, which is 4.7 times faster than Ten-sorMask. Even when equipped with DCN <ref type="bibr" target="#b7">[8]</ref>, PolarMask can still be three times faster than TensorMask.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>PolarMask is a single shot anchor-box free instance segmentation method. Different from previous works that typically solve mask prediction as binary classification in a spatial layout, PolarMask puts forward to represent a mask by its contour and model the contour by one center and rays emitted from the center to the contour in polar coordinate. PolarMask is designed almost as simple and clean as single-shot object detectors, introducing negligible computing overhead. We hope that the proposed PolarMask framework can serve as a fundamental and strong baseline for single-shot instance segmentation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Distance Label Generation</head><p>Here we explain the detail of distance label generation in Algorithm 1. Firstly, we get the contours of one instance. Methods like cv2.findContours in OpenCV can be applied to obtain the contour. Second we traverse every point on the contour to calculate the distance and the angle from this point to the center of instance. Thirdly we take out distance with corresponding angle (e.g., 36 rays, âˆ†Î¸ = 10 â€¢ ). Note that besides two corner cases discussed in the original paper, there remains another situation: The intersection point between a ray and the contour happens to be a subpixel (i.e., pixel coordinates are not integers), in this case the target angle misses and we can find the nearest angle to replace it. For instance, if 10 â€¢ and corresponding distance miss, however, 9 â€¢ exists, we can use 9 â€¢ as its regression target.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Computation Complexity and Speed Analysis</head><p>In this section, we compare the complexity and speed of different methods. All the settings are the same for fair comparison for all methods. <ref type="table" target="#tab_5">Table 3</ref> shows the computation complexity and parameters of different methods. We set the input image size equals 800 * 1280 and ResNet50 as the backbone for all methods. Note that PolarMask without box branch introduces marginal cost when compared with object detector FCOS in both computation complexity and parameters. for each point âˆˆ Contour do 4:</p><p>Calculate distance and angle from point to center 5:</p><p>Append distance to D, angle to A 6: end for 7:</p><p>Get distance set D, angle set A 8: 9:</p><p>Initialize distance label L D 10:</p><p>for angle Î¸ âˆˆ [0,10,20,. . . ,360] do 11:</p><p>if Find angle Î¸ in A then 12:</p><p>if angle has multiple distances d then 13:</p><p>Find the maximum d 14:</p><p>else 15:</p><p>Find   <ref type="table" target="#tab_3">Table 4</ref> -Speed analysis of different methods. All post-processing are included. The input images are resized to have their shorter side being 800 and their longer side less or equal to 1333. PolarMask has large advantages than TensorMask in speed even equipped with deformable convolutional operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Polar Centerness vs. Centerness</head><p>We visualize the predicted results of PolarMask trained with Polar Centerness and original Centerness in <ref type="figure" target="#fig_9">Fig. 9</ref>. Visualized images demonstrate that Polar Centerness can optimize the model to automatically increase the weight of high-quality positive samples while decreasing the weight of low-quality positive samples in instance segmentation tasks, and the distances regression of rays are more accuracy. Thanks to Polar Centerness, the high-quality center will make the network more easily to regress because of the distances of different rays are more balanced under the con- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. More Benchmark Results for PolarMask</head><p>We present more benchmark results of our proposed Po-larMask in <ref type="table">Table 5</ref>. All models were tested on the MS-COCO <ref type="bibr" target="#b23">[24]</ref> validation set (minival). "DCN" denotes deformable convolution layers <ref type="bibr" target="#b7">[8]</ref> in backbone and head. "mstrain" means we add multi-scale training strategy, same as TensorMask <ref type="bibr" target="#b3">[4]</ref>, randomly scaling the short side of the image from 640 to 800.</p><p>Multi-scale training can improve the final results at 1%-1.5%. And DCN can boost up to at least 2.3% at the different backbones. Note that the best PolarMask with ResNext-101, DCN, and multi-scale training achieves 35.9 mAP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5. Comparison with ESE-Seg</head><p>PolarMask and ESE-Seg are concurrent works and conducted independently. Although ESE-Seg also used polar representation, we propose 'Polar IoU Loss' and 'Polar Centerness', which are two modules that were not presented in ESE-Seg. They're unique in PolarMask and crucial to improve performance (ours outperforms ESE-Seg by 7.5 AP).</p><p>First, ESE-Seg requires box detection, which is not necessary in PolarMask. This is a significant difference. Second, ESE-Seg sets the inner center as positive sample. But PolarMask applies 'Polar Centerness' to automatically choose the best sample among multiple positive samples near mass center. Third, ESE-Seg regresses coefficients after Chebyshev polynomial fitting and trains with traditional L2 loss, while PolarMask regresses lengths directly using the proposed 'Polar IoU Loss'.</p><p>As a result, PolarMask significantly outperforms ESE-Seg.  <ref type="table">Table 5</ref> -Benchmark results of PolarMask on MS-COCO <ref type="bibr" target="#b23">[24]</ref> validation set (minival). All the models here are trained with FPN <ref type="bibr" target="#b21">[22]</ref>. For the backbone notation, "R-50" and "R-101" denotes ResNet-50 and ResNet-101 <ref type="bibr" target="#b15">[16]</ref> respectively. "DCN" denotes deformable convolution layers <ref type="bibr" target="#b7">[8]</ref> in backbone and head. "X" denotes the ResNeXt-101 <ref type="bibr" target="#b29">[30]</ref> backbone. "ms" indicates multi-scale.  Interests Chunhua Shen and his employer received no financial support for the research, authorship, and/or publication of this article.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 -</head><label>1</label><figDesc>Instance segmentation with different mask representations. (a) is the original image. (b) is the pixel-wise mask representation. (c) and (d) represent a mask by its contour, in the Cartesian and Polar coordinates, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 -</head><label>2</label><figDesc>(b). Such pixel-to-pixel correspondence The overall pipeline of PolarMask. The left part contains the backbone and feature pyramid to extract features of different levels. The middle part is the two heads for classification and polar mask regression. H, W, C are the height, width, channels of feature maps, respectively, and k is the number of categories (e.g., k = 80 on the COCO dataset), n is the number of rays (e.g., n = 36)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 -</head><label>3</label><figDesc>Mask Assembling. Polar Representation provides a directional angle. The contour points are connected one by one start from 0 â€¢ (bold line) and assemble the whole contour and mask.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 -</head><label>4</label><figDesc>Polar Centerness. Polar Centerness is used to down-weight such regression tasks as the high diversity of rays' lengths as shown in red lines in the middle plot. These examples are always hard to optimize and produce low-quality masks. During inference, the polar centerness predicted by the network is multiplied to the classification score, thus can down-weight the low-quality masks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>2 Figure 5 -</head><label>25</label><figDesc>Mask IoU in Polar Representation. Mask IoU (interaction area over union area) in the polar coordinate can be calculated by integrating the differential IoU area in terms of differential angles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 -</head><label>6</label><figDesc>Visualization of PolarMask with Smooth-l 1 loss and Polar IoU loss. Polar IoU Loss achieves to regress more accurate contour of instance while Smooth-l 1 Loss exhibits systematic artifacts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 -</head><label>8</label><figDesc>Results of PolarMask on COCO test-dev images with ResNet-101-FPN, achieving 30.4% mask AP (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 -</head><label>9</label><figDesc>Visualization of Cartesian Centerness and Polar Centerness. Left: Cartesian Centerness; Right: Polar Centerness. Orange lines are predicted distance of rays, emitting from the center to the contour. White lines are the contours of objects. straint of Polar Centerness.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 -</head><label>1</label><figDesc>26.2 48.7 25.4 11.8 28.2 38.0 24 27.3 49.5 26.9 12.4 29.5 40.1 36 27.7 49.6 27.4 12.6 30.2 39.7 72 27.6 49.7 27.2 12.9 30.0 39.7 (a) Number of Rays: More rays bring a large gain, while too many rays saturate since it already depicts the mask ground-truth well. 24.7 47.1 23.7 11.3 26.7 36.8 0.30 25.1 46.4 24.5 10.6 27.3 37.3 1.00 20.2 37.9 19.6 8.6 20.6 31.1 Polar IoU 1.00 27.7 49.6 27.4 12.6 30.2 39.7 (b) Polar IoU Loss vs. Smooth-L1 Loss: Polar IoU Loss outperforms Smooth-l1 loss, even the best variants of balancing regression loss and classification loss. Ablation experiments for PolarMask. All models are trained on trainval35k and tested on minival, using ResNet50-FPN backbone unless otherwise noted.</figDesc><table><row><cell>loss</cell><cell>Î± AP AP50 AP75 APS APM APL</cell></row><row><cell></cell><cell>0.05</cell></row><row><cell>Smooth-l1</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 )</head><label>2</label><figDesc>. method backbone epochs aug AP AP 50 AP 75 AP S AP M AP L</figDesc><table><row><cell>two-stage</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MNC [7]</cell><cell>ResNet-101-C4</cell><cell>12</cell><cell>â€¢</cell><cell>24.6 44.3</cell><cell>24.8</cell><cell>4.7</cell><cell>25.9 43.6</cell></row><row><cell>FCIS [21]</cell><cell>ResNet-101-C5-dilated</cell><cell>12</cell><cell>â€¢</cell><cell>29.2 49.5</cell><cell>-</cell><cell>7.1</cell><cell>31.3 50.0</cell></row><row><cell cols="2">Mask R-CNN [15] ResNeXt-101-FPN</cell><cell>12</cell><cell>â€¢</cell><cell>37.1 60.0</cell><cell cols="3">39.4 16.9 39.9 53.5</cell></row><row><cell>one-stage</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ExtremeNet [35]</cell><cell>Hourglass-104</cell><cell>100</cell><cell></cell><cell>18.9 44.5</cell><cell cols="3">13.7 10.4 20.4 28.3</cell></row><row><cell>TensorMask [4]</cell><cell>ResNet-101-FPN</cell><cell>72</cell><cell></cell><cell>37.1 59.3</cell><cell cols="3">39.4 17.1 39.1 51.6</cell></row><row><cell>YOLACT [2]</cell><cell>ResNet-101-FPN</cell><cell>48</cell><cell></cell><cell>31.2 50.6</cell><cell cols="3">32.8 12.1 33.3 47.1</cell></row><row><cell>PolarMask</cell><cell>ResNet-101-FPN</cell><cell>12</cell><cell>â€¢</cell><cell>30.4 51.9</cell><cell cols="3">31.0 13.4 32.4 42.8</cell></row><row><cell>PolarMask</cell><cell>ResNet-101-FPN</cell><cell>24</cell><cell></cell><cell>32.1 53.7</cell><cell cols="3">33.1 14.7 33.8 45.3</cell></row><row><cell>PolarMask</cell><cell>ResNeXt-101-FPN</cell><cell>12</cell><cell>â€¢</cell><cell>32.9 55.4</cell><cell cols="3">33.8 15.5 35.1 46.3</cell></row><row><cell>PolarMask</cell><cell>ResNeXt-101-FPN-DCN</cell><cell>24</cell><cell></cell><cell>36.2 59.4</cell><cell cols="3">37.7 17.8 37.7 51.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 -</head><label>2</label><figDesc>Instance segmentation mask AP on the COCO test-dev. The standard training strategy<ref type="bibr" target="#b13">[14]</ref> is training by 12 epochs; and 'aug' means data augmentation, including multi-scale and random crop. is training with 'aug', â€¢ is without 'aug'.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc>compare the speed of different methods. The testing time includes the model inference and postprocessing, such as Non-Maximum Suppression (NMS).The test images are resized to have their shorter side being 800 and their longer side less or equal to 1333. Polar-Mask is faster than the one-stage method TensorMask and two-stage method Mask R-CNN due to the simple pipeline design. Even when equipped with DCN PolarMask can still be three times faster than TensorMask.Algorithm 1 Distance Label Generation (e.g., 36 rays) Require: Contour: Contour, Center Sample: center, 1: function DISTANCE CALCULATE(Contour, center)</figDesc><table><row><cell>2:</cell><cell>Initialize distance set D, angle set A</cell></row><row><cell>3:</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 -</head><label>3</label><figDesc></figDesc><table><row><cell cols="5">Computation complexity and parameters comparison with</cell></row><row><cell cols="5">other methods. Note that "PolarMask w/o box" only introduces</cell></row><row><cell cols="3">marginal computation when compared with FCOS.</cell><cell></cell><cell></cell></row><row><cell>method</cell><cell>backbone</cell><cell cols="3">Device FPS Time(ms)</cell></row><row><cell>PolarMask</cell><cell>R-101</cell><cell>V100</cell><cell>12.3</cell><cell>81</cell></row><row><cell>PolarMask</cell><cell>R-101-DCN</cell><cell>V100</cell><cell>8.18</cell><cell>122</cell></row><row><cell>TensorMask [4]</cell><cell>R-101</cell><cell>V100</cell><cell>2.63</cell><cell>380</cell></row><row><cell cols="2">Mask R-CNN [15] R-101</cell><cell>M40</cell><cell>5.12</cell><cell>195</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 compares</head><label>6</label><figDesc>PolarMask with ESE-Seg on COCO 2017 val. It demonstrates PolarMask improves performance significantly and surpasses ESE-Seg by nearly 7.5 AP.Acknowledgements and Declaration of Conflicting</figDesc><table><row><cell>method</cell><cell>backbone</cell><cell cols="2">epochs ms-train</cell><cell>AP</cell></row><row><cell cols="2">PolarMask R-50</cell><cell>12</cell><cell>â€¢</cell><cell>29.1</cell></row><row><cell></cell><cell>R-50-DCN</cell><cell>12</cell><cell>â€¢</cell><cell>32.0</cell></row><row><cell></cell><cell>R-50</cell><cell>24</cell><cell></cell><cell>30.5</cell></row><row><cell></cell><cell>R-50-DCN</cell><cell>24</cell><cell></cell><cell>33.3</cell></row><row><cell></cell><cell>R-101</cell><cell>12</cell><cell>â€¢</cell><cell>30.4</cell></row><row><cell></cell><cell>R-101-DCN</cell><cell>12</cell><cell>â€¢</cell><cell>33.5</cell></row><row><cell></cell><cell>R-101</cell><cell>24</cell><cell></cell><cell>31.9</cell></row><row><cell></cell><cell>R-101-DCN</cell><cell>24</cell><cell></cell><cell>34.3</cell></row><row><cell></cell><cell>X-101</cell><cell>12</cell><cell>â€¢</cell><cell>32.6</cell></row><row><cell></cell><cell>X-101-DCN</cell><cell>12</cell><cell>â€¢</cell><cell>34.9</cell></row><row><cell></cell><cell>X-101</cell><cell>24</cell><cell></cell><cell>33.5</cell></row><row><cell></cell><cell>X-101-DCN</cell><cell>24</cell><cell></cell><cell>35.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 -</head><label>6</label><figDesc>Comparasion of PolarMask and ESE-Seg on COCO 2017 val. is equipped with 'Polar IoU Loss' and 'Polar Centerness', â€¢ is not.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">It is optional to have the box prediction branch or not. As we empirically show, the box prediction branch has little impact on mask prediction.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep watershed transform for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5221" to="5229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Yolact: Real-time instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bolya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanyi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Mmdetection: Open mmlab detection toolbox and benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dazhi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qijie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Tensormask: A foundation for dense object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>DollÃ¡r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Instance-sensitive fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="534" to="549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Instance-aware semantic segmentation via multi-task network cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3150" to="3158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Centernet: Keypoint triplets for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwen</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6569" to="6578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Piotr DollÃ¡r, and Kaiming He</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Piotr DollÃ¡r, and Ross Girshick. Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Densebox: Unifying landmark localization with end to end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yafeng</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinan</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.04874</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Mask scoring R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaojin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongchao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6409" to="6418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Foveabox: Beyond anchor-based object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.03797</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahab</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00982</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fully convolutional instance-aware semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2359" to="2367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollar. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifang</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8759" to="8768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Cell detection with star-convex polygons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Weigert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Coleman</forename><surname>Broaddus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gene</forename><surname>Myers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Medical Image Computing and Computer-Assisted Intervention</title>
		<meeting>Int. Medical Image Computing and Computer-Assisted Intervention</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="265" to="273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">FCOS: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Explicit shape encoding for real-time instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fubo</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5168" to="5177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Reppoints: Point set representation for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arXiv: Comp. Res. Repository</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unitbox: An advanced object detection network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhimin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Int. Conf. Multimedia</title>
		<meeting>ACM Int. Conf. Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="516" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>KrÃ¤henbÃ¼hl</surname></persName>
		</author>
		<title level="m">Objects as points. arXiv: Comp. Res. Repository</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Bottom-up object detection by grouping extreme and center points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="850" to="859" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
