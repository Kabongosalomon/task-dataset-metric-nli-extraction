<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Enhancing Relation Extraction Using Syntactic Indicators and Sentential Contexts</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiongxing</forename><surname>Tao</surname></persName>
							<email>taoqiongxing@shu.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cn</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangfeng</forename><surname>Luo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luoxf@shu</forename><forename type="middle">Edu</forename><surname>Cn</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
							<email>wang-hao@shu.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Engineering and Science</orgName>
								<orgName type="institution">Shanghai University Shanghai</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Engineering and Science</orgName>
								<orgName type="institution">Shanghai University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Engineering and Science</orgName>
								<orgName type="institution">Shanghai University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Enhancing Relation Extraction Using Syntactic Indicators and Sentential Contexts</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-relation extraction</term>
					<term>syntactic indicators</term>
					<term>senten- tial context</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>State-of-the-art methods for relation extraction consider the sentential context by modeling the entire sentence. However, syntactic indicators, certain phrases or words like prepositions that are more informative than other words and may be beneficial for identifying semantic relations. Other approaches using fixed text triggers capture such information but ignore the lexical diversity. To leverage both syntactic indicators and sentential contexts, we propose an indicator-aware approach for relation extraction. Firstly, we extract syntactic indicators under the guidance of syntactic knowledge. Then we construct a neural network to incorporate both syntactic indicators and the entire sentences into better relation representations. By this way, the proposed model alleviates the impact of noisy information from entire sentences and breaks the limit of text triggers. Experiments on the SemEval-2010 Task 8 benchmark dataset show that our model significantly outperforms the state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Relation extraction is the task of assigning a semantic relation to the target entity pair in a given sentence. Accurately extracting semantic relations from unstructured texts is important for many natural language applications, such as information extraction <ref type="bibr" target="#b0">[1]</ref>  <ref type="bibr" target="#b1">[2]</ref>, question answering <ref type="bibr" target="#b2">[3]</ref>  <ref type="bibr" target="#b3">[4]</ref>, and construction of semantic networks <ref type="bibr" target="#b4">[5]</ref>  <ref type="bibr" target="#b5">[6]</ref>.</p><p>Recent approaches for relation extraction primarily concentrate on deep neural networks <ref type="bibr" target="#b6">[7]</ref>- <ref type="bibr" target="#b11">[12]</ref>. Commonly, these models encode the entire sentence to capture the contextual information for relation representation, based on the assumption that each word in a sentence helps classify relations. A majority of these methods use entity information to improve the performance of relation extraction, such as entity position <ref type="bibr" target="#b6">[7]</ref>  <ref type="bibr" target="#b7">[8]</ref>, entity hypernym <ref type="bibr" target="#b6">[7]</ref> and latent entity typing <ref type="bibr" target="#b12">[13]</ref>. They all assume that the information related to target entities is more important. However, these models have two disadvantages: first, some words in a sentence irrelevant to the relation are as noises to classification; second, entity information is very limited in predicting relation types and the contributions from other words are prone to be ignored.</p><p>Besides, a few approaches rely on particular lexical constraints <ref type="bibr" target="#b13">[14]</ref> and relation triggers <ref type="bibr" target="#b14">[15]</ref> that explicitly indicate the occurrence of relations in sentences. However, these meth- ods are not suited to cases where no relation trigger found in the sentence.</p><p>In this paper, we revisit the problem from another perspective. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, the phrase moved into is the key to identify the relation type Entity-Destination(e1,e2). On the contrary, it is insufficient to recognize relation types by the linguistic features about entity boss and entity office, let alone a non-existent explicit relation trigger. Intuitively, words like of and from are informative for relation extraction. Here and after we call this kind of words or phrases syntactic indicator. Syntactic indicator contains vibrant information for identifying semantic relations between target entities. Besides, the words My, new, yesterday in the first sentence are ubiquitous while not useful for relation identification. We can acquire better performance by reducing their impact.</p><p>Therefore, we propose an indicator-aware neural model to condition both the syntactic indicator and the sentential context for better performance on relation extraction. This is achieved by a two-phase process. Firstly, under the guidance of syntactic knowledge, we extract syntactic indicators by removing unrelated words through entity disambiguation, principal component extraction, and unrelated entity removal. Then, we feed both of entire sentences and syntactic indicators into a contextual encoder based on the pre-trained BERT (Bidirectional Encoder Representations from Transformers) <ref type="bibr" target="#b15">[16]</ref> to encode the semantic relation representations. The syntactic indicator is treated as the principal constraint on the contextual representation. By this way, the proposed model takes advantage of the relevant information and reduces the impact of noisy words. Our main contributions are listed as follows:</p><p>• We define syntactic indicators that help to distinguish relation types and extract syntactic indicators under the guidance of syntactic knowledge, which is conducive to capture the important information and reduce noisy information that is irrelevant to relation extraction. <ref type="bibr">•</ref> We propose an indicator-aware neural model using the pre-extracted indicators to improve relation extraction, which makes use of the key information by imposing constraints on contextual representations for better prediction. • The proposed model obtains an F1-score of 90.36% on the benchmark dataset, outperforming the state-ofthe-art methods. More ablation experiments demonstrate that incorporating syntactic indicators into contextual representations significantly improves the performance of relation extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Conventional non-neural models for relation extraction include feature-based models <ref type="bibr" target="#b16">[17]</ref> [18] and kernel-based models <ref type="bibr" target="#b18">[19]</ref>  <ref type="bibr" target="#b19">[20]</ref>. These methods invariably suffer from error propagation due to their high dependence on the manual feature extraction process. Besides they may omit useful information for relation extraction. Therefore, the performance of these methods is very limited.</p><p>Recently, a variety of works for relation extraction focus on deep neural networks. These methods mitigate the problem of error propagation and show promising results. On the one hand, Zeng et al. <ref type="bibr" target="#b6">[7]</ref> propose a deep convolutional neural network (CNN) to address this task. They utilize sentence-level features and lexical level features, including entities, left and right tokens of entities and WordNet hypernyms of entities. Santos et al. <ref type="bibr" target="#b7">[8]</ref> propose the Ranking CNN (CR-CNN) model using a new rank loss to reduce the impact of artificial classes. They also demonstrate that the words between target nominals are almost as useful as using positing embeddings. Inspired by their work, we extract syntactic indicators from the text between two entities. Shen and Huang <ref type="bibr" target="#b11">[12]</ref> propose attentionbased convolutional neural network (Attention-CNN), which employs a word-level attention mechanism to get the critical information for relation representation. These methods have limitations on learning sequence structures because of the shortages of convolutional neural networks. On the other hand, the RNN-based models show outstanding performance in learning the linguistic structure in text. Zhang and Wang <ref type="bibr" target="#b20">[21]</ref> propose a bidirectional recurrent neural network (Bi-RNN) to learn the long-term dependency between two entities, however, it suffers the vanishing gradient problem in RNNs. Soon after, Zhang et al. <ref type="bibr" target="#b8">[9]</ref> apply the bidirectional LSTM network (Bi-LSTM) and utilize the word position and external features to improve the performance of relation extraction, including POS tags, named entity information, and dependency parse. In <ref type="bibr" target="#b9">[10]</ref>, Zhou et al. apply attention mechanisms in bidirectional LSTM networks (Attention Bi-LSTM). Xiao and Liu <ref type="bibr" target="#b10">[11]</ref> separate each sentence into three context subsequences according to the locations of two target entities and use a Hierarchical Recurrent Neural Network with two Attention Bi-LSTM networks (Hier Attention Bi-LSTM) to get a better result. Most recently, Lee et al. <ref type="bibr" target="#b12">[13]</ref> propose a model incorporating entity-aware attention mechanisms with a latent entity typing (LET) and obtain state-of-the-art performance.</p><p>Approaches mentioned above encode the entire sentence to capture the contextual features, resulting in ignorance of other important features in sentences. Although a number of methods utilize various entity information, including the entity position, entity semantics, latent entity typing, and entity hypernyms, and such information holds an irreplaceable impact on identifying relations, it is too limited to fully capture distinctive features.</p><p>There are also some works concentrate on relation triggers, the phrases that explicitly expresses the occurrence of one relationship in the given text. Björne et al. <ref type="bibr" target="#b14">[15]</ref> propose the relation triggers and determine their arguments to reduce the complexity of the task. Open IE systems ReVerb <ref type="bibr" target="#b13">[14]</ref> also uses special phrases to identify different relation types by lexical constraints. Nevertheless, there are many texts with no explicit relation trigger inside, semantical relations cannot be extracted from such sentences with these methods. Unlike these methods, our approach makes use of syntactic indicators, which can be able to vary with the different expressions of semantic relations rather than match fixed phrases templates.</p><p>Pre-trained Language models have shown the great success on many NLP tasks <ref type="bibr" target="#b21">[22]</ref>  <ref type="bibr" target="#b22">[23]</ref>. Especially, BERT proposed by Devlin et al. shows a significant impact <ref type="bibr" target="#b15">[16]</ref>, which learns the deep bidirectional representations by jointly conditioning on both left and right context in the training procedure. It has been applied to multiple NLP tasks and obtains new startof-the-art results on eleven tasks, such as text classification, sequence labeling, and question answering. In recent research, Wu and He <ref type="bibr" target="#b23">[24]</ref> propose an R-BERT model, which employs the pre-trained BERT language model and reaches the top of the leaderboard in relation extraction.</p><p>By the way, related works on the relation extraction can be mainly grouped into two categories, supervised methods <ref type="bibr" target="#b6">[7]</ref> [11] <ref type="bibr" target="#b24">[25]</ref> and distant supervised methods <ref type="bibr" target="#b25">[26]</ref>- <ref type="bibr" target="#b27">[28]</ref>. They are different in whether the data contains a large number of noisy labels. Supervised methods without noisy labels achieve more reliable results, which play a dominant role in the relation classification. In this paper, we focus on supervised relation extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. OUR MODEL</head><p>In this section, we first give an overview of the proposed indicator-aware neural model. After that, we present each module in details. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Model Architecture</head><p>The overall architecture of the proposed model is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>.</p><p>Given a sentence, we first extract the corresponding syntactic indicator under the guidance of syntactic knowledge (the process detailed in the following paragraphs). Subsequently, the entire sentence and the indicator sequence are concatenated after WordPiece tokenization <ref type="bibr" target="#b28">[29]</ref>. Then, we feed the aggregate token sequence into a BERT-based contextual encoder to learn the deep bidirectional representation for each token. The final representations of the aggregate sequence, two entities, and the syntactic indicators are respectively acquired with different operations in the later network layers. At last, these vector representations are concatenated to produce a final prediction distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Definition of Syntactic Indicators</head><p>Definition: The syntactic indicator is certain words or phrases in a sentence, providing essential information to identify the semantic relation between target entities.</p><p>Different from text triggers, syntactic indicators are rich in manifestation rather than match fixed phrase templates. Each sentence produces an exclusive syntactic indicator. It may consist of any verbs, prepositions, pronouns or phrases, relying on the current language expression. As shown in <ref type="figure" target="#fig_2">Fig. 3</ref>, caused by is the syntactic indicator in the first instance. Accordingly, we can affirm that relation Cause-Effect(e2,e1) exists in two target enties e 1 =shock and e 2 =attack. Similarly in the other two instances, we can recognize relation Content-Container(e1,e2) and relation Instrument-Agency(e2,e1) based are enclosed in and using, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Syntactic Indicator Extraction</head><p>We extract syntactic indicators from the text between two target entities by removing irrelevant words. Fortunately, the target subsequence is accessible from a sentence via entity markers. After that, we acquire the syntactic indicators under the guidance of syntactic knowledge, which can be characterized as follows: a) Entity Disambiguation: Nouns that are around with a conjunction word and or or, and compound nouns that consist of no less than two nouns will be disambiguated by removing the restrictive and supplementary words. As shown in <ref type="figure" target="#fig_2">Fig. 3</ref>, shock and anger is transformed to shock, plastic case and propagation method are transformed to case and method, we remove the highlighted parts marked with a subscript 1. Each instance in the labeled data contains only one relationship, like the relation in the first example of <ref type="figure" target="#fig_2">Fig. 3</ref> is about shock and attack, so nouns in target entities naturally remain. b) Principal Component Extraction: Remove adjectives, adverbs and other modifiers from the text to obtain the principal components, expressing the primary semantic relations. In <ref type="figure" target="#fig_2">Fig. 3</ref>, the highlighted parts marked with a subscript 2 are removed from subsequences, such as [the, surprise], [a, clear, hard], and [first, the, infeasible, the, constraint]. c) Unrelated Entity Removal: Remove any other named entity and the corresponding actions except two target entities to obtain an indicator sequence shaped like shock caused by attack, coins are enclosed in case and analyzer using method shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. In the third instance, the irrelevant entity paths and its corresponding action identifies are removed.</p><p>Finally, we acquire an exclusive indicator sequence from a given sentence, which deemed without any irrelevant words. The syntactic indicator is included between two target entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. BERT-based Contextual Encoder</head><p>The pre-trained BERT language representation model <ref type="bibr" target="#b15">[16]</ref> is a multi-layer bidirectional transformer encoder <ref type="bibr" target="#b29">[30]</ref>, designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers. The input of BERT can be able to a single sentence or a pair of sentences. A special token [CLS] is always the first token of each sequence. Sentence pairs are separated with a token [SEP] and packed together into a single sequence. BERT is the first fine-tuning based representation model for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. Because of the ubiquitous use of BERT recently, we will omit an exhaustive background description of the architecture of BERT.</p><p>a) BERT Module: Given the sentence S, we insert four markers e11, e12, e21 and e22 at the beginning and end of two target entities (e 1 , e 2 ), which conduces to capture the entity locations. While the corresponding indicator sequence S * always start with entity e 1 and end with e 2 , we insert # behind e 1 and insert $ before e 2 to mark the syntactic indicator.</p><p>To fine-tune BERT, we feed both two sequences into the WordPiece tokenizer and then concatenate the obtained subtokens into a single token sequence T . Following the original implementation of BERT, we add a token [CLS] to the beginning of the token sequence and separate two sequences with a token <ref type="bibr">[SEP]</ref>. Then, we feed T into the BERT to produce the current representation of each token. b) Aggregate Sequence Representation: The final hidden state sequence H output from the BERT module corresponds to the task-oriented embedding of each token. Suppose H 0 is the hidden state of first special token [CLS], we add an activation operation and a fully connected layer to obtain a vector H 0 as the representation of the aggregate sequence.</p><formula xml:id="formula_0">H 0 = W 0 (tanh (H 0 )) + b 0<label>(1)</label></formula><p>c) Entity Representations: Hidden state H m , H n , H p and H q are vector representations of four entity markers e11, e12, e21 and e22. For the target entities, vectors between H m and H n represent entity e 1 , and vectors between H p and H q represent entity e 2 . We apply an average operation to get a single vector representation following with a tanh activation operation and a fully connected layer. In this step, two entities share the same parameters W e and b e . As the following equations, the final representations of two target entities are respectively H e1 , H e2 :</p><formula xml:id="formula_1">H e1 =W e tanh 1 n − m + 1 n t=m H t + b e H e2 =W e tanh 1 q − p + 1 q t=p H t + b e (2)</formula><p>d) Syntactic Indicator Representation: H i+1 to H i+j are the hidden state vectors corresponds to the indicator sequence S * . We also apply an average operation following with a tanh activation operation and a fully connected layer to obtain the final representation:</p><formula xml:id="formula_2">z = W z tanh 1 j j t=1 H i+t + b z (3)</formula><p>where H i+t is the t th vector representation in S * . For fine-tuning, we concatenate H 0 , H e1 , H e2 , and z, then consecutively add two fully connected layers with weights W 1 , W 2 and biases b 1 , b 2 . Finally, we obtain a relation representation vector r used for classifying relations.</p><formula xml:id="formula_3">r = W 2 [W 1 [concat (H 0 , H e1 , H e2 , z)] + b 1 ] + b 2 (4)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Relation classifer</head><p>Given an instance x with entire sentence S and indicator sequence S * , we can obtaine the relation representation r by the relation encoder. For classifying, we apply a fully connected softmax layer to produce a probability distribution p (y|x, θ) over all predefined relation types:</p><formula xml:id="formula_4">p (y|x, θ) = softmax (W * r + b * )<label>(5)</label></formula><p>where y ∈ Y is the target relation type, θ refers all learnable parameters in the network including W * ∈ R |Y |×d h and b * ∈ R |Y | , where |Y | is the number of relation types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Training Procedure</head><p>For the perpose of making a clear distinction between different relation categories and reducing the influence of noise, we design our loss function based on the commonly used cross-entropy, referring to the rank loss function proposed by Santos et al. <ref type="bibr" target="#b7">[8]</ref>. The total loss L on a batch with the size of k can be expressed as the following equation:</p><formula xml:id="formula_5">L = − k i=1 log p y + |c, θ − β k i=1 log 1 − p y − |c, θ + λ θ 2 2 (6)</formula><p>where the first term in the right side decreases as the probability p (y + |c, θ) increases and the second term with a hyperparameter β in the right side decreases as the the probability p (y − |c, θ) decreases. For each instance, y + ∈ Y is the correct relation label, while y − ∈ Y is a negative category chose with the highest probability among all incorrect relation types in each training round:</p><formula xml:id="formula_6">y − = arg max y∈Y ;y =y + p (y|x, θ)<label>(7)</label></formula><p>In relation extraction, an artificial class Other is used to refer to the relation between target entities that does not belong to any of natural classes. Therefore, the class Other is too noisy to have common representative characteristics since it consists of many different categories of relations. For this reason, we calculate loss on each relation class except Other to reduce the impact of noise, reflected in the loss function as y + = Other and y − = Other.</p><p>To alleviate overfitting, we add a dropout layer before the fully connected softmax layer in training procedure and constrain the L2 regularization with a coefficient λ as the third term in the right side.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS A. Dataset and Evaluation Metric</head><p>To evaluate the performance of our model, we conduct experiments on the SemEval-2010 Task 8 dataset <ref type="bibr" target="#b30">[31]</ref>, the published benchmark for relation extraction. The dataset contains 10, 717 annotated instances, including 8, 000 instances for training and 2, 717 instances for testing. All instances are annotated with 9 directed relations types and an artificial class Other. Nine directed relations are respectively Cause-Effect, Instrument-Agency, Product-Producer, Content-Container, Entity-Origin, Entity-Destination, Component-Whole, Member-Collection, and Message-Topic. We take direction into consideration and the total number of relation types is 19. We adopt macro-averaged F1-score for nine actual relations (excluding Other) to evaluate the model, which is the official evaluation metric for SemEval-2010 Task 8. Hyper-parameter β in Loss Function 5.0</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental Settings</head><p>For the pre-trained BERT model, we use the uncased model to integrate our approach. The hyper-parameters we set in the proposed model are shown in table I. Furthermore, the parameters of the pre-trained BERT model are initialized according to the original <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Result</head><p>Results of various neural models are demonstrated in table II. We achieve a strong empirical result based on the proposed approach.  <ref type="table" target="#tab_0">Table II</ref> shows that our model obtains an F1-score of 90.36%, outperforming the state-of-the-art models substantially. The best results of the CNN-based and RNN-based models range from 84% to 86%, while the recent R-BERT model proposed by Wu and He <ref type="bibr" target="#b23">[24]</ref> obtains the best F1score of 89.25%, which has an approximately 4-point gap with previous methods. It is noteworthy that the proposed relation extraction model introducing syntactic indicators has a further performance improvement in this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Analysis</head><p>To demonstrate that introducing syntactic indicators indeed affects relation extraction, we create two more settings to conduct experiments for comparison and further build another neural model without BERT structure for more forceful evidence. Experimental results shown in table III provide ample proof that incorporating syntactic indicators indeed improves the performance of relation extraction. a) Experiments on BERT-based Model: that indicator sequences contain enough information for classifying relations but are likely to provide incomplete information. The proposed BERT-based model leverages both the syntactic indicator and the sentential context for relation extraction, which can be considered to be able to maintain a balance between reducing noise and capturing complete features. b) Experiments on Non-BERT Model:</p><p>• We construct a model without BERT structure for further confirmations, which consists of a CNN module to capture the indicative features from indicator sequences and a Bi-LSTM module to capture the contextual information from entire sentences. Experimental results obtained from the Non-BERT structure model are listed in lines six through nine in table III. The model obtains an F1score of 85.9% by combining the information from two modules, which outperforms the best CNN-based and RNN-based models. Even compared with the approaches using high-level lexical features such as WordNet, DPT, DEP, NLP tags or NER tags, it also has the best result. Likewise, we separately feed one of the sequences into the model. Correspondingly, the entire sentence is encoded using the Bi-LSTM module while the syntactic indicator is encoded using the CNN module. Unsurprisingly, both of the F1-scores are not bad but lower, which further proves the validity of the constraint on relation representations by syntactic indicators. • We further capture the features of the entire sentence twice using the CNN module and Bi-LSTM module respectively and then combine them to make a final prediction, the result becomes worse instead. This proves that noisy information unrelated to entity relations exist in the sentence, and excessive use of irrelevant features as relational features will degrade the performance of relation extraction. Therefore, it is very necessary to impose constraints on semantic relation representations to avoid the impact of noisy information. c) Contributions of Syntactic Indicators:</p><p>• <ref type="table" target="#tab_0">Table IV</ref> shows the contributions of syntactic indicators on precision, recall and F1-score for each relation cat- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>In this paper, we propose syntactic indicators that are insensitive to lexical word forms and a novel indicator-aware neural model leveraging both syntactic indicators and sentential contexts to fulfill the relation extraction. The proposed approach performed on BERT-based model achieves an F1score of 90.36% in SemEval-2010 Task 8, outperforming the state-of-the-art methods. The implementation with the non-BERT model also achieves the best result in CNN-based and RNN-based models. Thanks to the incorporating of syntactic indicators, capturing more determinative features for classifying relations while reducing noise impact, our approach effectively improves the performance of relation extraction.</p><p>In the future, we expect to leverage the syntactic indicators into more complex multi-relation extraction and distantly supervised relation extraction. Furthermore, we will research how to utilize the deep neural network to automatically locate the indicator in sentences, rather than extract indicators under the guidance of syntactic knowledge.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>The decisive influence of syntactic indicators in identifying relations. The right part shows the syntactic indicator and the correct relation between target entities for each instance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>The overall architecture of the proposed model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Syntactic Indicator Extraction. The blue highlights with a subscript 1 are removed abiding the first rule, Entity Disambiguation. And the orange highlights with a subscript 2 and the green highlights with a subscript 3 are removed respectively abiding Principal Component Extraction and Unrelated Entities Removal.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I</head><label>I</label><figDesc></figDesc><table><row><cell>HYPER-PARAMETERS</cell><cell></cell></row><row><cell>Description</cell><cell>Value</cell></row><row><cell>Max Sequence Length after Tokenization</cell><cell>128</cell></row><row><cell>Batch Size for Training</cell><cell>16</cell></row><row><cell>Initial Learning Rate for Adam</cell><cell>2 × 10 −5</cell></row><row><cell>Number of Training Epochs</cell><cell>5.0</cell></row><row><cell>Dropout Rate</cell><cell>0.1</cell></row><row><cell>L2 Regularixation Coefficient</cell><cell>5 × 10 −3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II</head><label>II</label><figDesc></figDesc><table><row><cell cols="2">PERFORMANCE COMPARISON ON EXTRACTING RELATIONS</cell></row><row><cell>Model</cell><cell>F1</cell></row><row><cell>CNN (Zeng et al., 2014) [7]</cell><cell>78.9</cell></row><row><cell>+ WN</cell><cell>82.7</cell></row><row><cell>CR-CNN (Santos et al., 2015) [8]</cell><cell>84.1</cell></row><row><cell>Attention CNN (Shen and Huang, 2016) [12]</cell><cell>84.3</cell></row><row><cell>+ POS, WN, WAN</cell><cell>85.9</cell></row><row><cell>Bi-LSTM (Zhang et al., 2015) [21]</cell><cell>82.7</cell></row><row><cell>+ POS, NER, DEP, WN</cell><cell>84.3</cell></row><row><cell>Attention Bi-LSTM (Zhou et al., 2016) [10]</cell><cell>84.0</cell></row><row><cell>Hier Attention Bi-LSTM (Xiao and Liu, 2016) [11]</cell><cell>84.3</cell></row><row><cell>Attention Bi-LSTM (Lee et al., 2019) [13]</cell><cell>84.7</cell></row><row><cell>+ LET</cell><cell>85.2</cell></row><row><cell>R-BERT (Wu et al., 2019) [24]</cell><cell>89.25</cell></row><row><cell>Indicator-aware BERT (Ours)</cell><cell>90.36</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III EXPERIMENTAL</head><label>III</label><figDesc>RESULTS BASED ON DIFFERENT INPUT AND MODELS</figDesc><table><row><cell>Model</cell><cell cols="2">Input</cell><cell>F1</cell></row><row><cell></cell><cell cols="2">Entire Sentence + Indicator Sequence</cell><cell>90.36</cell></row><row><cell>BERT-based</cell><cell>Entire Sentence</cell><cell></cell><cell>89.30</cell></row><row><cell></cell><cell>Indicator Sequence</cell><cell></cell><cell>86.79</cell></row><row><cell></cell><cell>LSTM</cell><cell>CNN</cell></row><row><cell></cell><cell cols="2">Entire Sentence Indicator Sequence</cell><cell>85.9</cell></row><row><cell>Non-BERT</cell><cell cols="2">Entire Sentence -</cell><cell>84.4</cell></row><row><cell></cell><cell>-</cell><cell>Indicator Sequence</cell><cell>82.5</cell></row><row><cell></cell><cell cols="2">Entire Sentence Entire Sentence</cell><cell>84.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV CONTRIBUTIONS</head><label>IV</label><figDesc>OF SYNTACTIC INDICATORS ON PRECISION, RECALL AND F1-SCORE FOR EACH RELATION CATEGORY on BERT-based model). The proposed model incorporating syntactic indicators increases the F1-score on each category, where the precisions on all categories except Entity-Destination are increased and the recalls on most categories are improved or remained the same. Especially, the precisions on Instrument-Agency, Member-Collection, Message-Topic and Product-Producer increased by 3.13, 3.06, 2.61 and 5.95 percentage points respectively. The effects of syntactic indicators are more prominently reflected on these categories because of instances containing such types of relations often have more noisy words in the text between target entities.</figDesc><table><row><cell>Relation</cell><cell cols="2">Precision</cell><cell cols="2">Recall</cell><cell cols="2">F1-score</cell></row><row><cell></cell><cell>-</cell><cell>+IS</cell><cell>-</cell><cell>+IS</cell><cell>-</cell><cell>+IS</cell></row><row><cell>Cause-Effect</cell><cell>93.27</cell><cell>94.48</cell><cell>92.99</cell><cell>93.90</cell><cell>93.13</cell><cell>94.19</cell></row><row><cell>Component-Whole</cell><cell>86.52</cell><cell>88.46</cell><cell>88.46</cell><cell>88.46</cell><cell>87.48</cell><cell>88.46</cell></row><row><cell>Content-Container</cell><cell>89.05</cell><cell>90.77</cell><cell>93.23</cell><cell>92.19</cell><cell>91.09</cell><cell>91.47</cell></row><row><cell>Entity-Destination</cell><cell>93.84</cell><cell>93.33</cell><cell>93.84</cell><cell>95.89</cell><cell>93.84</cell><cell>94.59</cell></row><row><cell>Entity-Origin</cell><cell>89.66</cell><cell>90.77</cell><cell>90.70</cell><cell>91.47</cell><cell>90.17</cell><cell>91.12</cell></row><row><cell>Instrument-Agency</cell><cell>85.92</cell><cell>89.05</cell><cell>78.21</cell><cell>78.21</cell><cell>81.88</cell><cell>83.28</cell></row><row><cell cols="2">Member-Collection 84.49</cell><cell>87.55</cell><cell>88.84</cell><cell>87.55</cell><cell>86.61</cell><cell>87.55</cell></row><row><cell>Message-Topic</cell><cell>87.54</cell><cell>90.15</cell><cell>96.93</cell><cell>94.64</cell><cell>92.00</cell><cell>92.34</cell></row><row><cell>Product-Producer</cell><cell>84.84</cell><cell>90.79</cell><cell>89.61</cell><cell>89.61</cell><cell>87.16</cell><cell>90.20</cell></row><row><cell>IS: Indicator Sequence</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">egory(performed</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">• Two additional experiments only use one of the sequences and the experimental results are listed in lines two through four in table III. The experiment only using the entire sentence as input produces an F1-score of 89.30%, which is 1.06% lower than the proposed approach. Although an indicator sequence just composed of a few words, the experiment only using the indicator sequence produces an F1-score of 86.79%. It can be said</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The research reported in this paper was supported in part by the National Natural Science Foundation of China under the grant No. 91746203.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Identifying relations for open information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on empirical methods in natural language processing</title>
		<meeting>the conference on empirical methods in natural language processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1535" to="1545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Open information extraction using wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th annual meeting of the association for computational linguistics</title>
		<meeting>the 48th annual meeting of the association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="118" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Using syntactic and semantic relation analysis in question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S J J Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H C T S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Kan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Text REtrieval Conference (TREC)</title>
		<meeting>the 14th Text REtrieval Conference (TREC)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="500" to="266" />
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
	<note>Special Publication</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semantic parsing for single-relation question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="643" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semantic networks of english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fellbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="197" to="229" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A multilingual database with lexical semantic networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vossen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Kluwer Academic Publishers. doi</publisher>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="978" to="94" />
			<pubPlace>Dordrecht</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Relation classification via convolutional deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Classifying relations by ranking with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="626" to="634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bidirectional long shortterm memory networks for relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Pacific Asia conference on language, information and computation</title>
		<meeting>the 29th Pacific Asia conference on language, information and computation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="73" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Attentionbased bidirectional long short-term memory networks for relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="207" to="212" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semantic relation classification via hierarchical recurrent neural network with attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1254" to="1263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Attention-based convolutional neural network for semantic relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2526" to="2536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semantic relation classification via bidirectional lstm networks with entity-aware attention using latent entity typing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">S</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Symmetry</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">785</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Open question answering over curated and extracted knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1156" to="1165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Extracting contextualized complex biological events with rich graph-based feature sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Björne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heimonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ginter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Airola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pahikkala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salakoski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Intelligence</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="557" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Combining lexical, syntactic, and semantic features with maximum entropy models for extracting relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kambhatla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2004 on Interactive poster and demonstration sessions</title>
		<meeting>the ACL 2004 on Interactive poster and demonstration sessions</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Combining linguistic and statistical analysis to extract relations from web documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ifrim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="712" to="717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Exploiting constituent dependencies for tree kernel-based semantic relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Computational Linguistics</title>
		<meeting>the 22nd International Conference on Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="697" to="704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Subsequence kernels for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Bunescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="171" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Relation classification via recurrent neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01006</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semi-supervised sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3079" to="3087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Universal language model fine-tuning for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="328" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Enriching pre-trained language model with entity information for relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.08284</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Semantic compositionality through recursive matrix-vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 joint conference on empirical methods in natural language processing and computational natural language learning</title>
		<meeting>the 2012 joint conference on empirical methods in natural language processing and computational natural language learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1201" to="1211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1003" to="1011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction via piecewise convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>The Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1753" to="1762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction with an incomplete knowledge base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grishman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gondek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. The Association for Computational Linguistics</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. The Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="777" to="782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hendrickx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Séaghdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Padó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pennacchiotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Szpakowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions</title>
		<meeting>the Workshop on Semantic Evaluations: Recent Achievements and Future Directions</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="94" to="99" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
