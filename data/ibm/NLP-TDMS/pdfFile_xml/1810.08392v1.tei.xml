<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Impact of Corpora Quality on Neural Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matīss</forename><surname>Rikters</surname></persName>
							<affiliation key="aff0">
								<address>
									<addrLine>Vienības gatve 75A</addrLine>
									<settlement>Tilde, Rīga</settlement>
									<country key="LV">Latvia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Impact of Corpora Quality on Neural Machine Translation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>machine translation</term>
					<term>parallel corpora</term>
					<term>corpora filtering</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large parallel corpora that are automatically obtained from the web, documents or elsewhere often exhibit many corrupted parts that are bound to negatively affect the quality of the systems and models that learn from these corpora. This paper describes frequent problems found in data and such data affects neural machine translation systems, as well as how to identify and deal with them. The solutions are summarised in a set of scripts that remove problematic sentences from input corpora.</p><p>Zipporah [2] is a trainable tool for selecting a high-quality subset of data from a huge amount of noisy data. The authors report that it can improve MT quality by up to 2.1 BLEU, but in order to use it, the tool requires a known high-quality data set for training.</p><p>Wolk <ref type="bibr" target="#b2">[3]</ref> proposes a method that uses online MT engines to translate source sentences from a parallel corpus and compare them with the given target sentences. It is very expensive to use on real-world parallel corpora, containing tens of millions of parallel sentences. The author reports results on using the method on rather small corpora of only several million words.</p><p>Khadivi and Ney [4] introduce a parallel corpora filtering method based on word alignment models. Similar to Zipporah, this method also relies on training using a highquality corpus.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Machine translation (MT) systems -both, statistical (SMT) and neural (NMT) -rely on large amounts of parallel data for training the models. It is often the case that larger amounts of corpora lead to higher quality models, therefore a common practice is automatic extraction of such corpora from web resources, digitised books and other sources. Such data is prone to be noisy and include all kinds of problematic sentences alongside the high-quality ones. Data quality plays an important role in training of statistical and, especially, neural network based models like NMT, which is quick to memorise bad examples. In the case of training SMT and NMT systems, often the only pre-processing is done using scripts from the Moses Toolkit <ref type="bibr" target="#b0">[1]</ref>, which is only capable of removing sentences that are longer or shorter than a specified amount or the source-target length ratio is too high.</p><p>In this paper, we explore the types of low-quality sentences commonly found in parallel corpora. We also compare the benefits of using additional filters to remove these sentences before training MT systems in contrast to using only the Moses scripts. We introduce a set of corpora cleaning tools 2 that remove sentences that have some of the most common problems found in large corpora. It is published in GitHub with the MIT open-source license.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Problems in Corpora</head><p>This section outlines some often occurring problems in parallel corpora. The specific examples were obtained from the English-Estonian part of the ParaCrawl 3 corpus.</p><p>One of the most common defects in parallel corpora is a high mismatch between the non-alphabetic characters between source and target sentences ( <ref type="figure">Figure 1</ref>). Also often are sentences that are completely or mostly composed of characters outside the scope of the language in question ( <ref type="figure" target="#fig_0">Figure 2</ref>).</p><p>In parallel corpora, we may occasionally see the same sentence of one language aligned to multiple different ones of the other language ( <ref type="figure" target="#fig_1">Figure 3</ref>), but this is not always a bad indication, since they may just be paraphrases of the same concept ( <ref type="figure" target="#fig_2">Figure 4</ref>). It is also wise to check if sentences in specific languages actually consist of text in that language ( <ref type="figure" target="#fig_3">Figure 5</ref>) as there may be citations and other parts of foreign language texts, especially in news domain corpora.</p><p>Finally, a little less common observation for automatically gathered corpora, but somewhat more often in automatically generated (translated) parallel corpora is the repeating of tokens ( <ref type="figure" target="#fig_4">Figure 6</ref>). Sentences like this may not always be incorrect, but they introduce ambiguity when used to train MT systems. <ref type="figure">Figure 1</ref>. An example of a high mismatch in non-alphabetical character counts between source and target. <ref type="bibr" target="#b2">3</ref> Large-Scale Parallel Web Crawl: http://statmt.org/paracrawl     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Corpora Filters</head><p>The filters described in this section are mainly intended for parallel corpora consisting of two files with identical line-counts where each line of one file is related to the same line of the other file. Several of the filters are applicable to monolingual data as well and can be used to clean data for unsupervised MT training, back-translation, and other use-cases.</p><p>Unique parallel sentence filter -removes duplicate source-target sentence pairs. Equal source-target filter -removes sentences that are identical in the source side and the target side of the corpus.</p><p>Multiple sources -one target and multiple targets -one source filters -removes repeating sentence pairs where the same source sentence is aligned to multiple different target sentences and multiple source sentences aligned to the same target sentence.</p><p>Non-alphabetical filters -remove sentences that contain over 50% non-alphabetical symbols on either the source side or the target and sentence pairs that have significantly more (at least 1:3) non-alphabetical symbols in the source side than in the target side (or vice versa).</p><p>Repeating token filter -especially useful for filtering back-translated parallel corpora that are created by translating a clean monolingual corpus into another language using NMT. NMT output may sometimes exhibit repeated words in the generated translation, which indicates that the system had problems translating a part of the sentence and it used the repetitions to fill the gap. In such cases the source-target sentence pair is likely to not be a good parallel sentence, therefore the repeating token filter removes them.</p><p>Correct language filter -uses language identification software <ref type="bibr" target="#b4">[5]</ref> to estimate the language of each sentence and removes any sentence that has a different identified language from the one specified.</p><p>Moses Scripts and Subword NMT -calls Moses scripts for tokenising, cleaning, truecasing, and Subword NMT <ref type="bibr" target="#b5">[6]</ref> for splitting into subword units. This process prepares the corpus up to the point where it can be passed on to the NMT system for training. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Corpora Cleaning</head><p>We used the toolkit to clean parallel corpora provided in the WMT17 4 and WMT18 5 news MT shared tasks for English ↔ Estonian/Finnish/Latvian. Detailed results of the cleaning process for three of the largest corpora -ParaCrawl, Rapid corpus of EU press releases (Rapid) and European Parliament Proceedings Parallel Corpus (Europarl) -are shown in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>The results show that ParaCrawl is the most problematic corpus, especially the Estonian part, where 85% had to be removed. The most frequent problems are 1) specified and identified language mismatch; 2) identical sentences appearing on source and target sides; 3) multiple source sentences aligned to the same target sentence; 4) an overwhelming amount of non-alphabetical characters; and 5) multiple target sentences aligned to the same source sentence. All examples of bad sentences in Section 3 were selected from the removed parts of the English-Estonian ParaCrawl corpus.</p><p>The Rapid corpus had an overall higher quality with only about 25% of parallel sentences removed. For the three languages it exhibited three main defects -1) duplicate parallel sentences; 2) specified and identified language mismatch; and 3) mismatch in amounts of non-alphabetical symbols between source and target sentences.</p><p>Europarl was by far the cleanest corpus, having only 5-6% of sentences removed by the cleaning toolkit. For all languages, most removed sentences were due to the same two defects as in the Rapid corpus.</p><p>We combined and shuffled all three English-Estonian corpora, resulting in 1 012 824 (46.50% of total) sentence parallel corpus for training NMT systems described in the next section. The total amount of English-Finnish parallel sentences was 2 719 104 (82.72% of total) after adding a cleaned version of the Wiki Headlines corpus, and English-Latvian -1 617 793 (35.85% of total) parallel sentences after adding cleaned versions of LETA translated news, Digital Corpus of European Parliament (DCEP), and Online Books corpora (cleaning details in <ref type="table" target="#tab_1">Table 2</ref>). We used the development data sets provided by the WMT shared tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Machine Translation</head><p>To observe the actual benefit of filtering data for NMT, we trained NMT models using filtered and non-filtered data in both translation directions for the three language pairs. We used Sockeye <ref type="bibr" target="#b6">[7]</ref> to train transformer architecture models with 6 encoder and decoder layers, 8 transformer attention heads per layer, word embeddings and hidden layers of size 512, dropout of 0.2, shared subword unit vocabulary of 50 000 tokens, maximum sentence length of 128 symbols, and a batch size of 3072 words. All models were trained until they reached convergence on development data. The final NMT system results in <ref type="table" target="#tab_2">Table 3</ref> show that corpora filtering improves NMT quality for Estonian and Latvian systems, but not Finnish. The lack of improvement for Finnish is mainly due to the Europarl being the largest (about 3 5 of total) and at the same time the cleanest corpus for this language pair. The biggest corpora for Estonian and Latvian -ParaCrawl (about 3 5 of total) and DCEP (about 4 5 of total) respectively were also the most problematic ones with 85% and 78% sentences removed respectively. <ref type="figure" target="#fig_5">Figure 7</ref> shows training progression of all 12 NMT systems. Filtered systems are depicted with solid lines, unfiltered ones -with dotted lines, Estonian systems are in light/dark blue colours, Finnish -orange/yellow, and Latvian are in light/dark red colours. The figure shows that the filtered Estonian and Latvian systems are much quicker to learn than the unfiltered ones, but eventually, they converge close to the unfiltered systems. As for the Finnish systems -there is no significant difference between filtered and unfiltered, as at times one is higher than the other or vice versa. It is generally visible that in both translation directions the filtered systems achieve higher BLEU scores and reach higher quality quicker. For both English-Estonian systems, the unfiltered version catches up to the filtered one later on in the training, but never quite reaches or surpasses it. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>This paper introduced several types of problematic sentences that can be found in large text corpora and a set of filters that help to remove them in order to train higher quality neural machine translation models using the remaining clean part of the corpora. Results show that in cases where the majority of given parallel corpora are very noisy and there is a small fraction of high-quality corpora, cleaning boosts NMT performance. This is especially evident for translation into morphologically rich languages like Estonian and Latvian.</p><p>In this paper, we mainly focused on cleaning parallel corpora, but the toolkit is also capable of cleaning monolingual corpora separately. In the MT system training workflow, cleaning monolingual data is useful before performing back-translation of an in-domain corpus, so that only filtered sentences get translated.</p><p>We release the corpora cleaning toolkit on GitHub under the MIT open-source license. The toolkit was used as an integral part of the runner-up English-Estonian NMT system submission <ref type="bibr" target="#b7">[8]</ref> in the WMT18 news translation task for cleaning parallel and back-translatable monolingual data, as well as synthetic parallel data produced via backtranslation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Examples of sentences with over 50% non-alphabetical symbols.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>An example of an English sentence aligned to multiple different Estonian sentences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Multiple English paraphrased sentences aligned to one Estonian sentence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Examples of sentences with a different identified language than the one specified.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>An example repeating tokens (underlined).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Training progress of English ↔ Estonian/Finnish/Latvian NMT systems.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Detailed results on filtering English-Estonian/Finnish/Latvian larger common parallel corpora from WMT shared tasks.</figDesc><table><row><cell></cell><cell cols="2">Paracrawl</cell><cell></cell><cell>Rapid</cell><cell></cell><cell></cell><cell>Europarl</cell><cell></cell></row><row><cell></cell><cell>En-Et</cell><cell>En-Fi</cell><cell>En-Et</cell><cell>En-Fi</cell><cell>En-Lv</cell><cell>En-Et</cell><cell>En-Fi</cell><cell>En-Lv</cell></row><row><cell cols="5">Corpus size 1298103 624058 226978 583223</cell><cell>306588</cell><cell cols="3">652944 1926114 638789</cell></row><row><cell>Unique</cell><cell>26 0.00%</cell><cell>37 0.01%</cell><cell>23 0.01%</cell><cell cols="2">161463 27.68% 26.39% 80894</cell><cell>23218 3.56%</cell><cell>52686 2.74%</cell><cell>19652 3.08%</cell></row><row><cell>src == tgt</cell><cell>242816 18.71%</cell><cell>41611 6.67%</cell><cell>428 0.19%</cell><cell>3488 0.60%</cell><cell>2929 0.96%</cell><cell>490 0.08%</cell><cell>528 0.03%</cell><cell>707 0.11%</cell></row><row><cell>* sources</cell><cell>267235</cell><cell>17239</cell><cell>1108</cell><cell>1513</cell><cell>990</cell><cell>1176</cell><cell>6631</cell><cell>979</cell></row><row><cell>1 target</cell><cell>20.59%</cell><cell>2.76%</cell><cell>0.49%</cell><cell>0.26%</cell><cell>0.32%</cell><cell>0.18%</cell><cell>0.34%</cell><cell>0.15%</cell></row><row><cell>* targets</cell><cell>69225</cell><cell>9532</cell><cell>752</cell><cell>1016</cell><cell>329</cell><cell>462</cell><cell>3536</cell><cell>435</cell></row><row><cell>1 source</cell><cell>5.33%</cell><cell>1.53%</cell><cell>0.33%</cell><cell>0.17%</cell><cell>0.11%</cell><cell>0.07%</cell><cell>0.18%</cell><cell>0.07%</cell></row><row><cell>&gt;50%</cell><cell>200338</cell><cell>12919</cell><cell>1226</cell><cell>5647</cell><cell>1699</cell><cell>66</cell><cell>285</cell><cell>72</cell></row><row><cell>non-alpha</cell><cell>15.43%</cell><cell>2.07%</cell><cell>0.54%</cell><cell>0.97%</cell><cell>0.55%</cell><cell>0.01%</cell><cell>0.01%</cell><cell>0.01%</cell></row><row><cell>Non-alpha</cell><cell>23777</cell><cell>12737</cell><cell>6674</cell><cell>13311</cell><cell>6361</cell><cell>7211</cell><cell>24847</cell><cell>4012</cell></row><row><cell>mismatch</cell><cell>1.83%</cell><cell>2.04%</cell><cell>2.94%</cell><cell>2.28%</cell><cell>2.07%</cell><cell>1.10%</cell><cell>1.29%</cell><cell>0.63%</cell></row><row><cell>Repeating</cell><cell>11210</cell><cell>1397</cell><cell>175</cell><cell>396</cell><cell>171</cell><cell>727</cell><cell>2594</cell><cell>703</cell></row><row><cell>tokens</cell><cell>0.86%</cell><cell>0.22%</cell><cell>0.08%</cell><cell>0.07%</cell><cell>0.06%</cell><cell>0.11%</cell><cell>0.13%</cell><cell>0.11%</cell></row><row><cell>Language</cell><cell>283152</cell><cell>36233</cell><cell>14762</cell><cell>24854</cell><cell>8739</cell><cell>8924</cell><cell>10932</cell><cell>3301</cell></row><row><cell>mismatch</cell><cell>21.81%</cell><cell>5.81%</cell><cell>6.50%</cell><cell>4.26%</cell><cell>2.85%</cell><cell>1.37%</cell><cell>0.57%</cell><cell>0.52%</cell></row><row><cell>∑ removed</cell><cell cols="3">1097779 131705 25148 85% 21% 11%</cell><cell>211688 36%</cell><cell>102112 33%</cell><cell>42274 6%</cell><cell>102039 5%</cell><cell>29861 5%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Detailed results on filtering English-Finnish/Latvian smaller parallel corpora from WMT shared tasks.</figDesc><table><row><cell></cell><cell>En-Fi</cell><cell></cell><cell>En-Lv</cell><cell></cell></row><row><cell></cell><cell>Wiki</cell><cell>DCEP</cell><cell>Leta</cell><cell>Books</cell></row><row><cell cols="2">Corpus size 153728</cell><cell cols="2">3542280 15671</cell><cell>9577</cell></row><row><cell>Unique</cell><cell>0 0.00%</cell><cell cols="3">2277397 454 64.29% 2.90% 4.53% 434</cell></row><row><cell>src == tgt</cell><cell cols="2">42438 27.61% 9.59% 339861</cell><cell cols="2">2 0.01% 0.04% 4</cell></row><row><cell>* sources</cell><cell>161</cell><cell>12474</cell><cell>2</cell><cell>35</cell></row><row><cell>1 target</cell><cell>0.10%</cell><cell>0.35%</cell><cell cols="2">0.01% 0.37%</cell></row><row><cell>* targets</cell><cell>339</cell><cell>9450</cell><cell>15</cell><cell>12</cell></row><row><cell>1 source</cell><cell>0.22%</cell><cell>0.27%</cell><cell cols="2">0.10% 0.13%</cell></row><row><cell>&gt;50%</cell><cell>488</cell><cell>31842</cell><cell>0</cell><cell>13</cell></row><row><cell>non-alpha</cell><cell>0.32%</cell><cell>0.90%</cell><cell cols="2">0.00% 0.14%</cell></row><row><cell>Non-alpha</cell><cell>4616</cell><cell>38838</cell><cell>946</cell><cell>20</cell></row><row><cell>mismatch</cell><cell>3.00%</cell><cell>1.10%</cell><cell cols="2">6.04% 0.21%</cell></row><row><cell>Repeating</cell><cell>38</cell><cell>1242</cell><cell>47</cell><cell>8</cell></row><row><cell>tokens</cell><cell>0.02%</cell><cell>0.04%</cell><cell cols="2">0.30% 0.08%</cell></row><row><cell>Language</cell><cell>74507</cell><cell>48910</cell><cell>59</cell><cell>1074</cell></row><row><cell>mismatch</cell><cell>48.47%</cell><cell>1.38%</cell><cell>0.38%</cell><cell>11.21%</cell></row><row><cell>∑ removed</cell><cell>122587 80%</cell><cell cols="2">2760014 1525 78% 10%</cell><cell>1600 17%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Translation quality results (BLEU scores) for all translation directions on development data. The best results are marked in bold. The second row shows how much of the initial parallel corpora remained after filtering for each language pair.</figDesc><table><row><cell></cell><cell cols="6">En-Et Et-En En-Fi Fi-En En-Lv Lv-En</cell></row><row><cell>Unfiltered</cell><cell>15.45</cell><cell>21.55</cell><cell>20.07</cell><cell>25.25</cell><cell>21.29</cell><cell>24.12</cell></row><row><cell>Corpus after filtering</cell><cell cols="2">46.50%</cell><cell cols="2">82.72%</cell><cell cols="2">35.85%</cell></row><row><cell>Filtered</cell><cell>15.80</cell><cell>21.62</cell><cell>19.64</cell><cell>25.04</cell><cell>22.89</cell><cell>24.37</cell></row><row><cell>Difference</cell><cell>+0.35</cell><cell>+0.07</cell><cell>-0.43</cell><cell>-0.21</cell><cell>+1.60</cell><cell>+0.25</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Second Conference on Machine Translationhttp://statmt.org/wmt17 5 Third Conference on Machine Translationhttp://statmt.org/wmt18</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The research has been supported by the European Regional Development Fund within the research project "Neural Network Modelling for Inflected Natural Languages" No. 1.1.1.1/16/A/215.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Moses: open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Herbst</surname></persName>
		</author>
		<ptr target="https://dl.acm.org/citation.cfm?id=1557821" />
		<imprint>
			<date type="published" when="2007" />
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Zipporah: a Fast and Scalable Data Cleaning System for Noisy Web-Crawled Parallel Corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D17-1319%0Ahttp://aclweb.org/anthology/D17-1318" />
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2935" to="2940" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Noisy-parallel and comparable corpora filtering methodology for the extraction of bi-lingual equivalent data at sentence level</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wolk</surname></persName>
		</author>
		<idno type="DOI">https:/arxiv.org/ftp/arxiv/papers/1510/1510.04500.pdfhttp:/dx.doi.org/10.7494/csci.2015.16.2.169</idno>
		<ptr target="https://arxiv.org/ftp/arxiv/papers/1510/1510.04500.pdfhttp://dx.doi.org/10.7494/csci.2015.16.2.169" />
	</analytic>
	<monogr>
		<title level="j">Computer Science @BULLET Computer Science</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">162</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Automatic Filtering of Bilingual Corpora for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khadivi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<idno type="DOI">http:/link.springer.com/10.1007/11428817_24</idno>
		<ptr target="http://link.springer.com/10.1007/11428817_24" />
	</analytic>
	<monogr>
		<title level="m">Natural Language Processing and Information Systems, 10th International Conference on Applications of Natural Language to Information Systems</title>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">3513</biblScope>
			<biblScope unit="page" from="263" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">py: An off-the-shelf language identification tool</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Baldwin</surname></persName>
		</author>
		<ptr target="https://dl.acm.org/citation.cfm?id=2390475http://dl.acm.org/citation.cfm?id=2390475" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2012 System Demonstrations</title>
		<meeting>the ACL 2012 System Demonstrations</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="25" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
		<ptr target="http://www.research.ed.ac.uk/portal/files/25478429/subword_1.pdfhttp://arxiv.org/abs/1508.07909" />
		<title level="m">Proceedings of the 54th An-nual Meeting of the Association for Computational Linguistics (ACL 2016)</title>
		<meeting>the 54th An-nual Meeting of the Association for Computational Linguistics (ACL 2016)<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
	<note>ISBN ISBN 9781510827585</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hieber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Domhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vilar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sokolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Clifton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Post</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1712.05690" />
		<title level="m">Sockeye: A Toolkit for Neural Machine Translation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Tilde&apos;s Machine Translation Systems for WMT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pinnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rikters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krišlauks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation (WMT 2018)</title>
		<meeting>the Third Conference on Machine Translation (WMT 2018)<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>Shared Task Papers</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
