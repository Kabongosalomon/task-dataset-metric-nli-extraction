<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Why Attention? Analyze BiLSTM Deficiency and Its Remedies in the Case of NER</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng-Hsuan</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Academia Sinica</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsu-Jui</forename><surname>Fu</surname></persName>
							<email>tsu-juifu@ucsb.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">UC Santa Barbara</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Yun</forename><surname>Ma</surname></persName>
							<email>ma@iis.sinica.edu.tw</email>
							<affiliation key="aff0">
								<orgName type="institution">Academia Sinica</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Why Attention? Analyze BiLSTM Deficiency and Its Remedies in the Case of NER</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T14:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>BiLSTM has been prevalently used as a core module for NER in a sequence-labeling setup. State-of-the-art approaches use BiLSTM with additional resources such as gazetteers, language-modeling, or multi-task supervision to further improve NER. This paper instead takes a step back and focuses on analyzing problems of BiLSTM itself and how exactly self-attention can bring improvements. We formally show the limitation of (CRF-)BiLSTM in modeling cross-context patterns for each word -the XOR limitation. Then, we show that two types of simple cross-structures -self-attention and Cross-BiLSTM -can effectively remedy the problem. We test the practical impacts of the deficiency on real-world NER datasets, OntoNotes 5.0 and WNUT 2017, with clear and consistent improvements over the baseline, up to 8.7% on some of the multi-token entity mentions. We give in-depth analyses of the improvements across several aspects of NER, especially the identification of multi-token mentions. This study should lay a sound foundation for future improvements on sequence-labeling NER 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>only the leftmost hidden state of the backward LSTM are used, and each of the endpoint hidden states sees and encodes the whole sentence. For computing sentence representations for sequence-labeling tasks such as NER, however, this becomes a limitation, as each token uses its own midpoint hidden states, which do not model the patterns that happen to cross past and future at this specific time step. This paper explores two types of cross-structures to help cope with the problem: Cross-BiLSTM-CNN and Att-BiLSTM-CNN. Previous studies have tried to stack multiple LSTMs for sequence-labeling NER <ref type="bibr" target="#b2">(Chiu and Nichols 2016)</ref>. As they follow the trend of stacking forward and backward LSTMs independently, the Baseline-BiLSTM-CNN is only able to learn higher-level representations of past or future per se. Instead, Cross-BiLSTM-CNN, which interleaves every layer of the two directions, models crosscontext in an additive manner by learning higher-level representations of the whole context of each token. On the other hand, Att-BiLSTM-CNN models cross-context in a multiplicative manner by capturing the interaction between past and future with a dot-product self-attentive mechanism <ref type="bibr">(Conneau et al. 2017;</ref><ref type="bibr" target="#b8">Lin et al. 2017b)</ref>.</p><p>Section 3 formulates the three Baseline, Cross, and Att-BiLSTM-CNN models, with Section 3.3, 3.4 giving formal proof that patterns forming an XOR cannot be modeled by (CRF-)BiLSTM-CNN used in all previous work. Cross-BiLSTM-CNN and Att-BiLSTM-CNN are shown to have additive and multiplicative cross-structures respectively to deal with the problem. Section 4 evaluates practical effectiveness of the approaches on two challenging NER datasets spanning a wide range of domains with complex, noisy, and emerging entities. The cross-structures bring consistent improvements over the prevalently used Baseline-BiLSTM-CNN without additional gazetteers, POS taggers, language-modeling, or multi-task supervision. The improved core module surpasses comparable bare-bone models on OntoNotes 5.0 and WNUT 2017 by 1.4% and 4.6% respectively. Ablation experiments reveal that emerging, complex, confusing, and multi-token entity mentions benefitted much from the cross-structures, up to 8.7% on some of the multi-token mentions. The in-depth entity-chunking analysis gives insights into how exactly self-attention helps real-world NER. As state-of-the-art approaches often use BiL-STM as their core module, they could benefit from the improvements brought by cross-structures against bare-bone models presented in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Many have attempted tackling the NER task with bare-bone LSTM-based sequence encoders <ref type="bibr" target="#b6">(Huang, Xu, and Yu 2015;</ref><ref type="bibr" target="#b9">Ma and Hovy 2016;</ref><ref type="bibr" target="#b2">Chiu and Nichols 2016;</ref><ref type="bibr" target="#b6">Lample et al. 2016</ref>). Among these, the most sophisticated and successful is the BiLSTM-CNN proposed by <ref type="bibr" target="#b2">Chiu and Nichols (2016)</ref>. They stack multiple layers of LSTM cells per direction and also use a CNN to compute character-level word vectors alongside pre-trained word vectors. To make the analysis results in this work comparable to past studies on BiLSTM, we largely follow their paper in constructing the Baseline-BiLSTM-CNN, including the selection of raw features, the CNN, and the multi-layer BiLSTM. A subtle difference is that they send the output of each direction through separate affine-softmax classifiers and then sum their probabilities, while this paper sum the scores from affine layers before computing softmax once. While not changing the modeling capacity regarded in this paper, this does provide an empirically stronger baseline model than their formulation.</p><p>Besides using additional gazetteers or POS taggers <ref type="bibr" target="#b0">(Aguilar et al. 2017;</ref><ref type="bibr" target="#b0">Aguilar et al. 2018;</ref><ref type="bibr" target="#b5">Ghaddar and Langlais 2018)</ref>, State-of-the-art models use additional large-scale language-modeling corpora (Akbik, Blythe, and Vollgraf 2018) or additional multi-task supervision <ref type="bibr" target="#b3">(Clark et al. 2018)</ref> to further improve NER performance beyond bare-bone models. This work does not intend to surpass their performance. Instead, as they rely on a core BiLSTM sentence encoder with the same limitation studied and remedied in this work, they would indeed benefit from the improvements of crossstructures against bare-bone models presented in this paper. In fact, on other tasks, many have used various ways to interleave BiLSTM layers <ref type="bibr" target="#b15">(Zhou and Xu 2015;</ref><ref type="bibr" target="#b3">Coavoux and Cohen 2019)</ref>. This work provides for a conscious decision with a formal treatment of the XOR limitation and its practical impacts on NER.</p><p>The modeling of global contexts for sequence-labeling NER has been partially accomplished using extensive feature engineering or conditional random fields (CRF). <ref type="bibr" target="#b11">Ratinov and Roth (2009)</ref> build the Illinois NER tagger with feature-based perceptrons. In their analysis, the usefulness of Viterbi decoding is minimal and conflicts their handcrafted global features. However, their model has limited capability to learn the extraction of new global input features. On the other hand, recent researches on LSTM or CNN-based sequence encoders report empirical improvements brought by CRF <ref type="bibr" target="#b6">(Huang, Xu, and Yu 2015;</ref><ref type="bibr" target="#b9">Ma and Hovy 2016;</ref><ref type="bibr" target="#b6">Lample et al. 2016;</ref><ref type="bibr" target="#b13">Strubell et al. 2017)</ref>, as it discourages illegal predictions by explicitly modeling tagtransition probabilities. However, with the speed penalty of Viterbi decoding, transition probabilities are still independent of input sentences and provide partial, limited help in untying two plausible tag sequences. In contrast, this work studies the remedies for the XOR problem of (CRF-)BiLSTM (Section 3.3, 3.4) that can directly provide the extraction of better global input features, improving class observation likelihoods.</p><p>Thought to lighten the burden of compressing all relevant information into a single hidden state, using attention mechanisms on top of LSTMs have shown empirical success for sequence encoders <ref type="bibr">(Conneau et al. 2017;</ref><ref type="bibr" target="#b8">Lin et al. 2017b</ref>) and decoders <ref type="bibr" target="#b9">(Luong, Pham, and Manning 2015)</ref>. Self-attention has also been used below encoders to compute word vectors conditioned on context <ref type="bibr" target="#b4">(Devlin et al. 2018</ref>). This work further formally analyzes the deficiency of BiLSTM encoders for sequence labeling and shows that using self-attention on top is actually providing one type of cross-structures that capture interactions between past and future context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">CNN and Word Features</head><p>All models in the experiments use the same set of raw features: character embedding, character type, word embedding, and word capitalization.</p><p>For character embedding, 25d vectors are trained from scratch, and 4d one-hot character-type features indicate whether a character is uppercase, lowercase, digit, or punctuation <ref type="bibr" target="#b2">(Chiu and Nichols 2016)</ref>. Word token lengths are unified to 20 by truncation and padding. The resulting 20-by-(25+4) feature map of each token is applied to a character-trigram CNN with 20 kernels per length 1 to 3 and max-over-time pooling to compute a 60d characterbased word vector <ref type="bibr" target="#b6">(Kim et al. 2016;</ref><ref type="bibr" target="#b2">Chiu and Nichols 2016;</ref><ref type="bibr" target="#b9">Ma and Hovy 2016)</ref>.</p><p>For word embedding, either pre-trained 300d GloVe vectors <ref type="bibr" target="#b10">(Pennington, Socher, and Manning 2014)</ref> or 400d Twitter vectors <ref type="bibr" target="#b5">(Godin et al. 2015)</ref> are used without further tuning. Also, 4d one-hot word capitalization features indicate whether a word is uppercase, upper-initial, lowercase, or mixed-caps <ref type="bibr" target="#b3">(Collobert et al. 2011;</ref><ref type="bibr" target="#b2">Chiu and Nichols 2016)</ref>.</p><p>Throughout this paper, X denotes the n-by-d x matrix of sequence features, where n is the sentence length and d x is either 364 (with GloVe) or 464 (with Twitter).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Baseline-BiLSTM-CNN</head><p>On top of the feature sequence, BiLSTM is used to capture the future and the past for each time step. Following <ref type="bibr" target="#b2">Chiu and Nichols (2016)</ref>, 4 distinct LSTM cells -two in each direction -are stacked to capture higher level representations:</p><formula xml:id="formula_0">− → H = −−−−→ LST M 2 ( −−−−→ LST M 1 (X)) ← − H = ←−−−− LST M 4 ( ←−−−− LST M 3 (X)) H = − → H || ← − H , where −−−−→ LST M i , ←−−−− LST M i denote applying LSTM cell i in forward, backward order, − → H , ← −</formula><p>H denote the resulting feature matrices of the stacked application, and || denotes row-wise concatenation. In all the experiments, 100d LSTM cells are used, so H ∈ R n×d h and d h = 200.</p><p>Finally, suppose there are d p token classes, the probability of each of which is given by the composition of affine and softmax transformations:</p><formula xml:id="formula_1">s t = H t W p + b p ti = e sti dp j=1 e stj ,</formula><p>where H t is the t th row of H, W p ∈ R d h ×dp , b ∈ R dp are a trainable weight matrix and bias, and s ti and s tj are the i-th and j-th elements of s t . Following <ref type="bibr" target="#b2">Chiu and Nichols (2016)</ref>, the 5 chunk labels O, S, B, I, E denote if a word token is Outside any entity mentions, the Sole token of a mention, the Beginning token of a multi-token mention, In the middle of a multi-token mention, or the Ending token of a multi-token mention. Hence when there are P types of named entities, the actual number of token classes d p = P × 4 + 1 for sequence labeling NER.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">XOR Limitation of Baseline-BiLSTM</head><p>Consider the following four phrases that form an XOR: 1. Key and Peele (work-of-art) 2. You and I (work-of-art) 3. Key and I 4. You and Peele The first two phrases are respectively a show title and a song title. The other two are not entities as a whole, where the last one actually occurs in an interview with Keegan-Michael Key. Suppose each phrase is the sequence given to Baseline-BiLSTM-CNN for sequence tagging, then the 2 nd token "and" should be tagged as work-of-art:I in the first two cases and as O in the last two cases.</p><p>Firstly, note that the score vector at each time step is simply the sum of contributions coming from forward and backward directions plus a bias.</p><formula xml:id="formula_2">s t = H t W p + b = − → H t − → W p + ← − H t ← − W p + b = − → s t + ← − s t + b</formula><p>where − → W p , ← − W p denotes the top-half and bottom-half of W p . Suppose the index of work-of-art:I and O are i, j respectively. To predict each "and" correctly, it must hold that</p><formula xml:id="formula_3">− → s 1 2i + ← − s 1 2i + b i &gt; − → s 1 2j + ← − s 1 2j + b j − → s 2 2i + ← − s 2 2i + b i &gt; − → s 2 2j + ← − s 2 2j + b j − → s 3 2i + ← − s 3 2i + b i &lt; − → s 3 2j + ← − s 3 2j + b j − → s 4 2i + ← − s 4 2i + b i &lt; − → s 4 2j + ← − s 4 2j + b j where superscripts denote the phrase number.</formula><p>Now, the catch is that phrase 1 and phrase 3 have exactly the same past context for "and". Hence the same</p><formula xml:id="formula_4">− → H 2 and the same − → s 2 , i.e., − → s 1 2 = − → s 3 2 . Similarly, − → s 2 2 = − → s 4 2 , ← − s 1 2 = ← − s 4 2 , and ← − s 2 2 = ← − s 3 2 .</formula><p>Rewriting the constraints with these equalities gives</p><formula xml:id="formula_5">− → s 1 2i + ← − s 1 2i + b i &gt; − → s 1 2j + ← − s 1 2j + b j − → s 2 2i + ← − s 2 2i + b i &gt; − → s 2 2j + ← − s 2 2j + b j − → s 1 2i + ← − s 2 2i + b i &lt; − → s 1 2j + ← − s 2 2j + b j − → s 2 2i + ← − s 1 2i + b i &lt; − → s 2 2j + ← − s 1 2j + b j</formula><p>Finally, summing the first two inequalities and the last two inequalities gives two contradicting constraints that cannot be satisfied. In other words, even if an oracle is given to training the model, Baseline-BiLSTM-CNN can only tag at most 3 out of 4 "and" correctly. No matter how many LSTM cells are stacked for each direction, the formulation in previous studies simply does not have enough modeling capacity to capture cross-context patterns for sequence labeling NER.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">XOR Limitation of CRF-BiLSTM</head><p>Consider the following four phrases that form an XOR:   The inequalities remain unsatisfiable, and the reason is twofold:</p><p>1. The addition of transition probabilities are linear, independent of word sequences, so it does not help untying plausible word-tag sequences that form XOR. 2. The consideration of each phrase as a whole, i.e. Viterbi decoding, does help to untie BIE with OOO, but not to untie OSO with OOO (recall "cancelling the same terms"). In other words, predicting a phrase as a whole partially mitigates the XOR problem, with or without transition probabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Cross-BiLSTM-CNN</head><p>Motivated by the limitation of the conventional Baseline-BiLSTM-CNN for sequence labeling, this paper proposes the use of Cross-BiLSTM-CNN by changing the deep structure in Section 3.2 to</p><formula xml:id="formula_6">− → H 1 = −−−−→ LST M 1 (X) ← − H 3 = ←−−−− LST M 3 (X) − → H 2 = −−−−→ LST M 2 ( − → H 1 || ← − H 3 ) ← − H 4 = ←−−−− LST M 4 ( − → H 1 || ← − H 3 ) H = − → H 2 || ← − H 4</formula><p>As the forward and backward hidden states are interleaved between stacked LSTM layers, Cross-BiLSTM-CNN models cross-context patterns by computing representations of the whole sequence in a feed-forward, additive manner.</p><p>Specifically, for the XOR cases introduced in Section 3.3, 3.4, although phrase 1 and phrase 3 still have the same past context for the middle token and hence the first layer −−−−→ LST M 1 can only extract the same low-level hidden</p><formula xml:id="formula_7">features − → H 1 2 , the second layer −−−−→ LST M 2 considers the whole context − → H 1 || ← − H 3</formula><p>and thus have the ability to extract different high-level hidden features − → H 2 2 for the two phrases. As the higher-level LSTMs of Cross-BiLSTM-CNN have interleaved input from forward and backward hidden states down below, their weight parameters double the size of the first-level LSTMs. Nevertheless, the cross formulation provides the modeling capacity absent in previous studies with how many more LSTM layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Att-BiLSTM-CNN</head><p>Another way to capture the interaction between past and future context per time step is to add a token-level selfattentive mechanism on top of the same BiLSTM formulation introduced in Section 3.2. Given the hidden features H of a whole sequence, the model projects each hidden state to different subspaces, depending on whether it is used as the query vector to consult other hidden states for each word token, the key vector to compute its dot-similarities with incoming queries, or the value vector to be weighted and actually convey information to the querying token. As different aspects of a task can call for different attention, multiple attention heads running in parallel are used <ref type="bibr" target="#b14">(Vaswani et al. 2017)</ref>.</p><p>Formally, let m be the number of attention heads and d c be the subspace dimension. For each head i ∈ {1..m}, the attention weight matrix and context matrix are computed by</p><formula xml:id="formula_8">α i = σ HW qi (HW ki ) T √ d c C i = α i HW vi , where W qi , W ki , W vi ∈ R d h</formula><p>×dc are trainable projection matrices and σ performs softmax along the second dimension. Each row of the resulting α 1 , α 2 , . . . , α m ∈ R n×n contains the attention weights of a token to its context, and each row of C 1 , C 2 , . . . , C m ∈ R n×dc is its context vector.</p><p>For Att-BiLSTM-CNN, the hidden vector and context vectors of each token are considered together for classification:</p><formula xml:id="formula_9">s c t = (H t ||C 1 t ||C 2 t ||...||C m t )W c + b p c ti = e s c<label>ti</label></formula><p>dp j=1 e s c tj ,  where C i t is the t-th row of C i , and W c ∈ R (d h +mdc)×dp is a trainable weight matrix. In all the experiments, m = 5 and d c = d h 5 , so W c ∈ R 2d h ×dp . While the BiLSTM formulation stays the same as Baseline-BiLSTM-CNN, the computation of attention weights α i and context features C i models the cross interaction between past and future. To see this, the computation of attention scores can be rewritten as follows.</p><formula xml:id="formula_10">HW qi (HW ki ) T = H(W qi W ki T )H T . = ( − → H || ← − H )(W qi W ki T )( − → H || ← − H ) T .</formula><p>With the un-shifted covariance matrix of the projected − → H || ← − H , Att-BiLSTM-CNN correlates past and future context for each token in a dot-product, multiplicative manner.</p><p>One advantage of using multiplicative attention to resolve the XOR problem is that it only needs to be computed once per sequence, and the matrix computations are highly parallelizable, resulting in little computation time overhead. Moreover, in Section 4, the attention weights provide a better understanding of how the model learns to tackle sequence-labeling NER.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>OntoNotes 5.0 Fine-Grained NER -a million-token corpus with diverse sources of newswires, web, broadcast news, broadcast conversations, magazines, and telephone conversations <ref type="bibr" target="#b5">(Hovy et al. 2006;</ref><ref type="bibr" target="#b11">Pradhan et al. 2013)</ref>. Some are transcriptions of talk shows, and some are translations from Chinese or Arabic. The dataset contains 18 fine-grained entity types, including hard ones such as law, event, and workof-art. All the diversities and noisiness require that models are robust across broad domains and able to capture a multitude of linguistic patterns for complex entities.</p><p>WNUT 2017 Emerging NER -a dataset providing maximally diverse, noisy, and drifting user-generated text <ref type="bibr" target="#b3">(Derczynski et al. 2017)</ref>. The training set consists of previously annotated tweets -social media text with nonstandard spellings, abbreviations, and unreliable capitalization <ref type="bibr" target="#b12">(Strauss et al. 2016)</ref>; the development set consists of newly sampled YouTube comments; the test set includes text newly drawn from Twitter, Reddit, and StackExchange. Besides drawing new samples from diverse topics across different sources, the shared task also filtered out text containing surface forms of entities seen in the training set. The resulting dataset requires models to generalize to emerging contexts and entities instead of relying on familiar surface cues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation and Baselines</head><p>All experiments for Baseline-, Cross-, and Att-BiLSTM-CNN used the same model parameters given in Section 3. The training minimized per-token cross-entropy loss with the Nadam optimizer (Dozat 2016) with uniform learning rate 0.001, batch size 32, and 35% variational dropout <ref type="bibr" target="#b5">(Gal and Ghahramani 2016)</ref>. Each training lasted 400 epochs when using GloVe embedding (OntoNotes), and 1600 epochs when using Twitter embedding (WNUT). The development set of each dataset was used to select the best epoch to restore model weights for testing. Following previous work on NER, model performances were evaluated with strict mention F1 score. Training of each model on each dataset repeated 6 times to report the mean score and standard deviation.</p><p>Besides the strong Baseline implemented in this paper, we also list results of bare-bone BiLSTM-CNN <ref type="bibr" target="#b2">(Chiu and Nichols 2016)</ref>, CRF-BiLSTM(-BiLSTM) <ref type="bibr" target="#b13">(Strubell et al. 2017;</ref><ref type="bibr" target="#b7">Lin et al. 2017a)</ref>, and CRF-IDCNN <ref type="bibr" target="#b13">(Strubell et al. 2017</ref>) from the literature. Among them, IDCNN was a CNNbased sentence encoder, which should not have the XOR limitation raised in this paper. Caveat: As the purpose of the experiments is to evaluate practical effectiveness in remedying the limitation of BiLSTM, comparisons are not made against models using additional resources, such as gazetteers or POS taggers <ref type="bibr" target="#b0">(Aguilar et al. 2017;</ref><ref type="bibr" target="#b0">Aguilar et al. 2018;</ref><ref type="bibr" target="#b5">Ghaddar and Langlais 2018)</ref>, large-scale language-modeling corpora (Akbik, Blythe, and Vollgraf 2018), or multi-task supervision <ref type="bibr" target="#b3">(Clark et al. 2018)</ref>, to further improve NER performance beyond bare-bone models. We do not claim to have surpassed state-of-the-art results. However, as they used BiLSTM sentence encoders with the XOR limitation, they could indeed integrate with and benefit from the crossstructures presented in this paper.    <ref type="bibr">00 -71.50 -93.12 -36.45 -39.19 -91.90 -90.83 -0.38</ref>  <ref type="table" target="#tab_0">Table 1</ref> shows overall results on the two datasets spanning broad domains of newswires, broadcast, telephone, and social media. The models proposed in this paper surpassed previous reported bare-bone models by 1.4% on OntoNotes and 4.6% on WNUT. Compared to the re-implemented Baseline-BiLSTM-CNN, the cross-structures brought 0.7% and 2.2% improvements on OntoNotes and WNUT. More substantial improvements were achieved for WNUT 2017 emerging NER, suggesting that cross-context patterns were even more crucial for emerging contexts and entities than familiar entities, which might often be memorized by their surface forms. <ref type="table" target="#tab_2">Table 3</ref> shows significant results per entity type compared to Baseline (&gt;3% absolute F1 differences for either Cross or Att). It could be seen that harder entity types generally benefitted more from the cross-structures. For example, workof-art/creative-work entities could in principle take any surface forms -unseen, the same as a person name, abbreviated, or written with unreliable capitalizations on social media. Such mentions require models to learn a deep, generalized understanding of their context to accurately identify their boundaries and disambiguate their types. Both crossstructures were more capable in dealing with such hard entities (2.1%/5.6%/3.2%/2.0%) than the prevalently used, problematic Baseline. Moreover, disambiguating fine-grained entity types is also a challenging task. For example, entities of language and NORP often take the same surface forms. <ref type="figure" target="#fig_2">Figure 1a</ref> shows an example containing "Dutch" and "English". While "En-glish" was much more frequently used as a language and was identified correctly, the "Dutch" mention was tricky for Baseline. The attention heat map <ref type="figure" target="#fig_4">(Figure 2a</ref>) further tells the story that Att has relied on its attention head to make context-aware decisions. Overall, both cross-structures were much better at disambiguating these fine-grained types (4.1%/0.8%/3.3%/3.4%). <ref type="table" target="#tab_3">Table 4</ref> shows results among different entity lengths. It could be seen that cross-structures were much better at dealing with multi-token mentions compared to the prevalently used, problematic Baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Overall Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Complex and Confusing Entity Mentions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Multi-Token Entity Mentions</head><p>In fact, identifying correct mention boundaries for multitoken mentions poses a unique challenge for sequencelabeling models -all tokens in a mention must be tagged with correct sequential labels to form one correct prediction. Although models often rely on strong hints from a token itself or a single side of the context, however, in general, cross-context modeling is required. For example, a token should be tagged as Inside if and only if it immediately follows a Begin or an I and is immediately followed by an I or an End. <ref type="figure" target="#fig_2">Figure 1b</ref> shows a sentence with multiple entity mentions. Among them, "the White house" is a triple-token facility mention with unreliable capitalization, resulting in an emerging surface form. Without usual strong hints given by a seen surface form, Baseline predicted a false single-token mention "White". In contrast, Att utilized its multiple attention heads <ref type="figure" target="#fig_4">(Figure 2b, 2c, 2d</ref>) to consider the preceding and succeeding tokens for each token and correctly tagged the three tokens as facility:B, facility:I, facility:E.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Entity-Chunking</head><p>Entity-chunking is a subtask of NER concerned with locating entity mentions and their boundaries without disambiguating their types. For sequence-labeling models, this means correct O, S, B, I, E tagging for each token. In addition to showing that cross-structures achieved superior performance on multi-token entity mentions (Section 4.5), an ablation study focused on the chunking tags was performed to better understand how it was achieved. <ref type="table" target="#tab_4">Table 5</ref> shows the entity-chunking ablation results on OntoNotes 5.0 development set. Both Att and Baseline models were taken without re-training for this subtask. The HC all column lists the performance of Att-BiLSTM-CNN on each chunking tag. Other columns list the performance compared to HC all . Columns H to C 5 are when the full model is deprived of all other information in testing time by forcefully zeroing all vectors except the one specified by the column header. The figures shown in the table are per-token recalls for each chunking tag, which tells if a part of the model is responsible for signaling the whole model to predict that tag. Bold font and underline mark relatively high and low values of interest.</p><p>Firstly, Att appeared to designate the task of scoring I to the attention mechanism: When context vectors C all were left alone, the recall for I tokens only dropped a little (-3.80); When token hidden states H were left alone, the recall for I tokens seriously degraded <ref type="bibr">(-28.18)</ref>. When H and C all work together, the full Att model was then better at predicting multi-token entity mentions than Baseline.</p><p>Then, breaking context vectors to each attention head reveals that they have worked in cooperation: C 2 , C 3 focused more on scoring E <ref type="bibr">(-36.45, -39.19</ref>) than I <ref type="bibr">(-60.56, -50.19</ref>), while C 4 focused more on scoring B (-12.21) than I <ref type="bibr">(-57.19</ref>). It was when information from all these heads were combined was Att able to better identify a token as being Inside a multi-token mention than Baseline.</p><p>Finally, the quantitative ablation analysis of chunking tags in this Section and the qualitative case-study attention visualizations in Section 4.5 explains each other: C 2 and especially C 3 tended to focus on looking for immediate preceding mention tokens (the diagonal shifted left in <ref type="figure" target="#fig_4">Figure 2b, 2c)</ref>, enabling them to signal for End and Inside; C 4 tended to focus on looking for immediate succeeding mention tokens (the diagonal shifted right in <ref type="figure" target="#fig_4">Figure 2d</ref>), enabling it to signal for Begin and Inside. In fact, without context vectors, instead of BIE, Att would tag "the White house" as BSE and extract the same false mention of "White" as the OSO of Baseline.</p><p>Lacking the ability to model cross-context patterns, Baseline inadvertently learned to retract to predict single-token entities (0.13 vs. -0.63, -0.41, -0.38) when an easy hint from a familiar surface form is not available. This indicates a major flaw in BiLSTM-CNNs prevalently used for real-world NER today.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper has given a formal treatment of the deficiency of the prevalently-used (CRF-)BiLSTM-CNN in modeling cross-context for sequence-labeling NER. Formal proof of its inability to capture XOR patterns has been given, and the practical impacts has been analyzed on OntoNotes 5.0 and WNUT 2017. Additive and multiplicative cross-structures have shown to be crucial in modeling cross-context, significantly enhancing recognition of emerging, complex, confusing, and multi-token entity mentions. Against comparable bare-bone models, 1.4% and 4.6% overall improvements on OntoNotes 5.0 and WNUT 2017 have been achieved, showing the importance of remedying the core module of NER. As state-of-the-art models use (CRF-)BiLSTM with XOR limitation, this study should lay a sound foundation for future improvements on sequence-labeling NER.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>a o m s c o b o m s d o a o m o d o b o m o c o a, b, m, c, d denote words. s (single) and o (outside) are tags. The correct tagging of all phrases requires that p(oso|amc) &gt; p(ooo|amc) p(oso|bmd) &gt; p(ooo|bmd) p(oso|amd) &lt; p(ooo|amd) p(oso|bmc) &lt; p(ooo|bmc) Note that this time we consider each phrase as a whole. Suppose there is only Softmax, the log-probability of a phrase is just the log-sum of each time step. Cancelling the same terms across two sides of each inequality, e.g. lp(o |amc), gives lp( s |amc) &gt; lp( o |amc) lp( s |bmd) &gt; lp( o |bmd) lp( s |amd) &lt; lp( o |amd) lp( s |bmc) &lt; lp( o |bmc) Without cross-structures, scores from two contexts are only linearly summed (See Section 3.3), which gives lp( s|am) + lp(s |mc) &gt; lp( o|am) + lp(o |mc) lp( s|bm) + lp(s |md) &gt; lp( o|bm) + lp(o |md) lp( s|am) + lp(s |md) &lt; lp( o|am) + lp(o |md) lp( s|bm) + lp(s |mc) &lt; lp( o|bm) + lp(o |mc) For pure BiLSTM, the original proof sums the top 2 and the bottom 2 inequalities, resulting in contradicting constraints. Now, if there had been a linear-chain CRF modeling label transition probabilities (call it q), it would only add yet another linear term and would require lp( s|am) + lp(s |mc) + lq(oso) &gt; lp( o|am) + lp(o |mc) + lq(ooo) lp( s|bm) + lp(s |md) + lq(oso) &gt; lp( o|bm) + lp(o |md) + lq(ooo)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a) A confusing surface form for language and nationality.(b) A triple-token mention with unreliable capitalization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Example problematic entities for Baseline-BiLSTM-CNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a) (Partial) α 1 of "...Dutch into English...". (b) α 2 of "...the White house...". (c) α 3 of "...the White house...". (d) α 4 of "...the White house...".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Attention heat maps for the mentions inFigure 1, best viewed on computer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Overall results. *Used on WNUT for character-based word vectors, reported better than CNN. 87.14 87.75±0.14 53.24 32.93 40.68±1.78 Cross-BiLSTM-CNN 88.37 88.17 88.27±0.17 58.28 33.92 42.85±0.99 Att-BiLSTM-CNN 88.71 88.11 88.40±0.18 55.82 34.08 42.26±0.82</figDesc><table><row><cell></cell><cell></cell><cell cols="2">OntoNotes 5.0</cell><cell></cell><cell cols="2">WNUT 2017</cell></row><row><cell></cell><cell>Prec.</cell><cell>Rec.</cell><cell>F1</cell><cell>Prec.</cell><cell>Rec.</cell><cell>F1</cell></row><row><cell>BiLSTM-CNN</cell><cell cols="3">86.04 86.53 86.28±0.26</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CRF-IDCNN</cell><cell>-</cell><cell>-</cell><cell>86.84±0.19</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CRF-BiLSTM(-BiLSTM*)</cell><cell>-</cell><cell>-</cell><cell>86.99±0.22</cell><cell>-</cell><cell>-</cell><cell>38.24</cell></row><row><cell>Baseline-BiLSTM-CNN</cell><cell>88.37</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Datasets (K-tokens / K-entities).</figDesc><table><row><cell></cell><cell cols="2">OntoNotes 5.0 WNUT 2017</cell></row><row><cell>train</cell><cell>1088.5 / 81.8</cell><cell>62.7 / 1.9</cell></row><row><cell>dev</cell><cell>147.7 / 11.0</cell><cell>15.7 / 0.8</cell></row><row><cell>test</cell><cell>152.7 / 11.2</cell><cell>23.3 / 1.0</cell></row></table><note>lp( s|am) + lp(s |md) + lq(oso) &lt; lp( o|am) + lp(o |md) + lq(ooo) lp( s|bm) + lp(s |mc) + lq(oso) &lt; lp( o|bm) + lp(o |mc) + lq(ooo)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Types with significant results (&gt;3% absolute F1 differences vs. Baseline); . *Nationalities, religious, political groups.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">OntoNotes 5.0</cell><cell></cell><cell></cell><cell>WNUT 2017</cell></row><row><cell></cell><cell cols="2">event language</cell><cell>law</cell><cell cols="5">NORP* work-of-art corporation creative-work location</cell></row><row><cell cols="2">Cross +3.0%</cell><cell>+4.1%</cell><cell cols="2">+4.5% +3.3%</cell><cell>+2.1%</cell><cell>+6.4%</cell><cell>+3.2%</cell><cell>+8.6%</cell></row><row><cell>Att</cell><cell>+4.6%</cell><cell>+0.8%</cell><cell cols="2">+0.8% +3.4%</cell><cell>+5.6%</cell><cell>+0.3%</cell><cell>+2.0%</cell><cell>+5.3%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Improvements vs. Baseline among different mention lengths.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">OntoNotes 5.0</cell><cell></cell><cell></cell><cell cols="2">WNUT 2017</cell></row><row><cell></cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>3+</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>3+</cell></row><row><cell cols="9">Cross +0.3% +0.6% +1.8% +1.3% +1.7% +2.9% +8.7% +5.4%</cell></row><row><cell>Att</cell><cell cols="8">+0.1% +1.1% +2.3% +1.8% +1.5% +2.0% +2.6% +0.9%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Entity-chunking ablation results.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Att-BiLSTM-CNN</cell><cell></cell><cell></cell><cell></cell><cell>Baseline-...</cell></row><row><cell></cell><cell>HC all</cell><cell>H</cell><cell>C all</cell><cell>C 1</cell><cell>C 2</cell><cell>C 3</cell><cell>C 4</cell><cell>C 5</cell><cell>H</cell></row><row><cell>O</cell><cell>99.05</cell><cell>-1.68</cell><cell>0.75</cell><cell>0.95</cell><cell cols="2">-1.67 -45.57</cell><cell cols="2">-0.81 -35.46</cell><cell>-0.03</cell></row><row><cell>S</cell><cell>93.74</cell><cell cols="7">2.69 -91.02 -90.56 -90.88 -25.61 -86.25 -84.32</cell><cell>0.13</cell></row><row><cell>B</cell><cell>90.99</cell><cell cols="7">1.21 -52.26 -90.78 -88.08 -90.88 -12.21 -87.45</cell><cell>-0.63</cell></row><row><cell>I</cell><cell cols="2">90.09 -28.18</cell><cell cols="6">-3.80 -87.93 -60.56 -50.19 -57.19 -79.63</cell><cell>-0.41</cell></row><row><cell>E</cell><cell>93.23</cell><cell>2.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Modeling noisiness to recognize named entities using multitask neural networks on social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aguilar</surname></persName>
		</author>
		<idno>Aguilar et al. 2018</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings of the 3rd Workshop on Noisy User-generated Text</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Contextual string embeddings for sequence labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blythe</forename><surname>Akbik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vollgraf ; Akbik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Blythe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vollgraf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nichols</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
		<title level="m">Named entity recognition with bidirectional LSTM-CNNs. Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>and Nichols</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Supervised learning of universal sentence representations from natural language inference data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings of the 3rd Workshop on Noisy User-generated Text</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Incorporating Nesterov momentum into Adam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Devlin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
	</analytic>
	<monogr>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proceedings of ICLR 2016 Workshop</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multimedia lab @ ACL WNUT NER shared task: Named entity recognition for twitter microposts using distributed word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghaddar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Langlais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Godin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Vandersmissen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>De Neve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Van De Walle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weischedel</surname></persName>
		</author>
		<idno>Godin et al. 2015</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>Proceedings of the Human Language Technology Conference of the NAACL</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural architectures for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu ;</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01991</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Bidirectional LSTM-CRF models for sequence tagging</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-channel BiLSTM-CRF model for emerging named entity recognition in social media</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Noisy User-generated Text</title>
		<meeting>the 3rd Workshop on Noisy User-generated Text</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A structured self-attentive sentence embedding</title>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Effective approaches to attentionbased neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pham</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Manning ; Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Socher</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Manning ; Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Design challenges and misconceptions in named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pradhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth Conference on Computational Natural Language Learning</title>
		<meeting>the Seventeenth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>Proceedings of the Thirteenth Conference on Computational Natural Language Learning</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Results of the WNUT16 named entity recognition shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Strauss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT)</title>
		<meeting>the 2nd Workshop on Noisy User-generated Text (WNUT)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fast and accurate entity recognition with iterated dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Strubell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">End-toend learning of semantic role labeling using recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>and Xu</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
