<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">E cient Parallel Translating Embedding For Knowledge Graphs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-08-23">2017. August 23-26, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denghui</forename><surname>Zhang</surname></persName>
							<email>zhangdenghui@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences No</orgName>
								<address>
									<addrLine>6 Kexueyuan South Road</addrLine>
									<postCode>100190</postCode>
									<settlement>Zhongguancun, Beijing</settlement>
									<region>Haidian</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manling</forename><surname>Li</surname></persName>
							<email>limanling@soware.ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences No</orgName>
								<address>
									<addrLine>6 Kexueyuan South Road</addrLine>
									<postCode>100190</postCode>
									<settlement>Zhongguancun, Beijing</settlement>
									<region>Haidian</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yantao</forename><surname>Jia</surname></persName>
							<email>jiayantao@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences No</orgName>
								<address>
									<addrLine>6 Kexueyuan South Road</addrLine>
									<postCode>100190</postCode>
									<settlement>Zhongguancun, Beijing</settlement>
									<region>Haidian</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhuo</forename><surname>Wang</surname></persName>
							<email>wangyuanzhuo@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences No</orgName>
								<address>
									<addrLine>6 Kexueyuan South Road</addrLine>
									<postCode>100190</postCode>
									<settlement>Zhongguancun, Beijing</settlement>
									<region>Haidian</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences No</orgName>
								<address>
									<addrLine>6 Kexueyuan South Road</addrLine>
									<postCode>100190</postCode>
									<settlement>Zhongguancun, Beijing</settlement>
									<region>Haidian</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denghui</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences No</orgName>
								<address>
									<addrLine>6 Kexueyuan South Road</addrLine>
									<postCode>100190</postCode>
									<settlement>Zhongguancun, Beijing</settlement>
									<region>Haidian</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manling</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences No</orgName>
								<address>
									<addrLine>6 Kexueyuan South Road</addrLine>
									<postCode>100190</postCode>
									<settlement>Zhongguancun, Beijing</settlement>
									<region>Haidian</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yantao</forename><surname>Jia</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences No</orgName>
								<address>
									<addrLine>6 Kexueyuan South Road</addrLine>
									<postCode>100190</postCode>
									<settlement>Zhongguancun, Beijing</settlement>
									<region>Haidian</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhuo</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences No</orgName>
								<address>
									<addrLine>6 Kexueyuan South Road</addrLine>
									<postCode>100190</postCode>
									<settlement>Zhongguancun, Beijing</settlement>
									<region>Haidian</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences No</orgName>
								<address>
									<addrLine>6 Kexueyuan South Road</addrLine>
									<postCode>100190</postCode>
									<settlement>Zhongguancun, Beijing</settlement>
									<region>Haidian</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">E cient Parallel Translating Embedding For Knowledge Graphs</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of WI &apos;17</title>
						<meeting>WI &apos;17 <address><addrLine>Leipzig, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="volume">8</biblScope>
							<date type="published" when="2017-08-23">2017. August 23-26, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3106426.3106447</idno>
					<note>ACM Reference format:</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Knowledge Graph Embedding</term>
					<term>Translation-based</term>
					<term>Parallel</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Knowledge graph embedding aims to embed entities and relations of knowledge graphs into low-dimensional vector spaces. Translating embedding methods regard relations as the translation from head entities to tail entities, which achieve the state-of-the-art results among knowledge graph embedding methods. However, a major limitation of these methods is the time consuming training process, which may take several days or even weeks for large knowledge graphs, and result in great di culty in practical applications. In this paper, we propose an e cient parallel framework for translating embedding methods, called ParTrans-X, which enables the methods to be paralleled without locks by utilizing the distinguished structures of knowledge graphs. Experiments on two datasets with three typical translating embedding methods, i.e., TransE [3], TransH <ref type="bibr" target="#b18">[19]</ref>, and a more e cient variant TransE-AdaGrad <ref type="bibr" target="#b10">[11]</ref> validate that ParTrans-X can speed up the training process by more than an order of magnitude.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>•Computing methodologies →Reasoning about belief and knowledge;</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Knowledge graphs are structured graphs with various entities as nodes and relations as edges. ey are usually in form of RDF-style triples (h, r , t), where h represents a head entity, t a tail entity, and r the relation between them. In the past decades, a quantity of large scale knowledge graphs have sprung up, e.g., Freebase <ref type="bibr" target="#b1">[2]</ref>, WordNet <ref type="bibr" target="#b13">[14]</ref>, YAGO <ref type="bibr" target="#b11">[12]</ref>, OpenKN <ref type="bibr" target="#b6">[7]</ref>, and have played a pivotal role in supporting many applications, such as link prediction, question answering, etc. Although these knowledge graphs are very large, i.e., usually containing thousands of relation types, millions of Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. WI '17, Leipzig, Germany © 2017 ACM. 978-1-4503-4951-2/17/08. . . $15.00 DOI: <ref type="bibr">10.1145/3106426.3106447</ref> entities and billions of triples, they are still far from complete. As a result, knowledge graph completion (KGC) has been payed much a ention to, which mainly aims to predict missing relations between entities under the supervision of existing triples.</p><p>Recent years have witnessed great advances of translating embedding methods to tackle KGC problem. e methods represent entities and relations as the embedding vectors by regarding relations as translations from head entities to tail entities, such as TransE <ref type="bibr" target="#b2">[3]</ref>, TransH <ref type="bibr" target="#b18">[19]</ref>, TransR <ref type="bibr" target="#b10">[11]</ref>, etc. However, the training procedure is time consuming, since they all employ stochastic gradient descent (SGD) to optimize a translation-based loss function, which may require days to converge for large knowledge graphs.</p><p>For instance, Table1 1 shows the complexity of typical translating embedding methods, where T tot al stands for the total training time with T epoch for the time of each epoch, and one epoch is a single pass over all triples. n e , n r and n t are the number of entities, relations and triples in the knowledge graph respectively. d is the embedding dimension which is the same for entities and relations in this case, and ep is the minimum epochs which used to be set to 1000. It can be seen that the time complexity of TransE is proportional to n t , d and ep. When d is 100 and ep is 1000, it will take 78 minutes for TransE to learn the embeddings of FB15k 2 , which is a subset of Freebase with 483,142 training triples, and has been widely used as experimental dataset in knowledge graph embedding methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b18">19]</ref>. Nevertheless, Freebase-rdf-latest 3 is the latest public available data dump of Freebase with 1.9 billion triples, which results in approximately 3932 times the training time, namely, 212 days. Furthermore, the whole Freebase contains over 3 billion triples <ref type="bibr" target="#b3">4</ref> , and it will take about 357 days to learn the embeddings of it. Despite its large size, Freebase still su ers from data incomplete problem, e.g., 75% persons do not have nationalities in Freebase <ref type="bibr" target="#b4">[5]</ref>. On top of that, most improved variants of TransE employ more complex loss function to be er train the embedding vectors, thus they possess higher time complexity or model complexity, and the training time of them will be even unbearable. For example, it will take more than 59 years for Freebase-rdf-latest when employing TransR, which is one of the typical improved variants and achieves far be er performance than TransE.</p><p>ere have been a empts to resolve the e ciency issue of translating embedding methods for knowledge graphs. Pasquale <ref type="bibr" target="#b14">[15]</ref> proposed TransE-AdaGrad to speed up the training process by leveraging adaptive learning rates. However, TransE-AdaGrad essentially reduces the number of epochs to converge, and still can not do well with large scale knowledge graphs. In fact, with more and more computation resources available, it is natural and more e ective to parallel these embedding methods, which will lead to signi cant improvement in training e ciency and can scale to quite large knowledge graphs if given su cient hardware resources. However, it is challenging to parallel the translating embedding methods, since the training processes mainly employ stochastic gradient descent algorithm (SGD) or the variants of it. SGD is inherently sequential, as a dependence exists between each iteration. Parallelizing translating embedding methods straightforwardly will result in collisions between di erent processors. For instance, an entity embedding vector is updated by two processors at the same time, and the gradients calculated by these processors are di erent. In this case, the diverse gradients are called collisions. To avoid collisions, some methods <ref type="bibr" target="#b8">[9]</ref> lock embedding vectors, which will slow the training process greatly as there are so many vectors. On the contrary, updating vectors without locks leads to high e ciency, but should be based on speci c assumptions <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b15">16]</ref>. Since the lockfree training process may result in poor convergence if adopting suboptimal strategy to resolve collisions.</p><p>Our key observation of translating embedding methods is that the update performed in one iteration of SGD is based on only one triple and its corrupted sample, which is not necessarily bound up with other embedding vectors. is gives us chance to learn the embedding vectors in parallel without being locked. In this article, we analyze the distinguished data structure of knowledge graphs, and propose an e cient parallel framework for translating embedding methods, called ParTrans-X. It enables translating methods to update the embedding vectors e ciently in shared memory without locks. us the training process is greatly speeded up with multi-processors, which can be more than an order of magnitude faster without lowering learning quality. e contribution of this aritcle is: 1. We explore the law of collisions along with increasing number of processors, by modelling the training data of knowledge graph into hypergraphs.</p><p>2. We propose ParTrans-X framework to train translating methods e ciently in parallel. It utilizes the training data sparsity of large scale knowledge graphs, and can be easily applied to many translating embedding methods.</p><p>3. We apply ParTrans-X to typical translating embedding methods, i.e., TransE <ref type="bibr" target="#b2">[3]</ref>, TransH <ref type="bibr" target="#b18">[19]</ref>, and a more e cient variant TransE-AdaGrad, and experiments validate the e ectiveness of ParTrans-X on two widely used datasets. e paper is organized as follows. Related work is in Sec.2. e collision formulation is introduced in Sec.3 and ParTrans-X is proposed based on it in Sec.4. en, experiments demonstrate training e ciency of ParTrans-X in Sec.5, with conclusions in Sec.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In recent years, translating embedding methods have played a pivotal role in Knowledge Graph Completion, which usually employ stochastic gradient descent algorithm to optimize a translationbased loss function, i.e.,</p><formula xml:id="formula_0">L = (h,r,t ) (h ,r,t ) max 0, f r (h, t) + M − f r (h , t ) ,<label>(1)</label></formula><p>where (h, r , t) represents the positive triple that exists in the knowledge graph, while (h , r , t ) stands for the negative triple that is not in the knowledge graph. max [0, ·] is the hinge loss , and M is the margin between positive and negative triples. f r (h, t) is the score function to determine whether the triple (h, r, t) should exist in the knowledge graph, which varies from di erent translating embedding methods. A signi cant work is TransE <ref type="bibr" target="#b2">[3]</ref>, which heralds the start of translating embedding methods. It looks upon a triple (h, r, t) as a translation from the head entity h to the tail entity t, i.e., h + r ≈ t, and the score function is f r (h, t) = ||h+r−t||, where || · || represents L1-similarity or L2-similarity. e boldface suggests the vectors in the embedding space, namely, h, t ∈ R d , r ∈ R d , where d = d e = d r is the dimension of embedding space, d e the dimension for entities and d r for relations. Moreover, TransH <ref type="bibr" target="#b18">[19]</ref> assumes that it is the projections of entities to a relation-speci c hyperplane that satisfy the translation constraint, i.e., f r (h, t) = ||h ⊥ + r − t ⊥ ||, where h ⊥ = h − w r hw r and t ⊥ = t − w r tw r , with w r ∈ R d e as the normal vector of the hyperplane related to r . Furthermore, TransR <ref type="bibr" target="#b10">[11]</ref> employs rotation transformation to project the entities to a relation-speci c space, i.e., f r (h, t) = ||h r + r − t r ||, where h r = M r h and t r = M r t, and M r ∈ R d r ×d e is the projection matrix relation to r . Some works also involves more information to be er embedding, e.g., paths <ref type="bibr" target="#b9">[10]</ref>, margins <ref type="bibr" target="#b7">[8]</ref>.</p><p>Although this category of methods achieve the state-of-the-art results, the main limitation is the computationally expensive training process when facing large scale knowledge graphs. Recently, a method TransE-AdaGrad <ref type="bibr" target="#b14">[15]</ref> was proposed to reduce the training time of TransE by employing AdaGrad <ref type="bibr" target="#b5">[6]</ref>, an variant of SGD, to adaptively modify the learning rate. Although the training time has been reduced greatly, there is still some way to go when facing large scale knowledge graphs. With the computation resources greatly enriched, training in parallel seems to be a more reliable way to relieve this issue. Actually, there are some works, e.g., <ref type="bibr" target="#b17">[18]</ref>, to parallel some graph computation paradigms, such as online query processing, o ine analytics, etc. Nevertheless, it is not easy to train translating embedding methods in parallel, since the main optimation algorithm SGD is born to run in sequence. e major obstacle to parallel SGD is the collisions between updates of different processors for the same parameter <ref type="bibr" target="#b16">[17]</ref>, to overcome which there are two main brunches of methods. e rst brunch is to design a strategy to resolve collisions according to speci c data structure. For example, Hogwild! <ref type="bibr" target="#b15">[16]</ref> is a lock-free scheme works well for sparse data, which means that there is only a small part of parameters to update by each iteration of SGD. It has been proved that processors are unlikely to overwrite each other's progress, and the method can achieve a nearly optimal rate of convergence. While the second brunch is to split the training data to to reduce collisions. Downpour SGD <ref type="bibr" target="#b3">[4]</ref> mainly employ Dist-Belief <ref type="bibr" target="#b3">[4]</ref> framework, which divides the training data into a number of subsets, then the model replicas run independently on each of these subsets, and do not communicate with each other. Inspired by this, TensorFlow [1] splits a computation graph into a subgraph for every worker and communication takes place using Send/Receive node pairs. Motivated by training large-scale convolutional neural networks for image classi cation, Elastic Averaging SGD (EASGD) <ref type="bibr" target="#b19">[20]</ref> reduces the amount of communication between local workers and the master to allow the parameters of local workers to uctuate further from the center ones. ere are also works to improve the performance in parallel se ings, e.g., Delay-tolerant Algorithms for SGD <ref type="bibr" target="#b12">[13]</ref> adapts not only to the sequence of gradients, but also to the precise update delays that occur, inspired by AdaGrad.</p><p>However, these parallel framework are based on speci c assumptions, and can not directly apply to translating embedding models without exploring distinguished data structures of knowledge graphs. erefore, we shall propose a parallel framework for translating embedding models, called ParTrans-X, as knowledge graphs are mainly in form of triples, and trained triple by triple, it will lead to particular parallel framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">LAW OF COLLISIONS EMERGING IN KG</head><p>As mentioned previously, there may exist collisions between processors when they update the same embedding vector, which ends up being one of the most challenging aspects of parallelizing translating embedding methods. Hence, we explore the law of collisions emerging in this section. At rst we formulate the training data of knowledge graphs into hypergraphs. en the collisions in training process are further discussed based on this formulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Hypergraph Formulation</head><p>Firstly, we model the knowledge graph formally as G = (E, R,T ), where E is the set of entities with R the set of relations, and T is the set of triples (h, r , t), in which h, t ∈ E and r ∈ R. e cardinalities of E, R and T are n e , n r and n respectively. In this graph, nodes are entities, and edges are triples that connecting nodes with a distinguished relation. For example, the knowledge graph shown in Figure1(a), where black nodes stand for the entities in knowledge graphs and lines for relations, can be represented as G = (E, R,T ), where E = {e 1 , e 2 , e 3 , e 4 , e 5 }, R = {r 1 , r 1 , r 3 } and T = {(e 1 , r 2 , e 4 ), (e 1 , r 1 , e 3 ), (e 2 , r 3 , e 5 ), (e 4 , r 3 , e 2 )}. In this case, n e = 5, n r = 3 and n = 4.</p><p>Secondly, the training data of knowledge graphs can be looked upon as hypergraphs. Recall the loss function of translating embedding methods in Eq. <ref type="formula" target="#formula_0">(1)</ref>, which means in one iteration of SGD, only one positive triple (h, r, t) and one negative triple (h , r , t ) are concerned. To be more clear, the data used in one iteration, i.e., [(h, r , t), (h , r , t )], is called a sample. Note that (h , r, t ) is constructed by substituting one entity h ∈ E or t ∈ E for h or t respectively, contributing to a corrupted triple (h , r, t) or (h, r, t ), which is just simply denoted by (h , r , t ) following <ref type="bibr" target="#b2">[3]</ref>.   <ref type="formula">)</ref> is (e 1 , r 1 , e 2 ), which contributes to a sample s 1 = {e 1 , r 1 ,e 2 ,e 3 }, thus the hyperedge colored by red contains e 1 , e 2 , e 3 and r 1 . Note that many other negative triples can be constucted, e.g., (e 1 , r 1 , e 5 ) for triple (e 1 , r 1 , e 3 ), and the hypergraph generated in <ref type="figure">Fig</ref> To be er analyze the collisions between processors, we de ne the following statistics of the hypergraph H . Given a hyperedge s,</p><formula xml:id="formula_1">, where s = {h, r , t, h (or t ) : h, r , t, h , t ∈ E, r ∈ R}.</formula><formula xml:id="formula_2">σ (s) = {s : ∃r ∈ s ∩ s , r ∈ R}<label>(2)</label></formula><p>denotes the set of hyperedges containing the same relations with hyperedge s.σ</p><formula xml:id="formula_3">= max s ∈S |σ (s)|<label>(3)</label></formula><p>denotes the maximal number of hyperedges containing same relations, where | · | denotes the cardinality.</p><formula xml:id="formula_4">ρ(s) = {s : ∃e ∈ s ∩ s , e ∈ E}<label>(4)</label></formula><p>denotes the set of hyperedges containing one or more same entities with hyperedge s.ρ</p><formula xml:id="formula_5">:= max s ∈S |ρ(s)|.<label>(5)</label></formula><p>denotes the maximal number of hyperedges containing same entities, where | · | denotes the cardinality the same as before.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Collision Formulation</head><p>In this section, we will verify that it is highly possible that few collisions happen when training by p processors for large and sparse knowledge graphs. Let X samp represent the event that p processors select p di erent samples. X r el represents the event that there are collisions between relations, i.e., di erent processors updates a same relation vector, and X ent between entities similarly. e veri cation is decomposed into two steps, 1) to prove it is quite likely that the processors handle di erent samples, i.e., P(X samp = 1) ≈ 1, which is the prerequisite to no collisions; 2) to prove it is unlikely that these di erent samples correspond to the same relations or entities, i.e., P(X r el = 0) ≈ 1 and P(X ent = 0) ≈ 1.</p><p>Supposing that for embedding methods and the knowledge graph, the training samples S = {s 1 , s 2 , . . . , s i , . . . , s n } of size n is drawn independent and identically distributed (i.e., i.i.d.) from some unknown distribution D.</p><p>erefore, the probability of s i being selected P i is supposed to be</p><formula xml:id="formula_6">P i = 1 n .<label>(6)</label></formula><p>Moreover, according to i.i.d., it is reasonable to assume that the sample selecting process by p processors is an observation from a Multinomial Distribution, i.e., selecting one sample from n samples and repeated p times. Let x i denote the number of processors that select s i during the same iteration of SGD, then the possibility of s i being selected by c 1 processors, . . ., s n e being selected by c n e processors is as follows,</p><formula xml:id="formula_7">P(x 1 = c 1 , ...,x n e = c n e ) =          p! c 1 !c 2 ! · · · c n e ! P c 1 1 ...P c ne n e , n e i=1 c i = p 0, otherwise<label>(7)</label></formula><p>where n e i=1 c i = p indicates that there are p and only p samples being selected in the same iteration of SGD.</p><formula xml:id="formula_8">T 3.2.</formula><p>For a knowledge graph with n triples and training by p processors in parallel, when n is large and p is relatively small, the possibility that p processors select p di erent samples is</p><formula xml:id="formula_9">P(X samp = 1) ≈ 1<label>(8)</label></formula><p>with probability at least γ , where</p><formula xml:id="formula_10">γ = p−1 i=1 (1 − i n ).<label>(9)</label></formula><p>P . Provided that samples selected by processors are di erent, it can be easily derived that ∀s i ∈ S, x i ≤ 1. en there are only n p sampling circumstances satisfying no collisions between samples, where p distinct samples are selected once, and other n −p samples are not selected, e.g., x 1 = 1, x 2 = 1, · · · , x p = 1, x p+1 = 0, · · · , x n = 0. erefore, according to Eq.(7) and Eq. <ref type="formula" target="#formula_6">(6)</ref>,</p><formula xml:id="formula_11">P(X samp = 1) = n p p! p i=1 1! p i=1 (P i ) 1 = n! (n − p)! ( 1 n ) p = n(n − 1)(n − 2) · · · (n − p + 1) n p = (1 − 1 n )(1 − 2 n ) · · · (1 − p − 1 n ) = p−1 i=1 (1 − i n )</formula><p>When n is large and p is relatively small, P(X samp = 1) ≈ 1.</p><formula xml:id="formula_12">T 3.3.</formula><p>For a knowledge graph with n triples and training in p processors in parallel, whenσ n is relatively small and p &lt; n σ + 1, we have the possibility of no relation in a collision is P(X r el = 0) ≈ 1 <ref type="bibr" target="#b9">(10)</ref> with probability at least γ , where</p><formula xml:id="formula_13">γ = p−1 i=1 (1 − iσ n ).<label>(11)</label></formula><p>P . Given that p processors select p di erent samples, the posibility of relations in a collision can be deduced according to conditional probability as follows,</p><formula xml:id="formula_14">P(X r el = 0) = P(X r el = 0|X samp = 1) · P(X samp = 1),<label>(12)</label></formula><p>where P(X r el = 0|X samp = 1) is the possibility of p samples containing distinct relations being selected, which is supposed to be similar to sampling without replacement. More precisely, assuming a sample s is selected randomly, then the next sample selected s should be from S − σ (s), and the third sample s should be selected from in S − σ (s) ∪ σ (s ). Accordingly, P(X r el = 0|X samp = 1) is deduced as follows when p &lt; n σ + 1 is satis ed, P(X r el = 0|X samp = 1)</p><formula xml:id="formula_15">= s 1 ,s 2 , ...,s p ∈S 1 n · |S − σ (s 1 )| n − 1 · |S − σ (s 1 ) ∪ σ (s 2 )| n − 2 . . . |S − σ (s 1 ) ∪ σ (s 2 ) ∪ · · · ∪ σ (s p−1 )| n − (p − 1) ≥ s 1 ,s 2 , ...,s p ∈S 1 n · n −σ n − 1 · n − 2σ n − 2 . . . n − (p − 1)σ n − (p − 1) = (1 −σ − 1 n − 1 )(1 − 2(σ − 1) n − 2 ) · · · (1 − (p − 1)(σ − 1) n − (p − 1) ) = p−1 i=1 (1 − i(σ − 1) n − i )</formula><p>By Eq. <ref type="formula" target="#formula_0">(12)</ref>, the possibility of no collisions between relations in di erent processors is</p><formula xml:id="formula_16">P(X r el = 0) = p−1 i=1 (1 − i(σ − 1) n − i ) · p−1 i=1 (1 − i n ) = p−1 i=1 (1 − iσ n ).<label>(13)</label></formula><p>Note that p &gt; n σ + 1 results in 1 − (p−1)(σ −1) n−(p−1) &lt; 0, which meanŝ σ is so large that one or more processors will de nitely select the same relation among p processors, namely, P(X r el = 0) = 0. Furthermore, whenσ n is relatively small, P(X r el = 0) ≈ 1.</p><p>Similarly, the possibility of no entities in a collision can be derived as follows, and no more tautology here due to the limitation of length. T 3.4. For a knowledge graph with n triples and training in p processors in parallel, whenρ n is relatively small and p &lt; n ρ + 1, the possibility of no collisions between entities is P(X ent = 0) ≈ 1 <ref type="bibr" target="#b13">(14)</ref> with probability at least γ , where</p><formula xml:id="formula_17">γ = p−1 i=1 (1 − iρ n ).<label>(15)</label></formula><p>It is veri ed in eorem3.2, eorem3.3 and eorem3.4 that if n is large andσ andρ are relatively small, i.e., the knowledge graph is large and sparse, the number of processors p can be very large with supportable collisions, which enables the training process to run in parallel. Motivated by this, we de ne sparsity of training data in a knowledge graph by min(σ n ,ρ n ). e smaller its value is, the more processors can be used to parallel the training process. Actually, it is the large and sparse knowledge graphs that are in dire need of parallel translating embedding methods. Since they are far from completion, but are too large to train in serial. Besides, sincê σ andρ is deduced by the worst case, it is reasonable to assume that the averageσ andρ can be er re ect the general structures in knowledge graphs, and the collisions will be less in practice. As a result, we suppose that it would still work well if the averagē σ andρ are relatively small, as a few collisions will not a ect the consistency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Special Insights on Parallelizing TransE</head><p>ere is an interesting nding that TransE can be further parallelized than other translating embedding methods, since there are less collisions due to the distinguished score function f r (h, t) = ||h + r − t||. More precisely, the gradient calculation of TransE when using L2-similarity is as follows,</p><formula xml:id="formula_18">h k := h k − η · 2(h k + r k − t k ), h k := h k + η · 2(h k + r k − t k ) r k := r k − η · 2(h k + r k − t k ), r k := r k + η · 2(h k + r k − t k ) t k := t k + η · 2(h k + r k − t k ), t k := t k − η · 2(h k + r k − t k )<label>(16)</label></formula><p>where h k represents the k-th dimension of embedding vector h, k ∈ {0, 1, · · · , d}, and d is the dimension of embedding space. It can be seen that in TransE, the gradient of each dimension is independent of other dimensions, which means that the collisions between di erent dimensions of the same embedding vector will not disturb each other. at is to say, only the collisions between the same dimension of the same embedding vector will ma er in the training process of TransE. For example, Figure2 shows the updating of h by two processors (Processor1 and Processor2) at the same time, where ∇ k is the gradient of h k calculated by Processor1, and ∇ k by Processor2. Normally, when Processor2 calculates the gradient ∇ k , the whole embedding vector h will be involved, which is half updated by Processor1. Obviously, this will result in training errors. On the contrary, if it is the training process of TransE in Figure2, the Consequently, the possibility of collisions emerging is greatly decreased for TransE. Since not only the entities or relations are the same one, but also the dimensions being updated are the same. Namely, the maximal degree of parallelism is far larger than other translating embedding methods. is indicates that parallelizing without locks is ideally situated for TransE, and may scale well to extremely large knowledge graphs by given su cient computation resources.</p><formula xml:id="formula_19">caculate ∇ 1 h 1 ← h 1 − α∇ 1 Processor 1 Processor 2 time caculate ∇ 2 h 2 ← h 2 − α∇ 2 caculate ∇ k h de ← h de − α∇ de .......... caculate ∇ 1 h 1 ← h 1 − α∇ 1 caculate ∇ 2 h 2 ← h 2 − α∇ 2 caculate ∇ k h de ← h de − α∇ de ..........</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">THE PARTRANS-X FRAMEWORK</head><p>Inspired by the ndings that collisions between processors are negligible when a knowledge graph is large and sparse, a parallel framework for these methods is designed, called ParTrans-X, and we will describe it in detail in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Framework Description</head><p>e pseudocode for implementation of ParTrans-X is shown in Algorithm 1. As the embedding vectors are updated frequently, they are stored in shared memory and every processors can perform updates to them freely. e training process of ParTrans-X starts with initializing the embedding vectors according to Uniform or Bernoulli Distribution, where no parallel section is needed since it takes constant time. However we can parallel the learning process of each epoch, which is the most time consuming part. Running by p processors in parallel can decrease the training epochs by p times, i.e., the parallel training epoch is ep = ep p . To do this, we rst determine the random sampling seed seed[i] by calling SEED RAN D for the i-th processor. e random sampling seeds di er from each other to avoid same pseudo-random sequence for di erent processors. en, each processor performs embedding learning procedure epoch by epoch asynchronously (lines 5-12). One epoch is a loop over all triples. Each loop is done by rstly normalizing the entity embedding vectors following <ref type="bibr" target="#b2">[3]</ref>. en a positive triple P[i] (j) = (h, r, t) is sampled from shared memory, where i means that the current processor is i-th processor, and superscript j stands for j-th epoch. According to P[i] (j) , a negative triple N [i] (j) = (h , r , t ) is generated by sampling a corrupted entity h (or t ) from shared memory, where i and j are the same as before. at is to say, a sample S[i] <ref type="bibr">(j)</ref> is constructed by P[i] <ref type="bibr">(j)</ref> and N [i] <ref type="bibr">(j)</ref> , which then be used to calculate the gradient ∇[f r (h, t) + M − f r (h , t )] according to Eq.(1), and update the embeddings of entities and relations (h, r, t, h , t ) (j+1) ← (h, r, t, h , t ) (j) − η∇[i] (j) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 ParTrans-X Require:</head><p>Training triples T = {(h, r, t)}, entities and relations set E and R, embedding dimension d, margin M, training epochs ep, the number of processors p; Ensure:</p><p>Embeddings of entities and relations; 1: Initialize r ∈ R and e ∈ E by uniform distribution and persist them in the shared memory 2: for i ← 0 to p do e := e | |e | | for each entity e ∈ E 8:</p><formula xml:id="formula_20">P[i] (j) ← SAMPLE(T , seed[i]) 9: N [i] (j) ← SAMPLE N EG(P[i] (j) ,T , E, seed[i])</formula><p>10: e framework can be applied to many translating embedding methods, which employ SGD or its variants to optimize the hinge loss with similar algorithm framework, and are only di erent in the score function f r (h, t) as mentioned in Sec.2, e.g., TransE, TransH and so on. Hence, the parallel algorithm of them can be obtained by applying the corresponding score function in Lines 11-12 of the pseudocode in Algorithm 1.</p><formula xml:id="formula_21">S[i] (j) ← P[i] (j) , N [i] (j) 11: ∇[i] (j) ← ∇[f r (h, t) + M − f r (h , t )], where h, r, t, h , t ∈ S[i] (j) 12: (h, r, t, h , t ) (j+1) ← (h, r , t, h , t ) (j) − η∇[i] (j)</formula><p>For example, for TransE, the gradient updating procedure in Lines 11 is performed according to Eq. <ref type="bibr" target="#b15">(16)</ref>. For TransH, which employs the score function f r (h, t) = ||h ⊥ + r − t ⊥ ||, the gradient updating procedure of h in Lines 11 is as follows,</p><formula xml:id="formula_22">h k := h k − η · 2 (h k − w r hw rk ) + r k − (t k − w r tw rk ) .<label>(17)</label></formula><p>Namely, ParTrans-X has the exibility to parallel many translating embedding methods, since they possess similar training process. Moreover, ParTrans-X can be directly applied to the improved variant TransE-AdaGrad, since the training data sparsity of knowledge graph still holds. In one iteration of AdaGrad, it updates the embedding vectors according to the gradient from the previous iteration. Highly similar to SGD, AdaGrad can be easily parallelled using our framework by only performing a learning rate calculation procedure during the gradient update procedure, i.e., Line 12 of the pseudocode in Algorithm 1. For example, to parallel TransE-AdaGrad, the learning rate is determined adaptively by adding</p><formula xml:id="formula_23">η (j) := ∇ (j) j k =1 (∇ (k ) ) 2 η *<label>(18)</label></formula><p>before Line 12 in Algorithm 1, where j is the current epoch, with η (j) the learning rate of j-th epoch. ∇ (k ) , k &lt; j represents all the previous gradient before j-th epoch. η * is the initial learning rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENT</head><p>Firstly, we apply ParTrans-X to TransE, TransH and TransE-Adagrad in Sec.5.1. In Sec.5.2, experiment results demonstrate excessive decline in training time by ParTrans-X, with scaling performance along with increasing number of processors shown in Sec.5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Settings</head><p>e datasets employed are two representative datasets WN18 and FB15k, which are subsets of well-known knowledge graphs Word-Net and Freebase respectively, and have been widely used by translating embedding methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b18">19]</ref>. Table2 shows the statistics of them. Without loss of generality,ρ n andσ n are also shown, and they are both small on WN18 and FB15k. Furthermore, it can be seen that the two datasets possess di erent characteristics. Namely, WN18 possesses only 18 relations, which results in large possibility of collisions between relations. On the contrary, FB15k is less unbalanced in the number of entities and relations. To tackle the KGC problem, experiments are conducted on the link prediction task which aims to predict the missing entities h or t for a triple (h, r , t). Namely, it predicts t given (h, r ) or predict h given (r, t). Similar to the se ing in <ref type="bibr" target="#b2">[3]</ref>, the task returns a list of candidate entities from the knowledge graph.</p><p>To evaluate the performance of link prediction, we adopt Mean Rank and Hits@10 under "Raw" and "Filter" se ings as evaluation measure following <ref type="bibr" target="#b2">[3]</ref>. Mean Rank is the average rank of the correct entities, and Hits@10 is proportion of correct entities ranked in top-10. It is clear that a good predictor has low mean rank and high Hits@10. is is called "Raw" se ing, and "Filter" se ing lters out the corrupted triples which are correct.</p><p>To evaluate the speed up performance, we adopt Training Time and Speed-up Ratio as evaluation measures, where Training Time is measured using wall-clock seconds. Speed-up Ratio is</p><formula xml:id="formula_24">Speedup Ratio = t ser ial t par all el ,<label>(19)</label></formula><p>where t ser ial is the training time in serial, and t par all el is the training time under parallel methods. Baselines include typical translating embedding methods, TransE, TransH and TransE-Adagrad, which can all be trained in parallel using the ParTrans-X framework, denoted by ParTransE, ParTransH and ParTransE-Adagrad respectively in Table3. Note that TransE and TransH adopt the programs publicly available 5 , which are the most e cient serial versions to our knowledge, and TransE-Adagrad is implemented based on TransE.</p><p>Each experiment is conducted 10 times and the average is taken as results, with all time measured in wall-clock seconds. Our experiments are carried out on dual Intel Xeon E5-2640 CPUs, and each of them possesses 10 physical cores 20 logical cores and running at 2.4 GHz. e machine has 128 GB RAM and runs Red Hat 4.4.7. e language used is C++ and the program is compiled with the gcc compiler version 6.3.0. We use OpenMP for multithreading, each thread binds a processor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Link Prediction Peformance of ParTrans-X</head><p>Experiments on each baseline and its parallel implementation in ParTrans-X employ the same hyper-parameters, which are decided on the validation set. e learning rate η during the stochastic gradient descent process is selected among {0.1,0.01,0.001}, the embedding dimension d e and d r are selected in {20,50,100}, the margin M between positive and negative triples is selected among {1,2,3,4}. It can be observed from Table3 that: 1. Link prediction performance in parallel is as good as the serial counterparts on both WN18 and FB15k, which demonstrates that ParTrans-X will not a ect embedding performance.</p><p>2. e training time is greatly reduced by ParTrans-X. On WN18, TransE-AdaGrad only speeds up TransE by 4.7 times, compared to our 28 times. On FB15k, the training time of TransE is reduced from more than 1 hour to less than 1 minute by ParTransE-AdaGrad. <ref type="bibr" target="#b4">5</ref> h ps://github.com/thunlp/KB2E 3. ParTrans-X achieves higher speedup ratio on FB15k than on WN18. Since FB15k has far more training triples than WN18, the time of each epoch on FB15k is much longer than WN18. As a result, the overhead of multi-threading is less important compared to the whole training time on FB15k, which leads to a higher speedup ratio. It further validates the superiority of ParTrans-X to handle the data with large size.</p><p>4. ParTrans-X achieves enormous improvement on training time when applying to TransEAdaGrad, especially on FB15k, where the speedup ratio has been improve to 111 from 9. Since AdaGrad decreases the total epochs needed by making the convergence come earlier, and ParTrans-X reduce training time by running in parallel, the two di erent strategies can achieve higher speedup ratio when combined. Moreover, the descent process of loss for the three algorithms on WN18 and FB15k is shown in Figure3. It can be seen that, for both datasets, the loss optimizing by ParTrans-X has already fallen sharply in the preceding epochs, and it yields sensibly lower values of the loss than TransE-AdaGrad and TransE even a er a few iterations(&lt; 5 epoches). Still, ParTrans-X performs be er on FB15k than WN18, shows that it is more e ective on large data size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Scaling Results for Multi-Processors</head><p>Furthermore, we carry out a number of experiments to test if the implementations scale with increasing number of processors. We mainly analyze two aspects of experiment results, i.e., the training time and the link prediction performance.</p><p>Figure4 shows the log-log plot of the training time in wall-clock seconds for di erent number of processors. We can observe that the training time continue to decrease along with the increasing number of mutli-processors on both WN18 and FB15k. While the absolute training time of ParTransE-AdaGrad is be er than ParTransE, which is be er than ParTransH, consistent with the previous result. Moreover, the total training time of ParTransE-AdaGrad drops sharply when processor number is less than four, it is because the training time of ParTransE-AdaGrad with few processors is fairly short, the increase of communication time cost with the more processors has larger e ect on the total training time compared with other methods, which leads to small decline.  <ref type="figure">Figure 5</ref>: Hits@10 performance along with number of processors e predictive performance measured by Hits@10 along with increasing number of processors is shown in Figure5. It can be seen that ParTransE, ParTransE-AdaGrad and ParTransH always maintain good performance, which validates the applicability and superiority of ParTrans-X. Note that the performance on FB15k is more stable than WN18, since there are more training triples in FB15k, and the model will learn more su cient so that the stability of predictive performance is be er on FB15k, which validates the superiority of ParTrans-X on large data size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we explore the law of collisions emerging in knowledge graphs by modelling training data to hypergraphs. Our key observation is that one learning iteration only concerns few embeddings, which is not necessarily bound up with others, thus the probability of collisions between di erent processors can be negligible. Based on this assumption, we propose an e cient parallel framework for translating embedding methods, called ParTrans-X. It employs the intrinsic sparsity of training data in large knowledge graphs, which enables the embedding vectors to be learnt without locks and not inducing errors. Experiments validate that ParTrans-X can speed up the training process by more than an order of magnitude, without degrading embedding performance. e source code of this paper can be obtained from here 6 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">ACKNOWLEDGE</head><p>We thank Jun Xu and the anonymous reviewers for valuable suggestions. e work was funded by National Natural Science Foundation of China <ref type="table" target="#tab_0">(No. 61572469</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>A Knowledge Graph (a) and one of the Hypergraphs generated by its training data (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>For example, the hypergraph in Figure1(b) is one of the hypergraphs generated by Figure1(a), where black nodes are entities and colored nodes are relations, and the colored blocks represent hyperedges. Here, di erent colors are related to di erent relations. For instance, for triple (e 1 , r 1 , e 3 ), the negative triple sampled in Figure1(b</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>-ure1(b) is just an example. Similarly, the other samples in Figure1(b) are s 2 = {e 1 , r 2 , e 4 , e 5 }, s 3 = {e 2 , r 3 , e 3 , e 5 } and s 4 = {e 4 , r 3 , e 2 , e 3 }.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Updating embedding vector h in parallel calculation of ∇ k by Processor2 only concerns the k-th dimension h k . As a result, there will no disturbance between Processor1 and Processor2, as long as the two processors are not performing update to the same dimension of the same embedding vector.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>seed[i] ← SEED RAN D(i) 5:for j ← 0 to ep do 6: loop 7:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>For</head><label></label><figDesc>TransE and ParTransE, the parameters are η = 0.01, d e = d r = 20, M = 3 on WN18, and η = 0.001, d e = d r = 100, M = 4 on Fb15k. For TransH and ParTransH, the parameters are η = 0.01, d e = d r = 20, M = 3 on WN18, and η = 0.001, d e = d r = 100, M = 3 on Fb15k. For TransEAdaGrad and ParTransE-AdaGrad, the parameters are η * = 0.3, d e = d r = 50, M = 4 on WN18, and η * = 0.1, d e = d r = 100, M = 3 on Fb15k. All the experiments employ L1-similarity. ParTransE, ParTransH and ParTransE-AdaGrad all run in 20 processors for both datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>FB15kFigure 3 :</head><label>3</label><figDesc>e descent process of loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>FB15kFigure 4 :</head><label>4</label><figDesc>Log-log plot of Training Time along with number of processors</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>, 61402442, 91646120,61572473, 61402 022), the National Key R&amp;D Program of China (No. 2016QY02D0405, 2016YFB1000902), and National Grand Fundamental Research 973 Program of China (No. 2013CB329602, 2014CB340401).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Complexity Analysis of Typical translating embedding Methods, with d = 100, ep = 1000 × d 2 × k) O(dn e + dn r + n r d 2 )</figDesc><table><row><cell>Time</cell><cell>Model</cell><cell></cell><cell>on FB15k</cell><cell cols="2">on Freebase-rdf-latest</cell><cell cols="2">on the whole Freebase</cell></row><row><cell>Complexity</cell><cell>Complexity</cell><cell>T epoch</cell><cell>T tot al</cell><cell>T epoch</cell><cell>T tot al</cell><cell>T epoch</cell><cell>T tot al</cell></row><row><cell>TransE O(n t × d × k)</cell><cell>O(dn e + dn r )</cell><cell>4.5s</cell><cell>4658s ≈ 78 minutes</cell><cell>17,696s ≈ 5 hours</cell><cell>18,323,395s ≈ 212 days</cell><cell>29,781s ≈ 8 hours</cell><cell>30,828,893s ≈ 357 days</cell></row><row><cell>TransH O(n t × d × k)</cell><cell>O(dn e + 2dn r )</cell><cell>6s</cell><cell cols="2">100 minutes 6.5 hours</cell><cell>273 days</cell><cell>11 hours</cell><cell>459 days</cell></row><row><cell cols="3">TransR O(n t 473s</cell><cell>5 days</cell><cell>21.5 days</cell><cell>59 years</cell><cell>36 days</cell><cell>99 years</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Consequently, a sample corresponding to three entities, i.e., h, t, h or t , and one relation r . As a result, the training data can be formulated in to a 4-uniform hypergraph, in which all the hyperedges have the same cardinality 4. In this hypergraph, nodes are entities and relations, and edges are training samples containing 4 nodes, i.e., three entities and one relation. More formally, De nition 3.1. e training data to embed the knowledge graph G = (E, R,T ) by translating embedding methods is organized as a 4-uniform hypergraph H = (V , S), where V = {E ∪ R} is the set of entities or relations, and S is the set of training samples s</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Two widely used datasets in KGs.</figDesc><table><row><cell cols="2">Data # Rel # Ent #Train #Valid #Testσ /nρ/n</cell></row><row><cell>WN18 18</cell><cell>40,943 141,442 5,000 5,000 1.7e-1 5.6e-4</cell></row><row><cell cols="2">FB15k 1,345 14,951 483,142 50,000 59,071 9.3e-3 2.3e-3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Link prediction performance with all time measured in wall-clock seconds.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>WN18</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>FB15k</cell><cell></cell><cell></cell></row><row><cell>Metric</cell><cell cols="2">Mean Rank</cell><cell cols="2">Hits@10</cell><cell>Training</cell><cell>Speedup</cell><cell cols="2">Mean Rank</cell><cell cols="2">Hits@10</cell><cell>Training</cell><cell>Speedup</cell></row><row><cell></cell><cell cols="4">Raw Filter Raw Filter</cell><cell>Time(s)</cell><cell>Ratio</cell><cell cols="4">Raw Filter Raw Filter</cell><cell>Time(s)</cell><cell>Ratio</cell></row><row><cell>TransE</cell><cell>214</cell><cell>203</cell><cell>58.2</cell><cell>65.9</cell><cell>473</cell><cell>-</cell><cell>184</cell><cell>73</cell><cell>44.5</cell><cell>60.7</cell><cell>4658</cell><cell>-</cell></row><row><cell>ParTransE</cell><cell>217</cell><cell>206</cell><cell>55.7</cell><cell>63.1</cell><cell>54</cell><cell>9</cell><cell>185</cell><cell>74</cell><cell>44.4</cell><cell>60.5</cell><cell>364</cell><cell>13</cell></row><row><cell>TransE-AdaGrad</cell><cell>209</cell><cell>197</cell><cell>68.9</cell><cell>77.7</cell><cell>100</cell><cell>4.7</cell><cell>185</cell><cell>69</cell><cell>45.3</cell><cell>62.3</cell><cell>496</cell><cell>9</cell></row><row><cell cols="2">ParTransE-AdaGrad 219</cell><cell>208</cell><cell>67.7</cell><cell>76.2</cell><cell>17</cell><cell cols="2">28 (4.7×6) 186</cell><cell>70</cell><cell>44.9</cell><cell>61.9</cell><cell>42</cell><cell>111 (9×12)</cell></row><row><cell>TransH</cell><cell>227</cell><cell>216</cell><cell>66.5</cell><cell>75.9</cell><cell>637</cell><cell>-</cell><cell>183</cell><cell>60</cell><cell>46.6</cell><cell>65.5</cell><cell>6066</cell><cell>-</cell></row><row><cell>ParTransH</cell><cell>215</cell><cell>203</cell><cell>66.8</cell><cell>76.6</cell><cell>134</cell><cell>4.8</cell><cell>183</cell><cell>60</cell><cell>46.8</cell><cell>65.7</cell><cell>474</cell><cell>13</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">h ps://github.com/zdh2292390/ParTrans-X</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martn</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Je</forename><surname>Rey Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ma</forename><surname>Hieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devin</forename></persName>
		</author>
		<title level="m">TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM SIGMOD international conference on Management of data</title>
		<meeting>the 2008 ACM SIGMOD international conference on Management of data</meeting>
		<imprint>
			<publisher>AcM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Large scale distributed deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Je Rey Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajat</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ma Hieu Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1223" to="1231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Knowledge vault: a web-scale approach to probabilistic knowledge fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geremy</forename><surname>Heitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilko</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohua</forename><surname>Omas Strohmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ACM</publisher>
			<biblScope unit="page" from="601" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">OpenKN: An open knowledge computational engine for network big data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yantao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Social Networks Analysis and Mining (ASONAM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="657" to="664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Locally Adaptive Translation for Knowledge Graph Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yantao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Slow learners are fast</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Zinkevich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2331" to="2339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Modeling Relation Paths for Representation Learning of Knowledge Bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning entity and relation embeddings for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Ninth AAAI Conference on Arti cial Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2181" to="2187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">YAGO3: A Knowledge Base from Multilingual Wikipedias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farzaneh</forename><surname>Mahdisoltani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Biega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Suchanek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Delay-tolerant algorithms for asynchronous distributed online learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">B</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Streeter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="2915" to="2923" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">WordNet: a lexical database for English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">E cient Learning of Entity and Predicate Embeddings for Link Prediction in Knowledge Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudia</forename><surname>Amato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Fanizzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Floriana</forename><surname>Esposito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">URSW@ ISWC</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="26" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hogwild: A lock-free approach to parallelizing stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="693" to="701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">An overview of gradient descent optimization algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yatao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haixun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Parallel Processing of Graphs</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Knowledge Graph Embedding by Translating on Hyperplanes. AAAI -Association for the Advancement of</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Arti cial Intelligence</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep learning with Elastic Averaging SGD</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sixin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Choromanska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<biblScope unit="page" from="685" to="693" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
