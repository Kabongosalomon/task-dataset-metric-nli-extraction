<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">3D human pose estimation in video with temporal convolutions and semi-supervised training</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Pavllo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eth</forename><surname>Zürich</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Brain</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">3D human pose estimation in video with temporal convolutions and semi-supervised training</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T15:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work, we demonstrate that 3D poses in video can be effectively estimated with a fully convolutional model based on dilated temporal convolutions over 2D keypoints. We also introduce back-projection, a simple and effective semi-supervised training method that leverages unlabeled video data. We start with predicted 2D keypoints for unlabeled video, then estimate 3D poses and finally back-project to the input 2D keypoints. In the supervised setting, our fully-convolutional model outperforms the previous best result from the literature by 6 mm mean per-joint position error on Human3.6M, corresponding to an error reduction of 11%, and the model also shows significant improvements on HumanEva-I. Moreover, experiments with back-projection show that it comfortably outperforms previous state-of-the-art results in semisupervised settings where labeled data is scarce. Code and models are available at https://github.com/ facebookresearch/</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Our work focuses on 3D human pose estimation in video. We build on the approach of state-of-the-art methods which formulate the problem as 2D keypoint detection followed by 3D pose estimation <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b56">56,</ref><ref type="bibr" target="#b32">33]</ref>. While splitting up the problem arguably reduces the difficulty of the task, it is inherently ambiguous as multiple 3D poses can map to the same 2D keypoints. Previous work tackled this ambiguity by modeling temporal information with recurrent neural networks <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b26">27]</ref>. On the other hand, convolutional networks have been very successful in modeling temporal information in tasks that were traditionally tackled with RNNs, such as neural machine translation <ref type="bibr" target="#b10">[11]</ref>, language modeling <ref type="bibr" target="#b6">[7]</ref>, speech generation <ref type="bibr" target="#b55">[55]</ref>, and speech recognition <ref type="bibr" target="#b5">[6]</ref>. Convolutional models enable parallel processing of multiple frames which is not possible with recurrent networks. * Work done while at Facebook AI Research. <ref type="figure">Figure 1</ref>: Our temporal convolutional model takes 2D keypoint sequences (bottom) as input and generates 3D pose estimates as output (top). We employ dilated temporal convolutions to capture long-term information.</p><p>In this paper, we present a fully convolutional architecture that performs temporal convolutions over 2D keypoints for accurate 3D pose prediction in video (see <ref type="figure">Figure 1</ref>). Our approach is compatible with any 2D keypoint detector and can effectively handle large contexts via dilated convolutions. Compared to approaches relying on RNNs <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b26">27]</ref>, it provides higher accuracy, simplicity, as well as efficiency, both in terms of computational complexity as well as the number of parameters ( §3).</p><p>Equipped with a highly accurate and efficient architecture, we turn to settings where labeled training data is scarce and introduce a new scheme to leverage unlabeled video data for semi-supervised training. Low resource settings are particularly challenging for neural network models which require large amounts of labeled training data and collecting labels for 3D human pose estimation requires an expensive motion capture setup as well as lengthy recording sessions. Our method is inspired by cycle consistency in unsupervised machine translation, where round-trip translation into an intermediate language and back into the original language should be close to the identity function <ref type="bibr" target="#b46">[46,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b8">9]</ref>. Specifically, we predict 2D keypoints for an unlabeled video with an off the shelf 2D keypoint detector, predict 3D poses, and then map these back to 2D space ( §4).</p><p>In summary, this paper provides two main contributions. First, we present a simple and efficient approach for 3D human pose estimation in video based on dilated temporal convolutions on 2D keypoint trajectories. We show that our model is more efficient than RNN-based models at the same level of accuracy, both in terms of computational complexity and the number of model parameters.</p><p>Second, we introduce a semi-supervised approach which exploits unlabeled video, and is effective when labeled data is scarce. Compared to previous semi-supervised approaches, we only require camera intrinsic parameters rather than ground-truth 2D annotations or multi-view imagery with extrinsic camera parameters.</p><p>In comparison to the state of the art our approach outperforms the previously best performing methods in both supervised and semi-supervised settings. Our supervised model performs better than other models even if these exploit extra labeled data for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Before the success of deep learning, most approaches to 3D pose estimation were based on feature engineering and assumptions about skeletons and joint mobility <ref type="bibr" target="#b48">[48,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b17">18]</ref>. The first neural methods with convolutional neural networks (CNN) focused on end-to-end reconstruction <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b53">53,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b40">41]</ref> by directly estimating 3D poses from RGB images without intermediate supervision. Two-step pose estimation. A new family of 3D pose estimators builds on top of 2D pose estimators by first predicting 2D joint positions in image space (keypoints) which are subsequently lifted to 3D <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b15">16]</ref>. These approaches outperform the end-to-end counterparts, since they benefit from intermediate supervision. We follow this approach. Recent work shows that predicting 3D poses is relatively straightforward given ground-truth 2D keypoints, and that the difficulty lies in predicting accurate 2D poses <ref type="bibr" target="#b33">[34]</ref>. Early approaches <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b3">4]</ref> simply perform a knearest neighbour search for a predicted set of 2D keypoints over a large set of 2D keypoints for which the 3D pose is available and then simply output the corresponding 3D pose. Some approaches leverage both image features and 2D ground-truth poses <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b52">52]</ref>. Alternatively, the 3D pose can be predicted from a given set of 2D keypoints by simply predicting their depth <ref type="bibr" target="#b58">[58]</ref>. Some works enforce priors about bone lengths and projection consistency with the 2D ground truth <ref type="bibr" target="#b1">[2]</ref>. Video pose estimation. Most previous work operates in a single-frame setting but recently there have been efforts in exploiting temporal information from video to produce more robust predictions and to be less sensitive to noise. <ref type="bibr" target="#b53">[53]</ref> infer 3D poses from the HoG features (histograms of oriented gradients) of spatio-temporal volumes. LSTMs have been used to refine 3D poses predicted from single images <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b23">24]</ref>. The most successful approaches, however, learn from 2D keypoint trajectories. Our work falls under this category.</p><p>Recently, LSTM sequence-to-sequence learning models have been proposed, which encode a sequence of 2D poses from a video into a fixed-size vector that is then decoded into a sequence of 3D poses <ref type="bibr" target="#b15">[16]</ref>. However, both the input and output sequences have the same length and a deterministic transformation of 2D poses is a much more natural choice. Our experiments with seq2seq models showed that output poses tend to drift over lengthy sequences. <ref type="bibr" target="#b15">[16]</ref> tackles this problem by re-initializing the encoder every 5 frames, at the expense of temporal consistency. There has also been work on RNN approaches which consider priors on body part connectivity <ref type="bibr" target="#b26">[27]</ref>. Semi-supervised training. There has been work on multitask networks <ref type="bibr" target="#b2">[3]</ref> for joint 2D and 3D pose estimation <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b32">33]</ref> as well as action recognition <ref type="bibr" target="#b32">[33]</ref>. Some works transfer the features learned for 2D pose estimation to the 3D task <ref type="bibr" target="#b34">[35]</ref>. Unlabeled multi-view recordings have been used for pre-training representations for 3D pose estimation <ref type="bibr" target="#b45">[45]</ref>, but these recordings are not readily available in unsupervised settings. Generative adversarial networks (GAN) can discriminate realistic poses from unrealistic ones in a second dataset where only 2D annotations are available <ref type="bibr" target="#b56">[56]</ref>, thus providing a useful form of regularization. <ref type="bibr" target="#b54">[54]</ref> use GANs to learn from unpaired 2D/3D datasets and include a 2D projection consistency term. Similarly, <ref type="bibr" target="#b7">[8]</ref> discriminate generated 3D poses after randomly projecting them to 2D. <ref type="bibr" target="#b39">[40]</ref> propose a weakly-supervised approach based on ordinal depth annotations which leverages a 2D pose dataset augmented with depth comparisons, e.g. "the left leg is behind the right leg". 3D shape recovery. While this paper and the discussed related work focus on reconstructing accurate 3D poses, a parallel line of research aims at recovering full 3D shapes of people from images <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b22">23]</ref>. These approaches are typically based on parameterized 3D meshes and give less importance to pose accuracy. Our work. Compared to <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b39">40]</ref>, we do not use heatmaps and instead describe poses with detected keypoint coordinates. This allows the use of efficient 1D convolution over coordinate time series, instead of 2D convolutions over individual heatmaps (or 3D convolutions over heatmap sequences). Our approach also makes computational complexity independent of keypoint spatial resolution. Our models can reach high accuracy with fewer parameters and allow for faster training and inference. Compared to the single-frame baseline proposed by <ref type="bibr" target="#b33">[34]</ref> and the LSTM model by <ref type="bibr" target="#b15">[16]</ref>, we exploit temporal information by performing 1D convolutions over the time dimension, and we propose several optimizations that result in lower reconstruction error. Unlike <ref type="bibr" target="#b15">[16]</ref>, we learn a deterministic mapping <ref type="figure">Figure 2</ref>: An instantiation of our fully-convolutional 3D pose estimation architecture. The input consists of 2D keypoints for a recpetive field of 243 frames (B = 4 blocks) with J = 17 joints. Convolutional layers are in green where 2J, 3d1, 1024 denotes 2 · J input channels, kernels of size 3 with dilation 1, and 1024 output channels. We also show tensor sizes in parentheses for a sample 1-frame prediction, where (243, 34) denotes 243 frames and 34 channels. Due to valid convolutions, we slice the residuals (left and right, symmetrically) to match the shape of subsequent tensors.</p><p>instead of a seq2seq model. Finally, contrary to most of the two-step models mentioned in this section (which use the popular stacked hourglass network <ref type="bibr" target="#b37">[38]</ref> for 2D keypoint detection), we show that Mask R-CNN <ref type="bibr" target="#b11">[12]</ref> and cascaded pyramid network (CPN) <ref type="bibr" target="#b4">[5]</ref> detections are more robust for 3D human pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Temporal dilated convolutional model</head><p>Our model is a fully convolutional architecture with residual connections that takes a sequence of 2D poses as input and transforms them through temporal convolutions. Convolutional models enable parallelization over both the batch and the time dimension while RNNs cannot be parallelized over time. In convolutional models, the path of the gradient between output and input has a fixed length regardless of the sequence length, which mitigates vanishing and exploding gradients which affect RNNs. A convolutional architecture also offers precise control over the temporal receptive field, which we found beneficial to model temporal dependencies for the task of 3D pose estimation. Moreover, we employ dilated convolutions <ref type="bibr" target="#b14">[15]</ref> to model long-term dependencies while at the same time maintaining efficiency. Architectures with dilated convolutions have been successful for audio generation <ref type="bibr" target="#b55">[55]</ref>, semantic segmentation <ref type="bibr" target="#b57">[57]</ref> and machine translation <ref type="bibr" target="#b21">[22]</ref>.</p><p>The input layer takes the concatenated (x, y) coordinates of the J joints for each frame and applies a temporal convolution with kernel size W and C output channels. This is followed by B ResNet-style blocks which are surrounded by a skip-connection <ref type="bibr" target="#b12">[13]</ref>. Each block first performs a 1D convolution with kernel size W and dilation factor D = W B , followed by a convolution with kernel size 1. Convolutions (except the very last layer) are followed by batch normalization <ref type="bibr" target="#b16">[17]</ref>, rectified linear units <ref type="bibr" target="#b36">[37]</ref>, and dropout <ref type="bibr" target="#b49">[49]</ref>. Each block increases the receptive field exponentially by a factor of W , while the number of parameters increases only linearly. The filter hyperparameters, W and D, are set so that the receptive field for any output frame forms a tree that covers all input frames (see §1). Finally, the last layer outputs a prediction of the 3D poses for all frames in the input sequence using both past and future data to exploit temporal information. To evaluate real-time scenarios, we also experiment with causal convolutions, i.e. convolutions that only have access to past frames. Appendix A.1 illustrates dilated convolutions and causal convolutions.</p><p>Convolutional image models typically apply zeropadding to obtain as many outputs as inputs. Early experiments however showed better results when performing only unpadded convolutions while padding the input sequence with replica of the boundary frames to the left and the right (see Appendix A.5, <ref type="figure">Figure 9a</ref> for an illustration). <ref type="figure">Figure 2</ref> shows an instantiation of our architecture for a receptive field size of 243 frames with B = 4 blocks. For convolutional layers, we set W = 3 with C = 1024 output channels and we use a dropout rate p = 0.25.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Semi-supervised approach</head><p>We introduce a semi-supervised training method to improve accuracy in settings where the availability of labeled 3D ground-truth pose data is limited. We leverage unlabeled video in combination with an off the shelf 2D keypoint detector to extend the supervised loss function with a back-projection loss term. We solve an auto-encoding problem on unlabeled data: the encoder (pose estimator) performs 3D pose estimation from 2D joint coordinates and the decoder (projection layer) projects the 3D pose back to 2D joint coordinates. Training penalizes when the 2D joint coordinates from the decoder are far from the original input. <ref type="figure" target="#fig_0">Figure 3</ref> represents our method which combines our supervised component with our unsupervised component which acts as a regularizer. The two objectives are optimized jointly, with the labeled data occupying the first half of a batch, and the unlabeled data occupying the second half. For the labeled data we use the ground truth 3D poses as target and train a supervised loss. The unlabeled data is used to implement an autoencoder loss where the predicted 3D poses are projected back to 2D and then checked for consistency with the input. the global position of the human root joint) and the 3D pose (the position of all joints with respect to the root joint). Without the global position, the subject would always be reprojected at the center of the screen with a fixed scale. We therefore also regress the 3D trajectory of the person, so that the back-projection to 2D can be performed correctly. To this end, we optimize a second network which regresses the global trajectory in camera space. The latter is added to the pose before projecting it back to 2D. The two networks have the same architecture but do not share any weights as we observed that they affect each other negatively when trained in a multi-task fashion. As it becomes increasingly difficult to regress a precise trajectory if the subject is further away from the camera, we optimize a weighted mean per-joint position error (WMPJPE) loss function for the trajectory:</p><formula xml:id="formula_0">E = 1 y z f (x) − y<label>(1)</label></formula><p>that is, we weight each sample using the inverse of the ground-truth depth (y z ) in camera space. Regressing a precise trajectory for far subjects is also unnecessary for our purposes, since the corresponding 2D keypoints tend to concentrate around a small area. Bone length L2 loss. We would like to incentivize the prediction of plausible 3D poses instead of just copying the input. To do so, we found it effective to add a soft constraint to approximately match the mean bone lengths of the subjects in the unlabeled batch to the subjects of the labeled batch ("Bone length L2 loss" in <ref type="figure" target="#fig_0">Figure 3</ref>). This term plays an important role in self-supervision, as we show in §6.2.</p><p>Discussion. Our method only requires the camera intrinsic parameters, which are often available for commercial cameras. <ref type="bibr" target="#b0">1</ref> The approach is not tied to any specific network architecture and can be applied to any 3D pose detector which takes 2D keypoints as inputs. In our experiments we use the architecture described in §3 to map 2D poses to 3D. To project 3D poses to 2D, we use a simple projection layer which considers linear parameters (focal length, principal point) as well as non-linear lens distortion coefficients (tangential and radial). We found the lens distortions of the cameras used in Human3.6M have negligible impact on the pose estimation metric, but we include these terms nonetheless because they always provide a more accurate modeling of the real camera projection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental setup 5.1. Datasets and Evaluation</head><p>We evaluate on two motion capture datasets, Hu-man3.6M <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b18">19]</ref> and HumanEva-I <ref type="bibr" target="#b47">[47]</ref>. Human3.6M contains 3.6 million video frames for 11 subjects, of which seven are annotated with 3D poses. Each subject performs 15 actions that are recorded using four synchronized cameras at 50 Hz. Following previous work <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b56">56,</ref><ref type="bibr" target="#b32">33]</ref>, we adopt a 17-joint skeleton, train on five subjects (S1, S5, S6, S7, S8), and test on two subjects (S9 and S11). We train a single model for all actions.</p><p>HumanEva-I is a much smaller dataset, with three subjects recorded from three camera views at 60 Hz. Following <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b15">16]</ref>, we evaluate on three actions (Walk, Jog, Box) by training a different model for each action (single action -SA). We also report results when training one model for all actions (multi action -MA), as in <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b26">27]</ref>. We adopt a 15-joint skeleton and use the provided train/test split.</p><p>In our experiments, we consider three evaluation protocols: Protocol 1 is the mean per-joint position error (MPJPE) in millimeters which is the mean Euclidean distance between predicted joint positions and ground-truth joint positions and follows <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b53">53,</ref><ref type="bibr" target="#b59">59,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b40">41]</ref>. Protocol 2 reports the error after alignment with the ground truth in translation, rotation, and scale (P-MPJPE) <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b56">56,</ref><ref type="bibr" target="#b15">16]</ref>. Protocol 3 aligns predicted poses with the ground-truth only in scale (N-MPJPE) following <ref type="bibr" target="#b45">[45]</ref> for semi-supervised experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Implementation details for 2D pose estimation</head><p>Most previous work <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b58">58,</ref><ref type="bibr" target="#b52">52]</ref> extracts the subject from ground-truth bounding boxes and then applies the stacked hourglass detector to predict the 2D keypoint locations within the ground-truth bounding box <ref type="bibr" target="#b37">[38]</ref>. Our approach ( §3 and §4) does not depend on any particular 2D keypoint detector. We therefore investigate several 2D detectors that do not rely on ground-truth boxes which enables the use of our setup in the wild. In addition to the stacked hourglass detector, we investigate Mask R-CNN <ref type="bibr" target="#b11">[12]</ref> with a ResNet-101-FPN <ref type="bibr" target="#b30">[31]</ref> backbone, using its reference implementation in Detectron, as well as cascaded pyramid network (CPN) <ref type="bibr" target="#b4">[5]</ref> which represents an extension of FPN. The CPN implementation requires bounding boxes to be provided externally (we use Mask R-CNN boxes for this case).</p><p>For both Mask R-CNN and CPN, we start with pretrained models on COCO <ref type="bibr" target="#b31">[32]</ref> and fine-tune the detectors on 2D projections of Human3.6M, since the keypoints in COCO differ from Human3.6M <ref type="bibr" target="#b19">[20]</ref>. In our ablations, we also experiment with directly applying our 3D pose estimator to pretrained 2D COCO keypoints for estimating the 3D joints of Human3.6M.</p><p>For Mask R-CNN, we adopt a ResNet-101 backbone trained with the "stretched 1x" schedule <ref type="bibr" target="#b11">[12]</ref>. <ref type="bibr" target="#b1">2</ref> When finetuning the model on Human3.6M, we reinitialize the last layer of the keypoint network, as well as the deconv layers that regress the heatmaps to learn a new set of keypoints. We train on 4 GPUs with a step-wise decaying learning rate: 1e-3 for 60k iterations, then 1e-4 for 10k iterations, and 1e-5 for 10k iterations. At inference, we apply a softmax over the the heatmaps and extract the expected value of the resulting 2D distribution (soft-argmax). This results in smoother and more precise predictions than hard-argmax <ref type="bibr" target="#b32">[33]</ref>.</p><p>For CPN, we use a ResNet-50 backbone with a 384×288 resolution. To fine-tune, we re-initialize the final layers of both GlobalNet and RefineNet (convolution weights and batch normalization statistics). Next, we train on one GPU with batches of 32 images and with a step-wise decaying learning rate: 5e-5 (1/10th of the initial value) for 6k iterations, then 5e-6 for 4k iterations, and finally 5e-7 for 2k iterations. We keep batch normalization enabled while finetuning. We train with ground-truth bounding boxes and test using the bounding boxes predicted by the fine-tuned Mask R-CNN model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Implementation details for 3D pose estimation</head><p>For consistency with other work <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b53">53,</ref><ref type="bibr" target="#b59">59,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b40">41]</ref>, we train and evaluate on 3D poses in camera space by rotating and translating the ground-truth poses according to the camera transformation, and not using the global trajectory (except for the semi-supervised setting, §4).</p><p>As optimizer we use Amsgrad <ref type="bibr" target="#b43">[43]</ref> and train for 80 epochs. For Human3.6M, we adopt an exponentially decaying learning rate schedule, starting from η = 0.001 with a shrink factor α = 0.95 applied each epoch.</p><p>All temporal models, i.e. models with receptive fields larger than one, are sensitive to the correlation of samples in pose sequences (cf. §3). This results in biased statistics for batch normalization which assumes independent samples <ref type="bibr" target="#b16">[17]</ref>. In preliminary experiments, we found that predicting a large number of adjacent frames during training yields results that are worse than a model exploiting no temporal information (which has well-randomized samples in the batch). We reduce correlation in the training samples by choosing training clips from different video segments. The clip set size is set to the width of the receptive field of our architecture so that the model predicts a single 3D pose per training clip. This is important for generalization and we analyze it in detail in Appendix A.5.</p><p>We can greatly optimize this single frame setting by replacing dilated convolutions with strided convolutions where the stride is set to be the dilation factor (see Appendix A.6). This avoids computing states that are never used and we apply this optimization only during training. At inference, we can process entire sequences and reuse intermediate states of other 3D frames for faster inference. This is possible because our model does not use any form of pooling over the time dimension. To avoid losing frames to valid convolutions, we pad by replication, but only at the input boundaries of a sequence (Appendix A.5, <ref type="figure">Figure 9a</ref> shows an illustration).</p><p>We observed that the default hyperparameters of batch normalization lead to large fluctuations of the test error (± 1 mm) as well as to fluctuations in the running estimates for inference. To achieve more stable running statistics, we use a schedule for the batch-normalization momentum β: we start from β = 0.1, and decay it exponentially so that it reaches β = 0.001 in the last epoch.</p><p>Finally, we perform horizontal flip augmentation at train and test time. We show the effect of this in Appendix A.4.</p><p>For HumanEva, we use N = 128, α = 0.996, and train for 1000 epochs using a receptive field of 27 frames. Some frames in HumanEva are corrupted by sensor dropout and we split the corrupted videos into valid contiguous chunks and treat them as independent videos. <ref type="table" target="#tab_1">Table 1</ref> shows results for our convolutional model with B = 4 blocks and a receptive field of 243 input frames for both evaluation protocols ( §5). The model has lower average error than all other approaches under both protocols, and does not rely on additional data such as many other approaches (+). Under protocol 1 <ref type="table" target="#tab_1">(Table 1a)</ref>, our model outperforms the previous best result <ref type="bibr" target="#b26">[27]</ref> by 6 mm on average, corresponding to an 11% error reduction. Notably, <ref type="bibr" target="#b26">[27]</ref> uses ground-truth boxes whereas our model does not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Temporal dilated convolutional model</head><p>The model clearly takes advantage of temporal infor-  mation as the error is about 5 mm higher on average for protocol 1 compared to a single-frame baseline where we set the width of all convolution kernels to W = 1. The gap is larger for highly dynamic actions, such as "Walk" (6.7 mm) and "Walk Together" (8.8 mm). The performance for a model with causal convolutions is about half way between the single frame baseline and our model; causal convolutions enable online processing by predicting the 3D pose for the rightmost input frame. Interestingly, groundtruth bounding boxes result in similar performance to predicted bounding boxes with Mask R-CNN, which suggests that predictions are almost-perfect in our single-subject scenario. <ref type="figure" target="#fig_1">Figure 4</ref> shows examples of predicted poses including the predicted 2D keypoints and we included a video illustration in the supplementary material (Appendix A.7) as well as at https://dariopavllo.github.io/ VideoPose3D. Next, we evaluate the impact of the 2D keypoint de- <ref type="bibr" target="#b2">3</ref> All subsequent results for <ref type="bibr" target="#b15">[16]</ref> in this paper were computed by us using their public implementation. tector on the final result. <ref type="table">Table 3</ref> reports accuracy of our model with ground-truth 2D poses, hourglass-network predictions from <ref type="bibr" target="#b33">[34]</ref> (both pre-trained on MPII and fine-tuned on Human3.6M), Detectron and CPN (both pre-trained on COCO and fine-tuned on Human3.6M). Both Mask R-CNN and CPN give better performance than the stacked hourglass network. The improvement is likely to be due to the higher heatmap resolution, stronger feature combination (feature pyramid network <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b44">44]</ref> for Mask R-CNN and RefineNet for CPN), and the more diverse dataset on which they are pretrained, i.e. COCO <ref type="bibr" target="#b31">[32]</ref>. When trained on 2D groundtruth poses, our model improves the lower bound of [34] by 8.3 mm, and the LSTM-based approach of Lee et al. <ref type="bibr" target="#b26">[27]</ref> by 1.2 mm for protocol 1. Therefore, our improvements are not merely due to a better 2D detector.</p><p>Absolute position errors do not measure the smoothness of predictions over time, which is important for video. To evaluate this, we measure joint velocity errors (MPJVE), corresponding to the MPJPE of the first derivative of the 3D pose sequences. <ref type="table" target="#tab_3">Table 2</ref> shows that our temporal model     Walk Jog Box S1 S2 S3 S1 S2 S3 S1 S2 S3</p><p>Pavlakos et al. <ref type="bibr" target="#b40">[41]</ref>   reduces the MPJVE of the single-frame baseline by 76% on average resulting in vastly smoother poses. <ref type="table" target="#tab_6">Table 4</ref> shows results on HumanEva-I and that our model generalizes to smaller datasets; results are based on pretrained Mask R-CNN 2D detections. Our models outperform the previous state-of-the-art.</p><p>Finally, <ref type="table" target="#tab_8">Table 5</ref> compares the convolutional model to the LSTM model of <ref type="bibr" target="#b15">[16]</ref> in terms of complexity. We report the number of model parameters and an estimate of the floating-  point operations (FLOPs) to predict one frame at inference time (details in Appendix A.2). For the latter, we only consider matrix multiplications and report the amortized cost over a hypothetical sequence of infinite length (to disregard padding). MPJPE results are based on models trained on ground-truth 2D poses without test-time augmentation. Our model achieves a significantly lower error even when the number of computations are halved. Our largest model with receptive field of 243 frames has roughly the same complexity as <ref type="bibr" target="#b15">[16]</ref>, but at 3.8 mm lower error. The table also highlights the effectiveness of dilated convolutions which increase complexity only logarithmically with respect to the receptive field.</p><p>Since our model is convolutional, it can be parallelized both over the number of sequences as well as over the temporal dimension. This contrasts to RNNs, which can only be parallelized over different sequences and are thus much less efficient for small batch sizes. For inference, we measured about 150k FPS on a single NVIDIA GP100 GPU over a single long sequence, i.e., batch size one, assuming that 2D poses were already available. Speed is largely independent of the batch size due to parallel temporal processing.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Semi-supervised approach</head><p>We adopt the setup of <ref type="bibr" target="#b45">[45]</ref> who consider various subsets of the Human3.6M training set as labeled data and the remaining samples are used as unlabeled data. Their setup also generally downsamples all data to 10 FPS (from 50 FPS). Labeled subsets are created by first reducing the number of subjects and then by downsampling Subject 1.</p><p>Since the dataset is downsampled, we use a receptive field of 9 frames, equivalent to 45 frames upsampled. For the very small subsets, 1% and 5% of S1, we use 3 frames, and we use a single-frame model for 0.1% of S1 where only 49 frames are available. We fine-tuned CPN on the labeled data only and warm up training by iterating only over labeled data for a few epochs (1 epoch for ≥ S1, 20 epochs for smaller subsets). <ref type="figure" target="#fig_4">Figure 5a</ref> shows that our semi-supervised approach becomes more effective as the amount of labeled data decreases. For settings with less than 5K labeled frames, our approach achieves improvements of about 9-10.4 mm N-MPJPE over our supervised baseline. Our supervised baseline is much stronger than <ref type="bibr" target="#b45">[45]</ref> and outperforms all of their results by a large margin. Although <ref type="bibr" target="#b45">[45]</ref> uses a single-frame model in all experiments, our findings still hold on 0.1% of S1 (where we also use a single-frame model). <ref type="figure" target="#fig_4">Figure 5b</ref> shows results for our method under the more common Protocol 1 for the non-downsampled version of the dataset <ref type="bibr">(50 FPS)</ref>. This setup is more appropriate for our approach since it allows us to exploit full temporal information in videos. Here we use a receptive field of 27 frames, except in 1% of S1, where we use 9 frames, and 0.1% of S1, where we use one frame. Our semi-supervised approach gains up to 14.7 mm MPJPE over the supervised baseline. <ref type="figure" target="#fig_4">Figure 5c</ref> switches the CPN 2D keypoints for groundtruth 2D poses to measure if we could perform better with a better 2D keypoint detector. In this case, improvements can be up to 22.6 mm MPJPE (1% of S1) which confirms that better 2D detections could improve performance. The same graph shows that the bone length term is crucial for predicting valid poses, since it forces the model to respect kinematic constraints (line "Ours semi-supervised GT abl."). Removing this term drastically decreases the effectiveness of semi-supervised training: for 1% of S1 the error increases from 78.1 mm to 91.3 mm which compares to 100.7 mm for the supervised baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We have introduced a simple fully convolutional model for 3D human pose estimation in video. Our architecture exploits temporal information with dilated convolutions over 2D keypoint trajectories. A second contribution of this work is back-projection, a semi-supervised training method to improve performance when labeled data is scarce. The method works with unlabeled video and only requires intrinsic camera parameters, making it practical in scenarios where motion capture is challenging (e.g. outdoor sports).</p><p>Our fully convolutional architecture improves the previous best result on the popular Human3.6M dataset by 6mm average joint error which corresponds to a relative reduction of 11% and also shows improvements on HumanEva-I. Back-projection can improve 3D pose estimation accuracy by about 10mm N-MPJPE (15mm MPJPE) over a strong baseline when 5K or fewer annotated frames are available.</p><formula xml:id="formula_1">f [D(n − m)] h[m]<label>(3)</label></formula><p>yielding roughly the same computational cost as regular convolutions for the same number of non-zero entries, while increasing the receptive field. For illustration purposes, in <ref type="figure" target="#fig_6">Figure 6</ref> we depict the information flow in our models. We also highlight the difference between symmetric convolutions and causal convolutions.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Computational cost analysis</head><p>In this section we show how we computed the computational complexity of our model and that of <ref type="bibr" target="#b15">[16]</ref>. The common practice is to consider only matrix multiplications, as other operations (e.g. biases, batch normalization, activations) have negligible impact on the final complexity. For <ref type="bibr" target="#b15">[16]</ref>, we evaluated its reference implementation in Ten-sorFlow. We computed the amortized cost to predict one frame using the TensorFlow profiler, and only counted operations corresponding to matrix multiplications. According to TensorFlow's approximation, multiplying a N × M matrix by a M × K matrix has a cost of 2 N M K FLOPs (floating-point operations), which is equivalent to N M K multiply-add operations.</p><p>For our model, we adopted the same convention. We provide a sample cost analysis for a model with a receptive field of 27 frames, which consists of 2 residual blocks. Since the matrix multiplications in our model are only due to convolutions, the analysis is straightforward and can be computed by hand.  <ref type="figure">Figure 7</ref>: Architecture of a model with a receptive field of 27 frames, with the corresponding amortized cost to predict one frame in convolutional layers.</p><p>As can be seen in <ref type="figure">Figure 7</ref>, the model consists of 6 convolutional layers. Disregarding padding (i.e. for sequences of length N 0), performing a 1D convolution with C in input channels, C out output channels, and width W has a cost of 2 N W C in C out FLOPs, i.e. 2 W C in C out FLOPs per frame. In our 27-frame model, the results can be summarized as follows: we stack a varying number of residual blocks, each of which multiplies the receptive field by 3. In the single-frame scenario, we use 2 blocks and set the convolution widths of all layers to 1, obtaining a model functionally equivalent to <ref type="bibr" target="#b33">[34]</ref>. As can be seen, the model does not seem to overfit as the receptive field increases. On the other hand, the error tends to saturate quickly, suggesting that the task of 3D human pose estimation does not require modeling longterm dependencies. Therefore, we generally adopt a receptive field of 243 frames. Similarly, in <ref type="figure" target="#fig_7">Figure 8a</ref> we vary the channel size between 128 and 2048, with the same findings: the model is not prone to overfitting, but the error saturates past a certain point. Since the computational complexity increases quadratically with respect to the channel size, we adopt C = 1024.</p><formula xml:id="formula_2">&amp;KDQQHOVL]H 03-3(PP (a)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Data augmentation and convolution type</head><p>When we remove test-time augmentation, the error increases to 47.7 mm (from 46.8 mm) in our top-performing model. If we also remove train-time augmentation, the error reaches 49.2 mm (another +1.5 mm).</p><p>Next, we replace dilated convolutions with regular dense convolutions. In a model with a receptive field of 27 frames and fine-tuned CPN detections, the error increases from 48.8 mm to 50.4 mm (+1.6 mm), while also increasing the number of parameters and computations by a factor of ≈ 3.5. This highlights that dilated convolutions are crucial for efficiency, and that they counteract overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5. Batching strategy</head><p>We argue that the reconstruction error is strongly dependent on how the model is trained, and we suggested to generate minibatches in a way that only one output frame at a time is predicted. To show why this is important, we introduce a new hyperparameter -the chunk size C (or step size), which specifies how many frames are predicted at once per sample. Predicting only one frame, i.e. C = 1, requires a full receptive field F as input. Predicting two frames (C = 2) requires F + 1 frames, and so on. It is evident that predicting multiple frames is computationally more efficient, as the results of intermediate convolutions can be shared among frames -and in fact, we do this at inference. On the other hand, we show that during training this is detrimental to generalization. <ref type="figure">Figure 9b</ref> illustrates the reconstruction error (as well as the relative speedup in training time) when training a 27frame model with different step sizes, namely 1, 2, 4, 8, 16, and 32 frames. Since predicting multiple frames is equivalent to increasing the batch size -thus hurting generalization [25] -we make the results comparable by adjusting the batch size so that the model always predicts 1024 frames. Therefore, the 1-frame experiment adopts a batch size of 1024 sequences, which becomes 512 for 2 frames, 256 for 4 frames, and so on. This methodology also ensures the models will be trained with the same number of weight updates.</p><p>The results show that the error decreases in conjunction with the step size, at the expense of training speed. The impaired performance of the models trained with high step size is caused by correlated batch statistics <ref type="bibr" target="#b13">[14]</ref>. Our implementation optimized for single-frame outputs achieves a speed-up factor of ≈ 2, but this gain is even higher on models with larger receptive fields (e.g. ≈ 4 with 81 frames), and enabled us to train the model with 243 frames.</p><p>A.6. Optimized training implementation <ref type="figure">Figure 10</ref> shows why our implementation tailored for single-frame predictions is important. A regular implementation computes intermediate states layer by layer. This is very efficient for long sequences, as the states computed in layer n can be reused by layer n + 1 without recomputing them. However, for short sequences, this approach becomes inefficient because states near boundaries are not used. In the extreme case of single-frame predictions (which we use for training), many intermediate computations are wasted,  <ref type="figure">Figure 9</ref>: Top: batch creation process for training. This example shows a video of 7 frames which is used to train a model with a receptive field of 5 frames. We generate a training example for each of the 7 frames, such that only the center frame is predicted. The video is padded by replicating boundary frames. Bottom: reconstruction error and training speed-up with different step sizes. The speed-up is relative to C = 1. The 1f variant shows the speed up corresponding to the implementation optimized for single-frame predictions.</p><p>as can be seen in <ref type="figure">Figure 10a</ref>. In this case, we replace dilated convolutions with strided convolutions, making sure to obtain a model which is functionally equivalent to the original one (e.g. by also adapting skip-connections). This strategy ensures that no intermediate states will be discarded.</p><p>As mentioned, at inference we use the regular layer-bylayer implementation since it is more efficient for multiframe predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7. Demo videos</head><p>The supplementary material contains several videos highlighting the smoothness of the predictions of our temporal convolutional model compared to the single frame baseline. Specifically, we show side by side the original video sequence, poses predicted by the single-frame baseline, poses from the temporal convolutional model as well as the ground-truth poses. Some demo videos can also be found at https://dariopavllo.github. io/VideoPose3D. (b) Implementation optimized for single-frame predictions. <ref type="figure">Figure 10</ref>: Comparison between two implementations for a single-frame prediction, receptive field of 27 frames. In the layer-by-layer implementation many intermediate states are wasted, whereas the optimized implementation computes only required states. As the length of the sequence increases, the layer-by-layer implementation becomes more efficient.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Trajectory model. Due to the perspective projection, the 2D pose on the screen depends both on the trajectory (i.e. Semi-supervised training with a 3D pose model that takes a sequence of possibly predicted 2D poses as input. We regress the 3D trajectory of the person and add a soft-constraint to match the mean bone lengths of the unlabeled predictions to the labeled ones. Everything is trained jointly. WMPJPE stands for "Weighted MPJPE".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Qualitative results for two videos. Top: video frames with 2D pose overlay. Bottom: 3D reconstruction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>3 :</head><label>3</label><figDesc>Effect of the 2D detector on the final result, under Protocol 1 (P1) and Protocol 2 (P2) Legend: groundtruth (GT), stacked hourglass (SH), Detectron (D), cascaded pyramid network (CPN), pre-trained (PT), fine-tuned (FT).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Full framerate under Protocol 1 with ground-truth 2D poses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Top: comparison with<ref type="bibr" target="#b45">[45]</ref> on Protocol 3, using a downsampled version of the dataset for consistency. Middle: our method under Protocol 1 (full frame rate). Bottom: our method under Protocol 1 when trained on ground-truth 2D poses (full frame rate). The small crosses ("abl." series) denote the ablation of the bone length term.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Information flow in our models, from input (bottom layer) to output (top layer). Dashed lines represent skip-connections.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Top: Error as a function of the channel size, with a fixed receptive field of 27 frames. Bottom: Error as a function of the receptive field, with a fixed channel size of 1024. Fine-tuned CPN detections for both experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>(a) Layer-by-layer implementation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Pavlakos et al.<ref type="bibr" target="#b40">[41]</ref> CVPR'17 ( * ) 67.4 71.9 66.7 69.1 72.0 77.0 65.0 68.3 83.7 96.5 71.7 65.8 74.9 59.1 63.2 71.9 Tekin et al. [52] ICCV'17 54.2 61.4 60.2 61.2 79.4 78.3 63.1 81.6 70.1 107.3 69.3 70.3 74.3 51.8 63.2 69.7 Martinez et al. [34] ICCV'17 ( * ) 51.8 56.2 58.1 59.0 69.5 78.4 55.2 58.1 74.0 94.6 62.3 59.1 65.1 49.5 52.4 62.9 Sun et al. [50] ICCV'17 (+) 52.8 54.8 54.2 54.3 61.8 67.2 53.1 53.6 71.7 86.7 61.5 53.4 61.6 47.1 53.4 59.1 Fang et al. [10] AAAI'18 50.1 54.3 57.0 57.1 66.6 73.3 53.4 55.7 72.</figDesc><table><row><cell></cell><cell>8 88.6</cell><cell>60.3 57.7</cell><cell>62.7 47.5</cell><cell>50.6 60.4</cell></row><row><cell>Pavlakos et al. [40] CVPR'18 (+)</cell><cell>48.5 54.4 54.4 52.0 59.4 65.3 49.9 52.9 65.8 71.1</cell><cell>56.6 52.9</cell><cell>60.9 44.7</cell><cell>47.8 56.2</cell></row><row><cell>Yang et al. [56] CVPR'18 (+)</cell><cell>51.5 58.9 50.4 57.0 62.1 65.4 49.8 52.7 69.2 85.2</cell><cell>57.4 58.4</cell><cell>43.6 60.1</cell><cell>47.7 58.6</cell></row><row><cell cols="2">Luvizon et al. [33] CVPR'18 ( * )(+) 49.2 51.6 47.6 50.5 51.8 60.3 48.5 51.7 61.5 70.9</cell><cell>53.7 48.9</cell><cell>57.9 44.4</cell><cell>48.9 53.2</cell></row><row><cell cols="2">Hossain &amp; Little [16] ECCV'18 ( †)( * ) 48.4 50.7 57.2 55.2 63.1 72.6 53.0 51.7 66.1 80.9</cell><cell>59.0 57.3</cell><cell>62.4 46.6</cell><cell>49.6 58.3</cell></row><row><cell>Lee et al. [27] ECCV'18 ( †)( * )</cell><cell>40.2 49.2 47.8 52.6 50.1 75.0 50.2 43.0 55.8 73.9</cell><cell>54.1 55.6</cell><cell>58.2 43.3</cell><cell>43.3 52.8</cell></row><row><cell>Ours, single-frame</cell><cell>47.1 50.6 49.0 51.8 53.6 61.4 49.4 47.4 59.3 67.4</cell><cell>52.4 49.5</cell><cell>55.3 39.5</cell><cell>42.7 51.8</cell></row><row><cell>Ours, 243 frames, causal conv. ( †)</cell><cell>45.9 48.5 44.3 47.8 51.9 57.8 46.2 45.6 59.9 68.5</cell><cell>50.6 46.4</cell><cell>51.0 34.5</cell><cell>35.4 49.0</cell></row><row><cell>Ours, 243 frames, full conv. ( †)</cell><cell>45.2 46.7 43.3 45.6 48.1 55.1 44.6 44.3 57.3 65.8</cell><cell>47.1 44.0</cell><cell>49.0 32.8</cell><cell>33.9 46.8</cell></row><row><cell>Ours, 243 frames, full conv. ( †)( * )</cell><cell>45.1 47.4 42.0 46.0 49.1 56.7 44.5 44.4 57.2 66.1</cell><cell>47.5 44.8</cell><cell>49.2 32.6</cell><cell>34.0 47.1</cell></row><row><cell></cell><cell>(a) Protocol 1: reconstruction error (MPJPE).</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">Dir. Disc. Eat Greet Phone Photo Pose Purch. Sit SitD. Smoke Wait WalkD. Walk WalkT. Avg</cell></row><row><cell>Martinez et al. [34] ICCV'17 ( * )</cell><cell>39.5 43.2 46.4 47.0 51.0 56.0 41.4 40.6 56.5 69.4</cell><cell>49.2 45.0</cell><cell>49.5 38.0</cell><cell>43.1 47.7</cell></row><row><cell>Sun et al. [50] ICCV'17 (+)</cell><cell>42.1 44.3 45.0 45.4 51.5 53.0 43.2 41.3 59.3 73.3</cell><cell>51.0 44.0</cell><cell>48.0 38.3</cell><cell>44.8 48.3</cell></row><row><cell>Fang et al. [10] AAAI'18</cell><cell>38.2 41.7 43.7 44.9 48.5 55.3 40.2 38.2 54.5 64.4</cell><cell>47.2 44.3</cell><cell>47.3 36.7</cell><cell>41.7 45.7</cell></row><row><cell>Pavlakos et al. [40] CVPR'18 (+)</cell><cell>34.7 39.8 41.8 38.6 42.5 47.5 38.0 36.6 50.7 56.8</cell><cell>42.6 39.6</cell><cell>43.9 32.1</cell><cell>36.5 41.8</cell></row><row><cell>Yang et al. [56] CVPR'18 (+)</cell><cell>26.9 30.9 36.3 39.9 43.9 47.4 28.8 29.4 36.9 58.4</cell><cell>41.5 30.5</cell><cell>29.5 42.5</cell><cell>32.2 37.7</cell></row><row><cell cols="2">Hossain &amp; Little [16] ECCV'18 ( †)( * ) 35.7 39.3 44.6 43.0 47.2 54.0 38.3 37.5 51.6 61.3</cell><cell>46.5 41.4</cell><cell>47.3 34.2</cell><cell>39.4 44.1</cell></row><row><cell>Ours, single-frame</cell><cell>36.0 38.7 38.0 41.7 40.1 45.9 37.1 35.4 46.8 53.4</cell><cell>41.4 36.9</cell><cell>43.1 30.3</cell><cell>34.8 40.0</cell></row><row><cell>Ours, 243 frames, causal conv. ( †)</cell><cell>35.1 37.7 36.1 38.8 38.5 44.7 35.4 34.7 46.7 53.9</cell><cell>39.6 35.4</cell><cell>39.4 27.3</cell><cell>28.6 38.1</cell></row><row><cell>Ours, 243 frames, full conv. ( †)</cell><cell>34.1 36.1 34.4 37.2 36.4 42.2 34.4 33.6 45.0 52.5</cell><cell>37.4 33.8</cell><cell>37.8 25.6</cell><cell>27.3 36.5</cell></row><row><cell>Ours, 243 frames, full conv. ( †)( * )</cell><cell>34.2 36.8 33.9 37.5 37.1 43.2 34.4 33.5 45.3 52.7</cell><cell>37.7 34.1</cell><cell>38.0 25.8</cell><cell>27.7 36.8</cell></row></table><note>(b) Protocol 2: reconstruction error after rigid alignment with the ground truth (P-MPJPE), where available.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Reconstruction error on Human3.6M. Legend: ( †) uses temporal information. ( * ) ground-truth bounding boxes.(+) extra data -[50, 40, 56, 33] use 2D annotations from the MPII dataset, [40] uses additional data from the Leeds Sports Pose (LSP) dataset as well as ordinal annotations. [50, 33] evaluate every 64th frame. [16] provided us with corrected results over the originally published results 3 . Lower is better, best in bold, second best underlined.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Dir. Disc. Eat Greet Phone Photo Pose Purch. Sit SitD. Smoke Wait WalkD. Walk WalkT. Avg</figDesc><table><row><cell cols="6">Single-frame 12.8 12.6 10.3 14.2 10.2 11.3 11.8 11.3 8.2 10.2</cell><cell>10.3 11.3</cell><cell>13.1 13.4</cell><cell>12.9 11.6</cell></row><row><cell>Temporal</cell><cell>3.0 3.1 2.2</cell><cell>3.4</cell><cell>2.3</cell><cell>2.7 2.7</cell><cell>3.1 2.1 2.9</cell><cell>2.3 2.4</cell><cell>3.7 3.1</cell><cell>2.8 2.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Velocity error over the 3D poses generated by a convolutional model that considers time and a single-frame baseline. Martinez et al. [34] (SH PT) 67.5 52.5 Ours (SH PT from [34]) 58.6 45.0 Martinez et al. [34] (SH FT) 62.9 47.7 Ours (SH FT from [34]) 53.4 40.1</figDesc><table><row><cell>Method</cell><cell cols="2">P1 P2 Method</cell><cell>P1 P2</cell></row><row><cell>Martinez et al. [34] (GT)</cell><cell cols="2">45.5 37.1 Ours (GT)</cell><cell>37.2 27.2</cell></row><row><cell cols="3">Hossain &amp; Little [16] (GT) 41.6 31.7 Ours (D PT)</cell><cell>54.8 42.0</cell></row><row><cell>Lee et al. [27] (GT)</cell><cell>38.4</cell><cell>-Ours (D FT)</cell><cell>51.6 40.3</cell></row><row><cell>Ours (CPN PT)</cell><cell cols="2">52.1 40.1 Ours (CPN FT)</cell><cell>46.8 36.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Error on HumanEva-I under Protocol 2 for singleaction (SA) and multi-action (MA) models. Best in bold, second best underlined. (+) uses extra data. The high error on "Walk" of S3 is due to corrupted mocap data.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table /><note>Computational complexity of various models un- der Protocol 1 trained on ground-truth 2D poses. Results are without test-time augmentation.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Even low-end devices typically embed this information in the EXIF metadata of images or videos.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/facebookresearch/Detectron/ blob/master/configs/12_2017_baselines/e2e_ keypoint_rcnn_R-101-FPN_s1x.yaml</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Supplementary material</head><p>A. <ref type="bibr" target="#b0">1</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. Dilated convolutions and information flow</head><p>Dilated convolutions are a particular form of convolution with a sparse structure, whose kernel points are spaced uniformly and filled with zeros in between. For instance, the discrete filter h = [ 1 2 3 ] (where 2 is the center) becomes [ 1 0 2 0 3 ] with dilation factor D = 2, and [ 1 0 0 2 0 0 3 ] with D = 3. This particular structure enables optimized implementations that skip computations over zero points.</p><p>Consider a discrete convolution of two signals f (of length N ) and h (zero-centered, of length 2M + 1), which can be computed as</p><p>Instead of spacing the kernel explicitly and applying a regular convolution, a dilated convolution can be computed as</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Keep it SMPL: Automatic estimation of 3d human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="561" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">3d human pose estimation via deep learning from 2d annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="582" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Multitask learning. Machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="41" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">3D human pose estimation = 2D pose estimation + matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5759" to="5767" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Wav2letter: an end-to-end convnet-based speech recognition system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03193</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Language modeling with gated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Can 3d pose be learned from 2d projections alone</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Drover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename><surname>Huynh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision Workshops (ECCVW)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="78" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Understanding back-translation at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning pose grammar to encode human body configuration for 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mask R-Cnn</surname></persName>
		</author>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Norm matters: efficient and accurate normalization schemes in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Banner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Golan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Soudry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01814</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A real-time algorithm for signal analysis with the help of the wavelet transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Holschneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kronland-Martinet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Morlet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tchamitchian</surname></persName>
		</author>
		<idno>1:286, 01 1989. 3</idno>
	</analytic>
	<monogr>
		<title level="m">Wavelets, Time-Frequency Methods and Phase Space</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Exploiting temporal information for 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R I</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Iterated second-order label sensitive pooling for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1661" to="1668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Latent structured models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hu-man3.6m: Large scale datasets and predictive methods for 3D human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transaction on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">3d human pose reconstruction using millions of exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1674" to="1677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno>abs/1610.10099</idno>
		<title level="m">Neural machine translation in linear time. arXiv</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Endto-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7122" to="7131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning latent representations of 3d human pose with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Katircioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">On large-batch training for deep learning: Generalization gap and sharp minima</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mudigere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Smelyanskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">T P</forename><surname>Tang</surname></persName>
		</author>
		<idno>2017. 12</idno>
	</analytic>
	<monogr>
		<title level="j">In International Conference on Learning Representations</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unsupervised machine translation using monolingual corpora only</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Propagating LSTM: 3d pose estimation based on joint interdependency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">3d human pose estimation from monocular images with deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision (ACCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="332" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Maximum-margin structured learning with deep networks for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Recurrent 3d pose sequence machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="810" to="819" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">2d/3d pose estimation and action recognition using multitask deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Luvizon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tabia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="506" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Vnect: Real-time 3d human pose estimation with a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">44</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">3d human pose estimation using convolutional neural networks with 2d pose information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="156" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Ordinal depth supervision for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric prediction for single-image 3d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Reconstructing 3d human pose from 2d image landmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="page" from="573" to="586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">On the convergence of Adam and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Unsupervised geometry-aware representation for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">HumanEva: Synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">3d human motion analysis in monocular video: techniques and challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Motion</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="185" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Compositional human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="2621" to="2630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Structured prediction of 3d human pose with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Katircioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning to fuse 2d and 3d image cues for monocular body pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Marquez Neila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Direct prediction of 3d body poses from motion compensated sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rozantsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Adversarial inverse graphics networks: Learning 2d-to-3d lifting and image-to-image translation from unpaired supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><forename type="middle">F</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Seto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4364" to="4372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03499</idno>
		<title level="m">Wavenet: A generative model for raw audio</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">3d human pose estimation in the wild by adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Towards 3d human pose estimation in the wild: a weakly-supervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Sparseness meets deepness: 3d human pose estimation from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Ablation of receptive field and channel size In Figure 8b we report the test error for different receptive fields, namely 1, 9</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<imprint>
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
	<note>and 243 frames. To this end</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
