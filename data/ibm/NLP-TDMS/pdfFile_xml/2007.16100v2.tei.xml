<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Searching Efficient 3D Architectures with Sparse Point-Voxel Convolution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haotian</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyu</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">IIIS</orgName>
								<orgName type="institution" key="instit2">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanrui</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Searching Efficient 3D Architectures with Sparse Point-Voxel Convolution</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Self-driving cars need to understand 3D scenes efficiently and accurately in order to drive safely. Given the limited hardware resources, existing 3D perception models are not able to recognize small instances (e.g., pedestrians, cyclists) very well due to the low-resolution voxelization and aggressive downsampling. To this end, we propose Sparse Point-Voxel Convolution (SPVConv), a lightweight 3D module that equips the vanilla Sparse Convolution with the high-resolution point-based branch. With negligible overhead, this point-based branch is able to preserve the fine details even from large outdoor scenes. To explore the spectrum of efficient 3D models, we first define a flexible architecture design space based on SPVConv, and we then present 3D Neural Architecture Search (3D-NAS) to search the optimal network architecture over this diverse design space efficiently and effectively. Experimental results validate that the resulting SPVNAS model is fast and accurate: it outperforms the state-of-the-art MinkowskiNet by 3.3%, ranking 1 st on the competitive SemanticKITTI leaderboard upon publication. It also achieves 8× computation reduction and 3× measured speedup over MinkowskiNet still with higher accuracy. Finally, we transfer our method to 3D object detection, and it achieves consistent improvements over the one-stage detection baseline on KITTI. * indicates equal contributions; order determined by a coin toss.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>3D deep learning has received increased attention thanks to its wide applications: e.g., it has been used in LiDAR perception that serves as the eyes of autonomous driving systems to understand the semantics of outdoor scenes. As the safety of the passenger is the top priority of the self-driving cars, 3D perception models are required to achieve high accuracy and low latency at the same time. However, the hardware resources on the self-driving cars are tightly constrained by the form factor (since we do not want a whole trunk of workstations) and heat dissipation. Thus, it is crucial to design efficient and effective 3D neural network models with limited computation resources (e.g., memory).</p><p>Researchers have primarily exploited two 3D data representations: point cloud and rasterized voxel grids. As analyzed in Liu et al . <ref type="bibr" target="#b31">[32]</ref>, point-based methods <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b27">28]</ref> waste up to 90% of their runtime on structuring the irregular data, not on the actual feature extraction. On the other hand, voxel-based methods usually suffer from severe information loss: i.e., the resolution of dense voxels <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b31">32]</ref> is strictly constrained by the memory; the sparse voxels <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b8">9]</ref> require aggressive downsampling to achieve larger receptive field, leading to low resolution at deeper layers. With low resolution (see <ref type="figure">Figure 1</ref>), multiple points or even multiple small objects might be merged into one grid and become indistinguishable. In this case, small instances (e.g., pedestrians and cyclists) are at a disadvantage compared with large objects (e.g., cars). Therefore, the effectiveness of previous 3D modules is discounted when hardware resources are limited and the resolution is low.</p><p>To tackle these problems, we propose a novel 3D module, Sparse Point-Voxel Convolution (SPVConv) that introduces a low-cost high-resolution point-based branch to the vanilla Sparse Convolution, which helps to capture the fine details. On top of SPVConv, we further present 3D Neural Architecture Search (3D-NAS) to search an efficient 3D model. We incorporate fine-grained channel numbers into the search space to increase the diversity and introduce the progressive depth shrinking to accelerate the training. Experimental results validate that our model is fast and accurate: it outperforms MinkowskiNet by 3.3% in mIoU with lower latency. It also achieves 8× computation reduction and 3× measured speedup over MinkowskiNet, while providing higher accuracy. We further transfer our method to KITTI for 3D object detection, and it achieves consistent improvements over the previous one-stage detection baseline.</p><p>The contribution of this paper has three aspects:</p><p>1. We design a lightweight 3D module, SPVConv, that boosts the performance on small objects, which used to be challenging under limited hardware resource. 2. We introduce the first AutoML framework for 3D scene understanding, 3D-NAS, that offers the best 3D model given a specific resource constraint. 3. Our method outperforms all previous methods with a large margin and ranks 1 st on the competitive SemanticKITTI leaderboard upon publication. It can also be transferred to object detection and achieves consistent improvements.</p><p>http://semantic-kitti.org/tasks.html#semseg 2 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">3D Perception Models</head><p>Increased attention has been paid to 3D deep learning, which is important for LiDAR perception in autonomous driving. Early research <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b76">77]</ref> relied on the volumetric representation and vanilla 3D convolution to process the 3D data. Due to the sparse nature of 3D representation, the dense volumetric representation is inherently inefficient, and it also inevitably introduces information loss. Therefore, researchers have proposed to directly learn on the 3D point cloud representation using the symmetric function <ref type="bibr" target="#b39">[40]</ref>. To improve the neighborhood modeling capability, researchers have defined point-based convolutions on either geometric <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b68">69]</ref> or semantic <ref type="bibr" target="#b62">[63]</ref> neighborhood. There are also 3D models tailored for specific tasks such as detection <ref type="bibr">[38, 39, 41, 47-49, 70, 73]</ref> and instance segmentation <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b70">71]</ref> built upon these modules.</p><p>Recently, a few researchers started to pay attention to the efficiency aspect of 3D deep learning. Riegler et al . <ref type="bibr" target="#b44">[45]</ref>, Wang et al . <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b60">61]</ref> and Lei et al . <ref type="bibr" target="#b24">[25]</ref> proposed to reduce the memory footprint of the volumetric representation using octrees where areas with lower density occupy fewer voxel grids. Liu et al . <ref type="bibr" target="#b31">[32]</ref> analyzed the bottlenecks of point-based and voxel-based methods, and proposed Point-Voxel Convolution. Graham et al . <ref type="bibr" target="#b13">[14]</ref> and Choy et al . <ref type="bibr" target="#b8">[9]</ref> proposed Sparse Convolution to accelerate the volumetric convolution by keeping the activation sparse and skipping the computations in the inactive regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Neural Architecture Search</head><p>To alleviate the burden of manually designing neural networks <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b19">20]</ref>, researchers have introduced neural architecture search (NAS) to automatically architect the neural network with high accuracy using reinforcement learning <ref type="bibr" target="#b78">[79,</ref><ref type="bibr" target="#b79">80]</ref> and evolutionary search <ref type="bibr" target="#b28">[29]</ref>. A new wave of research started to design efficient models with neural architecture search <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b53">54]</ref> for mobile deployment. However, conventional frameworks require high computation cost and considerable carbon footprint <ref type="bibr" target="#b50">[51]</ref>. To tackle these, researchers have proposed different techniques to reduce the search cost, including differentiable architecture search <ref type="bibr" target="#b29">[30]</ref>, path-level binarization <ref type="bibr" target="#b5">[6]</ref>, single-path one-shot sampling <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b3">4]</ref>, and weight sharing <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b56">57]</ref>. Besides, neural architecture search has also been used in compressing and accelerating neural networks, including pruning <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b26">27]</ref> and quantization <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b61">62]</ref>. Most of these methods are tailored for 2D visual recognition, which has many well-defined search spaces <ref type="bibr" target="#b43">[44]</ref>. Lately, researchers have applied neural architecture search to 3D medical image segmentation <ref type="bibr" target="#b77">[78,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b73">74]</ref> and 3D shape classification <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b25">26]</ref>. However, they are not directly applicable to 3D scene understanding since 3D medical data are still in the similar format as 2D images (which are entirely different from 3D scenes), and 3D objects are of much smaller scales than 3D scenes (which makes them less sensitive to the resolution).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SPVConv: Designing Effective 3D Modules</head><p>We first revisit two recent 3D modules: Point-Voxel Convolution <ref type="bibr" target="#b31">[32]</ref> and Sparse Convolution <ref type="bibr" target="#b8">[9]</ref> and analyze their bottlenecks. We observe that both of them suffer  <ref type="bibr" target="#b31">[32]</ref> is not suitable for large 3D scenes. If processing with sliding windows, the large latency is not affordable for real-time applications. If taking the whole scene, the resolution is too coarse to capture useful information.</p><p>from information loss (caused by coarse voxelization or aggressive downsampling) when the memory is constrained. To this end, we introduce Sparse Point-Voxel Convolution (SPVConv), to effectively process the large 3D scene (as in <ref type="figure" target="#fig_2">Figure 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Point-Voxel Convolution: Coarse Voxelization</head><p>Liu et al . <ref type="bibr" target="#b31">[32]</ref> proposed Point-Voxel Convolution that represents the 3D input data in points to reduce the memory consumption, while performing the convolutions in voxels to reduce the irregular data access and improve the locality. Specifically, its point-based branch transforms each point individually, and its voxel-based branch convolves over the voxelized input from the point-based branch. PVCNN (which is built upon Point-Voxel Convolution) can afford at most 128 3 voxels in its voxel-based branch on a single GPU (with 12 GB of memory). Given a large outdoor scene (with size of 100m×100m×10m), each voxel grid will correspond to a fairly large area (with size of 0.8m×0.8m×0.1m). In this case, the small instances (e.g., pedestrians) will only occupy a few voxel grids (see <ref type="figure">Figure 1</ref>). From such few points, PVCNN can hardly learn any useful information from the voxel-based branch, leading to a relatively low performance (see <ref type="table" target="#tab_0">Table 1</ref>). Alternatively, we can process the large 3D scene piece by piece so that each sliding window is of smaller scale. In order to preserve the fine-grained information (i.e., voxel size is smaller than 0.05m), we have to run PVCNN once for each of the 244 sliding windows. This takes 35 seconds to process a single scene, which is not affordable for most real-time applications (e.g., autonomous driving).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Sparse Convolution: Aggressive Downsampling</head><p>Volumetric convolution has always been considered inefficient and prohibitive to be scaled up. Lately, researchers proposed Sparse Convolution <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b8">9]</ref> that skips the non-activated regions to significantly reduce the memory consumption. More specifically, it first finds all active synapses (denoted as kernel map) between the input and output points; it then performs the convolution based on this kernel map. To keep the activation sparse, it only considers these output points that also belong to the input. We refer the readers to Choy et al . <ref type="bibr" target="#b8">[9]</ref> for more details.</p><p>As such, Sparse Convolution can afford a much higher resolution than the vanilla volumetric convolution. However, the network cannot be very deep due to the limited computation resource. As a result, the network has to downsample very aggressively in order to achieve a sufficiently large receptive field, which is very lossy. For instance, the state-of-the-art MinkowskiNet <ref type="bibr" target="#b8">[9]</ref> gradually applies  four downsampling layers to the input point cloud, after which, the voxel size will become 0.05 × 2 4 = 0.8m. Similar to Point-Voxel Convolution, this resolution is too coarse to capture the small instances (see <ref type="figure">Figure 4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Solution: Sparse Point-Voxel Convolution</head><p>In order to overcome the limitations of both modules, we present Sparse Point-Voxel Convolution (SPVConv) in <ref type="figure" target="#fig_2">Figure 2</ref>: the point-based branch always keeps the high-resolution representation, and the sparse voxel-based branch applies Sparse Convolution to model across different receptive field size. Two branches communicate at a negligible cost through sparse voxelization and devoxelization.</p><p>Data Representation. Our Sparse Point-Voxel Convolution operates on:</p><formula xml:id="formula_0">-sparse voxelized tensor S = ({(p s m , f s m )}, v), where p s m = (x s m , y s m , z s m )</formula><p>is the 3D coordinate and f s m is the feature vector of the m th nonzero grid, and v is the voxel size (i.e., side length) for one grid in the current layer;</p><formula xml:id="formula_1">-point cloud tensor T = {(p t k , f t k )}, where p k = (x k , y k , z k ) is the 3D coordi- nate and f k is feature vector of k th point.</formula><p>Sparse Voxelization. In the upper sparse voxel-based branch, we first transform the high-resolution point cloud tensor T to a sparse tensor S:</p><formula xml:id="formula_2">p t k = (x t k ,ŷ t k ,ẑ t k ) = (floor(x t k /v), floor(y t k /v), floor(z t k /v)),<label>(1)</label></formula><formula xml:id="formula_3">f s m = 1 N m n k=1 I[x t k = x s m ,ŷ t k = y s m ,ẑ t k = z s m ] · f t k ,<label>(2)</label></formula><p>where I[·] is the binary indicator of whetherp t k belongs to the voxel grid p s m , and N m is the normalization factor (i.e., the number of points that fall into the m th nonzero voxel grid). Such formulation, however, requires O(mn) complexity where m = |S| and n = |T |. With typical values of m, n at the order of 10 5 , the naive implementation is impractical for real-time applications.  To this end, we propose to use the GPU hash table to accelerate the sparse voxelization and devoxelization. Specifically, we first construct a hash table for all activated points in the sparse voxelized tensor S, which can be completed in O(n) time. After that, we iterate over all points in T , and for each point, we use its voxelized coordinate as the key to query the corresponding index in the sparse voxelized tensor. As the lookup over the hash table requires O(1) time <ref type="bibr" target="#b36">[37]</ref>, this query step will in total take O(m) time. Therefore, the total time of coordinate indexing will be reduced from O(mn) to O(m + n).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evolutionary Arch. Search</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Super Network Training</head><p>Feature Aggregation. We then perform the neighborhood feature aggregation on the sparse voxelized tensor using a sequence of residual Sparse Convolution blocks <ref type="bibr" target="#b8">[9]</ref>. We parallelize the kernel map operation in Sparse Convolution on GPU with the same hash table implementation as in sparse voxelization, which offers 1.3× speedup over the implementation from Choy et al . <ref type="bibr" target="#b8">[9]</ref>. Note that both our method and the baseline have been upgraded to this accelerated implementation.</p><p>Sparse Devoxelization. With the aggregated features (which are in the form of sparse tensors), we transform them back to the point-based representation so that the information from both branches can be fused together. Similar to Liu et al . <ref type="bibr" target="#b31">[32]</ref>, we choose to interpolate each point's feature with its 8 neighbor voxel grids using trilinear interpolation instead of the naïve nearest interpolation.</p><p>Point Transformation and Feature Fusion. In the lower point-based branch, we directly apply an MLP on each point to extract individual point features. We then fuse the outputs of two branches with an addition to combine the complementary information provided. Compared with the vanilla Sparse Convolution, MLP layers only cost little computation overhead (4% in terms of #MACs) but introduce important fine details into the information flow (see <ref type="figure">Figure 5</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">3D-NAS: Searching Efficient 3D Architectures</head><p>Even with our module, designing an efficient neural network is still challenging. We need to carefully adjust the network architecture (e.g., channel numbers and kernel sizes of all layers) to meet the constraints for real-world applications (e.g., latency, energy, and accuracy). To this end, we introduce 3D Neural Architecture Search (3D-NAS), to automatically design efficient 3D models (as in <ref type="figure" target="#fig_3">Figure 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Design Space</head><p>The performance of neural architecture search is greatly impacted by the design space quality. In our search space, we incorporate fine-grained channel numbers and elastic network depths; however, we do not support different kernel sizes.</p><p>Fine-grained Channel Numbers. The computation cost increases quadratically with the number of channels; therefore, the channel number selection has a large influence on the network efficiency. Most existing neural architecture frameworks <ref type="bibr" target="#b5">[6]</ref> only support the coarse-grained channel number selection: e.g., searching the expansion ratio of the ResNet/MobileNet blocks over a few <ref type="bibr" target="#b1">(2)</ref><ref type="bibr" target="#b2">(3)</ref> choices. In this case, only intermediate channel numbers of the blocks can be changed; while the input and output channel numbers will still remain the same. Empirically, we observe that this limits the variety of the search space. To this end, we enlarge the search space by allowing all channel numbers to be selected from a large collection of choices (with size of O(n)). This fine-grained channel number selection largely increase the number of candidates for each block: e.g., from constant (2-3) to O(n 2 ) for a block with two consecutive convolutions.</p><p>Elastic Network Depths. We support different network depth in our design space. For 3D CNNs, reducing the channel numbers alone cannot achieve significant measured speedup, which is drastically different from 2D CNNs. For example, by shrinking all channel numbers in MinkowskiNet <ref type="bibr" target="#b8">[9]</ref> by 4× and 8×, the number of MACs will be reduced to 7.5 G and 1.9 G, respectively. However, although the number of MACs is drastically reduced, their measured latency on the GPU is very similar: 105 ms and 96 ms (measured on a single GTX1080Ti GPU). This suggests that scaling down the number of channels alone cannot offer us with very efficient models, even though the number of MACs is very small. This might be because 3D modules are usually more memory-bounded than 2D modules; the number of MACs decreases quadratically with channel number, while memory decreases linearly. Motivated by this, we choose to incorporate the elastic network depth into our design space so that these layers with very small computation (and large memory cost) can be removed and merged into their neighboring layers.</p><p>Small Kernel Matters. Kernel sizes are usually included into the search space of 2D CNNs. This is because a single convolution with larger kernel size can be more efficient than multiple convolutions with smaller kernel sizes on GPUs. However, it is not the case for the 3D CNNs. From the computation perspective, a single 2D convolution with kernel size of 5 requires only 1.4× more MACs than two 2D convolutions with kernel sizes of 3; while a single 3D convolution with kernel size of 5 requires 2.3× more MACs than two 3D convolutions with kernel sizes of 3 (if applied to dense voxel grids). This larger computation cost makes it less suitable to use large kernel sizes in 3D CNNs. Furthermore, the computation overhead of 3D modules is also related to the kernel sizes. For example, Sparse Convolution <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b8">9]</ref> requires O(k 3 n) time to build the kernel map, where k is the kernel size and n is the number of points, which indicates that its cost grows cubically with respect to the kernel size. Based on these reasons, we decide to keep the kernel size of all convolutions to be 3 and do not allow the kernel size to change in our search space. Even with the small kernel size, we can still achieve a large receptive field by changing the network depth, which can achieve the same effect as changing the kernel size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training Paradigm</head><p>Searching over a fine-grained design space is very challenging as it is impossible to train every sampled candidate network from scratch <ref type="bibr" target="#b52">[53]</ref>. Motivated by Guo et al . <ref type="bibr" target="#b14">[15]</ref>, we incorporate all candidate networks into a single super network, and after training this super network once, each candidate network can be directly extracted with inherited weights. As such, the total training cost can be reduced from O(n) to O(1), where n is the number of candidate networks.</p><p>Uniform Sampling. At each training iteration, we randomly sample a candidate network from the super network: randomly select the channel number for each layer, and then randomly select the network depth (i.e. the number of blocks to be used) for each stage. The total number of candidate networks to be sampled during training is very limited; therefore, we choose to sample different candidate networks on different GPUs and average their gradients at each step so that more candidate networks can be sampled. For 3D, this is more critical because the 3D datasets usually contain fewer training samples than the 2D datasets: e.g. 20K on SemanticKITTI <ref type="bibr" target="#b2">[3]</ref> vs. 1M on ImageNet <ref type="bibr" target="#b10">[11]</ref>.</p><p>Weight Sharing. As the number of candidate networks is enormous, every candidate network will only be optimized for a small fraction of the total schedule. Therefore, uniform sampling alone is not enough to train all candidate networks sufficiently (i.e., achieving the same level of performance as being trained from scratch). To tackle this, we adopt the weight sharing technique so that every candidate network can be optimized at each iteration even if it is not sampled. Specifically, given the input channel number C in and output channel number C out of each convolution layer, we simply index the first C in and C out channels from the weight tensor accordingly to perform the convolution <ref type="bibr" target="#b14">[15]</ref>. For each batch normalization layer, we similarly crop the first c channels from the weight tensor based on the sampled channel number c. Finally, with the sampled depth d for each stage, we choose to keep the first d layers, instead of randomly sampling d of them. This ensures that each layer will always correspond to the same depth index within the stage. Progressive Depth Shrinking. Suppose we have n stages, each of which has m different depth choices from 1 to m. If we sample the depth d k for each stage k randomly, the expected total depth of the network will be E[d] = n k=1 E[d k ] = n × (m + 1)/2, which is much smaller than the maximum depth nm. Furthermore, the probability of the largest candidate network (with the maximum depth) being sampled is extremely small: m −n . Therefore, the largest candidate networks are poorly trained due to the small possibility of being sampled. To this end, we introduce progressively depth shrinking to alleviate this issue. We divide the training epochs into m segments for m different depth choices. During the k th training segment, we only allow the depth of each stage to be selected from m − k + 1 to m. This is essentially designed to enlarge the search space gradually so that these large candidate networks can be sampled more frequently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Search Algorithm</head><p>After the super network is fully trained, we use evolutionary architecture search to find the best architectures under a certain resource constraint.</p><p>Resource Constraints. We use the number of MACs as the resource constraint. For 3D CNNs, the number of MACs cannot be simply determined by the input size and the network architecture: e.g., Sparse Convolution only performs the computation over the active synapses; therefore, its computation is also determined by the input sparsity pattern. To address this, we first estimate the average kernel map size over the entire dataset for each convolution layer, and we can then measure the number of MACs based on these statistics.</p><p>Evolutionary Search. We automate the architecture search with the evolutionary algorithm <ref type="bibr" target="#b14">[15]</ref>. We initialize the starting population with n randomly sampled candidate networks. At every iteration, we evaluate all candidate networks in the population and select the k models with the highest mIoUs (i.e., the fittest individuals). The population for the next iteration is then generated with (n/2) mutations and (n/2) crossovers. For each mutation, we randomly select one among the top-k candidates and alter each of its architectural parameters (e.g., channel numbers, network depths) with a pre-defined probability; for each crossover, we select two from the top-k candidates and generate a new model by fusing them together randomly. Finally, the best model is obtained from the population of the last iteration. During the evolutionary search, we ensure that all the candidate networks in the population always meet the given resource constraint (we will resample another candidate network until the resource constraint is satisfied).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Based on our lightweight 3D module, we first manually construct our backbone network (denoted as SPVCNN). Then, we leverage our neural architecture search framework to explore the best 3D model (denoted as SPVNAS). We provide more implementation details in the appendix. Evaluated on 3D semantic segmentation and 3D object detection, our proposed method consistently outperforms previous state-of-the-art models with lower computation cost and measured latency (on a single GTX1080Ti GPU).  <ref type="table">Table 3</ref>. Results of outdoor scene segmentation on SemanticKITTI. SPVNAS outperforms the 2D projection-based methods with at least 3.6× model size reduction and 7.1× computation reduction. Here, red numbers correspond to the computation time, and blue numbers correspond to the projection time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">3D Scene Segmentation</head><p>We first evaluate our method on 3D semantic segmentation and conduct experiments on the large-scale outdoor scene dataset, SemanticKITTI <ref type="bibr" target="#b2">[3]</ref>. This dataset contains 23,201 LiDAR point clouds for training and 20,351 for testing, and it is annotated from all 22 sequences in the KITTI <ref type="bibr" target="#b11">[12]</ref> Odometry benchmark. We train all models on the entire training set and report the mean intersection-overunion (mIoU) on the official test set under the single scan setting. We provide additional experimental results (on both validation and test set) in the appendix.</p><p>Results. As shown in  much smaller model that outperforms MinkowskiNet by 0.6% in mIoU with 8.3× model size reduction, 7.6× computation reduction, and 2.7× measured speedup.</p><p>In <ref type="figure">Figure 4</ref>, we also provide some qualitative comparisons between SPVNAS and MinkowskiNet: our SPVNAS has lower errors especially for small instances. We further compare our SPVNAS with 2D projection-based models in <ref type="table">Table 3</ref>. With the smaller backbone (by removing the decoder layers), SPVNAS outperforms DarkNets <ref type="bibr" target="#b2">[3]</ref> by more than 10% in mIoU with 1.2× measured speedup even though 2D convolutions are much better optimized by modern deep learning libraries. Compared with other 2D methods, SPVNAS achieves at least 8.5× model size reduction and 15.2× computation reduction while being much more accurate. Furthermore, our SPVNAS achieves higher mIoU than KPConv <ref type="bibr" target="#b55">[56]</ref>, which is the previous state-of-the-art point-based model, with 17× model size reduction and 23× computation reduction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">3D Object Detection</head><p>We also evaluate our method on 3D object detection and conduct experiments on the outdoor scene dataset, KITTI <ref type="bibr" target="#b11">[12]</ref>. We follow the generally adopted trainingvalidation split, where 3,712 samples are used for training and 3,769 samples are left for validation. We report the mean average precision (mAP) on the test set with 3D IoU thresholds of 0.7 for car, 0.5 for cyclist and pedestrian. We refer the readers to the appendix for more experimental results on the validation set.</p><p>Results. We compare our method against SECOND <ref type="bibr" target="#b69">[70]</ref>, the state-of-the-art single-stage model for 3D object detection. SECOND consists of a sparse encoder using 3D Sparse Convolutions and a region proposal network that performs 2D convolutions after projecting the encoded features to the bird's-eye view (BEV). We reimplement and retrain SECOND: our implementation already outperforms the results in the original paper <ref type="bibr" target="#b69">[70]</ref>. As for our model, we only replace these 3D Sparse Convolutions in SECOND with our SPVConv while keeping all the other settings the same for fair comparison. As summarized in <ref type="table">Table 4</ref>, our SPVCNN achieves significant improvement in cyclist detection, for which we argue that the high-resolution point-based branch carries more information for small instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Analysis</head><p>Our SPVNAS achieves higher accuracy and better efficiency than the previous state-of-the-art MinkowskiNet. In this section, we provide more detailed analysis to better understand the contributions of SPVConv and 3D-NAS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Sparse Point-Voxel Convolution (SPVConv)</head><p>From <ref type="table">Table 5</ref>, our SPVNAS has a very large advantage (up to 25%) on relatively small objects such as pedestrians and cyclists. To explain this, we train SPVCNN on SemanticKITTI with the sequence 08 left out for visualization. In <ref type="figure">Figure 5</ref>, we highlight the points with top 5% feature norm within the point-based branch (in the final SPVConv). Clearly, the point-based branch learns to attend to small instances such as pedestrians, cyclists, trunks and traffic signs, which echos with our superior performance on these classes. <ref type="figure">Fig. 5</ref>. The point-based branch learns to put its attention on small instances (i.e., pedestrians, cyclists, traffic signs). Here, the points in red are the ones with the top 5% largest feature norm in the point-based branch.  Further, we quantitatively analyze the feature norms from both point-based and sparse voxel-based branches. Specifically, we first rank the points from both branches separately based on their feature norms, and then, we mark these points with top 10% feature norm in each branch as activated. From <ref type="figure">Figure 6</ref>, there are significantly more points in the point-based branch being activated for small instances: e.g., more than 80% for bicyclist. This suggests that our observation in <ref type="figure">Figure 5</ref> generally holds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">3D Neural Architecture Search (3D-NAS)</head><p>In <ref type="figure" target="#fig_7">Figure 7</ref>, we present both mIoU vs. #MACs and mIoU vs. latency trade-offs, where we uniformly scale the channel numbers in MinkowskiNet and SPVCNN down as our baselines. It can be observed that a better 3D module (SPVConv) and a well-designed network architecture (3D-NAS) are equally important to the final performance boost. Remarkably, SPVNAS outperforms MinkowskiNet by more than 6% in mIoU at 110 ms latency. Such a large improvement comes from the non-uniform channel scaling and elastic nework depth. In these manually-      designed models (MinkowskiNet and SPVCNN), 77% of the total computation is distributed to the upsampling stage. With 3D-NAS, this ratio is reduced to 47-63%, making the computation more balanced and the downsampling stage (i.e., feature extraction) more emphasized.</p><p>We also compare our evolutionary search with the random architecture search to show that the success of 3D-NAS does not entirely come from the search space. As in <ref type="figure" target="#fig_10">Figure 8a</ref>, random architecture search has poor sample efficiency: the best model at the 20 th generation performs even worse than the best model in the first generation. In contrast, our evolutionary search is capable of progressively finding better architecture, and the final best architecture performs around 3% better than the one in the first generation. We also retrain 20 random models sampled from our search space and compare them with SPVNAS in <ref type="figure" target="#fig_10">Figure 8b</ref>. As a result, our SPVNAS performs 0.8% better compared with the average performance of these random models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We present Sparse Point-Voxel Convolution (SPVConv), a lightweight 3D module specialized for small object recognition. With SPVCNN built upon SPVConv, we solve the problem that Sparse Convolution cannot always keep high-resolution representation and Point-Voxel Convolution does not scale up to large 3D scenes. Furthermore, we introduce 3D-NAS, the first architecture search framework for 3D scene understanding that greatly improves the efficiency and performance of SPVCNN. Extensive experiments on outdoor 3D scene benchmarks demonstrate that the resulting SPVNAS model is lightweight, fast and powerful. We hope that this work will inspire future research on efficient 3D deep learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Implementation Details</head><p>We provide more implementation details on how to build our backbone network (SPVCNN), and train the super network and search for the best model (3D-NAS).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.1 SPVCNN: Backbone Network</head><p>Based on MinkowskiNet <ref type="bibr" target="#b8">[9]</ref>, we build our backbone network by wrapping residual Sparse Convolution blocks with the high-resolution point-based branch. Specifically, the first SPVConv voxelizes before the first layer and devoxelizes after the stemming stage (i.e., before the first downsampling). The second SPVConv voxelizes right after the first SPVConv and devoxelizes after all four downsampling stages. The final two SPVConv's each wraps around two upsampling stages.</p><p>Also, we design a smaller backbone based on PVCNN <ref type="bibr" target="#b31">[32]</ref> by directly replacing each volumetric convolution with one convolution layer (followed by normalization and activation layers) and two residual Sparse Convolution blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.2 3D-NAS: Architecture Search</head><p>We train the super network for 15 epochs that supports the fine-grained channel setting with a starting learning rate 0.24 and cosine learning rate decay. Then, we train for another 15 epochs to incorporate elastic network depth with a starting learning rate 0.096 and cosine learning rate decay. After that, we perform evolutionary architecture search with a population of 50 candidates for 20 generations on the official validation set (sequence 08). Best architecture is directly extracted from the super network and submitted to the test server after finetuning for 10 epochs with a starting learning rate of 0.032 and cosine learning rate decay.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 More Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.1 3D Scene Segmentation</head><p>We present more detailed results of MinkowskiNet <ref type="bibr" target="#b8">[9]</ref>, SPVCNN and SPVNAS on both the official test set and validation set (sequence 08) of SemanticKITTI <ref type="bibr" target="#b2">[3]</ref> in <ref type="table" target="#tab_7">Table 6 and Table 7</ref>. For the results on the validation set in <ref type="table">Table 7</ref>, we run the whole architecture search pipeline again on sequences 00-07 and 09, leaving sequence 10 out as the mini-validation set, and report the results on sequence 08. We observe similar trends on both test and validation results: both a better 3D module (SPVConv) and our 3D Neural Architecture Search (3D-NAS) pipeline improve the performance of MinkowskiNet <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.2 3D Object Detection</head><p>We further provide the results of SPVCNN on the validation set of KITTI <ref type="bibr" target="#b12">[13]</ref> in <ref type="table">Table 8</ref>. We train both SECOND <ref type="bibr" target="#b69">[70]</ref> and our SPVCNN on the training set for three times, and we report the average results to reduce the variance. Similar to the results on the test set, SPVCNN also has consistent improvement in almost all classes on the validation set.  <ref type="table">Table 7</ref>. Results of 3D scene segmentation on the validation set of SemanticKITTI <ref type="bibr" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 More Visualizations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.1 3D Scene Segmentation</head><p>We provide more visualizations for MinkowskiNet <ref type="bibr" target="#b8">[9]</ref> and SPVNAS in <ref type="figure" target="#fig_0">Figure A1</ref> to demonstrate that the improvements brought by SPVConv on small objects and region boundaries are general. For instance, in the first row, MinkowskiNet segments the entire traffic sign and bicycle instances incorrectly; however, our SPVNAS is capable of making almost no mistakes on these very small objects. Also, we observe in next two rows that MinkowskiNet does not perform well on the sidewalk-building or sidewalk-vegetation boundaries where our SPVNAS has a clear advantage.</p><p>In <ref type="figure" target="#fig_2">Figure A2</ref>, we compare our smaller SPVNAS with DarkNet53 <ref type="bibr" target="#b2">[3]</ref>. Dark-Net53 uses spherical projections to project 3D point clouds to a 2D plane such  <ref type="table">Table 8</ref>. Results of 3D object detection on the validation set of KITTI <ref type="bibr" target="#b11">[12]</ref>. that part of the geometry information is lost. Our SPVNAS, in contrast, directly learns on 3D data and is more powerful in the geometric modeling. We observe that SPVNAS has significant advantages in both large regions (last three rows) and smaller instances (first row). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.2 3D Object Detection</head><p>In <ref type="figure" target="#fig_3">Figure A3</ref>, we demonstrate the superior performance of SPVCNN over SEC-OND <ref type="bibr" target="#b69">[70]</ref>, which is a state-of-the-art single-stage 3D detector. Our SPVCNN has a large advantage in scenes with crowded small objects. In the first row of <ref type="figure" target="#fig_3">Figure A3</ref>, our SPVCNN is capable of detecting a challenging small pedestrian instance missed by SECOND, and it also avoids duplicate predictions made by SECOND in the next three rows.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>( a )Fig. 1 .</head><label>a1</label><figDesc>Large 3D Scene (b) Low Resolution (0.8m) Small instances (e.g., pedestrians and cyclists) are hard to be recognized at a low resolution (due to the coarse voxelization or the aggressive downsampling).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Overview of Sparse Point-Voxel Convolution (SPVConv): it equips the sparse voxel-based branch with a lightweight, high-resolution point-based branch which can capture fine details in large scenes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Overview of 3D Neural Architecture Search (3D-NAS): we first train a super network composed of multiple SPVConv's, supporting fine-grained channel numbers and elastic network depths. Then, we perform the evolutionary architecture search to obtain the best candidate model under a given computation constraint.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>( a )Fig. 4 .</head><label>a4</label><figDesc>Error by MinkowskiNet (b) Error by SPVNAS(c) Ground Truth MinkowskiNet has a higher error recognizing small objects and region boundaries, while SPVNAS recognizes small objects better thanks to the high-resolution point-based branch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>1 Fig. 6 .</head><label>16</label><figDesc>Average percentage of activated points in point-based and sparse voxel-based branches for all 19 classes in SemanticKITTI<ref type="bibr" target="#b2">[3]</ref>: the point-based branch attends to smaller objects as the red bars are much higher.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>An efficient 3D module (SPVConv) and a well-designed network architecture (3D-NAS) are equally important to the final performance of SPVNAS: 7.6× computation reduction and 2.7× measured speedup over MinkowskiNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Search curves of ES and RS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Comparison with random models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 8 .</head><label>8</label><figDesc>Evolutionary Search (ES) is more sample-efficient than Random Search (RS).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>( a )</head><label>a</label><figDesc>Error by MinkowskiNet (b) Error by SPVNAS (c) Ground Truth Fig. A1. Qualitative comparisons between MinkowskiNet [9] and SPVNAS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>( a )</head><label>a</label><figDesc>Error by DarkNet (b) Error by SPVNAS (c) Ground Truth Fig. A2. Qualitative comparisons between DarkNet53 [3] and SPVNAS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. A3 .</head><label>A3</label><figDesc>Qualitative comparisons between SECOND [70] and SPVCNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell>Input</cell><cell cols="3">Voxel Size (m) Latency (ms) Mean IoU</cell></row><row><cell>PVConv [32]</cell><cell>Sliding Window Entire Scene</cell><cell>0.05 0.78</cell><cell>35640 146</cell><cell>-39.0</cell></row><row><cell>SPVConv (Ours)</cell><cell>Entire Scene</cell><cell>0.05</cell><cell>85</cell><cell>58.8</cell></row></table><note>Point-Voxel Convolution</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Results of outdoor scene segmentation on SemanticKITTI. SPVNAS outperforms MinkowskiNet with 2.7× measured speedup. Here, red numbers correspond to the computation time, and blue numbers correspond to the post-processing time.</figDesc><table><row><cell></cell><cell>#Params (M)</cell><cell>#MACs (G)</cell><cell>Latency (ms)</cell><cell>mIoU</cell></row><row><cell>PointNet [40]</cell><cell>3.0  *</cell><cell>-</cell><cell>500  *</cell><cell>14.6</cell></row><row><cell>SPGraph [24]</cell><cell>0.3  *</cell><cell>-</cell><cell>5200  *</cell><cell>17.4</cell></row><row><cell>PointNet++ [43]</cell><cell>6.0  *</cell><cell>-</cell><cell>5900  *</cell><cell>20.1</cell></row><row><cell>PVCNN [32]</cell><cell>2.5</cell><cell>42.4</cell><cell>146</cell><cell>39.0</cell></row><row><cell>TangentConv [55]</cell><cell>0.4  *</cell><cell>-</cell><cell>3000  *</cell><cell>40.9</cell></row><row><cell>RandLA-Net [19]</cell><cell>1.2</cell><cell>66.5</cell><cell>880 (256+624)</cell><cell>53.9</cell></row><row><cell>KPConv [56]</cell><cell>18.3</cell><cell>207.3</cell><cell>-</cell><cell>58.8</cell></row><row><cell>MinkowskiNet [9]</cell><cell>21.7</cell><cell>114.0</cell><cell>294</cell><cell>63.1</cell></row><row><cell>SPVNAS (Ours)</cell><cell>2.6 12.5</cell><cell>15.0 73.8</cell><cell>110 259</cell><cell>63.7 66.4</cell></row><row><cell></cell><cell cols="2">[3].</cell><cell></cell><cell></cell></row><row><cell></cell><cell>#Params (M)</cell><cell>#MACs (G)</cell><cell>Latency (ms)</cell><cell>mIoU</cell></row><row><cell>DarkNet21Seg [3]</cell><cell>24.7</cell><cell>212.6</cell><cell>73 (49+24)</cell><cell>47.4</cell></row><row><cell>DarkNet53Seg [3]</cell><cell>50.4</cell><cell>376.3</cell><cell>102 (78+24)</cell><cell>49.9</cell></row><row><cell>SqueezeSegV3-21 [68]</cell><cell>9.4</cell><cell>187.5</cell><cell>97 (73+24)</cell><cell>51.6</cell></row><row><cell>SqueezeSegV3-53 [68]</cell><cell>26.2</cell><cell>515.2</cell><cell>238 (214+24)</cell><cell>55.9</cell></row><row><cell>3D-MiniNet [1]</cell><cell>4.0</cell><cell>-</cell><cell>-</cell><cell>55.8</cell></row><row><cell>PolarNet [76]</cell><cell>13.6</cell><cell>135.0</cell><cell>62</cell><cell>57.2</cell></row><row><cell>SalsaNext [10]</cell><cell>6.7</cell><cell>62.8</cell><cell>71 (47+24)</cell><cell>59.5</cell></row><row><cell>SPVNAS (Ours)</cell><cell>1.1</cell><cell>8.9</cell><cell>89</cell><cell>60.3</cell></row></table><note>* : results directly taken from Behley et al .</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 ,</head><label>2</label><figDesc>SPVNAS outperforms the previous state-of-theart MinkowskiNet [9] by 3.3% in mIoU with 1.7× model size reduction, 1.5× computation reduction and 1.1× measured speedup. Further, we downscale our SPVNAS by setting the resource constraint to 15G MACs. This offers us with a</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .Table 5 .</head><label>45</label><figDesc>Repro.) 87.5 77.9 74.4 76.0 59.7 52.9 49.1 41.7 39.1 SPVCNN (Ours) 87.8 78.4 74.8 80.1 63.7 56.2 49.2 41.4 38.4 Results of outdoor object detection on KITTI. SPVCNN outperforms SEC-OND in most categories especially for the cyclist. Results of per-class performance on SemanticKITTI. SPVNAS has a large advantage on small objects, such as bicyclist and motorcyclist.</figDesc><table><row><cell></cell><cell>Car</cell><cell></cell><cell>Cyclist</cell><cell></cell><cell>Pedestrian</cell></row><row><cell></cell><cell cols="5">Easy Mod. Hard Easy Mod. Hard Easy Mod. Hard</cell></row><row><cell>SECOND [70]</cell><cell cols="5">84.7 76.0 68.7 75.8 60.8 53.7 45.3 35.5 33.1</cell></row><row><cell cols="2">SECOND (Person</cell><cell>Bicycle</cell><cell>Bicyclist</cell><cell>Motorcycle</cell><cell>Motorcyclist</cell></row><row><cell>MinkowskiNet [9]</cell><cell>60.9</cell><cell>40.4</cell><cell>61.9</cell><cell>47.4</cell><cell>18.7</cell></row><row><cell>SPVNAS (Ours)</cell><cell>65.7 (+4.8)</cell><cell>51.6 (+11.2)</cell><cell>65.2 (+3.3)</cell><cell>50.8 (+3.4)</cell><cell>43.7 (+25.0)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 .</head><label>6</label><figDesc>Results of 3D scene segmentation on the test set of SemanticKITTI<ref type="bibr" target="#b2">[3]</ref>.</figDesc><table><row><cell></cell><cell>#Params (M)</cell><cell>#MAdds (G)</cell><cell>Latency (ms)</cell><cell>Mean IoU</cell></row><row><cell>MinkowskiNet [9]</cell><cell>2.2</cell><cell>11.3</cell><cell>115.7</cell><cell>57.5</cell></row><row><cell>SPVCNN (Ours)</cell><cell>2.2</cell><cell>11.9</cell><cell>124.3</cell><cell>58.5</cell></row><row><cell>SPVNAS (Ours)</cell><cell>2.6</cell><cell>15.0</cell><cell>110.4</cell><cell>63.6</cell></row><row><cell>MinkowskiNet [9]</cell><cell>5.5</cell><cell>28.5</cell><cell>152.0</cell><cell>60.0</cell></row><row><cell>SPVCNN (Ours)</cell><cell>5.5</cell><cell>30.0</cell><cell>160.9</cell><cell>61.6</cell></row><row><cell>SPVNAS (Ours)</cell><cell>4.2</cell><cell>20.0</cell><cell>132.6</cell><cell>64.5</cell></row><row><cell>MinkowskiNet [9]</cell><cell>8.8</cell><cell>45.9</cell><cell>207.4</cell><cell>62.8</cell></row><row><cell>SPVCNN (Ours)</cell><cell>8.8</cell><cell>47.4</cell><cell>214.3</cell><cell>64.4</cell></row><row><cell>SPVNAS (Ours)</cell><cell>5.1</cell><cell>24.4</cell><cell>144.3</cell><cell>65.2</cell></row><row><cell>MinkowskiNet [9]</cell><cell>21.7</cell><cell>113.9</cell><cell>294.0</cell><cell>63.1</cell></row><row><cell>SPVCNN (Ours)</cell><cell>21.8</cell><cell>118.6</cell><cell>317.1</cell><cell>63.8</cell></row><row><cell>SPVNAS (Ours)</cell><cell>7.5</cell><cell>34.1</cell><cell>166.1</cell><cell>66.0</cell></row><row><cell>SPVNAS (Ours)</cell><cell>12.5</cell><cell>73.8</cell><cell>259.9</cell><cell>66.4</cell></row><row><cell></cell><cell>#Params (M)</cell><cell>#MAdds (G)</cell><cell>Latency (ms)</cell><cell>Mean IoU</cell></row><row><cell>MinkowskiNet [9]</cell><cell>5.5</cell><cell>28.5</cell><cell>152.0</cell><cell>58.9</cell></row><row><cell>SPVCNN (Ours)</cell><cell>5.5</cell><cell>30.0</cell><cell>160.9</cell><cell>60.7</cell></row><row><cell>SPVNAS (Ours)</cell><cell>4.5</cell><cell>24.6</cell><cell>158.1</cell><cell>62.9</cell></row><row><cell>MinkowskiNet [9]</cell><cell>8.8</cell><cell>45.9</cell><cell>207.4</cell><cell>60.3</cell></row><row><cell>SPVCNN (Ours)</cell><cell>8.8</cell><cell>47.4</cell><cell>214.3</cell><cell>61.4</cell></row><row><cell>SPVNAS (Ours)</cell><cell>7.0</cell><cell>34.7</cell><cell>175.8</cell><cell>63.5</cell></row><row><cell>MinkowskiNet [9]</cell><cell>21.7</cell><cell>113.9</cell><cell>294.0</cell><cell>61.1</cell></row><row><cell>SPVCNN (Ours)</cell><cell>21.8</cell><cell>118.6</cell><cell>317.1</cell><cell>63.8</cell></row><row><cell>SPVNAS (Ours)</cell><cell>10.8</cell><cell>64.5</cell><cell>248.7</cell><cell>64.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Car Cyclist Pedestrian Easy Mod. Hard Easy Mod. Hard Easy Mod. Hard SECOND [70] 89.8 80.9 78.4 82.5 62.8 58.9 68.3 60.8 55.3 SPVCNN (Ours) 90.9 81.8 79.2 85.1 63.8 60.1 68.2 61.6 55.9</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. We thank Nick Stathas and Yue Dong for their feedback on the draft. This work is supported by MIT Quest for Intelligence, MIT-IBM Watson AI Lab, Xilinx and Samsung. We thank AWS Machine Learning Research Awards for providing the computational resource.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Riazuelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Montesano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Murillo</surname></persName>
		</author>
		<title level="m">3D-MiniNet: Learning a 2D Representation from Point Clouds for Fast and Efficient 3D LIDAR Semantic Segmentation. arXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Resource Optimized Neural Architecture Search for 3D Medical Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H</forename><surname>Jung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>MICCAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">SemanticKITTI: A Dataset for Semantic Scene Understanding of LiDAR Sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Behley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garbade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milioto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quenzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Behnke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stachniss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Once for All: Train One Network and Specialize it for Efficient Deployment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">AutoML for Architecting Efficient and Specialized Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>IEEE</publisher>
			<pubPlace>Micro</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<title level="m">ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware</title>
		<imprint>
			<publisher>ICLR</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<title level="m">ShapeNet: An Information-Rich 3D Model Repository. arXiv</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">DetNAS: Backbone Search for Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<title level="m">4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural Networks</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cortinhal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzelepis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">E</forename><surname>Aksoy</surname></persName>
		</author>
		<title level="m">SalsaNext: Fast, Uncertainty-aware Semantic Segmentation of LiDAR Point Clouds for Autonomous Driving. arXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<title level="m">Vision meets Robotics: The KITTI Dataset. IJRR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Are we ready for Autonomous Driving? The KITTI Vision Benchmark Suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">3D Semantic Segmentation With Submanifold Sparse Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Single Path One-Shot Neural Architecture Search with Uniform Sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">OccuSeg: Occupancy-aware 3D Instance Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">AMC: AutoML for Model Compression and Acceleration on Mobile Devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<title level="m">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications. arXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">RandLA-Net: Efficient Semantic Segmentation of Large-Scale Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Trigoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Markham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<title level="m">SqueezeNet: AlexNet-Level Accuracy with 50x Fewer Parameters and &lt; 0.5MB Model Size. arXiv</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">PointGroup: Dual-Set Point Grouping for 3D Instance Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Scalable Neural Architecture Search for 3D Medical Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>MICCAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">3D Instance Segmentation via Multi-Task Metric Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lahoud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Oswald</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Large-Scale Point Cloud Semantic Segmentation With Superpoint Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Octree Guided CNN With Spherical Kernels for 3D Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">SGAS: Sequential Greedy Architecture Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">C</forename><surname>Delgadillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">GAN Compression: Efficient Architectures for Interactive Conditional GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">PointCNN: Convolution on X -Transformed Points</title>
		<imprint>
			<publisher>NeurIPS</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Progressive Neural Architecture Search</title>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<title level="m">DARTS: Differentiable Architecture Search</title>
		<imprint>
			<publisher>ICLR</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">MetaPruning: Meta Learning for Automatic Neural Network Channel Pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Point-Voxel CNN for Efficient 3D Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Auto-ORVNet: Orientation-Boosted Volumetric Neural Architecture Search for 3D Shape Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Interpolated Convolutional Networks for 3D Point Cloud Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">VoxNet: A 3D Convolutional Neural Network for Real-Time Object Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>IROS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Cuckoo Hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pagh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">F</forename><surname>Rodler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Algorithms</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">ImVoteNet: Boosting 3D Object Detection in Point Clouds with Image Votes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Deep Hough Voting for 3D Object Detection in Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Frustum PointNets for 3D Object Detection from RGB-D Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Volumetric and Multi-View CNNs for Object Classification on 3D Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niessner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<title level="m">PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space</title>
		<imprint>
			<publisher>NeurIPS</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">On Network Design Spaces for Visual Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Ulusoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<title level="m">OctNet: Learning Deep 3D Representations at High Resolutions</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">MobileNetV2: Inverted Residuals and Linear Bottlenecks</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">PV-RCNN: Point-Voxel Feature Set Abstraction for 3D Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">PointRCNN: 3D Object Proposal Generation and Detection from Point Cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<title level="m">PV-RCNN: Point-Voxel Feature Set Abstraction for 3D Object Detection. TPAMI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stamoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lymberopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Priyantha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marculescu</surname></persName>
		</author>
		<title level="m">Single-Path NAS: Designing Hardware-Efficient ConvNets in less than 4 Hours. arXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Energy and Policy Considerations for Deep Learning in NLP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">SPLATNet: Sparse Lattice Networks for Point Cloud Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<title level="m">MnasNet: Platform-Aware Neural Architecture Search for Mobile</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Tangent Convolutions for Dense Prediction in 3D</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">Y</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">KPConv: Flexible and Deformable Convolution for Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">HAT: Hardware-Aware Transformers for Efficient Natural Language Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">HAQ: Hardware-Aware Automated Quantization with Mixed Precision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Hardware-Centric AutoML for Mixed-Precision Quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
		<title level="m">O-CNN: Octree-based Convolutional Neural Networks for 3D Shape Analysis</title>
		<imprint>
			<publisher>SIGGRAPH</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Adaptive O-CNN: A Patchbased Deep Representation of 3D Shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGGRAPH Asia</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">APQ: Joint Search for Network Architecture, Pruning and Quantization Policy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Dynamic Graph CNN for Learning on Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>SIGGRAPH</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">VoxSegNet: Volumetric CNNs for Semantic Part Segmentation of 3D Shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>TVCG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">SegNAS3D: Network Architecture Search with Derivative-Free Global Optimization for 3D Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">C</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Moradi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>MICCAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">FBNet: Hardware-aware Efficient Convnet Design via Differentiable Neural Architecture Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">PointConv: Deep Convolutional Networks on 3D Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fuxin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Squeeze-SegV3: Spatially-Adaptive Convolution for Efficient Point-Cloud Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tomizuka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<title level="m">SpiderCNN: Deep Learning on Point Sets with Parameterized Convolutional Filters</title>
		<imprint>
			<publisher>ECCV</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">SECOND: Sparsely Embedded Convolutional Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Learning Object Bounding Boxes for 3D Instance Segmentation on Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Markham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Trigoni</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Searching Learning Strategy with Reinforcement Learning for 3D Medical Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>MICCAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">STD: Sparse-to-Dense 3D Object Detector for Point Cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">C2FNAS: Coarse-to-Fine Neural Architecture Search for 3D Medical Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Polar-Net: An Improved Grid Representation for Online LiDAR Point Clouds Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Foroosh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">V-NAS: Neural Architecture Search for Volumetric Medical Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Neural Architecture Search with Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Learning Transferable Architectures for Scalable Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
