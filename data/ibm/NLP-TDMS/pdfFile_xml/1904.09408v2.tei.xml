<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Language Models with Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenguang</forename><surname>Wang</surname></persName>
							<email>chgwang@amazon.com</email>
							<affiliation key="aff0">
								<orgName type="department">Amazon Web Services</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Amazon Web Services</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
							<email>smola@amazon.com</email>
							<affiliation key="aff0">
								<orgName type="department">Amazon Web Services</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Language Models with Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Transformer architecture is superior to RNN-based models in computational efficiency. Recently, GPT and BERT demonstrate the efficacy of Transformer models on various NLP tasks using pre-trained language models on large-scale corpora. Surprisingly, these Transformer architectures are suboptimal for language model itself. Neither self-attention nor the positional encoding in the Transformer is able to efficiently incorporate the word-level sequential context crucial to language modeling.</p><p>In this paper, we explore effective Transformer architectures for language model, including adding additional LSTM layers to better capture the sequential context while still keeping the computation efficient. We propose Coordinate Architecture Search (CAS) to find an effective architecture through iterative refinement of the model. Experimental results on the PTB, WikiText-2, and WikiText-103 show that CAS achieves perplexities between 20.42 and 34.11 on all problems, i.e. on average an improvement of 12.0 perplexity units compared to state-of-the-art LSTMs. The source code is publicly available 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Modeling the sequential context in language is the key to success in many NLP tasks. Recurrent neural networks (RNNs) <ref type="bibr" target="#b21">(Mikolov et al., 2010)</ref> memorize the sequential context in carefully designed cells. The sequential nature of these models, however, makes computation expensive <ref type="bibr" target="#b19">(Merity et al., 2017;</ref><ref type="bibr" target="#b32">Yang et al., 2017)</ref>, and therefore it is difficult to scale to large corpora.</p><p>The Transformer architecture <ref type="bibr" target="#b28">(Vaswani et al., 2017)</ref> replaces RNN cells with self-attention 1 https://github.com/cgraywang/ gluon-nlp-1/tree/lmtransformer/scripts/ language_model and point-wise fully connected layers, which are highly parallelizable and thus cheaper to compute. Together with positional encoding, Transformers are able to capture long-range dependencies with vague relative token positions. This results in a coarse-grained sequence representation at sentence level. Recent works such as GPT (or GPT-2) <ref type="bibr" target="#b24">(Radford et al., 2018</ref><ref type="bibr" target="#b25">(Radford et al., , 2019</ref> and BERT <ref type="bibr" target="#b7">(Devlin et al., 2018)</ref> show that the representations learned on large-scale language modeling datasets are effective for fine-tuning both sentence-level tasks, such as GLUE benchmark , and token-level tasks that do not rely on word order dependency in the context, such as question answering and NER.</p><p>Despite the fact that both GPT and BERT use language models for pre-training, neither of them achieves state-of-the-art performance in language modeling. Language model aims to predict the next word given the previous context, where finegrained order information of words in context is required. Neither self-attention nor positional encoding in the existing Transformer architecture is effective in modeling such information.</p><p>A second challenge (and opportunity) arises from the fact that we may often have access to models pre-trained on related, albeit not identical tasks. For instance, neither GPT or BERT is tuned for WikiText and neither of them aims to minimize perplexity directly. In fact, the architectures may not even be useful directly: BERT provides estimates of p(w i |context) rather than p(w i |history). This shows that there is a need for us to design algorithms which systematically explore the space of networks that can be derived (and adapted) from such tasks. This generalizes the problem of making use of pre-trained word embeddings for related tasks, only that in our case we do not have vectors but rather entire networks to deal with.</p><p>Lastly, the problem of architecture search per-se has received great interests. However, the size of the datasets where training a single model for GPT or BERT can cost in excess of $10,000, makes it prohibitively expensive to perform a fully-fledged model exploration with full retraining. Instead, we propose to use architecture search in a much more restricted (and economical) manner to investigate refining a trained architecture. This is much cheaper. Our pragmatic approach leads to improvements on the state-of-the-art in language modeling. Our contributions are as follows:</p><p>1. We propose a Transformer architecture for language model. It works by adding LSTM layers after all Transformer blocks (a result of the search algorithm). This captures finegrained word-level sequential context. 2. We describe an effective search procedure, Coordinate Architecture Search (CAS). This algorithm randomly generates variants of the Transformer architecture, based on the current best found architecture. Due to its greedy nature, CAS is simpler and faster than previous architecture search algorithms <ref type="bibr" target="#b33">(Zoph and Le, 2016;</ref><ref type="bibr" target="#b23">Pham et al., 2018;</ref><ref type="bibr" target="#b17">Liu et al., 2018)</ref>. 3. We show how this can be used to incorporate substantial prior knowledge in the form of GPT or BERT. Using this information via brute force architecture search would be prohibitively expensive.</p><p>Contributions 2 and 3 are general and apply to many cases beyond NLP. Contribution 1 is arguably more language specific. We evaluate CAS on three popular language model datasets: PTB, WikiText-2 and WikiText-103. The BERTbased CAS achieves in average 12.0 perplexity gains compared to the state-of-the-art LSTMbased language model AWD-LSTM-MoS <ref type="bibr" target="#b32">(Yang et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Transformers for Language Models</head><p>Our Transformer architectures are based on GPT and BERT. We will reuse the pre-trained weights in GPT and BERT to fine-tune the language model task. During fine-tuning, we modify and retrain the weights and network used by GPT and BERT to adapt to language model task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">GPT and BERT</head><p>GPT <ref type="bibr" target="#b24">(Radford et al., 2018)</ref> uses a variant of the Transformer architecture <ref type="bibr" target="#b28">(Vaswani et al., 2017)</ref>. That is, it employs a multi-layer Transformer decoder based language model. The original paper provides a pre-trained architecture with 12-layer Transformer decoder-only blocks. Each block has hidden size 768 and 12 self-attention heads. The weights are trained on BooksCorpus. This allows it to generate p(w i |history), one word at a time.</p><p>BERT is a multi-layer bidirectional Transformer encoder <ref type="bibr" target="#b7">(Devlin et al., 2018)</ref>. The original paper provides two BERT structures: BERT-Base, consists of 12-layer bidirectional Transformer encoder block with hidden size 768 and 12 self-attention heads; BERT-Large includes 24layer bidirectional Transformer encoder blocks with hidden size 1024 and 16 self-attention heads. The weights are trained on BooksCorpus and the English Wikipedia. Unless stated otherwise, we mean BERT Base when mentioning BERT.</p><p>Relation between GPT and BERT. Both models use virtually the same architecture. In fact, GPT and BERT-Base even use the same number of layers and dimensions. The only difference is that BERT is bidirectional since it tries to fill in individual words given their context, whereas GPT uses masked self-attention heads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Adapting GPT and BERT for Sub-word Language Model</head><p>GPT needs little modification, unless we want to explore different architectures. After all, it is already trained as a language model. At a minimum, during fine-tuning we add a linear layer with hidden size equal to the vocabulary size. These weights are tuned and fed into the softmax to generate a probability distribution of the target word over the vocabulary. Masked self-attention ensures that only causal information flow can occur. Recall the objective of BERT: masked language model and next sentence prediction. The masked language model uses bidirectional contextual information and randomly masks some tokens during training. Based on that it tries to infer the identity of the masked word. Unfortunately, estimating p(w i |w 1 , . . . w i−1 , w i+1 , . . . w n ) is not conducive to building an effective text generator: We would need to design a Gibbs sampler to sample w i |w −i , i.e. w i given its context w −i iteratively and repeatedly for all i to use a variant of this aspect directly.</p><p>The next sentence prediction aims to capture the binarized relationship between two sentences. Again, this is not directly useful for LM. We thus remove the objective and replace it by a loglikelihood measure during fine-tuning. Similar to GPT, we add an output linear layer and replace the self-attention heads with masked self-attention to prevent leftward information flow.</p><p>Note that GPT and BERT pre-trained weights are re-used in the language model fine-tuning process to save the costs of a full retraining. We are thus conducting the language model in the subword level since the sub-word tokenization is used in both GPT and BERT. More details will be described in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Fine-tuning Transformer Weights</head><p>GPT and BERT tune the weights of their respective models for the tasks mentioned above. For instance, BERT doesn't use windowing by default. Hence it makes sense to adjust the weights when fine-tuning for language modeling. However, updating all weights could lead to overfitting since datasets such as WikiText or Penn Tree Bank are over an order of magnitude smaller than the data used to train GPT and BERT.</p><p>To address this dilemma we propose to update only a subset of layer weights during finetuning. Since both GPT and BERT have 12 Transformer blocks, each of which contains a selfattention and a point-wise fully connected layer, it is not straightforward to choose the subset of layers whose parameters should be fixed. Instead, we will automatically search the subset which is most effective for the language model task. The search algorithm will be discussed in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Adding an LSTM</head><p>The positional encoding via a Fourier base in the Transformer only provides vague relative position information, forcing the layers to reinvent trigonometry at each layer for specific word access. This is problematic since LM requires strong word-level context information to predict the next word. RNNs explicitly model this sequential information. We therefore propose to add LSTM layers to the Transformer architecture.</p><p>In theory we could add LSTM layers anywhere, even interleaving them with Transformers. However, LSTMs add significant computational efficiency penalties, since they prevent parallel computation. Our reasoning is analogous to that guiding the design of the SRU (simple recurrent unit) <ref type="bibr" target="#b14">(Lei et al., 2018)</ref>. Hence we propose to add an LSTM layer either before all basic Transformer blocks or after these blocks. For the former, we add the LSTM layer immediately after the embedding layer and remove the positional and segment embedding, because we believe the LSTM layer is able to encode sufficient sequential information. For the latter, we insert the LSTM layer between the last Transformer block and the output linear layer. We determine the best location for the LSTM by automatic search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Coordinate Architecture Search</head><p>Now that we have the basic components, let's review the network transformations and the associated search procedures to obtain a wellperforming architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Network Transformations</head><p>Transformations modify a network. A modification could be adding a new layer or fixing the parameters during fine-tuning. In Section 2, we proposed multiple transformations. Let us define them formally below with randomization and practical constraints.</p><p>AddLinear adds a linear output layer with hidden size equal to the vocabulary size. It then randomly initializes its parameters. If such a linear layer already exists, this step is skipped. AddLSTM adds an LSTM layer if no such layer already exists. It attaches the LSTM either before or after all Transformer blocks. For the former we remove both positional embedding and segment embedding. If there exist fewer than 3 LSTM layers, we append another LSTM layer to the LSTM block. We randomly initialize parameters for the newly added layer. FixSubset Given n Transformer blocks, pick k ∈ [0, n] uniformly at random. Accordingly pick k blocks uniformly at random among the {1, . . . n} layers and fix the parameters for each selected block during fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Sampling a Search Candidate</head><p>We need to generate architecture candidates during search. To illustrate that restricted search is competitive to a full-fledged brute force reinforcement learning (or genetic algorithms) search, we adopt an exceedingly simple procedure: uniform random sampling. At each time we sample transformations uniformly at random (as per Algorithm 1) from the set of modifications of a base architecture until termination, as indicated Step i</p><p>Step <ref type="formula">i+1</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Coordinate Architecture Search</head><p>We use a simple greedy strategy for architecture search. Starting with either GPT or BERT as pre-trained model we repeat the search n times. Each time we sample a candidate, then fine-tune it and update the best candidate if necessary. See Algorithm 2 for a description and <ref type="figure">Figure 2</ref> for an illustration. At each (successful) step we fine-tune the variable parameters of the architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>To illustrate the effectiveness of the Transformer architectures found using coordinate search we present results on both WikiText and Penn Tree-Bank datasets. We also provide details about its speed relative to existing neural search strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Evaluation Metric</head><p>We evaluate the proposed methods on three widely-used language model benchmark datasets. Penn TreeBank (PTB): we use the preprocessed end if 8: end for 9: return net best version of <ref type="bibr" target="#b21">(Mikolov et al., 2010)</ref>, which contains 100M tokens. WikiText-2 (WT-2) is a small preprocessed version of Wikipedia, containing 200M tokens <ref type="bibr" target="#b20">(Merity et al., 2016)</ref>. WikiText-103 (WT-103) contains 1B tokens of the same origin as WT-2. We use the commonly adopted training, validation and test splits.</p><p>To illustrate the flexibility of our approach, we explore two pre-trained Transformers for subword level language models, i.e., BERT and GPT. For BERT related model architectures, we use WordPiece embedding <ref type="bibr" target="#b31">(Wu et al., 2016)</ref> to tokenize the training/validation/test split of the PTB, WT-2 and WT-103 respectively. The resulting sub-word vocabulary size is 30k, denoted as BERTVocab. The split word pieces are denoted with ## following <ref type="bibr" target="#b7">(Devlin et al., 2018)</ref>. For the model architectures based on GPT, the three datasets are tokenized based on bytepair encoding (BPE) <ref type="bibr" target="#b27">(Sennrich et al., 2016)</ref>, where the subword vocabulary size is 40k based on <ref type="bibr" target="#b24">(Radford et al., 2018)</ref>, denoted as GPTVocab. Note that BERT and the WordPiece embedding in BERT are trained on BooksCorpus and Wikipedia, whereas GPT and its BPE are trained only on BooksCorpus. Note the sub-word level vocabulary size is different from the word-level vocabulary size obtained on the training splits of the datasets. We use perplexity (PPL) to evaluate the sub-word language model results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training Details</head><p>We evaluate CAS (Algorithm 2) with both BERT and GPT pre-trained as the initial architecture, and trained on all three datasets. The same training configuration is used across all datasets. We pick n = 10 search steps. In a fine-tuning task, the number of epochs is 50, the gradients are computed using truncated back-propagation through time, and ADAM (Kingma and Ba, 2014) is used to update parameters. The perplexity on the validation dataset is used to choose architectures. We report results on the respective test datasets.</p><p>For GPT based architectures the hyperparameters of the Transformer decoder and embedding blocks are the same as in <ref type="bibr" target="#b24">(Radford et al., 2018)</ref>. If LSTM layers are added, we set the dropouts of the LSTM layers to 0.1. DropConnect is not applied. All other LSTM hyperparameters follow <ref type="bibr" target="#b19">(Merity et al., 2017)</ref>. The final linear layer is with dropout rate 0.1. Following <ref type="bibr" target="#b32">(Yang et al., 2017)</ref>, we use a mixture of softmax (MoS) to replace the standard softmax with 15 components. We set 64 as sequence length and 16 as minibatch size. ADAM with learning rate 6.25 · 10 −5 and L2 weight decay of 0.01 are used.</p><p>For BERT based architectures the hyperparameters of the Transformer encoder blocks and the embedding blocks are set the same as the original implementation <ref type="bibr" target="#b7">(Devlin et al., 2018)</ref>. The hyperparameters of the LSTM layers and linear layer are the same with GPT configuration. As with GPT we use MoS with 15 components. We pick 128 as sequence length and 16 as minibatch size. ADAM with learning rate 10 −4 , β 1 = 0.9, β 2 = 0.999 and L2 weight decay of 0.01 are used.</p><p>Lastly, for AWD-LSTM-MoS with BERT or GPT sub-word setting, we largely follow the parameter settings in the original implementation <ref type="bibr" target="#b32">(Yang et al., 2017)</ref>. We use NT-ASGD <ref type="bibr" target="#b19">(Merity et al., 2017)</ref> to train 50 epochs on training datasets.</p><p>Since the goal of this work is to discover bestperforming language model from the architecture perspective, we do not employ post-training methods such as neural cache model <ref type="bibr" target="#b9">(Grave et al., 2016)</ref> or dynamic evaluation <ref type="bibr" target="#b13">(Krause et al., 2018)</ref>. We expect that such methods would potentially improve the perplexity of all models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparing CAS to Other Methods</head><p>We compare CAS, denoted by BERT-CAS and GPT-CAS respectively to three other models.</p><p>BERT and GPT. This is straightforward. The only change needed is that we update the last output layer during fine-tuning.</p><p>AWD-LSTM-MoS-{BERT, GPT}Vocab. This is a state-of-the-art language model, based on LSTMs, improving on <ref type="bibr" target="#b32">(Yang et al., 2017)</ref> due to a more careful handling of tokens. For a fair comparison, instead of using word level vocabulary in the original implementation of AWD-LSTM-MoS <ref type="bibr" target="#b32">(Yang et al., 2017)</ref>, we use the subword vocabularies of BERT and GPT separately. Our implementation uses BERTVocab or GPTVocab to replace the word based vocabulary used in the original implementation of AWD-LSTM-MoS <ref type="bibr" target="#b32">(Yang et al., 2017)</ref>. Note that on PTB and WT-2, both AWD-LSTM-MoS-BERTVocab and AWD-LSTM-MoS-GPTVocab outperform the original AWS-LSTM-MoS models by 17.8 and 10.6 perplexity points respectively. This is likely due to the change in word vocabulary to a sub-word vocabulary.</p><p>The results are shown in <ref type="table">Table 1</ref> and illustrated in <ref type="figure" target="#fig_1">Figure 3</ref>. First note that GPT and BERT are significantly worse than AWD-LSTM-MoS. It confirms our hypothesis that neither BERT nor GPT are effective tools for language modeling. Applying them naively leads to significantly worse results compared to AWS-LSTM-MoS on three datasets. It demonstrates that language modeling requires strong capabilities in modeling the word order dependency within sentences. However, due to the combination of self-attention and positional encoding, both GPT and BERT primarily capture coarse-grained language representation but with limited word-level context.</p><p>On the other hand, the Transformer architectures picked by CAS (BERT-CAS) outperform AWS-LSTM-MoS on all datasets. The average test perplexity improvement with BERT pretrained models is 8.09 and 8.28 with GPT pretrained models. The results demonstrate that 1) CAS is able to locate an effective Transformer architecture for language model; and 2) that the combination of fixing a subset weights and adding LSTM layers is capable of capturing the wordlevel context. Furthermore, we apply CAS to BERT-Large (i.e., BERT-Large-CAS). Compare to BERT-CAS, the architectures generated achieve on average 7.70 perplexity gains, which are competitive results with recent approaches such as Transformer-XL <ref type="bibr" target="#b6">(Dai et al., 2019)</ref> and GPT-2 <ref type="bibr" target="#b25">(Radford et al., 2019)</ref>. This shows that robustness of the CAS method, which indicates that a stronger pretrained model would potentially produce a better language model.</p><p>In addition, BERT-CAS outperforms GPT-CAS on datasets PTB and WT-2, but is worse on WT-103. The reason is twofold. First, the GPT's BPE vocabulary is 10k larger than BERT's Word-Piece vocabulary, since the original word vocabulary size of WT-103 is around 10 times larger compared to PTB and WT-2, thus we infer that BPE vocabulary has stronger ability to represent large vocabulary. Second, unlike GPT, the pre-trained BERT weights are not based on a language modeling objective. Thus BERT based architectures may need more epochs to converge on large corpora. This is likely due to the fact that masking is not a part of BERT training. Its introduction amounts to a more significant change in the covariates, thus requires more adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>To elucidate the effects of different model improvements we compare CAS to the following three variants:</p><p>{BERT, GPT}-CAS-Subset applies Algorithm 2 without adding LSTM layers.   {BERT, GPT}-CAS-LSTM applies Algorithm 2 but it fixes all Transformer blocks during fine-tuning.</p><p>See  linear output layer (BERT-All) on PTB data as an example, to illustrate the over-fitting issue. From the results in <ref type="table" target="#tab_6">Table 3</ref>, we can see that, by leveraging CAS, we marginally relieve the over-fitting issue by fixing a subset of weights of the full Transformer architecture. Let's look into the details of adding LSTMs. There are 4 cases:</p><p>Only-LSTM implements a model consisting only We study whether to add LSTMs before or after the transformer layers (or none at all).</p><p>of LSTM layers. To make up for the loss of expressiveness due to removing all Transformer blocks we add a total of 6 LSTM layers. None-LSTM adds no LSTM layer at all. Instead, we add another stack of Transformer blocks. This effectively doubles the number of blocks to 24. First-LSTM adds LSTM layers only before all Transformer blocks. Last-LSTM adds LSTM layers only after all Transformer blocks.</p><p>The results are shown in <ref type="table" target="#tab_8">Table 4</ref> and in <ref type="figure" target="#fig_2">Figure 4</ref>. As can be seen, neither purely transformer blocks nor purely LSTM layers are effective for language modeling. The former is likely unsuitable due to the comparatively large number of parameters relative to the tuning set. Adding LSTM layers properly into Transformer architecture significantly improves the perplexity. In addition, adding LSTMs before the output linear layer outperforms replacing positional and segment embeddings with LSTM layers. These results confirm our intuition and indicate that we need to first preserve the coarse-grained representation using fixed subset weights; subsequently LSTMs can be used to model the word order dependency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Efficiency Analysis</head><p>Lastly, we compare CAS with other existing neural network search methods in terms of search cost. The main distinction being that we significantly constrain the architectures to be investi-  gated. This allows us to obtain significant computational savings.</p><p>NAS by <ref type="bibr" target="#b33">(Zoph and Le, 2016</ref>) is a reinforcement learning based search method, which uses a recurrent network to generate the model descriptions of neural networks and minimizes the expected perplexity of the generated architectures on the PTB validation set. ENAS by <ref type="bibr" target="#b23">(Pham et al., 2018)</ref> also leverages a reinforcement learning search method. ENAS's search space is the superposition of all possible child models in the NAS search space, which allows parameters to be shared among all child models. DARTS by <ref type="bibr" target="#b17">(Liu et al., 2018)</ref> is a recently proposed neural architecture search algorithm based on gradient descent.</p><p>We evaluate the efficiency of the methods using GPU days. The search costs of NAS, ENAS and DARTS are obtained from <ref type="bibr" target="#b17">(Liu et al., 2018)</ref>. The reported search costs of the above methods compared to CAS are shown in <ref type="table" target="#tab_9">Table 5</ref>. As can be seen, BERT-CAS is cheaper than all others. The results indicate that by leveraging the prior knowledge of the design of the neural networks for specific tasks, we could only optimize the architectures in a small confined sub-space, that leads to speed up the search process. For example, BERT-CAS is directly based on BERT, applying search upon such effective neural networks could facilitate the adaptation to similar tasks.</p><p>The reason of the search cost of GPT-CAS on WT-2 is higher than ENAS is three-fold:</p><p>1. ENAS is directly transferring an architecture searched based on PTB to WT-2. Instead we apply coordinate search to find one from scratch;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Search Method</head><p>Search Cost (GPU days) Method Class PTB WT-2 NAS <ref type="bibr" target="#b33">(Zoph and Le, 2016)</ref> 1,000 CPU days n.a. reinforcement ENAS <ref type="bibr" target="#b23">(Pham et al., 2018)</ref> 0.5 0.5 reinforcement DARTS (first order) <ref type="bibr" target="#b17">(Liu et al., 2018)</ref> 0.5 1 gradient descent DARTS (second order) <ref type="bibr" target="#b17">(Liu et al., 2018)</ref> 1 gradient descent BERT-CAS (Our) 0.15 0.38 greedy search GPT-CAS (Our) 0.23 0.53 greedy search   <ref type="bibr" target="#b25">(Radford et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Training Data Tokens</head><p>GPT-2 WebText 14.0B</p><p>BERT-Large-CAS PTB 0.1B WT-2 0.2B WT-103 1.0B 2. The model size of GPT-CAS is 149M, which is much larger compared to the size 37M from ENAS; 3. The GPT vocabulary size is 10k larger compared to the ENAS's vocabulary.</p><p>We note that the difference in vocabularies might affect the results, since the results of NAS, ENAS and DARTS are from the original implementations. The original implementations are based on basic word tokenization (such as space splitter) of the PTB and WT-2. Instead, we are using the sub-word tokenization (WordPiece and BPE respectively) for BERT and GPT architecture exploration. However, the vocabulary size after basic tokenization processing is similar to the results after the sub-word tokenization, which are all around 30k-40k. Given that, we consider the per-formance comparison as fair 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Comparison with GPT-2</head><p>We specifically compare the proposed model with the recent state-of-the-art language model GPT-2 <ref type="bibr" target="#b25">(Radford et al., 2019)</ref> on three dimensions: results 3 , parameter size, and scale of the training data. From the results shown in <ref type="table" target="#tab_10">Table 6</ref>, we conclude that with comparable size of the models' parameters, BERT-Large-CAS outperforms GPT-2 (345M) by on average 10.97 PPL on PTB and WT-103. More surprisingly, the proposed method performs better than GPT-2 (1542M) which has around 4 times more parameters. On WT-103, BERT-Large-CAS is better than GPT-2 (762M) which has around 2 times more parameters. Note that on WT-2, our method performs worse than GPT-2, we suspect the reason is that the Web-Text still contains the texts that are similar to the Wikipedia. WT-2 is quite small in terms of scale. In contrast, we regard the results on WT-103 (50 times larger than WT-2) as a more reasonable comparison with GPT-2.</p><p>The training data described in <ref type="table" target="#tab_11">Table 7</ref> suggests that, with significantly smaller training datasets, the proposed method generates competitive results. Once GPT-2 models are released, we expect CAS could generalize to the GPT-2 models to obtain better results for language model task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Architecture search has shown promising results in tasks such as image classification <ref type="bibr" target="#b33">(Zoph and Le, 2016;</ref><ref type="bibr">Liu et al., 2017a,b;</ref><ref type="bibr" target="#b26">Real et al., 2018;</ref><ref type="bibr" target="#b17">Liu et al., 2018)</ref>, object detection  as well as language modeling <ref type="bibr" target="#b33">(Zoph and Le, 2016;</ref><ref type="bibr" target="#b23">Pham et al., 2018;</ref><ref type="bibr" target="#b17">Liu et al., 2018)</ref> in NLP. Existing neural architecture search studies focus on leveraging different methods to build the neural network from scratch. For example, NAS <ref type="bibr" target="#b33">(Zoph and Le, 2016</ref>) uses reinforcement learning to obtain an architecture for CIFAR-10 and ImageNet. Designing the architecture from scratch using reinforcement learning is very costly. Many follow-up studies focus on speeding up the search process by weight-sharing across child models <ref type="bibr" target="#b23">(Pham et al., 2018;</ref><ref type="bibr" target="#b4">Cai et al., 2018)</ref>, by incorporating a particular structure into the search space <ref type="bibr">(Liu et al., 2017a,b)</ref>, or by enabling weights prediction for each architecture <ref type="bibr" target="#b3">(Brock et al., 2017;</ref><ref type="bibr" target="#b1">Baker et al., 2017)</ref>. Different from the above methods, the proposed coordinate search does not involve any controllers.</p><p>Recent studies start to explore using the idea of network transformation within reinforcement learning <ref type="bibr" target="#b4">(Cai et al., 2018)</ref> or via Bayesian optimization <ref type="bibr" target="#b11">(Jin et al., 2018)</ref> or simple greedy search <ref type="bibr" target="#b8">(Elsken et al., 2017)</ref>. DARTS <ref type="bibr" target="#b17">(Liu et al., 2018)</ref> enables gradient descent to optimize the architecture. Compared to these methods, the coordinate search is more straightforward and more efficient due to the direct incorporation of the pre-defined Transformer architecture. Notably, the major difference of the proposed search algorithm compared to the existing methods is that we focus on adapting an existing well-trained Transformer architecture with minimum changes in the task of language model, whereas a majority of the existing work focus on generating variants of RNN cells from scratch for better results.</p><p>Language models have been studied extensively in NLP. Neural language models have supplanted traditional n-gram models in recent years <ref type="bibr" target="#b2">(Bengio et al., 2003;</ref><ref type="bibr" target="#b22">Mnih and Hinton, 2007;</ref><ref type="bibr" target="#b21">Mikolov et al., 2010)</ref>. Particularly, recurrent neu-ral networks <ref type="bibr" target="#b10">(Inan et al., 2016;</ref><ref type="bibr" target="#b19">Merity et al., 2017;</ref><ref type="bibr" target="#b18">Melis et al., 2017;</ref><ref type="bibr" target="#b13">Krause et al., 2018)</ref>, such as LSTMs have achieved state-of-the-art results on various benchmark datasets with different regularization techniques and post-training methods <ref type="bibr" target="#b9">(Grave et al., 2016;</ref><ref type="bibr" target="#b13">Krause et al., 2018)</ref>. The mixture of softmax <ref type="bibr" target="#b32">(Yang et al., 2017)</ref> has helped address the low-rank embedding problem for word prediction. We used this in our model, too. It provides some improvement over a more conventional model.</p><p>The recently proposed GPT-2 <ref type="bibr" target="#b25">(Radford et al., 2019</ref>) is a deeper Transformer decoder based language model trained on a 40GB dataset. In contrast, the proposed model generates competitive results but with significantly less training cost and smaller model size. Transformer-XL <ref type="bibr" target="#b6">(Dai et al., 2019)</ref> is a word level language model that also delivers good results by incorporating longer context. The proposed method is a sub-word level language model thus the results are not comparable. We expect to generalize CAS to pre-trained Transformer-XL models as well to achieve better results. The adaptive input representations idea proposed in <ref type="bibr" target="#b0">(Baevski and Auli, 2018)</ref> could be combined with the proposed method to further speed up.</p><p>Network transformations were introduced in the context of the transfer learning <ref type="bibr" target="#b5">(Chen et al., 2015)</ref>. The main purpose of the transformations is to make networks deeper and wider. Often stagewise training accelerates training and architecture search. Recent studies <ref type="bibr" target="#b30">(Wei et al., 2016;</ref><ref type="bibr" target="#b4">Cai et al., 2018;</ref><ref type="bibr" target="#b8">Elsken et al., 2017)</ref> focus on extending the set of the network transformations to handle additional operations such as non-linear activation functions and skip connections. We instead introduce simple network modifications to perform modest modifications of an existing network. They allow us to treat a pre-trained Transformer block in a manner similar to that of a large pre-trained embedding vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We study the problem of finding an effective Transformer architecture for language model. We identify the issues of existing Transformer architectures, such as BERT and GPT, that are not able to capture the strong word-level context required in language model. We proposed two approaches to address this issue: we fine-tune a subset of pa-rameters to improve the coarse-grain representations obtained from the pre-trained Transformer models. Secondly, we add LSTM layers to capture the fine-grained sequence. We then propose a coordinate architecture search (CAS) algorithm to select an effective architecture based on finetuning results. It uses a greedy search strategy to accelerate architecture search. We experimentally show that CAS outperforms the state-of-the-art language models on three language model benchmark datasets.</p><p>Although we only show the effectiveness of CAS when applying Transformer architectures to the language model task, we feel it is possible to apply CAS to both other neural network architectures and fine-tuning other NLP tasks that require strong word-level context as well.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Search candidate sampling. net is the base architecture and candidate is returned in the next step. Transformers, Embeddings, LSTMs and Linear output transformations are as stated. Lightly shaded blocks are variable, dark blocks are fixed. See Algorithm 1 for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Comparison of test perplexities between CAS and other models (left: using BERT pre-trained models; right: using GPT pre-trained models). In particular, 'Subset' indicates variants without LSTMs and 'LSTM' corresponds to models without updating the transformer blocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>LSTM variants for Penn TreeBank.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Figure 2: Coordinate architecture search. net_best is the best architecture at step i of the search. We sample search candidates and keep the one that performs best, as measured by perplexity (Val PPL) on the target dataset after fine-tuning. See details in Algorithm 2. AddLinear 6: return candidate by adding an emissions layer AddLinear . This means that we have a valid architecture. See Figure 1 for an example.</figDesc><table><row><cell></cell><cell>candidate 0</cell><cell>candidate 0</cell></row><row><cell></cell><cell>candidate 1</cell><cell>candidate 1</cell></row><row><cell cols="2">Algorithm 1 Search Candidate Sampling</cell></row><row><cell cols="2">Input: Base architecture net</cell></row><row><cell cols="2">Output: A new architecture candidate</cell></row><row><cell cols="2">1: candidate ← net</cell></row><row><cell cols="2">2: repeat</cell></row><row><cell>3:</cell><cell>Sample a tranformation T uniformly from</cell></row><row><cell></cell><cell>{AddLinear , AddLSTM , FixSubset}.</cell></row><row><cell>4:</cell><cell>Apply T to candidate</cell></row><row><cell cols="2">5: until T =</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Algorithm 2 Coordinate Architecture Search Input: Initial architecture net, search steps n, fine-tuning dataset Output: Best architecture net best 1: net best ← net; 2: for i = 1 to n do</figDesc><table><row><cell>3:</cell><cell>Draw candidate from net best using Algo-</cell></row><row><cell></cell><cell>rithm 1.</cell></row><row><cell>4:</cell><cell>Fine-tune candidate on dataset</cell></row><row><cell>5:</cell><cell>if PPL(candidate) &lt; PPL(net best ) then</cell></row><row><cell>6:</cell><cell>net best ← candidate</cell></row><row><cell>7:</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Subset 42.53 36.57 51.15 44.96 44.34 43.33 BERT-CAS-LSTM 40.22 35.32 53.82 47.00 53.66 51.60 GPT-CAS-Subset 47.58 41.85 54.58 50.08 35.49 35.48 GPT-CAS-LSTM 47.24 41.61 50.55 46.62 36.68 36.61</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Datasets</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>PTB</cell><cell cols="2">WT-2</cell><cell cols="2">WT-103</cell></row><row><cell>Val</cell><cell>Test</cell><cell>Val</cell><cell>Test</cell><cell>Val</cell><cell>Test</cell></row><row><cell>BERT-CAS-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Ablation study. Compare CAS with not adding LSTM layers (CAS-Subset) and not updating Transformer block parameters (CAS-LSTM).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>Model</cell><cell cols="2">Validation Test</cell></row><row><cell>BERT-All</cell><cell>79.14</cell><cell>67.43</cell></row><row><cell cols="2">BERT-CAS 39.97</cell><cell>34.47</cell></row><row><cell>and Figure 3 for details of the results.</cell><cell></cell><cell></cell></row><row><cell>As can be seen, both CAS-Subset and CAS-LSTM</cell><cell></cell><cell></cell></row><row><cell>improve significantly upon a naive use of BERT</cell><cell></cell><cell></cell></row><row><cell>and GPT. This is to be expected since fine-tuning</cell><cell></cell><cell></cell></row><row><cell>improves performance. On the smaller dataset,</cell><cell></cell><cell></cell></row><row><cell>i.e. PTB, adding LSTMs is more effective. This</cell><cell></cell><cell></cell></row><row><cell>might be due to the overfitting incurred in updat-</cell><cell></cell><cell></cell></row><row><cell>ing Transformers. On the other hand, on the larger</cell><cell></cell><cell></cell></row><row><cell>datasets, i.e. WT-103, adding an LSTM is less</cell><cell></cell><cell></cell></row><row><cell>effective, which means that adapting the Trans-</cell><cell></cell><cell></cell></row><row><cell>former parameters for better sentence-level rep-</cell><cell></cell><cell></cell></row><row><cell>resentation is more important. Combing both to-</cell><cell></cell><cell></cell></row><row><cell>gether leads to further improvement. CAS outper-</cell><cell></cell><cell></cell></row><row><cell>forms AWD-LSTM-MoS on all three datasets.</cell><cell></cell><cell></cell></row><row><cell>Next, we unfreeze the pre-trained weights of</cell><cell></cell><cell></cell></row><row><cell>BERT to allow fully fine-tuning including the last</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: Over-fitting example on PTB data. BERT-</cell></row><row><cell>All: BERT with fully fine-tuning including the last</cell></row><row><cell>layer. BERT-CAS: BERT with coordinate archi-</cell></row><row><cell>tecture search.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Effects of different search constraints for placing the LSTM on perplexity on the PTB data.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Efficiency of different search methods on PTB and WT-2.</figDesc><table><row><cell></cell><cell></cell><cell>Datasets</cell></row><row><cell>Model</cell><cell>Parameters</cell><cell>PTB WT-2 WT-103</cell></row><row><cell></cell><cell>345M</cell><cell>47.33 22.76 26.37</cell></row><row><cell>GPT-2</cell><cell>762M</cell><cell>40.31 19.93 22.05</cell></row><row><cell></cell><cell>1542M</cell><cell>35.76 18.34 17.48</cell></row><row><cell cols="2">BERT-Large-CAS 395M</cell><cell>31.34 34.11 20.42</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Compare model parameter size and results with GPT-2. The GPT-2 model size and results are from</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Compare training data size with GPT-2.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The PPL results are not comparable since the vocabularies are different (i.e., sub-word versus word level), we omit the comparison here.3  The results comparison is fair since GPT-2's vocabulary is also based on sub-word tokenization.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.10853</idno>
		<title level="m">Adaptive input representations for neural language modeling</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Accelerating neural architecture search using performance prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otkrist</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.10823</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Ramesh Raskar, and Nikhil Naik</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theodore</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.05344</idno>
		<title level="m">Smash: one-shot model architecture search through hypernetworks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Efficient architecture search by network transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Net2net: Accelerating learning via knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Transformer-xl: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.02860</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pretraining of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Simple and efficient architecture search for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Hendrik</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Improving neural language models with a continuous cache</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Tying word vectors and word classifiers: A loss framework for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khashayar</forename><surname>Hakan Inan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Khosravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01462</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Efficient neural architecture search with network morphism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingquan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.10282</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dynamic evaluation of neural sequence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Kahembwe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Renals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2771" to="2780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Simple recurrent units for highly parallelizable recurrence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4470" to="4481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.00559</idno>
		<title level="m">Progressive neural architecture search</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chrisantha</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00436</idno>
		<title level="m">Hierarchical representations for efficient architecture search</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">DARTS: differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">On the state of the art of evaluation in neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gábor</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.05589</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Regularizing and optimizing LSTM language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Pointer sentinel mixture models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukáš</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jančernockỳ</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eleventh Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Three new graphical models for statistical language modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="641" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Efficient neural architecture search via parameter sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melody</forename><forename type="middle">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4092" to="4101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<ptr target="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Tim Salimans, and Ilya Sutskever</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alok</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.01548</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amapreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07461</idno>
		<title level="m">Glue: A multi-task benchmark and analysis platform for natural language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Network morphism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><forename type="middle">Wen</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="564" to="572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Breaking the softmax bottleneck: A high-rank RNN language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8697" to="8710" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
