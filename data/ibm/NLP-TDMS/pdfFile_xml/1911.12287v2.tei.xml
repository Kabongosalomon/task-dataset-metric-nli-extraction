<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Your Local GAN: Designing Two Dimensional Local Attention Mechanisms for Generative Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giannis</forename><surname>Daras</surname></persName>
							<email>daras.giannhs@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
							<email>augustusodena@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Brain</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
							<email>zhanghan@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Brain</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><forename type="middle">G</forename><surname>Dimakis</surname></persName>
							<email>dimakis@austin.utexas.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">National Technical University of Athens</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<address>
									<settlement>Austin</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Your Local GAN: Designing Two Dimensional Local Attention Mechanisms for Generative Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a new local sparse attention layer that preserves two-dimensional geometry and locality. We show that by just replacing the dense attention layer of SAGAN with our construction, we obtain very significant FID, Inception score and pure visual improvements. FID score is improved from 18.65 to 15.94 on ImageNet, keeping all other parameters the same. The sparse attention patterns that we propose for our new layer are designed using a novel information theoretic criterion that uses information flow graphs.</p><p>We also present a novel way to invert Generative Adversarial Networks with attention. Our method uses the attention layer of the discriminator to create an innovative loss function. This allows us to visualize the newly introduced attention heads and show that they indeed capture interesting aspects of two-dimensional geometry of real images.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Generative Adversarial Networks <ref type="bibr" target="#b9">[10]</ref> are making significant progress on modeling and generating natural images <ref type="bibr">[26,</ref><ref type="bibr" target="#b3">4]</ref>. Transposed convolutional layers are a fundmamental architectural component since they capture spatial invariance, a key property of natural images <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr">27]</ref>. The central limitation (e.g. as argued in <ref type="bibr">[26]</ref>) is that convolutions fail to model complex geometries and long-distance dependencies-the canonical example is generating dogs with fewer or more than four legs.</p><p>To compensate for this limitation, attention layers <ref type="bibr">[25]</ref> have been introduced in deep generative models <ref type="bibr">[26,</ref><ref type="bibr" target="#b3">4]</ref>. Attention enables the modeling of long range spatial dependencies in a single layer which automatically finds correlated parts of the image even if they are far apart. First introduced in SAGAN [26] and further improved in Big-GAN <ref type="bibr" target="#b3">[4]</ref>, attention layers have led to some of the best known GANs currently available.</p><p>Attention layers have a few limitations. The first is that they are computationally inefficient: Standard dense attention requires memory and time complexity that scales quadratically in the size of the input. Second, dense attention layers are statistically inefficient: A significant number of training samples is required to train attention layers, a problem that becomes more pronounced when multiple attention heads or layers are introduced <ref type="bibr" target="#b5">[6]</ref>. Statistical inefficiency also stems from the fact that dense attention does not benefit from locality, since most dependencies in images relate to nearby neighborhoods of pixels. Recent work indicates that most attention layer heads learn to attend mainly to local neighborhoods <ref type="bibr">[24]</ref>.</p><p>To mitigate these limitations, sparse attention layers were recently introduced in Sparse Transformers <ref type="bibr" target="#b5">[6]</ref>. In that paper, different types of sparse attention kernels were introduced and used to obtain excellent results for images, text and audio data. They key observation we make is that the patterns that were introduced in Sparse Transformers are actually designed for one-dimensional data, such as textsequences. Sparse Transformers <ref type="bibr" target="#b5">[6]</ref> were applied to images by reshaping tensors in a way that significantly distorts distances of the two-dimensional grid of image pixels. Therefore, local sparse attention kernels introduced in Sparse Transformers fail to capture image locality. Our Contributions:</p><p>• We introduce a new local sparse attention layer that preserves two-dimensional image locality and can support good information flow through attention steps.</p><p>• To design our attention patterns we use the information theoretic framework of Information Flow Graphs <ref type="bibr" target="#b7">[8]</ref>. This quantifies how information can flow through mul- tiple steps and preserve two-dimensional locality. We visualize learned attention maps and show that different heads indeed learn different aspects of the geometry of generated images.</p><p>• We modify SAGAN [26] using our new twodimensional sparse attention layers to introduce YLG-SAGAN. We empirically show that this change yields significant benefits. We train on ImageNet-128 and we achieve 14.53% improvement to the FID score of SAGAN and 8.95% improvement in Inception score, by only changing the attention layer while maintaining all other parameters of the architecture. Our ablation study shows that indeed the benefits come from two dimensional inductive bias and not from introducing multiple attention heads. Furthermore, YLG-SAGAN achieves this performance in 800k training steps as opposed to 1300k for SAGAN and hence reduces the training time by approximately 40%.</p><p>• To visualize our attention maps on natural images, we came across the problem of inverting a generator: given an image x, how to find a latent code z so that G(z) is as close as possible to x. The natural inversion process of performing gradient descent on this loss works in small GANs <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b13">14]</ref> but has been notoriously failing in bigger models with attention like SAGAN <ref type="bibr" target="#b0">1</ref> . We present a solution to the GAN inversion problem: We use the attention layer of the discriminator to obtain a weighting on the loss function that subsequently we use to invert with gradient descent. We empirically show excellent inversion results for numerous cases where standard gradient descent inversion fails.</p><p>We open-source our code and our models to encourage further research in this area. The code is available under the 1 This fact is folklore, known at least among researchers who try to solve inverse problems using deep generative models. There are, of course numerous other ways to invert, like training an encoder, but also show poor performance on modern GANs with attention. repository: https://github.com/giannisdaras/ylg 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>Dense Attention.</p><p>Given More specifically, attention of X to Y associates the following matrices with the inputs: The key matrix</p><formula xml:id="formula_0">matrices X ∈ R N X ×E X , Y ∈ R N Y ×E Y , attention of X to Y ,</formula><formula xml:id="formula_1">X K = X·W K , the query matrix Y Q = Y · W Q and the value matrix Y V = X · W V where W K ∈ R E X ×E , W Q ∈ R E Y ×E , W V ∈ R E×E V are learnable weight matrices.</formula><p>Intuitively, queries are compared with keys and values translate the result of this comparison to a new vector representation of X that integrates information from Y . Mathematically, the output of the attention is the matrix: X = σ(X Q · Y T K ) · Y V where σ(·) denotes the softmax operation along the last axis. Sparsified Attention.</p><p>The quadratic complexity of attention to the size of the input is due to the calculation of the matrix A X,Y = X Q · Y T K , ∈ R N X ×N Y . Instead of performing this calculation jointly, we can split attention in multiple steps. At each step i, we attend to a subset of input positions, specified by a binary mask</p><formula xml:id="formula_2">M i ∈ {0, 1} N X ×N Y . Mathematically, at step i we calculate matrix A i X,Y , where: A i X,Y [a, b] = A X,Y [a, b], M i [a, b] = 1 −∞, M i [a, b] = 0 .</formula><p>In this expression, −∞ means that after the softmax, this position will be zeroed and thus not contribute to the calculation of the output matrix. The design of the masks {M i } is key in reducing the number of positions attended.</p><p>There are several ways that we can use the matrices A i X,Y to perform multi-step attention <ref type="bibr" target="#b5">[6]</ref> in practice. The simplest is to have separate attention heads [25] calculating the different matrices {A i X,Y } in parallel and then concatenate along the feature dimension. We use this approach in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Your Local GAN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Full Information Attention Sparsification</head><p>As explained, an attention sparsification in p steps is described by binary masks {M 1 , ..., M p }. The question is how to design a good set of masks for these attention steps.</p><p>We introduce a tool from information theory to guide this design.</p><p>Information Flow Graphs are directed acyclic graphs introduced in <ref type="bibr" target="#b7">[8]</ref> to model distributed storage systems through network information flow <ref type="bibr" target="#b0">[1]</ref>. For our problem, this graph models how information flows across attention steps. For a given set of masks {M 1 , ..., M p }, we create a multi-partite</p><formula xml:id="formula_3">graph G(V = {V 0 , V 1 , ..., V p }, E) where directed con- nections between V i , V i+1 are determined by mask M i .</formula><p>Each group of vertices in partition V i corresponds to attention tokens of step i.</p><p>We say that an attention sparsification has Full Information if its corresponding Information Flow Graph has a directed path from every node a ∈ V 0 to every node b ∈ V p . Please note that the Fixed pattern <ref type="bibr" target="#b5">[6]</ref> shown in sub-figure 2a does not have Full Information: there is no path from node 1 of V 0 to node 2 of V 2 .</p><p>Sparse attention is usually considered as a way to reduce the computational overhead of dense attention at a hopefully small performance loss. However, we show that attention masks chosen with a bias toward two-dimensional locality, can surprisingly outperform dense attention layers (compare the second and the third row of <ref type="table" target="#tab_1">Table 1</ref>). This is an example of what we call the statistical inefficiency of dense attention. Sparse attention layers with locality create better inductive bias and hence can perform better in the finite sample regime. In the limit of infinite data, dense attention can always simulate sparse attention or perform better, in the same way that a fully connected layer can simulate a convolutional layer for a possible selection of weights.</p><p>We design the sparse patterns of YLG as the natural extensions of the patterns of <ref type="bibr" target="#b5">[6]</ref> while ensuring that the corresponding Information Flow Graph supports Full Information. The first pattern, which we call Left to Right (LTR), extends the pattern of <ref type="bibr" target="#b5">[6]</ref> to a bi-directional context. The second pattern, which we call Right to Left (RTL), is a transposed version of LTR. The corresponding 9 × 9 masks and associated Information Flow Graphs are presented in sub-figures 2b, 2e (LTR) and 2c, 2f (RTL). These patterns allow attention only to n √ n positions, significantly reducing the quadratic complexity of dense attention. It is possible to create very sparse Full Information graphs using multiple attention steps, but designing them and training them remains open for future work; in this paper we focus on twostep factorizations. We include more details about information flow graphs and how we use them to design attention patterns in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Two-Dimensional Locality</head><p>The factorization patterns of Sparse Transformers <ref type="bibr" target="#b5">[6]</ref> and their Full Information extensions illustrated in <ref type="figure" target="#fig_4">Figure  2</ref> are fundamentally matched to one-dimensional data, such as text-sequences.</p><p>The standard way to apply these layers on images is to reshape the three dimensional image tensors (having three color channels) to a two-dimensional tensor X ∈ R N ×C that enters attention. This corresponds to N tokens, each containing a C-dimensional representation of a region of the input image. This reshape arranges these N tokens linearly, significantly distorting which parts of the image are nearby in two dimensions. This behavior is illustrated in the sub-figure at the left of <ref type="figure" target="#fig_5">Figure 3</ref>.</p><p>We argue that this is the reason that one-dimensional sparsifications are not ideal for images. In fact, the authors of <ref type="bibr" target="#b5">[6]</ref> mention that the Fixed Pattern <ref type="figure" target="#fig_4">(Figure 2a</ref>) was designed for text-sequences and not for images. Our central finding is that these patterns can work very well for images, if their two dimensional structure is correctly considered.</p><p>The question is therefore how to take two-dimensional locality into account. We could create two-dimensional attention patterns directly on a grid but this would have significant computational overhead and also prevent us from extending one dimensional sparsifications that are known to work well <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b5">6]</ref>. Instead, we modify one dimensional sparsifications to become aware of two-dimensional locality with the following trick: (i) we enumerate pixels of the image based on their Manhattan distance from the pixel at location (0, 0) (breaking ties using row priority), (ii) shift the indices of any given one-dimensional sparsification to match the Manhattan distance enumeration instead of the reshape enumeration, and (iii) apply this new one dimensional sparsification pattern, that respects two-dimensional locality, to the one-dimensional reshaped version of the image. We call this procedure ESA (Enumerate, Shift, Apply) and illustrate it in <ref type="figure" target="#fig_5">Figure 3</ref>.</p><p>The ESA trick introduces some distortion compared to a true two-dimensional distance. We found however that this was not too limiting, at least for 128 × 128 resolution. On the other hand, ESA offers an important implementation advantage: it theoretically allows the use of one-dimensional block-sparse kernels <ref type="bibr" target="#b10">[11]</ref>. Currently these kernels exist only for GPUs, but making them work for TPUs is still under development.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Validation</head><p>We conduct experiments on the challenging Ima-geNet <ref type="bibr" target="#b20">[21]</ref> dataset. We choose SAGAN [26] as the baseline for our models because, unlike BigGAN <ref type="bibr" target="#b3">[4]</ref> it has official open-source Tensorflow code. BigGAN is not open-source and therefore training or modifying this architecture was not   (e) Information Flow Graph associated with LTR. This pattern has Full Information, i.e. there is a path between any node of V 0 and any node of V 2 . Note that the number of edges is only increased by a constant compared to the Fixed Attention Pattern <ref type="bibr" target="#b5">[6]</ref>, illustrated in 2d.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(f) Information Flow Graph associated with</head><p>RTL. This pattern also has Full Information. RTL is a "transposed" version of LTR, so that local context at the right of each node is attended at the first step. the different boolean masks that we apply to each of the two steps. Color of cell [i. j] indicates whether node i can attend to node j. With dark blue we indicate the attended positions in both steps. With light blue the positions of the first mask and with green the positions of the second mask. The yellow cells correspond to positions that we do not attend to any step (sparsity). The second row illustrates Information Flow Graph associated with the aforementioned attention masks. An Information Flow Graph visualizes how information "flows" in the attention layer. Intuitively, it visualizes how our model can use the 2-step factorization to find dependencies between image pixels. At each multipartite graph, the nodes of the first vertex set correspond to the image pixels, just before the attention. An edge from a node of the first vertex set, V 0 , to a node of the second vertex set, V 1 , means that the node of V 0 can attend to node of V 1 at the first attention step. Edges between V 1 , V 2 illustrate the second attention step. possible <ref type="bibr" target="#b2">3</ref> .</p><p>In all our experiments, we change only the attention layer of SAGAN, keeping all the other hyper-parameters unchanged (the number of parameters is not affected). We trained all models for up to 1,500,000 steps on individual Cloud TPU v3 devices (v3-8) using a 1e −4 learning rate for generator and 4e −4 for the discriminator. For all the models we report the best performance obtained, even if it was obtained at an earlier point during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attention Mechanism</head><p>We start with the Fixed Pattern ( <ref type="figure" target="#fig_4">Figure 2a</ref>) and modify it: First, we create Full Information extensions (Section 3.1), yielding the patterns Left-To-Right (LTR) and Right-To-Left (RTL) <ref type="figure" target="#fig_4">(Figures 2b and 2c</ref> respectively). We implement multi-step attention in parallel using different heads. Since each pattern is a two-step sparsification, this yields 4 attention heads. To encourage image, using the ESA framework. We use the Manhattan distance from the start (0, 0) as a criterion for enumeration. Although there is some distortion due to the projection into 1-D, locality is mostly maintained. diversity of learned patterns, we use each pattern twice, so the total number of heads in our new attention layer is 8. We use our ESA procedure (Section 3.2) to render these patterns aware of two dimensional geometry.</p><p>Non-Square Attention In SAGAN, the query image and the key image in the attention layer have different dimensions. This complicates things, because the sparsification patterns we discuss are designed for self-attention, where the number of query and key nodes is the same. Specifically, for SAGAN the query image is 32 × 32 and the key image is 16 × 16. We deal with this in the simplest possible way: we create masks for the 16 × 16 image and we shift these masks to cover the area of the 32 × 32 image. Results: As shown in <ref type="table" target="#tab_1">Table 1</ref>, YLG-SAGAN (3rd row) outperforms SAGAN by a large margin measured by both FID and Inception score. Specifically, YLG-SAGAN increases Inception score to 57.22 (8.95% improvement) and improves FID to 15.94 (14.53% improvement). Qualitatively, we observe really good-looking samples for categories with simple geometries and homogeneity. Intuitively, a twodimensional locality can benefit importantly categories such as valleys or mountains, because usually the image transitions for these categories are smoother compared to others and thus the dependencies are mostly local.</p><p>Additionally to the significantly improved scores, one important benefit of using YLG sparse layer instead of a dense attention layer, is that we observe significant reduction of the training time needed for the model to reach it's optimal performance. SAGAN reached it's best FID score after more that 1.3 million training steps while YLG-SAGAN reaches its' optimal score after only 865,000 steps (≈ 40% reduction to the training time). <ref type="figure" target="#fig_6">Figure 4</ref> illustrates SAGAN and YLG-SAGAN FID and Inception score as a function of the training time.</p><p>We create two collages to display samples from our YLG version of SAGAN. At the Upper Panel of <ref type="figure" target="#fig_8">Figure 7</ref>, we show dogs of different breeds generated by our YLG-SAN. At the Lower Panel, we use YLG-SAGAN to generate samples from randomly chosen classes of the ImageNet dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Ablation Studies</head><p>Number of Attention Heads The Original SAGAN implementation used a single-headed attention mechanism. In YLG, we use multiple heads to perform parallel multi-step sparse attention. Previous work has shown that multiple heads increased performance for Natural Language Processing tasks <ref type="bibr">[25]</ref>. To understand how multiple heads affect SAGAN performance, we train an 8 head version of SAGAN. The results are reported in the second row of Table 1. Multiple heads actually worsen significantly the performance of the original SAGAN, reducing Inception score from 52.52 to 46.01. We provide a post-hoc interpretation of this result. The image embedding of the query vector of SAGAN has only 32 vector positions. By using 8 heads, each head gets only 4 positions for its' vector representation. Our intuition is that a 4-positions vector representation is not sufficient for effective encoding of the image information for a dense head and that accounts for the decrease in performance. It is important to note that YLG-SAGAN does not suffer from this problem. The reason is that each head is sparse, which means that only attends to a percentage of the positions that dense head attends to. Thus, a smaller vector representation does not worsen performance. Having multiple divergent sparse heads allows YLG layer to discover complex dependencies in the image space throughout the multi-step attention. As it can be seen, YLG-SAGAN converges much faster compared to the baseline. Specifically, we obtain our best FID at step 865k, while SAGAN requires over 1.3M steps to reach its FID performance peak. Comparing peak performance for both models, we obtain an improvement from 18.65 to 15.94 FID, by only changing the attention layer.</p><p>Two-Dimensional Locality As described in Section 3.2 YLG uses the ESA procedure, to adapt 1-D sparse patterns to data with 2-D structure. Our motivation was that gridlocality could help our sparse attention layer to better model local regions. In order to validate this experimentally, we trained a version of YLG without the ESA procedure. We call this model YLG -No ESA. The results are shown in 4th row of <ref type="table" target="#tab_1">Table 1</ref>: without the ESA procedure, the performance of YLG is about the same with the original SAGAN. This experiment indicates that ESA trick is essential for using one-dimensional sparse patterns for grid-structured data. If ESA framework is used, FID improves from 17.47 to 15.94 and Inception score from 51.09 to 57.22, without any other difference in the model architecture. Thus, ESA is a plug-and-play framework that achieves great performance boosts to both FID and Inception score metrics. ESA allows the utilization of fast sparse one-dimensional patterns that were found to work well for text-sequences to be adapted to images, with great performance benefits. In section 5.1, we visualize attention maps to showcase how our model utilizes ESA framework in practice. <ref type="figure" target="#fig_4">2b and 2c</ref> respectively). Our intuition is that using multiple patterns at the same time increases performance because the model will be able to discover dependencies using multiple different paths. To test this intuition, we ran an experiment using the Full Information extension of the Strided <ref type="bibr" target="#b5">[6]</ref> pattern. We choose this pattern because it was found to be effective for modeling images <ref type="bibr" target="#b5">[6]</ref> due to its' periodic structure. As with LTR and RTL patterns, we extend the Strided pattern so that it has Full Information 4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sparse Patterns Our YLG layer uses the LTR and RTL patterns (Figures</head><p>We refer to the YLG model that instead of LTR and RTL patterns, has 8 heads implementing the Strided pattern as YLG -Strided. For our experiment, we use again the ESA trick. We report the results on the 5th row of <ref type="table" target="#tab_1">Table 1</ref>. YLG -Strided importantly surpasses SAGAN both in FID and Inception score, however, it is still behind YLG. Although in the Sparse Transformers <ref type="bibr" target="#b5">[6]</ref> it has been claimed that strided pattern is more suitable for images than the patterns we use in YLG, this experiment strongly suggests that it is the gridlocality which makes the difference, as both models are far better than SAGAN. Also, this experiment indicates that multiple sparse patterns can boost performance compared to using a single sparse pattern. To be noted, using multiple different patterns at the same attention layer requires scaling the number of heads as well. Although YLG variations of SAGAN were not impacted negatively by the increase of attention heads, more severe up-scaling of the number of heads could potentially harm performance, similarly to how 8 heads harmed performance of SAGAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Inverting Generative Models with Attention</head><p>We are interested in visualizing our sparse attention on real images, not just generated ones. This leads naturally to the problem of projecting an image on the range of a generator, also called inversion. Given a real image x ∈ R n and a generator G(z), inversion corresponds to finding a latent variable z * ∈ R k , so that G(z * ) ∈ R n approximates the given image x as well as possible. One approach for inversion is to try to solve the following non-convex optimization problem: argmin</p><formula xml:id="formula_4">z * { G(z * ) − x 2 }.<label>(1)</label></formula><p>To solve this optimization problem, we can perform gradient descent from a random initalization z 0 to minimize this projection distance in the latent space. This approach was introduced independently in several papers <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b19">20]</ref> and further generalized to solve inverse problems beyond inversion <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b13">14]</ref>. Very recent research <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b22">23]</ref> demonstrated that for fully connected generators with random weights and sufficient layer expansion, gradient descent will provably converge to the correct optimal inversion.</p><p>Unfortunately, this theory does not apply for generators that have attention layers. Even empirically, inversion by gradient descent fails for bigger generative models like SAGAN and YLG-SAGAN. As we show in our experiments the optimizer gets trapped in local minimima producing reconstructions that only vaguely resemble the target image. Other approaches for inversion have been tried in the literature, like training jointly an encoder <ref type="bibr" target="#b8">[9]</ref> but none of these methods have been known to successfully invert complex generative models with attention layers.</p><p>We propose a novel inversion method that uses the discriminator to solve the minimization problem in an different representation space. Interestingly, the discriminator yields representations with a smoother loss landscape, especially if we use the attention layer in a special way. In more detail:</p><p>We begin with a random latent variable z and a given real image x. We denote with D 0 the Discriminator network up to, but not including, the attention layer and obtain the representations D 0 (G(z)) and D 0 (x). We could perform gradient descent to minimize the distance of these discriminator representations:</p><formula xml:id="formula_5">D 0 (G(z)) − D 0 (x) 2 .</formula><p>We found, however, that we can use the attention map of the real image to further enhance inversion. We will use the example of the SAGAN architecture to illustrate this. Inside the SAGAN Discriminator's attention, an attention map M ∈ R 32×32×16×16 is calculated. For each pixel of the 32 × 32 image, this attention map is a distribution over the pixels of the 16 × 16 image. We can use this attention map to extract a saliency map. For each pixel of the 16 × 16 image, we can average the probabilities from all the pixels of the 32 × 32 image and create a probability distribution of shape 16 × 16. We denote this distribution with the letter S. Intuitively, this distribution represents how important each pixel of the image is to the discriminator.</p><p>Our proposed inversion algorithm is to perform gradient descent to minimize the discriminator embedding distance, weighted by these saliency maps:</p><formula xml:id="formula_6">D 0 (G(z)) − D 0 (x) · S 2 ,<label>(2)</label></formula><p>where S is a projected version of saliency map S to the dimensions of D 0 (x). We actually calculate one saliency map S per head and use their sum as the final loss function that we optimize for inversion. More details are included in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Inversion as lens to attention</head><p>Given an arbitrary real image, we can now solve for a z yielding a similar generated image from the generator, and visualize the attention maps.</p><p>We explain our approach using an example of a real image of a redshank <ref type="figure">(Figure 5a</ref>). <ref type="figure">Figure 5b</ref> shows how the standard method for inverting generators <ref type="bibr" target="#b2">[3]</ref> fails: the beak, legs, and rocks are missing. <ref type="figure">Figure 5c</ref> shows the result of our method. Using the z that we found using inversion, we can project maps of the attention layer back to the original image to get valuable insight into how the YLG layers work.</p><p>First, we analyze the differences between the YLG-SAGAN attention heads. For each attention head of the generator, we create a saliency map as described above and use these maps to analyze the attention mechanism. As shown in <ref type="figure">Figure 5d</ref>, the head-7 in the generator is mostly ignoring background focusing on the bird. Other heads function differently: The saliency map of head-2 ( <ref type="figure">Figure 5e</ref>) shows that this head attends globally. We also find that there are heads that that attend quite sparsely, for example, head-5 attends only to 5-6 background pixels.</p><p>We present a second inversion, this time an indigo bird ( <ref type="figure">Figure 6a</ref>). <ref type="figure">Figure 6b</ref> shows how the standard method <ref type="bibr" target="#b2">[3]</ref> for inverting fails: the head of the bird and the branch are not reconstructed. We also illustrate where specific query points attend to. We first illustrate that the the model exploited the local bias of ESA: We plot the attention map for query point (0, 0) for generator-head-0. This point, indicated with a blue dot, is part of the background. We clearly see a local bias in the positions this point attends to. Another example of two-dimensional local attention is shown in <ref type="figure">Figure 6e</ref>. This figure illustrates the attention map of generator-head-4 for a query point on the body of the bird (blue dot). This point attends to the edges of the bird body and to the bird head.</p><p>Finally, <ref type="figure">Figure 6f</ref> shows that there are query points that attend to long-distance, demonstrating that the attention mechanism is capable of exploiting both locality and longdistance relationships when these appear in the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Related Work</head><p>There has been a flourishing of novel ideas on making attention mechanisms more efficient. Dai et al. <ref type="bibr" target="#b6">[7]</ref> separate inputs into chunks and associate a state vector with previous chunks of the input. Attention is performed per chunk, but information exchange between chunks is possible via the state vector. Guo et al. <ref type="bibr" target="#b11">[12]</ref> show that a star-shaped topology can reduce attention cost from O(n 2 ) to O(n) in text sequences. Interestingly, this topology does have full information, under our framework. Sukhbaatar et al. introduced the idea of a learnable adaptive span for each attention layer. Calian et al. <ref type="bibr" target="#b4">[5]</ref> proposed a fast randomized algorithm that exploits spatial coherence and sparsity to design sparse approximations. We believe that all these methods can be possibly combined with YLG, but so far nothing has been demonstrated to improve generative models in a plug-and-play way that this work shows.</p><p>There is also prior work on using attention mechanisms to model images: One notable example is Zhang et al. <ref type="bibr">[26]</ref>, which we have discussed extensively and which adds a selfattention mechanism to GANs. See also Parmar et al. <ref type="bibr" target="#b16">[17]</ref>, which uses local-attention that is not multi-step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions and Future Work</head><p>We introduced a new type of local sparse attention layer designed for two-dimensional data. We believe that our layer will be widely applicable for any model with attention that works on two-dimensional data. An interesting future direction is the design of attention layers, thought of as multi-step networks. The two conflicting objectives are to make these networks as sparse as possible (for computational and statistical efficiency) but also support good information flow. We introduced information flow graphs as a mathematical abstraction and proposed full information as a desired criterion for such attention networks.</p><p>Finally, we presented a novel way to solve the inversion problem for GANs. Our technique uses the discriminator in two ways: First, using its attention to obtain pixel importance and second, as a smoothing representation of the inversion loss landscape. This new inversion method allowed us to visualize our network on approximations of real images and also to test how good a generative model is in this important coverage task. We believe that this is the first key step towards using generative models for inverse problems and we plan to explore this further in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Acknowledgments</head><p>We would like to wholeheartedly thank the TensorFlow Research Cloud (TFRC) program that gave us access to v3-8 Cloud TPUs and GCP credits to train our models on Ima-geNet.   9. Appendix</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1.">A closer look to our inversion method</head><p>This subsection aims to explain technical details of our inversion technique and clarify the details of our approach.</p><p>We begin with a recap of our method. Given a real image we pass it to the discriminator and we extract the attention map from the attention layer. This attention map contains for every point of the query image, a probability distribution over the pixels of the key image. We can then convert this attention map to a saliency map: by averaging the attention each key point gets from all the query points, we can get a probability distribution over the "importance" of the pixels of the key image. We denote this saliency map with S. Our proposed inversion algorithm is to perform gradient descent to minimize the discriminator embedding distance, weighted by this salience map:</p><formula xml:id="formula_7">D 0 (G(z)) − D 0 (x) · S 2 ,</formula><p>where S is a projected version of saliency map S, x is the image, and D 0 is the Discriminator network up to, but not including, the attention layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1.1">Multiple heads and saliency map</head><p>There are some practical considerations that we need to address before illustrating that our inversion method indeed works: the most important of which is how the saliency map S looks like.</p><p>In our analysis of the YLG attention layers, we explain that because of the Full Information property, our patterns are able, potentially, to discover a dependency between any two pixels of an image. If that is true, we should expect that in the general case our saliency map, generated by the average of all heads, allocates non-zero weights to all image pixels. The important question becomes whether this joint saliency map weights more the pixels that are important for a visually convincing inversion. For example, in case of a bird flying with a blue-sky in the background, we should be ready to accept a small error in some point in the clouds of the sky but not a bird deformation that will make the inverted image look unrealistic. Therefore, our saliency map should allocate more weight in the bird than in it allocates in the background sky.</p><p>We already showed in Section 5.1 that different heads specialize in discovering important image parts (for example, some heads learn to focus on local neighbhoords, important shape edges, background, etc.) so extracting a saliency map S by averaging all heads usually leads in a uniform distribution over pixels, which is not helping inversion. <ref type="figure" target="#fig_10">Figure 8b</ref> shows the saliency map jointly all heads of the attention layer of the discriminator produce. Although the bird receives a bit more attention than the background, it is not clear how this map would help weight our loss for inversion. However, as illustrated in 8c, there are heads that produce far more meaningful saliency maps for a good-looking inversion. There is a drawback here as well though; if we use that head only, we completely miss the background.</p><p>To address this problem, we find two solutions that work quite well.</p><p>• Solution 1: calculate Equation 2 separately for each head and then add the losses. In that case, the new loss function is given by the following equation:</p><formula xml:id="formula_8">i D 0 (G(z)) − D 0 (x) · S i 2 ,<label>(3)</label></formula><p>where S i is the saliency map extracted from head i.</p><p>• Solution 2: Examine manually the saliency maps for each head and remove the heads that are attending mainly to non-crucial for the inversion areas, such as homogeneous backgrounds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1.2">More inversion visualizations</head><p>We present several inversions for different categories of real images at <ref type="figure" target="#fig_11">Figure 9</ref>. In all our Figures, we use Solution 1 as it has the advantage that it does not require human supervision.</p><p>With our method, we can effectively invert real world scenes. We tested the standard inversion method <ref type="bibr" target="#b2">[3]</ref> for these images as well and the results were far less impressive for all images. Especially for the dogs, we noted complete failure of the previous approach, similar to what we illustrate in <ref type="figure" target="#fig_0">Figure 11</ref>. is more likely to help correct inversion of the bird. We can use saliency maps from other heads to invert the background as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1.3">Experiments setup</head><p>In this subsection, we will briefly describe the experimental setup for our inversion technique. We choose to use the recently introduced Lookahead [28] optimizer as we find that it reduces the number of different seeds we have to try for a successful inversion. For the vast majority of the examined real images, we are able to get a satisfying inversion by trying at most 4 different seeds. We set the learning rate to 0.05 and we update for maximum 1500 steps. On a single V100 GPU, a single image inversion takes less than a minute to complete. We choose to invert real-world images that were not present in the training set. We initialize our latent variables from a truncated normal distribution, as explained in 9.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2.">Truncation and how it helps inversion</head><p>In the BigGAN <ref type="bibr" target="#b3">[4]</ref> paper, the authors observed that latent variables sampled from a truncated normal distribution generated generally more photo-realistic images compared to ones generated from the normal distribution which was used during the training. This so-called truncation trick (resampling the values with magnitude above a chosen threshold) leads to improvement in sample quality at the cost of reduction in sample variety. For the generated images of YLG presented in this paper, we also utilized this trick.</p><p>Interestingly, the truncation trick can help inversion as well under some conditions. If the original image has good quality, then according to the truncation trick, it is more probable to be generated by a latent variable sampled from a truncated normal (where values which fall outside a range are resampled to fall inside that range) than the standard normal distribution N (0, I). For that reason, in our inversions we start our trainable latent variable from a sample of the truncated normal distribution. We found experimentally that setting the truncation threshold to two standard deviations from the median (in our case 0), is a good trade-off between producing photo-realistic images and having enough diversity to invert an arbitrary real world image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.3.">Strided Pattern</head><p>In the ablation studies of our paper, we train a model we name YLG -Strided. For this model, we report better results than the baseline SAGAN [26] model and slightly worse results than the proposed YLG model. The purpose of this section is to give more information on how YLG and YLG -Strided differ.</p><p>First of all, the only difference between YLG and YLG Strided is the choosing of attention masks for the attention heads: both models implement 2-step attention patterns with Full Information and two-dimensional locality using the ESA framework.</p><p>YLG model uses the RTL and LTR patterns introduced in the paper (see <ref type="figure" target="#fig_4">Figures 2c, 2b)</ref>. Each pattern corresponds to a two-step attention: in our implementation of multi-step attention we compute steps in parallel using multiple heads, so in total we need 8 attention heads for YLG. In YLG -Strided instead of using different patterns (RTL and LTR), we stick with using a single attention pattern. Our motivation is to: (i) investigate whether using multiple attention patterns simultaneously affects performance, (ii) discover whether the performance differences between onedimensional sparse patterns reported in the literature remain when the patterns are rendered to be aware of twodimensional geometry. To explore (i), (ii) a natural choice was to work with the Strided pattern proposed in Sparse Transformers <ref type="bibr" target="#b5">[6]</ref> as it was found to be (i) effective for modeling images and (ii) more suitable than the Fixed pattern (see <ref type="figure" target="#fig_4">Figure 2a</ref>), on which we built to invent LTR, RTL.</p><p>We illustrate the Strided pattern, as proposed in Sparse Transformers <ref type="bibr" target="#b5">[6]</ref>, in <ref type="figure" target="#fig_0">Figures 10a, 10c</ref>. For a fair comparison with LTR, RTL we need to expand Strided pattern in order for it to have Full Information. <ref type="figure" target="#fig_0">Figures 10b, 10d</ref> illustrate this expansion. The pattern illustrated in this Figure is exactly the pattern that YLG -Strided uses. Note that this pattern attends to the same order of positions, O(n √ n), as LTR and RTL. For one to one comparison with YLG, YLG -Strided has also 8 heads: the Full Information pattern is implemented 4 times, as we need 2 heads for a 2-step pattern. As already mentioned, we also use ESA framework for YLG -Strided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.4.">Things that did not work</head><p>In this subsection, we present several ideas, relevant to the paper, that we experimented on and found that their re-sults were not satisfying. Our motivation is to inform the research community about the observed shortcomings of these approaches so that other researchers can re-formulate them, reject them or compare their findings with ours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.4.1">Weighted inversion at the generator space</head><p>We already discussed that our key idea for the inversion: we pass a real image to the discriminator, extract the attention map, convert the attention map to a saliency distribution S and we perform gradient descent to minimize the discrimi- This pattern has Full Information, i.e. there is a path between any node of V 0 and any node of V 2 . Note that the number of edges is only increased by a constant compared to the Strided Attention Pattern <ref type="bibr" target="#b5">[6]</ref>, illustrated in 10a.  <ref type="bibr" target="#b5">[6]</ref> and the YLG -Strided pattern which has Full Information. First row demonstrates the different boolean masks that we apply to each of the two steps. Color of cell [i. j] indicates whether node i can attend to node j. With dark blue we indicate the attended positions in both steps. With light blue the positions of the first mask and with green the positions of the second mask. The yellow cells correspond to positions that we do not attend to any step (sparsity). The second row illustrates Information Flow Graph associated with the aforementioned attention masks. An Information Flow Graph visualizes how information "flows" in the attention layer. Intuitively, it visualizes how our model can use the 2-step factorization to find dependencies between image pixels. At each multipartite graph, the nodes of the first vertex set correspond to the image pixels, just before the attention. An edge from a node of the first vertex set, V 0 , to a node of the second vertex set, V 1 , means that the node of V 0 can attend to node of V 1 at the first attention step. Edges between V 1 , V 2 illustrate the second attention step. nator embedding distance, weighted by this saliency map:</p><formula xml:id="formula_9">D 0 (G(z)) − D 0 (x) · S 2 ,</formula><p>where S is a projected version of saliency map S, x is the image, and D 0 is the Discriminator network up to, but not including, the attention layer. In practise, we use Equation 3 for the reasons we explained in Section 9.1 but for the rest of this Section we will overlook this detail as it is not important for our point. (c) Weighted inversion at Generator.</p><p>(d) Inversion using the standard method <ref type="bibr" target="#b2">[3]</ref>. <ref type="figure" target="#fig_0">Figure 11</ref>: Inversion with different methods of the real image of 11a. Our method, 11b, is the only successful inversion. The inversion using the weights from the saliency map to the output of the Generator, 11c, fails badly. The same holds for inversion using the standard method in the literature <ref type="bibr" target="#b2">[3]</ref>, as shown in 11d.</p><p>could perform gradient descent on:</p><formula xml:id="formula_10">(G(z) − x) · S 2 ,<label>(4)</label></formula><p>where S is a projected version of S to match the dimensions of the Generator network.</p><p>In our experiments, we find that this approach generally leads to inversions of poor quality. To illustrate this, we present inversions of an image of a real husky from the the weighted generator inversion, the weighted discriminator inversion and standard inversion method <ref type="bibr" target="#b2">[3]</ref> at <ref type="figure" target="#fig_0">Figure  11</ref>.</p><p>There are several reasons that could explain the quality gap when we change from inversion to the space of the Discriminator to that of the Generator. First of all, the saliency map we use to weight our loss is extracted from the Discriminator, which means that the weights reflect what the Discriminator network considers important at that stage. Therefore, it is reasonable to expect that this saliency map would be more accurate to describe what is important for the input of the attention of the discriminator than to the output of the Generator. Also note that due to the layers of the Discriminator before the attention, the images of the output of the generator and the input of the attention of the Discriminator can be quite different. Finally, the Discriminator may provide an "easier" embedding space for inversion. The idea of using a different embedding space than the output of the Generator it is not new; activations from VGG16 <ref type="bibr" target="#b21">[22]</ref> have also been used for inversion <ref type="bibr" target="#b1">[2]</ref>. Our novelty is that we use the Discriminator instead of another pre-trained model to work on a new embedding space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.4.2">Combination of dense and sparse heads</head><p>In our paper, we provide strong experimental evidence that multi-step two-dimensional sparse local heads can be more efficient than the conventional dense attention layer. We justify this evidence theoretically by modelling the multi-step attention with Information Flow Graphs and indicating the implications of Full Information. Naturally, one might wonder what would happen if we combine YLG attention with dense attention. To answer this question, we split heads into two groups, the local -sparse heads and the dense ones. Specifically, we use 4 heads that implement the RTL, LTR patterns (see paper for more details) and 4 dense heads and we train this variation of SAGAN. We use the same setup as with our other experiments. We report FID 19.21 and Inception: 51.23. These scores are far behind than the scores of YLG and thus we did not see any benefit continuing the research in this direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.4.3">Different resolution heads</head><p>One idea we believed it would be interesting was to train SAGAN with a multi-headed dense attention layer of different resolution heads. In simple words, that means that in this attention layer some heads have a wider vector representation than others. Our motivation was that the different resolutions could have helped enforcing locality in a different way; we expected the heads with the narrow hidden representations to learn to attend only locally and the wider heads to be able to recover long-range dependencies.</p><p>In SAGAN, the number of channels in the query vector is 32, so for an 8-head attention layer normally each head would get 4 positions. We split the 8 heads into two equal groups: the narrow and the wide heads. In our experiment, narrow heads get only 2 positions for their vector representation while wide heads get 6. After training on the same setup with our other experiments, we obtain FID 19.57 and Inception score: 50.93. These scores are slightly worse than the original SAGAN, but are far better than SAGAN with dense 8-head attention which achieved FID 20.09 and Inception 46.01, as mentioned in the ablation study.</p><p>At least in our preliminary experiments, different resolution heads were not found to help very much. Perhaps they can be combined with YLG attention but we more research would be needed in this direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.5.">Information Flow Graphs</head><p>We found that thinking about sparse attention as a network with multiple stages is helpful in visualizing how information of different tokens is attended and combined. We use Information Flow Graphs (IFGs) that were introduced in <ref type="bibr" target="#b7">[8]</ref> for modeling how distributed storage codes preserve data. In full generality, IFGs are directed acyclic graphs with capacitated directed edges. Each storage node is represented with two copies of a vertex (x in and x out ) connected by a directed edge with capacity equal to the amount of information that can be stored into that node. The key insight is that a multi-stage attention network can be considered a storage network since intermediate tokens are representing combinations of tokens at the previous stage. The IFGs we use in this paper are a special case: every token of every stage of an attention layer is represented by a storage node. Since all the tokens have the same size, we can eliminate vertex splitting and compactly represent each storage node by a single vertex, as shown in <ref type="figure" target="#fig_0">Figure 10d</ref>.</p><p>Full information is a design requirement that we found to be helpful in designing attention networks. It simply means that any single input token is connected with a directed path to any output token and hence information (of entropy equal to one token representation) can flow from any one input into any one output. As we discussed in the paper, we found that previously used sparse attention patterns did not have this property and we augmented them to obtain the patterns we use. A stronger requirement would be that any pair of input nodes is connected to any pair of output nodes with two edge-disjoint paths. This would mean that flow of two tokens can be supported from any input to any output. Note that a fully connected network can support this for any pair or even for any set of k input-output pairs for ∀k ≤ n.</p><p>An interesting example is the star transformer <ref type="bibr" target="#b11">[12]</ref> where all n input tokens are connected to a single intermediate node which is then connected to all output tokens. This information flow graph has 2n directed edges and can indeed support full information. However, it cannot support a flow of 2 tokens for any pair, since there is a bottleneck at the intermediate node. We believe that enforcing good information flow for pairs or higher size sets improves the design of attention networks and we plan to investigate this further in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.6.">Generated images</head><p>We present some more generated images from our YLG SAGAN. The images are divided per category and are presented in <ref type="figure" target="#fig_0">Figures 12, 13</ref>, 14.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Samples generated by our model YLG-SAGAN after training on ImageNet. The images are visually significantly better compared to the SAGAN baseline, as also supported by FID and Inception score metrics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a) Attention masks for Fixed Pattern<ref type="bibr" target="#b5">[6]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(b) Attention masks for Left To Right (LTR) pattern.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(c) Attention masks for Right To Left (RTL) pattern.(d) Information Flow Graph associated withFixed Pattern. This pattern does not have Full Information, i.e. there are dependencies between nodes that the attention layer cannot model. For example, there is no path from node 0 of V 0 to node 1 of V 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>This Figure illustrates the different 2-step sparsifications of the attention layer we examine in this paper. First row demonstrates</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Reshape and ESA enumerations of the cells of an image grid that show how image grid is projected into a line. (Left) Enumeration of pixels of an 8 × 8 image using a standard reshape. This projection maintains locality only in rows. (Right) Enumeration of pixels of an 8 × 8</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Training comparison for YLG-SAGAN and SAGAN. We plot every 200k steps the Inception score (a) and the FID (b) of both YLG-SAGAN and SAGAN, up to 1M training steps on ImageNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Inversion and Saliency maps for different heads of the Generator network. We emphasize that this image of a redshank bird was not in the training set, it is rather obtained by a Google image search. Saliency is extracted by averaging the attention each pixel of the key image gets from the query image. We use the same trick to enhance inversion.(a) A real image of a redshank. (b) A demonstration of how the standard inversion method [3] fails. (c) The inverted image for this redshank, using our technique. (d) Saliency map for head 7. Attention is mostly applied to the bird body. (e) Saliency map for head 2. This head attends almost everywhere in the image. Inverted image of an indigo bird and visualization of the attention maps for specific query points. (a) The original image. Again, this was obtained with a Google image search and was not in the training set. (b) Shows how previous inversion methods fail to reconstruct the head of the bird and the branch. (c) A successful inversion using our method. (d) Specifically, 6d shows how attention uses our ESA trick to model background, homogeneous areas. (e) Attention applied to the bird. (f) Attention applied with a query on the branch. Notice how attention is non-local and captures the full branch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Upper Panel: YLG conditional image generation on different dog breeds from ImageNet dataset. From up to down: eskimo husky, siberian husky, saint bernard, maltese. Lower Panel: Random generated samples from YLG-SAGAN. Additional generated images are included in the Appendix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>[</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>(a) Real image of a redshank. (b) Saliency map extracted from all heads of the Discriminator. (c) Saliency map extracted from a single head of the Discriminator. Weighting our loss function with (b) does not have a huge impact, as the attention weights are almost uniform. Saliency map from (c)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 :</head><label>9</label><figDesc>More inversions using our technique. To the left we present real images and to the right our inversions using YLG SAGAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>(a) Attention masks for Strided Pattern<ref type="bibr" target="#b5">[6]</ref>.(b) Attention masks for YLG -Strided (Extended Strided with Full Information property) (c) Information Flow Graph associated with Strided Pattern. This pattern does not have Full Information, i.e. there are dependencies between nodes that the attention layer cannot model. For example, there is no path from node 2 of V 0 to node 1 of V 2 . (d) Information Flow Graph associated with YLG -Strided pattern.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 10 :</head><label>10</label><figDesc>This Figure illustrates the original Strided Pattern</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Equation 2</head><label>2</label><figDesc>implies that the inversion takes place in the embedding space of the Discriminator. However, naturally one might wonder if we could use the saliency map S to weight the inversion of the Generator, in other words, if we (a) Real image.(b) Inversion with our method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 12 :</head><label>12</label><figDesc>Generated images from YLG SAGAN divided by ImageNet category.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 13 :</head><label>13</label><figDesc>Generated images from YLG SAGAN divided by ImageNet category.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 14 :</head><label>14</label><figDesc>Generated images from YLG SAGAN divided by ImageNet category.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>updates the vector representation of X by integrating the vector representation of Y . In this paper, X, Y are intermediate image representations.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Thus every 16 × 16 block of the 32 × 32 query image attends with full information to the 16 × 16 key image. ImageNet Results: Table of results after training SAGAN and YLG-SAGAN on ImageNet. Table also includes Ablation Studies (SAGAN 8 heads, YLG -No ESA, YLG -Strided). Our best model, YLG, achieves 15.94 FID and 57.<ref type="bibr" target="#b21">22</ref> Inception score. Our scores correspond to 14.53% and 8.95% improvement to FID and Inception respectively. We emphasize that these benefits are obtained by only one layer change to SAGAN, replacing dense attention with the local sparse attention layer that we introduce.</figDesc><table><row><cell></cell><cell cols="2"># Heads FID</cell><cell>Inception</cell></row><row><cell>SAGAN</cell><cell>1</cell><cell cols="2">18.65 52.52</cell></row><row><cell>SAGAN</cell><cell>8</cell><cell cols="2">20.09 46.01</cell></row><row><cell>YLG-SAGAN</cell><cell>8</cell><cell cols="2">15.94 57.22</cell></row><row><cell cols="2">YLG -No ESA 8</cell><cell cols="2">17.47 51.09</cell></row><row><cell>YLG -Strided</cell><cell>8</cell><cell cols="2">16.64 55.21</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>24] Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, and Armand Joulin. Adaptive Attention Span in Transformers. arXiv e-prints, page arXiv:1905.07799, May 2019. 1, 7 [25] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention Is All You Need.</figDesc><table><row><cell>arXiv e-prints, page</cell></row><row><cell>arXiv:1706.03762, Jun 2017. 1, 2, 5</cell></row><row><cell>[26] Han Zhang, Ian Goodfellow, Dimitris Metaxas, and Augus-</cell></row><row><cell>tus Odena. Self-Attention Generative Adversarial Networks.</cell></row><row><cell>arXiv e-prints, page arXiv:1805.08318, May 2018. 1, 2, 3,</cell></row><row><cell>8, 11</cell></row><row><cell>[27] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiao-</cell></row><row><cell>gang Wang, Xiaolei Huang, and Dimitris Metaxas. Stack-</cell></row><row><cell>GAN: Text to Photo-realistic Image Synthesis with Stacked</cell></row><row><cell>Generative Adversarial Networks. arXiv e-prints, page</cell></row><row><cell>arXiv:1612.03242, Dec 2016. 1</cell></row><row><cell>[28] Michael R. Zhang, James Lucas, Geoffrey Hinton, and</cell></row><row><cell>Jimmy Ba. Lookahead Optimizer: k steps forward, 1 step</cell></row><row><cell>back. arXiv e-prints, page arXiv:1907.08610, Jul 2019. 11</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The code for our experiments is based on the tensorflow-gan library.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Note that there is an 'unofficial' BigGAN that is open in PyTorch. However, that implementation uses gradient checkpointing and requires 8 V100 GPUS for 15 days to train. We simply did not have such computing resources. We believe, however, that YLG can be easily combined with BigGAN (by simply replacing its dense attention layer) and will yield an even better model.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We include visualizations of the Full Information Strided Pattern in the Appendix.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolf</forename><surname>Ahlswede</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S-Yr</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">W</forename><surname>Yeung</surname></persName>
		</author>
		<title level="m">Network information flow. IEEE Transactions on information theory</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="1204" to="1216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Seeing What a GAN Cannot Generate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Peebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hendrik</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.11626</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Compressed sensing using generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Bora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajil</forename><surname>Jalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros G</forename><surname>Dimakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Large Scale GAN Training for High Fidelity Natural Image Synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.11096</idno>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><forename type="middle">A</forename><surname>Calian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Roelants</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacques</forename><surname>Cali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><surname>Dubba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">E</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dell</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.10308</idno>
		<title level="m">SCRAM: Spatially Coherent Randomized Attention Maps. arXiv e-prints</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10509</idno>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.02860</idno>
		<title level="m">Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context. arXiv e-prints</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Network coding for distributed storage systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alexandros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P Brighten</forename><surname>Dimakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunnan</forename><surname>Godfrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kannan</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ramchandran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on information theory</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Adversarial feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.09782</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2661</idno>
		<title level="m">Generative Adversarial Networks. arXiv eprints</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik P</forename><surname>Kingma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.09224</idno>
		<title level="m">Gpu kernels for block-sparse weights</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qipeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunfan</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09113</idno>
		<title level="m">Xiangyang Xue, and Zheng Zhang. Star-Transformer. arXiv e-prints</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Global guarantees for enforcing deep generative priors by empirical risk</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Hand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladislav</forename><surname>Voroninski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Task-aware compressed sensing with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maya</forename><surname>Kabkab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pouya</forename><surname>Samangouei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">A Style-Based Generator Architecture for Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.04948</idno>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Precise recovery of latent vectors from generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zachary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subarna</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tripathi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.04782</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05751</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Image transformer. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<title level="m">Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks. arXiv e-prints</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Gan-based projector for faster recovery with convergence guarantees in linear inverse problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Bresler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">One network to solve them all-solving linear inverse problems using deep projection models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rick</forename><surname>Jh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnabas</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aswin C</forename><surname>Bvk Vijaya Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sankaranarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very Deep Convolutional Networks for Large-Scale Image Recognition. arXiv e-prints</title>
		<imprint>
			<date type="published" when="2014-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Surfing: Iterative optimization over incrementally trained deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganlin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Zhou Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lafferty</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.08653</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
