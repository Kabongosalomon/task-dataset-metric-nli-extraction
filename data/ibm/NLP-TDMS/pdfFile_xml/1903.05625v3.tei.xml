<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Tracking without bells and whistles</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Bergmann</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Munich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Meinhardt</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Munich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taixe</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Munich</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Tracking without bells and whistles</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The problem of tracking multiple objects in a video sequence poses several challenging tasks. For tracking-bydetection, these include object re-identification, motion prediction and dealing with occlusions. We present a tracker (without bells and whistles) that accomplishes tracking without specifically targeting any of these tasks, in particular, we perform no training or optimization on tracking data. To this end, we exploit the bounding box regression of an object detector to predict the position of an object in the next frame, thereby converting a detector into a Tracktor. We demonstrate the potential of Tracktor and provide a new state-of-the-art on three multi-object tracking benchmarks by extending it with a straightforward re-identification and camera motion compensation.</p><p>We then perform an analysis on the performance and failure cases of several state-of-the-art tracking methods in comparison to our Tracktor. Surprisingly, none of the dedicated tracking methods are considerably better in dealing with complex tracking scenarios, namely, small and occluded objects or missing detections. However, our approach tackles most of the easy tracking scenarios. Therefore, we motivate our approach as a new tracking paradigm and point out promising future research directions. Overall, Tracktor yields superior tracking performance than any current tracking method and our analysis exposes remaining and unsolved tracking challenges to inspire future research directions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Scene understanding from video remains one of the big challenges of computer vision. Humans are often the center of attention in a scene, which leads to the fundamental problem of detecting and tracking them in a video. Tracking-bydetection has emerged as the preferred paradigm to solve the problem of tracking multiple objects as it simplifies the task by breaking it into two steps: (i) detecting object locations independently in each frame, (ii) form tracks by linking corresponding detections across time. The linking step, * Contributed equally. Correspondence to: tim.meinhardt@tum.de or data association, is a challenging task on its own, due to missing and spurious detections, occlusions, and target interactions in crowded environments. To address these issues, research in this area has produced increasingly complex models achieving only marginally better results, e.g., multiple object tracking accuracy has only improved 2.4% in the last two years on the MOT16 <ref type="bibr" target="#b44">[45]</ref> benchmark.</p><p>In this paper, we push tracking-by-detection to the limit by using only an object detection method to perform tracking. We show that one can achieve state-of-the-art tracking results by training a neural network only on the task of detection. As indicated by the blue arrows in <ref type="figure" target="#fig_11">Figure 1</ref>, the regressor of an object detector such as Faster-RCNN <ref type="bibr" target="#b51">[52]</ref> is sufficient to construct object trajectories in a multitude of challenging tracking scenarios. This raises an interesting question that we discuss in this paper: If a detector can solve most of the tracking problems, what are the real situations where a dedicated tracking algorithm is necessary? We hope our work and the presented Tracktor allows researchers to focus on the still unsolved critical challenges of multi-object tracking.</p><p>This paper presents four main contributions:</p><p>• We introduce the Tracktor which tackles multi-object tracking by exploiting the regression head of a detector to perform temporal realignment of object bounding boxes.</p><p>• We present two simple extensions to Tracktor, a reidentification Siamese network and a motion model. The resulting tracker yields state-of-the-art performance in three challenging multi-object tracking benchmarks.</p><p>• We conduct a detailed analysis on failure cases and challenging tracking scenarios, and show none of the dedicated tracking methods perform substantially better than our regression approach.</p><p>• We propose our method as a new tracking paradigm which exploits the detector and allows researchers to focus on the remaining complex tracking challenges. This includes an extensive study on promising future research directions.          </p><formula xml:id="formula_0">v I N J w o K Y D C W P O C V g J T / r h R E O p / f j P v R L F b f m L o A 3 i b c i l U a 5 + v 0 0 e y g 2 + 6 X P 3 k D R N G Y S q C D G + J 6 b Q J A R D Z w K N i 3 0 U s M S Q s d k y H x L J Y m Z C b L F y V N 8 Y Z U B j p S 2 J Q E v 1 N 8 T G Y m N m c S h 7 Y w J j M y 6 N x f / 8 / w U o q s g 4 z J J g U m 6 X B S l A o P C 8 / / x g G t G Q U w s I V R z e y u m I 6 I J B Z t S w Y b g r b + 8 S d r 1 m u f W v F u b R h 0 t k U d n 6 B x V k Y c u U Q P d o C Z q I Y o U e k a v 6 M 0 B 5 8 V 5 d z 6 W r T l n N X O K / s C Z / Q A v F J Q Y &lt; / l</formula><formula xml:id="formula_1">v I N J w o K Y D C W P O C V g J T / r h R E O p / f j P v R L F b f m L o A 3 i b c i l U a 5 + v 0 0 e y g 2 + 6 X P 3 k D R N G Y S q C D G + J 6 b Q J A R D Z w K N i 3 0 U s M S Q s d k y H x L J Y m Z C b L F y V N 8 Y Z U B j p S 2 J Q E v 1 N 8 T G Y m N m c S h 7 Y w J j M y 6 N x f / 8 / w U o q s g 4 z J J g U m 6 X B S l A o P C 8 / / x g G t G Q U w s I V R z e y u m I 6 I J B Z t S w Y b g r b + 8 S d r 1 m u f W v F u b R h 0 t k U d n 6 B x V k Y c u U Q P d o C Z q I Y o U e k a v 6 M 0 B 5 8 V 5 d z 6 W r T l n N X O K / s C Z / Q A v F J Q Y &lt; / l</formula><formula xml:id="formula_2">F C E = " &gt; A A A B 7 H i c b V A 9 S w N B E J 2 L X z F + x V j a L A Y h V b h L o 2 X A x j K C l w T i G f Y</formula><formula xml:id="formula_3">I q R K B C s H Y w v Z 3 7 7 n i n N Y 3 m D k 4 T 5 E R l K H n J K 0 E i e v h 3 3 s V + q O D V n D n u d u E t S a Z S r 3 w 8 f d 8 V m v / T Z G 8 Q 0 j Z h E K o j W X d d J 0 M + I Q k 4 F m x Z 6 q W Y J o W M y Z F 1 D J Y m Y 9 r P 5 s V P 7 z C g D O 4 y V K Y n 2 X P 0 9 k Z F I 6 0 k U m M 6 I 4 E i v e j P x P 6 + b Y n j h Z 1 w m K T J J F 4 v C V N g Y 2 7 P P 7 Q F X j K K Y G E K o 4 u Z W m 4 6 I I h R N P g U T g r v 6 8 j p p 1 W u u U 3 O v T R p 1 W C A P J 3 A K V X D h H B p w B U 3 w g A K H R 3 i G F 0 t a T 9 a r 9 b Z o z V n L m W P 4 A + v 9 B / d B k b E = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " / W m / 2 P Y v B w f P 1 o N E C N O d Q 0 c s R Z I = " &gt; A A A B 7 H i c b V A 9 T w J B E J 3 D L 0 R R x N L m I j G h I n c 0 W p L Y W G L i A Q m c Z G / Z g w 1 7 e + f u n A m 5 8 A s s b C w 0 x t b K X 2 N n / D M u H 4 W C L 5 n k 5 b 2 Z z M w L E s E 1 O s 6 X l d v Y 3 N r e y e 8 W 9 v a L B 4 e l o 3 J L x 6 m i z K O x i F U n I J o J L p m H H A X r J I q R K B C s H Y w v Z 3 7 7 n i n N Y 3 m D k 4 T 5 E R l K H n J K 0 E i e v h 3 3 s V + q O D V n D n u d u E t S a Z S r 3 w 8 f d 8 V m v / T Z G 8 Q 0 j Z h E K o j W X d d J 0 M + I Q k 4 F m x Z 6 q W Y J o W M y Z F 1 D J Y m Y 9 r P 5 s V P 7 z C g D O 4 y V K Y n 2 X P 0 9 k Z F I 6 0 k U m M 6 I 4 E i v e j P x P 6 + b Y n j h Z 1 w m K T J J F 4 v C V N g Y 2 7 P P 7 Q F X j K K Y G E K o 4 u Z W m 4 6 I I h R N P g U T g r v 6 8 j p p 1 W u u U 3 O v T R p 1 W C A P J 3 A K V X D h H B p w B U 3 w g A K H R 3 i G F 0 t a T</formula><formula xml:id="formula_4">v B F X x j N R 5 a b V q S 2 G q V m U 2 R 0 = " &gt; A A A B 6 H i c b V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 h V 0 R z D H g x W M C 5 g H J E m Y n v c m Y 2 Q c z v U I I + Q I v H h T x 6 i d 5 8 2 + c J H v Q x I K G o q q b 7 q 4 g V d K Q 6 3 4 7 G 5 t b 2 z u 7 h b 3 i / s H h 0 X H p 5 L R l k k w L b I p E J b o T c I N K x t g k S Q o 7 q U Y e B Q r b w f h u 7 r e f U B u Z x A 8 0 S d G P + D C W o R S c r N S g f q n s V t w F 2 D r x c l K G H P V + 6 a s 3 S E Q W Y U x C c W O 6 n p u S P + W a p F A 4 K / Y y g y k X Y z 7 E r q U x j 9 D 4 0 8 W h M 3 Z p l Q E L E 2 0 r J r Z Q f 0 9 M e W T M J A p s Z 8 R p Z F a 9 u f i f 1 8 0 o r P p T G a c Z Y S y W i 8 J M M U r Y / G s 2 k B o F q Y k l X G h p b 2 V i x D U X Z L M p 2 h C 8 1 Z f X S</formula><formula xml:id="formula_5">v B F X x j N R 5 a b V q S 2 G q V m U 2 R 0 = " &gt; A A A B 6 H i c b V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 h V 0 R z D H g x W M C 5 g H J E m Y n v c m Y 2 Q c z v U I I + Q I v H h T x 6 i d 5 8 2 + c J H v Q x I K G o q q b 7 q 4 g V d K Q 6 3 4 7 G 5 t b 2 z u 7 h b 3 i / s H h 0 X H p 5 L R l k k w L b I p E J b o T c I N K x t g k S Q o 7 q U Y e B Q r b w f h u 7 r e f U B u Z x A 8 0 S d G P + D C W o R S c r N S g f q n s V t w F 2 D r x c l K G H P V + 6 a s 3 S E Q W Y U x C c W O 6 n p u S P + W a p F A 4 K / Y y g y k X Y z 7 E r q U x j 9 D 4 0 8 W h M 3 Z p l Q E L E 2 0 r J r Z Q f 0 9 M e W T M J A p s Z 8 R p Z F a 9 u f i f 1 8 0 o r P p T G a c Z Y S y W i 8 J M M U r Y / G s 2 k B o F q Y k l X G h p b 2 V i x D U X Z L M p 2 h C 8 1 Z f X S</formula><formula xml:id="formula_6">v B F X x j N R 5 a b V q S 2 G q V m U 2 R 0 = " &gt; A A A B 6 H i c b V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 h V 0 R z D H g x W M C 5 g H J E m Y n v c m Y 2 Q c z v U I I + Q I v H h T x 6 i d 5 8 2 + c J H v Q x I K G o q q b 7 q 4 g V d K Q 6 3 4 7 G 5 t b 2 z u 7 h b 3 i / s H h 0 X H p 5 L R l k k w L b I p E J b o T c I N K x t g k S Q o 7 q U Y e B Q r b w f h u 7 r e f U B u Z x A 8 0 S d G P + D C W o R S c r N S g f q n s V t w F 2 D r x c l K G H P V + 6 a s 3 S E Q W Y U x C c W O 6 n p u S P + W a p F A 4 K / Y y g y k X Y z 7 E r q U x j 9 D 4 0 8 W h M 3 Z p l Q E L E 2 0 r J r Z Q f 0 9 M e W T M J A p s Z 8 R p Z F a 9 u f i f 1 8 0 o r P p T G a c Z Y S y W i 8 J M M U r Y / G s 2 k B o F q Y k l X G h p b 2 V i x D U X Z L M p 2 h C 8 1 Z f X S</formula><formula xml:id="formula_7">v B F X x j N R 5 a b V q S 2 G q V m U 2 R 0 = " &gt; A A A B 6 H i c b V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 h V 0 R z D H g x W M C 5 g H J E m Y n v c m Y 2 Q c z v U I I + Q I v H h T x 6 i d 5 8 2 + c J H v Q x I K G o q q b 7 q 4 g V d K Q 6 3 4 7 G 5 t b 2 z u 7 h b 3 i / s H h 0 X H p 5 L R l k k w L b I p E J b o T c I N K x t g k S Q o 7 q U Y e B Q r b w f h u 7 r e f U B u Z x A 8 0 S d G P + D C W o R S c r N S g f q n s V t w F 2 D r x c l K G H P V + 6 a s 3 S E Q W Y U x C c W O 6 n p u S P + W a p F A 4 K / Y y g y k X Y z 7 E r q U x j 9 D 4 0 8 W h M 3 Z p l Q E L E 2 0 r J r Z Q f 0 9 M e W T M J A p s Z 8 R p Z F a 9 u f i f 1 8 0 o r P p T G a c Z Y S y W i 8 J M M U r Y / G s 2 k B o F q Y k l X G h p b 2 V i x D U X Z L M p 2 h C 8 1 Z f X S</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Related work</head><p>Several computer vision tasks such as surveillance, activity recognition or autonomous driving rely on object trajectories as input. Despite the vast literature on multiobject tracking <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b37">38]</ref>, it still remains a challenging problem, especially in crowded environments where occlusions and false detections are common. Most state-of-the-art works follow the tracking-by-detection paradigm which heavily relies on the performance of the underlying detection method.</p><p>Recently, neural network based detectors have clearly outperformed all other methods for detection <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b49">50]</ref>. The family of detectors that evolved to Faster-RCNN <ref type="bibr" target="#b51">[52]</ref>, and further detectors such as SDP <ref type="bibr" target="#b62">[63]</ref>, rely on object proposals which are passed to an object classification and a bounding box regression head of a neural network. The latter refines bounding boxes to fit tightly around the object. In this paper, we show that one can rethink the use of this regressor for tracking purposes. Tracking as a graph problem. The data association problem deals with keeping the identity of the tracked objects given the available detections. This can be done on a frame by frame basis for online applications <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b47">48]</ref> or track-by-track <ref type="bibr" target="#b2">[3]</ref>. Since video analysis can be done offline, batch methods are preferred since they are more robust to occlusions. A common formalism is to represent the problem as a graph, where each detection is a node, and edges indicate a possible link. The data association can then be formulated as maximum flow <ref type="bibr" target="#b3">[4]</ref> or, equivalently, minimum cost problem with either fixed costs based on distance <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b65">66]</ref>, including motion models <ref type="bibr" target="#b38">[39]</ref>, or learned costs <ref type="bibr" target="#b35">[36]</ref>. Alternative formulations typically lead to more involved optimization problems, including minimum cliques <ref type="bibr" target="#b64">[65]</ref>, general-purpose solvers like MCMC <ref type="bibr" target="#b63">[64]</ref> or multi-cuts <ref type="bibr" target="#b58">[59]</ref>. A recent trend is to design ever more complex models which include other vision input such as reconstruction for multi-camera sequences <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b59">60]</ref>, activity recognition <ref type="bibr" target="#b11">[12]</ref>, segmentation <ref type="bibr" target="#b45">[46]</ref>, keypoint trajectories <ref type="bibr" target="#b9">[10]</ref> or joint detection <ref type="bibr" target="#b58">[59]</ref>. In general, the significantly higher computational costs do not translate to significantly higher accuracy. In fact, in this work, we show that we can outperform all graph-based trackers significantly while keeping the tracker online. Even within a graphical model optimization, one needs to define a measure to identify whether two bounding boxes belong to the same person or not. This can be done by analyzing either the appearance of the pedestrian, or its motion. Appearance models and re-identification. Discriminating and re-identifying (reID) objects by appearance is in particular a problem in crowded scenes with many object-object occlusions. In the exhaustive literature that uses appearance models or reID methods to improve multi-object tracking, color-based models are very common <ref type="bibr" target="#b30">[31]</ref>. However, these are not always reliable for pedestrian tracking, since people can wear very similar clothes, and color statistics are often contaminated by background pixels and illumination changes. The authors of <ref type="bibr" target="#b33">[34]</ref> borrow ideas from person re-identification and adapt them to "re-identify" targets during tracking. In <ref type="bibr" target="#b61">[62]</ref>, a CRF model is learned to better distinguish pedestrians with similar appearance. Both appearance and short-term motion in the form of optical flow can be used as input to a Siamese neural network to decide whether two boxes belong to the same track or not <ref type="bibr" target="#b34">[35]</ref>. Recently, <ref type="bibr" target="#b53">[54]</ref> showed the importance of learned reID features for multi-object tracking. We confirm this view in our experiments. Motion models and trajectory prediction. Several works resort to motion to discriminate between pedestrians, especially in highly crowded scenes. The most common assumption is the one of constant velocity (CVA) <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b1">2]</ref>, but pedestrian motion gets more complex in crowded scenarios for which researchers have turned to the more expressive Social Force Model <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b38">39]</ref>. Such a model can also be learned from data <ref type="bibr" target="#b35">[36]</ref>. Deep Learning has been extensively used to learn social etiquette in crowded scenarios for trajectory prediction <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b54">55]</ref>. <ref type="bibr" target="#b66">[67]</ref> use single object tracking trained networks to create tracklets for further postprocessing into trajectories. Recently, <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b50">51]</ref> proposed to use reinforcement learning to predict the position of an object in the next frame. While <ref type="bibr" target="#b6">[7]</ref> focuses on single object tracking, the authors of <ref type="bibr" target="#b50">[51]</ref> train a multi-object pedestrian tracker composed of a bounding box predictor and a decision network for collaborative decision making between tracked objects. Video object detection. Multi-object tracking without frame-to-frame identity prediction is a subproblem usually referred to as video object detection. In order to improve detections, many methods exploit spatio-temporal consistencies of object positions. Both <ref type="bibr" target="#b27">[28]</ref> and <ref type="bibr" target="#b26">[27]</ref> generate multiframe bounding box tuplet proposals and extract detection scores and features with a CNN and LSTM, respectively. Recently, the authors of <ref type="bibr" target="#b46">[47]</ref> improve object detections by applying optical flow to propagate scores between frames. Eventually, <ref type="bibr" target="#b17">[18]</ref> proposes to solve the tracking and detection problem jointly. They propose a network which processes two consecutive frames and exploits tracking ground truth data to improve detection regression, thereby, generating two-frame tracklets. With a subsequent offline method, these tracklets are combined to multi-frame tracks. However, we show that our regression tracker is not only online, but superior in dealing with object occlusions. In particular, we do not only temporally align detections, but preserve their identity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">A detector is all you need</head><p>We propose to convert a detector into a Tracktor performing multiple object tracking. Several CNN-based detection algorithms <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b62">63]</ref> contain some form of bounding box refinement through regression. We propose an exploitation of such a regressor for the task of tracking. This has two key advantages: (i) we do not require any tracking specific training, and (ii) we do not perform any complex optimization at test time, hence our tracker is online. Furthermore, we show that our method achieves state-of-the-art performance on several challenging tracking scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Object detector</head><p>The core element of our tracking pipeline is a regressionbased detector. In our case, we train a Faster R-CNN <ref type="bibr" target="#b51">[52]</ref> with ResNet-101 <ref type="bibr" target="#b21">[22]</ref> and Feature Pyramid Networks (FPN) <ref type="bibr" target="#b40">[41]</ref> on the MOT17Det <ref type="bibr" target="#b44">[45]</ref> pedestrian detection dataset.</p><p>To perform object detection, Faster R-CNN applies a Region Proposal Network to generate a multitude of bounding box proposals for each potential object. Feature maps for each proposal are extracted via Region of Interest (RoI) pooling <ref type="bibr" target="#b20">[21]</ref>, and passed to the classification and regression heads. The classification head assigns an object score to the proposal, in our case, it evaluates the likelihood of the proposal showing a pedestrian. The regression head refines the bounding box location tightly around an object. The detector yields the final set of object detections by applying non-maximum-suppression (NMS) to the refined bounding box proposals. Our presented method exploits the aforementioned ability to regress and classify bounding boxes to perform multi-object tracking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Tracktor</head><p>The challenge of multi-object tracking is to extract the spatial and temporal positions, i.e., trajectories, of k objects given a frame by frame video sequence. Such a trajectory is defined as a list of ordered object bounding boxes T k = {b k t1 , b k t2 , · · · }, where a bounding box is defined by its coordinates b k t = (x, y, w, h), and t represents a frame of the video. We denote the set object bounding boxes in frame t with B t = {b k1 t , b k2 t , · · · }. Note, that each T k or B t can contain less elements than the total number of frames or trajectories in a sequence, respectively. At t = 0, our tracker initializes tracks from the first set of detections D 0 = {d 1 0 , d 2 0 , · · · } = B 0 . In <ref type="figure" target="#fig_11">Figure 1</ref>, we illustrate the two subsequent processing steps (the nuts and bolts of our method) for a given frame t for all t &gt; 0, namely, the bounding box regression and track initialization. Bounding box regression. The first step, denoted with blue arrows, exploits the bounding box regression to extend active trajectories to the current frame t. This is achieved by regressing the bounding box b k t−1 of frame t − 1 to the object's new position b k t at frame t. In the case of Faster R-CNN, this corresponds to applying RoI pooling on the features of the current frame but with the previous bounding box coordinates. Our assumption is that the target has moved only slightly between frames, which is usually ensured from high frame rates (see Section B.5 of the sup-plementary for a frame rate robustness evaluation of Tracktor).The identity is automatically transferred from the previous to the regressed bounding box, effectively creating a trajectory. This is repeated for all subsequent frames.</p><p>After the bounding box regression, our tracker considers two cases for killing (deactivating) a trajectory: (i) an object leaving the frame or occluded by a non-object is killed if its new classification score s k t is below σ active and (ii) occlusions between objects are handled by applying nonmaximum suppression (NMS) to all remaining B t and their corresponding scores with an Intersection over Union (IoU) threshold λ active . Bounding box initialization. In order to account for new targets, the object detector also provides the detections D t for the entire frame t. This second step, indicated in <ref type="figure" target="#fig_11">Figure 1</ref> with red arrows, is analogous to the first initialization at t = 0. But a detection from D t starts a trajectory only if the IoU with any of the already active trajectories b k t is smaller than λ new . That is, we consider a detection for a new trajectory only if it is covering a potentially new object that is not explained by any trajectory. It should be noted again that our Tracktor does not require any tracking specific training or optimization and solely relies on an object detection method. This allows us to directly benefit from improved object detection methods and, most importantly, enables a comparatively cheap transfer to different tracking datasets or scenarios in which no ground truth tracking but only detection data is available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Tracking extensions</head><p>In this section, we present two straightforward extensions to our vanilla Tracktor: a motion model and a reidentification algorithm. Both are aimed at improving identity preservation across frames and are common examples of techniques used to enhance, e.g., graph-based tracking methods <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b34">35]</ref>. Motion model. Our previous assumption that the position of an object changes only slightly from frame to frame does not hold in two scenarios: large camera motion and low video frame rates. In extreme cases, the bounding boxes from frame t − 1 might not contain the tracked object in frame t at all. Therefore, we apply two types of motion models that will improve the bounding box position in future frames. For sequences with a moving camera, we apply a straightforward camera motion compensation (CMC) by aligning frames via image registration using the Enhanced Correlation Coefficient (ECC) maximization as introduced in <ref type="bibr" target="#b15">[16]</ref>. For sequences with comparatively low frame rates, we apply a constant velocity assumption (CVA) for all objects as in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b1">2]</ref>. Re-identification. In order to keep our tracker online, we suggest a short-term re-identification (reID) based on appearance vectors generated by a Siamese neural net-work <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b53">54]</ref>. To that end, we store killed (deactivated) tracks in their non-regressed version b k t−1 for a fixed number of F reID frames. We then compare the distance in the embedding space of the deactivated with the newly detected tracks and re-identify via a threshold. The embedding space distance is computed by a Siamese CNN and appearance feature vectors for each of the bounding boxes. It should be noted that the reID network is indeed trained on tracking ground truth data. To minimize the risk of false reIDs, we only consider pairs of deactivated and new bounding boxes with a sufficiently large IoU. The motion model is continuously applied to the deactivated tracks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head><p>We demonstrate the tracking performance of our proposed Tracktor tracker as well as its extension Tracktor++ on several datasets focusing on pedestrian tracking. <ref type="bibr" target="#b0">1</ref> In addition, we perform an ablation study of the aforementioned extensions and further show that our tracker outperforms state-of-the-art methods in tracking accuracy and excels at identity preservation. MOTChallenge. The multi-object tracking benchmark MOTChallenge 2 consists of several challenging pedestrian tracking sequences, with frequent occlusions and crowded scenes. Sequences vary in their angle of view, size of objects, camera motion and frame rate. The challenge contains three separate tracking benchmarks, namely 2D MOT 2015 <ref type="bibr" target="#b36">[37]</ref>, MOT16 and MOT17 <ref type="bibr" target="#b44">[45]</ref>. The MOT17 test set includes a total of 7 sequences each of which is provided with three sets of public detections. The detections originate from different object detectors each with increasing performance, namely DPM <ref type="bibr" target="#b18">[19]</ref>, Faster R-CNN <ref type="bibr" target="#b51">[52]</ref> and SDP <ref type="bibr" target="#b62">[63]</ref>. Our object detector is trained on the MOT17Det <ref type="bibr" target="#b44">[45]</ref> detection benchmark which contains the same images as MOT17. The MOT16 benchmark also contains the same sequences as MOT17 but only provides DPM public detections. The 2D MOT 2015 benchmark provides ACF <ref type="bibr" target="#b13">[14]</ref> detections for 11 sequences. The complexity of the tracking problem requires several metrics to measure different aspects of a tracker's performance. The Multiple Object Tracking Accuracy (MOTA) <ref type="bibr" target="#b28">[29]</ref> and ID F1 Score (IDF1) <ref type="bibr" target="#b52">[53]</ref> quantify two of the main aspects, namely, object coverage and identity. Public detections. For a fair comparison with other tracking methods, we perform all experiments with the public detections provided by MOTChallenge. That is, all methods compared in this paper, including our approach and its extension, process the same precomputed frame by frame detections. For our method, a new trajectory is only initialized from a public detection bounding box, i.e., we never  <ref type="table" target="#tab_7">Table 1</ref>: This ablation study illustrates multiple aspects on the performance of our Tracktor. In particular the improvements from extending it with tracking specific methods, i.e., a short-term bounding box re-identification and camera motion compensation by frame alignment. The combination yields the Tracktor++ tracker. We evaluated only on the Faster R-CNN set of MOT17 public detections. The arrows indicate low or high optimal metric values.</p><formula xml:id="formula_8">Method MOTA ↑ IDF1 ↑ MT ↑ ML ↓ FP ↓ FN ↓ ID</formula><p>use our object detector to detect a new bounding box. We only apply the bounding box regressor and classifier to obtain new b k t and s k t , respectively. The MOTChallenge public benchmark includes multiple methods <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b12">13]</ref> which classify the given detections with trained neural networks, hence, we consider our processing of the given detections also as public.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Ablation study</head><p>The ablation study on the MOT17 <ref type="bibr" target="#b44">[45]</ref> training set in <ref type="table" target="#tab_7">Table 1</ref> is intended to show three aspects: (i) the superiority of our approach when applying a detector for tracking, (ii) the potential from an improved object detection method and (iii) improvements from extending our vanilla Tracktor with tracking specific methods, namely, re-identification (reID) and camera motion compensation (CMC). It should be noted, that although MOT17Det and MOT17 contain the same images, we refrained from a cross-validation on the training set as our vanilla Tracktor was never trained on tracking ground truth data. The video object detector and tracker D&amp;T <ref type="bibr" target="#b17">[18]</ref> trains a detector on tracking ground truth data which generates two-frame tracklets. However, despite a subsequent offline dynamic programming track generation their detector-based tracker is inferior to our online regression-based track generation over multiple frames. In addition, we demonstrate the potential of our framework with respect to improved detection methods by showing the tracking performance of Tracktor-no-FPN, i.e., our approach and a Faster R-CNN without Feature Pyramid Networks (FPN) <ref type="bibr" target="#b40">[41]</ref>. Despite the simple nature of our extensions to Tracktor++, their contribution is significant towards the drastic reduction of identity switches and an increment of the IDF1 measure. In the next section, we show that this effect successfully translates to a comparison with other state-of-the-art methods on the test set.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Benchmark evaluation</head><p>We evaluate the performance of our Tracktor++ on the test set of the respective benchmark, without any training or optimization on the tracking train set. <ref type="table" target="#tab_3">Table 2</ref> presents the overall results accumulated over all sequences, and for MOT17 over all three sets of public detections. For our comparison, we only consider officially published and peerreviewed entries in the MOTChallenge benchmark. Our supplementary material provides a detailed summary of all results on individual sequences. For all sequences, camera motion compensation (CMC) and reID are used. The only low frame rate sequence is the 2D MOT 2015 AVG-TownCentre, for which we apply the aforementioned constant velocity assumption (CVA). For the two autonomous driving sequences, originally from the KITTI <ref type="bibr" target="#b19">[20]</ref> benchmark, we apply the rotation as well as translation camera motion compensation. Note, we use the same Track-tor++ tracker, trained on MOT17Det object detections, for all benchmarks. As we show, it is able to achieve a new state-of-the-art in terms of MOTA on all three challenges.</p><p>In particular, our results on MOT16 demonstrate the ability of our tracker to cope with detections of comparatively minor performance. Due to the nature of our tracker and the robustness of the frame by frame bounding box regression, we outperform all other trackers on MOT16 by a large margin, specifically in terms of false negatives (FN) and identity preserving (IDF1). It should be noted, that we also provide a new state-of-the-art on 2D MOT 2015, even though the characteristics of the scenes are very different from MOT17. We do not use MOT15 training sequences, which further illustrates the generalization strength of our tracker.  <ref type="figure">Figure 2</ref>: We illustrate the ratio of tracked objects with respect to their visibility evaluated on the Faster R-CNN public detections. The results clearly demonstrate that none of the presented more sophisticated methods achieves superior performance to our approach. This is especially noticeable for highly occluded boxes. The transparent red bars indicate the ground truth distribution of visibilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Online Graph reID Appearance model Motion model Other</p><formula xml:id="formula_9">Tracktor × Tracktor++ × × Camera FWT [23]</formula><p>Dense Face detection jCC <ref type="bibr" target="#b29">[30]</ref> Dense Point trajectories MOTDT17 <ref type="bibr" target="#b8">[9]</ref> × × × Kalman MHT DAM <ref type="bibr" target="#b31">[32]</ref> Sparse × Kalman <ref type="table" target="#tab_12">Table 3</ref>: A summary of the fundamental characteristics of our methods and other state-of-the-art trackers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Analysis</head><p>The superior performance of our tracker without any tracking specific training or optimization demands a more thorough analysis. Without sophisticated tracking methods, it is not expected to excel in crowded and occluded, but rather only in benevolent, tracking scenarios. Which begs the question whether more common tracking methods fail to specifically address these complex scenarios as well. Our experiments and the subsequent analysis ought to demonstrate the strengths of our approach for easy tracking scenarios and motivate future research to focus on remaining complex tracking problems. In particular, we question the common execution of tracking-by-detection and suggest a new tracking paradigm. The subsequent analysis is conducted on the MOT17 training data and we compare all top performing methods with publicly shared data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Tracking challenges</head><p>For a better understanding of our tracker, we want to analyse challenging tracking scenarios and compare its strengths and weaknesses to other trackers. To this end, we summarize their fundamental characteristics in <ref type="table" target="#tab_12">Table 3</ref>.</p><p>FWT <ref type="bibr" target="#b22">[23]</ref> and jCC <ref type="bibr" target="#b29">[30]</ref> both apply a dense offline graph optimization on all detections in a given sequence. In contrast, MHT DAM <ref type="bibr" target="#b31">[32]</ref> limits its optimization to a sparse forward view of hypothetical trajectories. Object visibility. Intuitively, we expect diminished tracking performance for object-object or object-non-object occlusions, i.e., for targets with diminished visibility. In <ref type="figure">Figure 2</ref>, we compare the ratio of successfully tracked bounding boxes with respect to their visibility. The transparent red bar indicates the occurrences of ground truth bounding boxes for each visibility, and illustrates the proportionate impact on the overall performance of the trackers. Our method achieves superior performance even for partially occluded bounding boxes with visibilities as low as 0.3. Neither the identify preserving aspects of MHT DAM and MOTDT17 <ref type="bibr" target="#b8">[9]</ref> nor the offline interpolation capabilities of MHT DAM and jCC seem to successfully tackle highly occluded objects. The high MOTA values in <ref type="table" target="#tab_3">Table 2</ref> are largely due to the unbalanced distribution of ground truth visibilities. As expected, our extended version only achieves minor improvements over our vanilla Tracktor. Object size. In view of the large fraction of visible but not tracked objects in <ref type="figure">Figure 2</ref>, we argue that the trackability of an object is not only dependent on its visibility, but also its size. Therefore, we conduct the same comparison as for the visibility but for the size of an object. In the first row of <ref type="figure" target="#fig_10">Figure 3</ref>, we assume the height of a pedestrian to be proportional to its size and compare on all three MOT17 public detection sets. All methods performed similarly well for object heights larger than 250 pixels. To demonstrate their shortcomings even for highly visible objects, we only compare objects with a visibility larger than 0.9. As expected, the trackability of an object decreases drastically with its size across all three detection sets. Our tracker shows its strength in compensating for insufficient DPM and Faster R-CNN detections for all object sizes. All methods except MOTDT17 benefit from the additional small detections provided by SDP. For our tracker this is largely due to the Feature Pyramid Network extension of our Faster-RCNN detector. However, the learned appearance model and reID of the online MOTDT17 method seem generally vulnerable to small detections. Appearance models generally suffer from small object sizes and few observed pixels. In conclusion, except from our compensation of inferior detections none of the trackers exhibit a notably better performance with respect to varying object sizes. Robustness to detections. The performance of trackingby-detection methods with respect to visibility and size is inherently limited by the robustness of the underlying detection method. However, as observed for the object size, trackers differ in their ability to cope with, or benefit from, varying quality of detections. In the second row of on their coverage by the tracker. We define a detection gap as part of a ground truth trajectory that was at least once detected, and compare coverage of each gap vs. the gap length. Intuitively, long gaps are harder to compensate for, as the online or offline tracker has to perform a longer hallucination or interpolation, respectively. We indicated the occurrences of gap lengths over the respective set of detections in transparent red. For DPM and Faster R-CNN detections, two solutions lead to notable gap coverage: (i) offline interpolation such as in jCC, or (ii) motion prediction with Kalman filter and reID as in MOTDT. Compared to the graph-based jCC method, the online MOTDT17 method excels at covering particularly long gaps. However, none of these dedicated tracking methods yields similar robustness to our frame by frame regression tracker, which achieves far superior coverage. This holds especially true for long detection gaps with more than 15 frames. Offline methods benefit the most from improved SDP detections and neither our nor the MOTDT17 tracker convince with a notable gap length robustness.</p><p>Identity preservation. The results of our Tracktor++ summarized in <ref type="table" target="#tab_3">Table 2</ref> indicate an identity preservation performance in terms of IDF1 and identity switches comparable with dedicated tracking methods. This is achieved without any offline graph optimization as in jCC <ref type="bibr" target="#b29">[30]</ref> or eHAF <ref type="bibr" target="#b57">[58]</ref>.</p><p>In particular, MOTDT17, which applies a sophisticated appearance model and reID, is not substantially superior to our regression tracker and its comparatively simple extensions. However, our method excels in reducing the number of false positives in MOT17 as well as MOT16. In addition, we have shown that our Tracktor is capable of incorporating additional identity preserving extension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Oracle trackers</head><p>We have shown that none of the dedicated tracking methods specifically targets challenging tracking scenarios, i.e., objects under heavy occlusions or small objects. We therefore want to motivate our Tracktor as a new tracking paradigm. To this end, we analyse our performance twofold: (i) the impact of the object detector on the killing policy and bounding box regression, (ii) identify performance upper bounds for potential extensions to our Tracktor. In <ref type="table" target="#tab_5">Table 4</ref>, we present several oracle trackers by replacing parts of our algorithm with ground truth information. If not mentioned otherwise, all other tracking aspects are handled by our vanilla Tracktor. Their analysis should provide researchers with useful insights regarding the most promising research directions and extensions of our Tracktor.  Detector oracles. To simulate a potentially perfect object detector, we introduce two oracles:</p><p>• Oracle-Kill: Instead of killing with NMS or classification score we use ground truth information.</p><p>• Oracle-REG: Instead of regression, we place the bounding boxes at their ground truth position.</p><p>Both oracles yield substantial improvements with respect to MOTA and FP. However, killing by ground truth instead of score deteriorates identity preservation as the regression struggles with otherwise unseen bounding boxes. Extension oracles. It should be noted, that Tracktor++ with non-perfect extensions already compensates for some of the detector's insufficiencies. The reID and motion model (MM) oracles simulate potential additional performance gains. In order to remain online, these exclude any form of hindsight tracking-gap interpolation.</p><p>• Oracle-MM: A motion model places each bounding box at the center of the ground truth in the next frame.</p><p>• Oracle-reID: Re-identification is performed with ground truth identities.</p><p>As expected, both oracles improve IDF1 and identity switches substantially. The combined Oracle-MM-reID represents the extension upper bound of Tracktor++. Omniscient oracle. Oracle-ALL performs ground truth killing, regression and reID. We consider its top MOTA of 72.2%, in combination with a high IDF1 and virtually no false positives, as the absolute upper bound of Tracktor with a Faster R-CNN and FPN object detector. The substantial performance gains from Oracle-MM indicate the potential of extending Tracktor with a sophis-ticated motion model. In particular, Oracle-MM-reID-INTER suggests a predictive motion model which hallucinates the position of an object through long occlusions. Such a motion model avoids offline post processing and additional false positives from wrong linear occlusion paths caused by long detection gaps and camera movement</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Towards a new tracking paradigm</head><p>To conclude our analysis we propose two approaches on how to utilize Tracktor as a starting point for future research directions: Tracktor with extensions. Apply Tracktor to a given set of detections and extend it with tracking specific methods. Scenarios with large and highly visible objects will be covered by the frame to frame bounding box regression. For the remaining, it seems most promising to implement a hallucinating motion model, taking into account the individual movements of objects. In addition, such a motion predictor reduces the necessity for an advanced killing policy. Tracklet generation. Analogous to tracking-by-detection, we propose a tracking-by-tracklet approach. Indeed, many algorithms already use tracklets as input <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b64">65]</ref>, as they are richer in information for computing motion or appearance models. However, usually a specific tracking method is used to create these tracklets.We advocate the exploitation of the detector itself, not only to create sparse detections, but frame to frame tracklets. The remaining complex tracking cases ought to be tackled by a subsequent tracking method.</p><p>In this work, we have formally defined those hard cases, analyzing the situations in which not only our method but other dedicated tracking solutions fail. And by doing so, we question the current focus of research in multi-object tracking, in particular, the missing confrontation with challenging tracking scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We have shown that the bounding box regressor of a trained Faster-RCNN detector is enough to solve most tracking scenarios present in current benchmarks. A detector converted to Tracktor needs no specific training on tracking ground truth data and is able to work in an online fashion. In addition, we have shown that our Tracktor is extendable with re-identification and camera motion compensation, providing a substantial new state-of-the-art on the MOTChallenge. We analyzed the performance of multiple dedicated tracking methods on challenging tracking scenarios and none yielded substantially better performance compared to our regression based Tracktor. We hope this work establishes a new tracking paradigm, utilizing the object detector's full capabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tracking without bells and whistles Supplementary Material</head><p>Philipp Bergmann * Tim Meinhardt * Laura Leal-Taixe</p><p>Technical University of Munich</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>The supplementary material complements our work with the pseudocode representation of Tracktor and additional implementation and training details of its object detector and tracking extensions. In addition, we provide more details on our experiments and analysis including the MOTChallenge benchmark results of our Tracktor++ tracker for each sequence and set of public detections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation</head><p>For the sake of completeness and in order to facilitate the reproduction of our results, we provide additional implementation details and references of our Tracktor and its extensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Tracktor</head><p>In Algorithm 1 and 2, we present a structured pseudocode representation of our Tracktor for private and public detections, respectively. Algorithm 1 corresponds to the method illustrated in <ref type="figure" target="#fig_11">Figure 1</ref> and Section 2.2 of our main work.</p><p>Object detector. As mentioned before, our approach requires no dedicated training or optimization on tracking ground truth data and performs tracking only with an object detection method. To this end, we train the Faster R-CNN (FRCNN) <ref type="bibr" target="#b51">[52]</ref> multiobject detector with Feature Pyramid Networks (FPN) <ref type="bibr" target="#b40">[41]</ref> on the MOT17Det <ref type="bibr" target="#b44">[45]</ref> dataset.</p><p>In addition, we follow the improvements suggested by [?]. These include a replacement of the Region of Interest (RoI) pooling <ref type="bibr" target="#b20">[21]</ref> by the crop and resize pooling suggested by Huang et al. <ref type="bibr">[?]</ref> and training with a batch size of N = 1 instead of N = 2 while increasing the number of extracted regions from R = 128 to R = 256. These changes and the addition of FPN ought to improve the detection results for comparatively small objects. We achieve the best results with a ResNet-101 <ref type="bibr" target="#b21">[22]</ref> as the underlying feature extractor. In <ref type="table" target="#tab_7">Table 1</ref>, we compare the performance on the official MOT17Det detection benchmark for the three object detection methods mentioned in this work. The results demonstrate the incremental gain in detection performance of DPM <ref type="bibr" target="#b18">[19]</ref>, FR-CNN and SDP <ref type="bibr" target="#b62">[63]</ref> (ascending order). Our FRCNN implementa- * Contributed equally. Correspondence to: tim.meinhardt@tum.de tion without FPN is on par with the official MOT17Det entry and represents the detector applied in the Tracktor-no-FPN variant of our ablation study in Section 3.1.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Tracking extensions</head><p>Our presented Tracktor++ tracker is an extension of the Tracktor that uses two multi-pedestrian tracking specific extensions, namely, a motion model and re-identification.</p><p>Motion model. For the motion model via camera motion compensation (CMC) we apply image registration using the Enhanced Correlation Coefficient (ECC) maximization as in <ref type="bibr" target="#b15">[16]</ref>. The underlying image registration allows either for an euclidean or affine image alignment mode. We apply the first for rotating camera movements, e.g., as a result of an unsteady camera movement. In the case of an additional camera translation such as in the autonomous driving sequences of 2D MOT 2015 <ref type="bibr" target="#b44">[45]</ref>, we resort to the affine transformation. It should be noted that in MOT17 <ref type="bibr" target="#b44">[45]</ref>, camera translation is comparatively slow and therefore we consider all sequences as only rotating. In addition, we present a second motion model which aims at facilitating the regression for sequences with low frame rates, i.e., large object displacements between frames. Before we perform bounding box regression, the constant velocity assumption (CVM) model shifts bounding boxes in the direction of their previous velocity. This is achieved by moving the center of the bounding box b k t−1 by the vectorial difference of the two previous bounding box centers at t − 2 and t − 1. The CVA motion model is only applied to the AVG-TownCentre sequence of 2D MOT 2015.</p><p>Re-identification. Our short-term re-identification utilizes a Siamese neural network to compare bounding box features and return a measure of their identity. To this end, we train the TriNet <ref type="bibr" target="#b24">[25]</ref> architecture which is based on ResNet-50 <ref type="bibr" target="#b21">[22]</ref> with the triplet loss and batch hard strategy as presented in <ref type="bibr" target="#b24">[25]</ref>. The network is optimized with Adam [?] with β = (0.9, 0.999) and a decaying learning rate as described in <ref type="bibr" target="#b24">[25]</ref>. Training samples with corresponding identity are generated from the MOT17 tracking ground truth training data. The TriNet architecture requires input data with a dimension of H × W = 256 × 128. To allow for a subsequent data augmentation via horizontal flip and random cropping, each ground truth bounding box is cropped and resized to <ref type="bibr">9 8</ref> (H × W ). A training batch consists of 18 randomly selected identities, each of which is represented with 4 different samples. Identities with less than 4 samples in the ground truth data are discarded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experiments</head><p>A detailed summary of our official and published MOTChallenge benchmark results for our Tracktor++ tracker is presented in <ref type="table" target="#tab_12">Table 3</ref>. For the corresponding results for each sequence and set of detections for the other trackers mentioned in this work we refer to the official MOTChallenge web page available at https: //motchallenge.net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Evaluation metrics</head><p>In order to measure the performance of a tracker, we mentioned the Multiple Object Tracking Accuracy (MOTA) <ref type="bibr" target="#b28">[29]</ref> and ID F1 Score (IDF1) <ref type="bibr" target="#b52">[53]</ref>. However, previous <ref type="table" target="#tab_12">Tables such as 3</ref> included additional informative metrics. The false positives (FP) and negatives (FN) account for the total number of either bounding boxes not covering any ground truth bounding box or ground truth bounding boxes not covered by any bounding box, respectively. To measure the track identity preserving capabilities, we report the total number of identity switches (ID Sw.), i.e., a bounding box covering a ground truth bounding box from a different track than in the previous frame. The mostly tracked (MT) and mostly lost (ML) metrics provide track wise information on how many ground truth tracks are covered by bounding boxes for either at least 80% or at most 20%, respectively. MOTA and IDF1 are meaningful combinations of the aforementioned basic metrics. All metrics were computed using the official evaluation code provided by the MOTChallenge benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Raw DPM detections</head><p>As most object detection methods, DPM applies a final nonmaximum-suppression (NMS) step to a large set of raw detections. The MOT16 <ref type="bibr" target="#b44">[45]</ref> benchmark provides both, the set before and after the NMS, as public DPM detections. However, this NMS step is performed with DPM classification scores and an unknown Intersection over Union (IoU) threshold. Therefore, we extracted  our own classification scores for all raw detections and applied our own NMS step. Although not specifically provided, we followed the convention to also process raw DPM detections for MOT17. Note, several other public trackers already work on raw detections <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b12">13]</ref> and their own classification score and NMS procedure. Therefore, we consider the comparison with public trackers as fair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Evaluation on public detections</head><p>By reclassifying and regressing the given public detections with a private object detector, Tracktor reduces the equalizing effect of public detections to the initialization of new tracks. In addition to our remarks in Section 3 regarding the publicness of our method, we emphasize the potential of Tracktor in comparison with other state-of-the-art trackers even without the advantage of the reclassification and regression. To this end, we show <ref type="table" target="#tab_3">Table 2</ref>, which evaluates all trackers on the MOT17 test set only with Faster R-CNN public detections. Tracktor-no-FPN++ (without Feature Pyramid Networks) uses a vanilla Faster R-CNN for reclassification and regression, effectively, not altering the public detections. However, the results support the overall conclusions from <ref type="table" target="#tab_3">Table 2</ref> of our main work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4. Tracktor thresholds</head><p>To demonstrate the robustness of our tracker with respect to the classification score and IoU thresholds, we refrained from any sequence or detection-specific fine-tuning. In particular, we performed our experiments on all benchmarks with σactive = 0.5, λactive = 0.6 and λnew = 0.3, which were chosen to be optimal for the MOT17 training dataset. In general, a higher λactive than λnew introduces stability into the tracker, as less active tracks are killed by the NMS and less new tracks are initialized. A comparatively higher λactive relaxes potential object-object occlusions and implies a certain confidence in the regression performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5. Tracktor video frame rate robustness</head><p>A successful Tracktor bounding box regression depends on sufficiently high video frame rates or, in other words, small frameby-frame object displacements. A possible approach to address this issue is the extension with a powerful motion model. A rudimentary motion model, the camera motion compensation (CMC), is presented in Section 2.3 and evaluated in the ablation study in <ref type="table" target="#tab_7">Table 1</ref>. However, MOT16 and MOT17 mostly consist of se-quences with benevolent video frame rates and slow moving objects (pedestrians).</p><p>We therefore complement our analysis of Tracktor in challenging tracking scenarios from Section 4.1 with an evaluation of its video frame rate robustness. To this end, we evaluate Tracktor and Tracktor++ on all MOT17 training sequences with originally 30 frames per second (FPS) and reduce their frame rates by removing frames from the data and ground truth. In <ref type="figure" target="#fig_11">Figure 1</ref>, both versions exhibit a fairly robust object tracking (MOTA) and identity preservation (IDF1) for rates as low as 5 FPS. As expected, the performance for very small rates suffers particularly with respect to identity preservation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Oracle trackers</head><p>In our main work, we conclude the analysis in Section 4 with a comparison of multiple oracle trackers that highlight the potential of future research directions. For each oracle, one or multiple aspects of our vanilla Tracktor are substituted with ground truth information, thereby simulating perfect behavior. For further understanding, we provide more details on the oracles for each of the distinct tracking aspects:</p><p>• Oracle-Kill: This oracle kills tracks only if they have an IoU less than 0.5 with the corresponding ground truth bounding box. The matching between predicted and ground truth tracks is performed with the Hungarian [?] algorithm. In the case of an object-object occlusion (IoU &gt; 0.8), the ground truth matching is applied to decide which of the objects is occluded by the other and therefore should be killed. • Oracle-REG: We simulate a perfect regression by matching tracks with an IoU threshold of 0.5 to the ground truth at frame t − 1 . The regression oracle then sets track bounding boxes to the corresponding ground truth coordinates at frame t.</p><p>• Oracle-MM: A perfect motion model works analogous to Oracle-REG but we only move the previous bounding box center to the center of the ground truth bounding box at frame t. However, the bounding box height and width are still determined by the regression.</p><p>• Oracle-reID: Again, we use the Hungarian algorithm to match the new set of detections to the ground truth data. Ground truth identity matches between inactive tracks and new detections yield a perfect re-identification.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>k t 1 &lt;k t 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 =</head><label>1114</label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " l F v E J 0 1 e k W 7 j t 9 0 b z 3 m Q I L A A v G w = " &gt; A A A B 9 H i c b V C 7 S g N B F L 3 r I 8 b 4 i l r a D A b B K u y m 0 T K g h W U C 5 g H J E m Y n k 2 T I 7 O w 6 c z c Q l n y H j Y U i t n 6 D 3 2 A n / o y z S Q p N P D B w O O d e 7 p k T x F I Y d N 0 v Z 2 N z a z u 3 k 9 8 t 7 O 0 f H B 4 V j 0 + a J k o 0 4 w 0 W y U i 3 A 2 q 4 F I o 3 U K D k 7 V h z G g a S t 4 L x T e a 3 J l w b E a l 7 n M b c D + l Q i Y F g F K 3 k d 0 O K I 0 Z l e j v r Y a 9 Y c s v u H G S d e E t S q u b q 3 x 8 A U O s V P 7 v 9 i C U h V 8 g k N a b j u T H 6 K d U o m O S z Q j c x P K Z s T I e 8 Y 6 m i I T d + O g 8 9 I x d W 6 Z N B p O 1 T S O b q 7 4 2 U h s Z M w 8 B O Z i H N q p e J / 3 m d B A f X f i p U n C B X b H F o k E i C E c k a I H 2 h O U M 5 t Y Q y L W x W w k Z U U 4 a 2 p 4 I t w V v 9 8 j p p V s q e W / b q t o 0 K L J C H M z i H S / D g C q p w B z V o A I M H e I R n e H E m z p P z 6 r w t R j e c 5 c 4 p / I H z / g P 3 y Z R f &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " a V j F P Q y d q U S K L 9 T t p M D K g L F J o j k = " &gt; A A A B 9 H i c b V C 7 S g N B F L 0 b N c b 4 i l r a D A b B K u y m 0 T K g h W U C 5 g H J E m Y n k 2 T I 7 M O Z u 4 G w 5 C N s t L F Q R O z 8 E E s 7 8 W e c T V J o 4 o G B w z n 3 c s 8 c L 5 J C o 2 1 / W Z m 1 9 Y 3 s Z m 4 r v 7 2 z u 7 d f O D h s 6 D B W j N d Z K E P V 8 q j m U g S 8 j g I l b 0 W K U 9 + T v O m N L l O / O e Z K i z C 4 w U n E X Z 8 O A t E X j K K R 3 I 5 P c c i o T K 6 m X e w W i n b J n o G s E m d B i p V s 7 f v j / u 6 t 2 i 1 8 d n o h i 3 0 e I J N U 6 7 Z j R + g m V K F g k k / z n V j z i L I R H f C 2 o Q H 1 u X a T W e g p O T V K j / R D Z V 6 A Z K b + 3 k i o r / X E 9 8 x k G l I v e 6 n 4 n 9 e O s X / h J i K I Y u Q B m x / q x 5 J g S N I G S E 8 o z l B O D K F M C Z O V s C F V l K H p K W 9 K c J a / v E o a 5 Z J j l 5 y a a a M M c + T g G E 7 g D B w 4 h w p c Q x X q w O A W H u A J n q 2 x 9 W i 9 W K / z 0 Y y 1 2 D m C P 7 D e f w B u i Z Y 3 &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " a V j F P Q y d q U S K L 9 T t p M D K g L F J o j k = " &gt; A A A B 9 H i c b V C 7 S g N B F L 0 b N c b 4 i l r a D A b B K u y m 0 T K g h W U C 5 g H J E m Y n k 2 T I 7 M O Z u 4 G w 5 C N s t L F Q R O z 8 E E s 7 8 W e c T V J o 4 o G B w z n 3 c s 8 c L 5 J C o 2 1 / W Z m 1 9 Y 3 s Z m 4 r v 7 2 z u 7 d f O D h s 6 D B W j N d Z K E P V 8 q j m U g S 8 j g I l b 0 W K U 9 + T v O m N L l O / O e Z K i z C 4 w U n E X Z 8 O A t E X j K K R 3 I 5 P c c i o T K 6 m X e w W i n b J n o G s E m d B i p V s 7 f v j / u 6 t 2 i 1 8 d n o h i 3 0 e I J N U 6 7 Z j R + g m V K F g k k / z n V j z i L I R H f C 2 o Q H 1 u X a T W e g p O T V K j / R D Z V 6 A Z K b + 3 k i o r / X E 9 8 x k G l I v e 6 n 4 n 9 e O s X / h J i K I Y u Q B m x / q x 5 J g S N I G S E 8 o z l B O D K F M C Z O V s C F V l K H p K W 9 K c J a / v E o a 5 Z J j l 5 y a a a M M c + T g G E 7 g D B w 4 h w p c Q x X q w O A W H u A J n q 2 x 9 W i 9 W K / z 0 Y y 1 2 D m C P 7 D e f w B u i Z Y 3 &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " j G H M q r v P F G P s q B z p K R v D F N o z k p U = " &gt; A A A B 9 H i c b V B N T 8 J A F H z F L 8 Q v 1 K O X j c T E E 2 m 5 6 J F E D x 4 x E T C B h m y X L W z Y b u v u K w l p + B 1 e P G i M V 3 + M N / + N W + h B w U k 2 m c y 8 l z c 7 Q S K F Q d f 9 d k o b m 1 v b O + X d y t 7 + w e F R 9 f i k Y + J U M 9 5 m s Y z 1 Y 0 A N l 0 L x N g q U / D H R n E a B 5 N 1 g c p P 7 3 S n X R s T q A W c J 9 y M 6 U i I U j K K V / H 5 E c c y o z G 7 n A x x U a 2 7 d X Y C s E 6 8 g N S j Q G l S / + s O Y p R F X y C Q 1 p u e 5 C f o Z 1 S i Y 5 P N K P z U 8 o W x C R 7 x n q a I R N 3 6 2C D 0 n F 1 Y Z k j D W 9 i k k C / X 3 R k Y j Y 2 Z R Y C f z k G b V y 8 X / v F 6 K 4 b W f C Z W k y B V b H g p T S T A m e Q N k K D R n K G e W U K a F z U r Y m G r K 0 P Z U s S V4 q 1 9 e J 5 1 G 3 X P r 3 r 1 b a z a K O s p w B u d w C R 5 c Q R P u o A V t Y P A E z / A K b 8 7 U e X H e n Y / l a M k p d k 7 h D 5 z P H w H s k i 0 = &lt; / l a t e x i t &gt; Regression &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Y / P w A T Z U N 3 d D O v W e h A / f B 2 Z u z j 4 = " &gt; A A A B 8 X i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 k J L 0 o s e C F 4 9 V 7 A e 2 o W y 2 k 3 b p Z h N 2 N 0 I J / R d e P C j i 1 X / j z X / j p s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 2 v r G 5 t b 2 6 W d 8 u 7 e / s F h 5 e i 4 r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J T e 5 3 n l B p H s s H M 0 3 Q j + h I 8 p A z a q z 0 e I 8 j h T p 3 B 5 W q W 3 P n I K v E K 0 g V C j Q H l a / + M G Z p h N I w Q b X u e W 5 i / I w q w 5 n A W b m f a k w o m 9 A R 9 i y V N E L t Z / O L Z + T c K k M S x s q W N G S u / p 7 I a K T 1 N A p s Z 0 T N W C 9 7 u f i f 1 0 t N e O 1 n X C a p Q c k W i 8 J U E B O T / H 0 y 5 A q Z E V N L K F P c 3 k r Y m C r K j A 2 p b E P w l l 9 e J e 1 6 z X N r 3 l 2 9 2 r g s 4 i j B K Z z B B X h w B Q 2 4 h S a 0 g I G E Z 3 i F N 0 c 7 L 8 6 7 8 7 F o X X O K m R P 4 A + f z B 8 m 6 k O c = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Y / P w A T Z U N 3 d D O v W e h A / f B 2 Z u z j 4 = " &gt; A A A B 8 X i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 k J L 0 o s e C F 4 9 V 7 A e 2 o W y 2 k 3 b p Z h N 2 N 0 I J / R d e P C j i 1 X / j z X / j p s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 2 v r G 5 t b 2 6 W d 8 u 7 e / s F h 5 e i 4 r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J T e 5 3 n l B p H s s H M 0 3 Q j + h I 8 p A z a q z 0 e I 8 j h T p 3 B 5 W q W 3 P n I K v E K 0 g V C j Q H l a / + M G Z p h N I w Q b X u e W 5 i / I w q w 5 n A W b m f a k w o m 9 A R 9 i y V N E L t Z / O L Z + T c K k M S x s q W N G S u / p 7 I a K T 1 N A p s Z 0 T N W C 9 7 u f i f 1 0 t N e O 1 n X C a p Q c k W i 8 J U E B O T / H 0 y 5 A q Z E V N L K F P c 3 k r Y m C r K j A 2 p b E P w l l 9 e J e 1 6 z X N r 3 l 2 9 2 r g s 4 i j B K Z z B B X h w B Q 2 4 h S a 0 g I G E Z 3 i F N 0 c 7 L 8 6 7 8 7 F o X X O K m R P 4 A + f z B 8 m 6 k O c = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Y / P w A T Z U N 3 d D O v W e h A / f B 2 Z u z j 4 = " &gt; A A A B 8 X i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 k J L 0 o s e C F 4 9 V 7 A e 2 o W y 2 k 3 b p Z h N 2 N 0 I J / R d e P C j i 1 X / j z X / j p s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 2 v r G 5 t b 2 6 W d 8 u 7 e / s F h 5 e i 4 r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J T e 5 3 n l B p H s s H M 0 3 Q j + h I 8 p A z a q z 0 e I 8 j h T p 3 B 5 W q W 3 P n I K v E K 0 g V C j Q H l a / + M G Z p h N I w Q b X u e W 5 i / I w q w 5 n A W b m f a k w o m 9 A R 9 i y V N E L t Z / O L Z + T c K k M S x s q W N G S u / p 7 I a K T 1 N A p s Z 0 T N W C 9 7 u f i f 1 0 t N e O 1 n X C a p Q c k W i 8 J U E B O T / H 0 y 5 A q Z E V N L K F P c 3 k r Y m C r K j A 2 p b E P w l l 9 e J e 1 6 z X N r 3 l 2 9 2 r g s 4 i j B K Z z B B X h w B Q 2 4 h S a 0 g I G E Z 3 i F N 0 c 7 L 8 6 7 8 7 F o X X O K m R P 4 A + f z B 8 m 6 k O c = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Y / P w A T Z U N 3 d D O v W e h A / f B 2 Z u z j 4 = " &gt; A A A B 8 X i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 k J L 0 o s e C F 4 9 V 7 A e 2 o W y 2 k 3 b p Z h N 2 N 0 I J / R d e P C j i 1 X / j z X / j p s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 2 v r G 5 t b 2 6 W d 8 u 7 e / s F h 5 e i 4 r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J T e 5 3 n l B p H s s H M 0 3 Q j + h I 8 p A z a q z 0 e I 8 j h T p 3 B 5 W q W 3 P n I K v E K 0 g V C j Q H l a / + M G Z p h N I w Q b X u e W 5 i / I w q w 5 n A W b m f a k w o m 9 A R 9 i y V N E L t Z / O L Z + T c K k M S x s q W N G S u / p 7 I a K T 1 N A p s Z 0 T N W C 9 7 u f i f 1 0 t N e O 1 n X C a p Q c k W i 8 J U E B O T / H 0 y 5 A q Z E V N L K F P c 3 k r Y m C r K j A 2 p b E P w l l 9 e J e 1 6 z X N r 3 l 2 9 2 r g s 4 i j B K Z z B B X h w B Q 2 4 h S a 0 g I G E Z 3 i F N 0 c 7 L 8 6 7 8 7 F o X X O K m R P 4 A + f z B 8 m 6 k O c = &lt; / l a t e x i t &gt; Classification &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " o I F p z 6 8 V x K y 7 J A k y 7 y z j V 9 K d c P s = " &gt; A A A B 9 X i c b V D L S g M x F L 3 j s 9 Z X 1 a W b Y B F c S J n p R p e F b l x W s A 9 o a 8 m k d 9 r Q T G Z I M k o Z + h 9 u X C j i 1 n 9 x 5 9 + Y a Q f R 1 g O B k 3 P u v b k 5 f i y 4 N q 7 7 5 a y t b 2 x u b R d 2 i r t 7 + w e H p a P j l o 4 S x b D J I h G p j k 8 1 C i 6 x a b g R 2 I k V 0 t A X 2 P Y n 9 c x v P 6 D S P J J 3 Z h p j P 6 Q j y Q P O q L H S f V 1 Q r X + u g 1 L Z r b h z k F X i 5 a Q M O R q D 0 m d v G L E k R G l Y N q n r u b H p p 1 Q Z z g T O i r 1 E Y 0 z Z h I 6 w a 6 m k I e p + O t 9 6 R s 6 t M i R B p O y R h s z V 3 x 0 p D b W e h r 6 t D K k Z 6 2 U v E / / z u o k J r v s p l 3 F i U L L F Q 0 E i i I l I F g E Z c o X M i K k l l C l u d y V s T B V l x g Z V t C F 4 y 1 9 e J a 1 q x X M r 3 m 2 1 X L v M 4 y j A K Z z B B X h w B T W 4 g Q Y 0 g Y G C J 3 i B V + f R e X b e n P d F 6 Z q T 9 5 z A H z g f 3 8 Z U k p o = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " o I F p z 6 8 V x K y 7 J A k y 7 y z j V 9 K d c P s = " &gt; A A A B 9 X i c b V D L S g M x F L 3 j s 9 Z X 1 a W b Y B F c S J n p R p e F b l x W s A 9 o a 8 m k d 9 r Q T G Z I M k o Z + h 9 u X C j i 1 n 9 x 5 9 + Y a Q f R 1 g O B k 3 P u v b k 5 f i y 4 N q 7 7 5 a y t b 2 x u b R d 2 i r t 7 + w e H p a P j l o 4 S x b D J I h G p j k 8 1 C i 6 x a b g R 2 I k V 0 t A X 2 P Y n 9 c x v P 6 D S P J J 3 Z h p j P 6 Q j y Q P O q L H S f V 1 Q r X + u g 1 L Z r b h z k F X i 5 a Q M O R q D 0 m d v G L E k R G l Y N q n r u b H p p 1 Q Z z g T O i r 1 E Y 0 z Z h I 6 w a 6 m k I e p + O t 9 6 R s 6 t M i R B p O y R h s z V 3 x 0 p D b W e h r 6 t D K k Z 6 2 U v E / / z u o k J r v s p l 3 F i U L L F Q 0 E i i I l I F g E Z c o X M i K k l l C l u d y V s T B V l x g Z V t C F 4 y 1 9 e J a 1 q x X M r 3 m 2 1 X L v M 4 y j A K Z z B B X h w B T W 4 g Q Y 0 g Y G C J 3 i B V + f R e X b e n P d F 6 Z q T 9 5 z A H z g f 3 8 Z U k p o = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " o I F p z 6 8 V x K y 7 J A k y 7 y z j V 9 K d c P s = " &gt; A A A B 9 X i c b V D L S g M x F L 3 j s 9 Z X 1 a W b Y B F c S J n p R p e F b l x W s A 9 o a 8 m k d 9 r Q T G Z I M k o Z + h 9 u X C j i 1 n 9 x 5 9 + Y a Q f R 1 g O B k 3 P u v b k 5 f i y 4 N q 7 7 5 a y t b 2 x u b R d 2 i r t 7 + w e H p a P j l o 4 S x b D J I h G p j k 8 1 C i 6 x a b g R 2 I k V 0 t A X 2 P Y n 9 c x v P 6 D S P J J 3 Z h p j P 6 Q j y Q P O q L H S f V 1 Q r X + u g 1 L Z r b h z k F X i 5 a Q M O R q D 0 m d v G L E k R G l Y N q n r u b H p p 1 Q Z z g T O i r 1 E Y 0 z Z h I 6 w a 6 m k I e p + O t 9 6 R s 6 t M i R B p O y R h s z V 3 x 0 p D b W e h r 6 t D K k Z 6 2 U v E / / z u o k J r v s p l 3 F i U L L F Q 0 E i i I l I F g E Z c o X M i K k l l C l u d y V s T B V l x g Z V t C F 4 y 1 9 e J a 1 q x X M r 3 m 2 1 X L v M 4 y j A K Z z B B X h w B T W 4 g Q Y 0 g Y G C J 3 i B V + f R e X b e n P d F 6 Z q T 9 5 z A H z g f 3 8 Z U k p o = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " o I F p z 6 8 V x K y 7 J A k y 7 y z j V 9 K d c P s = " &gt; A A A B 9 X i c b V D L S g M x F L 3 j s 9 Z X 1 a W b Y B F c S J n p R p e F b l x W s A 9 o a 8 m k d 9 r Q T G Z I M k o Z + h 9 u X C j i 1 n 9 x 5 9 + Y a Q f R 1 g O B k 3 P u v b k 5 f i y 4 N q 7 7 5 a y t b 2 x u b R d 2 i r t 7 + w e H p a P j l o 4 S x b D J I h G p j k 8 1 C i 6 x a b g R 2 I k V 0 t A X 2 P Y n 9 c x v P 6 D S P J J 3 Z h p j P 6 Q j y Q P O q L H S f V 1 Q r X + u g 1 L Z r b h z k F X i 5 a Q M O R q D 0 m d v G L E k R G l Y N q n r u b H p p 1 Q Z z g T O i r 1 E Y 0 z Z h I 6 w a 6 m k I e p + O t 9 6 R s 6 t M i R B p O y R h s z V 3 x 0 p D b W e h r 6 t D K k Z 6 2 U v E / / z u o k J r v s p l 3 F i U L L F Q 0 E i i I l I F g E Z c o X M i K k l l C l u d y V s T B V l x g Z V t C F 4 y 1 9 e J a 1 q x X M r 3 m 2 1 X L v M 4 y j A K Z z B B X h w B T W 4 g Q Y 0 g Y G C J 3 i B V + f R e X b e n P d F 6 Z q T 9 5 z A H z g f 3 8 Z U k p o = &lt; / l a t e x i t &gt; Init new b k t &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 8 + L s I k j Q B A y A P L W 3 r 2V v O u 0 f D 3 k = " &gt; A A A B / 3 i c b V B N S 8 N A E N 3 U r 1 q / o o I X L 4 u t 4 K k k v e i x 4 E V v F e w H t D F s t p t 2 6 W Y T d i d K i T 3 4 V 7 x 4 U M S r f 8 O b / 8 Z t m 4 O 2 P h h 4 v D f D z L w g E V y D 4 3 x b h Z X V t f W N 4 m Z p a 3 t n d 8 / e P 2 j p O F W U N W k s Y t U J i G a C S 9 Y E D o J 1 E s V I F A j W D k a X U 7 9 9 z 5 T m s b y F c c K 8 i A w k D z k l Y C T f P r q W H L B k D 7 i S 9 Y I Q B 5 O 7 k Q 8 V 3 y 4 7 V W c G v E z c n J R R j o Z v f / X 6 M U 0 j J o E K o n X X d R L w M q K A U 8 E m p V 6 q W U L o i A x Y 1 1 B J I q a 9 b H b / B J 8 a p Y / D W J m S g G f q 7 4 m M R F q P o 8 B 0 R g S G e t G b i v 9 5 3 R T C C y / j M k m B S T p f F K Y C Q 4 y n Y e A + V 4 y C G B t C q O L m V k y H R B E K J r K S C c F d f H m Z t G p V 1 6 m 6 N 7 V y v Z b H U U T H 6 A S d I R e do z q 6 Q g 3 U R B Q 9 o m f 0 i t 6 s J + v F e r c + 5 q 0 F K 5 8 5 R H 9 g f f 4 A i 4 C V F Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 8 + L s I k j Q B A y A P L W 3 r 2V v O u 0 f D 3 k = " &gt; A A A B / 3 i c b V B N S 8 N A E N 3 U r 1 q / o o I X L 4 u t 4 K k k v e i x 4 E V v F e w H t D F s t p t 2 6 W Y T d i d K i T 3 4 V 7 x 4 U M S r f 8 O b / 8 Z t m 4 O 2 P h h 4 v D f D z L w g E V y D 4 3 x b h Z X V t f W N 4 m Z p a 3 t n d 8 / e P 2 j p O F W U N W k s Y t U J i G a C S 9 Y E D o J 1 E s V I F A j W D k a X U 7 9 9 z 5 T m s b y F c c K 8 i A w k D z k l Y C T f P r q W H L B k D 7 i S 9 Y I Q B 5 O 7 k Q 8 V 3 y 4 7 V W c G v E z c n J R R j o Z v f / X 6 M U 0 j J o E K o n X X d R L w M q K A U 8 E m p V 6 q W U L o i A x Y 1 1 B J I q a 9 b H b / B J 8 a p Y / D W J m S g G f q 7 4 m M R F q P o 8 B 0 R g S G e t G b i v 9 5 3 R T C C y / j M k m B S T p f F K Y C Q 4 y n Y e A + V 4 y C G B t C q O L m V k y H R B E K J r K S C c F d f H m Z t G p V 1 6 m 6 N 7 V y v Z b H U U T H 6 A S d I R e do z q 6 Q g 3 U R B Q 9 o m f 0 i t 6 s J + v F e r c + 5 q 0 F K 5 8 5 R H 9 g f f 4 A i 4 C V F Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 8 + L s I k j Q B A y A P L W 3 r 2 V v O u 0 f D 3 k = " &gt; A A A B / 3 i c b V B N S 8 N A E N 3 U r 1 q / o o I X L 4 u t 4 K k k v e i x 4 E V v F e w H t D F s t p t 2 6 W Y T d i d K i T 3 4 V 7 x 4 U M S r f 8 O b / 8 Z t m 4 O 2 P h h 4 v D f D z L w g E V y D 4 3 x b h Z X V t f W N 4 m Z p a 3 t n d 8 / e P 2 j p O F W U N W k s Y t U J i G a C S 9 Y E D o J 1 E s V I F A j W D k a X U 7 9 9 z 5 T m s b y F c c K 8 i A w k D z k l Y C T f P r q W H L B k D 7 i S 9 Y I Q B 5 O 7 k Q 8 V 3 y 4 7 V W c G v E z c n J R R j o Z v f / X 6 M U 0 j J o E K o n X X d R L w M q K A U 8 E m p V 6 q W U L o i A x Y 1 1 B J I q a 9 b H b / B J 8 a p Y / D W J m S g G f q 7 4 m M R F q P o 8 B 0 R g S G e t G b i v 9 5 3 R T C C y / j M k m B S T p f F K Y C Q 4 y n Y e A + V 4 y C G B t C q O L m V k y H R B E K J r K S C c F d f H m Z t G p V 1 6 m 6 N 7 V y v Z b H U U T H 6 A S d I R e d o z q 6 Q g 3 U R B Q 9 o m f 0 i t 6 s J + v F e r c + 5 q 0 F K 5 8 5 R H 9 g f f 4 A i 4 C V F Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 8 + L s I k j Q B A y A P L W 3 r 2 V v O u 0 f D 3 k = " &gt; A A A B / 3 i c b V B N S 8 N A E N 3 U r 1 q / o o I X L 4 u t 4 K k k v e i x 4 E V v F e w H t D F s t p t 2 6 W Y T d i d K i T 3 4 V 7 x 4 U M S r f 8 O b / 8 Z t m 4 O 2 P h h 4 v D f D z L w g E V y D 4 3 x b h Z X V t f W N 4 m Z p a 3 t n d 8 / e P 2 j p O F W U N W k s Y t U J i G a C S 9 Y E D o J 1 E s V I F A j W D k a X U 7 9 9 z 5 T m s b y F c c K 8 i A w k D z k l Y C T f P r q W H L B k D 7 i S 9 Y I Q B 5 O 7 k Q 8 V 3 y 4 7 V W c G v E z c n J R R j o Z v f / X 6 M U 0 j J o E K o n X X d R L w M q K A U 8 E m p V 6 q W U L o i A x Y 1 1 B J I q a 9 b H b / B J 8 a p Y / D W J m S g G f q 7 4 m M R F q P o 8 B 0 R g S G e t G b i v 9 5 3 R T C C y / j M k m B S T p f F K Y C Q 4 y n Y e A + V 4 y C G B t C q O L m V k y H R B E K J r K S C c F d f H m Z t G p V 1 6 m 6 N 7 V y v Z b H U U T H 6 A S d I R e d o z q 6 Q g 3 U R B Q 9 o m f 0 i t 6 s J + v F e r c + 5 q 0 F K 5 8 5 R H 9 g f f 4 A i 4 C V F Q = = &lt; / l a t e x i t &gt; b l a t e x i t s h a 1 _ b a s e 6 4 = " o k L 4 p r Y J u q Y 7 I A Y U h g e b U S I 2 g 8 U = " &gt; A A A B + H i c b V D L S s N A F L 2 p r 1 o f j b o R 3 A w W o R t L 0 o 0 u C 2 5 c V r A P a G O Y T C f t 0 M k k z E y E G v I l b l w o 4 t Z P c e e u n + L 0 s d D W A x c O 5 9 z L v f c E C W d K O 8 6 3 V d j Y 3 N r e K e 6 W 9 v Y P D s v 2 0 X F b x a k k t E V i H s t u g B X l T N C W Z p r T b i I p j g J O O 8 H 4 Z u Z 3 H q l U L B b 3 e p J Q L 8 J D w U J G s D a S b 5 e z f h C i I H 8 Y + 5 m + d H P f r j g 1 Z w 6 0 T t w l q T R O q 9 M p A D R 9 + 6 s / i E k a U a E J x 0 r 1 X C f R X o a l Z o T T v N R P F U 0 w G e M h 7 R k q c E S V l 8 0 P z 9 G F U Q Y o j K U p o d F c / T 2 R 4 U i p S R S Y z g j r k V r 1 Z u J / X i / V 4 b W X M Z G k m g q y W B S m H O k Y z V J A A y Y p 0 X x i C C a S m V s R G W G J i T Z Z l U w I 7 u r L 6 6 R d r 7 l O z b 0z a d R h g S K c w T l U w Y U r a M A t N K E F B F J 4 h l d 4 s 5 6 s F + v d + l i 0 F q z l z A n 8 g f X 5 A 0 Z y l P 0 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " u b d 8 x V a x Q U P v W o x e U f H 1 h x l V L w E = " &gt; A A A B + H i c b V D L S s N A F J 3 4 r P X R q B v B z W A R u r E k 3 e i y I I j L C v Y B b Q 2 T 6 a Q d O p m E m R u h h n y J G x e K u P U v 3 O q q O z / F 6 W O h r Q c u H M 6 5 l 3 v v 8 W P B N T j O 2 F p Z X V v f 2 M x t 5 b d 3 d v c K 9 v 5 B Q 0 e J o q x O I x G p l k 8 0 E 1 y y O n A Q r B U r R k J f s K Y / v J z 4 z X u m N I / k L Y x i 1 g 1 J X / K A U w J G 8 u x C 2 v E D 7 G d 3 Q y + F M z f z 7 K J T d q b A y 8 S d k 2 L 1 q P T 9 M f 6 6 q n n 2 Z 6 c X 0 S R k E q g g W r d d J 4 Z u S h R w K l i W 7 y S a x Y Q O S Z + 1 D Z U k Z L q b T g / P 8 K l R e j i I l C k J e K r + n k h J q P U o 9 E 1 n S G C g F 7 2 J + J / X T i C 4 6 K Z c x g k w S W e L g k R g i P A k B d z j i l E Q I 0 M I V d z c i u m A K E L B Z J U 3 I b i L L y + T R q X s O m X 3 x q R R Q T P k 0 D E 6 Q S X k o n N U R d e o h u q I o g Q 9 o m f 0 Y j 1 Y T 9 a r 9 T Z r X b H m M 4 f o D 6 z 3 H 8 G H l t g = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " u b d 8 x V a x Q U P v W o x e U f H 1 h x l V L w E = " &gt; A A A B + H i c b V D L S s N A F J 3 4 r P X R q B v B z W A R u r E k 3 e i y I I j L C v Y B b Q 2 T 6 a Q d O p m E m R u h h n y J G x e K u P U v 3 O q q O z / F 6 W O h r Q c u H M 6 5 l 3 v v 8 W P B N T j O 2 F p Z X V v f 2 M x t 5 b d 3 d v c K 9 v 5 B Q 0 e J o q x O I x G p l k 8 0 E 1 y y O n A Q r B U r R k J f s K Y / v J z 4 z X u m N I / k L Y x i 1 g 1 J X / K A U w J G 8 u x C 2 v E D 7 G d 3 Q y + F M z f z 7 K J T d q b A y 8 S d k 2 L 1 q P T 9 M f 6 6 q n n 2 Z 6 c X 0 S R k E q g g W r d d J 4 Z u S h R w K l i W 7 y S a x Y Q O S Z + 1 D Z U k Z L q b T g / P 8 K l R e j i I l C k J e K r + n k h J q P U o 9 E 1 n S G C g F 7 2 J + J / X T i C 4 6 K Z c x g k w S W e L g k R g i P A k B d z j i l E Q I 0 M I V d z c i u m A K E L B Z J U 3 I b i L L y + T R q X s O m X 3 x q R R Q T P k 0 D E 6 Q S X k o n N U R d e o h u q I o g Q 9 o m f 0 Y j 1 Y T 9 a r 9 T Z r X b H m M 4 f o D 6 z 3 H 8 G H l t g = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " / f T S Q f + S f A R W o i p J Y r 6 0 E F c z 1 i Q = " &gt; A A A B + H i c b V D L S g N B E O y N r x g f W f X o Z T A I X g y 7 u e g x 4 M V j B P O A Z F 1 m J 7 P J k N k H M 7 1 C X P I l X j w o 4 t V P 8 e b f O E n 2 o I k F D U V V N 9 1 d Q S q F R s f 5 t k o b m 1 v b O + X d y t 7 + w W H V P j r u 6 C R T j L d Z I h P V C 6 j m U s S 8 j Q I l 7 6 W K 0 y i Q v B t M b u Z + 9 5 E r L Z L 4 H q c p 9 y I 6 i k U o G E U j + X Y 1 H w Q h C W Y P E z / H S 3 f m 2 z W n 7 i x A 1 o l b k B o U a P n 2 1 2 C Y s C z i M T J J t e 6 7 T o p e T h U K J v m s M s g 0 T y m b 0 B H v G x r T i G s v X x w + I + d G G Z I w U a Z i J A v 1 9 0 R O I 6 2 n U W A 6 I 4 p j v e r N x f + 8 f o b h t Z e L O M 2 Q x 2 y 5 K M w k w Y T M U y B D o T h D O T W E M i X M r Y S N q a I M T V Y V E 4 K 7 + v I 6 6 T T q r l N 3 7 5 x a s 1 H E U Y Z T O I M L c O E K m n A L L W g D g w y e 4 R X e r C f r x X q 3 P p a t J a u Y O Y E / s D 5 / A D 3 N k r 0 = &lt; / l a t e x i t &gt; b " o k L 4 p r Y J u q Y 7 I A Y U h g e b U S I 2 g 8 U = " &gt; A A A B + H i c b V D L S s N A F L 2 p r 1 o f j b o R 3 A w W o R t L 0 o 0 u C 2 5 c V r A P a G O Y T C f t 0 M k k z E y E G v I l b l w o 4 t Z P c e e u n + L 0 s d D W A x c O 5 9 z L v f c E C W d K O 8 6 3 V d j Y 3 N r e K e 6 W 9 v Y P D s v 2 0 X F b x a k k t E V i H s t u g B X l T N C W Z p r T b i I p j g J O O 8 H 4 Z u Z 3 H q l U L B b 3 e p J Q L 8 J D w U J G s D a S b 5 e z f h C i I H 8 Y + 5 m + d H P f r j g 1 Z w 6 0 T t w l q T R O q 9 M p A D R 9 + 6 s / i E k a U a E J x 0 r 1 X C f R X o a l Z o T T v N R P F U 0 w G e M h 7 R k q c E S V l 8 0 P z 9 G F U Q Y o j K U p o d F c / T 2 R 4 U i p S R S Y z g j r k V r 1 Z u J / X i / V 4 b W X M Z G k m g q y W B S m H O k Y z V J A A y Y p 0 X x i C C a S m V s R G W G J i T Z Z l U w I 7 u r L 6 6 R d r 7 l O z b 0 z a d R h g S K c w T l U w Y U r a M A t N K E F B F J 4 h l d 4 s 5 6 s F + v d + l i 0 F q z l z A n 8 g f X 5 A 0 Z y l P 0 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " u b d 8 x V a x Q U P v W o x e U f H 1 h x l V L w E = " &gt; A A A B + H i c b V D L S s N A F J 3 4 r P X R q B v B z W A R u r E k 3 e i y I I j L C v Y B b Q 2 T 6 a Q d O p m E m R u h h n y J G x e K u P U v 3 O q q O z / F 6 W O h r Q c u HM 6 5 l 3 v v 8 W P B N T j O 2 F p Z X V v f 2 M x t 5 b d 3 d v c K 9 v 5 B Q 0 e J o q x O I x G p l k 8 0 E 1 y y O n A Q r B U r R k J f s K Y / v J z 4 z X u m N I / k L Y x i 1 g 1 J X / K A U w J G 8 u x C 2 v E D 7 G d 3 Q y + F M z f z 7 K J T d q b A y 8 S d k 2 L 1 q P T 9 M f 6 6 q n n 2 Z 6 c X 0 S R k E q g g W r d d J 4 Z u S h R w K l i W 7 y S a x Y Q O S Z + 1 D Z U k Z L q b T g / P 8 K l R e j i I l C k J e K r + n k h J q P U o 9 E 1 n S G C g F 7 2 J + J / X T i C 4 6 K Z c x g k w S W e L g k R g i P A k B d z j i l E Q I 0 M I V d z c i u m A K E L B Z J U 3 I b i L L y + T R q X s O m X 3 x q R R Q T P k 0 D E 6 Q S X k o n N U R d e o h u q I o g Q 9 o m f 0 Y j 1 Y T 9a r 9 T Z r X b H m M 4 f o D 6 z 3 H 8 G H l t g = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " u b d 8 x V a x Q U P v W o x e U f H 1 h x l V L w E = " &gt; A A A B + H i c b V D L S s N A F J 3 4 r P X R q B v B z W A R u r E k 3 e i y I I j L C v Y B b Q 2 T 6 a Q d O p m E m R u h h n y J G x e K u P U v 3 O q q O z / F 6 W O h r Q c u HM 6 5 l 3 v v 8 W P B N T j O 2 F p Z X V v f 2 M x t 5 b d 3 d v c K 9 v 5 B Q 0 e J o q x O I x G p l k 8 0 E 1 y y O n A Q r B U r R k J f s K Y / v J z 4 z X u m N I / k L Y x i 1 g 1 J X / K A U w J G 8 u x C 2 v E D 7 G d 3 Q y + F M z f z 7 K J T d q b A y 8 S d k 2 L 1 q P T 9 M f 6 6 q n n 2 Z 6 c X 0 S R k E q g g W r d d J 4 Z u S h R w K l i W 7 y S a x Y Q O S Z + 1 D Z U k Z L q b T g / P 8 K l R e j i I l C k J e K r + n k h J q P U o 9 E 1 n S G C g F 7 2 J + J / X T i C 4 6 K Z c x g k w S W e L g k R g i P A k B d z j i l E Q I 0 M I V d z c i u m A K E L B Z J U 3 I b i L L y + T R q X s O m X 3 x q R R Q T P k 0 D E 6 Q S X k o n N U R d e o h u q I o g Q 9 o m f 0 Y j 1 Y T 9 a r 9 T Z r X b H m M 4 f o D 6 z 3 H 8 G H l t g = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " / f T S Q f + S f A R W o i p J Y r 6 0 E F c z 1 i Q = " &gt; A A A B + H i c b V D L S g N B E O y N r x g f W f X o Z T A I X g y 7 u e g x 4 M V j B P O A Z F 1 m J 7 P J k N k H M 7 1 C X P I l X j w o 4 t V P 8 e b f O E n 2 o I k F D U V V N 9 1 d Q S q F R s f 5 t k o b m 1 v b O + X d y t 7 + w W H V P j r u 6 C R T j L d Z I h P V C 6 j m U s S 8 j Q I l 7 6 W K 0 y i Q v B t M b u Z + 9 5 E r L Z L 4 H q c p 9 y I 6 i k U o G E U j + X Y 1 H w Q h C W Y P E z / H S 3 f m 2 z W n 7 i x A 1 o l b k B o U a P n 2 1 2 C Y s C z i M T J J t e 6 7 T o p e T h U K J v m s M s g 0 T y m b 0 B H v G x r T i G s v X x w + I + d G G Z I w U a Z i J A v 1 9 0 R O I 6 2 n U W A 6 I 4 p j v e r N x f + 8 f o b h t Z e L O M 2 Q x 2 y 5 K M w k w Y T M U y B D o T h D O T W E M i X M r Y S N q a I M T V Y V E 4 K 7 + v I 6 6 T T q r l N 3 7 5 x a s 1 H E U Y Z T O I M L c O E K m n A L L W g D g w y e 4 R X e r C f r x X q 3 P p a t J a u Y O Y E / s D 5 / A D 3 N k r 0 = &lt; / l a t e x i t &gt; Detection &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " a 3 z P z j 8 5 2 A u s n f o G g A R u K 7 O + b N w = " &gt; A A A B 8 H i c b V D L S s N A F L 3 x W e u r 6 t L N Y B F c S E m 6 0 W V B F y 4 r 2 I e 0 o U y m N + 3 Q y S T M T I Q S + h V u X C j i 1 s 9 x 5 9 8 4 a b P Q 1 g M D h 3 P u Z e 4 5 Q S K 4 N q 7 7 7 a y t b 2 x u b Z d 2 y r t 7 + w e H l a P j t o 5 T x b D F Y h G r b k A 1 C i 6 x Z b g R 2 E 0 U 0 i g Q 2 A k m N 7 n f e U K l e S w f z D R B P 6 I j y U P O q L H S 4 y 0 a Z D k b V K p u z Z 2 D r B K v I F U o 0 B x U v v r D m K U R S s M E 1 b r n u Y n x M 6 o M Z w J n 5 X 6 q M a F s Q k f Y s 1 T S C L W f z Q + e k X O r D E k Y K / u k I X P 1 9 0 Z G I 6 2 n U W A n I 2 r G e t n L x f + 8 X m r C a z / j M k k N S r b 4 K E w F M T H J 0 5 M h V z a v m F p C m e L 2 V s L G V F F m b E d l W 4 K 3 H H m V t O s 1 z 6 1 5 9 / V q 4 7 K o o w S n c A Y X 4 M E V N O A O m t A C B h E 8 w y u 8 O c p 5 c d 6 d j 8 X o m l P s n M A f O J 8 / 2 4 a Q W w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " a 3 z P z j 8 5 2 A u s n f o G g A R u K 7 O + b N w = " &gt; A A A B 8 H i c b V D L S s N A F L 3 x W e u r 6 t L N Y B F c S E m 6 0 W V B F y 4 r 2 I e 0 o U y m N + 3 Q y S T M T I Q S + h V u X C j i 1 s 9 x 5 9 8 4 a b P Q 1 g M D h 3 P u Z e 4 5 Q S K 4 N q 7 7 7 a y t b 2 x u b Z d 2 y r t 7 + w e H l a P j t o 5 T x b D F Y h G r b k A 1 C i 6 x Z b g R 2 E 0 U 0 i g Q 2 A k m N 7 n f e U K l e S w f z D R B P 6 I j y U P O q L H S 4 y 0 a Z D k b V K p u z Z 2 D r B K v I F U o 0 B x U v v r D m K U R S s M E 1 b r n u Y n x M 6 o M Z w J n 5 X 6 q M a F s Q k f Y s 1 T S C L W f z Q + e k X O r D E k Y K / u k I X P 1 9 0 Z G I 6 2 n U W A n I 2 r G e t n L x f + 8 X m r C a z / j M k k N S r b 4 K E w F M T H J 0 5 M h V z a v m F p C m e L 2 V s L G V F F m b E d l W 4 K 3 H H m V t O s 1 z 6 1 5 9 / V q 4 7 K o o w S n c A Y X 4 M E V N O A O m t A C B h E 8 w y u 8 O c p 5 c d 6 d j 8 X o m l P s n M A f O J 8 / 2 4 a Q W w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " a 3 z P z j 8 5 2 A u s n f o G g A R u K 7 O + b N w = " &gt; A A A B 8 H i c b V D L S s N A F L 3 x W e u r 6 t L N Y B F c S E m 6 0 W V B F y 4 r 2 I e 0 o U y m N + 3 Q y S T M T I Q S + h V u X C j i 1 s 9 x 5 9 8 4 a b P Q 1 g M D h 3 P u Z e 4 5 Q S K 4 N q 7 7 7 a y t b 2 x u b Z d 2 y r t 7 + w e H l a P j t o 5 T x b D F Y h G r b k A 1 C i 6 x Z b g R 2 E 0 U 0 i g Q 2 A k m N 7 n f e U K l e S w f z D R B P 6 I j y U P O q L H S 4 y 0 a Z D k b V K p u z Z 2 D r B K v I F U o 0 B x U v v r D m K U R S s M E 1 b r n u Y n x M 6 o M Z w J n 5 X 6 q M a F s Q k f Y s 1 T S C L W f z Q + e k X O r D E k Y K / u k I X P 1 9 0 Z G I 6 2 n U W A n I 2 r G e t n L x f + 8 X m r C a z / j M k k N S r b 4 K E w F M T H J 0 5 M h V z a v m F p C m e L 2 V s L G V F F m b E d l W 4 K 3 H H m V t O s 1 z 6 1 5 9 / V q 4 7 K o o w S n c A Y X 4 M E V N O A O m t A C B h E 8 w y u 8 O c p 5 c d 6 d j 8 X o m l P s n M A f O J 8 / 2 4 a Q W w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " a 3 z P z j 8 5 2 A u s n f o G g A R u K 7 O + b N w = " &gt; A A A B 8 H i c b V D L S s N A F L 3 x W e u r 6 t L N Y B F c S E m 6 0 W V B F y 4 r 2 I e 0 o U y m N + 3 Q y S T M T I Q S + h V u X C j i 1 s 9 x 5 9 8 4 a b P Q 1 g M D h 3 P u Z e 4 5 Q S K 4 N q 7 7 7 a y t b 2 x u b Z d 2 y r t 7 + w e H l a P j t o 5 T x b D F Y h G r b k A 1 C i 6 x Z b g R 2 E 0 U 0 i g Q 2 A k m N 7 n f e U K l e S w f z D R B P 6 I j y U P O q L H S 4 y 0 a Z D k b V K p u z Z 2 D r B K v I F U o 0 B x U v v r D m K U R S s M E 1 b r n u Y n x M 6 o M Z w J n 5 X 6 q M a F s Q k f Y s 1 T S C L W f z Q + e k X O r D E k Y K / u k I X P 1 9 0 Z G I 6 2 n U W A n I 2 r G e t n L x f + 8 X m r C a z / j M k k N S r b 4 K E w F M T H J 0 5 M h V z a v m F p C m e L 2 V s L G V F F m b E d l W 4 K 3 H H m V t O s 1 z 6 1 5 9 / V q 4 7 K o o w S n c A Y X 4 M E V N O A O m t A C B h E 8 w y u 8 O c p 5 c d 6 d j 8 X o m l P s n M A f O J 8 / 2 4 a Q W w = = &lt; / l a t e x i t &gt; Kill b k t ? &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " B Z W O I h b 3 2 8 d + 9 U t k v C R n o w r A j Q k = " &gt; A A A B / H i c b V B N S 8 N A E N 3 4 W e t X t E c v i 6 3 g q S S 9 6 M 2 C F 8 F L B f s B b Q y b 7 a Z d u t m E 3 Y k Q Q v 0 r X j w o 4 t U f 4 s 1 / 4 7 b N Q V s f D D z e m 2 F m X p A I r s F x v q 2 1 9 Y 3 N r e 3 S T n l 3 b / / g 0 D 4 6 7 u g 4 V Z S 1 a S x i 1 Q u I Z o J L 1 g Y O g v U S x U g U C N Y N J t c z v / v I l O a x v I c s Y V 5 E R p K H n B I w k m 9 X b r k Q u J Y P g h A H 0 4 e J D 7 U r 3 6 4 6 d W c O v E r c g l R R g Z Z v f w 2 G M U 0 j J o E K o n X f d R L w c q K A U 8 G m 5 U G q W U L o h I x Y 3 1 B J I q a 9 f H 7 8 F J 8 Z Z Y j D W J m S g O f q 7 4 m c R F p n U W A 6 I w J j v e z N x P + 8 f g r h p Z d z m a T A J F 0 s C l O B I c a z J P C Q K 0 Z B Z I Y Q q r i 5 F d M x U Y S C y a t s Q n C X X 1 4 l n U b d d e r u X a P a b B R x l N A J O k X n y E U X q I l u U A u 1 E U U Z e k a v 6 M 1 6 s l 6 s d + t j 0 b p m F T M V 9 A f W 5 w 8 0 m 5 P E &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " B Z W O I h b 3 2 8 d + 9 U t k v C R n o w r A j Q k = " &gt; A A A B / H i c b V B N S 8 N A E N 3 4 W e t X t E c v i 6 3 g q S S 9 6 M 2 C F 8 F L B f s B b Q y b 7 a Z d u t m E 3 Y k Q Q v 0 r X j w o 4 t U f 4 s 1 / 4 7 b N Q V s f D D z e m 2 F m X p A I r s F x v q 2 1 9 Y 3 N r e 3 S T n l 3 b / / g 0 D 4 6 7 u g 4 V Z S 1 a S x i 1 Q u I Z o J L 1 g Y O g v U S x U g U C N Y N J t c z v / v I l O a x v I c s Y V 5 E R p K H n B I w k m 9 X b r k Q u J Y P g h A H 0 4 e J D 7 U r 3 6 4 6 d W c O v E r c g l R R g Z Z v f w 2 G M U 0 j J o E K o n X f d R L w c q K A U 8 G m 5 U G q W U L o h I x Y 3 1 B J I q a 9 f H 7 8 F J 8 Z Z Y j D W J m S g O f q 7 4 m c R F p n U W A 6 I w J j v e z N x P + 8 f g r h p Z d z m a T A J F 0 s C l O B I c a z J P C Q K 0 Z B Z I Y Q q r i 5 F d M x U Y S C y a t s Q n C X X 1 4 l n U b d d e r u X a P a b B R x l N A J O k X n y E U X q I l u U A u 1 E U U Z e k a v 6 M 1 6 s l 6 s d + t j 0 b p m F T M V 9 A f W 5 w 8 0 m 5 P E &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " B Z W O I h b 3 2 8 d + 9 U t k v C R n o w r A j Q k = " &gt; A A A B / H i c b V B N S 8 N A E N 3 4 W e t X t E c v i 6 3 g q S S 9 6 M 2 C F 8 F L B f s B b Q y b 7 a Z d u t m E 3 Y k Q Q v 0 r X j w o 4 t U f 4 s 1 / 4 7 b N Q V s f D D z e m 2 F m X p A I r s F x v q 2 1 9 Y 3 N r e 3 S T n l 3 b / / g 0 D 4 6 7 u g 4 V Z S 1 a S x i 1 Q u I Z o J L 1 g Y O g v U S x U g U C N Y N J t c z v / v I l O a x v I c s Y V 5 E R p K H n B I w k m 9 X b r k Q u J Y P g h A H 0 4 e J D 7 U r 3 6 4 6 d W c O v E r c g l R R g Z Z v f w 2 G M U 0 j J o E K o n X f d R L w c q K A U 8 G m 5 U G q W U L o h I x Y 3 1 B J I q a 9 f H 7 8 F J 8 Z Z Y j D W J m S g O f q 7 4 m c R F p n U W A 6 I w J j v e z N x P + 8 f g r h p Z d z m a T A J F 0 s C l O B I c a z J P C Q K 0 Z B Z I Y Q q r i 5 F d M x U Y S C y a t s Q n C X X 1 4 l n U b d d e r u X a P a b B R x l N A J O k X n y E U X q I l u U A u 1 E U U Z e k a v 6 M 1 6 s l 6 s d + t j 0 b p m F T M V 9 A f W 5 w 8 0 m 5 P E &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " B Z W O I h b 3 2 8 d + 9 U t k v C R n o w r A j Q k = " &gt; A A A B / H i c b V B N S 8 N A E N 3 4 W e t X t E c v i 6 3 g q S S 9 6 M 2 C F 8 F L B f s B b Q y b 7 a Z d u t m E 3 Y k Q Q v 0 r X j w o 4 t U f 4 s 1 / 4 7 b N Q V s f D D z e m 2 F m X p A I r s F x v q 2 1 9 Y 3 N r e 3 S T n l 3 b / / g 0 D 4 6 7 u g 4 V Z S 1 a S x i 1 Q u I Z o J L 1 g Y O g v U S x U g U C N Y N J t c z v / v I l O a x v I c s Y V 5 E R p K H n B I w k m 9 X b r k Q u J Y P g h A H 0 4 e J D 7 U r 3 6 4 6 d W c O v E r c g l R R g Z Z v f w 2 G M U 0 j J o E K o n X f d R L w c q K A U 8 G m 5 U G q W U L o h I x Y 3 1 B J I q a 9 f H 7 8 F J 8 Z Z Y j D W J m S g O f q 7 4 m c R F p n U W A 6 I w J j v e z N x P + 8 f g r h p Z d z m a T A J F 0 s C l O B I c a z J P C Q K 0 Z B Z I Y Q q r i 5 F d M x U Y S C y a t s Q n C X X 1 4 l n U b d d e r u X a P a b B R x l N A J O k X n y E U X q I l u U A u 1 E U U Z e k a v 6 M 1 6 s l 6 s d + t j 0 b p m F T M V 9 A f W 5 w 8 0 m 5 P E &lt; / l a t e x i t &gt; b k t &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 3 s M d P i N 3 y Q n u 0 y G w R s g b 6 D C t 4 k 8 = " &gt; A A A B 8 n i c b V D J S g N B E K 1 x j X G L 8 e i l M Q g 5 h Z l c 9 B j w 4 j G C W W A y h p 5 O T 9 K k p 2 f o r h H C k M / w 4 s E F r 3 6 N N / F n 7 C w H T X x Q 8 H i v i q p 6 Y S q F Q d f 9 c j Y 2 t 7 Z 3 d g t 7 x f 2 D w 6 P j 0 k m 5 b Z J M M 9 5 i i U x 0 N 6 S G S 6 F 4 C w V K 3 k 0 1 p 3 E o e S c c X 8 / 8 z g P X R i T q D i c p D 2 I 6 V C I S j K K V / L w X R i S c 3 o / 7 2 C 9 V 3 J o 7 B 1 k n 3 p J U G u X q 9 y s A N P u l z 9 4 g Y V n M F T J J j f E 9 N 8 U g p x o F k 3 x a 7 G W G p 5 S N 6 Z D 7 l i o a c x P k 8 5 O n 5 M I q A x I l 2 p Z C M l d / T + Q 0 N m Y S h 7 Y z p j g y q 9 5 M / M / z M 4 y u g l y o N E O u 2 G J R l E m C C Z n 9 T w Z C c 4 Z y Y g l l W t h b C R t R T R n a l I o 2 B G / 1 5 X X S r t c 8 t + b d 2 j T q s E A B z u A c q u D B J T T g B p r Q A g Y J P M I z v D j o P D l v z v u i d c N Z z p z C H z g f P 7 x z k w M = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Y o G J d / f j c p 3 f V R y 9 u A D 9 1 M N a V 2 4 = " &gt; A A A B 8 n i c b V A 9 S w N B E N 2 L X z E a j b G 0 W Q x C q n C X R s u A j W U E 8 w G X M + x t 9 p I l e 7 v n 7 p w Q j v w C a x s L R W z z a + z E P + P m o 9 D E B w O P 9 2 a Y m R c m g h t w 3 S 8 n t 7 W 9 s 7 u X 3 y 8 c H B a P j k s n 5 b Z R q a a s R Z V Q u h s S w w S X r A U c B O s m m p E 4 F K w T j q / n f u e R a c O V</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Y o G J d / f j c p 3 f V R y 9 u A D 9 1 M N a V 2 4 = " &gt; A A A B 8 n i c b V A 9 S w N B E N 2 L X z E a j b G 0 W Q x C q n C X R s u A j W U E 8 w G X M + x t 9 p I l e 7 v n 7 p w Q j v w C a x s L R W z z a + z E P + P m o 9 D E B w O P 9 2 a Y m R c m g h t w 3 S 8 n t 7 W 9 s 7 u X 3 y 8 c H B a P j k s n 5 b Z R q a a s R Z V Q u h s S w w S X r A U c B O s m m p E 4 F K w T j q / n f u e R a c O V</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " I v 4 d D b z k m w K f 2 6 F x X w A f G m c 5 7 5 Y = " &gt; A A A B 8 n i c b V A 9 S w N B E J 3 z M 8 a v q K X N Y h C s w l 0 a L Q M 2 l h H M B y R n 2 N v s J U v 2 d o / d O S E c + R k 2 F o r Y + m v s / D d u k i s 0 8 c H A 4 7 0 Z Z u Z F q R Q W f f / b 2 9 j c 2 t 7 Z L e 2 V 9 w 8 O j 4 4 r J 6 d t q z P D e I t p q U 0 3 o p Z L o X g L B U r e T Q 2 n S S R 5 J 5 r c z v 3 O E z d W a P W A 0 5 S H C R 0 p E Q t G 0 U m 9 v B / F J J o 9 T g Y 4 q F T 9 m r 8 A W S d B Q a p Q o D m o f P W H m m U J V 8 g k t b Y X + C m G O T U o m O S z c j + z P K V s Q k e 8 5 6 i i C b d h v j h 5 R i 6 d M i S x N q 4 U k o X 6 e y K n i b X T J H K d C c W x X f X m 4 n 9 e L 8 P 4 J s y F S j P k i i 0 X x Z k k q M n 8 f z I U h j O U U 0 c o M 8 L d S t i Y G s r Q p V R 2 I Q S r L 6 + T d r 0 W + L X g 3 q 8 2 6 k U c J T i H C 7 i C A K 6 h A X f Q h B Y w 0 P A M r / D m o f f i v X s f y 9 Y N r 5 g 5 g z / w P n 8 A G L y R D g = = &lt; / l a t e x i t &gt; s k t &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " W v s D k t U r J R 2 2 n v y p r Y 4 p r o A J</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>2 e 8 m S v b 1 j d 0 4 I R 3 6 D j Y U i g p U / y E 7 8 M 2 4 + C k 1 8 M P B 4 b 4 a Z e W E q h U H X / X I K G 5 t b 2 z v F 3 d L e / s H h U f m 4 0 j Z J p h n 3 W S I T 3 Q 2 p 4 V I o 7 q N A y b u p 5 j Q O J e + E 4 6 u Z 3 3 n g 2 o h E 3 e I k 5 U F M h 0 p E g l G 0 k m / u x 3 3 s l 6 t u 3 Z 2 D r B N v S a r N S u 3 7 H Q B a / f L n 3 S B h W c w V M k m N 6 X l u i k F O N Q o m + b R 0 l x m e U j a m Q 9 6 z V N G Y m y C f H z s l 5 1 Y Z k C j R t h S S u f p 7 I q e x M Z M 4 t J 0 x x Z F Z 9 W b i f 1 4 v w + g y y I V K M + S K L R Z F m S S Y k N n n Z C A 0 Z y g n l l C m h b 2 V s B H V l K H N p 2 R D 8 F Z f X i f t R t 1 z 6 9 6 N T a M B C x T h F M 6 g B h 5 c Q B O u o Q U + M B D w C M / w 4 i j n y X l 1 3 h a t B W c 5 c w J / 4 H z 8 A I S v k J w = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " / W m / 2 P Y v B w f P 1 o N E C N O d Q 0 c s R Z I = " &gt; A A A B 7 H i c b V A 9 T w J B E J 3 D L 0 R R x N L m I j G h I n c 0 W p L Y W G L i A Q m c Z G / Z g w 1 7 e + f u n A m 5 8 A s s b C w 0 x t b K X 2 N n / D M u H 4 W C L 5 n k 5 b 2 Z z M w L E s E 1 O s 6 X l d v Y 3 N r e y e 8 W 9 v a L B 4 e l o 3 J L x 6 m i z K O x i F U n I J o J L p m H H A X r J</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>9 a r 9 b Z o z V n L m W P 4 A + v 9 B / d B k b E = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " M y Q l s A A G z X F t v 3 G T e S C 6 l s e I H Z Q = " &gt; A A A B 7 H i c b V B N T 8 J A E J 3 i F + I X 6 t H L R m L i i b R c 5 E j i x S M m F k i g k u 2 y h Q 3 b b b M 7 N S E N v 8 G L B 4 3 x 6 g / y 5 r 9 x g R 4 U f M k k L + / N Z G Z e m E p h 0 H W / n d L W 9 s 7 u X n m / c n B 4 d H x S P T 3 r m C T T j P s s k Y n u h d R w K R T 3 U a D k v V R z G o e S d 8 P p 7 c L v P n F t R K I e c J b y I K Z j J S L B K F r J N 4 / T I Q 6 r N b f u L k E 2 i V e Q G h R o D 6 t f g 1 H C s p g r Z J I a 0 / f c F I O c a h R M 8 n l l k B m e U j a l Y 9 6 3 V N G Y m y B f H j s n V 1 Y Z k S j R t h S S p f p 7 I q e x M b M 4 t J 0 x x Y l Z 9 x b i f 1 4 / w 6 g Z 5 E K l G X L F V o u i T B J M y O J z M h K a M 5 Q z S y j T w t 5 K 2 I R q y t D m U 7 E h e O s v b 5 J O o + 6 5 d e / e r b U a R R x l u I B L u A Y P b q A F d 9 A G H x g I e I Z X e H O U 8 + K 8 O x + r 1 p J T z J z D H z i f P + D p j q c = &lt; / l a t e x i t &gt; t &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " z e s 3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>e u 6 4 r k V r 3 F T r l X z O A p w D h d w B R 7 c Q g 3 u o Q 5 N E I D w D K / w 5 j w 6 L 8 6 7 8 7 F s 3 X D y m T P 4 A + f z B 9 0 n j O 4 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " z e s 3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>e u 6 4 r k V r 3 F T r l X z O A p w D h d w B R 7 c Q g 3 u o Q 5 N E I D w D K / w 5 j w 6 L 8 6 7 8 7 F s 3 X D y m T P 4 A + f z B 9 0 n j O 4 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " z e s 3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>e u 6 4 r k V r 3 F T r l X z O A p w D h d w B R 7 c Q g 3 u o Q 5 N E I D w D K / w 5 j w 6 L 8 6 7 8 7 F s 3 X D y m T P 4 A + f z B 9 0 n j O 4 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " z e s 3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>1 &lt;Figure 1 :</head><label>11</label><figDesc>e u 6 4 r k V r 3 F T r l X z O A p w D h d w B R 7 c Q g 3 u o Q 5 N E I D w D K / w 5 j w 6 L 8 6 7 8 7 F s 3 X D y m T P 4 A + f z B 9 0 n j O 4 = &lt; / l a t e x i t &gt; t &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " z e s 3 v B F X x j N R 5 a b V q S 2 G q V m U 2 R 0 = " &gt; A A A B 6 H i c b V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 h V 0 R z D H g x W M C 5 g H J E m Y n v c m Y 2 Q c z v U I I + Q I v H h T x 6 i d 5 8 2 + c J H v Q x I K G o q q b 7 q 4 g V d K Q 6 3 4 7 G 5 t b 2 z u 7 h b 3 i / s H h 0 X H p 5 L R l k k w L b I p E J b o T c I N K x t g k S Q o 7 q U Y e B Q r b w f h u 7 r e f U B u Z x A 8 0 S d G P + D C W o R S c r N S g f q n s V t w F 2 D r x c l K G H P V + 6 a s 3 S E Q W Y U x C c W O 6 n p u S P + W a p F A 4 K / Y y g y k X Y z 7 E r q U x j 9 D 4 0 8 W h M 3 Z p l Q E L E 2 0 r J r Z Q f 0 9 M e W T M J A p s Z 8 R p Z F a 9 u f i f 1 8 0 o r P p T G a c Z Y S y W i 8 J M M U r Y / G s 2 k B o F q Y k l X G h p b 2 V i x D U X Z L M p 2 h C 8 1 Z f X S e u 6 4 r k V r 3 F T r l X z O A p w D h d w B R 7 c Q g 3 u o Q 5 N E I D w D K / w 5 j w 6 L 8 6 7 8 7 F s 3 X D y m T P 4 A + f z B 9 0 n j O 4 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " z e s 3 v B F X x j N R 5 a b V q S 2 G q V m U 2 R 0 = " &gt; A A A B 6 H i c b V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 h V 0 R z D H g x W M C 5 g H J E m Y n v c m Y 2 Q c z v U I I + Q I v H h T x 6 i d 5 8 2 + c J H v Q x I K G o q q b 7 q 4 g V d K Q 6 3 4 7 G 5 t b 2 z u 7 h b 3 i / s H h 0 X H p 5 L R l k k w L b I p E J b o T c I N K x t g k S Q o 7 q U Y e B Q r b w f h u 7 r e f U B u Z x A 8 0 S d G P + D C W o R S c r N S g f q n s V t w F 2 D r x c l K G H P V + 6 a s 3 S E Q W Y U x C c W O 6 n p u S P + W a p F A 4 K / Y y g y k X Y z 7 E r q U x j 9 D 4 0 8 W h M 3 Z p l Q E L E 2 0 r J r Z Q f 0 9 M e W T M J A p s Z 8 R p Z F a 9 u f i f 1 8 0 o r P p T G a c Z Y S y W i 8 J M M U r Y / G s 2 k B o F q Y k l X G h p b 2 V i x D U X Z L M p 2 h C 8 1 Z f X S e u 6 4 r k V r 3 F T r l X z O A p w D h d w B R 7 c Q g 3 u o Q 5 N E I D w D K / w 5 j w 6 L 8 6 7 8 7 F s 3 X D y m T P 4 A + f z B 9 0 n j O 4 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " z e s 3 v B F X x j N R 5 a b V q S 2 G q V m U 2 R 0 = " &gt; A A A B 6 H i c b V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 h V 0 R z D H g x W M C 5 g H J E m Y n v c m Y 2 Q c z v U I I + Q I v H h T x 6 i d 5 8 2 + c J H v Q x I K G o q q b 7 q 4 g V d K Q 6 3 4 7 G 5 t b 2 z u 7 h b 3 i / s H h 0 X H p 5 L R l k k w L b I p E J b o T c I N K x t g k S Q o 7 q U Y e B Q r b w f h u 7 r e f U B u Z x A 8 0 S d G P + D C W o R S c r N S g f q n s V t w F 2 D r x c l K G H P V + 6 a s 3 S E Q W Y U x C c W O 6 n p u S P + W a p F A 4 K / Y y g y k X Y z 7 E r q U x j 9 D 4 0 8 W h M 3 Z p l Q E L E 2 0 r J r Z Q f 0 9 M e W T M J A p s Z 8 R p Z F a 9 u f i f 1 8 0 o r P p T G a c Z Y S y W i 8 J M M U r Y / G s 2 k B o F q Y k l X G h p b 2 V i x D U X Z L M p 2 h C 8 1 Z f X S e u 6 4 r k V r 3 F T r l X z O A p w D h d w B R 7 c Q g 3 u o Q 5 N E I D w D K / w 5 j w 6 L 8 6 7 8 7 F s 3 X D y m T P 4 A + f z B 9 0 n j O 4 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " z e s 3 v B F X x j N R 5 a b V q S 2 G q V m U 2 R 0 = " &gt; A A A B 6 H i c b V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 h V 0 R z D H g x W M C 5 g H J E m Y n v c m Y 2 Q c z v U I I + Q I v H h T x 6 i d 5 8 2 + c J H v Q x I K G o q q b 7 q 4 g V d K Q 6 3 4 7 G 5 t b 2 z u 7 h b 3 i / s H h 0 X H p 5 L R l k k w L b I p E J b o T c I N K x t g k S Q o 7 q U Y e B Q r b w f h u 7 r e f U B u Z x A 8 0 S d G P + D C W o R S c r N S g f q n s V t w F 2 D r x c l K G H P V + 6 a s 3 S E Q W Y U x C c W O 6 n p u S P + W a p F A 4 K / Y y g y k X Y z 7 E r q U x j 9 D 4 0 8 W h M 3 Z p l Q E L E 2 0 r J r Z Q f 0 9 M e W T M J A p s Z 8 R p Z F a 9 u f i f 1 8 0 o r P p T G a c Z Y S y W i 8 J M M U r Y / G s 2 k B o F q Y k l X G h p b 2 V i x D U X Z L M p 2 h C 8 1 Z f X S e u 6 4 r k V r 3 F T r l X z O A p w D h d w B R 7 c Q g 3 u o Q 5 N E I D w D K / w 5 j w 6 L 8 6 7 8 7 F s 3 X D y m T P 4 A + f z B 9 0 n j O 4 = &lt; / l a t e x i t &gt; t l a t e x i t s h a 1 _ b a s e 6 4 = " m Y y e R Y K F t l p L J P z e f R y q E G c K O r w = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B i y W R g j 0 W v H i s a D + g D W W z 3 b R L N 5 u w O x F K 6 E / w 4 k E R r / 4 i b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k Q K g 6 7 7 7 R Q 2 N r e 2 d 4 q 7 p b 3 9 g 8 O j 8 v F J 2 8 S p Z r z F Y h n r b k A N l 0 L x F g q U v J t o T q N A 8 k 4 w u Z 3 7 n S e u j Y j V I 0 4 T 7 k d 0 p E Q o G E U r P e C V N y h X 3 K q 7 A F k n X k 4 q k K M 5 K H / 1 h z F L I 6 6 Q S W p M z 3 M T 9 D O q U T D J Z 6 V + a n h C 2 Y S O e M 9 S R S N u / G x x 6 o x c W G V I w l j b U k g W 6 u + J j E b G T K P A d k Y U x 2 b V m 4 v / e b 0 U w 7 q f C Z W k y B V b L g p T S T A m 8 7 / J U G j O U E 4 t o U w L e y t h Y 6 o p Q 5 t O y Y b g r b 6 8 T t r X V c + t e v e 1 S q O e x 1 G E M z i H S / D g B h p w B 0 1 o A Y M R P M M r v D n S e X H e n Y 9 l a 8 H J Z 0 7 h D 5 z P H 7 a N j W A = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " m Y y e R Y K F t l p L J P z e f R y q E G c K O r w = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B i y W R g j 0 W v H i s a D + g D W W z 3 b R L N 5 u w O x F K 6 E / w 4 k E R r / 4 i b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k Q K g 6 7 7 7 R Q 2 N r e 2 d 4 q 7 p b 3 9 g 8 O j 8 v F J 2 8 S p Z r z F Y h n r b k A N l 0 L x F g q U v J t o T q N A 8 k 4 w u Z 3 7 n S e u j Y j V I 0 4 T 7 k d 0 p E Q o G E U r P e C V N y h X 3 K q 7 A F k n X k 4 q k K M 5 K H / 1 h z F L I 6 6 Q S W p M z 3 M T 9 D O q U T D J Z 6 V + a n h C 2 Y S O e M 9 S R S N u / G x x 6 o x c W GV I w l j b U k g W 6 u + J j E b G T K P A d k Y U x 2 b V m 4 v / e b 0 U w 7 q f C Z W k y B V b L gp T S T A m 8 7 / J U G j O U E 4 t o U w L e y t h Y 6 o p Q 5 t O y Y b g r b 6 8 T t r X V c + t e v e 1 S q O e x 1 G E M z i H S / D g B h p w B 0 1 o A Y M R P M M r v D n S e X H e n Y 9 l a 8 H J Z 0 7 h D 5 z P H 7 a N j W A = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " m Y y e R Y K F t l p L J P z e f R y q E G c K O r w = " &gt; A AA B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B i y W R g j 0 W v H i s a D + g D W W z 3 b R L N 5 u w O x F K 6 E / w 4 k E R r / 4 i b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k Q K g 6 7 7 7 R Q 2 N r e 2 d 4 q 7 p b 3 9 g 8 O j 8 v F J 2 8 S p Z r z F Y h n r b k A N l 0 L x F g q U v J t o T q N A 8 k 4 w u Z 3 7 n S e u j Y j V I 0 4 T 7 k d 0 p E Q o G E U r P e C V N y h X 3 K q 7 A F k n X k 4 q k K M 5 K H / 1 h z F L I 6 6 Q S W p M z 3 M T 9 D O q U T D J Z 6 V + a n h C 2 Y S O e M 9 S R S N u / G x x 6 o x c W G V I w l j b U k g W 6 u + J j E b G T K P A d k Y U x 2 b V m 4 v / e b 0 U w 7 q f C Z W k y B V b L g p T S T A m 8 7 / J U G j O U E 4 t o U w L e y t h Y 6 o p Q 5 t O y Y b g r b 6 8 T t r X V c + t e v e 1 S q O e x 1 G E M z i H S / D g B h p w B 0 1 o A Y M R P M M r v D n S e X H e n Y 9 l a 8 H J Z 0 7 h D 5 z P H 7 a N j W A = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " m Y y e R Y K F t l p L J P z e f R y q E G c K O r w = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B i y W R g j 0 W v H i s a D + g D W W z 3 b R L N 5 u w O x F K 6 E / w 4 k E R r / 4 i b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k Q K g 6 7 7 7 R Q 2 N r e 2 d 4 q 7 p b 3 9 g 8 O j 8 v F J 2 8 S p Z r z F Y h n r b k A N l 0 L x F g q U v J t o T q N A 8 k 4 w u Z 3 7 n S e u j Y j V I 0 4 T 7 k d 0 p E Q o G E U r P e C V N y h X 3 K q 7 A F k n X k 4 q k K M 5 K H / 1 h z F L I 6 6 Q S W p M z 3 M T 9 D O q U T D J Z 6 V + a n h C 2 Y S O e M 9 S R S N u / G x x 6 o x c W G V I w l j b U k g W 6 u + J j E b G T K P A d k Y U x 2 b V m 4 v / e b 0 U w 7 q f C Z W k y B V b L g p T S T A m 8 7 / J U G j O U E 4 t o U w L e y t h Y 6 o p Q 5 t O y Y b g r b 6 8 T t r X V c + t e v e 1 S q O e x 1 G E M z i H S / D g B h p w B 0 1 o A Y M R P M M r v D n S e X H e n Y 9 l a 8 H J Z 0 7 h D 5 z P H 7 a N j W A = &lt; / l a t e x i t &gt; t &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " z e s 3 v B F X x j N R 5 a b V q S 2 G q V m U 2 R 0 = " &gt; A A A B 6 H i c b V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 h V 0 R z D H g x W M C 5 g H J E m Y n v c m Y 2 Q c z v U I I + Q I v H h T x 6 i d 5 8 2 + c J H v Q x I K G o q q b 7 q 4 g V d K Q 6 3 4 7 G 5 t b 2 z u 7 h b 3 i / s H h 0 X H p 5 L R l k k w L b I p E J b o T c I N K x t g k S Q o 7 q U Y e B Q r b w f h u 7 r e f U B u Z x A 8 0 S d G P + D C W o R S c r N S g f q n s V t w F 2 D r x c l K G H P V + 6 a s 3 S E Q W Y U x C c W O 6 n p u S P + W a p F A 4 K / Y y g y k X Y z 7 E r q U x j 9 D 4 0 8 W h M 3 Z p l Q E L E 2 0 r J r Z Q f 0 9 M e W T M J A p s Z 8 R p Z F a 9 u f i f 1 8 0 o r P p T G a c Z Y S y W i 8 J M M U r Y / G s 2 k B o F q Y k l X G h p b 2 V i x D U X Z L M p 2 h C 8 1 Z f X S e u 6 4 r k V r 3 F T r l X z O A p w D h d w B R 7 c Q g 3 u o Q 5 N E I D w D K / w 5 j w 6 L 8 6 7 8 7 F s 3 X D y m T P 4 A + f z B 9 0 n j O 4 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " z e s 3 v B F X x j N R 5 a b V q S 2 G q V m U 2 R 0 = " &gt; A A A B 6 H i c b V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 h V 0 R z D H g x W M C 5 g H J E m Y n v c m Y 2 Q c z v U I I + Q I v H h T x 6 i d 5 8 2 + c J H v Q x I K G o q q b 7 q 4 g V d K Q 6 3 4 7 G 5 t b 2 z u 7 h b 3 i / s H h 0 X H p 5 L R l k k w L b I p E J b o T c I N K x t g k S Q o 7 q U Y e B Q r b w f h u 7 r e f U B u Z x A 8 0 S d G P + D C W o R S c r N S g f q n s V t w F 2 D r x c l K G H P V + 6 a s 3 S E Q W Y U x C c W O 6 n p u S P + W a p F A 4 K / Y y g y k X Y z 7 E r q U x j 9 D 4 0 8 W h M 3 Z p l Q E L E 2 0 r J r Z Q f 0 9 M e W T M J A p s Z 8 R p Z F a 9 u f i f 1 8 0 o r P p T G a c Z Y S y W i 8 J M M U r Y / G s 2 k B o F q Y k l X G h p b 2 V i x D U X Z L M p 2 h C 8 1 Z f X S e u 6 4 r k V r 3 F T r l X z O A p w D h d w B R 7 c Q g 3 u o Q 5 N E I D w D K / w 5 j w 6 L 8 6 7 8 7 F s 3 X D y m T P 4 A + f z B 9 0 n j O 4 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " z e s 3 v B F X x j N R 5 a b V q S 2 G q V m U 2 R 0 = " &gt; A A A B 6 H i c b V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 h V 0 R z D H g x W M C 5 g H J E m Y n v c m Y 2 Q c z v U I I + Q I v H h T x 6 i d 5 8 2 + c J H v Q x I K G o q q b 7 q 4 g V d K Q 6 3 4 7 G 5 t b 2 z u 7 h b 3 i / s H h 0 X H p 5 L R l k k w L b I p E J b o T c I N K x t g k S Q o 7 q U Y e B Q r b w f h u 7 r e f U B u Z x A 8 0 S d G P + D C W o R S c r N S g f q n s V t w F 2 D r x c l K G H P V + 6 a s 3 S E Q W Y U x C c W O 6 n p u S P + W a p F A 4 K / Y y g y k X Y z 7 E r q U x j 9 D 4 0 8 W h M 3 Z p l Q E L E 2 0 r J r Z Q f 0 9 M e W T M J A p s Z 8 R p Z F a 9 u f i f 1 8 0 o r P p T G a c Z Y S y W i 8 J M M U r Y / G s 2 k B o F q Y k l X G h p b 2 V i x D U X Z L M p 2 h C 8 1 Z f X S e u 6 4 r k V r 3 F T r l X z O A p w D h d w B R 7 c Q g 3 u o Q 5 N E I D w D K / w 5 j w 6 L 8 6 7 8 7 F s 3 X D y m T P 4 A + f z B 9 0 n j O 4= &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " z e s 3 v B F X x j N R 5 a b V q S 2 G q V m U 2 R 0 = " &gt; A A A B 6 H i c b V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 h V 0 R z D H g x W M C 5 g H J E m Y n v c m Y 2 Q c z v U I I + Q I v H h T x 6 i d 5 8 2 + c J H v Q x I K G o q q b 7 q 4 g V d K Q 6 3 4 7 G 5 t b 2 z u 7 h b 3 i / s H h 0 X H p 5 L R l k k w L b I p E J b o T c I N K x t g k S Q o 7 q U Y e B Q r b w f h u 7 r e f U B u Z x A 8 0 S d G P + D C W o R S c r N S g f q n s V t w F 2 D r x c l K G H P V + 6 a s 3 S E Q W Y U x C c W O 6 n p u S P + W a p FA 4 K / Y y g y k X Y z 7 E r q U x j 9 D 4 0 8 W h M 3 Z p l Q E L E 2 0 r J r Z Q f 0 9 M e W T M J A p s Z 8 R p Z F a 9 u f i f 1 8 0 o r P p T G a c Z Y S y W i 8 J M M U r Y / G s 2 k B o F q Y k l X G h p b 2 V i x D U X Z L M p 2 h C 8 1 Z f X S e u 6 4 r k V r 3 F T r l X z O A p w D h d w B R 7 c Q g 3 u o Q 5 N E I D w D K / w 5 j w 6 L 8 6 7 8 7 F s 3 X D y m T P 4 A + f z B 9 0 n j O 4 = &lt; / l a t e x i t &gt; t &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " z e s 3 v B F X x j N R 5 a b V q S 2 G q V m U 2 R 0 = " &gt; A A A B 6 H i c b V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 h V 0 R z D H g x W M C 5 g H J E m Y n v c m Y 2 Q c z v U I I + Q I v H h T x 6 i d 5 8 2 + c J H v Q x I K G o q q b 7 q 4 g V d K Q 6 3 4 7 G 5 t b 2 z u 7 h b 3 i / s H h 0 X H p 5 L R l k k w L b I p E J b o T c I N K x t g k S Q o 7 q U Y e B Q r b w f h u 7 r e f U B u Z x A 8 0 S d G P + D C W o R S c r N S g f q n s V t w F 2 D r x c l K G H P V + 6 a s 3 S E Q W Y U x C c W O 6 n p u S P + W a p F A 4 K / Y y g y k X Y z 7 E r q U x j 9 D 4 0 8 W h M 3 Z p l Q E L E 2 0 r J r Z Q f 0 9 M e W T M J A p s Z 8 R p Z F a 9 u f i f 1 8 0 o r P p T G a c Z Y S y W i 8 J M M U r Y / G s 2 k B o F q Y k l X G h p b 2 V i x D U X Z L M p 2 h C 8 1 Z f X S e u 6 4 r k V r 3 F T r l X z O A p w D h d w B R 7 c Q g 3 u o Q 5 N E I D w D K / w 5 j w 6 L 8 6 7 8 7 F s 3 X D y m T P 4 A + f z B 9 0 n j O 4 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " z e s 3 v B F X x j N R 5 a b V q S 2 G q V m U 2 R 0 = " &gt; A A A B 6 H i c b V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 h V 0 R z D H g x W M C 5 g H J E m Y n v c m Y 2 Q c z v U I I + Q I v H h T x 6 i d 5 8 2 + c J H v Q x I K G o q q b 7 q 4 g V d K Q 6 3 4 7 G 5 t b 2 z u 7 h b 3 i / s H h 0 X H p 5 L R l k k w L b I p E J b o T c I N K x t g k S Q o 7 q U Y e B Q r b w f h u 7 r e f U B u Z x A 8 0 S d G P + D C W o R S c r N S g f q n s V t w F 2 D r x c l K G H P V + 6 a s 3 S E Q W Y U x C c W O 6 n p u S P + W a p F A 4 K / Y y g y k X Y z 7 E r q U x j 9 D 4 0 8 W h M 3 Z p l Q E L E 2 0 r J r Z Q f 0 9 M e W T M J A p s Z 8 R p Z F a 9 u f i f 1 8 0 o r P p T G a c Z Y S y W i 8 J M M U r Y / G s 2 k B o F q Y k l X G h p b 2 V i x D U X Z L M p 2 h C 8 1 Z f X S e u 6 4 r k V r 3 F T r l X z O A p w D h d w B R 7 c Q g 3 u o Q 5 N E I D w D K / w 5 j w 6 L 8 6 7 8 7 F s 3 X D y m T P 4 A + f z B 9 0 n j O 4 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " z e s 3 v B F X x j N R 5 a b V q S 2 G q V m U 2 R 0 = " &gt; A A A B 6 H i c b V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 h V 0 R z D H g x W M C 5 g H J E m Y n v c m Y 2 Q c z v U I I + Q I v H h T x 6 i d 5 8 2 + c J H v Q x I K G o q q b 7 q 4 g V d K Q 6 3 4 7 G 5 t b 2 z u 7 h b 3 i / s H h 0 X H p 5 L R l k k w L b I p E J b o T c I N K x t g k S Q o 7 q U Y e B Q r b w f h u 7 r e f U B u Z x A 8 0 S d G P + D C W o R S c r N S g f q n s V t w F 2 D r x c l K G H P V + 6 a s 3 S E Q W Y U x C c W O 6 n p u S P + W a p F A 4 K / Y y g y k X Y z 7 E r q U x j 9 D 4 0 8 W h M 3 Z p l Q E L E 2 0 r J r Z Q f 0 9 M e W T M J A p s Z 8 R p Z F a 9 u f i f 1 8 0 o r P p T G a c Z Y S y W i 8 J M M U r Y / G s 2 k B o F q Y k l X G h p b 2 V i x D U X Z L M p 2 h C 8 1 Z f X S e u 6 4 r k V r 3 F T r l X z O A p w D h d w B R 7 c Q g 3 u o Q 5 N E I D w D K / w 5 j w 6 L 8 6 7 8 7 F s 3 X D y m T P 4 A + f z B 9 0 n j O 4 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " z e s 3 v B F X x j N R 5 a b V q S 2 G q V m U 2 R 0 = " &gt; A A A B 6 H i c b V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 h V 0 R z D H g x W M C 5 g H J E m Y n v c m Y 2 Q c z v U I I + Q I v H h T x 6 i d 5 8 2 + c J H v Q x I K G o q q b 7 q 4 g V d K Q 6 3 4 7 G 5 t b 2 z u 7 h b 3 i / s H h 0 X H p 5 L R l k k w L b I p E J b o T c I N K x t g k S Q o 7 q U Y e B Q r b w f h u 7 r e f U B u Z x A 8 0 S d G P + D C W o R S c r N S g f q n s V t w F 2 D r x c l K G H P V + 6 a s 3 S E Q W Y U x C c W O 6 n p u S P + W a p F A 4 K / Y y g y k X Y z 7 E r q U x j 9 D 4 0 8 W h M 3 Z p l Q E L E 2 0 r J r Z Q f 0 9 M e W T M J A p s Z 8 R p Z F a 9 u f i f 1 8 0 o r P p T G a c Z Y S y W i 8 J M M U r Y / G s 2 k B o F q Y k l X G h p b 2 V i x D U X Z L M p 2 h C 8 1 Z f X S e u 6 4 r k V r 3 F T r l X z O A p w D h d w B R 7 c Q g 3 u o Q 5 N E I D w D K / w 5 j w 6 L 8 6 7 8 7 F s 3 X D y m T P 4 A + f z B 9 0 n j O 4 = &lt; / l a t e x i t &gt; {b k1 t , b k2 t , · · · } &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " d t N S 1 w s L p Z A S 1 4 a s e C 2 v A 5 + A 9 + 8 = " &gt; A A A C G H i c b Z D L S s N A F I Y n X m u 9 R V 2 6 G S y C C 6 l J E X R Z c O O y g r 1 A E 8 N k O m m H T i 7 M n A g l 5 D H c + C p u X C j i t j v f x m m a R W 3 9 Y e D n O + d w 5 v x + I r g C y / o x 1 t Y 3 N r e 2 K z v V 3 b 3 9 g 0 P z 6 L i j 4 l R S 1 q a x i G X P J 4 o J H r E 2 c B C s l 0 h G Q l + w r j + + m 9 W 7 z 0 w q H k e P M E m Y G 5 J h x A N O C W j k m V d O l m W O H 2 A / f 8 r G n p 1 7 k F 8 u k k Z B s E M H M S g n 9 8 y a V b c K 4 V V j l 6 a G S r U 8 c + o M Y p q G L A I q i F J 9 2 0 r A z Y g E T g X L q 0 6 q W E L o m A x Z X 9 u I h E y 5 W X F Y j s 8 1 G e A g l v p F g A u 6 O J G R U K l J 6 O v O k M B I L d d m 8 L 9 a P 4 X g 1 s 1 4 l K T A I j p f F K Q C Q 4 x n K e E B l 4 y C m G h D q O T 6 r 5 i O i C Q U d J Z V H Y K 9 f P K q 6 T T q t l W 3 H 6 5 r z U Y Z R w W d o j N 0 g W x 0 g 5 r o H r V Q G 1 H 0 g t 7 Q B / o 0 X o 1 3 4 8 v 4 n r e u G e X M C f o j Y / o L X o O g h w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " d t N S 1 w s L p Z A S 1 4 a s e C 2 v A 5 + A 9 + 8 = " &gt; A A A C G H i c b Z D L S s N A F I Y n X m u 9 R V 2 6 G S y C C 6 l J E X R Z c O O y g r 1 A E 8 N k O m m H T i 7 M n A g l 5 D H c + C p u X C j i t j v f x m m a R W 3 9 Y e D n O + d w 5 v x + I r g C y / o x 1 t Y 3 N r e 2 K z v V 3 b 3 9 g 0 P z 6 L i j 4 l R S 1 q a x i G X P J 4 o J H r E 2 c B C s l 0 h G Q l + w r j + + m 9 W 7 z 0 w q H k e P M E m Y G 5 J h x A N O C W j k m V d O l m W O H 2 A / f 8 r G n p 1 7 k F 8 u k k Z B s E M H M S g n 9 8 y a V b c K 4 V V j l 6 a G S r U 8 c + o M Y p q G L A I q i F J 9 2 0 r A z Y g E T g X L q 0 6 q W E L o m A x Z X 9 u I h E y 5 W X F Y j s 8 1 G e A g l v p F g A u 6 O J G R U K l J 6 O v O k M B I L d d m 8 L 9 a P 4 X g 1 s 1 4 l K T A I j p f F K Q C Q 4 x n K e E B l 4 y C m G h D q O T 6 r 5 i O i C Q U d J Z V H Y K 9 f P K q 6 T T q t l W 3 H 6 5 r z U Y Z R w W d o j N 0 g W x 0 g 5 r o H r V Q G 1 H 0 g t 7 Q B / o 0 X o 1 3 4 8 v 4 n r e u G e X M C f o j Y / o L X o O g h w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " d t N S 1 w s L p Z A S 1 4 a s e C 2 v A 5 + A 9 + 8 = " &gt; A A A C G H i c b Z D L S s N A F I Y n X m u 9 R V 2 6 G S y C C 6 l J E X R Z c O O y g r 1 A E 8 N k O m m H T i 7 M n A g l 5 D H c + C p u X C j i t j v f x m m a R W 3 9 Y e D n O + d w 5 v x + I r g C y / o x 1 t Y 3 N r e 2 K z v V 3 b 3 9 g 0 P z 6 L i j 4 l R S 1 q a x i G X P J 4 o J H r E 2 c B C s l 0 h G Q l + w r j + + m 9 W 7 z 0 w q H k e P M E m Y G 5 J h x A N O C W j k m V d O l m W O H 2 A / f 8 r G n p 1 7 k F 8 u k k Z B s E M H M S g n 9 8 y a V b c K 4 V V j l 6 a G S r U 8 c + o M Y p q G L A I q i F J 9 2 0 r A z Y g E T g X L q 0 6 q W E L o m A x Z X 9 u I h E y 5 W X F Y j s 8 1 G e A g l v p F g A u 6 O J G R U K l J 6 O v O k M B I L d d m 8 L 9 a P 4 X g 1 s 1 4 l K T A I j p f F K Q C Q 4 x n K e E B l 4 y C m G h D q O T 6 r 5 i O i C Q U d J Z V H Y K 9 f P K q 6 T T q t l W 3 H 6 5 r z U Y Z R w W d o j N 0 g W x 0 g 5 r o H r V Q G 1 H 0 g t 7 Q B / o 0 X o 1 3 4 8 v 4 n r e u G e X M C f o j Y / o L X o O g h w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " d t N S 1 w s L p Z A S 1 4 a s e C 2 v A 5 + A 9 + 8 = " &gt; A A A C G H i c b Z D L S s N A F I Y n X m u 9 R V 2 6 G S y C C 6 l J E X R Z c O O y g r 1 A E 8 N k O m m H T i 7 M n A g l 5 D H c + C p u X C j i t j v f x m m a R W 3 9 Y e D n O + d w 5 v x + I r g C y / o x 1 t Y 3 N r e 2 K z v V 3 b 3 9 g 0 P z 6 L i j 4 l R S 1 q a x i G X P J 4 o J H r E 2 c B C s l 0 h G Q l + w r j + + m 9 W 7 z 0 w q H k e P M E m Y G 5 J h x A N O C W j k m V d O l m W O H 2 A / f 8 r G n p 1 7 k F 8 u k k Z B s E M H M S g n 9 8 y a V b c K 4 V V j l 6 a G S r U 8 c + o M Y p q G L A I q i F J 9 2 0 r A z Y g E T g X L q 0 6 q W E L o m A x Z X 9 u I h E y 5 W X F Y j s 8 1 G e A g l v p F g A u 6 O J G R U K l J 6 O v O k M B I L d d m 8 L 9 a P 4 X g 1 s 1 4 l K T A I j p f F K Q C Q 4 x n K e E B l 4 y C m G h D q O T 6 r 5 i O i C Q U d J Z V H Y K 9 f P K q 6 T T q t l W 3 H 6 5 r z U Y Z R w W d o j N 0 g W x 0 g 5 r o H r V Q G 1 H 0 g t 7 Q B / o 0 X o 1 3 4 8 v 4 n r e u G e X M C f o j Y / o L X o O g h w = = &lt; / l a t e x i t &gt; The presented Tracktor accomplishes multi-object tracking only with an object detector and consists of two primary processing steps, indicated in blue and red, for a given frame t. First, the regression of the object detector aligns already existing track bounding boxes b k t−1 of frame t − 1 to the object's new position at frame t. The corresponding object classification scores s k t of the new bounding box positions are then used to kill potentially occluded tracks. Second, the object detector (or a given set of public detections) provides a set of detections D t of frame t. Finally, a new track is initialized if a detection has no substantial Intersection over Union with any bounding box of the set of active tracks B t = {b k1 t , b k2 t , · · · }.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3, we quantify this ability in terms of detection gaps The two rows illustrate the ratio of tracked objects with respect to: (i) object heights and (ii) the length of gaps in the provided public detections. The transparent red bars indicate the ground truth distribution of heights and gap lengths in the detections, respectively. To demonstrate the shortcomings of the presented trackers we limited the height comparison to objects with visibility greater or equal than 0.9. Tracks that are not detected at all are not considered as a gap. Hence, SPD generates the most gaps. For it also provides the most detections.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 1 :</head><label>1</label><figDesc>Tracking performance of Tracktor and Tracktor++ on low frame rate versions of the MOT17-{02, 04, 09, 10, 11}-FRCNN sequences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Method MOTA ↑ IDF1 ↑ MT ↑ ML ↓ FP ↓ FN ↓ ID Sw. ↓</figDesc><table><row><cell></cell><cell>Tracktor++</cell><cell>53.5</cell><cell cols="2">52.3 19.5 36.6 12201 248047 2072</cell></row><row><cell>MOT17</cell><cell>eHAF [58] FWT [23] jCC [30] MOTDT17 [9]</cell><cell>51.8 51.3 51.2 50.9</cell><cell cols="2">54.7 23.4 37.9 33212 236772 1834 47.6 21.4 35.2 24101 247921 2648 54.5 20.9 37.0 25937 247822 1802 52.7 17.5 35.7 24069 250768 2474</cell></row><row><cell></cell><cell>MHT DAM [32]</cell><cell>50.7</cell><cell cols="2">47.2 20.8 36.9 22875 252889 2314</cell></row><row><cell></cell><cell>Tracktor++</cell><cell>54.4</cell><cell>52.5 19.0 36.9 3280 79149</cell><cell>682</cell></row><row><cell>MOT16</cell><cell>HCC [44] LMP [59] GCRA [43] FWT [23]</cell><cell>49.3 48.8 48.2 47.8</cell><cell>50.7 17.8 39.9 5333 86795 51.3 18.2 40.1 6654 86245 48.6 12.9 41.1 5104 88586 44.3 19.1 38.2 8886 85487</cell><cell>391 481 821 852</cell></row><row><cell></cell><cell>MOTDT [9]</cell><cell>47.6</cell><cell>50.9 15.2 38.3 9253 85431</cell><cell>792</cell></row><row><cell>2D MOT 2015</cell><cell>Tracktor++ AP HWDPL p [8] AMIR15 [56] JointMC [30] RAR15pub [17]</cell><cell>44.1 38.5 37.6 35.6 35.1</cell><cell>46.7 18.0 26.2 6477 26577 47.1 8.7 37.4 4005 33203 46.0 15.8 26.8 7933 29397 45.1 23.2 39.3 10580 28508 45.4 13.0 42.3 6771 32717</cell><cell>1318 586 1026 457 381</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell>We compare our online multi-object tracker Track-</cell></row><row><cell>tor++ with other modern tracking methods. As a result, we</cell></row><row><cell>achieve a new state-of-the-art in terms of MOTA for pub-</cell></row><row><cell>lic detections on all three MOTChallenge benchmarks. The</cell></row><row><cell>arrows indicate low or high optimal metric values.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>MethodMOTA ↑ IDF1 ↑ FP ↓ FN ↓ ID Sw. ↓</figDesc><table><row><cell>Tracktor</cell><cell>61.5</cell><cell>61.1</cell><cell cols="2">367 42903</cell><cell>1747</cell></row><row><cell>Tracktor++</cell><cell>+0.4</cell><cell>+3.6</cell><cell>-44</cell><cell>-449</cell><cell>-1421</cell></row><row><cell>Oracle-Kill</cell><cell>+0.7</cell><cell>-0.7</cell><cell cols="2">-178 -694</cell><cell>+129</cell></row><row><cell>Oracle-REG</cell><cell>+1.4</cell><cell>+5.6</cell><cell cols="3">-218 -1401 -1463</cell></row><row><cell>Oracle-MM</cell><cell>+0.9</cell><cell>+5.2</cell><cell cols="2">-168 -898</cell><cell>-1332</cell></row><row><cell>Oracle-reID</cell><cell>0.0</cell><cell>+10.0</cell><cell>0</cell><cell>0</cell><cell>-1094</cell></row><row><cell>Oracle-MM-reID</cell><cell>+0.9</cell><cell cols="3">+13.9 -168 -898</cell><cell>-1706</cell></row><row><cell cols="2">Oracle-MM-reID-INTER +2.6</cell><cell cols="4">+15.9 +3774 -6769 -1680</cell></row><row><cell>Oracle-ALL</cell><cell>+10.7</cell><cell cols="4">+22.5 -360 -11745 -1743</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>: To show the potential of Tracktor and indicate</cell></row><row><cell>promising future research directions, we present multiple</cell></row><row><cell>oracle trackers. Each oracle exploits ground truth data for</cell></row><row><cell>a specific task, simulating, e.g., a perfect re-identification</cell></row><row><cell>(reID) or motion model (MM). We evaluate only on the</cell></row><row><cell>Faster R-CNN set of MOT17 public detections and high-</cell></row><row><cell>light performance gains and losses with respect to the</cell></row><row><cell>vanilla Tracktor in green and red, respectively. The arrows</cell></row><row><cell>indicate low or high optimal metric values.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 1 :</head><label>1</label><figDesc>A comparison of our Faster R-CNN (FRCNN) with Feature Pyramid Networks (FPN) implementation on the MOT17Det detection benchmark with the three object detection methods mentioned in this work. Our vanilla FR-CNN results are on par with the official FRCNN implementation. The extension with FPN yields a detection performance close to SDP. For a detailed summary of the shown detection metrics we refer to the official MOTChallenge web page: https://motchallenge.net.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>To appear in the Proc. of the IEEE International Conference on Computer Vision (ICCV). Seoul, Korea, October 2019. c 2019 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Method MOTA ↑ IDF1 ↑ MT ↑ ML ↓ FP ↓ FN ↓ ID Sw. ↓Tracktor++42.14 45.76 18.17 38.93 3918 83904 648 Tracktor-no-FPN++ 39.41 43.46 16.63 39.00 6975 83380 922 eHAF17 37.37 46.44 20.63 35.83 11050 86510 605 FWT 39.06 42.07 17.60 37.53 8397 88290 780 jCC 37.64 46.66 18.70 36.33 9984 86897</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>577</cell></row><row><cell>MOTDT17</cell><cell>38.81</cell><cell>46.34 14.47 36.91 8911 88773</cell><cell>731</cell></row><row><cell>MHT DAM</cell><cell>37.54</cell><cell>46.17 17.43 34.86 9795 89294</cell><cell>742</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 2 :</head><label>2</label><figDesc>Comparison on MOT17 test set with Faster R-CNN public detections. Tracktor-no-FPN++ applies vanilla Faster R-CNN.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>d t , s t ← detector.reg and class(d t ); NMS(D t , S, λ new ); ← T k + {d t }; 35 T active ← T active + {T k }; 36 T ← T + T active ; Sequence Detection MOTA ↑ IDF1 ↑ MT ↑ ML ↓ FP ↓ FN ↓ ID Sw. ↓</figDesc><table><row><cell></cell><cell></cell><cell cols="2">MOT17 [45]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MOT17-01</cell><cell>DPM [19]</cell><cell>35.9</cell><cell cols="4">37.1 20.8 50.0 131</cell><cell>3962</cell><cell>39</cell></row><row><cell>MOT17-03</cell><cell>DPM</cell><cell>65.2</cell><cell cols="5">57.0 35.1 12.8 1338 34840</cell><cell>222</cell></row><row><cell>MOT17-06</cell><cell>DPM</cell><cell>52.7</cell><cell cols="4">55.7 18.5 40.1 184</cell><cell>5310</cell><cell>80</cell></row><row><cell>MOT17-07</cell><cell>DPM</cell><cell>40.5</cell><cell cols="4">42.5 10.0 40.0 363</cell><cell>9603</cell><cell>90</cell></row><row><cell>MOT17-08</cell><cell>DPM</cell><cell>27.0</cell><cell>30.7</cell><cell cols="4">9.2 50.0 213 15130</cell><cell>83</cell></row><row><cell>MOT17-12</cell><cell>DPM</cell><cell>45.6</cell><cell cols="3">55.2 16.5 48.4</cell><cell>88</cell><cell>4596</cell><cell>29</cell></row><row><cell>MOT17-14</cell><cell>DPM</cell><cell>26.9</cell><cell>37.1</cell><cell cols="4">6.7 53.0 591 12834</cell><cell>92</cell></row><row><cell>MOT17-01</cell><cell>FRCNN [52]</cell><cell>34.9</cell><cell cols="4">34.8 20.8 41.7 406</cell><cell>3753</cell><cell>39</cell></row><row><cell>MOT17-03</cell><cell>FRCNN</cell><cell>66.4</cell><cell cols="5">59.7 37.2 13.5 1014 33961</cell><cell>189</cell></row><row><cell>MOT17-06</cell><cell>FRCNN</cell><cell>56.7</cell><cell cols="4">59.0 23.0 27.5 359</cell><cell>4647</cell><cell>96</cell></row><row><cell>MOT17-07</cell><cell>FRCNN</cell><cell>39.4</cell><cell cols="4">43.1 11.7 40.0 555</cell><cell>9588</cell><cell>93</cell></row><row><cell>MOT17-08</cell><cell>FRCNN</cell><cell>27.1</cell><cell cols="5">31.7 11.8 50.0 197 15119</cell><cell>74</cell></row><row><cell>MOT17-12</cell><cell>FRCNN</cell><cell>43.4</cell><cell cols="4">53.9 15.4 51.6 185</cell><cell>4697</cell><cell>25</cell></row><row><cell>MOT17-14</cell><cell>FRCNN</cell><cell>27.1</cell><cell>38.1</cell><cell cols="4">7.3 48.2 1202 12139</cell><cell>132</cell></row><row><cell>MOT17-01</cell><cell>SDP [63]</cell><cell>37.5</cell><cell cols="4">36.8 25.0 41.7 283</cell><cell>3706</cell><cell>42</cell></row><row><cell>MOT17-03</cell><cell>SDP</cell><cell>69.6</cell><cell cols="5">60.1 39.9 10.8 2469 29065</cell><cell>248</cell></row><row><cell>MOT17-06</cell><cell>SDP</cell><cell>56.8</cell><cell cols="4">59.2 26.1 28.8 354</cell><cell>4638</cell><cell>93</cell></row><row><cell>MOT17-07</cell><cell>SDP</cell><cell>41.2</cell><cell cols="4">42.6 11.7 33.3 596</cell><cell>9231</cell><cell>111</cell></row><row><cell>MOT17-08</cell><cell>SDP</cell><cell>28.7</cell><cell cols="5">32.1 13.2 47.4 253 14715</cell><cell>103</cell></row><row><cell>MOT17-12</cell><cell>SDP</cell><cell>45.3</cell><cell cols="4">56.9 18.7 48.4 212</cell><cell>4492</cell><cell>34</cell></row><row><cell>MOT17-14</cell><cell>SDP</cell><cell>27.6</cell><cell>38.5</cell><cell cols="4">7.3 48.2 1208 12021</cell><cell>158</cell></row><row><cell>All</cell><cell></cell><cell>53.5</cell><cell cols="6">52.3 19.5 36.6 12201 248047 2072</cell></row><row><cell></cell><cell></cell><cell cols="2">MOT16 [45]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MOT16-03</cell><cell>DPM</cell><cell>65.8</cell><cell cols="5">57.9 35.1 12.2 1397 34101</cell><cell>226</cell></row><row><cell>MOT16-06</cell><cell>DPM</cell><cell>53.9</cell><cell cols="4">57.9 20.4 39.4 243</cell><cell>5000</cell><cell>80</cell></row><row><cell>MOT16-07</cell><cell>DPM</cell><cell>43.0</cell><cell cols="4">43.6 13.0 33.3 405</cell><cell>8808</cell><cell>97</cell></row><row><cell>MOT16-08</cell><cell>DPM</cell><cell>34.3</cell><cell cols="5">36.8 12.7 38.1 314 10577</cell><cell>101</cell></row><row><cell>MOT16-12</cell><cell>DPM</cell><cell>48.0</cell><cell cols="4">57.0 18.6 44.2 108</cell><cell>4172</cell><cell>30</cell></row><row><cell>MOT16-14</cell><cell>DPM</cell><cell>27.4</cell><cell>37.6</cell><cell cols="4">6.7 51.2 659 12645</cell><cell>108</cell></row><row><cell>All</cell><cell></cell><cell>54.4</cell><cell cols="5">52.5 19.0 36.9 3280 79149</cell><cell>682</cell></row><row><cell></cell><cell cols="3">2D MOT 2015 [37]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>TUD-Crossing</cell><cell>ACF [14]</cell><cell>78.3</cell><cell cols="3">58.3 53.8 0.0</cell><cell>14</cell><cell>207</cell><cell>18</cell></row><row><cell>PETS09-S2L2</cell><cell>ACF</cell><cell>44.5</cell><cell>28.4</cell><cell>4.8</cell><cell>2.4</cell><cell>644</cell><cell>4420</cell><cell>289</cell></row><row><cell>ETH-Jelmoli</cell><cell>ACF</cell><cell>57.8</cell><cell cols="4">67.4 35.6 24.4 317</cell><cell>732</cell><cell>21</cell></row><row><cell cols="2">ETH-Linthescher ACF ETH-Crossing ACF AVG-TownCentre ACF ADL-Rundle ACF-1 ADL-Rundle ACF-3 KITTI-16 ACF</cell><cell>49.3 43.0 39.0 33.7 45.6 48.1</cell><cell cols="6">28 55.5 15.7 50.8 178 for d t ∈ D t do 29 for b k 54.2 11.5 38.5 22 t ∈ B do 4303 538 30 if IoU(d t , b k 38.5 17.3 19.0 620 3075 t ) &gt; λ new then 48 12 665 31 49.3 28.1 9.4 2497 3615 56 46.0 15.9 13.6 750 4713 68 D t ← D t − {d t }; 50.8 17.6 5.9 174 672 37</cell></row><row><cell>KITTI-19 Venice-1</cell><cell>ACF ACF</cell><cell>49.4 35.1</cell><cell cols="4">32 59.5 14.5 14.5 553 for d t ∈ D t do 33 T k ← ∅; 42.6 23.5 29.4 708</cell><cell>2082 2220</cell><cell>71 33</cell></row><row><cell>All</cell><cell></cell><cell>44.1</cell><cell cols="5">46.7 18.0 26.2 6477 26577</cell><cell>1318</cell></row></table><note>23 if s t &lt; σ active then24 Dt ← D t − {d t };25 else26 S ← S + {s t };27 Dt ←34 Tk</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 3 :</head><label>3</label><figDesc>A detailed summary of the tracking results of our Tracktor++ tracker on all three MOTChallenge benchmarks. The results are separated into individual sequences and sets of public detections.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Tracktor code: https://git.io/fjQr8.<ref type="bibr" target="#b1">2</ref> The MOTChallenge web page: https://motchallenge.net.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This research was funded by the Humboldt Foundation through the Sofja Kovalevskaja Award.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Tracktor algorithm (private detections)</head><p>Data: Video sequence as ordered list I = {i 0 , i 1 , · · · , i T −1 } of images i t .</p><p>Result: Set of object trajectories</p><p>Algorithm 2: Tracktor algorithm (public detections) Data: Video sequence as ordered list I = {i 0 , i 1 , · · · , i T −1 } of images i t and public detections as ordered list </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Social lstm: Human trajectory prediction in crowded spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kratarth</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Robicquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multi-target tracking by continuous energy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Andriyenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Robust people tracking with global trajectory optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jérôme</forename><surname>Berclaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franois</forename><surname>Fleuret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="744" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multiple object tracking using k-shortest paths optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerome</forename><surname>Berclaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Fleuret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Engin</forename><surname>Turetken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1806" to="1819" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Robust tracking-bydetection using a detector confidence particle filter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">D</forename><surname>Breitenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Reichlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esther</forename><surname>Koller-Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1515" to="1522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Signature verification using a &quot;siamese&quot; time delay neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jane</forename><surname>Bromley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Saeckinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roopak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Real-time &apos;actor-critic&apos; tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peixia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Online multi-object tracking with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haizhou</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijie</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Bai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="645" to="649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Real-time multiple people tracking with deeply learned candidate selection and person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haizhou</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijie</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Shang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Near-online multi-target tracking with aggregated local flow descriptor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wongun</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multiple target tracking in world coordinate with single, minimally calibrated camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wongun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A unified framework for multi-target tracking and collective activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wongun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="215" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Online multi-object tracking with historical appearance matching and scene adaptive detection filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijeet</forename><surname>Young Chul Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Boragule</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwangjin</forename><surname>Young Min Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moongu</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AVSS</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Serge Belongie, and Pietro Perona. Fast feature pyramids for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Appel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1532" to="1545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A mobile vision system for robust multi-person tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Parametric image alignment using enhanced correlation coefficient maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Georgios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanouil</forename><forename type="middle">Z</forename><surname>Evangelidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Psarakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1858" to="1865" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Recurrent autoregressive networks for online multi-object tracking. WACV, abs/1711.02741</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Detect to track and track to detect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Object detection with discriminatively trained part based models. pami</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<title level="m">Fast r-cnn. ICCV</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1512.03385</idno>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Improvements to frank-wolfe optimization for multi-detector multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
		</author>
		<idno>abs/1705.08314</idno>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Efficient multiple people tracking using minimum cost arborescences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition (GCPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">In defense of the triplet loss for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<idno>abs/1703.07737</idno>
		<imprint>
			<date type="published" when="2004" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A linear programming approach for multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sidney</forename><forename type="middle">S</forename><surname>Fels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Object detection in videos with tubelet proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xihui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="889" to="897" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Object detection from video tubelets with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="817" to="825" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Framework for performance evaluation for face, text and vehicle detection and tracking in video: data, metrics, and protocol</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rangachar</forename><surname>Kasturi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dmitry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Padmanabhan</forename><surname>Goldgof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasant</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">S</forename><surname>Manohar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Garofolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Bowers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentina</forename><forename type="middle">N</forename><surname>Boonstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Korzhova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="319" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Motion segmentation &amp; multiple object tracking by correlation co-clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margret</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>PAMI</publisher>
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multiple hypothesis tracking revisited: Blending in modern appearance model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chanho</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arridhana</forename><surname>Ciptadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multiple hypothesis tracking revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chanho</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arridhana</forename><surname>Ciptadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015-12" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">How does person identity recognition help multi-person tracking?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Hao</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning by tracking: siamese cnn for robust target association</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Canton-Ferrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note>DeepVision: Deep Learning for Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning an image-based motion context for multiple people tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Fenzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.01942</idno>
		<title level="m">Motchallenge 2015: Towards a benchmark for multi-target tracking</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Tracking the trackers: An analysis of the state of the art in multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<idno>abs/1704.02781</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Everybody needs somebody: Modeling social and grouping behavior on a linear programming multiple people tracker</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV) Workshops. 1st Workshop on Modeling, Simulation and Visual Analysis of Large Crowds</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Branch-and-price global optimization for multi-view multitarget tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Kyun</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.7618</idno>
		<title level="m">Multiple object tracking: A review</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Trajectory factory: Tracklet cleaving and re-connection by deep siamese bi-gru for multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changshui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueqing</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizhu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Xie</surname></persName>
		</author>
		<idno>abs/1804.04555</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Customized multi-person tracker</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.00831</idno>
		<title level="m">Mot16: A benchmark for multi-object tracking</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Joint tracking and segmentation of multiple targets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">T-cnn: Tubelets with convolutional neural networks for object detection from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melissa</forename><surname>Ogden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingchen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tannan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruohui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2896" to="2907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">You&apos;ll never walk alone: modeling social behavior for multi-target tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Pellegrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Globally-optimal greedy algorithms for tracking a variable number of objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1201" to="1208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<title level="m">Yolo9000: Better, faster, stronger. CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Collaborative deep reinforcement learning for multiobject tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangliang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Performance measures and a data set for multi-target, multi-camera tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ergys</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Solera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><forename type="middle">S</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV Workshops</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Features for multi-target multi-camera tracking and re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ergys</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Tomasi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Learning social etiquette: Human trajectory prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Robicquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Tracking the untrackable: Learning to track multiple cues with long-term dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<idno>abs/1701.01909</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Learning pedestrian dynamics from the real world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Scovanner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marshall</forename><forename type="middle">F</forename><surname>Tappen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="381" to="388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Heterogeneous association graph fusion for target association in multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Multiple people tracking by lifted multicut and person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Efficient track linking methods for track graphs using network-flow and set-cover techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">H</forename><surname>Kunz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margrit</forename><surname>Betke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1185" to="1192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kota</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><forename type="middle">E</forename><surname>Ortiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<title level="m">Who are you with and where are you going? IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1345" to="1352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">An online learned crf model for multi-target tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Exploit all the layers: Fast and accurate cnn object detector with scale dependent pooling and cascaded rejection classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wongun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanqing</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="2129" to="2137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Multiple target tracking using spatio-temporal markov chain monte carlo data association</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Medioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isaac</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Gmcptracker: Global multi-object tracking using generalized minimum clique graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afshin</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Global data association for multi-object tracking using network flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakant</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Online multi-object tracking with dual matching attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minyoung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
